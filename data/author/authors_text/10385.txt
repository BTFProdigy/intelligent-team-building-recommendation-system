Cross Language Text Categorization Using a Bilingual Lexicon
Ke Wu, Xiaolin Wang and Bao-Liang Lu?
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dong Chuan Rd., Shanghai 200240, China
{wuke,arthur general,bllu}@sjtu.edu.cn
Abstract
With the popularity of the Internet at a phe-
nomenal rate, an ever-increasing number of
documents in languages other than English
are available in the Internet. Cross lan-
guage text categorization has attracted more
and more attention for the organization of
these heterogeneous document collections.
In this paper, we focus on how to con-
duct effective cross language text catego-
rization. To this end, we propose a cross
language naive Bayes algorithm. The pre-
liminary experiments on collected document
collections show the effectiveness of the pro-
posed method and verify the feasibility of
achieving performance close to monolingual
text categorization, using a bilingual lexicon
alone. Also, our algorithm is more efficient
than our baselines.
1 Introduction
Due to the popularity of the Internet, an ever-
increasing number of documents in languages other
than English are available in the Internet. The or-
ganization of these heterogeneous document collec-
tions increases cost of human labor significantly. On
the one hand, experts who know different languages
are required to organize these collections. On the
other hand, maybe there exist a large amount of la-
belled documents in a language (e.g. English) which
are in the same class structure as the unlabelled doc-
uments in another language. As a result, how to ex-
?Corresponding author.
ploit the existing labelled documents in some lan-
guage (e.g. English) to classify the unlabelled doc-
uments other than the language in multilingual sce-
nario has attracted more and more attention (Bel et
al., 2003; Rigutini et al, 2005; Olsson et al, 2005;
Fortuna and Shawe-Taylor, 2005; Li and Shawe-
Taylor, 2006; Gliozzo and Strapparava, 2006). We
refer to this task as cross language text categoriza-
tion. It aims to extend the existing automated text
categorization system from one language to other
languages without additional intervention of human
experts. Formally, given two document collections
{De,Df} from two different languages e and f re-
spectively, we use the labelled document collection
De in the language e to deduce the labels of the doc-
ument collection Df in the language f via an algo-
rithm A and some external bilingual resources.
Typically, some external bilingual lexical re-
sources, such as machine translation system (MT),
large-scale parallel corpora and multilingual ontol-
ogy etc., are used to alleviate cross language text
categorization. However, it is hard to obtain them
for many language pairs. In this paper, we focus on
using a cheap bilingual resource, e.g. bilingual lexi-
con without any translation information, to conduct
cross language text categorization. To my knowl-
edge, there is little research on using a bilingual lex-
icon alone for cross language text categorization.
In this paper, we propose a novel approach for
cross language text categorization via a bilingual
lexicon alone. We call this approach as Cross Lan-
guage Naive Bayes Classifier (CLNBC). The pro-
posed approach consists of two main stages. The
first stage is to acquire a probabilistic bilingual lex-
165
icon. The second stage is to employ naive Bayes
method combined with Expectation Maximization
(EM) (Dempster et al, 1977) to conduct cross lan-
guage text categorization via the probabilistic bilin-
gual lexicon. For the first step, we propose two dif-
ferent methods. One is a naive and direct method,
that is, we convert a bilingual lexicon into a proba-
bilistic lexicon by simply assigning equal translation
probabilities to all translations of a word. Accord-
ingly, the approach in this case is named as CLNBC-
D. The other method is to employ an EM algorithm
to deduce the probabilistic lexicon. In this case, the
approach is called as CLNBC-EM. Our preliminary
experiments on our collected data have shown that
the proposed approach (CLNBC) significantly out-
performs the baselines in cross language case and is
close to the performance of monolingual text cate-
gorization.
The remainder of this paper is organized as fol-
lows. In Section 2, we introduce the naive Bayes
classifier briefly. In Section 3, we present our cross
language naive Bayes algorithm. In Section 4, eval-
uation over our proposed algorithm is performed.
Section 5 is conclusions and future work.
2 The Naive Bayes Classifier
The naive Bayes classifier is an effective known al-
gorithm for text categorization (Domingos and Paz-
zani, 1997). When it is used for text categorization
task, each document d ? D corresponds to an exam-
ple. The naive Bayes classifier estimates the prob-
ability of assigning a class c ? C to a document d
based on the following Bayes? theorem.
P (c|d) ? P (d|c)P (c) (1)
Then the naive Bayes classifier makes two as-
sumptions for text categorization. Firstly, each word
in a document occurs independently. Secondly, there
is no linear ordering of the word occurrences.
Therefore, the naive Bayes classifier can be fur-
ther formalized as follows:
P (c|d) ? P (c)
?
w?d
P (w|c) (2)
The estimates of P (c) and P (w|c) can be referred
to (McCallum and Nigam, 1998)
Some extensions to the naive Bayes classifier with
EM algorithm have been proposed for various text
categorization tasks. The naive Bayes classifier was
combined with EM algorithm to learn the class label
of the unlabelled documents by maximizing the like-
lihood of both labelled and unlabelled documents
(Nigam et al, 2000). In addition, the similar way
was adopted to handle the problem with the positive
samples alone (Liu et al, 2002). Recently, transfer
learning problem was tackled by applying EM algo-
rithm along with the naive Bayes classifier (Dai et
al., 2007). However, they all are monolingual text
categorization tasks. In this paper, we apply a simi-
lar method to cope with cross language text catego-
rization using bilingual lexicon alone.
3 Cross Language Naive Bayes Classifier
Algorithm
In this section, a novel cross language naive Bayes
classifier algorithm is presented. The algorithm con-
tains two main steps below. First, generate a prob-
abilistic bilingual lexicon; second, apply an EM-
based naive Bayes learning algorithm to deduce the
labels of documents in another language via the
probabilistic lexicon.
Table 1: Notations and explanations.
Notations Explanations
e Language of training set
f Language of test set
d Document
De Document collection in language e
Df Document collection in language f
Ve Vocabulary of language e
Vf Vocabulary of language f
L Bilingual lexicon
T ? Ve ? Vf Set of links in L
?? Set of words whose translation is ? in L
E ? Ve Set of words of language e in L
we ? E Word in E
F ? Vf Set of words of language f in L
wf ? F Word in F
|E| Number of distinct words in set E
|F | Number of distinct words in set F
N(we) Word frequency in De
N(wf , d) Word frequency in d in language f
De Data distribution in language e
166
For ease of description, we first define some nota-
tions in Table 1. In the next two sections, we detail
the mentioned-above two steps separately.
3.1 Generation of a probabilistic bilingual
lexicon
To fill the gap between different languages, there are
two different ways. One is to construct the multi-
lingual semantic space, and the other is to transform
documents in one language into ones in another lan-
guage. Since we concentrate on use of a bilingual
lexicon, we adopt the latter method. In this paper,
we focus on the probabilistic model instead of se-
lecting the best translation. That is, we need to cal-
culate the probability of the occurrence of word we
in language e given a document d in language f , i.e.
P (we|d). The estimation can be calculated as fol-
lows:
P (we|d) =
?
wf?d
P (we|wf , d)P (wf |d) (3)
Ignoring the context information in a document
d, the above probability can be approximately esti-
mated as follows:
P (we|d) '
?
wf?d
P (we|wf )P (wf |d) (4)
where P (wf |d) denotes the probability of occur-
rence of wf in d, which can be estimated by relative
frequency of wf in d.
In order to induce P (we|d), we have to know the
estimation of P (we|wf ). Typically, we can obtain a
probabilistic lexicon from a parallel corpus. In this
paper, we concentrate on using a bilingual lexicon
alone as our external bilingual resource. Therefore,
we propose two different methods for cross language
text categorization.
First, a naive and direct method is that we assume
a uniform distribution on a word?s distribution. For-
mally, P (we|wf ) = 1?wf , where (we, wf ) ? T ; oth-
erwise P (we|wf ) = 0.
Second, we can apply EM algorithm to deduce
the probabilistic bilingual lexicon via the bilingual
lexicon L and the training document collection at
hand. This idea is motivated by the work (Li and Li,
2002).
We can assume that each word we in language e
is independently generated by a finite mixture model
as follows:
P (we) =
?
wf?F
P (wf )P (we|wf ) (5)
Therefore we can use EM algorithm to estimate
the parameters of the model. Specifically speaking,
we can iterate the following two step for the purpose
above.
? E-step
P (wf |we) =
P (wf )P (we|wf )
?
w?F P (w)P (we|w)
(6)
? M-step
P (we|wf ) =
(N(we) + 1)P (wf |we)
?
w?E (N(w) + 1) P (wf |w)(7)
P (wf ) = ? ?
?
we?E
P (we)P (wf |we)
+ (1? ?) ? P ?(wf ) (8)
where 0 ? ? ? 1, and
P ?(wf ) =
?
d?Df N(wf , d) + 1
?
wf?F
?
d?Df N(wf , d) + |F |(9)
The detailed algorithm can be referred to Algorithm
1. Furthermore, the probability that each word in
language e occurs in a document d in language f ,
P (we|d), can be calculated according to Equation
(4).
3.2 EM-based Naive Bayes Algorithm for
Labelling Documents
In this sub-section, we present an EM-based semi-
supervised learning method for labelling documents
in different language from the language of train-
ing document collection. Its basic model is naive
Bayes model. This idea is motivated by the transfer
learning work (Dai et al, 2007). For simplicity of
description, we first formalize the problem. Given
the labelled document set De in the source language
and the unlabelled document set Df , the objective is
to find the maximum a posteriori hypothesis hMAP
167
Algorithm 1 EM-based Word Translation Probabil-
ity Algorithm
Input: Training document collectionD(l)e , bilingual
lexicon L and maximum times of iterations T
Output: Probabilistic bilingual lexicon P (we|wf )
1: Initialize P (0)(we|wf ) = 1|?wf | , where
(we, wf ) ? T ; otherwise P (0)(we|wf ) = 0
2: Initialize P (0)(wf ) = 1|F |
3: for t =1 to T do
4: Calculate P (t)(wf |we) based on
P (t?1)(we|wf ) and P (t?1)(wf ) accord-
ing to Equation (6)
5: Calculate P (t)(we|wf ) and P (t)(wf ) based
on P (t)(wf |we) according to Equation (7)
and Equation (8)
6: end for
7: return P (T )(we|wf )
from the hypothesis space H under the data distri-
bution of the language e, De, according to the fol-
lowing formula.
hMAP = arg max
h?H
PDe(h|De,Df ) (10)
Instead of trying to maximize PDe(h|De,Df ) in
Equation (10), we can work with `(h|De,Df ), that
is, log (PDe(h)P (De,Df |h)) . Then, using Equa-
tion (10), we can deduce the following equation.
`(h|De,Df ) ? log PDe(h)
+
?
d?De
log
?
c?C
PDe(d|c)PDe(c|h)
+
?
d?Df
log
?
c?C
PDe(d|c)PDe(c|h)
(11)
EM algorithm is applied to find a local maximum
of `(h|De,Df ) by iterating the following two steps:
? E-step:
PDe(c|d) ? PDe(c)PDe(d|c) (12)
? M-step:
PDe(c) =
?
k?{e,f}
PDe(Dk)PDe(c|Dk) (13)
PDe(we|c) =
?
k?{e,f}
PDe(Dk)PDe(we|c,Dk)
(14)
Algorithm 2 Cross Language Naive Bayes Algo-
rithm
Input: Labelled document collection De, unla-
belled document collection Df , a bilingual lexi-
con L from language e to language f and maxi-
mum times of iterations T .
Output: the class label of each document in Df
1: Generate a probabilistic bilingual lexicon;
2: Calculate P (we|d) according to Equation (4).
3: Initialize P (0)De (c|d) via the traditional naiveBayes model trained from the labelled collec-
tion D(l)e .
4: for t =1 to T do
5: for all c ? C do
6: Calculate P (t)De(c) based on P
(t?1)
De (c|d) ac-cording to Equation (13)
7: end for
8: for all we ? E do
9: Calculate P (t)De(we|c) based on P
(t?1)
De (c|d)and P (we|d) according to Equation (14)
10: end for
11: for all d ? Df do
12: Calculate P (t)De(c|d) based on P
(t)
De(c) and
P (t)De(we|c) according to Equation (12)
13: end for
14: end for
15: for all d ? Df do
16: c = arg max
c?C
P (T )De (c|d)
17: end for
For the ease of understanding, we directly put the
details of the algorithm in cross-language text cate-
gorization algorithmin which we ignore the detail of
the generation algorithm of a probabilistic lexicon.
In Equation (12), PDe(d|c) can be calculated by
PDe(d|c) =
?
{we|we??wf ?wf?d}
PDe(we|c)NDe (we,d)
(15)
where NDe(we, d) = |d|PDe(we|d).
168
In Equation (13), PDe(c|Dk) can be estimated as
follows:
PDe(c|Dk) =
?
d?Dk
PDe(c|d)PDe(d|Dk) (16)
In Equation (14), similar to section 2, we can es-
timate PDe(we|c,Dk) through Laplacian smoothing
as follows:
PDe(we|c,Dk) =
1 + NDe(we, c,Dk)
|Vk|+ NDe(c,Dk)
(17)
where
NDe(we, c,Dk) =
?
d?Dk
|d|PDe(we|d)PDe(c|d)
(18)
NDe(c,Dk) =
?
d?Dk
|d|PDe(c|d) (19)
In addition, in Equation (13) and (14), PDe(Dk)
can be actually viewed as the trade-off parame-
ter modulating the degree to which EM algorithm
weights the unlabelled documents translated from
the language f to the language e via a bilingual lex-
icon. In our experiments, we assume that the con-
straints are satisfied, i.e. PDe(De) + PDe(Df ) = 1
and PDe(d|Dk) = 1|Dk| .
4 Experiments
4.1 Data Preparation
We chose English and Chinese as our experimen-
tal languages, since we can easily setup our exper-
iments and they are rather different languages so
that we can easily extend our algorithm to other
language pairs. In addition, to evaluate the per-
formance of our algorithm, experiments were per-
formed over the collected data set. Standard evalu-
ation benchmark is not available and thus we devel-
oped a test data from the Internet, containing Chi-
nese Web pages and English Web pages. Specifi-
cally, we applied RSS reader1 to acquire the links
to the needed content and then downloaded the Web
pages. Although category information of the con-
tent can be obtained by RSS reader, we still used
three Chinese-English bilingual speakers to organize
these Web pages into the predefined categories. As
a result, the test data containing Chinese Web pages
1http://www.rssreader.com/
and English Web pages from various Web sites are
created. The data consists of news during Decem-
ber 2005. Also, 5462 English Web pages are from
18 different news Web sites and 6011 Chinese Web
pages are from 8 different news Web sites. Data dis-
tribution over categories is shown in Table 2. They
fall into five categories: Business, Education, Enter-
tainment, Science and Sports.
Some preprocessing steps are applied to Web
pages. First we extract the pure texts of all Web
pages, excluding anchor texts which introduce much
noise. Then for Chinese corpus, all Chinese charac-
ters with BIG5 encoding first were converted into
ones with GB2312 encoding, applied a Chinese seg-
menter tool2 by Zhibiao Wu from LDC to our Chi-
nese corpus and removed stop words and words
with one character and less than 4 occurrences; for
English corpus, we used the stop words list from
SMART system (Buckley, 1985) to eliminate com-
mon words. Finally, We randomly split both the En-
glish and Chinese document collection into 75% for
training and 25% for testing.
we compiled a large general-purpose English-
Chinese lexicon, which contains 276,889 translation
pairs, including 53,111 English entries and 38,517
Chinese entries. Actually we used a subset of the
lexicon including 20,754 English entries and 13,471
Chinese entries , which occur in our corpus.
Table 2: Distribution of documents over categories
Categories English Chinese
Sports 1797 2375
Business 951 1212
Science 843 1157
Education 546 692
Entertainment 1325 575
Total 5462 6011
4.2 Baseline Algorithms
To investigate the effectiveness of our algorithms
on cross-language text categorization, three baseline
methods are used for comparison. They are denoted
by ML, MT and LSI respectively.
ML (Monolingual). We conducted text catego-
rization by training and testing the text categoriza-
2http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
169
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 1: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
tion system on document collection in the same lan-
guage.
MT (Machine Translation). We used Systran
premium 5.0 to translate training data into the lan-
guage of test data, since the machine translation sys-
tem is one of the best machine translation systems.
Then use the translated data to learn a model for
classifying the test data.
LSI (Latent Semantic Indexing). We can use
the LSI or SVD technique to deduce language-
independent representations through a bilingual par-
allel corpus. In this paper, we use SVDS command
in MATLAB to acquire the eigenvectors with the
first K largest eigenvalues. We take K as 400 in our
experiments, where best performance is achieved.
In this paper, we use SVMs as the classifier of our
baselines, since SVMs has a solid theoretic founda-
tion based on structure risk minimization and thus
high generalization ability. The commonly used
one-vs-all framework is used for the multi-class
case. SVMs uses the SV M light software pack-
age(Joachims, 1998). In all experiments, the trade-
off parameter C is set to 1.
4.3 Results
In the experiments, all results are averaged on 5 runs.
Results are measured by accuracy, which is defined
as the ratio of the number of labelled correctly docu-
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 2: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
ments to the number of all documents. When inves-
tigating how different training data have effect on
performance, we randomly select the corresponding
number of training samples from the training set 5
times. The results are shown in Figure 1 and Fig-
ure 2. From the two figures, we can draw the fol-
lowing conclusions. First, CLNBC-EM has a stable
and good performance in almost all cases. Also, it
can achieve the best performance among cross lan-
guage methods. In addition, we notice that CLNBC-
D works surprisingly better than CLNBC-EM, when
there are enough test data and few training data. This
may be because the quality of the probabilistic bilin-
gual lexicon derived from CLNBC-EM method is
poor, since this bilingual lexicon is trained from in-
sufficient training data and thus may provide biased
translation probabilities.
To further investigate the effect of varying the
amount of test data, we randomly select the cor-
responding number of test samples from test set 5
times. The results are shown in Figure 3 and Fig-
ure 4, we can draw the following conclusions . First,
with the increasing test data, performance of our two
approaches is improved. Second, CLNBC-EM sta-
tistically significantly outperforms CLNBC-D.
From figures 1 through 4, we also notice that MT
and LSI always achieve some poor results. For MT,
170
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5
0.6
0.7
0.8
0.9
1
Ratio of test data
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 3: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
maybe it is due to the large difference of word usage
between original documents and the translated ones.
For example,   (Qi Shi) has two common trans-
lations, which are cavalier and knight. In sports do-
main, it often means a basketball team of National
Basketball Association (NBA) in U.S. and should
be translated into cavalier. However, the transla-
tion knight is provided by Systran translation system
we use in the experiment. In term of LSI method,
one possible reason is that the parallel corpus is too
limited. Another possible reason is that it is out-of-
domain compared with the domain of the used doc-
ument collections.
From Table 3, we can observe that our algorithm
is more efficient than three baselines. The spent time
are calculated on the machine, which has a 2.80GHz
Dual Pentium CPU.
5 Conclusions and Future Work
In this paper, we addressed the issue of how to con-
duct cross language text categorization using a bilin-
gual lexicon. To this end, we have developed a cross
language naive Bayes classifier, which contains two
main steps. In the first step, we deduce a proba-
bilistic bilingual lexicon. In the second step, we
adopt naive Bayes method combined with EM to
conduct cross language text categorization. We have
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Ratio of test data
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 4: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
proposed two different methods, namely CLNBC-D
and CLNBC-EM, for cross language text categoriza-
tion. The preliminary experiments on collected data
collections show the effectiveness of the proposed
two methods and verify the feasibility of achieving
performance near to monolingual text categorization
using a bilingual lexicon alone.
As further work, we will collect larger compara-
ble corpora to verify our algorithm. In addition, we
will investigate whether the algorithm can be scaled
to more fine-grained categories. Furthermore, we
will investigate how the coverage of bilingual lex-
icon have effect on performance of our algorithm.
Table 3: Comparison of average spent time by dif-
ferent methods, which are used to conduct cross-
language text categorization from English to Chi-
nese.
Methods Preparation Computation
CLNBC-D - ?1 Min
CLNBC-EM - ?2 Min
ML - ?10 Min
MT ?48 Hra ?14 Min
LSI ?90 Minb ?15 Min
aMachine Translation Cost
bSVD Decomposition Cost
171
Acknowledgements. The authors would like to
thank three anonymous reviewers for their valu-
able suggestions. This work was partially sup-
ported by the National Natural Science Founda-
tion of China under the grants NSFC 60375022 and
NSFC 60473040, and the Microsoft Laboratory for
Intelligent Computing and Intelligent Systems of
Shanghai Jiao Tong University.
References
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In ECDL,
pages 126?139.
Chris Buckley. 1985. Implementation of the SMART
information retrieval system. Technical report, Ithaca,
NY, USA.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classifiers for text
classification. In Proceedings of Twenty-Second AAAI
Conference on Artificial Intelligence (AAAI 2007),
pages 540?545, July.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple bayesian classifier under zero-
one loss. Machine Learning, 29(2-3):103?130.
Blaz? Fortuna and John Shawe-Taylor. 2005. The use
of machine translation tools for cross-lingual text min-
ing. In Learning With Multiple Views, Workshop at the
22nd International Conference on Machine Learning
(ICML).
Alfio Massimiliano Gliozzo and Carlo Strapparava.
2006. Exploiting comparable corpora and bilingual
dictionaries for cross-language text categorization. In
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics. The Association for
Computer Linguistics, July.
Thorsten Joachims. 1998. Making large-scale sup-
port vector machine learning practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 343?351.
Yaoyong Li and John Shawe-Taylor. 2006. Using KCCA
for Japanese-English cross-language information re-
trieval and document classification. Journal of Intel-
ligent Information Systems, 27(2):117?133.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.
2002. Partially supervised classification of text doc-
uments. In ICML ?02: Proceedings of the Nineteenth
International Conference on Machine Learning, pages
387?394, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classification.
In Proceedings of AAAI-98, Workshop on Learning for
Text Categorization.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 2000. Text classification from labeled
and unlabeled documents using EM. Machine Learn-
ing, 39(2/3):103?134.
J. Scott Olsson, Douglas W. Oard, and Jan Hajic?. 2005.
Cross-language text classification. In Proceedings of
the 28th Annual international ACM SIGIR Confer-
ence on Research and Development in information Re-
trieval, pages 645?646, New York, NY, August. ACM
Press.
Leonardo Rigutini, Marco Maggini, and Bing Liu. 2005.
An EM based training algorithm for cross-language
text categorization. In Proceedings of Web Intelligence
Conference (WI-2005), pages 529?535, Compie`gne,
France, September. IEEE Computer Society.
172
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845?850,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Converting Continuous-Space Language Models into
N-gram Language Models for Statistical Machine Translation
Rui Wang1,2,3, Masao Utiyama2, Isao Goto2, Eiichro Sumita2, Hai Zhao1,3 and Bao-Liang Lu1,3
1 Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong Unviersity, Shanghai, 200240, China
2 Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3 MOE-Microsoft Key Lab. for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong Unviersity, Shanghai 200240 China
wangrui.nlp@gmail.com, mutiyama/igoto/eiichiro.sumita@nict.go.jp, zhaohai@cs.sjtu.edu.cn, bllu@sjtu.edu.cn
Abstract
Neural network language models, or
continuous-space language models (CSLMs),
have been shown to improve the performance
of statistical machine translation (SMT)
when they are used for reranking n-best
translations. However, CSLMs have not
been used in the first pass decoding of SMT,
because using CSLMs in decoding takes a lot
of time. In contrast, we propose a method
for converting CSLMs into back-off n-gram
language models (BNLMs) so that we can
use converted CSLMs in decoding. We show
that they outperform the original BNLMs and
are comparable with the traditional use of
CSLMs in reranking.
1 Introduction
Language models are important in natural language
processing tasks such as speech recognition and
statistical machine translation. Traditionally, back-
off n-gram language models (BNLMs) (Chen and
Goodman, 1996; Chen and Goodman, 1998;
Stolcke, 2002) are being widely used for these tasks.
Recently, neural network language models,
or continuous-space language models (CSLMs)
(Bengio et al, 2003; Schwenk, 2007; Le et al, 2011)
are being used in statistical machine translation
(SMT) (Schwenk et al, 2006; Son et al, 2010;
Schwenk et al, 2012; Son et al, 2012; Niehues
and Waibel, 2012). These works have shown that
CSLMs can improve the BLEU (Papineni et al,
2002) scores of SMT when compared with BNLMs,
on the condition that the training data for language
modeling are the same size. However, in practice,
CSLMs have not been widely used in SMT.
One reason is that the computational costs of
training and using CSLMs are very high. Various
methods have been proposed to tackle the training
cost issues (Son et al, 2010; Schwenk et al, 2012;
Mikolov et al, 2011). However, there has been little
work on reducing using costs. Since the using costs
of CSLMs are very high, it is difficult to use CSLMs
in decoding directly.
A common approach in SMT using CSLMs is
the two pass approach, or n-best reranking. In this
approach, the first pass uses a BNLM in decoding
to produce an n-best list. Then, a CSLM is used to
rerank those n-best translations in the second pass.
(Schwenk et al, 2006; Son et al, 2010; Schwenk et
al., 2012; Son et al, 2012)
Another approach is using restricted Boltzmann
machines (RBMs) (Niehues and Waibel, 2012)
instead of using multi-layer neural networks
(Bengio et al, 2003; Schwenk, 2007; Le et al,
2011). Since probability in a RBM can be calculated
very efficiently (Niehues and Waibel, 2012), they
can use the RBM language model in SMT decoding.
However, the RBM was just used in an adaptation of
SMT, not in a large SMT task, because the training
costs of RBMs are very high.
The last approach is using a BNLM to simulate
a CSLM (Deoras et al, 2011; Arsoy et al, 2013).
(Deoras et al, 2011) used a recurrent neural network
language model (RNNLM) to generate a large
amount of text, which was generated by sampling
words from the probability distributions calculated
by the RNNLM. Then, they trained the BNLM
845
from the text using the interpolated Kneser-Ney
smoothing method. (Arsoy et al, 2013) converted
neural network language models of increasing order
to pruned back-off language models, using lower-
order models to constrain the n-grams allowed in
higher-order models.
Both of these methods were used in decoding for
speech recognition. These methods were applied
to not-so-large scale experiments (55 million (M)
words for training their BNLMs) (Arsoy et al,
2013). In contrast, our method is applied to SMT
and can be used to improve a BNLM created from
746 M words by using a CSLM trained from 42 M
words.
Because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs, improving a BNLM by using a CSLM
trained from a smaller corpus is very important.
Actually, a CSLM trained from a smaller corpus
can improve the BLEU scores of SMT if it is used
in the n-best reranking (Schwenk, 2010; Huang et
al., 2013). In contrast, we will demonstrate that a
BNLM simulating a CSLM can improve the BLEU
scores of SMT in the first pass decoding.
Our approach is as follows: (1) First, we train a
CSLM (Schwenk, 2007) from a corpus. (2) Second,
we also train a BNLM from the same corpus or
larger corpus. (3) Finally, we rewrite the probability
of each n-gram of the BNLM with that probability
calculated from the CSLM.We also re-normalize the
probabilities of the BNLM, then use the re-written
BNLM in SMT decoding.
In Section 2, we describe the BNLM and CSLM
(Schwenk, 2010) used for re-writing BNLMs. In
Section 3, we describe the method of converting
a CSLM into a BNLM. In Sections 4 and 5, we
evaluate our method and conclude.
2 Language Models
In this section, we will introduce the standard
BNLM and CSLM structure and probability
calculation.
2.1 Standard back-off ngram language model
A BNLM predicts the probability of a wordwi given
its preceding n ? 1 words hi = wi?1i?n+1. But
it will suffer from data sparseness if the context,
hi, does not appear in the training data. So an
estimation by ?backing-off? to models with smaller
histories is necessary. In the case of the modified
Kneser-Ney smoothing (Chen and Goodman, 1998),
the probability of wi given hi under a BNLM,
Pb(wi|hi), is:
Pb(wi|hi) = P?b(wi|hi) + ?(hi)Pb(wi|wi?1i?n+2) (1)
where P?b(wi|hi) is a discounted probability and
?(hi) is the back-off weight. A BNLM is used with
a CSLM as shown below.
2.2 CSLM structure and probability
calculation
The main structure of a CSLM using a multi-
layer neural network contains four layers: the input
layer projects all words in the context hi onto
the projection layer (the first hidden layer); the
second hidden layer and the output layer achieve the
non-liner probability estimation and calculate the
language model probability P (wi|hi) for the given
context. (Schwenk, 2007).
The CSLM calculates the probabilities of all
words in the vocabulary of the corpus given
the context at once. However, because the
computational complexity of calculating the
probabilities of all words is quite high, the CSLM is
only used to calculate the probabilities of a subset
of the whole vocabulary. This subset is called
a short-list, which consists of the most frequent
words in the vocabulary. The CSLM also calculates
the sum of the probabilities of all words not in the
short-list by assigning a neuron for that purpose.
The probabilities of other words not in the short-list
are obtained from a BNLM (Schwenk, 2007;
Schwenk, 2010).
Let wi, hi be the current word and history. The
CSLM with a BNLM calculates the probability of
wi given hi, P (wi|hi), as follows:
P (wi|hi) =
{
Pc(wi|hi)
1?Pc(o|hi)
Ps(hi) if wi ? short-list
Pb(wi|hi) otherwise
(2)
where Pc(?) is the probability calculated by the
CSLM, Pc(o|hi) is the probability of the neuron
for the words not in the short-list, Pb(?) is the
probability calculated by the BNLM as in Eq. 1,
and
Ps(hi) =
?
v?short-list
Pb(v|hi). (3)
846
It can be considered that the CSLM redistributes
the probability mass of all words in the short-list.
This probability mass is calculated by using the
BNLM.
3 Conversion of CSLM into BNLM
As described in the introduction, we first train a
CSLM from a corpus. We also train a BNLM from
the same corpus or a larger corpus. Then, we rewrite
the probability of each ngram in the BNLM with the
probability calculated from the CSLM.
First, we use the probabilities of 1-grams in
the BNLM as they are. Next, we rewrite the
probabilities of n-grams (n=2,3,4,5) in the BNLM
with the probabilities calculated by using the n-gram
CSLM, respectively. Note that the n-gram CSLM
means that the length of its history is n ? 1. Note
also that we only need to rewrite the probabilities
of n-grams ending with a word in the short-list.
Finally, we re-normalize the probabilities of the
BNLM using the SRILM?s ?-renorm? option.
When we rewrite a BNLM trained from a larger
corpus, the ngrams in the BNLM often contain
unknown words for the CSLM. In that case, we use
the probabilities in the BNLM as they are.
4 Experiments
4.1 Common settings
We used the patent data for the Chinese to English
patent translation subtask from the NTCIR-9 patent
translation task (Goto et al, 2011). The parallel
training, development, and test data consisted of 1
M, 2,000, and 2,000 sentences, respectively.
We followed the settings of the NTCIR-9 Chinese
to English translation baseline system (Goto et al,
2011) except that we used various language models
to compare them. We used the MOSES phrase-
based SMT system (Koehn et al, 2003), together
with Giza++ (Och and Ney, 2003) for alignment and
MERT (Och, 2003) for tuning on the development
data. The translation performance was measured by
the case-insensitive BLEU scores on the tokenized
test data. We used mteval-v13a.pl for
calculating BLEU scores.1
1It is available at http://www.itl.nist.gov/iad/
mig/tests/mt/2009/
We used the 14 standard SMT features: five
translation model scores, one word penalty score,
seven distortion scores and one language model
score. Each of the different language models was
used to calculate the language model score.
As the baseline BNLM, we trained a 5-gram
BNLM with modified Kneser-Ney smoothing using
the English side of the 1 M sentences training data,
which consisted of 42 M words. We did not discard
any n-grams in training this model. That is, we
did not use count cutoffs. We call this BNLM as
BNLM42.
A 5-gram CSLM was trained on the same
1 M training sentences using the CSLM toolkit
(Schwenk, 2010). The settings for the CSLM
were: projection layer of dimension 256 for each
word, hidden layer of dimension 384 and output
layer (short-list) of dimension 8192, which were
recommended in the CSLM toolkit. We call this
CSLM CSLM42. CSLM42 used BNLM42 as the
background BNLM.
We also trained a larger 5-gram BNLM with
modified Kneser-Ney smoothing by adding
sentences from the 2005 US patent data distributed
in the NTCIR-8 patent translation task (Fujii et al,
2010) to the 42 M words. The data consisted of
746 M words. We call this BNLM BNLM746. We
discarded 3,4,5-grams that occurred only once when
we created BNLM746.
Next, we re-wrote BNLM42 with CSLM42 by
using the method described in Section 3. This
re-written BNLM was interpolated with BNLM42.
The interpolation weight was determined by the grid
search. That is, we changed the interpolation weight
to 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolated
BNLM. Then we used that BNLM in the SMT
system to tune the weight parameters on the first
half of the development data. Next, we selected
the interpolation weight that obtained the highest
BLEU score on the second half of the development
data. After we selected the interpolation weight,
we applied MERT again to the 2,000 sentence
development data to tune the weight parameters.2
We call this BNLM CONV42. We also obtained
CONV746 by re-writing BNLM746 with CSLM42
2We aware that the interpolation weight might be
determined by minimizing the perplexity on the development
data. However, we opted to directly maximize the BLEU score.
847
in the same way.
The vocabulary of these language models was the
same, which was extracted from the 1 M training
sentences.
4.2 Experimental results
Table 1 shows the percent BLEU scores on the test
data. The figures in the ?1st pass? column show
the BLEU scores in the first pass decoding when
we changed the language model. The figures in the
?reranking? column show the BLEU scores when
we applied CSLM42 to rerank the 100-best lists for
the different language models. When we applied
CSLM42 for reranking, we added the CSLM42
score as the additional 15th feature. The weight
parameters were tuned by using Z-MERT (Zaidan,
2009).
LMs 1st pass rerank
BNLM42 31.60 32.44
CONV42 32.58 32.98
BNLM746 32.83 33.36
CONV746 33.22 33.54
Table 1: Comparison of BLEU scores
We also performed the paired bootstrap re-
sampling test (Koehn, 2004).3 We sampled 2000
samples for each significance test.
Table 2 shows the results of a statistical
significance test, in which the ?1st? is short for
the ?1st pass?. The marks indicate whether the
LM to the left of a mark is significantly better
than that above the mark at a certain level. (???:
significantly better at ? = 0.01, ?>?: ? = 0.05,
???: not significantly better at ? = 0.05)
First, as shown in the tables, the reranking
by applying CSLM42 increased the BLEU scores
for all language models. This observation is in
accordance with those of previous work (Schwenk,
2010; Huang et al, 2013).
Second, the reranking results of BNLM42 (32.44)
were not better than those of the first pass of
BNLM746 (32.83). This indicates that if the
underlying BNLM is made from a small corpus, the
reranking using CSLM can not compensate for it.
3We used the code available at http://www.ark.cs.
cmu.edu/MT/.
BN
LM
74
6
(re
ra
nk
)
CO
N
V
74
6
(1
st)
CO
N
V
42
(re
ra
nk
)
BN
LM
74
6
(1
st)
CO
N
V
42
(1
st)
BN
LM
42
(re
ra
nk
)
BN
LM
42
(1
st)
CONV746 (rerank) ? ? ? ? ? ? ?
BNLM746 (rerank) ? ? > ? ? ?
CONV746 (1st) ? ? ? ? ?
CONV42 (rerank) ? ? ? ?
BNLM746 (1st) ? ? ?
CONV42 (1st) ? ?
BNLM42 (rerank) ?
Table 2: Significance tests for systems with different LMs
Third, CONV42 was better than BNLM42 for
both first-pass and reranking. This also holds in the
case of CONV746 and BNLM746. This indicated
that our conversion method improved the BNLMs,
even if the underlying BNLMwas trained on a larger
corpus than that used for training the CSLM. As
described in the introduction, this is very important
because BNLMs can be trained from much larger
corpora than those that can be used for training
CSLMs. This observation has not been found in the
previous work.
In addition, the first-pass of CONV42 and
CONV746 (32.58 and 33.22) were comparable with
those of the reranking results of BNLM42 and
BNLM746 (32.44 and 33.36), respectively. That is,
there were no significant differences between these
results. This indicates that our conversion method
preserves the performance of the reranking using
CSLM.
5 Conclusion
We have proposed a method for converting CSLMs
into BNLMs. The method can be used to improve
a BNLM by using a CSLM trained from a smaller
corpus than that used for training the BNLM. We
have also shown that BNLMs created by our method
performs as good as the reranking using CSLMs.
Our future work is to compare our conversion
method with that of (Arsoy et al, 2013).4
4We aware that (Arsoy et al, 2013) compared their method
with the one that is identical with our method. However, the
experiments were conducted on a speech recognition task and
the scale of the experiment was not so large. Since we noticed
their work just before the submission of our paper, we did not
have time to compare their method with our method in SMT.
848
Acknowledgments
We appreciate the helpful discussion with Andrew
Finch and Paul Dixon, and three anonymous
reviewers for many invaluable comments and
suggestions to improve our paper. This work
is supported by the National Natural Science
Foundation of China (Grant No. 60903119, No.
61170114 and No. 61272248), the National
Basic Research Program of China (Grant No.
2013CB329401) and the Science and Technology
Commission of Shanghai Municipality (Grant No.
13511500200).
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural network
language models into back-off language models for
efficient decoding in automatic speech recognition.
In Proc. of IEEE Int. Conf. on Acoustics, Speech
and Signal Processing (ICASSP 2013), Vancouver,
Canada, May. IEEE.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic
language model. Journal of Machine Learning
Research (JMLR), 3:1137?1155, March.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meeting
on Association for Computational Linguistics, ACL
?96, pages 310?318, Santa Cruz, California, June.
Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical report, Computer Science Group,
Harvard Univ.
A. Deoras, T. Mikolov, S. Kombrink, M. Karafiat,
and Sanjeev Khudanpur. 2011. Variational
approximation of long-span language models for lvcsr.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 5532?
5535, Prague, Czech Republic, May. IEEE.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meeting
on Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and Cross-
lingual Information Access, pages 293?302, Tokyo,
Japan, June.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR-9 Workshop Meeting, pages
559?578, Tokyo, Japan, December.
Zhongqiang Huang, Jacob Devlin, and Spyros
Matsoukas. 2013. Bbn?s systems for the chinese-
english sub-task of the ntcir-10 patentmt evaluation.
In NTCIR-10, Tokyo, Japan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the
North American Chapter of the Association for
Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Edmonton, Canada. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, and
F. Yvon. 2011. Structured output layer neural
network language model. In Acoustics, Speech and
Signal Processing (ICASSP), 2011 IEEE International
Conference on, pages 5524?5527, Prague, Czech
Republic, May. IEEE.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernock. 2011. Strategies for
training large scale neural network language models.
In Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages 196?
201, Prague, Czech Republic, May. IEEE.
Jan Niehues and Alex Waibel. 2012. Continuous space
language models using restricted boltzmann machines.
In Proceedings of the International Workshop for
Spoken Language Translation, IWSLT 2012, pages
311?318, Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ?02, pages 311?
849
318, Philadelphia, Pennsylvania, June. Association for
Computational Linguistics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proceedings
of the COLING/ACL on Main conference poster
sessions, COLING-ACL ?06, pages 723?730, Sydney,
Australia, July. Association for Computational
Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of LanguageModeling for HLT,
WLM ?12, pages 11?19, Montreal, Canada, June.
Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, pages 137?146.
Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski,
and Franc?ois Yvon. 2010. Training continuous
space language models: some practical issues. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 778?788, Cambridge, Massachusetts,
October. Association for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL HLT ?12, pages
39?48, Montreal, Canada, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings International
Conference on Spoken Language Processing, pages
257?286, November.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
850
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Neural Network Based Bilingual Language Model Growing
for Statistical Machine Translation
Rui Wang
1,3,?
, Hai Zhao
1,3
, Bao-Liang Lu
1,3
, Masao Utiyama
2
and Eiichro Sumita
2
1
Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai, 200240, China
2
Multilingual Translation Laboratory, MASTAR Project,
National Institute of Information and Communications Technology,
3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan
3
Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China
wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn,
{mutiyama, eiichiro.sumita}@nict.go.jp
Abstract
Since larger n-gram Language Model
(LM) usually performs better in Statistical
Machine Translation (SMT), how to con-
struct efficient large LM is an important
topic in SMT. However, most of the ex-
isting LM growing methods need an extra
monolingual corpus, where additional LM
adaption technology is necessary. In this
paper, we propose a novel neural network
based bilingual LM growing method, only
using the bilingual parallel corpus in SMT.
The results show that our method can im-
prove both the perplexity score for LM e-
valuation and BLEU score for SMT, and
significantly outperforms the existing LM
growing methods without extra corpus.
1 Introduction
?Language Model (LM) Growing? refers to adding
n-grams outside the corpus together with their
probabilities into the original LM. This operation
is useful as it can make LM perform better through
letting it become larger and larger, by only using a
small training corpus.
There are various methods for adding n-grams
selected by different criteria from a monolingual
corpus (Ristad and Thomas, 1995; Niesler and
Woodland, 1996; Siu and Ostendorf, 2000; Si-
ivola et al., 2007). However, all of these approach-
es need additional corpora. Meanwhile the extra
corpora from different domains will not result in
better LMs (Clarkson and Robinson, 1997; Iyer et
al., 1997; Bellegarda, 2004; Koehn and Schroeder,
?
Part of this work was done as Rui Wang visited in NICT.
2007). In addition, it is very difficult or even im-
possible to collect an extra large corpus for some
special domains such as the TED corpus (Cettolo
et al., 2012) or for some rare languages. There-
fore, to improve the performance of LMs, without
assistance of extra corpus, is one of important re-
search topics in SMT.
Recently, Continues Space Language Model
(CSLM), especially Neural Network based Lan-
guage Model (NNLM) (Bengio et al., 2003;
Schwenk, 2007; Mikolov et al., 2010; Le et al.,
2011), is being actively used in SMT (Schwenk
et al., 2006; Son et al., 2010; Schwenk, 2010;
Schwenk et al., 2012; Son et al., 2012; Niehues
and Waibel, 2012). One of the main advantages
of CSLM is that it can more accurately predic-
t the probabilities of the n-grams, which are not in
the training corpus. However, in practice, CSLM-
s have not been widely used in the current SMT
systems, due to their too high computational cost.
Vaswani and colleagues (2013) propose a
method for reducing the training cost of CSLM
and apply it to SMT decoder. However, they do
not show their improvement for decoding speed,
and their method is still slower than the n-gram
LM. There are several other methods for attempt-
ing to implement neural network based LM or
translation model for SMT (Devlin et al., 2014;
Liu et al., 2014; Auli et al., 2013). However, the
decoding speed using n-gram LM is still state-of-
the-art one. Some approaches calculate the prob-
abilities of the n-grams n-grams before decoding,
and store them in the n-gram format (Wang et al.,
2013a; Arsoy et al., 2013; Arsoy et al., 2014). The
?converted CSLM? can be directly used in SMT.
Though more n-grams which are not in the train-
189
ing corpus can be generated by using some of
these ?converting? methods, these methods only
consider the monolingual information, and do not
take the bilingual information into account.
We observe that the translation output of a
phrase-based SMT system is concatenation of
phrases from the phrase table, whose probabilities
can be calculated by CSLM. Based on this obser-
vation, a novel neural network based bilingual LM
growing method is proposed using the ?connecting
phrases?. The remainder of this paper is organized
as follows: In Section 2, we will review the exist-
ing CSLM converting methods. The new neural
network based bilingual LM growing method will
be proposed in Section 3. In Section 4, the exper-
iments will be conducted and the results will be
analyzed. We will conclude our work in Section
5.
2 Existing CSLM Converting Methods
Traditional Backoff N -gram LMs (BNLMs) have
been widely used in many NLP tasks (Zhang and
Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013;
Zhang et al., 2012; Xu and Zhao, 2012; Wang et
al., 2013b; Jia and Zhao, 2013; Wang et al., 2014).
Recently, CSLMs become popular because they
can obtain more accurate probability estimation.
2.1 Continues Space Language Model
A CSLM implemented in a multi-layer neural net-
work contains four layers: the input layer projects
(first layer) all words in the context h
i
onto the
projection layer (second layer); the hidden layer
(third layer) and the output layer (fourth layer)
achieve the non-liner probability estimation and
calculate the LM probability P (w
i
|h
i
) for the giv-
en context (Schwenk, 2007).
CSLM is able to calculate the probabilities of
all words in the vocabulary of the corpus given the
context. However, due to too high computational
complexity, CSLM is mainly used to calculate the
probabilities of a subset of the whole vocabulary
(Schwenk, 2007). This subset is called a short-
list, which consists of the most frequent words in
the vocabulary. CSLM also calculates the sum of
the probabilities of all words not included in the
short-list by assigning a neuron with the help of
BNLM. The probabilities of other words not in the
short-list are obtained from an BNLM (Schwenk,
2007; Schwenk, 2010; Wang et al., 2013a).
Let w
i
and h
i
be the current word and history,
respectively. CSLM with a BNLM calculates the
probability P (w
i
|h
i
) of w
i
given h
i
, as follows:
P (w
i
|h
i
) =
?
?
?
P
c
(w
i
|h
i
)
?
w?V
0
P
c
(w|h
i
)
P
s
(h
i
) if w
i
? V
0
P
b
(w
i
|h
i
) otherwise
(1)
where V
0
is the short-list, P
c
(?) is the probabil-
ity calculated by CSLM,
?
w?V
0
P
c
(w|h
i
) is the
summary of probabilities of the neuron for all the
words in the short-list, P
b
(?) is the probability cal-
culated by the BNLM, and
P
s
(h
i
) =
?
v?V
0
P
b
(v|h
i
). (2)
We may regard that CSLM redistributes the
probability mass of all words in the short-list,
which is calculated by using the n-gram LM.
2.2 Existing Converting Methods
As baseline systems, our approach proposed in
(Wang et al., 2013a) only re-writes the probabil-
ities from CSLM into the BNLM, so it can only
conduct a convert LM with the same size as the o-
riginal one. The main difference between our pro-
posed method in this paper and our previous ap-
proach is that n-grams outside the corpus are gen-
erated firstly and the probabilities using CSLM are
calculated by using the same method as our previ-
ous approach. That is, the proposed new method
is the same as our previous one when no grown
n-grams are generated.
The method developed by Arsoy and colleagues
(Arsoy et al., 2013; Arsoy et al., 2014) adds al-
l the words in the short-list after the tail word of
the i-grams to construct the (i+1)-grams. For ex-
ample, if the i-gram is ?I want?, then the (i+1)-
grams will be ?I want *?, where ?*? stands for any
word in the short list. Then the probabilities of
the (i+1)-grams are calculated using (i+1)-CSLM.
So a very large intermediate (i+1)-grams will have
to be grown
1
, and then be pruned into smaller
suitable size using an entropy-based LM pruning
method modified from (Stolcke, 1998). The (i+2)-
grams are grown using (i+1)-grams, recursively.
1
In practice, the probabilities of all the target/tail words
in the short list for the history i-grams can be calculated by
the neurons in the output layer at the same time, which will
save some time. According to our experiments, the time cost
for Arsoy?s growing method is around 4 times more than our
proposed method, if the LMs which are 10 times larger than
the original one are grown with other settings all the same.
190
3 Bilingual LM Growing
The translation output of a phrase-based SMT sys-
tem can be regarded as a concatenation of phrases
in the phrase table (except unknown words). This
leads to the following procedure:
Step 1. All the n-grams included in the phrase
table should be maintained at first.
Step 2. The connecting phrases are defined in
the following way.
The w
b
a
is a target language phrase starting from
the a-th word ending with the b-th word, and ?w
b
a
?
is a phrase includingw
b
a
as a part of it, where ? and
? represent any word sequence or none. An i-gram
phrase w
k
1
w
i
k+1
(1 ? k ? i ? 1) is a connecting
phrase
2
, if :
(1) w
k
1
is the right (rear) part of one phrase ?w
k
1
in the phrase table, or
(2) w
i
k+1
is the left (front) part of one phrase
w
i
k+1
? in the phrase table.
After the probabilities are calculated using C-
SLM (Eqs.1 and 2), we combine the n-grams in
the phrase table from Step 1 and the connecting
phrases from Step 2.
3.1 Ranking the Connecting Phrases
Since the size of connecting phrases is too huge
(usually more than one Terabyte), it is necessary
to decide the usefulness of connecting phrases for
SMT. The more useful connecting phrases can be
selected, by ranking the appearing probabilities of
the connecting phrases in SMT decoding.
Each line of a phrase table can be simplified
(without considering other unrelated scores in the
phrase table) as
f ||| e ||| P (e|f), (3)
where the P (e|f) means the translation probabili-
ty from f(source phrase) to e(target phrase),
which can be calculated using bilingual parallel
training data. In decoding, the probability of a tar-
get phrase e appearing in SMT should be
P
t
(e) =
?
f
P
s
(f) ? P (e|f), (4)
2
We are aware that connecting phrases can be applied to
not only two phrases, but also three or more. However the ap-
pearing probabilities (which will be discussed in Eq. 5 of next
subsection) of connecting phrases are approximately estimat-
ed. To estimate and compare probabilities of longer phrases
in different lengths will lead to serious bias, and the experi-
ments also showed using more than two connecting phrases
did not perform well (not shown for limited space), so only
two connecting phrases are applied in this paper.
where the P
s
(f) means the appearing probability
of a source phrase, which can be calculated using
source language part in the bilingual training data.
Using P
t
(e)
3
, we can select the connecting
phrases e with high appearing probabilities as
the n-grams to be added to the original n-
grams. These n-grams are called ?grown n-
grams?. Namely, we build all the connecting
phrases at first, and then we use the appearing
probabilities of the connecting phrases to decide
which connecting phrases should be selected. For
an i-gram connecting phrasew
k
1
w
i
k+1
, wherew
k
1
is
part of ?w
k
1
and w
i
k+1
is part of w
i
k+1
? (the ?w
k
1
and w
i
k+1
? are from the phrase table), the prob-
ability of the connecting phrases can be roughly
estimated as
P
con
(w
k
1
w
i
k+1
) =
i?1
?
k=1
(
?
?
P
t
(?w
k
1
)?
?
?
P
t
(w
i
k+1
?)).
(5)
A threshold for P
con
(w
k
1
w
i
k+1
) is set, and only
the connecting phrases whose appearing probabil-
ities are higher than the threshold will be selected
as the grown n-grams.
3.2 Calculating the Probabilities of Grown
N -grams Using CSLM
To our bilingual LM growing method, a 5-gram
LM and n-gram (n=2,3,4,5) CSLMs are built by
using the target language of the parallel corpus,
and the phrase table is learned from the parallel
corpus.
The probabilities of unigram in the original n-
gram LM will be maintained as they are. The
n-grams from the bilingual phrase table will be
grown by using the ?connecting phrases? method.
As the whole connecting phrases are too huge, we
use the ranking method to select the more useful
connecting phrases. The distribution of different
n-grams (n=2,3,4,5) of the grown LMs are set as
the same as the original LM.
The probabilities of the grown n-grams
(n=2,3,4,5) are calculated using the 2,3,4,5-
CSLM, respectively. If the tail (target) words of
the grown n-grams are not in the short-list of C-
SLM, the P
b
(?) in Eq. 1 will be applied to calcu-
late their probabilities.
3
This P
t
(e) hence provides more bilingual information,
in comparison with using monolingual target LMs only.
191
We combine the n-grams (n=1,2,3,4,5) togeth-
er and re-normalize the probabilities and backof-
f weights of the grown LM. Finally the original
BNLM and the grown LM are interpolated. The
entire process is illustrated in Figure 1.
Corpus
Phrase Table
Grown n-grams 
with Probabilities
Grown LM
Output
Input
Interpolate
Grown n-grams
CSLM
BNLM
Connecting
Phrases
Figure 1: NN based bilingual LM growing.
4 Experiments and Results
4.1 Experiment Setting up
The same setting up of the NTCIR-9 Chinese to
English translation baseline system (Goto et al.,
2011) was followed, only with various LMs to
compare them. The Moses phrase-based SMT
system was applied (Koehn et al., 2007), togeth-
er with GIZA++ (Och and Ney, 2003) for align-
ment and MERT (Och, 2003) for tuning on the de-
velopment data. Fourteen standard SMT features
were used: five translation model scores, one word
penalty score, seven distortion scores, and one LM
score. The translation performance was measured
by the case-insensitive BLEU on the tokenized test
data.
We used the patent data for the Chinese to En-
glish patent translation subtask from the NTCIR-9
patent translation task (Goto et al., 2011). The par-
allel training, development, and test data sets con-
sist of 1 million (M), 2,000, and 2,000 sentences,
respectively.
Using SRILM (Stolcke, 2002; Stolcke et al.,
2011), we trained a 5-gram LM with the interpo-
lated Kneser-Ney smoothing method using the 1M
English training sentences containing 42M words
without cutoff. The 2,3,4,5-CSLMs were trained
on the same 1M training sentences using CSLM
toolkit (Schwenk, 2007; Schwenk, 2010). The set-
tings for CSLMs were: input layer of the same
dimension as vocabulary size (456K), projection
layer of dimension 256 for each word, hidden lay-
er of dimension 384 and output layer (short-list) of
dimension 8192, which were recommended in the
CSLM toolkit and (Wang et al., 2013a)
4
.
4
Arsoy used around 55 M words as the corpus, including
4.2 Results
The experiment results were divided into four
groups: the original BNLMs (BN), the CSLM
Re-ranking (RE), our previous converting (WA),
the Arsoy?s growing, and our growing methods.
For our bilingual LM growing method, 5 bilingual
grown LMs (BI-1 to 5) were conducted in increas-
ing sizes. For the method of Arsoy, 5 grown LMs
(AR-1 to 5) with similar size of BI-1 to 5 were also
conducted, respectively.
For the CSLM re-ranking, we used CSLM to
re-rank the 100-best lists of SMT. Our previous
converted LM, Arsoy?s grown LMs and bilingual
grown LMs were interpolated with the original
BNLMs, using default setting of SRILM
5
. To re-
duce the randomness of MERT, we used twometh-
ods for tuning the weights of different SMT fea-
tures, and two BLEU scores are corresponding to
these twomethods. TheBLEU-s indicated that the
same weights of the BNLM (BN) features were
used for all the SMT systems. The BLEU-i indi-
cated that the MERT was run independently by
three times and the average BLEU scores were
taken.
We also performed the paired bootstrap re-
sampling test (Koehn, 2004)
6
. Two thousands
samples were sampled for each significance test.
The marks at the right of the BLEU score indicated
whether the LMs were significantly better/worse
than the Arsoy?s grown LMs with the same IDs
for SMT (?++/???: significantly better/worse at
? = 0.01, ?+/??: ? = 0.05, no mark: not signif-
icantly better/worse at ? = 0.05).
From the results shown in Table 1, we can get
the following observations:
(1) Nearly all the bilingual grown LMs outper-
formed both BNLM and our previous converted
LM on PPL and BLEU. As the size of grown LM-
s is increased, the PPL always decreased and the
BLEU scores trended to increase. These indicated
that our proposed method can give better probabil-
ity estimation for LM and better performance for
SMT.
(2) In comparison with the grown LMs in Ar-
84K words as vocabulary, and 20K words as short-list. In this
paper, we used the same setting as our previous work, which
covers 92.89% of the frequency of words in the training cor-
pus, for all the baselines and our method for fair comparison.
5
In our previous work, we used the development data to
tune the weights of interpolation. In this paper, we used the
default 0.5 as the interpolation weights for fair comparison.
6
We used the code available at http://www.ark.cs.
cmu.edu/MT
192
Table 1: Performance of the Grown LMs
LMs n-grams PPL BLEU-s BLEU-i ALH
BN 73.9M 108.8 32.19 32.19 3.03
RE N/A 97.5 32.34 32.42 N/A
WA 73.9M 104.4 32.60 32.62 3.03
AR-1 217.6M 103.3 32.55 32.75 3.14
AR-2 323.8M 103.1 32.61 32.64 3.18
AR-3 458.5M 103.0 32.39 32.71 3.20
AR-4 565.6M 102.8 32.67 32.51 3.21
AR-5 712.2M 102.5 32.49 32.60 3.22
BI-1 223.5M 101.9 32.81+ 33.02+ 3.20
BI-2 343.6M 101.0 32.92+ 33.11++ 3.24
BI-3 464.5M 100.6 33.08++ 33.25++ 3.26
BI-4 571.0M 100.3 33.15++ 33.12++ 3.28
BI-5 705.5M 100.1 33.11++ 33.24++ 3.31
soy?s method, our grown LMs obtained better P-
PL and significantly better BLEU with the sim-
ilar size. Furthermore, the improvement of PPL
and BLEU of the existing methods became satu-
rated much more quickly than ours did, as the LMs
grew.
(3) The last column was the Average Length of
the n-grams Hit (ALH) in SMT decoding for dif-
ferent LMs using the following function
ALH =
5
?
i=1
P
i?gram
? i, (6)
where the P
i?gram
means the ratio of the i-grams
hit in SMT decoding. There were also positive
correlations between ALH, PPL and BLEUs. The
ALH of bilingual grown LM was longer than that
of the Arsoy?s grown LM of the similar size. In
another word, less back-off was used for our pro-
posed grown LMs in SMT decoding.
4.3 Experiments on TED Corpus
The TED corpus is in special domain as discussed
in the introduction, where large extra monolingual
corpora are hard to find. In this subsection, we
conducted the SMT experiments on TED corpora
using our proposed LM growing method, to eval-
uate whether our method was adaptable to some
special domains.
We mainly followed the baselines of the IWSLT
2014 evaluation campaign
7
, only with a few mod-
ifications such as the LM toolkits and n-gram or-
der for constructing LMs. The Chinese (CN) to
English (EN) language pair was chosen, using de-
v2010 as development data and test2010 as evalu-
ation data. The same LM growing method was ap-
7
https://wit3.fbk.eu/
plied on TED corpora as on NTCIR corpora. The
results were shown in Table 2.
Table 2: CN-EN TED Experiments
LMs n-grams PPL BLEU-s
BN 7.8M 87.1 12.41
WA 7.8M 85.3 12.73
BI-1 23.1M 79.2 12.92
BI-2 49.7M 78.3 13.16
BI-3 73.4M 77.6 13.24
Table 2 indicated that our proposed LM grow-
ing method improved both PPL and BLEU in com-
parison with both BNLM and our previous CSLM
converting method, so it was suitable for domain
adaptation, which is one of focuses of the current
SMT research.
5 Conclusion
In this paper, we have proposed a neural network
based bilingual LM growing method by using the
bilingual parallel corpus only for SMT. The results
show that our proposed method can improve both
LM and SMT performance, and outperforms the
existing LM growing methods significantly with-
out extra corpus. The connecting phrase-based
method can also be applied to LM adaptation.
Acknowledgments
We appreciate the helpful discussion with Dr.
Isao Goto and Zhongye Jia, and three anony-
mous reviewers for valuable comments and sug-
gestions on our paper. Rui Wang, Hai Zhao
and Bao-Liang Lu were partially supported by
the National Natural Science Foundation of Chi-
na (No. 60903119, No. 61170114, and No.
61272248), the National Basic Research Program
of China (No. 2013CB329401), the Science and
Technology Commission of Shanghai Municipali-
ty (No. 13511500200), the European Union Sev-
enth Framework Program (No. 247619), the Cai
Yuanpei Program (CSC fund 201304490199 and
201304490171), and the art and science interdis-
cipline funds of Shanghai Jiao Tong University
(A study on mobilization mechanism and alerting
threshold setting for online community, and media
image and psychology evaluation: a computation-
al intelligence approach). The corresponding au-
thor of this paper, according to the meaning given
to this role by Shanghai Jiao Tong University, is
Hai Zhao.
193
References
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2013. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. In Proceedings of ICASSP-2013, Vancouver,
Canada, May. IEEE.
Ebru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,
and Abhinav Sethy. 2014. Converting neural net-
work language models into back-off language mod-
els for efficient decoding in automatic speech recog-
nition. IEEE/ACM Transactions on Audio, Speech,
and Language, 22(1):184?192.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
cessings of EMNLP-2013, pages 1044?1054, Seat-
tle, Washington, USA, October. Association for
Computational Linguistics.
Jerome R Bellegarda. 2004. Statistical language mod-
el adaptation: review and perspectives. Speech
Communication, 42(1):93?108. Adaptation Meth-
ods for Speech Recognition.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR), 3:1137?1155, March.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of EAMT-2012,
pages 261?268, Trento, Italy, May.
Philip Clarkson and A.J. Robinson. 1997. Lan-
guage model adaptation using mixtures and an ex-
ponentially decaying cache. In Proceedings of
ICASSP-1997, volume 2, pages 799?802 vol.2, Mu-
nich,Germany.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL-
2014, pages 1370?1380, Baltimore, Maryland, June.
Association for Computational Linguistics.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the paten-
t machine translation task at the NTCIR-9 work-
shop. In Proceedings of NTCIR-9 Workshop Meet-
ing, pages 559?578, Tokyo, Japan, December.
Rukmini Iyer, Mari Ostendorf, and Herbert Gish.
1997. Using out-of-domain data to improve in-
domain language models. Signal Processing Letter-
s, IEEE, 4(8):221?223.
Zhongye Jia and Hai Zhao. 2013. Kyss 1.0: a
framework for automatic evaluation of chinese input
method engines. In Proceedings of IJCNLP-2013,
pages 1195?1201, Nagoya, Japan, October. Asian
Federation of Natural Language Processing.
Zhongye Jia and Hai Zhao. 2014. A joint graph mod-
el for pinyin-to-chinese conversion with typo cor-
rection. In Proceedings of ACL-2014, pages 1512?
1523, Baltimore, Maryland, June. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of ACL-2007 Workshop
on Statistical Machine Translation, pages 224?227,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL-2007, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP-2004, pages 388?395, Barcelona, Spain,
July. Association for Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-
vain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceed-
ings of ICASSP-2011, pages 5524?5527, Prague,
Czech Republic, May. IEEE.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL-2014,
pages 1491?1500, Baltimore, Maryland, June. As-
sociation for Computational Linguistics.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
Proceedings of INTERSPEECH-2010, pages 1045?
1048.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using restricted boltzman-
n machines. In Proceedings of IWSLT-2012, pages
311?318, Hong Kong.
Thomas Niesler and Phil Woodland. 1996. A variable-
length category-based n-gram language model. In
Proceedings of ICASSP-1996, volume 1, pages 164?
167 vol. 1.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignmen-
t models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of ACL-2003, pages 160?167, Sapporo, Japan, July.
Association for Computational Linguistics.
194
Eric Sven Ristad and Robert G. Thomas. 1995. New
techniques for context modeling. In Proceedings
of ACL-1995, pages 220?227, Cambridge, Mas-
sachusetts. Association for Computational Linguis-
tics.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of
COLING ACL-2006, pages 723?730, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space
language models on a gpu for statistical machine
translation. In Proceedings of the NAACL-HLT 2012
Workshop: Will We Ever Really Replace the N-gram
Model? On the Future of Language Modeling for
HLT, WLM ?12, pages 11?19, Montreal, Canada,
June. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space lan-
guage models. Computer Speech and Language,
21(3):492?518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. The
Prague Bulletin of Mathematical Linguistics, pages
137?146.
Vesa Siivola, Teemu Hirsimki, and Sami Virpioja.
2007. On growing and pruning kneser-ney s-
moothed n-gram models. IEEE Transactions on Au-
dio, Speech, and Language, 15(5):1617?1624.
Manhung Siu and Mari Ostendorf. 2000. Variable n-
grams and extensions for conversational speech lan-
guage modeling. IEEE Transactions on Speech and
Audio, 8(1):63?75.
Le Hai Son, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc?ois Yvon. 2010. Training con-
tinuous space language models: some practical is-
sues. In Proceedings of EMNLP-2010, pages 778?
788, Cambridge, Massachusetts, October. Associa-
tion for Computational Linguistics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of NAACL HLT-
2012, pages 39?48, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Andreas Stolcke, Jing Zheng, Wen Wang, and Vic-
tor Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of INTERSPEECH 2011,
Waikoloa, HI, USA, December.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274, Lansdowne, VA, USA.
Andreas Stolcke. 2002. Srilm-an extensible
language modeling toolkit. In Proceedings of
INTERSPEECH-2002, pages 257?286, Seattle, US-
A, November.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of EMNLP-2013, pages 1387?1392,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013a. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of EMNLP-2013, pages 845?850,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Xiaolin Wang, Hai Zhao, and Bao-Liang Lu. 2013b.
Labeled alignment for recognizing textual entail-
ment. In Proceedings of IJCNLP-2013, pages 605?
613, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Xiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and Bao-
Liang Lu. 2014. Parallelized extreme learning ma-
chine ensemble based on minmax modular network.
Neurocomputing, 128(0):31 ? 41.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING-2012, pages 1341?1350,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of IJCAI-2013, pages 2211?
2217. AAAI Press.
Xiaotian Zhang, Hai Zhao, and Cong Hui. 2012.
A machine learning approach to convert CCGbank
to Penn treebank. In Proceedings of COLING-
2012, pages 535?542, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word
segmentation for chinese machine translation. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7817 of
Lecture Notes in Computer Science, pages 248?263.
Springer Berlin Heidelberg.
195
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 67?71,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
An Empirical Study on Development Set Selection Strategy
for Machine Translation Learning?
Cong Hui12, Hai Zhao12?, Yan Song3, Bao-Liang Lu12
1Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China
3Department of Chinese, Translation and Linguistics, City University of Hong Kong
huicong@sjtu.edu.cn, {zhaohai,blu}@cs.sjtu.edu.cn
Abstract
This paper describes a statistical machine
translation system for our participation
for the WMT10 shared task. Based on
MOSES, our system is capable of translat-
ing German, French and Spanish into En-
glish. Our main contribution in this work
is about effective parameter tuning. We
discover that there is a significant perfor-
mance gap as different development sets
are adopted. Finally, ten groups of devel-
opment sets are used to optimize the model
weights, and this does help us obtain a sta-
ble evaluation result.
1 Introduction
We present a machine translation system that rep-
resents our participation for the WMT10 shared
task from Brain-like Computing and Machine In-
telligence Lab of Shanghai Jiao Tong University
(SJTU-BCMI Lab). The system is based on the
state-of-the-art SMT toolkit MOSES (Koehn et al,
2007). We use it to translate German, French and
Spanish into English. Though different develop-
ment sets used for training parameter tuning will
certainly lead to quite different performance, we
empirically find that the more sets we combine to-
gether, the more stable the performance is, and a
development set similar with test set will help the
performance improvement.
2 System Description
The basic model of the our system is a log-linear
model (Och and Ney, 2002). For given source lan-
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119, Grant
No. 60773090 and Grant No. 90820018), the National Basic
Research Program of China (Grant No. 2009CB320901), and
the National High-Tech Research Program of China (Grant
No.2008AA02Z315).
?corresponding author
guage strings, the target language string t will be
obtained by the following equation,
t?I1 =argmaxtI1
{p?m1 (tI1 | sJ1 )}
=argmax
tI1
{ exp[
?M
m=1 ?mhm(tI1, sJ1 )]?
t?I1 exp[
?M
m=1 ?mhm(t?I1, sJ1 )]
},
where hm is the m-th feature function and ?m is
the m-th model weight. There are four main parts
of features in the model: translation model, lan-
guage model, reordering model and word penalty.
The whole model has been well implemented by
the state-of-the-art statistical machine translation
toolkit MOSES.
For each language that is required to translated
into English, two sets of bilingual corpora are pro-
vided by the shared task organizer. The first set
is the new release (version 5) of Europarl cor-
pus which is the smaller. The second is a com-
bination of other available data sets which is the
larger. In detail, two corpora, europarl-v5 and
news-commentary10 are for German, europarl-v5
and news-commentary10 plus undoc for French
and Spanish, respectively. Details of training data
are in Table 1. Only sentences with length 1 to 40
are acceptable for our task. We used the larger set
for our primary submission.
We adopt word alignment toolkit GIZA++ (Och
and Ney, 2003) to learn word-level alignment with
its default setting and grow-diag-final-and param-
eters. Given a sentence pair and its corresponding
word-level alignment, phrases will be extracted by
using the approach in (Och and Ney, 2004). Phrase
probability is estimated by its relative frequency
in the training corpus. Lexical reordering is deter-
mined by using the default setting of MOSES with
msd-bidirectional parameter.
For training the only language model (English),
the data sets are extracted from monolingual parts
of both europarl-v5 and news-commentary10,
67
sentences words(s) words(t)
de small 1540549 35.76M 38.53M
large 1640818 37.95M 40.64M
fr small 1683156 44.02M 44.20M
large 8997997 251.60M 228.50M
es small 1650152 43.17M 41.25M
large 7971200 236.24M 207.79M
Table 1: Bilingual training corpora from Ger-
man(de), French(fr) and Spanish(es) to English.
which include 1968914 sentences and 47.48M
words. And SRILM is adopted with 5-gram, in-
terpolate and kndiscount settings (Stolcke, 2002)
.
The next step is to estimate feature weights by
optimizing translation performance on a develop-
ment set. We consider various combinations of 10
development sets with 18207 sentences to get a
stable performance in our primary submission.
We use the default toolkits which are provided
by WMT10 organizers for preprocessing (i.e., to-
kenize) and postprocessing (i.e., detokenize, re-
caser).
3 Development Set Selection
3.1 Motivation
Given the previous feature functions, the model
weights will be obtained by optimizing the follow-
ing maximum mutual information criterion, which
can be derived from the maximum entropy princi-
ple:
??M1 = argmax?M1
{
S?
i=1
log p?M1 (ti | si)}
As usual, minimum error rate training (MERT) is
adopted for log-linear model parameter estimation
(Och, 2003). There are many improvements on
MERT in existing work (Bertoldi et al, 2009; Fos-
ter and Kuhn, 2009), but there is no demonstration
that the weights with better performance on the
development set would lead to a better result on
the unseen test set. In our experiments, we found
that different development sets will cause signifi-
cant BLEU score differences, even as high as one
percent. Thus the remained problem will be how
to effectively choose the development set to obtain
a better and more stable performance.
3.2 Experimental Settings
Our empirical study will be demonstrated through
German to English translation on the smaller cor-
pus. The development sets are all development
sets and test sets from the previous WMT shared
translation task as shown in Table 2, and labeled
as dev-0 to dev-9. Meanwhile, we denote 10 batch
sets from batch-0 to batch-9 where the batch-i set
is the combination of dev- sets from dev-0 to dev-i.
The test set is newstest2009, which includes 2525
sentences, 54K German words and 58K English
words, and news-test2008, which includes 2051
sentences, 41K German words and 43K English
words.
id name sent w(de) w(en)
dev-0 dev2006 2000 49K 53K
dev-1 devtest2006 2000 48K 52K
dev-2 nc-dev2007 1057 23K 23K
dev-3 nc-devtest2007 1064 24K 23K
dev-4 nc-test2007 2007 45K 44K
dev-5 nc-test2008 2028 45K 44K
dev-6 news-dev2009 2051 41K 43K
dev-7 test2006 2000 49K 54K
dev-8 test2007 2000 49K 54K
dev-9 test2008 2000 50K 54K
Table 2: Development data.
3.3 On the Scale of Development Set
Having 20 different development sets (10 dev- sets
and batch- sets), 20 models are correspondingly
trained.The decode results on the test set are sum-
marized in Table 3 and Figure 1. The dotted lines
are the performances of 10 different development
sets on the two test sets, we will see that there
is a huge gap between the highest and the lowest
score, and there is not an obvious rule to follow. It
will bring about unsatisfied results if a poor devel-
opment set is chosen. The solid lines represents
the performances of 10 incremental batch sets on
the two test sets, the batch processing still gives a
poor performance at the beginning, but the results
become better and more stable when the develop-
ment sets are continuously enlarged. This sort of
results suggest that a combined development set
may produce reliable results in the worst case. Our
primary submission used the combined develop-
ment set and the results as Table 4.
68
id 09-dev 09-batch 08-dev 08-batch
0 16.46 16.46 16.38 16.38
1 16.67 16.25 16.66 16.44
2 16.74 16.20 16.94 16.22
3 16.15 16.83 16.18 17.02
4 16.44 16.73 16.64 16.89
5 16.50 16.97 16.75 17.13
6 17.15 17.03 17.67 17.24
7 16.51 17.00 16.34 17.09
8 17.03 16.97 17.15 17.22
9 16.25 16.99 16.24 17.26
Table 3: BLEU scores on the two test
sets(newstest2009 & news-test2008), which use
two data set sequences(dev- sequence & batch- se-
quence) to optimize model weights.
de-en fr-en es-en
18.90 24.30 26.40
Table 4: BLEU scores of our primary submission.
3.4 On BLEU Score Difference
To compare BLEU score differences between test
set and development set, we consider two groups
of BLEU score differences, For each development
set, dev-i, the BLEU score difference will be com-
puted between b1 from which adopts itself as the
development set and b2 from which adopts test
set as the development set. For the test set, the
BLEU score difference will be computed between
b?1 from which adopts each development set, dev-i,
as the development set and b?2 from which adopts
itself as the development set.
These two groups of results are illustrated in
Figure 2 (the best score of the test set under self
tuning, newstest2009 is 17.91). The dotted lines
have the inverse trend with the dotted in Figure
1(because the addition of these two values is con-
stant), and the solid lines have the same trend
with the dotted, which means that the good per-
formance is mutual between test set and develop-
ment sets: if tuning using A set could make a good
result over B set, then vice versa.
3.5 On the Similarity between Development
Set and Test Set
This experiment is motivated by (Utiyama et al,
2009), where they used BLEU score to measure
the similarity of a sentences pair and then ex-
tracted sentences similar with those in test set to
0 1 2 3 4 5 6 7 8 90
0.5
1
1.5
2
2.5
DATA SET ID
DIF
F of
 BLE
U S
COR
E
 
 
09?Ddev09?Dtest
Figure 2: The trend of BLEU score differences
construct a specific tuning set. In our experiment,
we will try to measure data set similarity instead.
Given two sets of sentences, one is called as candi-
date(cnd) set and the other reference(ref) set. For
any cnd sentence, we let the whole ref set to be its
reference and then multi-references BLEU score is
computed for cnd set. There comes a problem that
the sentence penalty will be constant for any cnd
sentence, we turn to calculate the average length
of whose sentences which have common n-gram
with the given cnd sentence.
Now we may define three measures. The mea-
sure which uses dev- and batch- sets as cnd sets
and news-test2009 set as ref set is defined as
precision-BLEU , and the measure which uses the
above sets on the contrary way is defined as recall-
BLEU. Then F1-BLEU is defined as the harmonic
mean of precision-BLEU and recall-BLEU. These
results are illustrated in Figure 3. From the fig-
ure, we find that F1-BLEU plays an important
role to predict the goodness of a development set,
F1-BLEU scores of batch- sets have an ascending
curve and batch data set sequence will cause a sta-
ble good test performance, the point on dev- sets
which has high F1-BLEU(eg, dev-0,4,5) would
also has a good test performance.
3.6 Related Work
The special challenge of the WMT shared task is
domain adaptation, which is a hot topic in recent
years and more relative to our experiments. Many
existing works are about this topic (Koehn and
Schroeder, 2007; Nakov, 2008; Nakov and Ng,
2009; Paul et al, 2009; Haque et al, 2009). How-
ever, most of previous works focus on language
69
model, translation phrase table, lexicons model
and factored translation model, few of them pay
attention to the domain adaptation on the develop-
ment set. For future work we consider to use some
machine learning approaches to select sentences in
development sets more relevant with the test set in
order to further improve translation performance.
4 Conclusion
In this paper, we present our machine translation
system for the WMT10 shared task and perform an
empirical study on the development set selection.
According to our experimental results, Choosing
different development sets would play an impor-
tant role for translation performance. We find that
a development set with higher F1-BLEU yields
better and more stable results.
References
Nicola Bertoldi, Barry Haddow, and Jean Baptiste
Fouet. 2009. Improved Minimum Error Rate Train-
ing in Moses. The Prague Bulletin of Mathematical
Linguistics, 91:7?16.
George Foster and Roland Kuhn. 2009. Stabiliz-
ing minimum error rate training. In Proceedings
of the 4th Workshop on Statistical Machine Trans-
lation(WMT), Boulder, Colorado, USA.
Rejwanul Haque, Sudip Kumar Naskar, Josef Van Gen-
abith, and Andy Way. 2009. Experiments on Do-
main Adaptation for English?Hindi SMT. In 7th In-
ternational Conference on Natural Language Pro-
cessing(ICNLP), Hyderabad, India.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the 2nd Workshop on Sta-
tistical Machine Translation(WMT), Prague, Czech
Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics(ACL), Prague, Czech Republic.
Preslav Nakov and Hwee Tou Ng. 2009. NUS
at WMT09: domain adaptation experiments for
English-Spanish machine translation of news com-
mentary text. In Proceedings of the 4th Workshop on
Statistical Machine Translation(WMT), Singapore.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the 3rd Workshop on
Statistical Machine Translation(WMT), Columbus,
Ohio, USA.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics(ACL), Philadelphia, Pennsylva-
nian, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Compu-
tational Linguistics(ACL), Sapporo, Japan.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2009. NICT@ WMT09: model adaptation and
transliteration for Spanish-English SMT. In Pro-
ceedings of the 4th Workshop on Statistical Machine
Translation(WMT), Singapore.
Andreas Stolcke. 2002. SRILM: an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing(ICSLP),
Denver, Colorado, USA.
Masao Utiyama, Hirofumi Yamamoto, and Eiichiro
Sumita. 2009. Two methods for stabilizing MERT:
NICT at IWSLT 2009. In Proceedings of Inter-
national Workshop on Spoken Language Transla-
tion(IWSLT), Tokyo, Japan.
70
0 1 2 3 4 5 6 7 8 915
15.5
16
16.5
17
17.5
18
DATA SET ID
BL
EU
 SC
OR
E
 
 
09?dev
09?batch
08?dev
08?batch
Figure 1: The BLEU score trend in Tabel 3, we will see that the batch lines output a stable and good
performance.
0 1 2 3 4 5 6 7 8 910
15
20
25
30
DATA SET ID
BL
EU
 VA
LU
E
 
 
pDev
pBatch
rDev
rBatch
fDev
fBatch
Figure 3: The precision(p), recall(r) and F1(f) BLEU score on the dev(Dev) and batch(Batch) sets based
on the comparison with news-test2009 set.
71
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 92?99,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hedge Detection and Scope Finding by Sequence Labeling
with Normalized Feature Selection?
Shaodian Zhang12, Hai Zhao123?, Guodong Zhou3 and Bao-Liang Lu12
1Center for Brain-Like Computing and Machine Intelligence
Dept of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University
3School of Computer Science and Technology, Soochow University
zhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cn
gdzhou@suda.edu.cn, blu@cs.sjtu.edu.cn
Abstract
This paper presents a system which adopts
a standard sequence labeling technique for
hedge detection and scope finding. For
the first task, hedge detection, we formu-
late it as a hedge labeling problem, while
for the second task, we use a two-step la-
beling strategy, one for hedge cue label-
ing and the other for scope finding. In par-
ticular, various kinds of syntactic features
are systemically exploited and effectively
integrated using a large-scale normalized
feature selection method. Evaluation on
the CoNLL-2010 shared task shows that
our system achieves stable and competi-
tive results for all the closed tasks. Fur-
thermore, post-deadline experiments show
that the performance can be much further
improved using a sufficient feature selec-
tion.
1 Introduction
Hedges are linguistic devices representing spec-
ulative parts of articles. Previous works such as
(Hyland, 1996; Marco and Mercer, 2004; Light et
al., 2004; Thompson et al, 2008) present research
on hedge mainly as a linguistic phenomenon.
Meanwhile, detecting hedges and their scopes au-
tomatically are increasingly important tasks in nat-
ural language processing and information extrac-
tion, especially in biomedical community. The
shared task of CoNLL-2010 described in Farkas
et al (2010) aims at detecting hedges (task 1)
and finding their scopes (task 2) for the literature
? This work is partially supported by the National
Natural Science Foundation of China (Grants 60903119,
60773090, 90820018 and 90920004), the National Basic Re-
search Program of China (Grant No. 2009CB320901), and
the National High-Tech Research Program of China (Grant
No.2008AA02Z315).
?corresponding author
from BioScope corpus (Szarvas et al, 2008) and
Wikipedia. This paper describes a system adopt-
ing sequence labeling which performs competitive
in the official evaluation, as well as further test.
In addition, a large-scale feature selection proce-
dure is applied in training and development. Con-
sidering that BioScope corpus is annotated by two
independent linguists according to a formal guide-
line (Szarvas, 2008), while Wikipedia weasels are
tagged by netizens who are diverse in background
and various in evaluation criterion, it is needed to
handle them separately. Our system selects fea-
tures for Wikipedia and BioScope corpus indepen-
dently and evaluate them respectively, leading to
fine performances for all of them.
The rest of the paper is organized as follows.
The next section presents the technical details of
our system of hedge detection and scope finding.
Section 3 gives information of features. Section
4 shows the evaluation results, including official
results and further ones after official outputs col-
lection. Section 5 concludes the paper.
2 Methods
Basically, the tasks are formulated as sequence la-
beling in our approach. The available label set dif-
fers between task 1 and 2. In addition, it is needed
to introduce an indicator in order to find scopes for
the multi-hedge sentences properly.
2.1 Hedge detection
The valid label set of task 1, hedge detection, con-
tains only two labels: ?Hedge? and ? ?, which
represent that a word is in a hedge cue or not
respectively. Since results of hedge detection in
this shared task are evaluated at sentence level, a
sentence will be classified as ?uncertain? in the
post-process if it has one or more words labeled
?Hedge? in it and otherwise ?certain?.
92
2.2 Scope finding
The second task is divided into two steps in our
system. The first step is quite the same as what
the system does in task 1: labeling the words as in
hedge cues or not. Then the scope of each hedge
will be labeled by taking advantage of the result
of the first step. A scope can be denoted by a
beginning word and an ending word to represent
the first and the last element. In scope finding the
available label set contains ?Begin?, ?End?, ?Mid-
dle? and ? ?, representing the first and last word in
the scope, in-scope and out-of-scope. As an exam-
ple, a sentence with hedge cue and scope labeling
is given in Table 1. Hedge cue ?indicating? with
its scope from ?indicating? itself to ?transcription?
are labeled. While evaluating outputs, only ?Be-
gin?s and ?End?s will be taken into consideration
and be treated as the head and tail tokens of the
scopes of specific hedge cues.
Furthermore ...
, ...
inhibition ...
can ...
be ...
blocked ...
by ...
actinomycin ...
D ...
, ...
indicating ... Hedge Begin
a ... Middle
requirement ... Middle
for ... Middle
de ... Middle
novo ... Middle
transcription ... End
. ...
Table 1: A sentence with hedge cue and scope la-
beling
It seems that the best labeling result of task 1
can be used directly to be the proper intermediate
representation of task 2. However, the complexity
of scope finding for multi-hedge sentences forces
us to modify the intermediate result of task 2 for
the sake of handling the sentences with more than
one hedge cue correctly. Besides, since task 1 is
a sentence classification task essentially, while the
goal of the first step of task 2 is to label the words
as accurately as possible, it is easy to find that
the optimal labeling results of task 1 may not be
optimal to be the intermediate representations for
task 2. This problem can be solved if sentence-
level hedge detection and intermediate representa-
tion finding are treated as two separate tasks with
independent feature selection procedures. The de-
tails of feature selection will be given in section
3.
2.3 Scope finding for multi-hedge cases
Sentences with more than one hedge cue are quite
common in both datasets of BioScope corpus and
Wikipedia. By counting hedges in every sentence,
we find that about one fourth of the sentences with
hedges have more than one hedge cue in all three
data sources (Table 2). In Morante and Daele-
mans (2009), three classifiers predict whether each
token is Begin, End or None and a postprocess-
ing is needed to associate Begins and Ends with
their corresponding hedge cues. In our approach,
in order to decrease ambiguous or illegal outputs
e.g. inequivalent numbers of Begins and Ends, a
pair of Begin and End without their correspond-
ing hedge cue between them, etc., sentences with
more than one hedge cue will be preprocessed by
making copies as many as the number of hedges
and be handled separately.
The sentence which is selected as a sample has
two hedge cues: ?suggesting? and ?may?, so our
system preprocesses the sentence into two single-
hedge ones, which is illustrated in Table 3. Now it
comes to the problem of finding scope for single-
hedge sentence. The two copies are labeled sep-
arately, getting one scope from ?suggesting? to
?mitogenesis? for the hedge cue ?suggesting? and
the other from ?IFN-alpha? to ?mitogenesis? for
?may?. Merging the two results will give the final
scope resolution of the sentence.
However, compared with matching Begins and
Ends in postprocessing given by Morante and
Daelemans (2009), the above method gives rise
to out of control of projections of the scopes,
i.e. scopes of hedges may partially overlap after
copies are merged. Since scopes should be in-
tact constituents of sentences, namely, subtrees in
syntax tree which never partly overlap with each
other, results like this are linguistically illegal and
should be discarded. We solve this problem by in-
troducing an instructional feature called ?Indica-
tor?. For sentences with more than one hedge cue,
namely more than one copy while finding scopes,
words inside the union of existing (labeled) scopes
will be tagged as ?Indicator? in unhandled copies
before every labeling. For example, after finding
scope for the first copy in Table 3 and words from
93
Dataset # Sentence # No-hedge ratio # One-hedge ratio # Multi-hedge ratio
Biomedical Abstracts 11871 9770 82.3% 1603 13.5% 498 4.2%
Biomedical Fulltexts 2670 2151 80.6% 385 14.4% 134 5.0%
Wikipedia 11111 8627 77.6% 1936 17.4% 548 4.9%
Table 2: Statistics of hedge amount
IFN-alpha IFN-alpha
also also
sensitized sensitized
T T
cells cells
to to
IL-2-induced IL-2-induced
proliferation proliferation
, ,
further further
suggesting Hedge suggesting
that that
IFN-alpha IFN-alpha
may may Hedge
be be
involved involved
in in
the the
regulation regulation
of of
T-cell T-cell
mitogenesis mitogenesis
. .
Table 3: An example of 2-hedge sentence before
scope finding
?suggesting? to ?mitogenesis? are put in the scope
of cue ?suggesting?, these words should be tagged
?Indicator? in the second copy, whose result is il-
lustrated in Table 4. If not in a scope, any word is
tagged ? ? as the indicator. The ?Indicator?s tag-
ging from ?suggesting? to ?mitogenesis? in Table
4 mean that no other than the situations of a) ?Be-
gin? is after or at ?suggesting? and ?End? is before
or at ?mitogenesis? b) Both ?Begin? and ?End? are
before ?suggesting? c) Both next ?Begin? and next
?End? are after ?mitogenesis? can be accepted. In
other words, new labeling should keep the projec-
tions of scopes in the result. Although it is only
an instructional indicator and does not have any
coerciveness, the evaluation result of experiment
shows it effective.
3 Feature selection
Since hedge and scope finding are quite novel
tasks and it is not easy to determine the effective
features by experience, a greedy feature selection
is conducted. As it mentioned in section 2, our
system divides scope finding into two sub-tasks:
IFN-alpha ...
also ...
sensitized ...
T ...
cells ...
to ...
IL-2-induced ...
proliferation ...
, ...
further ...
suggesting ... Indicator
that ... Indicator
IFN-alpha ... Indicator Begin
may ... Indicator Hedge Middle
be ... Indicator Middle
involved ... Indicator Middle
in ... Indicator Middle
the ... Indicator Middle
regulation ... Indicator Middle
of ... Indicator Middle
T-cell ... Indicator Middle
mitogenesis ... Indicator End
. ...
Table 4: Scope resolution with instructional fea-
ture: ?Indicator?
a) Hedge cue labeling
b) Scope labeling
The first one is the same as hedge detection task
in strategy, but quite distinct in target of feature
set, because hedge detection is a task of sentence
classification while the first step of scope find-
ing aims at high accuracy of labeling hedge cues.
Therefore, three independent procedures of fea-
ture selection are conducted for BioScope corpus
dataset. AsWikipedia is not involved in the task of
scope finding, it only needs one final feature set.
About 200 feature templates are initially con-
sidered for each task. We mainly borrow ideas and
are enlightened by following sources while initial-
izing feature template sets:
a) Previous papers on hedge detection and
scope finding (Light et al, 2004; Medlock,
2008; Medlock and Briscoe, 2008; Kilicoglu
and Bergler, 2008; Szarvas, 2008; Ganter
and Strube, 2009; Morante and Daelemans,
2009);
94
b) Related works such as named entity recog-
nition (Collins, 1999) and text chunking
(Zhang et al, 2001);
c) Some literature on dependency parsing
(Nivre and Scholz, 2004; McDonald et al,
2005; Nivre, 2009; Zhao et al, 2009c; Zhao
et al, 2009a);
3.1 Notations of Feature Template
A large amount of advanced syntactic features in-
cluding syntactic connections, paths, families and
their concatenations are introduced. Many of these
features come from dependency parsing, which
aims at building syntactic tree expressed by depen-
dencies between words. More details about de-
pendency parsing are given in Nivre and Scholz
(2004) and McDonald et al (2005). The parser
in Zhao et al (2009a) is used to construct de-
pendency structures in our system, and some of
the notations in this paper adopt those presented
in Zhao et al (2009c). Feature templates are from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This part of features includes
word form (form), lemma (lemma), part-of-speech
tag (pos), syntactic dependency (dp) , syntactic de-
pendency label (dprel).
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm and rn) and high (low) support verb, noun or
preposition. Here we specify the last one as an
example, support verb(noun/preposition). From a
given word to the syntactic root along the syntac-
tic tree, the first verb/noun/preposition that is met
is called its low support verb/noun/preposition,
and the nearest one to the root(farthest to
the given word) is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al, 2005;
Xue, 2006; Jiang and Ng, 2006), and it is extended
to nouns and prepositions in Zhao et al (2009b).
In addition, a slightly modified syntactic head, pp-
head, is introduced, it returns the left most sibling
of a given word if the word is headed by a prepo-
sition, otherwise it returns the original head.
Path. There are two basic types of path. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For example, m:n|dpPath represents the
dependency path from word m to n. Assuming
that the two paths from m and n to the root are
pm and pn, m:n|dpPathShare, m:n|dpPathPred
and m:n|dpPathArgu represent the common part
of pm and pn, part of pm which does not belong
to pn and part of pn which does not belong to pm,
respectively.
Family. A children set includes all syntactic
children(children) are used in the template nota-
tions.
Concatenation of Elements. For all collected
elements according to dpPath, children and so on,
we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
Hedge Cue Dictionary and Scope Indicator.
Hedge cues in the training set are collected and put
in a dictionary. Whether a word in the training or
testing set is in the dictionary (dic) is introduced
into feature templates. As the evaluation is non-
open, we do not put in any additional hedge cues
from other resources. An indicator (indicator) is
given for multi-hedge scope finding, as specified
in section 2.At last, in feature set for scope label-
ing, hedge represents that the word is in a hedge
cue.
At last, we take x as current token to be labeled,
and xm to denote neighbor words. m > 0 repre-
sents that it is a word goes mth after current word
and m < 0 for word ?mth before current word.
3.2 Feature template sets for each task
As optimal feature template subsets cannot be ex-
pected to be extracted from so large sets by hand,
greedy feature selections according to Zhao et al
(2009b) are applied. The normalized feature selec-
tion has been proved to be effective in quite a lot
of NLP tasks and can often successfully select an
optimal or very close to optimal feature set from a
large-scale superset. Although usually it needs 3
to 4 loops denoted by ?While? in the Algorithm 1
of Zhao et al (2009b) to get the best template set,
we only complete one before official outputs col-
lection because of time limitation, which to a large
extent hinders the performance of the system.
Three template sets are selected for BioScope
corpus. One with the highest accuracy for
sentence-level hedge detection (Set B), one with
the best performance for word-level hedge cue la-
95
beling (Set H) and another one with the maximal
F-score for scope finding (Set S). In addition, one
set is discovered for sentence-level hedge detec-
tion of Wikipedia (Set W)1 . Table 52 lists some
selected feature templates which are basic word or
hedging properties for the three sets of BioScope
corpus and Wikipedia. From the table we can see
it is clear that the combinations of lemma, POS
and word form of words in context, which are usu-
ally basic and common elements in NLP, are also
effective for hedge detection. And as we expected,
the feature that represents whether the word is in
the hedge list or not is very useful especially in
hedge cue finding, indicating that methods based
on a hedge cue lists (Light et al, 2004) or keyword
selection (Szarvas, 2008) are quite significant way
to accomplish such tasks.
Some a little complicated syntactic features
based on dependencies are systemically exploited
as features for tasks. Table 6 enumerates some of
the syntactic features which proves to be highly
effective. We noticed that lowSupportNoun, high-
SupportNoun and features derived from dpPath is
notably useful. It can be explained by the aware-
ness that hedge labeling and scope finding are to
process literatures in the level of semantics where
syntactic features are often helpful.
We continue our feature selection procedures
for BioScope corpus after official outputs collec-
tion and obtain feature template sets that bring bet-
ter performance. Table 7 gives some of the fea-
tures in the optimized sets for BioScope corpus
resolution. One difference between the new sets
and the old ones is the former contain more syntac-
tic elements, indicating that exploiting syntactic
feature is a correct choice. Another difference is
the new sets assemble more information of words
before or after the current word, especially words
linearly far away but close in syntax tree. Appear-
ance of combination of these two factors such as
x?1.lm.form seems to provide an evidence of the
insufficiency training and development of our sys-
tem submitted to some extent.
4 Evaluation results
Two tracks (closed and open challenges) are pro-
vided for CoNLL-2010 shared task. We partici-
pated in the closed challenge, select features based
1num in the set of Wikipedia represents the sequential
number of word in the sentence
2Contact the authors to get the full feature lists, as well as
entire optimized sets in post-deadline experiment
- x.lemma + x1.lemma + x?1.lemma
+ x.dic + x1.dic + x?1.dic
- x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.form
Set B x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.dic + x1.dic + x?1.dic
- x1.pos
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic
- x.pos + x?1.pos
- x.dic
Set H x.dic + x.lemma + x.pos + x.form
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x?2.form + x?2.lemma
- x?1.form + x.form
- x.dic + x1.dic + x?1.dic
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x.indicator
- x.hedge + x1.hedge + x?1.hedge
Set S x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.pos + x.hedge + x.dp + x.dprel
- x1.pos
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.lemma + x1.lemma + x?1.lemma
- + x.dic + x1.dic + x?1.dic
- x.lemma + x1.lemma + x?1.lemma
+x2.lemma + x?2.lemma + x.dic
+ x1.dic + x?1.dic + x2.dic + x?2.dic
- x.lemma + x1.lemma
Set W x.hedge + x1.hedge + x?1.hedge
+ x2.hedge + x?2.hedge + x3.hedge
+ x?3.hedge
- x.pos + x1.pos + x?1.pos +x2.pos
+ x?2.pos + x.dic + x1.dic + x?1.dic
+ x2.dic + x?2.dic
- x.pos + x.dic
- x.num + x.dic
Table 5: Selected feature template sets
96
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x.lowSupoortNoun.pos
- x.pos + x.children.dprel.bag
- x.rm.dprel + x.form
Set B x.pphead.lemma
- x.form + x.children.dprel.bag
- x.lowSupportNoun:x?dpTreeRelation
- x.lowSupportProp.lemma
- x.form + x.children.dprel.noDup
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.highSupportNoun.pos
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
Set H + x.highSupportProp:x|dpPathArgu.dprel.seq
- xlowSupportProp.lemma
- x.rm.dprel
- x.lm.form
- x.lemma + x.pphead.form
- x.lowSupportVerb.form
- x.rm.lemma + x.rm.form
- x.children.dprel.noDup
- x.children.dprel.bag
- x.highSupportNoun:x|dpTreeRelation
- x.lemma + x.pphead.form
Set S x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportVerb.lemma
- x.h.children.dprel.bag
- x.highSupportVerb.form
- x.lm.form
- x.lemma + x.pphead.form
- x.lm.dprel + x.pos
- x.lowSupportProp:x|dpPathPred.dprel.seq
- x.pphead.lemma
Set W x.rm.lemma
- x.lowSupportProp:x|dpTreeRelation
- x.lowSupportVerb:x|dpPathPred.dprel.seq
- x.lowSupportVerb:x|dpPathPred.pos.seq
- x.lowSupportVerb:x|dpPathShared.pos.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.lowSupportProp.form
Table 6: Syntactic features
- x?1.lemma
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x?1.pos + x1.pos
Set H x.rm.lemma
- x.rm.dprel
- x.lm.dprel + x.pos
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x?1.lemma
- x.lemma + x1.lemma + x?1.lemma + x.dic
+ x1.dic + x?1.dic
- x.form + x.lemma + x.pos + x.dic
Set B x?2.form + x?1.form
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x?1.lm.form
- x1.form
- x.pos + x.dic
- x.hedge + x1.hedge + x?1.hedge
- x.pos + x1.pos + x?1.pos + x2.pos + x?2.pos
Set S x.children.dprel.bag
- x.lemma + x.pphead.form
- x.highSupportVerb.form
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportNoun:x|dpTreeRelation + x.form
Table 7: Selected improved feature template sets
for BioScope corpus
on the in-domain data and evaluated our system
on the in-domain and cross-domain evaluation set.
All the experiments are implemented and run by
Maximum Entropy Markov Models (McCallum,
2000).
4.1 Official results
The official results for tasks are in Table 8, in
which three in-domain tests and cue matching
result for biomedical texts are listed. For the
first task for BioCorpus, our system gives F-score
0.8363 in in-domain test and for Wikipedia we
give F-score 0.5618 in closed evaluation. For the
second task, our system gives results in closed and
open test, with F-score 0.4425 and 0.4441 respec-
tively.
We compare the F-score of our system with the
best in the final result in Table 9. We rank pretty
high in Wikipedia hedge detection, while other
three are quite steady but not prominent. This is
mainly due to two reasons:
1. Feature selection procedures are not perfectly
conducted.
2. Abstracts and fulltexts in BioScope are mixed
to be the training set, which proves quite in-
appropriate when the evaluation set contains
97
only fulltext literature, since abstract and full-
text are quite different in terms of hedging.
Dataset F-score Best
Task1-closed 0.8363 0.8636
BioScope Task2-closed 0.4425 0.5732
Cue-matching 0.7853 0.8134
Wikipedia Task1-closed 0.5618 0.6017
Table 9: Comparing results with the best
4.2 Further results
Intact feature selection procedures for BioScope
corpus are conducted after official outputs collec-
tions. The results of evaluation with completely
selected features compared with the incomplete
one are given in Table 7. The system performs a
higher score on evaluation data (Table 10), which
is more competitive in both tasks on BioScope cor-
pus. The improvement for task 2 is significant, but
the increase of performance of hedge cue detec-
tion is less remarkable. We believe that a larger
fulltext training set and a more considerate train-
ing plan will help us to do better job in the future
work.
Dataset Complete Incomplete
Task1-closed 0.8522 0.8363
BioScope Task2-closed 0.5151 0.4425
Cue-matching 0.7990 0.7853
Table 10: Comparing improved outputs with the
best
5 Conclusion
We describe the system that uses sequence label-
ing with normalized feature selection and rich fea-
tures to detect hedges and find scopes for hedge
cues. Syntactic features which are derived from
dependencies are exploited, which prove to be
quite favorable. The evaluation results show that
our system is steady in performance and does
pretty good hedging and scope finding in both Bio-
Scope corpus and Wikipedia, especially when the
feature selection procedure is carefully and totally
conducted. The results suggest that sequence la-
beling and a feature-oriented method are effective
in such NLP tasks.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore, 4,
August.
Ken Hyland. 1996. Writing without conviction: Hedg-
ing in science research articles. Applied Linguistics,
17:433?54.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proceedings of the EMNLP-2006, pages
138?145, Sydney, Australia.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9.
Marc Light, Xin Ying Qiu, and Padimini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proc. of the
BioLINK 2004, pages 17?24.
Chrysanne Di Marco and Robert E. Mercer. 2004.
Hedging in scientific articles as a means of classify-
ing citations. In Working Notes of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications, pages 50?54.
Andrew McCallum. 2000. Maximum entropy markov
models for information extraction and segmentation.
In Proceedings of ICML 2000, pages 591?598, Stan-
ford, California.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT/EMNLP 05, pages 523?530, Vancouver,
Canada, October.
Ben Medlock and Ted Briscoe. 2008. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of 45th Annual Meeting
of the ACL, pages 992?999, Prague, Czech Repub-
lic, June.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41:636?654.
98
Dataset TP FP FN precision recall F-score
BioScope Task1-closed 669 141 121 0.8259 0.8468 0.8363
Task2-closed 441 519 592 0.4594 0.4269 0.4425
Cue-matching 788 172 259 0.8208 0.7526 0.7853
Wikipedia Task1-closed 991 303 1243 0.7658 0.4436 0.5618
Table 8: Official results of our submission for in-domain tasks
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36,
Boulder, Colorado, June.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004, pages 64?70, Geneva, Switzer-
land, August 23rd-27th.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP 2009, pages 351?359, Suntec, Singapore,
2-7 August.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of BioNLP 2008,
pages 38?45, Columbus, Ohio, USA, June.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL-08, pages 281?
289, Columbus, Ohio, USA, June.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proc. of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text Mining,
pages 27?34, Marrakech.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589?596, Ann Arbor, USA.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in Chinese. In Proceedings of
the Human Language Technology Conference of the
NAACL (NAACL-2006), pages 431?438, New York
City, USA, June.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Pro-
ceedings of the 39th Annual Meeting on Associa-
tion for Computational Linguistics, pages 539?546,
Toulouse, France.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of CoNLL-2009, June 4-5,
Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009b.
Semantic dependency parsing of NomBank and
PropBank: An efficient integrated approach via a
large-scale feature selection. In Proceedings of
EMNLP-2009, pages 30?39, Singapore.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009c. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
June 4-5, Boulder, Colorado, USA.
99
Dependency Parser for Chinese Constituent Parsing ?
Xuezhe Ma, Xiaotian Zhang, Hai Zhao, Bao-Liang Lu
1Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University, 800 Dong Chuan Rd., Shanghai 200240, China
{xuezhe.ma,xtian.zh}@gmail.com, {zhaohai,blu}@cs.sjtu.edu.cn
Abstract
This paper presents our work for participation
in the 2010 CIPS-ParsEval shared task on Chi-
nese syntactic constituent tree parsing. We
use dependency parsers for this constituent
parsing task based on a formal dependency-
constituent transformation method which con-
verts dependency to constituent structures us-
ing a machine learning approach. A condi-
tional random fields (CRF) tagger is adopted
for head information recognition. Our ex-
periments shows that acceptable parsing and
head tagging results are obtained on our ap-
proaches.
1 Introduction
Constituent parsing is a challenging but useful task
aiming at analyzing the constituent structure of a sen-
tence. Recently, it is widely adopted by the popular ap-
plications of natural language processing techniques,
such as machine translation (Ding and Palmer, 2005),
synonym generation (Shinyama et al, 2002), relation
extraction (Culotta and Sorensen, 2004) and lexical re-
source augmentation (Snow et al, 2004). A great deal
of researches have been conducted on this topic with
promising progress (Magerman, 1995; Collins, 1999;
Charniak, 2000; Charniak and Johnson, 2005; Sagae
and Lavie, 2006; Petrov and Klein, 2007; Finkel et al,
2008; Huang, 2008).
Recently, several effective dependency parsing al-
gorithms has been developed and shows excellent per-
formance in the responding parsing tasks (McDonald,
2006; Nivre and Scholz, 2004). Since graph struc-
tures of dependency and constituent parsing over a
sentence are strongly related, they should be benefited
from each other. It is true that constituent parsing may
be smoothly altered to fit dependency parsing. How-
ever, due to the inconvenience from dependency to
constituent structure, it is not so easy to adopt the latter
?This work is partially supported by the National Natu-
ral Science Foundation of China (Grant No. 60903119, Grant
No. 60773090 and Grant No. 90820018), the National Ba-
sic Research Program of China (Grant No. 2009CB320901),
and the National High-Tech Research Program of China
(Grant No.2008AA02Z315).
for the former. This means that most of these popular
and effective dependency parsing models can not be di-
rectly extended to constituents parsing. This paper pro-
poses an formal method for such a conversion which
adoptively solves the problem of ambiguity. Based on
the proposed method, a dependency parsing algorithm
can be used to solve tasks of constituent parsing.
A part of Tsinghua Chinese Treebank (TCT) (Zhou,
2004; Zhou, 2007; Chen et al, 2008) is used as
the training and test data for the 2010 CIPS-ParsEval
shared task. Being different from the annotation
scheme of the Penn Chinese Treebank (CTB), the TCT
has another annotation scheme, which combines both
the constituent tree structure and the head informa-
tion of each constituent. Specifically, there can be al-
ways multiple heads in a constituent. For the 2010
CIPS-ParsEval shared task, only segmented sentences
are given in test data without part-of-speech (POS)
tags, a POS tagger is required for this task. There-
fore, we divide our system into three major cascade
stages, namely POS tagging, constituent parsing and
head information recognition, which are connected as
a pipeline of processing. For the POS tagging, we
adopt the SVMTool tagger (Gimenez and Marquez,
2004); for the constituent parsing, we use the Maxi-
mum Spanning Tree (MST) (McDonald, 2006) parser
combined with a dependencies-to-constituents conver-
sion; and for the head information recognition, we ap-
ply a sequence labeling method to label head informa-
tion.
Section 2 presents the POS tagger in our approach.
The details of our parsing method is presented in sec-
tion 3. The head information recognition is described
in section 4. The data and experimental results are
shown in section 5. The last section is the conclusion
and future work.
2 POS Tagging
The SVMTool tagger (Gimenez and Marquez, 2004) is
used as our POS tagging tool for the first stage. It is a
POS tagger based on SVM classifier, written in Perl. It
can be trained on standardized collection of hand POS-
tagged sentences. It uses SVM-Light1 toolkit as the
1http://www.cs.cornell.edu/People/tj/
svm_light/.
implementation of SVM classifier and achieves 97.2%
accuracy on the Penn English Treebank. We test the
accuracy of the SVMTool tagger on the development
set of the TCT (see section 5.1) and achieve accuracy
of 94.98%.
3 Parsing Constituents Using
Dependency Parsing Algorithms
3.1 Convert Dependencies to Constituents
The conversion from constituent to dependency struc-
tures is straightforward with some specific rules based
on linguistic theory. However, there is not an effective
method which can accurately accomplish the opposite
transformation, from the dependency structures back
into constituent ones due to the existence of ambiguity
introduced by the former transformation.
Aimed at the above difficulty, our solution is to in-
troduce a formal dependency structure and a machine
learning method so that the ambiguity from depen-
dency structures to constituent structures can be dealt
with automatically.
3.1.1 Binarization
We first transform constituent trees into the form
that all productions for all subtrees are either unary or
binary, before converting them to dependency struc-
tures. Due to the binarization, the target constituent
trees of the conversion from dependency back to con-
stituent structures are binary branching.
This binarization is done by the left-factoring ap-
proach described in (Charniak et al, 1998; Petrov and
Klein, 2008), which converts each production with n
children, where n > 2, into n? 1 binary productions.
Additional non-terminal nodes introduced in this con-
version must be clearly marked. Transforming the bi-
nary branching trees into arbitrary branching trees is
accomplished by using the reverse process.
3.1.2 Using Binary Classifier
We train a classifier to decide which dependency
edges should be transformed first at each step of con-
version automatically. After the binarization described
in the previous section, only one dependency edge
should be transformed at each step. Therefore the
classifier only need to decide which dependency edge
should be transformed at each step during the conver-
sion.
As a result of the projective property of constituent
structures, this problem only happens in the cases that
modifiers are at both sides of their heads. And for these
cases that one head has multiple modifiers, only the
leftmost or the rightmost dependency edge could be
transformed first. Therefore, a binary classifier is al-
ways enough for the disambiguation at each step.
1. Word form of the parent
2. Part-of-speech (POS) tag of the parent
3. Word form of the leftmost child
4. POS tag of the leftmost child
5. Dependency label of the leftmost child
6. Word form of the rightmost child
7. POS tag of the rightmost child
8. Dependency label of the rightmost child
9. Distance between the leftmost child and
the parent
10. Distance between the rightmost child
and the parent
Table 1: Features used for conversion classifier.
Support Vector Machine (SVM) is adopted as the
learning algorithm for the binary classifier and the fea-
tures are in Table 1.
3.1.3 Convert Constituent Labels
The rest problem is that we should restore the label
for each constituent when dependency structure trees
are again converted to constituent structures. The prob-
lem is solved by storing constituent labels as labels of
dependency types. The label for each constituent is just
used as the label dependency type for each dependency
edge.
The conversion method is tested on the develop-
ment, too. Constituent trees are firstly converted into
dependency structures using the head rules described
in (Li and Zhou, 2009). Then, we transform those
trees back to constituent structure using our conversion
method and use the PARSEVAL (Black et al, 1991)
measures to evaluate the performance of the conver-
sion method. Our conversion method obtains 99.76%
precision and 99.76% recall, which is a great perfor-
mance.
3.2 Dependency Parser for Constituent Parsing
Based on the proposed conversion method, depen-
dency parsing algorithms can be used for constituent
parsing. This can be done by firstly transforming train-
ing data from constituents into dependencies and ex-
tract training instances to train a binary classifier for
dependency-constituent conversion, then training a de-
pendency parser using the transformed training data.
On the test step, parse the test data using the depen-
dency parser and convert output dependencies to con-
stituents using the binary classifier trained in advance.
In addition, since our conversion method needs depen-
dency types, labeled dependency parsing algorithms
are always required.
1. Constituent label of the constituent
2. Constituent label of each child of
the constituent.
3. Wether it is a terminal for each
child of the constituent
4. The leftmost word in the sentence
of each child of the constituent.
5. The leftmost word in the sentence
of each child of the constituent.
Table 2: CRF features for head information recogni-
tion.
1. Word form and POS tag of the parent.
2. Word form and POS tag of each child.
3. POS tag of the leftmost child of each child.
4. POS tag of the rightmost child of each child.
5. Dependency label between the parent and
its parent
Table 3: CRF features for dependency type labeling.
4 Head Information Recognition
Since head information of each constituent is always
determined by the syntactic label of its own and the
categories of the constituents in subtrees, the order and
relations between the productions of each constituent
strongly affects the head information labeling. It is
natural to apply a sequential labeling strategy to tackle
this problem. The linear chain CRF model is adopted
for the head information labeling, and the implemen-
tation of CRF model we used is the 0.53 version of
the CRF++ toolkit2. We assume that head information
is independent between different constituents, which
could decrease the length of sequence to be labeled for
the CRF model.
We use a binary tag set to determine whether a con-
stituent is a head, e.g. H for a head, O for a non-head,
which is the same as (Song and Kit, 2009). The fea-
tures in Table 2 are used for CRF model.
To test our CRF tagger, we remove all head informa-
tion from the development set, and use the CRF tagger
to retrieve the head. The result strongly proves its ef-
fectiveness by showing an accuracy of 99.52%.
5 Experiments
All experiments reported here were performed on a
Core 2 Quad 2.83Ghz CPU with 8GB of RAM.
2The CRF++ toolkit is publicly available from
http://crfpp.sourceforge.net/.
5.1 Data
There are 37,219 short sentences in official released
training data for the first sub-task and 17,744 long sen-
tences for the second sub-task (for the second sub-task,
one line in the training data set may contain more than
one sentence). We split one eighth of the data as our
development set. On the other hand, there are both
1,000 sentences in released test data for the first and
second sub-tasks.
5.2 Constituent Parsing
As mentioned in section 3, constituent parsing is
done by using a dependency parser combined with
our conversion method. We choose the second or-
der maximum spanning tree parser with k-best online
large-margin learning algorithm (Crammer and Singer,
2003; Crammer et al, 2003). The MST parser we use
is in the form of an open source program implemented
in C++3.
The features used for MST parser is the same as
(McDonald, 2006). Both the single-stage and two-
stage dependency type labeling approaches are applied
in our experiments. For the two-stage dependency type
labeling, The linear chain CRF model is adopted in-
stead of the first-order Markov model used in (McDon-
ald, 2006). The features in Table 3 are used for CRF
model. It takes about 7 hours for training the MST
parser, and about 24 hours for training the CRF model.
As mentioned in section 3.1.2, SVM is adopted as
the learning algorithm for the binary classifier. There
are about 40,000 training instances in the first sub-task
and about 80,000 in the second sub-task. Develop-
ment sets are used for tuning parameter C of SVM
and the training time of the SVM classifier for the first
and second sub-task is about 8 and 24 hours, respec-
tively. However, the conversion from dependencies to
constituents is extremely fast. Converting more than
2,000 trees takes less than 1 second.
To transform the constituent trees in training set into
dependency structures, we use the head rules of (Li and
Zhou, 2009).
5.3 Results
The evaluation metrics used in 2010 CIPS-ParsEval
shared task is shown in following:
1. syntactic parsing
Precision = number of correct constituents in proposed parsenumber of constituents in proposed parse
Recall = number of correct constituents in proposed parsenumber of constituents in standard parse
F1 = 2*Precision*RecallPrecision+Recall
3The Max-MSTParser is publicly available from
http://max-mstparser.sourceforge.net/.
without head with head
Precision Recall F1 Precision Recall F1
single-stage 77.78 78.13 77.96 75.78 76.13 75.95
two-stage 78.61 78.76 78.69 76.61 76.75 76.68
Table 4: Official scores of syntactic parsing. single-stage and two-stage are for single-stage and two-stage depen-
dency type labeling approached, respectively.
Micro-R Macro-R
single-stage 62.74 62.47
two-stage 63.14 62.48
Table 5: Official scores of event recognition
The correctness of syntactic constituents is judged
based on the following two criteria:
(a) the boundary, the POS tags of all the words
in the constituent and the constituent type la-
bel should match that of the constituent in
the gold standard data.
(b) the boundary, the POS tags of all the words
in the constituent, the constituent type la-
bel and head child index of the constituent
should match that of the constituent in the
gold standard data. (if the constituent con-
tains more than one head child index, at least
one of them should be correct.)
2. event pattern recognition
Micro-R = number of all correct events in proposed parsenumber of all events in standard parse
Macro-R = sum of recall of different target verbsnumber of target verbs
Here the event pattern of a sentence is defined to
be the sequence of event blocks controlled by the
target verb in a sentence. The criteria for judging
the correctness of event pattern recognition is:
? the event pattern should be completely con-
sistent with gold standard data (information
of each event block should completely match
and the order of event blocks should also
consistent).
There are both two submissions for the first and sec-
ond sub-tasks. One is using the single-stage depen-
dency type labeling and the other is two-stage. Since
there are some mistakes in our models for the second
sub-task, the results of our submissions are unexpect-
edly poor and are not shown in this paper. All the re-
sults in this paper is reported by the official organizer
of the 2010 CIPS-ParsEval shared task.
The accuracy of POS tagging on the official test data
is 92.77%. The results of syntactic parsing for the first
sub-task is shown in Table 4. And results of event
recognition is shown in Table 5.
From the Table 4 and 5, we can see that our system
achieves acceptable parsing and head tagging results,
and the results of event recognition is also reasonably
high.
5.4 Comparison with Previous Works
We comparison our approach with previous works of
2009 CIPS-ParsEval shared task. The data set and
evaluation measures of 2009 CIPS-ParsEval shared
task, which are quite different from that of 2010 CIPS-
ParsEval shared task, are used in this experiment for
the comparison purpose. Table 6 shows the compari-
son.
We compare our method with several main parsers
on the official data set of 2009 CIPS-ParsEval shared
task. All these results are evaluated with official
evaluation tool by the 2009 CIPS-ParsEval shared
task. Bikel?s parser4 (Bikel, 2004) in Table 6 is
a implementation of Collins? head-driven statistical
model (Collins, 2003). The Stanford parser5 is based
on the factored model described in (Klein and Man-
ning, 2002). The Charniak?s parser6 is based on the
parsing model described in (Charniak, 2000). Berke-
ley parser7 is based on unlexicalized parsing model de-
scribed in (Petrov and Klein, 2007). According to Ta-
ble 6, the performance of our method is better than all
the four parsers described above. Chen et al (2009)
and Jiang et al (2009) both make use of combination
of multiple parsers and achieve considerably high per-
formance.
4http://www.cis.upenn.edu/?dbikel/
software.html
5http://nlp.stanford.edu/software/
lex-parser.shtml/
6ftp://ftp.cs.brown.edu/pub/nlparser/
7http://nlp.cs.berkeley.edu/Main.html
F1
Bikel?s parser 81.8
Stanford parser 83.3
Charniak?s parser 83.9
Berkeley parser 85.2
this paper 85.6
Jiang et al(2009). 87.2
Chen et al(2009). 88.8
Table 6: Comparison with previous works
6 Conclusion
This paper describes our approaches for the parsing
task in CIPS-ParsEval 2010 shared task. A pipeline
system is used to solve the POS tagging, constituent
parsing and head information recognition. SVMTool
tagger is used for the POS tagging. For constituent
parsing, we proposes a conversion based method,
which can use dependency parsers for constituent pars-
ing. MST parser is chosen as our dependency parser.
A CRF tagger is used for head information recognition.
The official scores indicate that our system obtains ac-
ceptable results on constituent parsing and high perfor-
mance on head information tagging.
One of future work should apply parser combination
and reranking approaches to leverage this in producing
more accurate parsers.
References
Bikel, Daniel M. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4):480?511.
Black, Ezra W., Steven P. Abney, Daniel P. Flickinger,
Cluadia Gdaniec, Ralph Grishman, Philio Harrison,
Donald Hindle, Robert J.P. Inqria, Frederick Jelinek,
Judith L. Klavans, Mark Y. Liberman, Mitchell P.
Marcus, Salim Roukos, and B Santorini. 1991. A
procedure for quantitatively comparing the syntac-
tic coverage of english grammars. In Proceedings
of the February 1991 DARPA Speech and Natural
Language Workshop.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the 43rd ACLL, pages
132?139.
Charniak, Eugene, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very Large
Corpora.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, seattle, WA.
Chen, Yi, Qiang Zhou, and Hang Yu. 2008. Anal-
ysis of the hierarchical Chinese funcitional chunk
bank. Journal of Chinese Information Processing,
22(3):24?31.
Chen, Xiao, Changning Huang, Mu Li, and Chunyu
Kit. 2009. Better parser combination. In CIPS-
ParsEval-2009 shared task.
Collins, Michael. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learining.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2003. Online
passive aggressive algorithms. In Proceedings of
NIPS.
Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Finkel, Jenny Rose, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. pages 959?967, The
Ohio State University, Columbus, Ohio, USA.
Gimenez and Marquez. 2004. Svmtool: A general
POS tagger generator based on support vector ma-
chines. In Proceedings of the 4th International Con-
ference of Language Resources and Evaluation, Lis-
bon, Portugal.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL/HLT.
Jiang, Wenbin, Hao Xiong, and Qun Liu. 2009. Muti-
path shift-reduce parsing with online training. In
CIPS-ParsEval-2009 shared task.
Klein, Dan and Christopher Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In In Advances in NIPS 2002, pages
3?10.
Li, Junhui and Guodong Zhou. 2009. Soochow uni-
versity report for the 1st china workshop on syntac-
tic parsing. In CIPS-ParsEval-2009 shared task.
Magerman, David M. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283, MIT, Cambridge, Massachusetts, USA.
McDonald, Ryan. 2006. Discriminative Learning
Spanning Tree Algorithm for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, Joakim and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics (COLING-2004), pages 64?70,
Geneva, Switzerland, August 23rd-27th.
Petrov, Slav and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York.
Petrov, Slav and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. In Proceed-
ings of NIPS 20.
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 689?691, Sydney, Australia.
Shinyama, Yusuke, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In HLT-2002.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2004. Learning syntactic patterns for automatic hy-
pernym discovery. In Proceedings of NIPS.
Song, Yan and Chunyu Kit. 2009. PCFG parsing with
crf tagging for head recognition. In CIPS-ParsEval-
2009 shared task.
Zhou, Qiang. 2004. Annotation scheme for Chinese
treebank. Journal of Chinese Information Process-
ing, 18(4):1?8.
Zhou, Qiang. 2007. Base chuck scheme for the Chi-
nese language. Journal of Chinese Information Pro-
cessing, 21(3):21?27.
