Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency-based Syntactic and Semantic Parsing
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, Ting Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yqli, yhguo, qinb, tliu}@ir.hit.edu.cn
Abstract
Our CoNLL 2009 Shared Task system in-
cludes three cascaded components: syntactic
parsing, predicate classification, and semantic
role labeling. A pseudo-projective high-order
graph-based model is used in our syntactic de-
pendency parser. A support vector machine
(SVM) model is used to classify predicate
senses. Semantic role labeling is achieved us-
ing maximum entropy (MaxEnt) model based
semantic role classification and integer linear
programming (ILP) based post inference. Fi-
nally, we win the first place in the joint task,
including both the closed and open challenges.
1 System Architecture
Our CoNLL 2009 Shared Task (Hajic? et al, 2009):
multilingual syntactic and semantic dependencies
system includes three cascaded components: syn-
tactic parsing, predicate classification, and semantic
role labeling.
2 Syntactic Dependency Parsing
We extend our CoNLL 2008 graph-based
model (Che et al, 2008) in four ways:
1. We use bigram features to choose multiple pos-
sible syntactic labels for one arc, and decide the op-
timal label during decoding.
2. We extend the model with sibling features (Mc-
Donald, 2006).
3. We extend the model with grandchildren fea-
tures. Rather than only using the left-most and right-
most grandchildren as Carreras (2007) and Johans-
son and Nugues (2008) did, we use all left and right
grandchildren in our model.
4. We adopt the pseudo-projective approach in-
troduced in (Nivre and Nilsson, 2005) to handle the
non-projective languages including Czech, German
and English.
2.1 Syntactic Label Determining
The model of (Che et al, 2008) decided one la-
bel for each arc before decoding according to uni-
gram features, which caused lower labeled attach-
ment score (LAS). On the other hand, keeping all
possible labels for each arc made the decoding in-
efficient. Therefore, in the system of this year, we
adopt approximate techniques to compromise, as
shown in the following formulas.
f lbluni(h, c, l) = f lbl1 (h, 1, d, l) ? f lbl1 (c, 0, d, l)
L1(h, c) = arg maxK1l?L(w ? f lbluni(h, c, l))
f lblbi (h, c, l) = f lbl2 (h, c, l)
L2(h, c) = arg maxK2l?L1(h,c)(w ? {f lbluni ? f lblbi })
For each arc, we firstly use unigram features to
choose the K1-best labels. The second parameter of
f lbl1 (?) indicates whether the node is the head of the
arc, and the third parameter indicates the direction.
L denotes the whole label set. Then we re-rank the
labels by combining the bigram features, and choose
K2-best labels. During decoding, we only use the
K2 labels chosen for each arc (K2 ? K1 < |L|).
2.2 High-order Model and Algorithm
Following the Eisner (2000) algorithm, we use spans
as the basic unit. A span is defined as a substring
of the input sentence whose sub-tree is already pro-
duced. Only the start or end words of a span can link
with other spans. In this way, the algorithm parses
the left and the right dependence of a word indepen-
dently, and combines them in the later stage.
We follow McDonald (2006)?s implementation of
first-order Eisner parsing algorithm by modifying its
scoring method to incorporate high-order features.
Our extended algorithm is shown in Algorithm 1.
There are four different span-combining opera-
tions. Here we explain two of them that correspond
to right-arc (s < t), as shown in Figure 1 and 2. We
49
Algorithm 1 High-order Eisner Parsing Algorithm
1: C[s][s][c] = 0, 0 ? s ? N , c ? cp, icp # cp: complete; icp: incomplete
2: for j = 1 to N do
3: for s = 0 to N do
4: t = s+ jL
5: if t > N then
6: break
7: end if
# Create incomplete spans
8: C[s][t][icp] = maxs?r<t;l?L2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l))
9: C[t][s][icp] = maxs?r<t;l?L2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l))
# Create complete spans
10: C[s][t][cp] = maxs<r?t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l))
11: C[t][s][cp] = maxs?r<t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l))
12: end for
13: end for
follow the way of (McDonald, 2006) and (Carreras,
2007) to represent spans. The other two operations
corresponding to left-arc are similar.
 
Figure 1: Combining two spans into an incomplete span
Figure 1 illustrates line 8 of the algorithm in Al-
gorithm 1, which combines two complete spans into
an incomplete span. A complete span means that
only the head word can link with other words fur-
ther, noted as ??? or ???. An incomplete span
indicates that both the start and end words of the
span will link with other spans in the future, noted as
?99K? or ?L99?. In this operation, we combine two
smaller spans, sps?r and spr+1?t, into sps99Kt with
adding arcs?t. As shown in the following formu-
las, the score of sps99Kt is composed of three parts:
the score of sps?r, the score of spr+1?t, and the
score of adding arcs?t. The score of arcs?t is
determined by four different feature sets: unigram
features, bigram features, sibling features and left
grandchildren features (or inside grandchildren fea-
tures, meaning that the grandchildren lie between s
and t). Note that the sibling features are only related
to the nearest sibling node of t, which is denoted as
sck here. And the inside grandchildren features are
related to all the children of t. This is different from
the models used by Carreras (2007) and Johansson
and Nugues (2008). They only used the left-most
child of t, which is tck? here.
ficp(s, r, t, l) = funi(s, t, l) ? fbi(s, t, l)
? fsib(s, sck, t) ? {?k?i=1 fgrand(s, t, tci, l)}
Sicp(s, r, t, l) = w ? ficp(s, r, t, l)
S(sps99Kt) = S(sps?r) + S(spr+1?t)
+ Sicp(s, r, t, l)
In Figure 2 we combine sps99Kr and spr?t into
sps?t, which explains line 10 in Algorithm 1. The
score of sps?t also includes three parts, as shown
in the following formulas. Although there is no new
arc added in this operation, the third part is neces-
sary because it reflects the right (or called outside)
grandchildren information of arcs?r.
r trc1 rcks r s tr rc1 rck
l l
 
Figure 2: Combining two spans into a complete span
fcp(s, r, t, l) = ?ki=1 fgrand(s, r, rci, l)
Scp(s, r, t, l) = w ? fcp(s, r, t, l)
S(sps?t) = S(sps99Kr)
+ S(spr?t) + Scp(s, r, t, l)
50
2.3 Features
As shown above, features used in our model can be
decomposed into four parts: unigram features, bi-
gram features, sibling features, and grandchildren
features. Each part can be seen as two different sets:
arc-related and label-related features, except sibling
features, because we do not consider labels when us-
ing sibling features. Arc-related features can be un-
derstood as back-off of label-related features. Actu-
ally, label-related features are gained by simply at-
taching the label to the arc-features.
The unigram and bigram features used in our
model are similar to those of (Che et al, 2008), ex-
cept that we use bigram label-related features. The
sibling features we use are similar to those of (Mc-
Donald, 2006), and the grandchildren features are
similar to those of (Carreras, 2007).
3 Predicate Classification
The predicate classification is regarded as a super-
vised word sense disambiguation (WSD) task here.
The task is divided into four steps:
1. Target words selection: predicates with multi-
ple senses appearing in the training data are selected
as target words.
2. Feature extraction: features in the context
around these target words are extracted as shown in
Table 4. The detailed explanation about these fea-
tures can be found from (Che et al, 2008).
3. Classification: for each target word, a Support
Vector Machine (SVM) classifier is used to classify
its sense. As reported by Lee and Ng (2002) and
Guo et al (2007), SVM shows good performance on
the WSD task. Here libsvm (Chang and Lin, 2001)
is used. The linear kernel function is used and the
trade off parameter C is 1.
4. Post processing: for each predicate in the test
data which does not appear in the training data, its
first sense in the frame files is used.
4 Semantic Role Labeling
The semantic role labeling (SRL) can be divided
into two separate stages: semantic role classification
(SRC) and post inference (PI).
During the SRC stage, a Maximum en-
tropy (Berger et al, 1996) classifier is used to
predict the probabilities of a word in the sentence
Language No-duplicated-roles
Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc
Chinese A0, A1, A2, A3, A4, A5,
Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND
English A0, A1, A2, A3, A4, A5,
German A0, A1, A2, A3, A4, A5,
Japanese DE, GA, TMP, WO
Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr,
arg2-loc, arg2-null, arg4-des, argL-null, argM-
cau, argM-ext, argM-fin
Table 1: No-duplicated-roles for different languages
to be each semantic role. We add a virtual role
?NULL? (presenting none of roles is assigned)
to the roles set, so we do not need semantic role
identification stage anymore. For a predicate
of each language, two classifiers (one for noun
predicates, and the other for verb predicates) predict
probabilities of each word in a sentence to be each
semantic role (including virtual role ?NULL?). The
features used in this stage are listed in Table 4.
The probability of each word to be a semantic role
for a predicate is given by the SRC stage. The re-
sults generated by selecting the roles with the largest
probabilities, however, do not satisfy some con-
strains. As we did in the last year?s system (Che et
al., 2008), we use the ILP (Integer Linear Program-
ming) (Punyakanok et al, 2004) to get the global op-
timization, which is satisfied with three constrains:
C1: Each word should be labeled with one and
only one label (including the virtual label ?NULL?).
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?). The
threshold we use in our system is 0.3.
C3: Statistics show that some roles (except for
the virtual role ?NULL?) usually appear once for
a predicate. We impose a no-duplicate-roles con-
straint with a no-duplicate-roles list, which is con-
structed according to the times of semantic roles?
duplication for each single predicate. Table 1 shows
the no-duplicate-roles for different languages.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit1. The
classifier parameters are tuned with the development
data for different languages respectively. lp solve
5.52 is chosen as our ILP problem solver.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
2http://sourceforge.net/projects/lpsolve
51
5 Experiments
5.1 Experimental Setup
We participate in the CoNLL 2009 shared task
with all 7 languages: Catalan (Taule? et al, 2008),
Chinese (Palmer and Xue, 2009), Czech (Hajic? et
al., 2006), English (Surdeanu et al, 2008), Ger-
man (Burchardt et al, 2006), Japanese (Kawahara
et al, 2002), and Spanish (Taule? et al, 2008). Be-
sides the closed challenge, we also submitted the
open challenge results. Our open challenge strategy
is very simple. We add the SRL development data
of each language into their training data. The pur-
pose is to examine the effect of the additional data,
especially for out-of-domain (ood) data.
Three machines (with 2.5GHz Xeon CPU and
16G memory) were used to train our models. Dur-
ing the peak time, Amazon?s EC2 (Elastic Com-
pute Cloud)3 was used, too. Our system requires
15G memory at most and the longest training time
is about 36 hours.
During training the predicate classification (PC)
and the semantic role labeling (SRL) models, golden
syntactic dependency parsing results are used. Pre-
vious experiments show that the PC and SRL test re-
sults based on golden parse trees are slightly worse
than that based on cross trained parse trees. It is,
however, a pity that we have no enough time and ma-
chines to do cross training for so many languages.
5.2 Results and Discussion
In order to examine the performance of the ILP
based post inference (PI) for different languages, we
adopt a simple PI strategy as baseline, which se-
lects the most likely label (including the virtual la-
bel ?NULL?) except for those duplicate non-virtual
labels with lower probabilities (lower than 0.5). Ta-
ble 2 shows their performance on development data.
We can see that the ILP based post inference can
improve the precision but decrease the recall. Ex-
cept for Czech, almost all languages are improved.
Among them, English benefits most.
The final system results are shown in Table 3.
Comparing with our CoNLL 2008 (Che et al, 2008)
syntactic parsing results on English4, we can see that
our new high-order model improves about 1%.
3http://aws.amazon.com/ec2/
4devel: 85.94%, test: 87.51% and ood: 80.73%
Precision Recall F1
Catalan simple 78.68 77.14 77.90
Catalan ILP 79.42 76.49 77.93
Chinese simple 80.74 74.36 77.42
Chinese ILP 81.97 73.92 77.74
Czech simple 88.54 84.68 86.57
Czech ILP 89.23 84.05 86.56
English simple 83.03 83.55 83.29
English ILP 85.63 83.03 84.31
German simple 78.88 75.87 77.34
German ILP 82.04 74.10 77.87
Japanese simple 88.04 70.68 78.41
Japanese ILP 89.23 70.16 78.56
Spanish simple 76.73 75.92 76.33
Spanish ILP 77.71 75.34 76.51
Table 2: Comparison between different PI strategies
For the open challenge, because we did not mod-
ify the syntactic training data, its results are the same
as the closed ones. We can, therefore, examine the
effect of the additional training data on SRL. We can
see that along with the development data are added
into the training data, the performance on the in-
domain test data is increased. However, it is inter-
esting that the additional data is harmful to the ood
test.
6 Conclusion and Future Work
Our CoNLL 2009 Shared Task system is com-
posed of three cascaded components. The pseudo-
projective high-order syntactic dependency model
outperforms our CoNLL 2008 model (in English).
The additional in-domain (devel) SRL data can help
the in-domain test. However, it is harmful to the ood
test. Our final system achieves promising results. In
the future, we will study how to solve the domain
adaptive problem and how to do joint learning be-
tween syntactic and semantic parsing.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60675034, and the ?863? National High-
Tech Research and Development of China via grant
2008AA01Z144.
52
Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Score
devel test ood devel test ood devel test ood
Catalan closed 86.65 86.56 ?? 77.93 77.10 ?? 82.30 81.84 ??open ?? ?? 77.36 ?? 81.97
Chinese closed 75.73 75.49 ?? 77.74 77.15 ?? 76.79 76.38 ??open ?? ?? 77.23 ?? 76.42
Czech closed 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66open ?? ?? 86.57 85.21 ?? 83.31 80.63
English closed 87.09 88.48 81.57 84.30 85.51 73.82 85.70 87.00 77.71open ?? ?? 85.61 73.66 ?? 87.05 77.63
German closed 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19open ?? ?? 78.61 70.09 ?? 82.44 73.20
Japanese closed 92.55 92.57 ?? 78.56 78.26 ?? 85.86 85.65 ??open ?? ?? 78.35 ?? 85.70
Spanish closed 87.22 87.33 ?? 76.51 76.47 ?? 81.87 81.90 ??open ?? ?? 76.66 ?? 82.00
Average closed ?? 85.23 77.90 ?? 79.94 76.38 ?? 82.64 77.19open 80.06 76.32 82.70 77.15
Table 3: Final system results
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In LREC-2006.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP/CoNLL-
2007.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded
syntactic and semantic dependency parsing system. In
CoNLL-2008.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies.
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang,
and Ting Liu. 2007. HIT-IR-WSD: A wsd system for
english lexical sample task. In SemEval-2007.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP-2008.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In LREC-2002.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In EMNLP-
2002.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In ACL-2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1).
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Coling-2004.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In LREC-2008.
53
Catalan Chinese Czech English German Japanese Spanish
ChildrenPOS ? ? ??
ChildrenPOSNoDup ? ? ? ?
ConstituentPOSPattern ? ? ? ? ? ? ? ? ? ? ? ? ? ?
ConstituentPOSPattern+DepRelation ? ? ? ? ? ?
ConstituentPOSPattern+DepwordLemma ? ? ? ? ? ?
ConstituentPOSPattern+HeadwordLemma ? ? ? ? ? ? ? ? ? ?
DepRelation N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? N M ? ?
DepRelation+DepwordLemma ? ? ? ?
DepRelation+Headword N M N M N N M N M N
DepRelation+HeadwordLemma ? ? ? ? ? ? ? ?
DepRelation+HeadwordLemma+DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepRelation+HeadwordPOS N M N M N M N M N M N
Depword ? ? ? ?
DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepwordLemma+HeadwordLemma ? ? ? ? ? ?
DepwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ?
DepwordPOS N M N M N M ? ? N M N M ? ? N M
DepwordPOS+HeadwordPOS ? ? ? ?
DownPathLength ? ? ? ?
FirstLemma ? ? ? ? ? ? ? ? ? ? ? ?
FirstPOS ? ? ? ?
FirstPOS+DepwordPOS ? ? ? ? ? ?
FirstWord ? ? ? ?
Headword N M N M N M N M N M ? ? N
HeadwordLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N
HeadwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ? ? ?
HeadwordPOS N M N M N M ? ? N M ? ? N M ? ? N M
LastLemma ? ? ? ? ? ? ? ? ? ?
LastPOS ? ? ? ?
LastWord ? ?
Path ? ? ? ? ? ? ? ? ? ? ? ?
Path+RelationPath ? ? ? ? ? ? ? ? ? ?
PathLength ? ? ? ? ? ? ? ? ? ? ? ?
PFEAT N M N M N M
PFEATSplit N M ? ? N M ? ? N M ? ? N M ? ?
PFEATSplitRemoveNULL N M N M N M
PositionWithPredicate ? ? ? ? ? ? ? ? ? ?
Predicate N M ? ? N M N M ? ? N M N M N M ? ?
Predicate+PredicateFamilyship ? ? ? ? ? ? ? ? ? ?
PredicateBagOfPOSNumbered M N M N M N M
PredicateBagOfPOSNumberedWindow5 N M N M N M N M N M
PredicateBagOfPOSOrdered N M N M N M N M N
PredicateBagOfPOSOrderedWindow5 N M N M N M N M N M N M
PredicateBagOfPOSWindow5 N N M N M N M N M N
PredicateBagOfWords M N M N N M N M
PredicateBagOfWordsAndIsDesOfPRED N M N M M N M N M
PredicateBagOfWordsOrdered M N M N M M N M N M
PredicateChildrenPOS N M ? ? N M N M N M N M N M ? ?
PredicateChildrenPOSNoDup N M N M N M N M N M N M
PredicateChildrenREL N M ? ? N M N M N M N M ? ? N M
PredicateChildrenRELNoDup N M ? ? N M N M N M N M ? ? N M
PredicateFamilyship ? ?
PredicateLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N M ? ?
PredicateLemma+PredicateFamilyship ? ? ? ? ? ?
PredicateSense ? ? ? ? ? ? ? ? ? ? ? ? ? ?
PredicateSense+DepRelation ? ? ? ?
PredicateSense+DepwordLemma ? ? ? ?
PredicateSense+DepwordPOS ? ? ? ?
PredicateSiblingsPOS N M N M N N M N M N M
PredicateSiblingsPOSNoDup N M ? ? N M N M N M N M N M ? ?
PredicateSiblingsREL N M ? ? N M N M N M N M N M
PredicateSiblingsRELNoDup N M N M ? ? M N M N M ? ? N M ? ?
PredicateVoiceEn N M
PredicateWindow5Bigram N M N M N M N M
PredicateWindow5BigramPOS N M N M N M N M N M N M
RelationPath ? ? ? ? ? ? ? ? ? ? ? ? ? ?
SiblingsPOS ? ? ? ?
SiblingsREL ?
SiblingsRELNoDup ? ? ? ?
UpPath ? ? ? ? ? ? ?
UpPathLength ? ?
UpRelationPath ? ? ? ? ? ?
UpRelationPath+HeadwordLemma ? ? ? ? ? ? ? ?
Table 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL). N: noun predicate
PC, M: verb predicate PC, ?: noun predicate SRL, ?: verb predicate SRL.
54
Coling 2010: Demonstration Volume, pages 13?16,
Beijing, August 2010
LTP: A Chinese Language Technology Platform
Wanxiang Che, Zhenghua Li, Ting Liu
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
Harbin Institute of Technology
{car, lzh, tliu}@ir.hit.edu.cn
Abstract
LTP (Language Technology Platform) is
an integrated Chinese processing platform
which includes a suite of high perfor-
mance natural language processing (NLP)
modules and relevant corpora. Espe-
cially for the syntactic and semantic pars-
ing modules, we achieved good results
in some relevant evaluations, such as
CoNLL and SemEval. Based on XML in-
ternal data representation, users can easily
use these modules and corpora by invok-
ing DLL (Dynamic Link Library) or Web
service APIs (Application Program Inter-
face), and view the processing results di-
rectly by the visualization tool.
1 Introduction
A Chinese natural language processing (NLP)
platform always includes lexical analysis (word
segmentation, part-of-speech tagging, named en-
tity recognition), syntactic parsing and seman-
tic parsing (word sense disambiguation, semantic
role labeling) modules. It is a laborious and time-
consuming work for researchers to develop a full
NLP platform, especially for Chinese, which has
fewer existing NLP tools. Therefore, it should be
of particular concern to build an integrated Chi-
nese processing platform. There are some key
problems for such a platform: providing high per-
formance language processing modules, integrat-
ing these modules smoothly, using processing re-
sults conveniently, and showing processing results
directly.
LTP (Language Technology Platform), a Chi-
nese processing platform, is built to solve the
above mentioned problems. It uses XML to trans-
fer data through modules and provides all sorts
? 
? 	

 ? 



? 




Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 783?793, Dublin, Ireland, August 23-29 2014.
Soft Cross-lingual Syntax Projection for Dependency Parsing
Zhenghua Li , Min Zhang?, Wenliang Chen
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University
{zhli13,minzhang,wlchen}@suda.edu.cn
Abstract
This paper proposes a simple yet effective framework of soft cross-lingual syntax projection to
transfer syntactic structures from source language to target language using monolingual treebanks
and large-scale bilingual parallel text. Here, soft means that we only project reliable dependencies
to compose high-quality target structures. The projected instances are then used as additional
training data to improve the performance of supervised parsers. The major issues for this
idea are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic
syntactic non-isomorphism between languages; 3) incomplete parse trees after projection. To
handle the first two issues, we propose to use a probabilistic dependency parser trained on the
target-language treebank, and prune out unlikely projected dependencies that have low marginal
probabilities. To make use of the incomplete projected syntactic structures, we adopt a new
learning technique based on ambiguous labelings. For a word that has no head words after
projection, we enrich the projected structure with all other words as its candidate heads as long
as the newly-added dependency does not cross any projected dependencies. In this way, the
syntactic structure of a sentence becomes a parse forest (ambiguous labels) instead of a single
parse tree. During training, the objective is to maximize the mixed likelihood of manually labeled
instances and projected instances with ambiguous labelings. Experimental results on benchmark
data show that our method significantly outperforms a strong baseline supervised parser and
previous syntax projection methods.
1 Introduction
During the past decade, supervised dependency parsing has made great progress. However, due to
the limitation of scale and genre coverage of labeled data, it is very difficult to further improve the
performance of supervised parsers. On the other hand, it is very time-consuming and labor-intensive to
manually construct treebanks. Therefore, lots of recent work has been devoted to get help from bilingual
constraints. The motivation behind are two-fold. First, a difficult syntactic ambiguity in one language
may be very easy to resolve in another language. Second, a more accurate parser on one language may
help an inferior parser on another language, where the performance difference may be due to the intrinsic
complexity of languages or the scale of accessible labeled resources.
Following the above research line, much effort has been done recently to explore bilingual constraints
for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing
of bitext, which can make use of structural correspondence features in both languages. Their method
needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et
al. (2009) compose useful parsing features based on word reordering information in source-language
sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language
sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with
target-language gold-standard dependency trees. All above work shows significant performance gain
?Correspondence author
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
783
over monolingual counterparts. However, one potential disadvantage is that bilingual treebanks and
bitext with one-side annotation are difficult to obtain. Therefore, They usually conduct experiments on
treebanks with a few thousand sentences. To break this constraint, Chen et al. (2011) extend their work
in Chen et al. (2010a) and translate text of monolingual treebanks to obtain bilingual treebanks with a
statistical machine translation system.
This paper explores another line of research and aims to boost the state-of-the-art parsing accuracy
via syntax projection. Syntax projection typically works as follows. First, we train a parser on source-
language treebank, called a source parser. Then, we use the source parser to produce automatic syntactic
structures on the source side of bitext. Next, with the help of automatic word alignments, we project the
source-side syntactic structures into the target side. Finally, the target-side structures are used as gold-
standard to train new parsing models of target language. Previous work on syntax projection mostly
focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al.,
2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose
quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds
of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to
explore projected structures to further improve the performance of statistical parsers trained on full-scale
monolingual treebanks (see Section 4.4 for performance comparison).
The major issues for syntax projection are 1) errors from the source-language parser and unsupervised
word aligner; 2) intrinsic syntactic non-isomorphism between languages; 3) incomplete parse trees after
projection. Hwa et al. (2005) propose a simple projection algorithm based on the direct correspondence
assumption (DCA). They apply post-editing to the projected structures with a set of hand-crafted heuristic
rules, in order to handle some typical cross-lingual syntactic divergences. Similarly, Ganchev et al.
(2009) manually design several language-specific constrains during projection, and use projected partial
structures as soft supervision during training based on posterior regularization (Ganchev et al., 2010).
To make use of projected instances with incomplete trees, Spreyer and Kuhn (2009) propose a heuristic
method to adapt training procedures of dependency parsing. Instead of directly using incomplete trees
to train dependency parsers, Jiang et al. (2010) train a local dependency/non-dependency classifier on
projected syntactic structures, and use outputs of the classifier as auxiliary features to help supervised
parsers. One potential common drawback of above work is the lack of a systematic way to handle
projection errors and incomplete trees.
Different from previous work, this paper proposes a simple yet effective framework of soft syntax
projection for dependency parsing, and provides a more elegant and systematic way to handle the
above issues. First, we propose to use a probabilistic parser trained on target-language treebank, and
prune unlikely projected dependencies which have very low marginal probabilities. Second, we adopt
a new learning technique based on ambiguous labelings to make use of projected incomplete trees
for training. For a word that has no head words after projection, we enrich the projected structure
by adding all possible words as its heads as long as the newly-added dependency does not cross any
projected dependencies. In this way, the syntactic structure of a sentence becomes a parse forest
(ambiguous labelings) instead of a single parse tree. During training, the objective is to maximize
the mixed likelihood of manually labeled instances and projected instances with ambiguous labelings.
Experimental results on benchmark data show that our method significantly outperforms a strong baseline
supervised parser and previous syntactic projection methods.
2 Syntax Projection
Given an input sentence x = w
0
w
1
...w
n
, a dependency tree is d = {(h,m) : 0 ? h ? n, 0 < m ? n},
where (h,m) indicates a directed arc from the head word w
h
to the modifier w
m
, and w
0
is an artificial
node linking to the root of the sentence.
Syntax projection aims to project the dependency tree ds of a source-language sentence xs into the
dependency structure of its target-language translation x via word alignments a, where a word alignment
a
i
= z means the target-side word w
i
is aligned into the source-side word ws
z
, as depicted in Figure
1(a) and Figure 1(b). For simplicity, we avoid one-to-many alignments by keeping the one with highest
784
w0
things
1
I
2
did
3
w
0

1
Z
2
?
3
?
4
?
5
(a) Source tree and word alignments
w
0

1
Z
2
?
3
?
4
?
5
(b) Projected incomplete tree
w
0

1
Z
2
?
3
?
4
?
5
(c) Forest (ambiguous labelings)
Figure 1: Illustration of syntax projection from English to Chinese with a sentence fragment. The two
Chinese auxiliary words, ??
3
? (past tense marker) and ??
4
? (relative clause marker), are not aligned to
any English words.
marginal probability when the target word is aligned to multiple source words. We first introduce a
simple syntax projection approach based on DCA (Hwa et al., 2005), and then propose two extensions
to handle parsing and aligning errors and cross-lingual syntactic divergences.
Projection with DCA. If two target words w
i
and w
j
are aligned to two different source words ws
a
i
and
w
s
a
j
, and the two words compose a dependency in the source tree (a
i
, a
j
) ? d
s
, then add a dependency
(i, j) into the projected syntactic structure. For example, as shown in Figure 1(a), the two Chinese
words ?Z
2
? and ??
5
? are aligned to the two English words ?did
3
? and ?things
1
?, and the dependency
?things
1
ydid
3
? is included in the source tree. Therefore, we project the dependency into the target side
and add a dependency ?Z
2
x?
5
? into the projected structure, as shown in Figure 1(b). An obvious
drawback of DCA is that it may produce many wrong dependencies due to the errors in the automatic
source-language parse trees and word alignments. Even with manual parse trees and word alignments,
syntactic divergences between languages can also lead to projection errors.
Pruned with target-side marginals. To overcome the weakness of DCA, we propose to use target-
side marginal probabilities to constrain the projection process and prune obviously bad projections. We
train a probabilistic parser on an existing target-side treebank. For each projected dependency, we
compute its marginal probability with the target parser, and prune it off the projected structure if the
probability is below a pruning threshold ?
p
. Our study shows that dependencies with very low marginal
probabilities are mostly wrong (Figure 2).
Supplemented with target-side marginals. To further improve the quality of projected structures, we
add dependencies with high marginal probabilities according to the target parser. Specifically, if a target
word w
j
obtain a head word w
i
after projection, and if another word w
k
has higher marginal probability
than a supplement threshold ?
s
to be the head word of w
j
, then we also add the dependency (k, j) into
the projected structure. In other words, we allow one word to have multiple heads so that the projected
structure can cover more correct dependencies.
From incomplete tree to forest. Some words in the target sentence may not obtain any head words
after projection due to incomplete word alignments or the pruning process, which leads to incomplete
parse trees after projection. Also, some words may have multiple head words resulting from the
supplement process. To handle these issues, we first convert the projected structures into parse forests,
and then propose a generalized training technique based on ambiguous labelings to make use of the
projected instances. Specifically, if a word does not have head words after projection, we simply
add into the projected structure all possible words as its candidate heads as long as the newly-added
dependency does not cross any projected dependencies, as illustrated in Figure 1(c). We introduce three
new dependencies to compose candidate heads for the unattached word ??
3
?. Note that it is illegal to
add the dependency ?
1
y?
3
? since it would cross the projected dependency ?Z
2
x?
5
?.
785
3 Dependency Parsing with Ambiguous Labelings
In parsing community, two mainstream methods tackle the dependency parsing problem from different
perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view
the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005;
McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods
try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and
Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011).
3.1 Graph-based Dependency Parser (GParser)
We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic
parser, which is required to compute the marginal probabilities of dependencies and likelihood of both
manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors
the score of a dependency tree into scores of small subtrees p.
Score(x,d;w) = w ? f(x,d) =
?
p?d
Score(x,p;w) (1)
We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1
which defines the score of a dependency tree as:
Score(x,d;w) =
?
{(h,m)}?d
w
dep
? f
dep
(x, h,m) +
?
{(h,s),(h,m)}?d
w
sib
? f
sib
(x, h, s,m) (2)
where f
dep
(x, h,m) and f
sib
(x, h, s,m) are feature vectors corresponding to two kinds of subtree;
w
dep/sib
are the feature weight vectors; the dot product gives the scores contributed by the corresponding
subtrees. We adopt the state-of-the-art syntactic features proposed in Bohnet (2010).
3.2 Probabilistic CRF-based GParser
Previous work on dependency parsing mostly adopts linear models and online perceptron training, which
lack probabilistic explanations of dependency trees and likelihood of the training data. Instead, we build
a log-linear CRF-based probabilistic dependency parser, which defines the probability of a dependency
tree as:
p(d|x;w) =
exp{Score(x,d;w)}
Z(x;w)
; Z(x;w) =
?
d
?
?Y(x)
exp{Score(x,d
?
;w)} (3)
where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x.
3.3 Likelihood and Gradient of Training Data with Ambiguous Labelings
Traditional CRF models assume one gold-standard label for each training instance, which means each
sentence is labeled with a single parse tree in the case of parsing. To make use of projected instances
with ambiguous labelings, we propose to use a generalized training framework which allows a sentence
to have multiple parse trees (forest) as its gold-standard reference (Ta?ckstro?m et al., 2013). The goal
of the training procedure is to maximize the likelihood of the training data, and the model is updated to
improve the probabilities of parse forests, instead of single parse trees. In other words, the model has
the flexibility to distribute the probability mass among the parse trees inside the forest, as long as the
probability of the forest improves. In this generalized framework, a traditional instance labeled with a
single parse tree can be regarded as a special case that the forest contains only one parse tree.
The probability of a sentence x with ambiguous labelings F is defined as the sum of probabilities of
all parse tree d contained in the forest F :
p(F|x;w) =
?
d?F
p(d|x;w) (4)
1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve a little bit higher accuracy, but suffer from
higher time cost of O(n4) and system complexity. Our method is applicable to the third-order model.
786
Train Dev Test
PTB 39,832 1,346 2416
CTB5 16,091 803 1,910
CTB5X 18,104 352 348
Bitext 0.9M ? ?
Table 1: Data sets (in sentence number).
Suppose the training data set is D = {(x
i
,F
i
)}
N
i=1
. Then the log likelihood of D is:
L(D;w) =
N
?
i=1
log p(F
i
|x
i
;w) (5)
Then we can derive the partial derivative of the log likelihood with respect to w:
?L(D;w)
?w
=
N
?
i=1
(
?
d?F
i
p?(d|x
i
,F
i
;w)f(x
i
,d) ?
?
d?Y(x
i
)
p(d|x
i
;w)f(x
i
,d)
)
(6)
where p?(d|x
i
,F
i
;w) is the probability of d under the space constrained by the parse forest F
i
:
p?(d|x
i
,F
i
;w) =
exp{Score(x
i
,d;w)}
Z(x
i
,F
i
;w)
; Z(x
i
,F
i
;w) =
?
d?F
i
exp{Score(x
i
,d;w)} (7)
The first term in Eq. (6) is the model expectations in the search space constrained by F
i
, and the second
term is the model expectations in the complete search space Y(x
i
). Since Y(x
i
) contains exponentially
many legal dependency trees, direct calculation of the second term is prohibitive. Instead, we can use the
classic Inside-Outside algorithm to efficiently compute the second term within O(n3) time complexity,
where n is the length of the input sentence. Similarly, the first term can be solved by running the Inside-
Outside algorithm in the constrained search space F
i
.
3.4 Stochastic Gradient Descent (SGD) Training
With the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the feature
weights w for our CRF-based baseline and bitext-enhanced parsers. We follow the implementation
in CRFsuite.2 At each step, the algorithm approximates a gradient with a small subset of training
examples, and then updates the feature weights. Finkel et al. (2008) show that SGD achieves optimal
test performance with far fewer iterations than other optimization routines such as L-BFGS. Moreover,
it is very convenient to parallel SGD since computation among examples in the same batch is mutually
independent.
Once the feature weights w are learnt, we can parse the test data and try to find the optimal parse tree
with the Viterbi decoding algorithm in O(n3) parsing time (Eisner, 2000; McDonald and Pereira, 2006).
d
?
= arg max
d?Y(x)
p(d|x;w) (8)
4 Experiments and Analysis
To verify the effectiveness of our proposed method, we carry out experiments on English-to-Chinese
syntax projection, and aim to enhance our baseline Chinese parser with additional training instances
projected from automatic English parse trees on bitext. For monolingual treebanks, we use Penn
English Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5). For English, we follow the standard
practice to split the data into training (sec 02-21), development (sec 22), and test (sec 23). For CTB5, we
adopt the data split of (Duan et al., 2007). We convert the original bracketed structures into dependency
2http://www.chokkan.org/software/crfsuite/
787
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
UA
S 
& 
Pe
rc
en
t (%
)
Marginal Probability Interval
Percent
UAS
Figure 2: Distribution (Percent) and accuracy (UAS) of dependencies under different marginal
probability interval for Chinese baseline parser on CTB5 development set. For example, 0.8 at x-axis
means the interval [0.8, 0.9).
structures using Penn2Malt with its default head-finding rules. We build a CRF-based bigram part-
of-speech (POS) tagger with the features described in (Li et al., 2012b), and produce POS tags for
all train/development/test datasets and bitext (10-way jackknifing for training datasets). The tagging
accuracy on test sets is 97.3% on English and 94.0% on Chinese.
To compare with the recent work on syntax projection of Jiang et al. (2010) who use a smaller test
dataset, we follow their data split of CTB5 and use gold-standard POS tags during training and test. We
refer to this setting as CTB5X.
For bitext, we collect a parallel corpus from FBIS news (LDC03E14, 0.25M sentence pairs), United
Nations (LDC04E12, 0.62M), IWSLT2008 (0.04M), and PKU-863 (0.2M). After corpus cleaning, we
obtain a large-scale bilingual parallel corpus containing 0.9M sentence pairs. We run the unsupervised
BerkeleyAligner3 (Liang et al., 2006) for 4 iterations to obtain word alignments. Besides hard
alignments, we also make use of posterior probabilities to simplify one-to-many alignments to one-to-one
as discussed in Section 2. Table 1 shows the data statistics.
For training both the baseline and bitext-enhanced parsers, we set the batch size to 100 and run SGD
until a maximum iteration number of 50 is met or the change on likelihood of training data becomes too
small. Since the number of projected sentences is much more than that of manually labeled instances
(0.9M vs. 16K), it is likely that the projected data may overwhelm manually labeled data during training.
Therefore, we adopt a simple corpus-weighting strategy. Before each iteration, we randomly sample 50K
projected sentences and 15K manually labeled sentences from all training data, and run SGD to train
feature weights using the sampled data. To speed up training, we adopt multi-thread implementation of
gradient computations in the same batch. It takes about 1 day to train our bitext-enhanced parser for one
iteration using a single CPU core, while using 24 CPU cores only needs about 2 hours.
We measure parsing performance using unlabeled attachment score (UAS, percent of words with
correct heads), excluding punctuation marks. For significance test, we adopt Dan Bikel?s randomized
parsing evaluation comparator (Noreen, 1989).4
4.1 Analysis on Marginal Probabilities
In order to gain insights for parameter settings of syntax projection, we analyse the distribution and
accuracy of dependencies under different marginal probability interval. We train the baseline Chinese
parser on CTB5 train set, and use the parser to produce the marginal probabilities of all dependencies
for sentences in CTB5 development set. We discard all dependencies that have a marginal probability
less than 0.0001 for better illustration. Figure 2 shows the results, where we can see that UAS is roughly
proportional to marginal probabilities. In other word, dependencies with higher marginal probabilities
are more accurate. For example, dependencies with probabilities under interval [0.8, 0.9) has a 80%
chance to be correct. From another aspect, we can see that 50% of dependencies fall in probability
3http://code.google.com/p/berkeleyaligner/
4http://www.cis.upenn.edu/
?
dbikel/software.html
788
 77
 78
 79
 80
 81
 82
 83
 84
 1  6  11  16  21  26  31  36  41  46
UA
S 
(%
)
Iteration Number
Supervised
DCA (0.0 1.0)
DCA Pruned (0.1 1.0)
DCA Pruned (0.5 1.0)
(a) Parameters for DCA and DCA Pruned
 77
 78
 79
 80
 81
 82
 83
 84
 1  6  11  16  21  26  31  36  41  46
UA
S 
(%
)
Iteration Number
Supervised
(0.1 0.5)
(0.1 0.6)
(0.1 0.8)
(b) Parameters for DCA Pruned & Supplemented
Figure 3: Performance with different parameter settings of (?
p
?
s
) on CTB5 development set.
interval [0, 0.1), and such dependencies have very low accuracy (4%). These observations are helpful for
our parameter selection and methodology study during syntax projection.
4.2 Results of Syntax Projection on Development Dataset
We apply the syntax projection methods described in Section 2 to the bilingual text, and use the projected
sentences with ambiguous labelings as additional training instances to train new Chinese parsers based on
the framework described in Section 3. Figure 3 shows the UAS curves on development set with different
parameters settings. The pruning threshold ?
p
(see Section 2) balances the quality and coverage of
projection. Larger ?
p
leads to more accurate but fewer projections. The supplement threshold ?
s
(see
Section 2) balances the size and oracle score of the projected forest. Smaller ?
s
can increase the oracle
score of the forest by adding more dependencies with lower marginal probabilities, but takes the risk of
making the resulted forest too ambiguous and weak to properly supervise the model during training. 5
The DCA method corresponds to the results with ?
p
= 0.0 and ?
s
= 1.0. We can see that DCA
largely decreases UAS compared with the baseline CRF-based parser. The reason is that although DCA
projects many source-language dependencies to the target side (44% of target-language words obtain
head words), it also introduces a lot of noise during projection.
DCA pruned with target-side marginals corresponds to the results with ?
p
> 0.0 and ?
s
= 1.0.
Pruning with target-side marginals can clearly improve the projection quality by pruning out bad
projections. When ?
p
= 0.1, 31% of target-language words obtain head words, and the model
outperforms the baseline parser by 0.6% at peak UAS. When ?
p
= 0.5, the projection ratio decreases to
26% and the improvement is 0.3%. Based on the results, we choose ?
p
= 0.1 in later experiments.
Figure 3(b) presents the results of DCA pruned & supplemented with different ?
s
. The supplement
process adds a small amount of dependencies of high probabilities into the projected forest and therefore
increases the oracle score, which provides the model with flexibility to distribute the probability mass to
more preferable parse trees. We can see that although the peak UAS does not increase much, the training
curve is more smooth and stable than that without supplement. Based on the results, we choose ?
s
= 0.6
in later experiments.
4.3 Final Results and Comparisons on Test Dataset
Table 2 presents the final results on CTB5 test set. For each parser, we choose the parameters
corresponding to the iteration number with highest UAS on development set. To further verify the
usefulness of syntax projection, we also conduct experiments with self-training, which is known as a
typical semi-supervised method. For the standard self-training, we use Chinese-side bitext with self-
predicted parse trees produced by the baseline parser as additional training instances, which turns out
to be hurtful to parsing performance. This is consistent with earlier results (Spreyer and Kuhn, 2009).
5Please note when ?
p
+?
s
>= 1, ?
s
becomes useless. The reason is that if the probability of a projected dependency (i, j)
is larger ?
p
, then no other word beside w
i
can have a probability larger than ?
s
of being the head word of w
j
.
789
UAS
Baseline Supervised Parser 81.04
Standard Self-training 80.51 (-0.53)
Self-training with Ambiguous Labelings 81.09 (+0.05)
DCA 78.70 (-2.34)
DCA Pruned 81.46 (+0.42 ?)
DCA Pruned & Supplemented 81.71 (+0.67 ?)
Table 2: UAS on CTB5 test set. ? indicate statistical significance at confidence level of p < 0.01.
Supervised Bitext-enhanced
Jiang et al. (2010) 87.15 87.65 (+0.50)
This work 89.62 90.50 (+0.88 ?)
Table 3: UAS on CTB5X test set. ? indicate statistical significance at confidence level of p < 0.01.
Then, we try a variant of self-training with ambiguous labelings following the practice in Ta?ckstro?m
et al. (2013), and use a parse forest composed of dependencies of high probabilities as the syntactic
structure of an instance. We can see that ambiguous labelings help traditional self-training, but still have
no significant improvement over the baseline parser. Results in Table 2 indicate that our syntax projection
method is able to project useful knowledge from source-language parse trees to the target-side forest, and
then helps the target parser to learn effective features.
4.4 Comparisons with Previous Results on Syntax Projection on CTB5X
To make comparison with the recent work of Jiang et al. (2010), We rerun the process of syntax projection
with CTB5X as the target treebank with the DCA pruned & supplemented method (?
p
= 0.1 and ?
s
=
0.6).6 Table 3 shows the results. Jiang et al. (2010) employ the second-order MSTParser of McDonald
and Pereira (2006) with a basic feature set as their base parser. We can see that our baseline parser is
much stronger than theirs. Even though, our approach leads to larger UAS improvement.
This work is different from theirs in a few aspects. First, the purpose of syntax projection in their
work is to produce dependency/non-dependency instances which are used to train local classifiers to
produce auxiliary features for MSTParser. In contrast, the outputs of syntax projection in our work
are partial trees/forests where only reliable dependencies are kept and some words may receive more
than one candidate heads. We directly use these partial structures as extra training data to learn model
parameters. Second, their work measures the reliability of a projected dependencies only from the
perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side
marginal probabilities to throw away bad projections, which turns out effective in handling syntactic
non-isomorphism and errors in word alignments and source-side parses.
5 Related work
Cross-lingual annotation projection has been applied to many different NLP tasks to help processing
resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and
Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous
work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et
al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013).
Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train
delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext
(Zeman and Resnik, 2008; Cohen et al., 2011; S?gaard, 2011). Ta?ckstro?m et al. (2012) derive cross-
lingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively
sharing to better explore multi-source transfer information.
6In the previous draft of this paper, we directly use the projected data with in previous subsection for simplicity, and find
that UAS can reach 91.39% (+1.77). The reason is that the CTB5X test is overlapped with CTB5 train. We correct this mistake
in this version.
790
Our idea of training with ambiguous labelings is originally inspired by the work of Ta?ckstro?m et al.
(2013) on multilingual parser transfer for unsupervised dependency parsing. They use a delexicalized
parser trained on source-language treebank to obtain parse forests for target-language sentences, and re-
train a lexicalized target parser using the sentences with ambiguous labelings. Similar ideas of learning
with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) and
sequence labeling problems (Dredze et al., 2009).
6 Conclusions
This paper proposes a simple yet effective framework of soft cross-lingual syntax projection. We
make use of large-scale projected structures as additional training instances to boost performance of
supervised parsing models trained on full-set manually labeled treebank. Compared with previous work,
we make two innovative contributions: 1) using the marginal probabilities of a target-side supervised
parser to control the projection quality with the existence of parsing and aligning errors and cross-lingual
syntax divergences; 2) adopting a new learning technique based ambiguous labelings to make use of
projected incomplete dependency trees for model training. Experimental results on two Chinese datasets
demonstrate the effectiveness of the proposed framework, and show that the bitext-enhanced parser
significantly outperforms all baselines, including supervised parsers, semi-supervised parsers based on
self-training, and previous syntax projection methods.
Our anonymous reviewers present many great comments, especially on the experimental section. We
will improve this work accordingly and release an extended version of this paper at the homepage of
the first author. Such extensions include: 1) further exploring source-language parsing probabilities and
alignment probabilities to help syntax projection; 2) studying the effect of the scale of source/target
treebank and bilingual text.
Acknowledgments
The authors would like to thank Wanxiang Che and Jiang Guo for sharing their bilingual data, and our
anonymous reviewers for their critical and insightful comments, which will certainly help our future
work. This work was supported by National Natural Science Foundation of China (Grant No. 61373095,
61203314, 61373097).
References
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COLING,
pages 89?97.
David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings
EMNLP, pages 877?886.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010. Learning better monolingual models with
unannotated bilingual text. In Proceedings of CoNLL 2010, pages 46?54.
Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of
EMNLP/CoNLL, pages 141?150.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of NAACL 2013.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa. 2010a. Bitext dependency parsing with bilingual subtree
constraints. In Proceedings of ACL, pages 21?29.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010b. On jointly recognizing and aligning bilingual named
entities. In Proceedings of ACL 2010.
Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro Torisawa,
and Haizhou Li. 2011. SMT helps bitext dependency parsing. In EMNLP.
791
Shay B. Cohen, Dipanjan Das, , and Noah A. Smith. 2011. Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of ACL-HLT 2011, pages 600?609.
Mark Dredze, Partha Pratim Talukdar, and Koby Crammer. 2009. Sequence learning from data with multiple
labels. In ECML/PKDD Workshop on Learning from Multi-Label Data.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic parsing action models for multi-lingual dependency
parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 940?946.
Jason Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29?62.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages 959?967.
Ruiji Fu, Bing Qin, and Ting Liu. 2011. Generating chinese named entity data from a parallel corpus. In
Proceedings of IJCNLP 2011, pages 264?272.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext
projection constraints. In Proceedings of ACL-IJCNLP 2009, pages 369?377.
Kuzman Ganchev, Jo ao Graca, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured
latent variable models. Journal of Artifical Intellignece Research.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In
Proceedings of EMNLP, pages 1222?1231.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Boostrapping parsers via
syntactic projection across parallel texts. Natural Language Engineering, 11(3):311?325.
Wenbin Jiang, , and Qun Liu. 2010. Dependency parsing and projection based on word-pair classification. In
ACL, pages 897?904.
Rong Jin and Zoubin Ghahramani. 2002. Learning with multiple labels. In Proceedings of NIPS.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In ACL, pages 1?11.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zeng, and Fei Huang. 2012a. Joint bilingual name tagging for parallel
corpora. In Proceedings of CIKM 2012.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu. 2012b. A separately passive-aggressive training algorithm
for joint POS tagging and dependency parsing. In COLING 2012, pages 1681?1698.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In HLT-NAACL.
Kai Liu, Yajuan Lu?, Wenbin Jiang, and Qun Liu. 2013. Bilingually-guided monolingual dependency grammar
induction. In Proceedings of ACL.
Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In
Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91?98.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers.
In Proceedings of EMNLP.
Tahira Naseem, Benjamin Snyder, Jacob Eisentein, and Regina Barzilay. 2009. Multilingual part-of-speech
tagging: two unsupervised approaches. Journal of Artifical Intellignece Research, 36(1):341?385.
Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency
parsing. In Proceedings of ACL.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT, pages
149?160.
792
Eric W. Noreen. 1989. Computer-intensive methods for testing hypotheses: An introduction. John Wiley & Sons,
Inc., New York.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. In ArXiv:1104.2086.
David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar
features. In Proceedings of EMNLP, pages 822?831.
Anders S?gaard. 2011. Data point selection for cross-language adaptation of dependency parsers. In Proceedings
of ACL 2011, pages 682?686.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven dependency parsing of new languages using incomplete and
noisy training data. In CoNLL, pages 12?20.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre. 2013. Target language adaptation of discriminative transfer
parsers. In Proceedings of NAACL, pages 1061?1071.
Mengqiu Wang, Wanxiang Che, and Christopher D. Manning. 2013. Joint word alignment and bilingual named
entity recognition using dual decomposition. In Proceedings of ACL 2013.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection
across aligned corpora. In Proceedings of NAACL 2001.
Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In
Proceedings of IJCNLP 2008.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193.
793
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 14?16, Dublin, Ireland, August 23-29 2014.
Dependency Parsing: Past, Present, and Future
Wenliang Chen and Zhenghua Li and Min Zhang
School of Computer Science and Technology,
Soochow University, China
{wlchen, Zhli13, mzhang}@suda.edu.cn
Dependency parsing has gained more and more interest in natural language processing in recent years
due to its simplicity and general applicability for diverse languages. The international conference of
computational natural language learning (CoNLL) has organized shared tasks on multilingual dependen-
cy parsing successively from 2006 to 2009, which leads to extensive progress on dependency parsing
in both theoretical and practical perspectives. Meanwhile, dependency parsing has been successfully
applied to machine translation, question answering, text mining, etc.
To date, research on dependency parsing mainly focuses on data-driven supervised approaches and
results show that the supervised models can achieve reasonable performance on in-domain texts for a
variety of languages when manually labeled data is provided. However, relatively less effort is devoted to
parsing out-domain texts and resource-poor languages, and few successful techniques are bought up for
such scenario. This tutorial will cover all these research topics of dependency parsing and is composed of
four major parts. Especially, we will survey the present progress of semi-supervised dependency parsing,
web data parsing, and multilingual text parsing, and show some directions for future work.
In the first part, we will introduce the fundamentals and supervised approaches for dependency pars-
ing. The fundamentals include examples of dependency trees, annotated treebanks, evaluation metrics,
and comparisons with other syntactic formulations like constituent parsing. Then we will introduce a few
mainstream supervised approaches, i.e., transition-based, graph-based, easy-first, constituent-based de-
pendency parsing. These approaches study dependency parsing from different perspectives, and achieve
comparable and state-of-the-art performance for a wide range of languages. Then we will move to the
hybrid models that combine the advantages of the above approaches. We will also introduce recent
work on efficient parsing techniques, joint lexical analysis and dependency parsing, multiple treebank
exploitation, etc.
In the second part, we will survey the work on semi-supervised dependency parsing techniques. Such
work aims to explore unlabeled data so that the parser can achieve higher performance. This tutorial
will present several successful techniques that utilize information from different levels: whole tree level,
partial tree level, and lexical level. We will discuss the advantages and limitations of these existing
techniques.
In the third part, we will survey the work on dependency parsing techniques for domain adaptation
and web data. To advance research on out-domain parsing, researchers have organized two shared tasks,
i.e., the CoNLL 2007 shared task and the shared task of syntactic analysis of non-canonical languages
(SANCL 2012). Both two shared tasks attracted many participants. These participants tried different
techniques to adapt the parser trained on WSJ texts to out-domain texts with the help of large-scale
unlabeled data. Especially, we will present a brief survey on text normalization, which is proven to be
very useful for parsing web data.
In the fourth part, we will introduce the recent work on exploiting multilingual texts for dependency
parsing, which falls into two lines of research. The first line is to improve supervised dependency parser
with multilingual texts. The intuition behind is that ambiguities in the target language may be unam-
biguous in the source language. The other line is multilingual transfer learning which aims to project the
syntactic knowledge from the source language to the target language.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
14
In the fifth part, we will conclude our talk by discussing some new directions for future work.
Outline
? Part A: Dependency parsing and supervised approaches
? A.1 Introduction to dependency parsing
? A.2 Supervised methods
? A.3 Non-projective dependency parsing
? A.4 Probabilistic and generative models for dependency parsing
? A.5 Other work
? Part B: Semi-supervised dependency parsing
? B.1 Lexical level
? B.2 Partial tree level
? B.3 Whole tree level
? B.4 Other work
? Part C: Parsing the web and domain adaptation
? C.1 CoNLL 2007 shared task (domain adaptation subtask)
? C.2 Works on domain adaptation
? C.3 SANCL 2012 (parsing the web)
? C.4 Text normalization
? C.5 Attempts and challenges for parsing the web
? Part D: Multilingual dependency parsing
? D.1 Dependency parsing on bilingual text
? D.2 Multilingual transfer learning for resource-poor languages
? D.3 Other work
? Part E: Conclusion and open problems
Instructors
Wenliang Chen received his Bachelor degree in Mechanical Engineering and Ph.D. degree in comput-
er science from Northeastern University (Shenyang, China) in 1999 and 2005, respectively. He joined
Soochow University since 2013 and is currently a professor in the university. Prior to joining Soochow
University, he was a research scientist in the Institute for Infocomm Research of Singapore from 2011
to 2013. From 2005 to 2010, he worked as an expert researcher in NICT, Japan. His current research
interests include parsing, machine translation, and machine learning. He is currently working on a syn-
tactic parsing project where he applies semi-supervised learning techniques to explore the information
from large-scale data to improve Dependency parsing. Based on the semi-supervised techniques, he de-
veloped a dependency parser, named DuDuPlus (http://code.google.com/p/duduplus/), for the research
and industry communities.
Zhenghua Li is currently an assistant professor in Soochow University. He received his PhD in Com-
puter Science and Technology from Harbin Institute of Technology (HIT) in April 2013. Zhenghuas
research interests include natural language processing and machine learning. More specifically, his
PhD research focuses on the dependency parsing of the Chinese language using discriminative machine-
learning approaches. He has been working on joint POS tagging and dependency parsing (EMNLP 2011,
COLING 2012), and multiple treebank exploitation for dependency parsing (ACL 2012).
Min Zhang, a distinguished professor and Director of the Research Center of Human Language Tech-
nology at Soochow University (China), received his Bachelor degree and Ph.D. degree in computer
15
science from Harbin Institute of Technology in 1991 and 1997, respectively. From 1997 to 1999, he
worked as a postdoctoral research fellow in Korean Advanced Institute of Science and Technology in
South Korea. He began his academic and industrial career as a researcher at Lernout & Hauspie Asia
Pacific (Singapore) in Sep. 1999. He joined Infotalk Technology (Singapore) as a researcher in 2001
and became a senior research manager in 2002. He joined the Institute for Infocomm Research (Singa-
pore) as a research scientist in Dec. 2003. His current research interests include machine translation,
natural language processing, information extraction, social network computing and Internet intelligence.
He has co-authored more than 150 papers in leading journals and conferences, and co-edited 10 book-
s/proceedings published by Springer and IEEE. He was the recipient of several awards in China and
oversea. He is the vice president of COLIPS (2011-2013), the elected vice chair of SIGHAN/ACL
(2014-2015), a steering committee member of PACLIC (2011-now), an executive member of AFNLP
(2013-2014) and a member of ACL (since 2006). He supervises Ph.D students at National University of
Singapore, Harbin Institute of Technology and Soochow University.
16
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1180?1191,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Joint Models for Chinese POS Tagging and Dependency Parsing
Zhenghua Li?, Min Zhang?, Wanxiang Che?, Ting Liu?, Wenliang Chen? and Haizhou Li?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{lzh,car,tliu}@ir.hit.edu.cn
?Institute for Infocomm Research, Singapore
{mzhang,wechen,hli}@i2r.a-star.edu.sg
Abstract
Part-of-speech (POS) is an indispensable fea-
ture in dependency parsing. Current research
usually models POS tagging and dependency
parsing independently. This may suffer from
error propagation problem. Our experiments
show that parsing accuracy drops by about
6% when using automatic POS tags instead
of gold ones. To solve this issue, this pa-
per proposes a solution by jointly optimiz-
ing POS tagging and dependency parsing in a
unique model. We design several joint models
and their corresponding decoding algorithms
to incorporate different feature sets. We fur-
ther present an effective pruning strategy to re-
duce the search space of candidate POS tags,
leading to significant improvement of parsing
speed. Experimental results on Chinese Penn
Treebank 5 show that our joint models sig-
nificantly improve the state-of-the-art parsing
accuracy by about 1.5%. Detailed analysis
shows that the joint method is able to choose
such POS tags that are more helpful and dis-
criminative from parsing viewpoint. This is
the fundamental reason of parsing accuracy
improvement.
1 Introduction
In dependency parsing, features consisting of part-
of-speech (POS) tags are very effective, since pure
lexical features lead to severe data sparseness prob-
lem. Typically, POS tagging and dependency pars-
ing are modeled in a pipelined way. However, the
pipelined method is prone to error propagation, es-
pecially for Chinese. Due to the lack of morpholog-
ical features, Chinese POS tagging is even harder
than other languages such as English. The state-of-
the-art accuracy of Chinese POS tagging is about
93.5%, which is much lower than that of English
(about 97% (Collins, 2002)). Our experimental re-
sults show that parsing accuracy decreases by about
6% on Chinese when using automatic POS tagging
results instead of gold ones (see Table 3 in Section
5). Recent research on dependency parsing usually
overlooks this issue by simply adopting gold POS
tags for Chinese data (Duan et al, 2007; Zhang and
Clark, 2008b; Huang and Sagae, 2010). In this pa-
per, we address this issue by jointly optimizing POS
tagging and dependency parsing.
Joint modeling has been a popular and effec-
tive approach to simultaneously solve related tasks.
Recently, many successful joint models have been
proposed, such as joint tokenization and POS tag-
ging (Zhang and Clark, 2008a; Jiang et al, 2008;
Kruengkrai et al, 2009), joint lemmatization and
POS tagging (Toutanova and Cherry, 2009), joint
tokenization and parsing (Cohen and Smith, 2007;
Goldberg and Tsarfaty, 2008), joint named en-
tity recognition and parsing (Finkel and Manning,
2009), joint parsing and semantic role labeling
(SRL) (Li et al, 2010), joint word sense disambigua-
tion and SRL (Che and Liu, 2010), joint tokenization
and machine translation (MT) (Dyer, 2009; Xiao et
al., 2010) and joint parsing and MT (Liu and Liu,
2010). Note that the aforementioned ?parsing? all
refer to constituent parsing.
As far as we know, there are few successful mod-
els for jointly solving dependency parsing and other
tasks. Being facilitated by Conference on Com-
putational Natural Language Learning (CoNLL)
2008 and 2009 shared tasks, several joint models
of dependency parsing and SRL have been pro-
posed. Nevertheless, the top-ranked systems all
adopt pipelined approaches (Surdeanu et al, 2008;
1180
Hajic? et al, 2009). Theoretically, joint modeling
of POS tagging and dependency parsing should be
helpful to the two individual tasks. On the one hand,
syntactic information can help resolve some POS
ambiguities which are difficult to handle for the se-
quential POS tagging models. On the other hand,
more accurate POS tags should further improve de-
pendency parsing.
For joint POS tagging and dependency parsing,
the major issue is to design effective decoding algo-
rithms to capture rich features and efficiently search
out the optimal results from a huge hypothesis
space.1 In this paper, we propose several dynamic
programming (DP) based decoding algorithms for
our joint models by extending existing parsing algo-
rithms. We also present effective pruning techniques
to speed up our decoding algorithms. Experimen-
tal results on Chinese Penn Treebank show that our
joint models can significantly improve the state-of-
the-art parsing accuracy by about 1.5%.
The remainder of this paper is organized as fol-
lows. Section 2 describes the pipelined method, in-
cluding the POS tagging and parsing models. Sec-
tion 3 discusses the joint models and the decod-
ing algorithms, while Section 4 presents the pruning
techniques. Section 5 reports the experimental re-
sults and error analysis. We review previous work
closely related to our method in Section 6, and con-
clude this paper in Section 7.
2 The Baseline Pipelined Method
Given an input sentence x = w1...wn, we denote its
POS tag sequence by t = t1...tn, where ti ? T , 1 ?
i ? n, and T is the POS tag set. A dependency tree
is denoted by d = {(h,m) : 0 ? h ? n, 0 < m ?
n}, where (h,m) represents a dependency wh ?
wm whose head word (or father) is wh and modifier
(or child) is wm. w0 is an artificial root token which
is used to simplify the formalization of the problem.
The pipelined method treats POS tagging and de-
pendency parsing as two cascaded problems. First,
1It should be noted that it is straightforward to simultane-
ously do POS tagging and constituent parsing, as POS tags can
be regarded as non-terminals in the constituent structure (Levy
and Manning, 2003). In addition, Rush et al (2010) describes
an efficient and simple inference algorithm based on dual de-
composition and linear programming relaxation to combine a
lexicalized constituent parser and a trigram POS tagger.
an optimal POS tag sequence t? is determined.
t? = arg max
t
Scorepos(x, t)
Then, an optimal dependency tree d? is determined
based on x and t?.
d? = arg max
d
Scoresyn(x, t?,d)
2.1 POS Tagging
POS tagging is a typical sequence labeling prob-
lem. Many models have been successfully applied
to sequence labeling problems, such as maximum-
entropy (Ratnaparkhi, 1996), conditional random
fields (CRF) (Lafferty et al, 2001) and perceptron
(Collins, 2002). We use perceptron to build our POS
tagging baseline for two reasons. Firstly, as a linear
model, perceptron is simple, fast, and effective. It is
competitive to CRF in tagging accuracy but requires
much less training time (Shen et al, 2007). Sec-
ondly, perceptron has been successfully applied to
dependency parsing as well (Koo and Collins, 2010).
In this paper, perceptron is used in all models includ-
ing the POS tagging model, the dependency parsing
models and the joint models.
In a perceptron, the score of a tag sequence is
Scorepos(x, t) = wpos ? fpos(x, t)
where fpos(x, t) refers to the feature vector andwpos
is the corresponding weight vector.
For POS tagging features, we follow the work of
Zhang and Clark (2008a). Three feature sets are
considered: POS unigram, bigram and trigram fea-
tures. For brevity, we will refer to the three sets as
wi ti, ti?1 ti and ti?2 ti?1 ti.
Given wpos, we adopt the Viterbi algorithm to get
the optimal tagging sequence.
2.2 Dependency Parsing
Recently, graph-based dependency parsing has
gained more and more interest due to its state-of-
the-art accuracy. Graph-based dependency parsing
views the problem as finding the highest scoring tree
from a directed graph. Based on dynamic program-
ming decoding, it can efficiently find an optimal tree
in a huge search space. In a graph-based model, the
1181
score of a dependency tree is factored into scores of
small parts (subtrees).
Scoresyn(x, t,d) = wsyn ? fsyn(x, t,d)
=
?
p?d
Scoresyn(x, t, p)
where p is a scoring part which contains one or more
dependencies in the dependency tree d. Figure 1
shows different types of scoring parts used in current
graph-based models.
h m
dependency
h s
sibling
m g h
grandparent
m
h s
tri-sibling
mth s
grand-sibling
mg
Figure 1: Different types of scoring parts used in current
graph-based models (Koo and Collins, 2010).
Eisner (1996) proposes an O(n3) decoding al-
gorithm for dependency parsing. Based on the al-
gorithm, McDonald et al (2005) propose the first-
order model, in which the scoring parts only con-
tains dependencies. The second-order model of Mc-
Donald and Pereira (2006) incorporates sibling parts
and also needs O(n3) parsing time. The second-
order model of Carreras (2007) incorporates both
sibling and grandparent parts, and needs O(n4)
parsing time. However, the grandparent parts are
restricted to those composed of outermost grand-
children. Koo and Collins (2010) propose efficient
decoding algorithms of O(n4) for third-order mod-
els. In their paper, they implement two versions
of third-order models, Model 1 and Model 2 ac-
cording to their naming. Model 1 incorporates only
grand-sibling parts, while Model 2 incorporates both
grand-sibling and tri-sibling parts. Their experi-
ments on English and Czech show that Model 1 and
Model 2 obtain nearly the same parsing accuracy.
Therefore, we use Model 1 as our third-order model
in this paper.
We use three versions of graph-based dependency
parsing models.
? The first-order model (O1): the same with Mc-
Donald et al (2005).
? The second-order model (O2): the same with
Model 1 in Koo and Collins (2010), but without
using grand-sibling features.2
? The third-order model (O3): the same with
Model 1 in Koo and Collins (2010).
We adopt linear models to define the score of a de-
pendency tree. For the third-order model, the score
of a dependency tree is represented as:
Scoresyn(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s)(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
+
?
{(g,h),(h,s),(h,m)}?d
wgsib ? fgsib(x, t, g, h, s,m)
For the first- and second-order models, the above
formula is modified by deactivating extra parts.
For parsing features, we follow standard prac-
tice for graph-based dependency parsing (McDon-
ald, 2006; Carreras, 2007; Koo and Collins, 2010).
Since these features are highly related with our joint
decoding algorithms, we summarize the features as
follows.
? Dependency Features, fdep(x, t, h,m)
? Unigram Features: whth dir, wmtm dir
? Bigram Features: whth wmtm dir dist
? In Between Features: th tb tm dir dist
? Surrounding Features:
th?1 th th+1 tm?1 tm tm+1 dir dist
? Sibling Features, fsib(x, t, h, s,m)
wh th ws ts wm tm dir
? Grandparent Features, fgrd(x, t, g, h,m)
wg tg wh th wm tm dir gdir
? Grand-sibling Features, fgsib(x, t, g, h, s,m)
wg tg wh th ws ts wm tm dir gdir
2This second-order model incorporates grandparent features
composed of all grandchildren rather than just outermost ones,
and outperforms the one of Carreras (2007) according to the
results in Koo and Collins (2010).
1182
where b denotes an index between h and m; dir
and dist are the direction and distance of (h,m);
gdir is the direction of (g, h). We also use back-
off features by generalizing from very specific fea-
tures over word forms, POS tags, directions and dis-
tances to less sparse features over just POS tags or
considering fewer nodes. To avoid producing too
many sparse features, at most two word forms are
used at the same time in sibling, grandparent and
grand-sibling features, while POS tags are used in-
stead for other nodes; meanwhile, at most four POS
tags are considered at the same time for surrounding
features.
3 Joint Models
In the joint method, we aim to simultaneously solve
the two problems.
(t?, d?) = arg max
t,d
Scorejoint(x, t,d)
Under the linear model, the score of a tagged de-
pendency tree is:
Scorejoint(x, t,d) = Scorepos(x, t)
+ Scoresyn(x, t,d)
= wpos?syn ? fpos?syn(x, t,d)
where fpos?syn(.) means the concatenation of fpos(.)
and fsyn(.). Under the joint model, the weights of
POS and syntactic features, wpos?syn, are simulta-
neously learned. We expect that POS and syntactic
features can interact each other to determine an op-
timal joint result.
Similarly to the baseline dependency parsing
models, we define the first-, second-, and third-order
joint models according to the syntactic features con-
tained in fsyn(.).
In the following, we propose two versions of joint
models which can capture different feature sets and
have different complexity.
3.1 Joint Models of Version 1
The crucial problem for the joint method is to de-
sign effective decoding algorithms to capture rich
features and efficiently search out the optimal re-
sults from a huge hypothesis space. Eisner (2000)
describes a preliminary idea to handle polysemy by
extending parsing algorithms. Based on this idea,
we extend decoding algorithms of McDonald et al
(2005) and Koo and Collins (2010), and propose two
DP based decoding algorithms for our joint models
of version 1.
(b)
(a)
i r r j
r+1 ji ri j
i j
Figure 2: The DP structures and derivations of the first-
order decoding algorithm of joint models of version 1.
We omit symmetric right-headed versions for brevity.
Trapezoids denote incomplete spans. Triangles denote
complete spans. Solid circles denote POS tags of the cor-
responding indices.
The decoding algorithm of O1: As shown in
Figure 2, the first-order joint decoding algorithm
utilizes two types of dynamic programming struc-
tures. (1) Incomplete spans consist of a dependency
and the region between the head and modifier; (2)
Complete spans consist of a headword and its de-
scendants on one side. Each span is recursively cre-
ated by combining two smaller and adjacent spans
in a bottom-up fashion.
The pseudo codes are given in Algorithm 1.
I(i,j)(ti,tj) denotes an incomplete span from i to j
whose boundary POS tags are ti and tj . C(i,j)(ti,tj)
refers to a complete span from i to j whose bound-
ary POS tags are ti and tj . Conversely, I(j,i)(tj ,ti)
andC(j,i)(tj ,ti) represent spans of the other direction.
Note that in these notations the first argument index
always refers to the head of the span.
Line 6 corresponds to the derivation in Figure 2-
(a). Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)}) cap-
tures the joint features invented by this combina-
tion, where p = {(i, j)} means that the newly ob-
served scoring part is the dependency (i, j). The
syntactic features, denoted by fsyn(x, ti, tj , i, j), can
only incorporate syntactic unigram and bigram fea-
tures. The surrounding and in between features
are unavailable, because the context POS tags, such
as tb and ti?1, are not contained in the DP struc-
1183
Algorithm 1 The first-order joint decoding algorithm of version 1
1: ?0 ? i ? n, ti ? T C(i,i)(ti,ti) = 0 ? initialization
2: for w = 1..n do ? span width
3: for i = 0..(n? w) do ? span start index
4: j = i + w ? span end index
5: for (ti, tj) ? T 2 do
6: I(i,j)(ti,tj) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)})}7: I(j,i)(tj ,ti) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(j, i)})}8: C(i,j)(ti,tj) = maxi<r?j maxtr?T {I(i,r)(ti,tr) + C(r,j)(tr,tj) + Scorejoint(x, ti, tr, tj , p = ?)}
9: C(j,i)(tj ,ti) = maxi?r<j maxtr?T {C(r,i)(tr,ti) + I(j,r)(tj ,tr) + Scorejoint(x, ti, tr, tj , p = ?)}
10: end for
11: end for
12: end for
tures. Therefore, we adopt pseudo surrounding
and in between features by simply fixing the con-
text POS tags as the single most likely ones (Mc-
Donald, 2006). Taking the in between features
as an example, we use ti t?b tj dir dist instead,
where t?b is the 1-best tag determined by the base-
line POS tagger. The POS features, denoted by
fpos(x, ti, tr, tr+1, tj), can only incorporate all POS
unigram and bigram features.3 Similarly, we use
pseudo POS trigram features such as t?r?1 tr tr+1.
Line 8 corresponds to the derivation in Figure 2-
(b). Since this combination invents no scoring part
(p = ?), Scorejoint(x, ti, tr, tj , p = ?) is only com-
posed of POS features.4
Line 7 and Line 9 create spans in the opposite di-
rection, which can be analogously illustrated. The
space and time complexity of the algorithm are re-
spectively O(n2q2) and O(n3q4), where q = |T |.5
The decoding algorithm of O2 & O3: Figure
3 illustrates the second- and third-order decoding
algorithm of joint models of version 1. A new
kind of span, named the sibling span, is used to
capture sibling structures. Furthermore, each span
is augmented with a grandparent-index to capture
both grandparent and grand-sibling structures. It is
straightforward to derive the pseudo codes of the al-
3? wr tr if i ?= r; ? wr+1 tr+1 if r + 1 ?= j; ? tr tr+1
if r ?= i or r + 1 ?= j; ? ti tr if r ? 1 = i; ? tr+1 tj if
r + 2 = j. Note that wi ti, wj tj and ti tj (if i = j ? 1) are
not incorporated here to avoid double counting.
4? wr tr if r ?= j;? ti tr if i = r?1;? tr tj if r+1 = j.
Pseudo trigram features can be added accordingly.
5We can reduce the time complexity to O(n3q3) by strictly
adopting the DP structures in the parsing algorithm of Eisner
(1996). However, that may make the algorithm harder to com-
prehend.
ig j g i i ji+1
(a)
g i j g i k i k j
(b)
i k j i ik jr+1r
(c)
g i j g i r i r j
(d)
Figure 3: The DP structures and derivations of the
second- and third-order joint decoding algorithm of ver-
sion 1. For brevity, we elide the right-headed and right-
grandparented versions. Rectangles represent sibling
spans.
i j i r r j
i j i r r+1 j
(b)
(a)
Figure 4: The DP structures and derivations of the first-
order joint decoding algorithm of version 2. We omit the
right-headed version for brevity.
1184
gorithm from Figure 3. We omit them due to space
limitation. Pseudo surrounding, in between and POS
trigram features are used due to the same reason as
above. The space and time complexity of the algo-
rithm are respectively O(n3q3) and O(n4q5).
3.2 Joint Models of Version 2
To further incorporate genuine syntactic surround-
ing and POS trigram features in the DP structures,
we extend the algorithms of joint models of version
1, and propose our joint models of version 2.
The decoding algorithm of O1: Figure 4 illus-
trates the first-order joint decoding algorithm of ver-
sion 2. Compared with the structures in Figure 2,
each span is augmented with the POS tags surround-
ing the boundary indices. These context POS tags
enable Scorejoint(.) in line 6-9 of Algorithm 1 to
capture the syntactic surrounding and POS trigram
features, but also require enumeration of POS tags
over more indices. For brevity, we skip the pseudo
codes which can be easily derived from Algorithm
1. The space and time complexity of the algorithm
are respectively O(n2q6) and O(n3q10).
The decoding algorithm of O2 & O3: Using the
same idea as above, the second- and third-order joint
decoding algorithms of version 2 can be derived
based on Figure 3. Again, we omit both its DP struc-
tures and pseudo codes for the sake of brevity. Its
space and time complexity are respectively O(n3q7)
and O(n4q11).
In between features, which should be regarded as
non-local features in the joint situation, still cannot
be incorporated in our joint models of version 2.
Again, we adopt the pseudo version.
3.3 Comparison
Based on the above illustration, we can see that joint
models of version 1 are more efficient with regard
to the number of POS tags for each word, but fail to
incorporate syntactic surrounding features and POS
trigram features in the DP structures. On the con-
trary, joint models of version 2 can incorporate both
aforementioned feature sets, but have higher com-
plexity. These two versions of models will be thor-
oughly compared in the experiments.
4 Pruning Techniques
In this section, we introduce two pruning strategies
to constrain the search space of our models due to
their high complexity.
4.1 POS Tag Pruning
The time complexity of the joint decoding algorithm
is unbearably high with regard to the number of can-
didate POS tags for each word (q = |T |). We
find that it would be extremely time-consuming even
when we only use two most likely POS tags for each
word (q = 2) even for joint models of version 1.
To deal with this problem, we propose a pruning
method that can effectively reduce the POS tag space
based on a probabilistic tagging model.
We adopt a conditional log-linear model (Lafferty
et al, 2001), which defines a conditional distribution
of a POS tag sequence t given x:
P (t|x) = e
wpos?fpos(x,t)
?
t ewpos?fpos(x,t)
We use the same feature set fpos defined in Sec-
tion 2.1, and adopt the exponentiated gradient algo-
rithm to learn the weight vector wpos (Collins et al,
2008).
The marginal probability of tagging a word wi as
t is
P (ti = t|x) =
?
t:t[i]?t
P (t|x)
which can be efficiently computed using the
forward-backward algorithm.
We define pmaxi(x) to be the highest marginal
probability of tagging the word wi:
pmaxi(x) = maxt?T P (ti = t|x)
We then define the allowable candidate POS tags
of the word wi to be
Ti(x) = {t : t ? T , P (ti = t|x) ? ?t?pmaxi(x)}
where ?t is the pruning threshold. Ti(x) is used to
constrain the POS search space by replacing T in
Algorithm 1.
1185
4.2 Dependency Pruning
The parsing time grows quickly for the second- and
third-order models (both baseline and joint) when
the input sentence gets longer (O(n4)). Follow-
ing Koo and Collins (2010), we eliminate unlikely
dependencies using a form of coarse-to-fine prun-
ing (Charniak and Johnson, 2005; Petrov and Klein,
2007). On the development set, 68.87% of the de-
pendencies are pruned, while the oracle dependency
accuracy is 99.77%. We use 10-fold cross validation
to do pruning on the training set.
5 Experiments
We use the Penn Chinese Treebank 5.1 (CTB5) (Xue
et al, 2005). Following the setup of Duan et al
(2007), Zhang and Clark (2008b) and Huang and
Sagae (2010), we split CTB5 into training (secs 001-
815 and 1001-1136), development (secs 886-931
and 1148-1151), and test (secs 816-885 and 1137-
1147) sets. We use the head-finding rules of Zhang
and Clark (2008b) to turn the bracketed sentences
into dependency structures.
We use the standard tagging accuracy to evalu-
ate POS tagging. For dependency parsing, we use
word accuracy (also known as dependency accu-
racy), root accuracy and complete match rate (all
excluding punctuation) .
For the averaged training, we train each model for
15 iterations and select the parameters that perform
best on the development set.
5.1 Results of POS Tag Pruning
Figure 5 shows the distribution of words with dif-
ferent number of candidate POS tags and the k-best
oracle tagging accuracy under different ?t. To avoid
dealing with words that have many candidate POS
tags, we further apply a hard criterion that the decod-
ing algorithms only consider top k candidate POS
tags.
To find the best ?t, we train and evaluate the
second-order joint model of version 1 on the train-
ing and development sets pruned with different ?t
(top k = 5). We adopt the second-order joint model
of version 1 because of its efficiency compared with
the third-order models and its capability of captur-
ing rich features compared with the first-order mod-
els. The results are shown in Table 1. The model
0
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 >5
pro
por
tion
 of 
wor
ds (
%)
number of candidate POS tags 
0.1
0.01
0.001
93
94
95
96
97
98
99
100
1 2 3 4 5 ?
k-b
est 
ora
cle 
tagg
ing
 acc
ura
cy (
%)
k
0.1
0.01
0.001
Figure 5: Results of POS tag pruning with different prun-
ing threshold ?t on the development set.
?t word root compl. acc. speed
0.1 81.53 76.88 30.00 94.17 2.5
0.01 81.83 76.62 30.62 93.16 1.2
0.001 81.73 77.38 30.50 93.41 0.5
Table 1: Performance of the second-order joint model of
version 1 with different pruning threshold ?t (top k = 5)
on the development set. ?Acc.? means the tagging accu-
racy. ?Speed? refers to the parsing speed (the number of
sentences processed per second).
with ?t = 0.1 obtains the highest tagging accuracy,
which is much higher than that of both ?t = 0.01
and ?t = 0.001. However, its parsing accuracy
is inferior to the other two. ?t = 0.01 produces
slightly better parsing accuracy than ?t = 0.001,
and is twice faster. Finally, we choose ?t = 0.01
due to the efficiency factor and our priority over the
parsing accuracy.
Then we do experiments to find an optimal top
k. Table 2 shows the results. We decide to choose
k = 3 since it leads to best parsing accuracy.
From Table 1 and 2, we can have an interesting
finding: it seems that the harder we filter the POS
tag space, the higher tagging accuracy we get. In
other words, giving the joint model less flexibility
of choosing POS tags leads to better tagging per-
formance.
Due to time limitation, we do not tune ?t and k for
other joint models. Instead, we simply adopt ?t =
0.01 and top k = 3.
5.2 Final Results
Table 3 shows the final results on the test set. We list
a few state-of-the-art results in the bottom. Duan07
refers to the results of Duan et al (2007). They
enhance the transition-based parsing model with
1186
Syntactic Metrics Tagging Accuracy Parsing Speed
word root compl. all-word known unknown Sent/Sec
Joint Models V2
O3 80.79 75.84 29.11 92.80 93.88 76.80 0.3
O2 80.49 75.49 28.24 92.68 93.77 76.27 0.6
O1 77.37 68.64 23.09 92.96 94.05 76.64 2.0
Joint Models V1
O3 80.69 75.90 29.06 92.89 93.96 76.80 0.5
O2 80.74 75.80 28.24 93.08 94.11 77.53 1.7
O1 77.38 69.69 22.62 93.20 94.23 77.76 8.5
Auto POS
O3 79.29 74.65 27.24
93.51 94.36 80.78
2.0
O2 79.03 74.70 27.19 5.8
O1 75.68 68.06 21.10 17.4
MSTParser2 77.95 72.04 25.50 4.1
MSTParser1 75.84 68.55 21.36 5.2
MaltParser 75.24 65.92 23.19 2.6
Gold POS
O3 86.00 77.59 34.02
100.0 100.0 100.0
-
O2 86.18 78.58 34.07 -
O1 82.24 70.10 26.02 -
MSTParser2 85.24 77.41 33.19 -
MSTParser1 83.04 71.49 27.59 -
MaltParser 82.62 69.34 29.06 -
H&S10 85.20 78.32 33.72 -
Z&C08 single 84.33 76.73 32.79 -
Z&C08 hybrid 85.77 76.26 34.41 -
Duan07 83.88 73.70 32.70 -
Table 3: Final results on the test set. ?Gold POS? means that gold POS tags are used as input by the pipelined parsing
models; while ?Auto POS? means that the POS tags are generated by the baseline POS tagging model.
top k word root compl. acc. speed
2 81.46 76.12 30.50 93.51 2.7
3 82.11 76.75 29.75 93.31 1.7
4 81.75 76.62 30.38 93.25 1.4
5 81.83 76.62 30.62 93.16 1.2
Table 2: Performance of the second-order joint model of
version 1 with different top k (?t = 0.01) on the devel-
opment set.
the beam search. H&S10 refers to the results of
Huang and Sagae (2010). They greatly expand the
search space of the transition-based model by merg-
ing equivalent states with dynamic programming.
Z&C08 refers to the results of Zhang and Clark
(2008b). They use a hybrid model to combine the
advantages of both graph-based and transition-based
models. We also do experiments with two publicly
available and widely-used parsers, MSTParser6 and
MaltParser7. MSTParser1 refers to the first-order
6http://sourceforge.net/projects/mstparser/
7http://maltparser.org/
graph-based model of McDonald et al (2005), while
MSTParser2 is the second-order model of McDon-
ald and Pereira (2006). MaltParser is a transition-
based parsing system. It integrates a number of clas-
sification algorithms and transition strategies. We
adopt the support vector machine classifier and the
arc-standard strategy (Nivre, 2008).
We can see that when using gold tags, our
pipelined second- and third-order parsing models
achieve best parsing accuracy, which is even higher
than the hybrid model of Zhang and Clark (2008b).
It is a little surprising that the second-order model
slightly outperforms the third-order one. This may
be possible, since Koo and Collins (2010) shows that
the third-order model outperforms the second-order
one by only 0.32% on English and 0.07% on Czech.
In addition, we only use basic third-order features.
Both joint models of version 1 and 2 can consis-
tently and significantly improve the parsing accu-
racy by about 1.5% for all first-, second- and third-
order cases. Accidentally, the parsing accuracy of
the second-order joint model of version 2 is lower
1187
error pattern # ? error pattern # ?
DEC ? DEG 237 114 NR ? NN 184 100
NN ? VV 389 73 NN ? NR 106 91
DEG ? DEC 170 39 NN ? JJ 95 70
VV ? NN 453 27 VA ? VV 29 41
P ? VV 52 24 JJ ? NN 126 29
P ? CC 39 13 VV ? VA 67 10
Table 4: Error analysis of POS tagging. # means the
error number of the corresponding pattern made by the
baseline tagging model. ? and ? mean the error number
reduced or increased by the joint model.
than that of its counterparts by about 0.3%. More
experiments and further analysis may be needed to
find out the reason. The two versions of joint models
performs nearly the same, which indicates that using
pseudo surrounding and POS trigram features may
be sufficient for the joint method on this data set.
In summary, we can conclude that the joint frame-
work is certainly helpful for dependency parsing.
It is clearly shown in Table 3 that the joint
method surprisingly hurts the tagging accuracy,
which diverges from our discussion in Section 1.
Some insights into this issue will be given in Sec-
tion 5.3. Moreover, it seems that the more syntac-
tic features the joint method incorporates (from
O1 to O3), the more the tagging accuracy drops.
We suspect that this is because the joint models are
dominated by the syntactic features. Take the first-
order joint model as an example. The dimension of
the syntactic features fsyn is about 3.5 million, while
that of fpos is only about 0.5 million. The gap be-
comes much larger for the second- and third-order
cases.
Comparing the parsing speed, we can find that the
pruning of POS tags is very effective. The second-
order joint model of version 1 can parse 1.7 sen-
tences per second, while the pipelined second-order
parsing model can parse 5.8 sentences per second,
which is rather close considering that there is a fac-
tor of q5.
5.3 Error Analysis
To find out the impact of our joint models on the
individual tasks, we conduct detailed error analy-
sis through comparing the results of the pipelined
second-order parsing model and the second-order
joint model of version 1.
Impact on POS tagging: Table 4 shows how the
joint model changes the quantity of POS tagging er-
ror patterns compared with the pipelined model. An
error pattern ?X ? Y? means that the focus word,
whose true tag is ?X?, is assigned a tag ?Y?. We
choose these patterns with largest reduction or in-
crease in the error number, and rank them in de-
scending order of the variation.
From the left part of Table 4, we can see that
the joint method is clearly better at resolving tag-
ging ambiguities like {VV, NN} and {DEG, DEC}.8
One common characteristic of these ambiguous
pairs is that the local or even whole syntactic struc-
ture will be destructed if the wrong tag is chosen. In
other words, resolving these ambiguities is critical
and helpful from the parsing viewpoint. From an-
other perspective, the joint model is capable of pre-
ferring the right tag with the help of syntactic struc-
tures, which is impossible for the baseline sequential
labeling model.
In contrast, pairs like {NN, NR}, {VV, VA} and
{NN, JJ} only slightly influence the syntactic struc-
ture when mis-tagged. The joint method performs
worse on these ambiguous pairs, as shown in the
right part of Table 4.
Impact on parsing: Table 5 studies the change of
parsing error rates between the pipelined and joint
model on different POS tag patterns. We present the
most typical and prominent patterns in the table, and
rank them in descending order of X?s frequency of
occurrence. We also show the change of proportion
of different patterns, which is consistent with the re-
sults in Table 4.
From the table, we can see the joint model can
achieve a large error reduction (0.8?4.0%) for all
the patterns ?X ? X?. In other words, the joint
model can do better given the correct tags than
the pipelined method.
For all the patterns marked by ?, except for the
ambiguous pair {NN, JJ} (which we find is difficult
to explain even after careful result analysis), the joint
model also reduces the error rates (2.2?15.4%). As
8DEG and DEC are the two POS tags for the frequently used
auxiliary word ??? (de?, of) in Chinese. The associative ???
is tagged as DEG, such as ???/father ? ??/eyes (eyes of
the father)?; while the one in a relative clause is tagged as DEC,
such as ??/he ??/made ? ??/progress (progress that he
made)?.
1188
pattern pipelined jointprop (%) error (%) prop (%) error (%)
NN ? NN 94.6 16.8 -1.1 -1.8
? VV ? 2.9 55.5 -0.5 +15.1
? NR ? 0.8 24.5 +0.7 -2.2
? JJ ? 0.7 17.9 +0.5 +2.1
VV ? VV 89.6 34.2 -0.3 ?4.0
? NN ? 6.6 66.4 -0.4 +0.7
? VA ? 1.0 38.8 +0.1 -15.4
NR ? NR 91.7 15.4 -3.7 -0.8
? NN ? 5.9 21.7 +3.2 -3.7
P ? P 92.8 22.6 +3.4 -3.2
? VV ? 3.0 50.0 -1.4 +10.7
? CC ? 2.3 74.4 -0.7 +21.9
JJ ? JJ 80.5 11.2 -2.8 -2.0
? NN ? 9.8 18.3 +2.2 +1.8
DEG ? DEG 86.5 11.1 +2.8 -3.6
? DEC ? 13.5 61.8 -3.1 +37.4
DEC ? DEC 79.7 17.2 +12.1 -4.0
? DEG ? 20.2 56.5 -9.7 +40.2
Table 5: Comparison of parsing error rates on different
POS tag patterns between the pipelined and joint models.
Given a pattern ?X ? Y?, ?prop? means its proportion in
all occurrence of ?X? (Count(X?Y )Count(X) ), and ?error? refers
to its parsing error rate ( Count(wrongly headed X?Y )Count(X?Y ) ).
The last two columns give the absolute reduction (-) or
increase (+) in proportion and error rate made by the joint
model. ? marks the patterns appearing in the left part of
Table 4, while ? marks those in the right part of Table 4.
discussed earlier, these patterns concern ambiguous
tag pairs which usually play similar roles in syn-
tactic structures. This demonstrates that the joint
model can do better on certain tagging error pat-
terns.
For patterns marked by ?, the error rate of the
joint model usually increases by large margin. How-
ever, the proportion of these patterns is substantially
decreased, since the joint model can better resolve
these ambiguities with the help of syntactic knowl-
edge.
In summary, we can conclude that the joint model
is able to choose such POS tags that are more helpful
and discriminative from parsing viewpoint. This is
the fundamental reason of the parsing performance
improvement.
6 Related Work
Theoretically, Eisner (2000) proposes a preliminary
idea of extending the decoding algorithm for de-
pendency parsing to handle polysemy. Here, word
senses can be understood as POS-tagged words.
Koo and Collins (2010) also briefly discuss that their
third-order decoding algorithm can be modified to
handle word senses using the idea of Eisner (2000).
In his PhD thesis, McDonald (2006) extends his
second-order model with the idea of Eisner (2000)
to study the impact of POS tagging errors on pars-
ing accuracy. To make inference tractable, he uses
top 2 candidate POS tags for each word based on
a maximum entropy tagger, and adopts the single
most likely POS tags for the surrounding and in be-
tween features. He conducts primitive experiments
on English Penn Treebank, and shows that parsing
accuracy can be improved from 91.5% to 91.9%.
However, he finds that the model is unbearably time-
consuming.
7 Conclusions
In this paper, we have systematically investigated
the issue of joint POS tagging and dependency pars-
ing. We propose and compare several joint models
and their corresponding decoding algorithms which
can incorporate different feature sets. We also pro-
pose an effective POS tag pruning method which can
greatly improve the decoding efficiency. The experi-
mental results show that our joint models can signif-
icantly improve the state-of-the-art parsing accuracy
by more than 1.5%. Detailed error analysis shows
that the fundamental reason for the parsing accu-
racy improvement is that the joint method is able to
choose POS tags that are helpful and discriminative
from parsing viewpoint.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 60803093, 60975055, the Natural Scientific
Research Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069) and the Fun-
damental Research Funds for the Central Universi-
ties (HIT.KLOF.2010064).
References
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
1189
EMNLP/CoNLL, pages 141?150.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 161?169.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL 2007, pages 208?217.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775?1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Xiangyu Duan, Jun Zhao, , and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 406?
414.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING 1996, pages 340?345.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
326?334.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, Ohio, June. Association
for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1077?1086,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897?904.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 513?521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 439?446,
Sapporo, Japan, July. Association for Computational
Linguistics.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108?
1117.
Yang Liu and Qun Liu. 2010. Joint parsing and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 707?715.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. In Computational Lin-
guistics, volume 34, pages 513?553.
1190
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
760?767, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 486?494.
Xinyan Xiao, Yang Liu, YoungSook Hwang, Qun Liu,
and Shouxun Lin. 2010. Joint tokenization and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1200?1208.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
1191
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675?684,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Multiple Treebanks for Parsing with Quasi-synchronous
Grammars
Zhenghua Li, Ting Liu?, Wanxiang Che
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{lzh,tliu,car}@ir.hit.edu.cn
Abstract
We present a simple and effective framework
for exploiting multiple monolingual treebanks
with different annotation guidelines for pars-
ing. Several types of transformation patterns
(TP) are designed to capture the systematic an-
notation inconsistencies among different tree-
banks. Based on such TPs, we design quasi-
synchronous grammar features to augment the
baseline parsing models. Our approach can
significantly advance the state-of-the-art pars-
ing accuracy on two widely used target tree-
banks (Penn Chinese Treebank 5.1 and 6.0)
using the Chinese Dependency Treebank as
the source treebank. The improvements are
respectively 1.37% and 1.10% with automatic
part-of-speech tags. Moreover, an indirect
comparison indicates that our approach also
outperforms previous work based on treebank
conversion.
1 Introduction
The scale of available labeled data significantly af-
fects the performance of statistical data-driven mod-
els. As a structural classification problem that is
more challenging than binary classification and se-
quence labeling problems, syntactic parsing is more
prone to suffer from the data sparseness problem.
However, the heavy cost of treebanking typically
limits one single treebank in both scale and genre.
At present, learning from one single treebank seems
inadequate for further boosting parsing accuracy.1
?Correspondence author: tliu@ir.hit.edu.cn
1Incorporating an increased number of global features, such
as third-order features in graph-based parsers, slightly affects
parsing accuracy (Koo and Collins, 2010; Li et al, 2011).
Treebanks # of Words Grammar
CTB5 0.51 million Phrase structure
CTB6 0.78 million Phrase structure
CDT 1.11 million Dependency structure
Sinica 0.36 million Phrase structure
TCT about 1 million Phrase structure
Table 1: Several publicly available Chinese treebanks.
Therefore, studies have recently resorted to other re-
sources for the enhancement of parsing models, such
as large-scale unlabeled data (Koo et al, 2008; Chen
et al, 2009; Bansal and Klein, 2011; Zhou et al,
2011), and bilingual texts or cross-lingual treebanks
(Burkett and Klein, 2008; Huang et al, 2009; Bur-
kett et al, 2010; Chen et al, 2010).
The existence of multiple monolingual treebanks
opens another door for this issue. For example, ta-
ble 1 lists a few publicly available Chinese treebanks
that are motivated by different linguistic theories or
applications. In the current paper, we utilize the
first three treebanks, i.e., the Chinese Penn Tree-
bank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al, 2005),
and the Chinese Dependency Treebank (CDT) (Liu
et al, 2006). The Sinica treebank (Chen et al, 2003)
and the Tsinghua Chinese Treebank (TCT) (Qiang,
2004) can be similarly exploited with our proposed
approach, which we leave as future work.
Despite the divergence of annotation philosophy,
these treebanks contain rich human knowledge on
the Chinese syntax, thereby having a great deal of
common ground. Therefore, exploiting multiple
treebanks is very attractive for boosting parsing ac-
curacy. Figure 1 gives an example with different an-
675
??1 ??2 ?3 ??4
VV NN CC NN
promote trade and industry
v n c n
OBJ
NMOD
NMOD
VOB COO
LAD
w0
ROOT
ROOT
Figure 1: Example with annotations from CTB5 (upper)
and CDT (under).
notations from CTB5 and CDT.2 This example illus-
trates that the two treebanks annotate coordination
constructions differently. In CTB5, the last noun is
the head, whereas the first noun is the head in CDT.
One natural idea for multiple treebank exploita-
tion is treebank conversion. First, the annotations
in the source treebank are converted into the style
of the target treebank. Then, both the converted
treebank and the target treebank are combined. Fi-
nally, the combined treebank are used to train a
better parser. However, the inconsistencies among
different treebanks are normally nontrivial, which
makes rule-based conversion infeasible. For exam-
ple, a number of inconsistencies between CTB5 and
CDT are lexicon-sensitive, that is, they adopt dif-
ferent annotations for some particular lexicons (or
word senses). Niu et al (2009) use sophisticated
strategies to reduce the noises of the converted tree-
bank after automatic treebank conversion.
The present paper proposes a simple and effective
framework for this problem. The proposed frame-
work avoids directly addressing the difficult anno-
tation transformation problem, but focuses on mod-
eling the annotation inconsistencies using transfor-
mation patterns (TP). The TPs are used to compose
quasi-synchronous grammar (QG) features, such
that the knowledge of the source treebank can in-
spire the target parser to build better trees. We con-
duct extensive experiments using CDT as the source
treebank to enhance two target treebanks (CTB5 and
CTB6). Results show that our approach can signifi-
cantly boost state-of-the-art parsing accuracy. More-
over, an indirect comparison indicates that our ap-
2CTB5 is converted to dependency structures following the
standard practice of dependency parsing (Zhang and Clark,
2008b). Notably, converting a phrase-structure tree into its
dependency-structure counterpart is straightforward and can be
performed by applying heuristic head-finding rules.
proach also outperforms the treebank conversion ap-
proach of Niu et al (2009).
2 Related Work
The present work is primarily inspired by Jiang et
al. (2009) and Smith and Eisner (2009). Jiang et al
(2009) improve the performance of word segmen-
tation and part-of-speech (POS) tagging on CTB5
using another large-scale corpus of different annota-
tion standards (People?s Daily). Their framework is
similar to ours. However, handling syntactic anno-
tation inconsistencies is significantly more challeng-
ing in our case of parsing. Smith and Eisner (2009)
propose effective QG features for parser adaptation
and projection. The first part of their work is closely
connected with our work, but with a few impor-
tant differences. First, they conduct simulated ex-
periments on one treebank by manually creating a
few trivial annotation inconsistencies based on two
heuristic rules. They then focus on better adapting a
parser to a new annotation style with few sentences
of the target style. In contrast, we experiment with
two real large-scale treebanks, and boost the state-
of-the-art parsing accuracy using QG features. Sec-
ond, we explore much richer QG features to fully
exploit the knowledge of the source treebank. These
features are tailored to the dependency parsing prob-
lem. In summary, the present work makes substan-
tial progress in modeling structural annotation in-
consistencies with QG features for parsing.
Previous work on treebank conversion primar-
ily focuses on converting one grammar formalism
of a treebank into another and then conducting a
study on the converted treebank (Collins et al, 1999;
Xia et al, 2008). The work by Niu et al (2009)
is, to our knowledge, the only study to date that
combines the converted treebank with the existing
target treebank. They automatically convert the
dependency-structure CDT into the phrase-structure
style of CTB5 using a statistical constituency parser
trained on CTB5. Their experiments show that
the combined treebank can significantly improve
the performance of constituency parsers. However,
their method requires several sophisticated strate-
gies, such as corpus weighting and score interpo-
lation, to reduce the influence of conversion errors.
Instead of using the noisy converted treebank as ad-
ditional training data, our approach allows the QG-
676
enhanced parsing models to softly learn the system-
atic inconsistencies based on QG features, making
our approach simpler and more robust.
Our approach is also intuitively related to stacked
learning (SL), a machine learning framework that
has recently been applied to dependency parsing
to integrate two main-stream parsing models, i.e.,
graph-based and transition-based models (Nivre and
McDonald, 2008; Martins et al, 2008). However,
the SL framework trains two parsers on the same
treebank and therefore does not need to consider the
problem of annotation inconsistencies.
3 Dependency Parsing
Given an input sentence x = w0w1...wn and its POS
tag sequence t = t0t1...tn, the goal of dependency
parsing is to build a dependency tree as depicted in
Figure 1, denoted by d = {(h,m, l) : 0 ? h ?
n, 0 < m ? n, l ? L}, where (h,m, l) indicates an
directed arc from the head word (also called father)
wh to the modifier (also called child or dependent)
wm with a dependency label l, and L is the label set.
We omit the label l because we focus on unlabeled
dependency parsing in the present paper. The artifi-
cial node w0, which always points to the root of the
sentence, is used to simplify the formalizations.
In the current research, we adopt the graph-based
parsing models for their state-of-the-art performance
in a variety of languages.3 Graph-based models
view the problem as finding the highest scoring tree
from a directed graph. To guarantee the efficiency of
the decoding algorithms, the score of a dependency
tree is factored into the scores of some small parts
(subtrees).
Scorebs(x, t,d) = wbs ? fbs(x, t,d)
=
?
p?d
wpart ? fpart(x, t, p)
where p is a scoring part which contains one or more
dependencies of d, and fbs(.) denotes the basic pars-
ing features, as opposed to the QG features. Figure
2 lists the scoring parts used in our work, where g,
h, m, and s, are word indices.
We implement three parsing models of varying
strengths in capturing features to better understand
the effect of the proposed QG features.
3Our approach can equally be applied to transition-based
parsing models (Yamada and Matsumoto, 2003; Nivre, 2003)
with minor modifications.
dependency sibling grandparent
h
m
h
ms
h
m
g
Figure 2: Scoring parts used in our graph-based parsing
models.
? The first-order model (O1) only incorporates
dependency parts (McDonald et al, 2005), and
requires O(n3) parsing time.
? The second-order model using only sibling
parts (O2sib) includes both dependency and
sibling parts (McDonald and Pereira, 2006),
and needs O(n3) parsing time.
? The second-order model (O2) uses all the
scoring parts in Figure 2 (Koo and Collins,
2010). The time complexity of the decoding
algorithm is O(n4).4
For the O2 model, the score function is rewritten as:
Scorebs(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s),(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
where fdep(.), fsib(.) and fgrd(.) correspond to the
features for the three kinds of scoring parts. We
adopt the standard features following Li et al
(2011). For the O1 and O2sib models, the above
formula is modified by deactivating the extra parts.
4 Dependency Parsing with QG Features
Smith and Eisner (2006) propose the QG for ma-
chine translation (MT) problems, allowing greater
syntactic divergences between the two languages.
Given a source sentence x? and its syntactic tree
d?, a QG defines a monolingual grammar that gen-
erates translations of x?, which can be denoted by
p(x,d,a|x?,d?), where x and d refer to a translation
and its parse, and a is a cross-language alignment.
Under a QG, any portion of d can be aligned to any
4We use the coarse-to-fine strategy to prune the search
space, which largely accelerates the decoding procedure (Koo
and Collins, 2010).
677
hm
h
m
m
h
Consistent: 55.4% Reverse: 8.6%Sibling: 10.0%Grand: 11.7% Reverse-grand: 1.4%( ', , )dep d h m? ?
( ', , , )grd d g h m? ?
( ', , , )sib d h s m? ?
i
m
h
i
h
m
28.2%
i
mh
h
ms
h
ms
6.7%
i
m
h
s
h
s
i
m
6.4%
i
msh
4.9%
s
m
h
4.4%
m
s
h
4.2%
h
m
g
h
m
g
30.1% 6.5%
h
mg
6.2%
h
m
i
g
6.1%
i
m
h
g
m
h
g
5.4% 5.3%
i
h
g
m
Syntactic Structures of the Corresponding Source SideTarget Side
Figure 4: Most frequent transformation patterns (TPs) when using CDT as the source treebank and CTB5 as the
target. A TP comprises two syntactic structures, one in the source side and the other in the target side, and denotes
the process by which the left-side subtree is transformed into the right-side structure. Functions ?dep(.), ?sib(.), and
?grd(.) return the specific TP type for a candidate scoring part according to the source tree d?.
Source Parser
ParserS
Target Parser
ParserT
Train
Train
Parse
Target 
Treebank
T={(xj, dj)}j
Source Treebank
S={(xi, di)}i
Parsed 
Treebank
TS={(xj, djS)}j
Target Treebank with 
Source Annotations
T+S={(xj, djS, dj)}j
Out
Figure 3: Framework of our approach.
portion of d?, and the construction of d can be in-
spired by arbitrary substructures of d?. To date, QGs
have been successfully applied to various tasks, such
as word alignment (Smith and Eisner, 2006), ma-
chine translation (Gimpel and Smith, 2011), ques-
tion answering (Wang et al, 2007), and sentence
simplification (Woodsend and Lapata, 2011).
In the present work, we utilize the idea of the QG
for the exploitation of multiple monolingual tree-
banks. The key idea is to let the parse tree of one
style inspire the parsing process of another style.
Different from a MT process, our problem consid-
ers one single sentence (x = x?), and the alignment
a is trivial. Figure 3 shows the framework of our
approach. First, we train a statistical parser on the
source treebank, which is called the source parser.
The source parser is then used to parse the whole tar-
get treebank. At this point, the target treebank con-
tains two sets of annotations, one conforming to the
source style, and the other conforming to the target
style. During both the training and test phases, the
target parser are inspired by the source annotations,
and the score of a target dependency tree becomes
Score(x, t,d?,d) =Scorebs(x, t,d)
+Scoreqg(x, t,d?,d)
The first part corresponds to the baseline model,
whereas the second part is affected by the source tree
d? and can be rewritten as
Scoreqg(x, t,d?,d) = wqg ? fqg(x, t,d?,d)
where fqg(.) denotes the QG features. We expect the
QG features to encourage or penalize certain scor-
ing parts in the target side according to the source
tree d?. Taking Figure 1 as an example, suppose
that the upper structure is the target. The target
parser can raise the score of the candidate depen-
dence ?and? ? ?industry?, because the depen-
678
dency also appears in the source structure, and ev-
idence in the training data shows that both annota-
tion styles handle conjunctions in the same manner.
Similarly, the parser may add weight to ?trade? ?
?industry?, considering that the reverse arc is in
the source structure. Therefore, the QG-enhanced
model must learn the systematic consistencies and
inconsistencies from the training data.
To model such consistency or inconsistency sys-
tematicness, we propose the use of TPs for encoding
the structural correspondence between the source
and target styles. Figure 4 presents the three kinds
of TPs used in our model, which correspond to the
three scoring parts of our parsing models.
Dependency TPs shown in the first row consider
how one dependency in the target side is trans-
formed in the source annotations. We only consider
the five cases shown in the figure. The percentages
in the lower boxes refer to the proportion of the
corresponding pattern, which are counted from the
training data of the target treebank with source anno-
tations T+S . We can see that the noisy source struc-
tures and the gold-standard target structures have
55.4% common dependencies. If the source struc-
ture does not belong to any of the listed five cases,
?dep(d?, h,m) returns ?else? (12.9%). We could
consider more complex structures, such as h being
the grand grand father of m, but statistics show that
more complex transformations become very scarce
in the training data.
For the reason that dependency TPs can only
model how one dependency in the target structure is
transformed, we consider more complex transforma-
tions for the other two kinds of scoring parts of the
target parser, i.e., the sibling and grand TPs shown
in the bottom two rows. We only use high-frequency
TPs of a proportion larger than 1.0%, aggregate oth-
ers as ?else?, which leaves us with 21 sibling TPs
and 22 grand TPs.
Based on these TPs, we propose the QG fea-
tures for enhancing the baseline parsing models,
which are shown in Table 2. The type of the
TP is conjoined with the related words and POS
tags, such that the QG-enhanced parsing models can
make more elaborate decisions based on the context.
Then, the score contributed by the QG features can
be redefined as
Scoreqg(x, t,d?,d) =
?
{(h,m)}?d
wqg-dep ? fqg-dep(x, t,d?, h,m)
+
?
{(h,s),(h,m)}?d
wqg-sib ? fqg-sib(x, t,d?, h, s,m)
+
?
{(g,h),(h,m)}?d
wqg-grd ? fqg-grd(x, t,d?, g, h,m)
which resembles the baseline model and can be nat-
urally handled by the decoding algorithms.
5 Experiments and Analysis
We use the CDT as the source treebank (Liu et
al., 2006). CDT consists of 60,000 sentences from
the People?s Daily in 1990s. For the target tree-
bank, we use two widely used versions of Penn Chi-
nese Treebank, i.e., CTB5 and CTB6, which con-
sist of Xinhua newswire, Hong Kong news and ar-
ticles from Sinarama news magazine (Xue et al,
2005). To facilitate comparison with previous re-
sults, we follow Zhang and Clark (2008b) for data
split and constituency-to-dependency conversion of
CTB5. CTB6 is used as the Chinese data set in the
CoNLL 2009 shared task (Hajic? et al, 2009). There-
fore, we adopt the same setting.
CDT and CTB5/6 adopt different POS tag sets,
and converting from one tag set to another is difficult
(Niu et al, 2009).5 To overcome this problem, we
use the People?s Daily corpus (PD),6 a large-scale
corpus annotated with word segmentation and POS
tags, to train a statistical POS tagger. The tagger
produces a universal layer of POS tags for both the
source and target treebanks. Based on the common
tags, the source parser projects the source annota-
tions into the target treebanks. PD comprises ap-
proximately 300 thousand sentences of with approx-
imately 7 million words from the first half of 1998
of People?s Daily.
Table 3 summarizes the data sets used in the
present work. CTB5X is the same with CTB5 but
follows the data split of Niu et al (2009). We use
CTB5X to compare our approach with their treebank
conversion method (see Table 9).
5The word segmentation standards of the two treebanks also
slightly differs, which are not considered in this work.
6http://icl.pku.edu.cn/icl_groups/
corpustagging.asp
679
fqg-dep(x, t,d?, h,m) fqg-sib(x, t,d?, h, s,m) fqg-grd(x, t,d?, g, h,m)
?dir(h,m) ? dist(h,m) ?dir(h,m) ?dir(h,m) ? dir(g, h)
?dep(d?, h,m) ? th ? tm ?sib(d?, h, s,m) ? th ? ts ? tm ?grd(d?, g, h,m) ? tg ? th ? tm
?dep(d?, h,m) ? wh ? tm ?sib(d?, h, s,m) ? wh ? ts ? tm ?grd(d?, g, h,m) ? wg ? th ? tm
?dep(d?, h,m) ? th ? wm ?sib(d?, h, s,m) ? th ? ws ? tm ?grd(d?, g, h,m) ? tg ? wh ? tm
?dep(d?, h,m) ? wh ? wm ?sib(d?, h, s,m) ? th ? ts ? wm ?grd(d?, g, h,m) ? tg ? th ? wm
?sib(d?, h, s,m) ? ts ? tm ?grd(d?, g, h,m) ? tg ? tm
Table 2: QG features used to enhance the baseline parsing models. dir(h,m) denotes the direction of the dependency
(h,m), whereas dist(h,m) is the distance |h ?m|. ?dir(h,m) ? dist(h,m) indicates that the features listed in the
corresponding column are also conjoined with dir(h,m) ? dist(h,m) to form new features.
Corpus Train Dev Test
PD 281,311 5,000 10,000
CDT 55,500 1,500 3,000
CTB5 16,091 803 1,910
CTB5X 18,104 352 348
CTB6 22,277 1,762 2,556
Table 3: Data used in this work (in sentence number).
We adopt unlabeled attachment score (UAS) as
the primary evaluation metric. We also use Root ac-
curacy (RA) and complete match rate (CM) to give
more insights. All metrics exclude punctuation. We
adopt Dan Bikel?s randomized parsing evaluation
comparator for significance test (Noreen, 1989).7
For all models used in current work (POS tagging
and parsing), we adopt averaged perceptron to train
the feature weights (Collins, 2002). We train each
model for 10 iterations and select the parameters that
perform best on the development set.
5.1 Preliminaries
This subsection describes how we project the source
annotations into the target treebanks. First, we train
a statistical POS tagger on the training set of PD,
which we name TaggerPD .8 The tagging accuracy
on the test set of PD is 98.30%.
We then use TaggerPD to produce POS tags for
all the treebanks (CDT, CTB5, and CTB6).
Based on the common POS tags, we train a
second-order source parser (O2) on CDT, denoted
by ParserCDT . The UAS on CDT-test is 84.45%.
We then use ParserCDT to parse CTB5 and CTB6.
7http://www.cis.upenn.edu/[normal-wave
?
]
dbikel/software.html
8We adopt the Chinese-oriented POS tagging features pro-
posed in Zhang and Clark (2008a).
Models without QG with QG
O2 86.13 86.44 (+0.31, p = 0.06)
O2sib 85.63 86.17 (+0.54, p = 0.003)
O1 83.16 84.40 (+1.24, p < 10?5)
Li11 86.18 ?
Z&N11 86.00 ?
Table 4: Parsing accuracy (UAS) comparison on CTB5-
test with gold-standard POS tags. Li11 refers to the
second-order graph-based model of Li et al (2011),
whereas Z&N11 is the feature-rich transition-based
model of Zhang and Nivre (2011).
At this point, both CTB5 and CTB6 contain depen-
dency structures conforming to the style of CDT.
5.2 CTB5 as the Target Treebank
Table 4 shows the results when the gold-standard
POS tags of CTB5 are adopted by the parsing mod-
els. We aim to analyze the efficacy of QG features
under the ideal scenario wherein the parsing mod-
els suffer from no error propagation of POS tag-
ging. We determine that our baseline O2 model
achieves comparable accuracy with the state-of-the-
art parsers. We also find that QG features can
boost the parsing accuracy by a large margin when
the baseline parser is weak (O1). The improve-
ment shrinks for stronger baselines (O2sib and O2).
This phenomenon is understandable. When gold-
standard POS tags are available, the baseline fea-
tures are very reliable and the QG features becomes
less helpful for more complex models. The p-values
in parentheses present the statistical significance of
the improvements.
We then turn to the more realistic scenario
wherein the gold-standard POS tags of the target
treebank are unavailable. We train a POS tagger on
the training set of CTB5 to produce the automatic
680
Models without QG with QG
O2 79.67 81.04 (+1.37)
O2sib 79.25 80.45 (+1.20)
O1 76.73 79.04 (+2.31)
Li11 joint 80.79 ?
Li11 pipeline 79.29 ?
Table 5: Parsing accuracy (UAS) comparison on CTB5-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p < 10?5).
Setting UAS CM RA
fbs(.) 79.67 26.81 73.82
fqg(.) 79.15 26.34 74.71
fbs(.) + fqg(.) 81.04 29.63 77.17
fbs(.) + fqg-dep(.) 80.82 28.80 76.28
fbs(.) + fqg-sib(.) 80.86 28.48 76.18
fbs(.) + fqg-grd(.) 80.88 28.90 76.34
Table 6: Feature ablation for Parser-O2 on CTB5-test
with automatic POS tags.
POS tags for the development and test sets of CTB5.
The tagging accuracy is 93.88% on the test set. The
automatic POS tags of the training set are produced
using 10-fold cross-validation.9
Table 5 shows the results. We find that QG fea-
tures result in a surprisingly large improvement over
the O1 baseline and can also boost the state-of-
the-art parsing accuracy by a large margin. Li et
al. (2011) show that a joint POS tagging and de-
pendency parsing model can significantly improve
parsing accuracy over a pipeline model. Our QG-
enhanced parser outperforms their best joint model
by 0.25%. Moreover, the QG features can be used to
enhance a joint model and achieve higher accuracy,
which we leave as future work.
5.3 Analysis Using Parser-O2 with AUTO-POS
We then try to gain more insights into the effect of
the QG features through detailed analysis. We se-
lect the state-of-the-art O2 parser and focus on the
realistic scenario with automatic POS tags.
Table 6 compares the efficacy of different feature
sets. The first major row analyzes the efficacy of
9We could use the POS tags produced by TaggerPD in Sec-
tion 5.1, which however would make it difficult to compare our
results with previous ones. Moreover, inferior results may be
gained due to the differences between CTB5 and PD in word
segmentation standards and text sources.
the basic features fbs(.) and the QG features fqg(.).
When using the few QG features in Table 2, the ac-
curacy is very close to that when using the basic
features. Moreover, using both features generates
a large improvement. The second major row com-
pares the efficacy of the three kinds of QG features
corresponding to the three types of scoring parts. We
can see that the three feature sets are similarly effec-
tive and yield comparable accuracies. Combining
these features generate an additional improvement
of approximately 0.2%. These results again demon-
strate that all the proposed QG features are effective.
Figure 5 describes how the performance varies
when the scale of CTB5 and CDT changes. In
the left subfigure, the parsers are trained on part
of the CTB5-train, and ?16? indicates the use of
all the training instances. Meanwhile, the source
parser ParserCDT is trained on the whole CDT-
train. We can see that QG features render larger
improvement when the target treebank is of smaller
scale, which is quite reasonable. More importantly,
the curves indicate that a QG-enhanced parser
trained on a target treebank of 16,000 sentences
may achieve comparable accuracy with a base-
line parser trained on a treebank that is double
the size (32,000), which is very encouraging.
In the right subfigure, the target treebank is
trained on the whole CTB5-train, whereas the source
parser is trained on part of the CDT-train, and ?55.5?
indicates the use of all. The curve clearly demon-
strates that the QG features are more helpful when
the source treebank gets larger, which can be ex-
plained as follows. A larger source treebank can
teach a source parser of higher accuracy; then, the
better source parser can parse the target treebank
more reliably; and finally, the target parser can better
learn the annotation divergences based on QG fea-
tures. These results demonstrate the effectiveness
and stability of our approach.
Table 7 presents the detailed effect of the QG fea-
tures on different dependency patterns. A pattern
?VV ? NN? refers to a right-directed dependency
with the head tagged as ?VV? and the modifier
tagged as ?NN?. whereas ??? means left-directed.
The ?w/o QG? column shows the number of the cor-
responding dependency pattern that appears in the
gold-standard trees but misses in the results of the
baseline parser, whereas the signed figures in the
?+QG? column are the changes made by the QG-
681
71
72
73
74
75
76
77
78
79
80
81
82
1 2 4 8 16
Training Set Size of CTB5
w/o QG
with QG
79.4
79.6
79.8
80
80.2
80.4
80.6
80.8
81
81.2
0 3 6 12 24 55.5
Training Set Size of CDT
with QG
Figure 5: Parsing accuracy (UAS) comparison on CTB5-
test when the scale of CDT and CTB5 varies (thousands
in sentence number).
Dependency w/o QG +QG Descriptions
NN? NN 858 -78 noun modifier or coordinating nouns
VV? VV 777 -41 object clause or coordinating verbs
VV? VV 570 -38 subject clause
VV? NN 509 -79 verb and its object
w0 ? VV 357 -57 verb as sentence root
VV? NN 328 -32 attributive clause
P? VV 278 -37 preposition phrase attachment
VV? DEC 233 -33 attributive clause and auxiliary DE
P? NN 175 -35 preposition and its object
Table 7: Detailed effect of QG features on different de-
pendency patterns.
enhanced parser. We only list the patterns with an
absolute change larger than 30. We find that the QG
features can significantly help a variety of depen-
dency patterns (i.e., reducing the missing number).
5.4 CTB6 as the Target Treebank
We use CTB6 as the target treebank to further verify
the efficacy of our approach. Compared with CTB5,
CTB6 is of larger scale and is converted into de-
pendency structures according to finer-grained head-
finding rules (Hajic? et al, 2009). We directly adopt
the same transformation patterns and features tuned
on CTB5. Table 8 shows results. The improvements
are similar to those on CTB5, demonstrating that our
approach is effective and robust. We list the top three
systems of the CoNLL 2009 shared task in Table 8,
showing that our approach also advances the state-
of-the-art parsing accuracy on this data set.10
10We reproduce their UASs using the data released
by the organizer: http://ufal.mff.cuni.cz/conll2009-st/results/
results.php. The parsing accuracies of the top systems may be
underestimated since the accuracy of the provided POS tags in
CoNLL 2009 is only 92.38% on the test set, while the POS tag-
ger used in our experiments reaches 94.08%.
Models without QG with QG
O2 83.23 84.33 (+1.10)
O2sib 82.87 84.11 (+1.37)
O1 80.29 82.76 (+2.47)
Bohnet (2009) 82.68 ?
Che et al (2009) 82.11 ?
Gesmundo et al (2009) 81.70 ?
Table 8: Parsing accuracy (UAS) comparison on CTB6-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p < 10?5).
Models baseline with another treebank
Ours 84.16 86.67 (+2.51)
GP (Niu et al, 2009) 82.42 84.06 (+1.64)
Table 9: Parsing accuracy (UAS) comparison on the test
set of CTB5X. Niu et al (2009) use the maximum en-
tropy inspired generative parser (GP) of Charniak (2000)
as their constituent parser.
5.5 Comparison with Treebank Conversion
As discussed in Section 2, Niu et al (2009) automat-
ically convert the dependency-structure CDT to the
phrase-structure annotation style of CTB5X and use
the converted treebank as additional labeled data.
We convert their phrase-structure results on CTB5X-
test into dependency structures using the same head-
finding rules. To compare with their results, we
run our baseline and QG-enhanced O2 parsers on
CTB5X. Table 9 presents the results.11 The indirect
comparison indicates that our approach can achieve
larger improvement than their treebank conversion
based method.
6 Conclusions
The current paper proposes a simple and effective
framework for exploiting multiple large-scale tree-
banks of different annotation styles. We design
rich TPs to model the annotation inconsistencies and
consequently propose QG features based on these
TPs. Extensive experiments show that our approach
can effectively utilize the syntactic knowledge from
another treebank and significantly improve the state-
of-the-art parsing accuracy.
11We thank the authors for sharing their results. Niu et al
(2009) also use the reranker (RP) of Charniak and Johnson
(2005) as a stronger baseline, but the results are missing. They
find a less improvement on F score with RP than with GP (0.9%
vs. 1.1%). We refer to their Table 5 and 6 for details.
682
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Major Projects via
grant 2011AA01A207, and the National ?863?
Leading Technology Research Project via grant
2012AA011102.
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 693?702, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Bernd Bohnet. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceedings
of the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2009): Shared Task,
pages 67?72, Boulder, Colorado, June. Association for
Computational Linguistics.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 877?886, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 46?54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ANLP?00, pages 132?139.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL 2009: Shared Task, pages 49?
54.
Keh-Jiann Chen, Chi-Ching Luo, Ming-Chung Chang,
Feng-Yi Chen, Chao-Jan Chen, Chu-Ren Huang, and
Zhao-Ming Gao, 2003. Sinica treebank: Design crite-
ria,representational issues and implementation, chap-
ter 13, pages 231?248. Kluwer Academic Publishers.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
570?579, Singapore, August. Association for Compu-
tational Linguistics.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 21?29, Uppsala, Sweden, July. Association
for Computational Linguistics.
Micheal Collins, Lance Ramshaw, Jan Hajic, and
Christoph Tillmann. 1999. A statistical parser for
czech. In ACL 1999, pages 505?512.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of CoNLL 2009: Shared Task,
pages 37?42.
Kevin Gimpel and Noah A. Smith. 2011. Quasi-
synchronous phrase dependency grammars for ma-
chine translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 474?485, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1222?1231, Singapore, August. Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 522?530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
683
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models
for chinese pos tagging and dependency parsing. In
EMNLP 2011, pages 1180?1191.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building
a dependency treebank for improving Chinese parser.
In Journal of Chinese Language and Computing, vol-
ume 16, pages 207?224.
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
EMNLP?08, pages 157?166.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46?54, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL 2008, pages 950?958.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: An introduction. John Wiley &
Sons, Inc., New York. Book (ISBN 0471611360 ).
Zhou Qiang. 2004. Annotation scheme for chinese tree-
bank. Journal of Chinese Information Processing,
18(4):1?8.
David Smith and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntac-
tic dependencies. In Proceedings on the Workshop
on Statistical Machine Translation, pages 23?30, New
York City, June. Association for Computational Lin-
guistics.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 822?831, Singapore, August. Association for
Computational Linguistics.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 409?420, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer,
and Dipti Misra. Sharma. 2008. Towards a multi-
representational treebank. In In Proceedings of the 7th
International Workshop on Treebanks and Linguistic
Theories.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1556?1565, Portland, Oregon, USA,
June. Association for Computational Linguistics.
684
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457?467,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Ambiguity-aware Ensemble Training for Semi-supervised
Dependency Parsing
Zhenghua Li , Min Zhang?, Wenliang Chen
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University
{zhli13,minzhang,wlchen}@suda.edu.cn
Abstract
This paper proposes a simple yet
effective framework for semi-supervised
dependency parsing at entire tree level,
referred to as ambiguity-aware ensemble
training. Instead of only using 1-
best parse trees in previous work, our
core idea is to utilize parse forest
(ambiguous labelings) to combine
multiple 1-best parse trees generated
from diverse parsers on unlabeled data.
With a conditional random field based
probabilistic dependency parser, our
training objective is to maximize mixed
likelihood of labeled data and auto-parsed
unlabeled data with ambiguous labelings.
This framework offers two promising
advantages. 1) ambiguity encoded in
parse forests compromises noise in 1-best
parse trees. During training, the parser is
aware of these ambiguous structures, and
has the flexibility to distribute probability
mass to its preferred parse trees as long
as the likelihood improves. 2) diverse
syntactic structures produced by different
parsers can be naturally compiled into
forest, offering complementary strength
to our single-view parser. Experimental
results on benchmark data show that
our method significantly outperforms
the baseline supervised parser and
other entire-tree based semi-supervised
methods, such as self-training, co-training
and tri-training.
1 Introduction
Supervised dependency parsing has made great
progress during the past decade. However, it
is very difficult to further improve performance
?Correspondence author
of supervised parsers. For example, Koo and
Collins (2010) and Zhang and McDonald (2012)
show that incorporating higher-order features into
a graph-based parser only leads to modest increase
in parsing accuracy. In contrast, semi-supervised
approaches, which can make use of large-scale
unlabeled data, have attracted more and more
interest. Previously, unlabeled data is explored to
derive useful local-context features such as word
clusters (Koo et al, 2008), subtree frequencies
(Chen et al, 2009; Chen et al, 2013), and word
co-occurrence counts (Zhou et al, 2011; Bansal
and Klein, 2011). A few effective learning meth-
ods are also proposed for dependency parsing to
implicitly utilize distributions on unlabeled data
(Smith and Eisner, 2007; Wang et al, 2008;
Suzuki et al, 2009). All above work leads to
significant improvement on parsing accuracy.
Another line of research is to pick up some
high-quality auto-parsed training instances from
unlabeled data using bootstrapping methods, such
as self-training (Yarowsky, 1995), co-training
(Blum and Mitchell, 1998), and tri-training (Zhou
and Li, 2005). However, these methods gain
limited success in dependency parsing. Although
working well on constituent parsing (McClosky et
al., 2006; Huang and Harper, 2009), self-training
is shown unsuccessful for dependency parsing
(Spreyer and Kuhn, 2009). The reason may be that
dependency parsing models are prone to amplify
previous mistakes during training on self-parsed
unlabeled data. Sagae and Tsujii (2007) apply
a variant of co-training to dependency parsing
and report positive results on out-of-domain text.
S?gaard and Rish?j (2010) combine tri-training
and parser ensemble to boost parsing accuracy.
Both work employs two parsers to process the
unlabeled data, and only select as extra training
data sentences on which the 1-best parse trees of
the two parsers are identical. In this way, the auto-
parsed unlabeled data becomes more reliable.
457
w0
He
1
saw
2
a
3
deer
4
riding
5
a
6
bicycle
7
in
8
the
9
park
10
.
11
Figure 1: An example sentence with an ambiguous parse forest.
However, one obvious drawback of these methods
is that they are unable to exploit unlabeled data
with divergent outputs from different parsers.
Our experiments show that unlabeled data with
identical outputs from different parsers tends to be
short (18.25 words per sentence on average), and
only has a small proportion of 40% (see Table 6).
More importantly, we believe that unlabeled data
with divergent outputs is equally (if not more)
useful. Intuitively, an unlabeled sentence with
divergent outputs should contain some ambiguous
syntactic structures (such as preposition phrase
attachment) that are very hard to resolve and
lead to the disagreement of different parsers.
Such sentences can provide more discriminative
instances for training which may be unavailable
in labeled data.
To solve above issues, this paper proposes
a more general and effective framework for
semi-supervised dependency parsing, referred to
as ambiguity-aware ensemble training. Different
from traditional self/co/tri-training which only use
1-best parse trees on unlabeled data, our approach
adopts ambiguous labelings, represented by parse
forest, as gold-standard for unlabeled sentences.
Figure 1 shows an example sentence with an
ambiguous parse forest. The forest is formed by
two parse trees, respectively shown at the upper
and lower sides of the sentence. The differences
between the two parse trees are highlighted
using dashed arcs. The upper tree take ?deer?
as the subject of ?riding?, whereas the lower
one indicates that ?he? rides the bicycle. The
other difference is where the preposition phrase
(PP) ?in the park? should be attached, which
is also known as the PP attachment problem, a
notorious challenge for parsing. Reserving such
uncertainty has three potential advantages. First,
noise in unlabeled data is largely alleviated, since
parse forest encodes only a few highly possible
parse trees with high oracle score. Please note
that the parse forest in Figure 1 contains four
parse trees after combination of the two different
choices. Second, the parser is able to learn useful
features from the unambiguous parts of the parse
forest. Finally, with sufficient unlabeled data, it is
possible that the parser can learn to resolve such
uncertainty by biasing to more reasonable parse
trees.
To construct parse forest on unlabeled data, we
employ three supervised parsers based on different
paradigms, including our baseline graph-based
dependency parser, a transition-based dependency
parser (Zhang and Nivre, 2011), and a generative
constituent parser (Petrov and Klein, 2007). The
1-best parse trees of these three parsers are aggre-
gated in different ways. Evaluation on labeled data
shows the oracle accuracy of parse forest is much
higher than that of 1-best outputs of single parsers
(see Table 3). Finally, using a conditional random
field (CRF) based probabilistic parser, we train
a better model by maximizing mixed likelihood
of labeled data and auto-parsed unlabeled data
with ambiguous labelings. Experimental results
on both English and Chinese datasets demon-
strate that the proposed ambiguity-aware ensem-
ble training outperforms other entire-tree based
methods such as self/co/tri-training. In summary,
we make following contributions.
1. We propose a generalized ambiguity-aware
ensemble training framework for semi-
supervised dependency parsing, which can
458
make better use of unlabeled data, especially
when parsers from different views produce
divergent syntactic structures.
2. We first employ a generative constituent pars-
er for semi-supervised dependency parsing.
Experiments show that the constituent parser
is very helpful since it produces more diver-
gent structures for our semi-supervised parser
than discriminative dependency parsers.
3. We build the first state-of-the-art CRF-based
dependency parser. Using the probabilistic
parser, we benchmark and conduct systemat-
ic comparisons among ours and all previous
bootstrapping methods, including self/co/tri-
training.
2 Supervised Dependency Parsing
Given an input sentence x = w
0
w
1
...w
n
, the goal
of dependency parsing is to build a dependency
tree as depicted in Figure 1, denoted by d =
{(h,m) : 0 ? h ? n, 0 < m ? n}, where (h,m)
indicates a directed arc from the head word w
h
to the modifier w
m
, and w
0
is an artificial node
linking to the root of the sentence.
In parsing community, two mainstream meth-
ods tackle the dependency parsing problem from
different perspectives but achieve comparable ac-
curacy on a variety of languages. The graph-
based method views the problem as finding an
optimal tree from a fully-connected directed graph
(McDonald et al, 2005; McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010),
while the transition-based method tries to find a
highest-scoring transition sequence that leads to
a legal dependency tree (Yamada and Matsumoto,
2003; Nivre, 2003; Zhang and Nivre, 2011).
2.1 Graph-based Dependency Parser
(GParser)
In this work, we adopt the graph-based paradigm
because it allows us to naturally derive conditional
probability of a dependency tree d given a sen-
tence x, which is required to compute likelihood
of both labeled and unlabeled data. Under the
graph-based model, the score of a dependency tree
is factored into the scores of small subtrees p.
Score(x,d;w) = w ? f(x,d)
=
?
p?d
Score(x,p;w)
h m
(a) single dependency
h s
(b) adjacent sibling
m
Figure 2: Two types of scoring subtrees in our
second-order graph-based parsers.
Dependency features f
dep
(x, h,m):
w
h
, w
m
, t
h
, t
m
, t
h?1
, t
m?1
, t
b
, dir(h,m), dist(h,m)
Sibling features f
sib
(x, h,m, s):
w
h
, w
s
, w
m
, t
h
, t
m
, t
s
, t
h?1
, t
m?1
, t
s?1
dir(h,m), dist(h,m)
Table 1: Brief illustration of the syntactic features.
t
i
denotes the POS tag of w
i
. b is an index
between h and m. dir(i, j) and dist(i, j) denote
the direction and distance of the dependency (i, j).
We adopt the second-order graph-based depen-
dency parsing model of McDonald and Pereira
(2006) as our core parser, which incorporates
features from the two kinds of subtrees in Fig. 2.1
Then the score of a dependency tree is:
Score(x,d;w) =
?
{(h,m)}?d
w
dep
? f
dep
(x, h,m)
+
?
{(h,s),(h,m)}?d
w
sib
? f
sib
(x, h, s,m)
where f
dep
(x, h,m) and f
sib
(x, h, s,m) are the
feature vectors of the two subtree in Fig. 2;
w
dep/sib
are feature weight vectors; the dot prod-
uct gives scores contributed by corresponding sub-
trees.
For syntactic features, we adopt those of Bohnet
(2010) which include two categories correspond-
ing to the two types of scoring subtrees in Fig. 2.
We summarize the atomic features used in each
feature category in Table 1. These atomic features
are concatenated in different combinations to com-
pose rich feature sets. Please refer to Table 4 of
Bohnet (2010) for the complete feature list.
2.2 CRF-based GParser
Previous work on graph-based dependency pars-
ing mostly adopts linear models and perceptron
based training procedures, which lack probabilis-
tic explanations of dependency trees and do not
need to compute likelihood of labeled training
1Higher-order models of Carreras (2007) and Koo and
Collins (2010) can achieve higher accuracy, but has much
higher time cost (O(n4)). Our approach is applicable to these
higher-order models, which we leave for future work.
459
data. Instead, we build a log-linear CRF-based
dependency parser, which is similar to the CRF-
based constituent parser of Finkel et al (2008).
Assuming the feature weights w are known, the
probability of a dependency tree d given an input
sentence x is defined as:
p(d|x;w) =
exp{Score(x,d;w)}
Z(x;w)
Z(x;w) =
?
d
?
?Y(x)
exp{Score(x,d
?
;w)}
(1)
where Z(x) is the normalization factor and Y(x)
is the set of all legal dependency trees for x.
Suppose the labeled training data is
D = {(x
i
,d
i
)}
N
i=1
. Then the log likelihood
of D is:
L(D;w) =
N
?
i=1
log p(d
i
|x
i
;w)
The training objective is to maximize the log
likelihood of the training data L(D). The partial
derivative with respect to the feature weights w is:
?L(D;w)
?w
=
N
?
i=1
?
?
?
f(x
i
,d
i
) ?
?
d
?
?Y(x
i
)
p(d
?
|x
i
;w)f(x
i
,d
?
)
?
?
?
(2)
where the first term is the empirical counts and
the second term is the model expectations. Since
Y(x
i
) contains exponentially many dependency
trees, direct calculation of the second term is
prohibitive. Instead, we can use the classic inside-
outside algorithm to efficiently compute the model
expectations within O(n3) time complexity, where
n is the input sentence length.
3 Ambiguity-aware Ensemble Training
In standard entire-tree based semi-supervised
methods such as self/co/tri-training, automatically
parsed unlabeled sentences are used as additional
training data, and noisy 1-best parse trees are
considered as gold-standard. To alleviate the
noise, the tri-training method only uses unlabeled
data on which multiple parsers from different
views produce identical parse trees. However,
unlabeled data with divergent syntactic structures
should be more useful. Intuitively, if several
parsers disagree on an unlabeled sentence, it
implies that the unlabeled sentence contains
some difficult syntactic phenomena which are
not sufficiently covered in manually labeled
data. Therefore, exploiting such unlabeled data
may introduce more discriminative syntactic
knowledge, largely compensating labeled training
data.
To address above issues, we propose ambiguity-
aware ensemble training, which can be interpreted
as a generalized tri-training framework. The key
idea is the use of ambiguous labelings for the
purpose of aggregating multiple 1-best parse trees
produced by several diverse parsers. Here, ?am-
biguous labelings? mean an unlabeled sentence
may have multiple parse trees as gold-standard
reference, represented by parse forest (see Figure
1). The training procedure aims to maximize
mixed likelihood of both manually labeled and
auto-parsed unlabeled data with ambiguous label-
ings. For an unlabeled instance, the model is
updated to maximize the probability of its parse
forest, instead of a single parse tree in traditional
tri-training. In other words, the model is free to
distribute probability mass among the trees in the
parse forest to its liking, as long as the likelihood
improves (Ta?ckstro?m et al, 2013).
3.1 Likelihood of the Unlabeled Data
The auto-parsed unlabeled data with ambiguous
labelings is denoted as D? = {(u
i
,V
i
)}
M
i=1
, where
u
i
is an unlabeled sentence, and V
i
is the corre-
sponding parse forest. Then the log likelihood of
D
? is:
L(D
?
;w) =
M
?
i=1
log
?
?
?
d
?
?V
i
p(d
?
|u
i
;w)
?
?
where p(d?|u
i
;w) is the conditional probability of
d
? given u
i
, as defined in Eq. (1). For an unlabeled
sentence u
i
, the probability of its parse forest V
i
is
the summation of the probabilities of all the parse
trees contained in the forest.
Then we can derive the partial derivative of the
log likelihood with respect to w:
?L(D
?
;w)
?w
=
M
?
i=1
?
?
?
?
d
?
?V
i
p?(d
?
|u
i
,V
i
;w)f(u
i
,d
?
)
?
?
d
?
?Y(u
i
)
p(d
?
|u
i
;w)f(u
i
,d
?
)
?
?
?
(3)
where p?(d?|u
i
,V
i
;w) is the probability of d? un-
460
der the space constrained by the parse forest V
i
.
p?(d
?
|u
i
,V
i
;w) =
exp{Score(u
i
,d
?
;w)}
Z(u
i
,V
i
;w)
Z(u
i
,V
i
;w) =
?
d
?
?V
i
exp{Score(u
i
,d
?
;w)}
The second term in Eq. (3) is the same with the
second term in Eq. (2). The first term in Eq. (3)
can be efficiently computed by running the inside-
outside algorithm in the constrained search space
V
i
.
3.2 Stochastic Gradient Descent (SGD)
Training
We apply L2-norm regularized SGD training to
iteratively learn feature weights w for our CRF-
based baseline and semi-supervised parsers. We
follow the implementation in CRFsuite.2 At each
step, the algorithm approximates a gradient with
a small subset of the training examples, and then
updates the feature weights. Finkel et al (2008)
show that SGD achieves optimal test performance
with far fewer iterations than other optimization
routines such as L-BFGS. Moreover, it is very
convenient to parallel SGD since computations
among examples in the same batch is mutually
independent.
Training with the combined labeled and unla-
beled data, the objective is to maximize the mixed
likelihood:
L(D;D
?
) = L(D) + L(D
?
)
Since D? contains much more instances than D
(1.7M vs. 40K for English, and 4M vs. 16K for
Chinese), it is likely that the unlabeled data may
overwhelm the labeled data during SGD training.
Therefore, we propose a simple corpus-weighting
strategy, as shown in Algorithm 1, where Db
i,k
is the subset of training data used in kth update
and b is the batch size; ?
k
is the update step,
which is adjusted following the simulated anneal-
ing procedure (Finkel et al, 2008). The idea is
to use a fraction of training data (D
i
) at each
iteration, and do corpus weighting by randomly
sampling labeled and unlabeled instances in a
certain proportion (N
1
vs. M
1
).
Once the feature weights w are learnt, we can
2http://www.chokkan.org/software/crfsuite/
Algorithm 1 SGD training with mixed labeled and
unlabeled data.
1: Input: Labeled data D = {(x
i
,d
i
)}
N
i=1
, and unlabeled
data D? = {(u
i
,V
i
)}
M
j=1
; Parameters: I , N
1
, M
1
, b
2: Output: w
3: Initialization: w(0) = 0, k = 0;
4: for i = 1 to I do {iterations}
5: Randomly select N
1
instances from D and M
1
instances from D? to compose a new dataset D
i
, and
shuffle it.
6: Traverse D
i
: a small batch Db
i,k
? D
i
at one step.
7: w
k+1
= w
k
+ ?
k
1
b
?L(D
b
i,k
;w
k
)
8: k = k + 1
9: end for
parse the test data to find the optimal parse tree.
d
?
= arg max
d
?
?Y(x)
p(d
?
|x;w)
= arg max
d
?
?Y(x)
Score(x,d
?
;w)
This can be done with the Viterbi decoding algo-
rithm described in McDonald and Pereira (2006)
in O(n3) parsing time.
3.3 Forest Construction with Diverse Parsers
To construct parse forests for unlabeled data, we
employ three diverse parsers, i.e., our baseline
GParser, a transition-based parser (ZPar3) (Zhang
and Nivre, 2011), and a generative constituen-
t parser (Berkeley Parser4) (Petrov and Klein,
2007). These three parsers are trained on labeled
data and then used to parse each unlabeled sen-
tence. We aggregate the three parsers? outputs on
unlabeled data in different ways and evaluate the
effectiveness through experiments.
4 Experiments and Analysis
To verify the effectiveness of our proposed ap-
proach, we conduct experiments on Penn Tree-
bank (PTB) and Penn Chinese Treebank 5.1 (CT-
B5). For English, we follow the popular practice
to split data into training (sections 2-21), devel-
opment (section 22), and test (section 23). For
CTB5, we adopt the data split of (Duan et al,
2007). We convert original bracketed structures
into dependency structures using Penn2Malt with
its default head-finding rules.
For unlabeled data, we follow Chen et al (2013)
and use the BLLIP WSJ corpus (Charniak et al,
2000) for English and Xinhua portion of Chinese
3http://people.sutd.edu.sg/
?
yue_zhang/doc/
4https://code.google.com/p/berkeleyparser/
461
Train Dev Test Unlabeled
PTB 39,832 1,700 2,416 1.7M
CTB5 16,091 803 1,910 4M
Table 2: Data sets (in sentence number).
Gigaword Version 2.0 (LDC2009T14) (Huang,
2009) for Chinese. We build a CRF-based bigram
part-of-speech (POS) tagger with the features de-
scribed in (Li et al, 2012), and produce POS tags
for all train/development/test/unlabeled sets (10-
way jackknifing for training sets). The tagging ac-
curacy on test sets is 97.3% on English and 94.0%
on Chinese. Table 2 shows the data statistics.
We measure parsing performance using the s-
tandard unlabeled attachment score (UAS), ex-
cluding punctuation marks. For significance test,
we adopt Dan Bikel?s randomized parsing evalua-
tion comparator (Noreen, 1989).5
4.1 Parameter Setting
When training our CRF-based parsers with SGD,
we use the batch size b = 100 for all experiments.
We run SGD for I = 100 iterations and choose
the model that performs best on development
data. For the semi-supervised parsers trained with
Algorithm 1, we use N
1
= 20K and M
1
= 50K
for English, and N
1
= 15K and M
1
= 50K for
Chinese, based on a few preliminary experiments.
To accelerate the training, we adopt parallelized
implementation of SGD and employ 20 threads for
each run. For semi-supervised cases, one iteration
takes about 2 hours on an IBM server having 2.0
GHz Intel Xeon CPUs and 72G memory.
Default parameter settings are used for training
ZPar and Berkeley Parser. We run ZPar for 50
iterations, and choose the model that achieves
highest accuracy on the development data. For
Berkeley Parser, we use the model after 5 split-
merge iterations to avoid over-fitting the train-
ing data according to the manual. The phrase-
structure outputs of Berkeley Parser are converted
into dependency structures using the same head-
finding rules.
4.2 Methodology Study on Development Data
Using three supervised parsers, we have many
options to construct parse forest on unlabeled data.
To examine the effect of different ways for forest
construction, we conduct extensive methodology
study on development data. Table 3 presents the
5http://www.cis.upenn.edu/
?
dbikel/software.html
results. We divide the systems into three types: 1)
supervised single parsers; 2) CRF-based GParser
with conventional self/co/tri-training; 3) CRF-
based GParser with our approach. For the latter
two cases, we also present the oracle accuracy and
averaged head number per word (?Head/Word?)
of parse forest when applying different ways to
construct forests on development datasets.
The first major row presents performance of
the three supervised parsers. We can see that the
three parsers achieve comparable performance on
English, but the performance of ZPar is largely
inferior on Chinese.
The second major row shows the results when
we use single 1-best parse trees on unlabeled
data. When using the outputs of GParser itself
(?Unlabeled ? G?), the experiment reproduces
traditional self-training. The results on both En-
glish and Chinese re-confirm that self-training
may not work for dependency parsing, which
is consistent with previous studies (Spreyer and
Kuhn, 2009). The reason may be that dependency
parsers are prone to amplify previous mistakes on
unlabeled data during training.
The next two experiments in the second ma-
jor row reimplement co-training, where another
parser?s 1-best results are projected into unlabeled
data to help the core parser. Using unlabeled
data with the results of ZPar (?Unlabeled ? Z?)
significantly outperforms the baseline GParser by
0.30% (93.15-82.85) on English. However, the
improvement on Chinese is not significant. Using
unlabeled data with the results of Berkeley Parser
(?Unlabeled? B?) significantly improves parsing
accuracy by 0.55% (93.40-92.85) on English and
1.06% (83.34-82.28) on Chinese. We believe the
reason is that being a generative model designed
for constituent parsing, Berkeley Parser is more
different from discriminative dependency parsers,
and therefore can provide more divergent syntactic
structures. This kind of syntactic divergence is
helpful because it can provide complementary
knowledge from a different perspective. Surdeanu
and Manning (2010) also show that the diversity of
parsers is important for performance improvement
when integrating different parsers in the super-
vised track. Therefore, we can conclude that
co-training helps dependency parsing, especially
when using a more divergent parser.
The last experiment in the second major row
is known as tri-training, which only uses unla-
462
English Chinese
UAS Oracle Head/Word UAS Oracle Head/Word
GParser 92.85
? ?
82.28
? ?Supervised ZPar 92.50 81.04
Berkeley 92.70 82.46
Unlabeled? G (self-train) 92.88 92.85
1.000
82.14 82.28
1.000Semi-supervised GParser Unlabeled? Z (co-train) 93.15 ? 92.50 82.54 81.04
with Single 1-best Trees Unlabeled? B (co-train) 93.40 ? 92.70 83.34 ? 82.46
Unlabeled? B=Z (tri-train) 93.50 ? 97.52 83.10 ? 95.05
Unlabeled? Z+G 93.18 ? 94.97 1.053 82.78 86.66 1.136
Unlabeled? B+G 93.35 ? 96.37 1.080 83.24 ? 89.72 1.188
Semi-supervised GParser Unlabeled? B+Z 93.78 ?? 96.18 1.082 83.86 ?? 89.54 1.199
Ambiguity-aware Ensemble Unlabeled? B+(Z?G) 93.77 ?? 95.60 1.050 84.26 ?? 87.76 1.106
Unlabeled? B+Z+G 93.50 ? 96.95 1.112 83.30 ? 91.50 1.281
Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
? means the corresponding parser significantly outperforms supervised parsers, and ? means the result
significantly outperforms co/tri-training at confidence level of p < 0.01.
beled sentences on which Berkeley Parser and
ZPar produce identical outputs (?Unlabeled ?
B=Z?). We can see that with the verification of
two views, the oracle accuracy is much higher
than using single parsers (97.52% vs. 92.85% on
English, and 95.06% vs. 82.46% on Chinese).
Although using less unlabeled sentences (0.7M
for English and 1.2M for Chinese), tri-training
achieves comparable performance to co-training
(slightly better on English and slightly worse on
Chinese).
The third major row shows the results of
the semi-supervised GParser with our proposed
approach. We experiment with different com-
binations of the 1-best parse trees of the three
supervised parsers. The first three experiments
combine 1-best outputs of two parsers to compose
parse forest on unlabeled data. ?Unlabeled ?
B+(Z?G)? means that the parse forest is initialized
with the Berkeley parse and augmented with the
intersection of dependencies of the 1-best outputs
of ZPar and GParser. In the last setting, the parse
forest contains all three 1-best results.
When the parse forests of the unlabeled data
are the union of the outputs of GParser and ZPar,
denoted as ?Unlabeled ? Z+G?, each word has
1.053 candidate heads on English and 1.136 on
Chinese, and the oracle accuracy is higher than
using 1-best outputs of single parsers (94.97%
vs. 92.85% on English, 86.66% vs. 82.46%
on Chinese). However, we find that although
the parser significantly outperforms the supervised
GParser on English, it does not gain significant im-
provement over co-training with ZPar (?Unlabeled
? Z?) on both English and Chinese.
Combining the outputs of Berkeley Parser and
GParser (?Unlabeled ? B+G?), we get higher
oracle score (96.37% on English and 89.72% on
Chinese) and higher syntactic divergence (1.085
candidate heads per word on English, and 1.188
on Chinese) than ?Unlabeled ? Z+G?, which
verifies our earlier discussion that Berkeley Pars-
er produces more different structures than ZPar.
However, it leads to slightly worse accuracy than
co-training with Berkeley Parser (?Unlabeled ?
B?). This indicates that adding the outputs of
GParser itself does not help the model.
Combining the outputs of Berkeley Parser and
ZPar (?Unlabeled ? B+Z?), we get the best per-
formance on English, which is also significantly
better than both co-training (?Unlabeled ? B?)
and tri-training (?Unlabeled ? B=Z?) on both
English and Chinese. This demonstrates that our
proposed approach can better exploit unlabeled
data than traditional self/co/tri-training. More
analysis and discussions are in Section 4.4.
During experimental trials, we find that ?Unla-
beled?B+(Z?G)? can further boost performance
on Chinese. A possible explanation is that by
using the intersection of the outputs of GParser
and ZPar, the size of the parse forest is better
controlled, which is helpful considering that ZPar
performs worse on this data than both Berkeley
Parser and GParser.
Adding the output of GParser itself (?Unlabeled
? B+Z+G?) leads to accuracy drop, although the
oracle score is higher (96.95% on English and
91.50% on Chinese) than ?Unlabeled ? B+Z?.
We suspect the reason is that the model is likely to
distribute the probability mass to these parse trees
produced by itself instead of those by Berkeley
Parser or ZPar under this setting.
463
Sup Semi
McDonald and Pereira (2006) 91.5
?Koo and Collins (2010) [higher-order] 93.04
Zhang and McDonald (2012) [higher-order] 93.06
Zhang and Nivre (2011) [higher-order] 92.9
Koo et al (2008) [higher-order] 92.02 93.16
Chen et al (2009) [higher-order] 92.40 93.16
Suzuki et al (2009) [higher-order,cluster] 92.70 93.79
Zhou et al (2011) [higher-order] 91.98 92.64
Chen et al (2013) [higher-order] 92.76 93.77
This work 92.34 93.19
Table 4: UAS comparison on English test data.
In summary, we can conclude that our proposed
ambiguity-aware ensemble training is significant-
ly better than both the supervised approaches and
the semi-supervised approaches that use 1-best
parse trees. Appropriately composing the forest
parse, our approach outperforms the best results of
co-training or tri-training by 0.28% (93.78-93.50)
on English and 0.92% (84.26-83.34) on Chinese.
4.3 Comparison with Previous Work
We adopt the best settings on development data
for semi-supervised GParser with our proposed
approach, and make comparison with previous
results on test data. Table 4 shows the results.
The first major row lists several state-of-the-
art supervised methods. McDonald and Pereira
(2006) propose a second-order graph-based parser,
but use a smaller feature set than our work. Koo
and Collins (2010) propose a third-order graph-
based parser. Zhang and McDonald (2012) ex-
plore higher-order features for graph-based de-
pendency parsing, and adopt beam search for
fast decoding. Zhang and Nivre (2011) propose
a feature-rich transition-based parser. All work
in the second major row adopts semi-supervised
methods. The results show that our approach
achieves comparable accuracy with most previous
semi-supervised methods. Both Suzuki et al
(2009) and Chen et al (2013) adopt the higher-
order parsing model of Carreras (2007), and Suzu-
ki et al (2009) also incorporate word cluster
features proposed by Koo et al (2008) in their sys-
tem. We expect our approach may achieve higher
performance with such enhancements, which we
leave for future work. Moreover, our method
may be combined with other semi-supervised ap-
proaches, since they are orthogonal in method-
ology and utilize unlabeled data from different
perspectives.
Table 5 make comparisons with previous results
UAS
Supervised
Li et al (2012) [joint] 82.37
Bohnet and Nivre (2012) [joint] 81.42
Chen et al (2013) [higher-order] 81.01
This work 81.14
Semi Chen et al (2013) [higher-order] 83.08This work 82.89
Table 5: UAS comparison on Chinese test data.
Unlabeled data UAS #Sent Len Head/Word Oracle
NULL 92.34 0 ? ? ?
Consistent (tri-train) 92.94 0.7M 18.25 1.000 97.65
Low divergence 92.94 0.5M 28.19 1.062 96.53
High divergence 93.03 0.5M 27.85 1.211 94.28
ALL 93.19 1.7M 24.15 1.087 96.09
Table 6: Performance of our semi-supervised
GParser with different sets of ?Unlabeled ?
B+Z? on English test set. ?Len? means averaged
sentence length.
on Chinese test data. Li et al (2012) and Bohnet
and Nivre (2012) use joint models for POS tagging
and dependency parsing, significantly outperform-
ing their pipeline counterparts. Our approach can
be combined with their work to utilize unlabeled
data to improve both POS tagging and parsing
simultaneously. Our work achieves comparable
accuracy with Chen et al (2013), although they
adopt the higher-order model of Carreras (2007).
Again, our method may be combined with their
work to achieve higher performance.
4.4 Analysis
To better understand the effectiveness of our pro-
posed approach, we make detailed analysis using
the semi-supervised GParser with ?Unlabeled ?
B+Z? on English datasets.
Contribution of unlabeled data with regard
to syntactic divergence: We divide the unlabeled
data into three sets according to the divergence of
the 1-best outputs of Berkeley Parser and ZPar.
The first set contains those sentences that the two
parsers produce identical parse trees, denoted by
?consistent?, which corresponds to the setting for
tri-training. Other sentences are split into two sets
according to averaged number of heads per word
in parse forests, denoted by ?low divergence? and
?high divergence? respectively. Then we train
semi-supervised GParser using the three sets of
unlabeled data. Table 6 illustrates the results and
statistics. We can see that unlabeled data with
identical outputs from Berkeley Parser and ZPar
tends to be short sentences (18.25 words per sen-
464
tence on average). Results show all the three sets
of unlabeled data can help the parser. Especially,
the unlabeled data with highly divergent struc-
tures leads to slightly higher improvement. This
demonstrates that our approach can better exploit
unlabeled data on which parsers of different views
produce divergent structures.
Impact of unlabeled data size: To under-
stand how our approach performs with regards to
the unlabeled data size, we train semi-supervised
GParser with different sizes of unlabeled data. Fig.
3 shows the accuracy curve on the test set. We
can see that the parser consistently achieves higher
accuracy with more unlabeled data, demonstrating
the effectiveness of our approach. We expect
that our approach has potential to achieve higher
accuracy with more additional data.
 92.3
 92.4
 92.5
 92.6
 92.7
 92.8
 92.9
 93
 93.1
 93.2
0 50K 100K 200K 500K 1M 1.7M
UA
S
Unlabeled Data Size
B+Z Parser
Figure 3: Performance of GParser with different
sizes of ?Unlabeled? B+Z? on English test set.
5 Related Work
Our work is originally inspired by the work of
Ta?ckstro?m et al (2013). They first apply the
idea of ambiguous labelings to multilingual parser
transfer in the unsupervised parsing field, which
aims to build a dependency parser for a resource-
poor target language by making use of source-
language treebanks. Different from their work, we
explore the idea for semi-supervised dependency
parsing where a certain amount of labeled training
data is available. Moreover, we for the first
time build a state-of-the-art CRF-based depen-
dency parser and conduct in-depth comparisons
with previous methods. Similar ideas of learning
with ambiguous labelings are previously explored
for classification (Jin and Ghahramani, 2002) and
sequence labeling problems (Dredze et al, 2009).
Our work is also related with the parser ensem-
ble approaches such as stacked learning and re-
parsing in the supervised track. Stacked learning
uses one parser?s outputs as guide features for
another parser, leading to improved performance
(Nivre and McDonald, 2008; Torres Martins et
al., 2008). Re-parsing merges the outputs of
several parsers into a dependency graph, and then
apply Viterbi decoding to find a better tree (Sagae
and Lavie, 2006; Surdeanu and Manning, 2010).
One possible drawback of parser ensemble is that
several parsers are required to parse the same
sentence during the test phase. Moreover, our
approach can benefit from these methods in that
we can get parse forests of higher quality on
unlabeled data (Zhou, 2009).
6 Conclusions
This paper proposes a generalized training
framework of semi-supervised dependency
parsing based on ambiguous labelings. For
each unlabeled sentence, we combine the 1-best
parse trees of several diverse parsers to compose
ambiguous labelings, represented by a parse
forest. The training objective is to maximize the
mixed likelihood of both the labeled data and
the auto-parsed unlabeled data with ambiguous
labelings. Experiments show that our framework
can make better use of the unlabeled data,
especially those with divergent outputs from
different parsers, than traditional tri-training.
Detailed analysis demonstrates the effectiveness
of our approach. Specifically, we find that our
approach is very effective when using divergent
parsers such as the generative parser, and it is also
helpful to properly balance the size and oracle
accuracy of the parse forest of the unlabeled data.
For future work, among other possible
extensions, we would like to see how our
approach performs when employing more diverse
parsers to compose the parse forest of higher
quality for the unlabeled data, such as the easy-
first non-directional dependency parser (Goldberg
and Elhadad, 2010) and other constituent parsers
(Collins and Koo, 2005; Charniak and Johnson,
2005; Finkel et al, 2008).
Acknowledgments
The authors would like to thank the critical
and insightful comments from our anonymous
reviewers. This work was supported by National
Natural Science Foundation of China (Grant No.
61373095, 61333018).
465
References
Mohit Bansal and Dan Klein. 2011. Web-scale
features for full-scale parsing. In Proceedings of
ACL, pages 693?702.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory, pages 92?100.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In
Proceedings of EMNLP 2012, pages 1455?1465.
Bernd Bohnet. 2010. Top accuracy and fast
dependency parsing is not a contradiction. In
Proceedings of COLING, pages 89?97.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of EMNLP/CoNLL, pages 141?150.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP
1987-89 WSJ Corpus Release 1, LDC2000T43.
Linguistic Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving
dependency parsing with subtrees from auto-parsed
data. In Proceedings of EMNLP, pages 570?579.
Wenliang Chen, Min Zhang, and Yue Zhang. 2013.
Semi-supervised feature transformation for depen-
dency parsing. In Proceedings of EMNLP, pages
1303?1313.
Michael J. Collins and Terry Koo. 2005. Dis-
criminative reranking for natural language parsing.
Computational Linguistics, pages 25?70.
Mark Dredze, Partha Pratim Talukdar, and Koby
Crammer. 2009. Sequence learning from data
with multiple labels. In ECML/PKDD Workshop on
Learning from Multi-Label Data.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD,
pages 559?566.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condition-
al random field parsing. In Proceedings of ACL,
pages 959?967.
Yoav Goldberg and Michael Elhadad. 2010. An
efficient algorithm for easy-first non-directional
dependency parsing. In Proceedings of NAACL.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of EMNLP 2009,
pages 832?841.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword
Version 2.0, LDC2009T14. Linguistic Data
Consortium.
Rong Jin and Zoubin Ghahramani. 2002. Learning
with multiple labels. In Proceedings of NIPS.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In ACL, pages 1?11.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL, pages 595?603.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting
Liu. 2012. A separately passive-aggressive training
algorithm for joint POS tagging and dependency
parsing. In COLING 2012, pages 1681?1698.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, pages 152?159.
Ryan McDonald and Fernando Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of ACL, pages
91?98.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL, pages 950?958.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT, pages 149?160.
Eric W. Noreen. 1989. Computer-intensive methods
for testing hypotheses: An introduction. John Wiley
& Sons, Inc., New York.
Slav Petrov and Dan Klein. 2007. Improved
inference for unlexicalized parsing. In Proceedings
of NAACL.
Kenji Sagae and Alon Lavie. 2006. Parser
combination by reparsing. In Proceedings of
NAACL, pages 129?132.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL,
pages 1044?1050.
David A. Smith and Jason Eisner. 2007. Bootstrap-
ping feature-rich dependency parsers with entropic
priors. In Proceedings of EMNLP, pages 667?677.
466
Anders S?gaard and Christian Rish?j. 2010. Semi-
supervised dependency parsing using generalized
tri-training. In Proceedings of ACL, pages 1065?
1073.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-
driven dependency parsing of new languages using
incomplete and noisy training data. In CoNLL,
pages 12?20.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of NAACL, pages 649?
652.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of
semi-supervised structured conditional models for
dependency parsing. In Proceedings of EMNLP,
pages 551?560.
Oscar Ta?ckstro?m, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. In Proceedings of NAACL, pages
1061?1071.
Andre? Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking
dependency parsers. In Proceedings of EMNLP,
pages 157?166.
Qin Iris Wang, Dale Schuurmans, and Dekang
Lin. 2008. Semi-supervised convex training for
dependency parsing. In Proceedings of ACL, pages
532?540.
Hiroyasu Yamada and Yuji Matsumoto. 2003.
Statistical dependency analysis with support vector
machines. In Proceedings of IWPT, pages 195?206.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of ACL, pages 189?196.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proceedings of EMNLP-CoNLL, pages 320?331.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193.
Zhi-Hua Zhou and Ming Li. 2005. Tri-training:
Exploiting unlabeled data using three classifiers.
In IEEE Transactions on Knowledge and Data
Engineering, pages 1529?1541.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
Proceedings of ACL, pages 1556?1565.
Zhi-Hua Zhou. 2009. When semi-supervised learning
meets ensemble learning. In MCS.
467
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238?242
Manchester, August 2008
A Cascaded Syntactic and Semantic Dependency Parsing System
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng Li
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cn
Abstract
We describe our CoNLL 2008 Shared Task
system in this paper. The system includes
two cascaded components: a syntactic and
a semantic dependency parsers. A first-
order projective MSTParser is used as our
syntactic dependency parser. In order to
overcome the shortcoming of the MST-
Parser, that it cannot model more global in-
formation, we add a relabeling stage after
the parsing to distinguish some confusable
labels, such as ADV, TMP, and LOC. Be-
sides adding a predicate identification and
a classification stages, our semantic de-
pendency parsing simplifies the traditional
four stages semantic role labeling into two:
a maximum entropy based argument clas-
sification and an ILP-based post inference.
Finally, we gain the overall labeled macro
F1 = 82.66, which ranked the second posi-
tion in the closed challenge.
1 System Architecture
Our CoNLL 2008 Shared Task (Surdeanu et al,
2008) participating system includes two cascaded
components: a syntactic and a semantic depen-
dency parsers. They are described in Section 2
and 3 respectively. Their experimental results are
shown in Section 4. Section 5 gives our conclusion
and future work.
2 Syntactic Dependency Parsing
MSTParser (McDonald, 2006) is selected as our
basic syntactic dependency parser. It views the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
syntactic dependency parsing as a problem of
finding maximum spanning trees (MST) in di-
rected graphs. MSTParser provides the state-of-
the-art performance for both projective and non-
projective tree banks.
2.1 Features
The score of each labeled arc is computed through
the Eq. (1) in MSTParser.
score(h, c, l) = w ? f(h, c, l) (1)
where node h represents the head node of the arc,
while node c is the dependent node (or child node).
l denotes the label of the arc.
There are three major differences between our
feature set and McDonald (2006)?s:
1) We use the lemma as a generalization feature
of a word, while McDonald (2006) use the word?s
prefix.
2) We add two new features: ?bet-pos-h-same-
num? and ?bet-pos-c-same-num?. They represent
the number of nodes which locate between node h
and node c and whose POS tags are the same with
h and c respectively.
3) We use more back-off features than McDon-
ald (2006) by completely enumerating all of the
possible combinatorial features.
2.2 Relabeling
By observing the current results of MSTParser on
the development data, we find that the performance
of some labels are far below average, such as ADV,
TMP, LOC. We think the main reason lies in that
MSTParser only uses local features restricted to a
single arc (as shown in Eq. (1)) and fails to use
more global information. Consider two sentences:
?I read books in the room.? and ?I read books in
the afternoon.?. It is hard to correctly label the arc
238
Deprel Total Mislabeled as
NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],
AMOD [0.1]
OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]
ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],
DIR [1.5]
NAME 1,138 NMOD [2.2]
VC 953 PRD [0.9]
DEP 772 NMOD [4.0]
TMP 755 ADV [9.9], LOC [6.5]
LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]
AMOD 536 ADV [2.2]
PRD 509 VC [4.7]
APPO 444 NMOD [2.5]
OPRD 373 OBJ [4.6]
DIR 119 ADV [18.5]
MNR 109 ADV [28.4]
Table 1: Error Analysis of Each Label
between ?read? and ?in? unless we know the object
of ?in?.
We count the errors of each label, and show the
top ones in Table 1. ?Total? refers to the total num-
ber of the corresponding label in the development
data. The column of ?Mislabeled as? lists the la-
bels that an arc may be mislabeled as. The number
in brackets shows the percentage of mislabeling.
As shown in the table, some labels are often con-
fusable with each other, such as ADV, LOC and
TMP.
2.3 Relabeling using Maximum Entropy
Classifier
We constructed two confusable label set which
have a higher mutual mislabeling proportion:
(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,
OPRD). A maximum entropy classifier is used to
relabel them.
Features are shown in Table 2. The first column
lists local features, which contains information of
the head node h and the dependent node c of an arc.
?+ dir dist? means that conjoining existing features
with arc direction and distance composes new fea-
tures. The second column lists features using the
information of node c?s children. ?word c c? rep-
resents form or lemma of one child of the node
c. ?dir c? and ?dist c? represents the direction and
distance of the arc which links node c to its child.
The back-off technique is also used on these fea-
tures.
Local features (+ dir dist) Global features (+ dir c dist c)
word h word c word h word c word c c
Table 2: Relabeling Feature Set (+ dir dist)
3 Semantic Dependency Parsing
3.1 Architecture
The whole procedure is divided into four separate
stages: Predicate Identification, Predicate Classifi-
cation, Semantic Role Classification, and Post In-
ference.
During the Predicate Identification stage we ex-
amine each word in a sentence to discover target
predicates, including both noun predicates (from
NomBank) and verb predicates (from PropBank).
In the Predicate Classification stage, each predi-
cate is assigned a certain sense number. For each
predicate, the probabilities of a word in the sen-
tence to be each semantic role are predicted in the
Semantic Role Classification stage. Maximum en-
tropy model is selected as our classifiers in these
stages. Finally an ILP (Integer Linear Program-
ming) based method is adopted for post infer-
ence (Punyakanok et al, 2004).
3.2 Predicate Identification
The predicate identification is treated as a binary
classification problem. Each word in a sentence is
predicted to be a predicate or not to be. A set of
features are extracted for each word, and an opti-
mized subset of them are adopted in our final sys-
tem. The following is a full list of the features:
DEPREL (a1): Type of relation to the parent.
WORD (a21), POS (a22), LEMMA (a23),
HEAD (a31), HEAD POS (a32), HEAD LEMMA
(a33): The forms, POS tags and lemmas of a word
and it?s headword (parent) .
FIRST WORD (a41), FIRST POS (a42),
FIRST LEMMA (a43), LAST WORD (a51),
LAST POS (a52), LAST LEMMA (a53): A
corresponding ?constituent? for a word consists
of all descendants of it. The forms, POS tags and
lemmas of both the first and the last words in the
constituent are extracted.
POS PAT (a6): A ?POS pattern? is produced for
the corresponding constituent as follows: a POS
bag is produced with the POS tags of the words
in the constituent except for the first and the last
ones, duplicated tags removed and the original or-
der ignored. Then we have the POS PAT feature
239
by combining the POS tag of the first word, the
bag and the POS tag of the last word.
CHD POS (a71), CHD POS NDUP (a72),
CHD REL (a73), CHD REL NDUP (a74): The
POS tags of the child words are joined to-
gether to form feature CHD POS. With adja-
cently duplicated tags reduced to one, feature
CHD POS NDUP is produced. Similarly we can
get CHD REL and CHD REL NDUP too, with
the relation types substituted for the POS tags.
SIB REL (a81), SIB REL NDUP (a82),
SIB POS (a83), SIB POS NDUP (a84): Sibling
words (including the target word itself) and the
corresponding dependency relations (or POS tags)
are considered as well. Four features are formed
similarly to those of child words.
VERB VOICE (a9): Verbs are examined for
voices: if the headword lemma is either ?be? or
?get?, or else the relation type is ?APPO?, then the
verb is considered passive, otherwise active.
Also we used some ?combined? features which
are combinations of single features. The final op-
timized feature set is (a1, a21, a22, a31, a32, a41,
a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,
a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,
a81+a83).
3.3 Predicate Classification
After predicate identification is done, the resulting
predicates are processed for sense classification. A
classifier is trained for each predicate that has mul-
tiple senses on the training data (There are totally
962 multi-sense predicates on the training corpus,
taking up 14% of all) In additional to those fea-
tures described in the predicate identification sec-
tion, some new ones relating to the predicate word
are introduced:
BAG OF WORD (b11), BAG OF WORD O
(b12): All words in a sentence joined, namely
?Bag of Words?. And an ?ordered? version is in-
troduced where each word is prefixed with a letter
?L?, ?R? or ?T? indicating it?s to the left or right of
the predicate or is the predicate itself.
BAG OF POS O (b21), BAG OF POS N
(b22): The POS tags prefixed with ?L?, ?R? or
?T? indicating the word position joined together,
namely ?Bag of POS (Ordered)?. With the
prefixed letter changed to a number indicating
the distance to the predicate (negative for being
left to the predicate and positive for right), an-
other feature is formed, namely ?Bag of POS
(Numbered)?.
WIND5 BIGRAM (b3): 5 closest words from
both left and right plus the predicate itself, in total
11 words form a ?window?, within which bigrams
are enumerated.
The final optimized feature set for the task of
predicate classification is (a1, a21, a23, a71, a72,
a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,
a71+a9).
3.4 Semantic Role Classification
In our system, the identification and classifica-
tion of semantic roles are achieved in a single
stage (Liu et al, 2005) through one single classi-
fier (actually two, one for noun predicates, and the
other for verb predicates). Each word in a sentence
is given probabilities to be each semantic role (in-
cluding none of the these roles) for a predicate.
Features introduced in addition to those of the pre-
vious subsections are the following:
POS PATH (c11), REL PATH (c12): The ?POS
Path? feature consists of POS tags of the words
along the path from a word to the predicate. Other
than ?Up? and ?Down?, the ?Left? and ?Right? di-
rection of the path is added. Similarly, the ?Re-
lation Path? feature consists of the relation types
along the same path.
UP PATH (c21), UP REL PATH (c22): ?Up-
stream paths? are parts of the above paths that stop
at the common ancestor of a word and the predi-
cate.
PATH LEN (c3): Length of the paths
POSITION (c4): The relative position of a word
to the predicate: Left or Right.
PRED FAMILYSHIP (c5): ?Familyship rela-
tion? between a word and the predicate, being one
of ?self?, ?child?, ?descendant?, ?parent?, ?ances-
tor?, ?sibling?, and ?not-relative?.
PRED SENSE (c6): The lemma plus sense
number of the predicate
As for the task of semantic role classification,
the features of the predicate word in addition to
those of the word under consideration can also
be used; we mark features of the predicate with
an extra ?p?. For example, the head word of
the current word is represented as a31, and the
head word of the predicate is represented as pa31.
So, with no doubt for the representation, our fi-
nal optimized feature set for the task of seman-
tic role classification is (a1, a23, a33, a43, a53,
a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,
240
pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,
a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).
3.5 ILP-based Post Inference
The final semantic role labeling result is gener-
ated through an ILP (Integer Linear Programming)
based post inference method. An ILP problem is
formulated with respect to the probability given by
the above stage. The final labeling is formed at the
same time when the problem is solved.
Let W be the set of words in the sentence, and
R be the set of semantic role labels. A virtual label
?NULL? is also added to R, representing ?none of
the roles is assigned?.
For each word w ? W and semantic role label
r ? R we create a binary variable v
wr
? (0, 1),
whose value indicates whether or not the word w
is labeled as label r. p
wr
denotes the possibil-
ity of word w to be labeled as role r. Obviously,
when objective function f =
?
w,r
log(p
wr
? v
wr
)
is maximized, we can read the optimal labeling for
a predicate from the assignments to the variables
v
wr
. There are three constrains used in our system:
C1: Each relation should be and only be la-
beled with one label (including the virtual label
?NULL?), i.e.:
?
r
v
wr
= 1
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?).
The threshold we use in our system is 0.3, which
is optimized from the development data. i.e.:
v
wr
= 0, if p
wr
< 0.3 and r 6= ?NULL?
C3: Statistics shows that the most roles (ex-
cept for the virtual role ?NULL?) usually appear
only once for a predicate, except for some rare ex-
ception. So we impose a no-duplicated-roles con-
straint with an exception list, which is constructed
according to the times of semantic roles? duplica-
tion for each single predicate (different senses of a
predicate are considered different) and the ratio of
duplication to non-duplication.
?
r
v
wr
? 1,
if < p, r > /? {< p, r > |p ? P, r ? R;
d
pr
c
pr
?d
pr
> 0.3 ? d
pr
> 10}
(2)
where P is the set of predicates; c
pr
denotes the
count of words in the training corpus, which are
Predicate Type Predicate Label
Noun president.01 A3
Verb match.01 A1
Verb tie.01 A1
Verb link.01 A1
Verb rate.01 A0
Verb rate.01 A2
Verb attach.01 A1
Verb connect.01 A1
Verb fit.01 A1
Noun trader.01 SU
Table 3: No-duplicated-roles constraint exception
list (obtained by Eq. (2))
labeled as r ? R for predicate p ? P ; while d
pr
denotes something similar to c
pr
, but what taken
into account are only those words labeled with r,
and there are more than one roles within the sen-
tence for the same predicate. Table 3 lists the com-
plete exception set, which has a size of only 10.
4 Experiments
The original MSTParser
1
is implemented in Java.
We were confronted with memory shortage when
trying to train a model with the entire CoNLL 2008
training data with 4GB memory. Therefore, we
rewrote it with C++ which can manage the mem-
ory more exactly. Since the time was limited, we
only rewrote the projective part without consider-
ing second-order parsing technique.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit
2
. The
classifier parameters: gaussian prior and iterations,
are tuned with the development data for different
stages respectively.
lp solve 5.5
3
is chosen as our ILP problem
solver during the post inference stage.
The training time of the syntactic and the se-
mantic parsers are 22 and 5 hours respectively, on
all training data, with 2.0GHz Xeon CPU and 4G
memory. While the prediction can be done within
10 and 5 minutes on the development data.
4.1 Syntactic Dependency Parsing
The experiments on development data show that
relabeling process is helpful, which improves the
1
http://sourceforge.net/projects/mstparser
2
http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
3
http://sourceforge.net/projects/lpsolve
241
Precision (%) Recall (%) F1
Pred Identification 91.61 91.36 91.48
Pred Classification 86.61 86.37 86.49
Table 4: The performance of predicate identifica-
tion and classification
Precision (%) Recall (%) F1
Simple 81.02 76.00 78.43
ILP-based 82.53 75.26 78.73
Table 5: Comparison between different post infer-
ence strategies
LAS performance from 85.41% to 85.94%. The fi-
nal syntactic dependency parsing performances on
the WSJ and the Brown test data are 87.51% and
80.73% respectively.
4.2 Semantic Dependency Parsing
The semantic dependency parsing component is
based on the last syntactic dependency parsing
component. All stages of the system are trained
with the closed training corpus, while predicted
against the output of the syntactic parsing.
Performance for predicate identification and
classification is given in Table 4, wherein the clas-
sification is done on top of the identification.
Semantic role classification and the post infer-
ence are done on top of the result of predicate iden-
tification and classification. The final performance
is presented in Table 5. A simple post inference
strategy is given for comparison, where the most
possible label (including the virtual label ?NULL?)
is select except for those duplicated non-virtual la-
bels with lower probabilities (lower than 0.5). Our
ILP-based method produces a gain of 0.30 with re-
spect to the F1 score.
The final semantic dependency parsing perfor-
mance on the development and the test (WSJ and
Brown) data are shown in Table 6.
Precision (%) Recall (%) F1
Development 82.53 75.26 78.73
Test (WSJ) 82.67 77.50 80.00
Test (Brown) 64.38 68.50 66.37
Table 6: Semantic dependency parsing perfor-
mances
4.3 Overall Performance
The overall macro scores of our syntactic and se-
mantic dependency parsing system are 82.38%,
83.78% and 73.57% on the development and two
test (WSJ and Brown) data respectively, which is
ranked the second position in the closed challenge.
5 Conclusion and Future Work
We present our CoNLL 2008 Shared Task system
which is composed of two cascaded components:
a syntactic and a semantic dependency parsers,
which are built with some state-of-the-art methods.
Through a fine tuning features and parameters, the
final system achieves promising results. In order
to improve the performance further, we will study
how to make use of more resources and tools (open
challenge) and how to do joint learning between
syntactic and semantic parsing.
Acknowledgments
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(NSFC) via grant 60675034, 60575042, and the
?863? National High-Tech Research and Develop-
ment of China via grant 2006AA01Z145.
References
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, June.
McDonald, Ryan. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
242
