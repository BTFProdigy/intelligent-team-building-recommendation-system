Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 21?30,
New York City, USA, June 2006. c?2006 Association for Computational Linguistics
Learning Quantity Insensitive Stress Systems via Local Inference
Jeffrey Heinz
Linguistics Department
University of California, Los Angeles
Los Angeles, California 90095
jheinz@humnet.ucla.edu
Abstract
This paper presents an unsupervised batch
learner for the quantity-insensitive stress
systems described in Gordon (2002). Un-
like previous stress learning models, the
learner presented here is neither cue based
(Dresher and Kaye, 1990), nor reliant on
a priori Optimality-theoretic constraints
(Tesar, 1998). Instead our learner ex-
ploits a property called neighborhood-
distinctness, which is shared by all of the
target patterns. Some consequences of this
approach include a natural explanation
for the occurrence of binary and ternary
rhythmic patterns, the lack of higher n-ary
rhythms, and the fact that, in these sys-
tems, stress always falls within a certain
window of word edges.
1 Introduction
The central premise of this research is that phonotac-
tic patterns are have properties which reflect prop-
erties of the learner. This paper illustrates this ap-
proach for quantity-insensitive (QI) stress systems
(see below).
I present an unsupervised batch learner that cor-
rectly learns every one of these languages. The
learner succeeds because there is a universal prop-
erty of QI stress systems which I refer to as
neighborhood-distinctness (to be defined below).
This property, which is a structural notion of local-
ity, is used by the learning algorithm to successfully
infer the target pattern from samples.
A learner is a function from a set of observations
to a grammar. An observation is some linguistic
sign, in this case a word-sized sequence of stress val-
ues. A grammar is some device that must at least re-
spond Yes or No when asked if a linguistic sign is a
possible sign for this language (Chomsky and Halle,
1968; Halle, 1978).1
The remainder of the introduction outlines the ty-
pology of the QI stress systems, motivates represent-
ing phonotactics with regular languages, and exam-
ines properties of the attested patterns. In ?2, I define
the class of neighborhood-distinct languages. The
learning algorithm is presented in two stages. ?3 in-
troduces a basic version of the learner the learner,
which successfully acquires just under 90% of the
target patterns. In ?4, one modification is made to
this learner which consequently succeeds on all tar-
get patterns. ?5 discusses predictions made by these
learning algorithms. The appendix summarizes the
target patterns and results.
1.1 Quantity-Insensitive Stress Systems
Stress assignment in QI languages is indifferent to
the weight of a syllable. For example, Latin is
quantity-sensitive (QS) because stress assignment
depends on the syllable type: if the penultimate syl-
lable is heavy (i.e. has a long vowel or coda) then
it receives stress, but otherwise the antepenult does.
The stress systems under consideration here, unlike
Latin, do not distinguish syllable types.
1In this respect, this work departs from (or is a special case
of) gradient phonotactic models (Coleman and Pierrehumbert,
1997; Frisch et al, 2000; Albright, 2006; Hayes and Wilson,
2006)
21
There are 27 types of QI stress systems found in
Gordon?s (2002) typology. Gordon adds six plausi-
bly attestable QI systems by considering the behav-
ior of all-light-syllabled words from QS systems.
These 33 patterns are divided into four kinds: sin-
gle, dual, binary and ternary. Single systems have
one stressed syllable per word, and dual systems up
to two. Binary and ternary systems stress every sec-
ond (binary) or third (ternary) syllable.
The choice to study QI stress systems was made
for three reasons. First, they are well studied and the
typology is well established (Hayes, 1995; Gordon,
2002). Secondly, learning of stress systems has been
approached before (Dresher and Kaye, 1990; Gupta
and Touretzky, 1991; Goldsmith, 1994; Tesar, 1998)
making it possible to compare learners and results.
Third, these patterns have been analyzed with ad-
jacency restrictions (e.g. no clash), as disharmony
(e.g. a primary stress may not be followed by an-
other), and with recurrence requirements (e.g. build
trochaic feet iteratively from the left). Thus the pat-
terns found in the QI stress systems are represen-
tative of other phonotactic domains that the learner
should eventually be extended to.
The 33 types are shown in Table 1. See Gor-
don (2002) and Hayes (1995) for details, exam-
ples, and original sources. Note that some patterns
have a minimal word condition (Prince, 1980; Mc-
Carthy and Prince, 1990; Hayes, 1995), banning ei-
ther monosyllables or light monosyllables. For ex-
ample, Cayuvava bans all monosyllables, whereas
Hopi bans only light monosyllables. Because this
paper addresses QI stress patterns I abstract away
from the internal structure of the syllable. For con-
venience, when stress patterns are explicated in this
paper I assume (stressed) monosyllables are permit-
ted. The learning study, however, includes each
stress pattern both with and without stressed mono-
syllables. Predictions our learner makes with respect
to the minimal word condition are given in ?5.2.
2We use the (first) language name to exemplify the stress
pattern. The number in parentheses is an index to the lan-
guage Gordon?s 2003 appendix. All stress representations fol-
low Gordon?s notation, who uses the metrical grid (Liberman
and Prince, 1977; Prince, 1983). Thus, primary stress is indi-
cated by 2, secondary stress by 1, and no stress by 0.
1.2 Phonotactics as Regular Languages
I represent phonotactic descriptions as regular sets,
accepted by finite-state machines. A finite state ma-
chine is a 5-tuple (?, Q, q
0
, F, ?) where ? is a finite
alphabet, Q is a set of states, q
0
? Q is the start state,
F ? Q is a set of final states, and ? is a set of tran-
sitions. Each transition has an origin and a terminus
and is labeled with a symbol of the alphabet; i.e. a
transition is a 3-tuple (o, a, t) where o, t ? Q and
a ? ?.
Empirically, it has been observed that most
phonological phenomena are regular (Johnson,
1972; Kaplan and Kay, 1981; Kaplan and Kay, 1994;
Ellison, 1994; Eisner, 1997; Karttunen, 1998). This
is especially true of phonotactics: reduplication and
metathesis, which have higher complexity, are not
phonotactic patterns as they involve alternations.3
Formally, regular languages are widely studied in
computer science, and their basic properties are well
understood (Hopcroft et al, 2001). Also, a learning
literature exists. E.g. the class of regular languages
is not exactly identifiable in the limit (Gold, 1967),
but certain subsets of it are (Angluin, 1980; Angluin,
1982). Thus it is becomes possible to ask: What
subset of the regular languages delimits the class of
possible human phonotactics and can properties of
this class be exploited by a learner?
This perspective also connects to finite state mod-
els of Optimality Theory (OT) (Prince and Smolen-
sky, 1993). Riggle (2004) shows that if OT con-
straints are made finite-state, it is possible to build a
transducer that takes any input to a grammatical out-
put. Removing from this transducer the input labels
and hidden structural symbols (such as foot bound-
aries) in the output labels yields a phonotactic ac-
ceptor for the language, a target for our learner.
Consider Pintupi, #26 in Table 1, which exempli-
fies a binary stress pattern. Its phonotactic gram-
mar is given in Figure 1. The hexagon indicates the
start state, and final states are marked by the double
perimeter.
This machine accepts the Pintupi words, but not
other words of the same length. Also, the Pin-
tupi grammar accepts an infinite number of words?
just like the grammars in Hayes (1995) and Gordon
3See Albro (1998; 2005) for restricted extensions to regular
languages.
22
Table 1: The Quantity-Insensitive Stress Systems.2
Single Systems
1. (1) Chitimacha 20000000 2000000 200000 20000 2000 200 20 2
2. (2) Lakota 02000000 0200000 020000 02000 0200 020 02 2
3. (3) Hopi (qs) 02000000 0200000 020000 02000 0200 020 20 2
4. (4) Macedonian 00000200 0000200 000200 00200 0200 200 20 2
5. (5) Nahuatl / Mohawk? 00000020 0000020 000020 00020 0020 020 20 2
6. (6) Atayal / Dieguen?o? 00000002 0000002 000002 00002 0002 002 02 2
Dual Systems
7. (7f) Quebec French 10000002 1000002 100002 10002 1002 102 12 2
8. (9f) Udihe 10000002 1000002 100002 10002 1002 102 02 2
9. (10i) Lower Sorbian 20000010 2000010 200010 20010 2010 200 20 2
10. (11f) Sanuma 10000020 1000020 100020 10020 1020 020 20 2
11. (15f) Georgian 10000200 1000200 100200 10200 0200 200 20 2
12. (16i) Walmatjari 20000100 2000100 200100 20100 2010 200 20 2
(optional variants) 20000010 2000010 200010 20010
Binary Systems
13. (24i) Araucanian 02010101 0201010 020101 02010 0201 020 02 2
14. (24f) Creek? (qs) 01010102 0101020 010102 01020 0102 020 02 2
15. (25f) Urubu Kaapor 01010102 1010102 010102 10102 0102 102 02 2
16. (26i) Malakmalak 20101010 0201010 201010 02010 2010 020 20 2
17. (26f) Cavinen?a? 10101020 0101020 101020 01020 1020 020 20 2
18. (27i) Maranungku 20101010 2010101 201010 20101 2010 201 20 2
19. (27f) Palestinean Arabic? (qs) 10101020 1010102 101020 10102 1020 102 20 2
Binary Systems with Clash
20. (28i) Central Alaskan Yupik? 01010102 0101012 010102 01012 0102 012 02 2
21. (29i) Southern Paiute? 02010110 0201010 020110 02010 0210 020 20 2
22. (30i) Gosiute Shoshone 20101011 2010101 201011 20101 2011 201 21 2
23. (32f) Biangai 10101020 1101020 101020 11020 1020 120 20 2
24. (33f) Tauya 11010102 1010102 110102 10102 1102 102 12 2
Binary Systems with Lapse
25. (34f) Piro 10101020 1010020 101020 10020 1020 020 20 2
26. (36i) Pintupi / Diyari? 20101010 2010100 201010 20100 2010 200 20 2
27. (40f) Indonesian 10101020 1001020 101020 10020 1020 020 20 2
28. (42i) Garawa 20101010 2001010 201010 20010 2010 200 20 2
Ternary Systems
29. (48i) Ioway-Oto 02001001 0200100 020010 02001 0200 020 02 2
30. (49f) Cayuvava? 00100200 0100200 100200 00200 0200 200 20 2
31. (67i) Estonian (qs) 20010010 2001010 201010 20010 2010 200 20 2
(optional variants) 20101010 2010100 200100 20100
20100100 2010010
20010100
32. (71f) Pacific Yupik (qs) 01001002 0100102 010020 01002 0102 020 02 2
33. (72i) Winnebago? (qs) 00200101 0020010 002001 00201 0020 002 02 2
? Bans monosyllables.
? Bans light monosyllables.
23
0 1
2
2
0
4
1
3
0
0
Figure 1: Stress in Pintupi as a finite state machine
(2002), who take the observed forms as instances of
a pattern that extends to longer words. The learner?s
task is to take the Pintupi words in Table 1 and return
the pattern represented by Figure 1.
1.3 Properties of QI Stress Patterns
The deterministic acceptor with the fewest states for
a language is called the language?s canonical accep-
tor. Therefore, let us ask what properties the canoni-
cal acceptors for the 33 stress types have in common
that might be exploited by a learner.
One property shared by all grammars except Es-
tonian is that they have exactly one loop (Estonian
has two). Though this restriction is nontrivial, it is
insufficient for learning to be guaranteed.4 A second
shared property is slenderness. A machine is slender
iff it accepts only one word of length n. The only ex-
ceptions to this are Walmatjari and Estonian, which
have free variation in longer words (see Table 1).
I focus in this paper on another property which are
shared by all machines without exception. In 29 of
the canonical acceptors, each state can be uniquely
identified by its incoming symbol set, its outgo-
ing symbol set, and whether it is final or non-final.
These items make up the neighborhood of a state,
which will be formally defined in the next section.
The other four stress systems have non-canonical
acceptors wherein each state can also be uniquely
identified by its neighborhood. This property I call
neighborhood-distinctness. Thus, neighborhood-
distinctness is a universal property of QI stress sys-
tems, and it is this property that the learner will ex-
ploit.
4The proof is similar to the one used to show the cofinite
languages are not learnable (Osherson et al, 1986).
2 Neighborhood-Distinctness
2.1 Neighborhood-Distinct Acceptors
The neighborhood of a state in an acceptor
(?, Q, q
0
, F, ?) is defined in (1).
(1) The neighborhood of a state q is triple
(f, I,O) where f = 1 iff q ? F and f = 0
otherwise, I = {a | ?o ? Q, (o, a, q) ? ?},
and O = {a | ?t ? Q, (q, a, t) ? ?}
Thus the neighborhood of state can be determined
by looking solely at whether or not it is final, the set
of symbols labeling the transitions which reach that
state, and the set of symbols labeling the transitions
which depart that state. For example in Figure 2,
states p and q have the same neighborhood because
they are both nonfinal, can both be reached by some
element of {a, b}, and because each state can only
be exited by observing a member of {c, d}.5
q
a c
db
p
a
c
d
a
b
Figure 2: Two states with the same neighborhood.
Neighborhood-distinct acceptors are defined in
(2).
(2) An acceptor is said to be neighborhood-
distinct iff no two states have the same
neighborhood.
This class of acceptors is finite: there are 22|?|+1
neighborhoods, i.e. types of states. Since each state
in a neighborhood-distinct machine has a unique
neighborhood, this becomes an upper bound on ma-
chine size.6
5The notion of neighborhood can be generalized to neigh-
borhoods of size k, where sets I and O are defined as the in-
coming and outgoing paths of length k. However, this paper is
only concerned with neighborhoods of size 1.
6For some acceptor, the notion of neighborhood lends it-
self to an equivalence relation R
N
over Q: pR
N
q iff p and q
have the same neighborhood. Therefore, R
N
partitions Q into
blocks, and neighborhood-distinct machines are those where
this partition equals the trivial partition.
24
2.2 Neighborhood-Distinct Languages
The class of neighborhood-distinct languages is de-
fined in (3).
(3) The neighborhood-distinct languages are
those for which there is an acceptor which
is neighborhood-distinct.
The neighborhood-distinct languages are a (finite)
proper subset of the regular languages over an
alphabet ?: all regular languages whose small-
est acceptors have more than 22|?|+1 states cannot
be neighborhood-distinct (since at least two states
would have the same neighborhood).
The canonically neighborhood-distinct languages
are defined in (4).
(4) The canonically neighborhood-distinct
languages are those for which the canonical
acceptor is neighborhood-distinct.
The canonically neighborhood-distinct languages
form a proper subset of the neighborhood-distinct
languages. For example, the canonical accep-
tor shown in Figure 3 of Lower Sorbian (#9 in
Table 1) is not neighborhood-distinct (states 2
and 3 have the same neighborhood). However,
there is a non-canonical (because non-deterministic)
neighborhood-distinct acceptor for this language, as
shown in Figure 4.
0 1
2
2
0
5
1
3
0
6
01
4
0 1
0
Figure 3: The canonical acceptor for Lower Sorbian.
0
1
2
32
4
2
5
2
0
2
0
0
0
0
1
Figure 4: A neighborhood-distinct acceptor for
Lower Sorbian.
Neighborhood-distinctness is a universal property
of the patterns under consideration. Additionally, it
is a property which a learner can use to induce a
grammar from surface forms.
3 The Neighborhood Learner
In this section, I present the basic unsupervised
batch learner, called the Neighborhood Learner,
which learns 29 of the 33 patterns. In the next
section, I introduce one modification to this learner
which results in perfect accuracy.
The basic version of the learner operates in two
stages: prefix tree construction and state-merging,
cf. Angluin (1982). These two stages find smaller
descriptions of the observed data; in particular state-
merging may lead to generalization (see below).
A prefix tree is constructed as follows. Set the ini-
tial machine M = (?, {q
0
}, q
0
, ?, ?) and the current
state c = q
0
. With each word, each symbol a is con-
sidered in order. If ?t ? Q, (c, a, t) ? ? then set
c = t. Otherwise, add a new state n to Q and a new
arc (c, a, n) to ?. A new arc is therefore created on
every symbol in the first word. The last state for a
word is added to F . The process is repeated for each
word. The prefix tree for Pintupi words from Table
1 is shown in Figure 5.
0 1
2
2
0
31
11
0
4
0
51
10
0
6
0
71
9
0
8
0
Figure 5: The prefix tree of Pintupi words.
The second stage of the learner is state-merging,
a process which reduces the number of states in the
machine. A key concept in state merging is that
when two states are merged into a single state, their
transitions are preserved. Specifically, if states p and
q merge, then a merged state pq is added to the ma-
chine, and p and q are removed. For every arc that
left p (or q) to a state r, there is now an arc from pq
going to r. Likewise, for every arc from a state r to
p (or q), there is now an arc from r to pq.
The post-merged machine accepts every word that
the pre-merged machine accepts, and possibly more.
For example, if there is a path between two states
which become merged, a loop is formed.
25
What remains to be explained is the criteria the
learner uses to determine whether two states in
the prefix tree merge. The Neighborhood Learner
merges two states iff they have the same neighbor-
hood, guaranteeing that the resulting grammar is
neighborhood-distinct.
The intuition is that the prefix tree provides
a structured representation of the input and has
recorded information about different environments,
which are represented in the tree as states. Learning
is a process which identifies actually different envi-
ronments as ?the same?? here states are ?the same?
iff their local features, i.e their neighborhoods, are
the same. For example, suppose states p and q in the
prefix tree are both final or both nonfinal, and they
share the same incoming symbol set and outgoing
symbol set. In the learner?s eyes they are then ?the
same?, and will be merged.
The merging criteria partitions the states of the
Pintupi prefix tree into five groups. States 3,5 and
7 are merged; states 2,4,6 are merged, and states
8,9,10,12 are merged. Merging of states halts when
no two nodes have the same neighborhood? thus, the
resulting machine is neighborhood-distinct. The re-
sult for Pintupi is shown in Figure 6.
0 1
2
2-4-6
0
3-5-7
1
8-9-10-11
0
0
Figure 6: The grammar learned for Pintupi.
The machine in Figure 6 is equivalent to the one
in Figure 1? they accept exactly the same language.7
I.e. neighborhood merging of the prefix tree in Fig-
ure 5 generalizes from the data exactly as desired.
3.1 Results of Neighborhood Learning
The Neighborhood Learner successfully learns 29 of
the 33 language types (see appendix). These are ex-
actly the 29 canonically neighborhood-distinct lan-
guages. This suggests the following claim, which
has not been proven.8
7This can be verified by checking to see if the minimized
versions of the two machines are isomorphic.
8The proof is made difficult by the fact that the acceptor
returned by the Neighborhood Learner is not necessarily the
(5) Conjecture: The Neighborhood Learner
identifies the class of canonically
neighborhood-distinct languages.
In ?4, I discuss why the learner fails where it does,
and introduce a modification which results in perfect
accuracy.
4 Reversing the Prefix Tree
This section examines the four cases where neigh-
borhood learning failed and modifies the learning al-
gorithm, resulting in perfect accuracy. The goal is to
restrict generalization because in every case where
learning failed, the learner overgeneralized by merg-
ing more states than it should have. Thus, the re-
sulting grammars recognize multiple words with n
syllables.
The dual stress pattern of Lower Sorbian places
stress initially and, in words of four or more sylla-
bles, on the penult (see #9 Table 1). The prefix tree
built from these words is shown in Figure 7.
0 1
2
2
0
151
3
0
11 12
0
13 14
0
16
0
1
4
0
1
5
0
9
1
6
0
10
0
7
1
8
0
Figure 7: The prefix tree for Lower Sorbian.
Here the Neighborhood Learner fails because it
merges states 2 and 3. The resulting grammar incor-
rectly accepts words of the form 20?.
The proposed solution follows from the observa-
tion that if the prefix tree were constructed in reverse
(reading each word from right to left) then the corre-
sponding states in this structure would not have the
same neighborhoods, and thus not be merged. A re-
verse prefix tree is constructed like a forward prefix
tree, the only difference being that the order of sym-
bols in each word is reversed. When neighborhood
learning is applied to this structure and the result-
ing machine reversed again, the correct grammar is
obtained, shown in Figure 4.
How is the learner to know whether to construct
the prefix tree normally or in reverse? It simply does
both and intersects the results. Intersection of two
canonical acceptor.
26
languages is an operation which returns a language
consisting of the words common to both. Similarly,
machine intersection returns an acceptor which rec-
ognizes just those words that both machines recog-
nize. This strategy is thus conservative: the learner
keeps only the most robust generalizations, which
are the ones it ?finds? in both the forward and reverse
prefix trees.
This new learner is called the Forward Backward
Neighborhood (FBN) Learner and it succeeds with
all the patterns (see appendix).
Interestingly, the additional languages the FBN
Learner can acquire are ones that, under foot-based
analyses like those in Hayes (1995), require feet to
be built from the right word edge. For example,
Lower Sorbian has a binary trochee aligned to the
right word edge; Indonesian iteratively builds binary
trochaic feet from the right word edge; Cayuvava it-
eratively builds anapests from the right word edge.
Thus structuring the input in reverse appears akin to
a footing procedure which proceeds from the right
word boundary.
5 Predictions of Neighborhood Learning
In this section, let us examine some of the predic-
tions that are made by neighborhood learning. In
particular, let us consider the kinds of languages that
the Neighborhood Learner can and cannot learn and
compare them with the attested typology.
5.1 Binary and Ternary Stress Patterns
Neighborhood learning suggests an explanation of
the fact that the stress rhythms found in natural
language are binary or ternary and not higher n-
ary, and of the fact that stress falls within a three-
syllable window of the word edge: perhaps only sys-
tems with these properties are learnable. This is be-
cause the neighborhood learner cannot distinguish
between sequences of the same symbol with length
greater than two.
As an example, consider the quaternary (and
higher n-ary) stress pattern 2(0001)?(0|00|000).9 If
the learner is exposed to samples from this pattern,
it incorrectly generalizes to 2(000?1)?(0|00|000).
9I follow Hopcroft et al(2001) in our notation of regular
expressions with one substitution? we use | instead of + to in-
dicate disjunction.
Similarly, neighborhood learning cannot distin-
guish a form like 02000 from 020000, so a sys-
tem that places stress on the pre-antepenult (e.g.
02000, 002000, 0002000) is not learnable. With
samples from the pre-antepenultimate language
(0
?
2000|200|20|2), the learner incorrectly general-
izes to 0?20?.
5.2 Minimal Word Conditions
A subtle prediction made by neighborhood-learning
is that a QI stress language with a pattern like the
one exemplified by Hopi (shown in Figure 8) cannot
have a minimal word condition banning monosylla-
bles. This is because if there were no monosyllables
in this language, then state 4 in Figure 8 would have
the same neighborhood as state 2 (as in Figure 9).
0
4
2
1
0
5
0
2
2
3
0
0
Figure 8: The stress pattern exemplified by Hopi,
allowing monosyllables.
0
4
2
1
0
50
2
2
3
0
0
Figure 9: The stress pattern exemplified by Hopi,
not allowing monosyllables.
Since such a grammar recognizes a non-
neighborhood-distinct language it cannot be learned
by the Neighborhood Learner.
As it happens, Hopi is a QS language which pro-
hibits light, but permits heavy, monosyllables. Since
I have abstracted away from the internal structure of
the syllable in this paper, this prediction is not dis-
confirmed by the known typology: there are in fact
no QI Hopi-like stress patterns in Gordon?s (2002)
typology which ban all monosyllables; i.e there are
no QI patterns like the one in Figure 9.
Some QI languages do have a minimal word con-
dition banning all monosyllables. To our knowl-
edge these are Cavinen?a and Cayuvava (see Ta-
ble 1), Mohawk (which places stress on the penult
27
like Nahuatl), and Diyari, Mohwak, Pitta Pitta and
Wangkumara (all which assign stress like Pintupi)
(Hayes, 1995). The Forward Backward Neighbor-
hood Learner learns all of these patterns successfully
irrespective of whether the patterns (and correspond-
ing input samples) permit monosyllables, predicting
that such patterns do not correlate with a prohibition
on monosyllables (see appendix).
Other QI languages prohibit light monosyllables.
Dieguen?o, for example, places stress finally like
Atayal (see Table 1), but only allows heavy mono-
syllables. This is an issue to attend to in future re-
search when trying to extend the learning algorithm
to QS patterns, when the syllable type (light/heavy)
is included in the representational scheme.
5.3 Restrictiveness and Other Approaches
There are languages that can be learned by neighbor-
hood learning that phonologists do not consider to
be natural. For example, the Neighborhood Learner
learns a pattern in which words with an odd number
of syllables bear initial stress but words with an even
number of syllables bear stress on all odd syllables.
However, the grammar for this language differs from
all of the attested systems in that it has two loops but
is slender (cf. Estonian which has two loops but is
not slender). Thus this case suggests a further formal
restriction to the class of possible stress systems.
More serious challenges of unattestable, but
Neighborhood Learner-able, patterns exist; e.g.
21*. In other words, it does not follow
from neighborhood-distinctness that languages with
stress must have stressless syllables. Nor does the
notion that every word must bear some stress some-
where (i.e. Culminativity? see Hayes (1995)).
However, despite the existence of learnable patho-
logical languages, this approach is not unrestricted.
The class of languages to be learned is finite?as
in the Optimality-theoretic and Principles and Pa-
rameters frameworks?and is a proper subset of the
regular languages. Future research will seek addi-
tional properties to better approximate the class of
QI stress systems that can be exploited by inductive
learning.
This approach offers more insight into QI stress
systems than earlier learning models. Optimality-
theoretic learning models (e.g. (Tesar, 1998)) and
models set in the Principles and Parameters frame-
work (e.g. (Dresher and Kaye, 1990)) make no use
of any property of the class of patterns to be learned
beyond its finiteness. Also, our learner is much sim-
pler than these other models, which require a large
set of a priori switches and cues or constraints.
6 Conclusions
This paper presented a batch learner which correctly
infers the attested QI stress patterns from surface
forms. The key to the success of this learner is that it
takes advantage of a universal property of QI stress
systems, neighborhood-distinctness. This property
provides a natural explanation for why stress falls
within a particular window of the word edge and
why rhythms are binary and ternary. It is strik-
ing that all of the attested patterns are learned by
this simple approach, suggesting that it will be fruit-
ful and revealing when applied to other phonotactic
learning problems.
Acknowledgements
I especially thank Bruce Hayes, Ed Stabler, Colin
Wilson, Kie Zuraw and the anonymous reviewers for
insightful comments and suggestions. I also thank
Sarah Churng, Greg Kobele, Katya Pertsova, and
Sarah VanWagenen for helpful discussion.
References
Adam Albright. 2006. Gradient phonotactic effects: lex-
ical? grammatical? both? neither? Talk handout from
the 80th Annual LSA Meeting, Albuquerque, NM.
Dan Albro. 1998. Evaluation, implementation, and ex-
tension of primitive optimality theory. Master?s thesis,
University of California, Los Angeles.
Dan Albro. 2005. A Large-Scale, LPM-OT Analysis of
Malagasy. Ph.D. thesis, University of California, Los
Angeles.
Dana Angluin. 1980. Finding patterns common to a set
of strings. Journal of Computer and System Sciences,
21:46?62.
Dana Angluin. 1982. Inference of reversible languages.
Journal for the Association of Computing Machinery,
29(3):741?765.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper & Row.
28
John Coleman and Janet Pierrehumbert. 1997. Stochas-
tic phonological grammars and acceptability. In Com-
puational Phonolgy, pages 49?56. Somerset, NJ: As-
sociation for Computational Linguistics. Third Meet-
ing of the ACL Special Interest Group in Computa-
tional Phonology.
Elan Dresher and Jonathan Kaye. 1990. A computa-
tional learning model for metrical phonology. Cogni-
tion, 34:137?195.
Jason Eisner. 1997. What constraints should ot allow?
Talk handout, Linguistic Society of America, Chicago,
January. Available on the Rutgers Optimality Archive,
ROA#204-0797, http://roa.rutgers.edu/.
T.M. Ellison. 1994. The iterative learning of phonologi-
cal constraints. Computational Linguistics, 20(3).
S. Frisch, N.R. Large, and D.B. Pisoni. 2000. Percep-
tion of wordlikeness: Effects of segment probability
and length on the processing of nonwords. Journal of
Memory and Language, 42:481?496.
E.M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447?474.
John Goldsmith. 1994. A dynamic computational the-
ory of accent systems. In Jennifer Cole and Charles
Kisseberth, editors, Perspectives in Phonology, pages
1?28. Stanford: Center for the Study of Language and
Information.
Matthew Gordon. 2002. A factorial typology of
quantity-insensitive stress. Natural Language and
Linguistic Theory, 20(3):491?552. Appendices avail-
able at http://www.linguistics.ucsb.edu/faculty/
gordon/pubs.html.
Prahlad Gupta and David Touretzky. 1991. What a per-
ceptron reveals about metrical phonology. In Proceed-
ings of the Thirteenth Annual Conference of the Cog-
nitive Science Society, pages 334?339.
Morris Halle. 1978. Knowledge unlearned and untaught:
What speakers know about the sounds of their lan-
guage. In Linguistic Theory and Psychological Real-
ity. The MIT Prss.
Bruce Hayes and Colin Wilson. 2006. The ucla phono-
tactic learner. Talk handout from UCLA Phonology
Seminar.
Bruce Hayes. 1995. Metrical Stress Theory. Chicago
University Press.
John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.
2001. Introduction to Automata Theory, Languages,
and Computation. Addison-Wesley.
C. Douglas Johnson. 1972. Formal Aspects of Phonolog-
ical Description. The Hague: Mouton.
Ronald Kaplan and Martin Kay. 1981. Phonological
rules and finite state transducers. Paper presented at
ACL/LSA Conference, New York.
Ronald Kaplan and Martin Kay. 1994. Regular models
of phonological rule systems. Computational Linguis-
tics, 20(3):331?378.
Lauri Karttunen. 1998. The proper treatment of optimal-
ity theory in computational phonology. Finite-state
methods in natural language processing, pages 1?12.
Mark Liberman and Alan Prince. 1977. On stress and
linguistic rhythm. Linguistic Inquiry, 8:249?336.
John McCarthy and Alan Prince. 1990. Foot and word
in prosodic morphology. Natural Language and Lin-
guistic Theory, 8:209?283.
Daniel Osherson, Scott Weinstein, and Michael Stob.
1986. Systems that Learn. MIT Press, Cambridge,
Massachusetts.
Alan Prince and Paul Smolensky. 1993. Optimal-
ity theory: Constraint interaction in generative gram-
mar. Technical Report 2, Rutgers University Center
for Cognitive Science.
Alan Prince. 1980. A metrical theory for estonian quan-
tity. Linguistic Inquiry, 11:511?562.
Alan Prince. 1983. Relating to the grid. Linguistic In-
quiry, 14(1).
Jason Riggle. 2004. Generation, Recognition, and
Learning in Finite State Optimality Theory. Ph.D. the-
sis, University of California, Los Angeles.
Bruce Tesar. 1998. An interative strategy for language
learning. Lingua, 104:131?145.
Appendix. Target Grammars and Results
See Table 2. Circled numbers mean the learner iden-
tified the pattern. The ? mark means the learner
failed to identify the pattern. The number inside the
circle indicates which forms were necessary for con-
vergence. Specifically, in the ?With Monosyllables?
column, ?n means the learner succeeded learning
the ?With Monosyllables? pattern with words with
one to n syllables. Likewise, in the ?Without Mono-
syllables? column, ?n means the learner succeeded
learning the ?Without Monosyllables? pattern with
words with two to n syllables. For example, in
the ?With Monosyllables? column, ?5 means that
the learner succeeded only with words with one to
five syllables. The learners still succeed when given
longer words. The number n may be thought of as
the smallest word needed for generalization.
29
Table 2: Learning Results
With Monosyllables Without Monosyllables
Language RegExp N
H
L
F
B
N
L
RegExp N
H
L
F
B
N
L
Single
1. Chitimacha 20? ?4 ?4 20+ ?4 ?4
2. Lakota (2|020?) ?5 ?5 020? ?5 ?5
3. Hopi (qs) (2|20|020+) ?5 ?5 (020?|2)0 ? ?
4. Macedonian (2|20|0?200) ?6 ?6 (2|0?20)0 ? ?6
5. Nahuatl (2|0?20) ?5 ?5 0?20 ?5 ?5
6. Atayal 0?2 ?4 ?4 0+2 ?4 ?4
Dual
7. Quebec French (2|10?2) ?5 ?5 10?2 ?5 ?5
8. Udihe (2|(10?)?02) ?5 ?6 (10?)?02 ?5 ?6
9. Lower Sorbian (2|2(0|0+1)0) ? ?6 2(0|0+1)0 ? ?6
10. Sanuma (2|20|020|10+20) ?6 ?7 (2|02|10+2)0 ?6 ?7
11. Georgian (2|20|0?200|10+200) ?7 ?8 (2|0?20|10+20)0 ? ?8
12. Walmatjari (2|20(0?10)?0?) ? ?6 20(0?10)?0? ? ?6
Binary
13. Araucanian (2|02(01)?0?) ?6 ?6 02(01)?0? ?6 ?6
14. Creek (qs) (2|(01)?020?) ?6 ?6 (01)?020? ?6 ?6
15. Urubu Kappor 0?(10)?2 ?5 ?5 (0|10)(10)?2 ?5 ?5
16. Malakmalak (2|0?2(01)?0) ?6 ?6 0?2(01)?0 ?6 ?6
17. Cavinen?a (2|0?(10)?20) ?6 ?6 0?(10)?20 ?6 ?6
18. Maranungku 2(01)?0? ?5 ?5 20(10)?1? ?5 ?5
19. Palestinean Arabic (qs) (10)?20? ?5 ?5 (20|(10)+20?) ?5 ?5
Binary w/clash
20. Central Alaskan Yupik (0(10)?1?)?2 ?5 ?5 0(10)?1?2 ?5 ?5
21. Southern Paiute (2|(2|02(01) ? 1?)0) ?7 ?8 (2|02(01) ? 1?)0 ?7 ?8
22. Gosiute Shoshone 2((01)?0?1)? ?5 ?6 2(01)?0?1 ?5 ?6
23. Biangai (2|1?(10)?20) ?7 ?7 1?(10)?20 ?7 ?7
24. Tauya (2|1?(10)?2) ?6 ?6 1?(10)?2 ?6 ?6
Binary w/lapse
25. Piro (2|(10)?0?20) ?6 ?7 (10)?0?20 ?6 ?7
26. Pintupi 2(0(10)?0?)? ?6 ?6 20(10)?0? ?6 ?6
27. Indonesian (2|(10)?0?(10)?20) ? ?8 (10)?0?(10)?20 ? ?8
28. Garawa 2(00?(10)?)? ?6 ?6 200?(10)? ?6 ?6
Ternary
29. Ioway Oto (2|02(001)?0?0?) ?7 ?8 02(001)?0?0? ?7 ?8
30. Cayuvava (0?0?(100)?200|20|2) ? ?9 (0?0?(100)?20|2)0 ? ?9
31. Estonian (qs) 20?0?(100|10)? ?6 ?6 200?(100|10)? ?6 ?6
32. Pacific Yupik (qs) (2|0(100)?(20?|102)) ?7 ?7 0(100)?(20?|102) ?7 ?7
33. Winnebago (qs) (2|02|002(001)?0?1?) ?9 ?9 (02|002(001)?0?1?) ?9 ?9
NHL : Neighborhood Learner FBNL : Forward Backward Neighborhood Learner
30
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 886?896,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Estimating Strictly Piecewise Distributions
Jeffrey Heinz
University of Delaware
Newark, Delaware, USA
heinz@udel.edu
James Rogers
Earlham College
Richmond, Indiana, USA
jrogers@quark.cs.earlham.edu
Abstract
Strictly Piecewise (SP) languages are a
subclass of regular languages which en-
code certain kinds of long-distance de-
pendencies that are found in natural lan-
guages. Like the classes in the Chom-
sky and Subregular hierarchies, there are
many independently converging character-
izations of the SP class (Rogers et al, to
appear). Here we define SP distributions
and show that they can be efficiently esti-
mated from positive data.
1 Introduction
Long-distance dependencies in natural language
are of considerable interest. Although much at-
tention has focused on long-distance dependencies
which are beyond the expressive power of models
with finitely many states (Chomsky, 1956; Joshi,
1985; Shieber, 1985; Kobele, 2006), there are
some long-distance dependencies in natural lan-
guage which permit finite-state characterizations.
For example, although it is well-known that vowel
and consonantal harmony applies across any ar-
bitrary number of intervening segments (Ringen,
1988; Bakovic?, 2000; Hansson, 2001; Rose and
Walker, 2004) and that phonological patterns are
regular (Johnson, 1972; Kaplan and Kay, 1994),
it is less well-known that harmony patterns are
largely characterizable by the Strictly Piecewise
languages, a subregular class of languages with
independently-motivated, converging characteri-
zations (see Heinz (2007, to appear) and especially
Rogers et al (2009)).
As shown by Rogers et al (to appear), the
Strictly Piecewise (SP) languages, which make
distinctions on the basis of (potentially) discon-
tiguous subsequences, are precisely analogous to
the Strictly Local (SL) languages (McNaughton
and Papert, 1971; Rogers and Pullum, to appear),
which make distinctions on the basis of contigu-
ous subsequences. The Strictly Local languages
are the formal-language theoretic foundation for
n-gram models (Garcia et al, 1990), which are
widely used in natural language processing (NLP)
in part because such distributions can be estimated
from positive data (i.e. a corpus) (Jurafsky and
Martin, 2008). N -gram models describe prob-
ability distributions over all strings on the basis
of the Markov assumption (Markov, 1913): that
the probability of the next symbol only depends
on the previous contiguous sequence of length
n ? 1. From the perspective of formal language
theory, these distributions are perhaps properly
called Strictly k-Local distributions (SLk) where
k = n. It is well-known that one limitation of the
Markov assumption is its inability to express any
kind of long-distance dependency.
This paper defines Strictly k-Piecewise (SPk)
distributions and shows how they too can be effi-
ciently estimated from positive data. In contrast
with the Markov assumption, our assumption is
that the probability of the next symbol is condi-
tioned on the previous set of discontiguous subse-
quences of length k ? 1 in the string. While this
suggests the model has too many parameters (one
for each subset of all possible subsequences), in
fact the model has on the order of |?|k+1 parame-
ters because of an independence assumption: there
is no interaction between different subsequences.
As a result, SP distributions are efficiently com-
putable even though they condition the probabil-
ity of the next symbol on the occurrences of ear-
lier (possibly very distant) discontiguous subse-
quences. Essentially, these SP distributions reflect
a kind of long-term memory.
On the other hand, SP models have no short-
term memory and are unable to make distinctions
on the basis of contiguous subsequences. We do
not intend SP models to replace n-gram models,
but instead expect them to be used alongside of
886
them. Exactly how this is to be done is beyond the
scope of this paper and is left for future research.
Since SP languages are the analogue of SL lan-
guages, which are the formal-language theoretical
foundation for n-gram models, which are widely
used in NLP, it is expected that SP distributions
and their estimation will also find wide applica-
tion. Apart from their interest to problems in the-
oretical phonology such as phonotactic learning
(Coleman and Pierrehumbert, 1997; Hayes and
Wilson, 2008; Heinz, to appear), it is expected that
their use will have application, in conjunction with
n-gram models, in areas that currently use them;
e.g. augmentative communication (Newell et al,
1998), part of speech tagging (Brill, 1995), and
speech recognition (Jelenik, 1997).
?2 provides basic mathematical notation. ?3
provides relevant background on the subregular hi-
erarchy. ?4 describes automata-theoretic charac-
terizations of SP languages. ?5 defines SP distri-
butions. ?6 shows how these distributions can be
efficiently estimated from positive data and pro-
vides a demonstration. ?7 concludes the paper.
2 Preliminaries
We start with some mostly standard notation. ?
denotes a finite set of symbols and a string over
? is a finite sequence of symbols drawn from
that set. ?k, ??k, ??k, and ?? denote all
strings over this alphabet of length k, of length
less than or equal to k, of length greater than
or equal to k, and of any finite length, respec-
tively. ? denotes the empty string. |w| denotes
the length of string w. The prefixes of a string
w are Pfx(w) = {v : ?u ? ?? such that vu = w}.
When discussing partial functions, the notation ?
and ? indicates that the function is undefined, re-
spectively is defined, for particular arguments.
A language L is a subset of ??. A stochastic
language D is a probability distribution over ??.
The probability p of word w with respect to D is
written PrD(w) = p. Recall that all distributions
D must satisfy ?w??? PrD(w) = 1. If L is lan-
guage then PrD(L) =
?
w?L PrD(w).
A Deterministic Finite-state Automaton (DFA)
is a tuple M = ?Q,?, q0, ?, F ? where Q is the
state set, ? is the alphabet, q0 is the start state,
? is a deterministic transition function with do-
main Q ? ? and codomain Q, F is the set of
accepting states. Let d? : Q ? ?? ? Q be
the (partial) path function of M, i.e., d?(q, w)
is the (unique) state reachable from state q
via the sequence w, if any, or d?(q, w)? other-
wise. The language recognized by a DFA M is
L(M) def= {w ? ?? | d?(q0, w)? ? F}.
A state is useful iff for all q ? Q, there exists
w ? ?? such that ?(q0, w) = q and there exists
w ? ?? such that ?(q, w) ? F . Useless states
are not useful. DFAs without useless states are
trimmed.
Two strings w and v over ? are distinguished
by a DFA M iff d?(q0, w) 6= d?(q0, v). They are
Nerode equivalent with respect to a language L
if and only if wu ? L ?? vu ? L for
all u ? ??. All DFAs which recognize L must
distinguish strings which are inequivalent in this
sense, but no DFA recognizing L necessarily dis-
tinguishes any strings which are equivalent. Hence
the number of equivalence classes of strings over
? modulo Nerode equivalence with respect to L
gives a (tight) lower bound on the number of states
required to recognize L.
A DFA is minimal if the size of its state set
is minimal among DFAs accepting the same lan-
guage. The product of n DFAs M1 . . .Mn is
given by the standard construction over the state
space Q1 ? . . .?Qn (Hopcroft et al, 2001).
A Probabilistic Deterministic Finite-
state Automaton (PDFA) is a tuple
M = ?Q,?, q0, ?, F, T ? where Q is the state
set, ? is the alphabet, q0 is the start state, ? is
a deterministic transition function, F and T are
the final-state and transition probabilities. In
particular, T : Q ? ? ? R+ and F : Q ? R+
such that
for all q ? Q, F (q) +
?
a??
T (q, a) = 1. (1)
Like DFAs, for all w ? ??, there is at most one
state reachable from q0. PDFAs are typically rep-
resented as labeled directed graphs as in Figure 1.
A PDFA M generates a stochastic language
DM. If it exists, the (unique) path for a word w =
a0 . . . ak belonging to ?? through a PDFA is a
sequence ?(q0, a0), (q1, a1), . . . , (qk, ak)?, where
qi+1 = ?(qi, ai). The probability a PDFA assigns
to w is obtained by multiplying the transition prob-
abilities with the final probability along w?s path if
887
A:2/10
b:2/10
c:3/10
B:4/9a:3 /10
a:2/9
b:2/9
c:1/9
Figure 1: A picture of a PDFA with states labeled
A and B. The probabilities of T and F are located
to the right of the colon.
it exists, and zero otherwise.
PrDM(w) =
( k
?
i=1
T (qi?1, ai?1)
)
?F (qk+1) (2)
if d?(q0, w)? and 0 otherwise
A probability distribution is regular deterministic
iff there is a PDFA which generates it.
The structural components of a PDFA M are
its states Q, its alphabet ?, its transitions ?, and
its initial state q0. By structure of a PDFA, we
mean its structural components. Each PDFA M
defines a family of distributions given by the pos-
sible instantiations of T and F satisfying Equa-
tion 1. These distributions have |Q|? (|?| + 1) in-
dependent parameters (since for each state there
are |?| possible transitions plus the possibility of
finality.)
We define the product of PDFA in terms of co-
emission probabilities (Vidal et al, 2005a).
Definition 1 Let A be a vector of PDFAs and let
|A| = n. For each 1 ? i ? n let Mi =
?Qi,?, q0i, ?i, Fi, Ti? be the ith PDFA in A. The
probability that ? is co-emitted from q1, . . . , qn in
Q1, . . . , Qn, respectively, is
CT (??, q1 . . . qn?) =
n
?
i=1
Ti(qi, ?).
Similarly, the probability that a word simultane-
ously ends at q1 ? Q1 . . . qn ? Qn is
CF (?q1 . . . qn?) =
n
?
i=1
Fi(qi).
Then
?A = ?Q,?, q0, ?, F, T ? where
1. Q, q0, and ? are defined as with DFA product.
2. For all ?q1 . . . qn? ? Q, let
Z(?q1 . . . qn?) =
CF (?q1 . . . qn?) +
?
???
CT (??, q1 . . . qn?)
be the normalization term; and
(a) let F (?q1 . . . qn?) = CF (?q1 ... qn?)Z(?q1 ... qn?) ;
and
(b) for all ? ? ?, let
T (?q1 . . . qn?, ?) = CT (??, q1 ... qn?)Z(?q1 ... qn?)
In other words, the numerators of T and F are de-
fined to be the co-emission probabilities (Vidal et
al., 2005a), and division by Z ensures that M de-
fines a well-formed probability distribution. Sta-
tistically speaking, the co-emission product makes
an independence assumption: the probability of ?
being co-emitted from q1, . . . , qn is exactly what
one expects if there is no interaction between the
individual factors; that is, between the probabil-
ities of ? being emitted from any qi. Also note
order of product is irrelevant up to renaming of
the states, and so therefore we also speak of tak-
ing the product of a set of PDFAs (as opposed to
an ordered vector).
Estimating regular deterministic distributions is
well-studied problem (Vidal et al, 2005a; Vidal et
al., 2005b; de la Higuera, in press). We limit dis-
cussion to cases when the structure of the PDFA is
known. Let S be a finite sample of words drawn
from a regular deterministic distribution D. The
problem is to estimate parameters T and F of M
so that DM approaches D. We employ the widely-
adopted maximum likelihood (ML) criterion for
this estimation.
(T? , F? ) = argmax
T,F
(
?
w?S
PrM(w)
)
(3)
It is well-known that if D is generated by some
PDFA M? with the same structural components as
M, then optimizing the ML estimate guarantees
that DM approaches D as the size of S goes to
infinity (Vidal et al, 2005a; Vidal et al, 2005b;
de la Higuera, in press).
The optimization problem (3) is simple for de-
terministic automata with known structural com-
ponents. Informally, the corpus is passed through
the PDFA, and the paths of each word through the
corpus are tracked to obtain counts, which are then
normalized by state. Let M = ?Q,?, ?, q0, F, T ?
be the PDFA whose parameters F and T are to be
estimated. For all states q ? Q and symbols a ?
?, The ML estimation of the probability of T (q, a)
is obtained by dividing the number of times this
transition is used in parsing the sample S by the
888
A:2
b:2
c:3
B:4a :3
a :2
b:2
c:1
Figure 2: The automata shows the counts
obtained by parsing M with sample
S = {ab, bba, ?, cab, acb, cc}.
SL SP
LT PT
LTT
SF
FO
Reg MSO
Prop
+1 <
Figure 3: Parallel Sub-regular Hierarchies.
number of times state q is encountered in the pars-
ing of S. Similarly, the ML estimation of F (q) is
obtained by calculating the relative frequency of
state q being final with state q being encountered
in the parsing of S. For both cases, the division is
normalizing; i.e. it guarantees that there is a well-
formed probability distribution at each state. Fig-
ure 2 illustrates the counts obtained for a machine
M with sample S = {ab, bba, ?, cab, acb, cc}.1
Figure 1 shows the PDFA obtained after normaliz-
ing these counts.
3 Subregular Hierarchies
Within the class of regular languages there are
dual hierarchies of language classes (Figure 3),
one in which languages are defined in terms of
their contiguous substrings (up to some length k,
known as k-factors), starting with the languages
that are Locally Testable in the Strict Sense (SL),
and one in which languages are defined in terms
of their not necessarily contiguous subsequences,
starting with the languages that are Piecewise
1Technically, this acceptor is neither a simple DFA or
PDFA; rather, it has been called a Frequency DFA. We do
not formally define them here, see (de la Higuera, in press).
Testable in the Strict Sense (SP). Each language
class in these hierarchies has independently mo-
tivated, converging characterizations and each has
been claimed to correspond to specific, fundamen-
tal cognitive capabilities (McNaughton and Pa-
pert, 1971; Brzozowski and Simon, 1973; Simon,
1975; Thomas, 1982; Perrin and Pin, 1986; Garc??a
and Ruiz, 1990; Beauquier and Pin, 1991; Straub-
ing, 1994; Garc??a and Ruiz, 1996; Rogers and Pul-
lum, to appear; Kontorovich et al, 2008; Rogers et
al., to appear).
Languages in the weakest of these classes are
defined only in terms of the set of factors (SL)
or subsequences (SP) which are licensed to oc-
cur in the string (equivalently the complement of
that set with respect to ??k, the forbidden fac-
tors or forbidden subsequences). For example, the
set containing the forbidden 2-factors {ab, ba} de-
fines a Strictly 2-Local language which includes
all strings except those with contiguous substrings
{ab, ba}. Similarly since the parameters of n-
gram models (Jurafsky and Martin, 2008) assign
probabilities to symbols given the preceding con-
tiguous substrings up to length n? 1, we say they
describe Strictly n-Local distributions.
These hierarchies have a very attractive model-
theoretic characterization. The Locally Testable
(LT) and Piecewise Testable languages are exactly
those that are definable by propositional formulae
in which the atomic formulae are blocks of sym-
bols interpreted factors (LT) or subsequences (PT)
of the string. The languages that are testable in the
strict sense (SL and SP) are exactly those that are
definable by formulae of this sort restricted to con-
junctions of negative literals. Going the other way,
the languages that are definable by First-Order for-
mulae with adjacency (successor) but not prece-
dence (less-than) are exactly the Locally Thresh-
old Testable (LTT) languages. The Star-Free lan-
guages are those that are First-Order definable
with precedence alone (adjacency being FO defin-
able from precedence). Finally, by extending to
Monadic Second-Order formulae (with either sig-
nature, since they are MSO definable from each
other), one obtains the full class of Regular lan-
guages (McNaughton and Papert, 1971; Thomas,
1982; Rogers and Pullum, to appear; Rogers et al,
to appear).
The relation between strings which is funda-
mental along the Piecewise branch is the subse-
889
quence relation, which is a partial order on ??:
w ? v def?? w = ? or w = ?1 ? ? ? ?n and
(?w0, . . . , wn ? ??)[v = w0?1w1 ? ? ? ?nwn].
in which case we say w is a subsequence of v.
For w ? ??, let
Pk(w)
def= {v ? ?k | v ? w} and
P?k(w)
def= {v ? ??k | v ? w},
the set of subsequences of length k, respectively
length no greater than k, of w. Let Pk(L) and
P?k(L) be the natural extensions of these to sets
of strings. Note that P0(w) = {?}, for all w ? ??,
that P1(w) is the set of symbols occurring in w and
that P?k(L) is finite, for all L ? ??.
Similar to the Strictly Local languages, Strictly
Piecewise languages are defined only in terms of
the set of subsequences (up to some length k)
which are licensed to occur in the string.
Definition 2 (SPk Grammar, SP) A SPk gram-
mar is a pair G = ??, G? where G ? ?k. The
language licensed by a SPk grammar is
L(G) def= {w ? ?? | P?k(w) ? P?k(G)}.
A language is SPk iff it is L(G) for some SPk
grammar G. It is SP iff it is SPk for some k.
This paper is primarily concerned with estimat-
ing Strictly Piecewise distributions, but first we
examine in greater detail properties of SP lan-
guages, in particular DFA representations.
4 DFA representations of SP Languages
Following Sakarovitch and Simon (1983),
Lothaire (1997) and Kontorovich, et al (2008),
we call the set of strings that contain w as a
subsequence the principal shuffle ideal2 of w:
SI(w) = {v ? ?? | w ? v}.
The shuffle ideal of a set of strings is defined as
SI(S) = ?w?SSI(w)
Rogers et al (to appear) establish that the SP lan-
guages have a variety of characteristic properties.
Theorem 1 The following are equivalent:3
2Properly SI(w) is the principal ideal generated by {w}
wrt the inverse of ?.
3For a complete proof, see Rogers et al (to appear). We
only note that 5 implies 1 by DeMorgan?s theorem and the
fact that every shuffle ideal is finitely generated (see also
Lothaire (1997)).
1
b
c
2a
b
c
Figure 4: The DFA representation of SI(aa).
1. L =
?
w?S [SI(w)], S finite,
2. L ? SP
3. (?k)[P?k(w) ? P?k(L) ? w ? L],
4. w ? L and v ? w ? v ? L (L is subse-
quence closed),
5. L = SI(X), X ? ?? (L is the complement
of a shuffle ideal).
The DFA representation of the complement of a
shuffle ideal is especially important.
Lemma 1 Let w ? ?k, w = ?1 ? ? ? ?k,
and MSI(w) = ?Q,?, q0, ?, F ?, where Q =
{i | 1 ? i ? k}, q0 = 1, F = Q and for all
qi ? Q,? ? ?:
?(qi, ?) =
?
?
?
qi+1 if ? = ?i and i < k,
? if ? = ?i and i = k,
qi otherwise.
Then MSI(w) is a minimal, trimmed DFA that rec-
ognizes the complement of SI(w), i.e., SI(w) =
L(MSI(w)).
Figure 4 illustrates the DFA representation of
the complement of SI(aa) with ? = {a, b, c}. It is
easy to verify that the machine in Figure 4 accepts
all and only those words which do not contain an
aa subsequence.
For any SPk language L = L(??, G?) 6= ??,
the first characterization (1) in Theorem 1 above
yields a non-deterministic finite-state representa-
tion of L, which is a set A of DFA representations
of complements of principal shuffle ideals of the
elements of G. The trimmed automata product of
this set yields a DFA, with the properties below
(Rogers et al, to appear).
Lemma 2 Let M be a trimmed DFA recognizing
a SPk language constructed as described above.
Then:
1. All states of M are accepting states: F = Q.
890
ab
c
b
c
b
a
c
a
b
b
c
b
b
a
b
? ?,a
?,b
?,c
?,a,b
?,b,c
?,a,c
?,a,b,c
Figure 5: The DFA representation of the of the
SP language given by G = ?{a, b, c}, {aa, bc}?.
Names of the states reflect subsets of subse-
quences up to length 1 of prefixes of the language.
Note this DFA is trimmed, but not minimal.
2. For all q1, q2 ? Q and ? ? ?, if d?(q1, ?)?
and d?(q1, w) = q2 for some w ? ?? then
d?(q2, ?)?. (Missing edges propagate down.)
Figure 5 illustrates with the DFA representa-
tion of the of the SP2 language given by G =
?{a, b, c}, {aa, bc}?. It is straightforward to ver-
ify that this DFA is identical (modulo relabeling of
state names) to one obtained by the trimmed prod-
uct of the DFA representations of the complement
of the principal shuffle ideals of aa and bc, which
are the prohibited subsequences.
States in the DFA in Figure 5 correspond to the
subsequences up to length 1 of the prefixes of the
language. With this in mind, it follows that the
DFA of ?? = L(?,?k) has states which corre-
spond to the subsequences up to length k ? 1 of
the prefixes of ??. Figure 6 illustrates such a DFA
when k = 2 and ? = {a, b, c}.
In fact, these DFAs reveal the differences be-
tween SP languages and PT languages: they are
exactly those expressed in Lemma 2. Within the
state space defined by the subsequences up to
length k ? 1 of the prefixes of the language, if the
conditions in Lemma 2 are violated, then the DFAs
describe languages that are PT but not SP. Pictori-
ally, PT2 languages are obtained by arbitrarily re-
moving arcs, states, and the finality of states from
the DFA in Figure 6, and SP2 ones are obtained by
non-arbitrarily removing them in accordance with
Lemma 2. The same applies straightforwardly for
any k (see Definition 3 below).
a
b
c
a b
c
b a
c
c
a
b
a
b
c
a
c b
b
c
a
a
b
c
? ?,a
?,b
?,c
?,a,b
?,b,c
?,a,c
?,a,b,c
Figure 6: A DFA representation of the of the SP2
language given by G = ?{a, b, c},?2?. Names
of the states reflect subsets of subsequences up to
length 1 of prefixes of the language. Note this
DFA is trimmed, but not minimal.
5 SP Distributions
In the same way that SL distributions (n-gram
models) generalize SL languages, SP distributions
generalize SP languages. Recall that SP languages
are characterizable by the intersection of the com-
plements of principal shuffle ideals. SP distribu-
tions are similarly characterized.
We begin with Piecewise-Testable distributions.
Definition 3 A distribution D is k-Piecewise
Testable (written D ? PTDk) def?? D can be de-
scribed by a PDFA M = ?Q,?, q0, ?, F, T ? with
1. Q = {P?k?1(w) : w ? ??}
2. q0 = P?k?1(?)
3. For all w ? ?? and all ? ? ?,
?(P?k?1(w), a) = P?k?1(wa)
4. F and T satisfy Equation 1.
In other words, a distribution is k-Piecewise
Testable provided it can be represented by a PDFA
whose structural components are the same (mod-
ulo renaming of states) as those of the DFA dis-
cussed earlier where states corresponded to the
subsequences up to length k ? 1 of the prefixes
of the language. The DFA in Figure 6 shows the
891
structure of a PDFA which describes a PT2 distri-
bution as long as the assigned probabilities satisfy
Equation 1.
The following lemma follows directly from the
finite-state representation of PTk distributions.
Lemma 3 Let D belong to PTDk and let M =
?Q,?, q0, ?, F, T ? be a PDFA representing D de-
fined according to Definition 3.
PrD(?1 . . . ?n) = T (P?k?1(?), ?1) ?
?
?
?
2?i?n
T (P?k?1(?1 . . . ?i?1), ?i)
?
? (4)
? F (P?k?1(w))
PTk distributions have 2|?|
k?1
(|?|+1) parameters
(since there are 2|?|k?1 states and |?|+1 possible
events, i.e. transitions and finality).
Let Pr(? | #) and Pr(# | P?k(w)) denote
the probability (according to some D ? PTDk)
that a word begins with ? and ends after observ-
ing P?k(w). Then Equation 4 can be rewritten in
terms of conditional probability as
PrD(?1 . . . ?n) = Pr(?1 | #) ?
?
?
?
2?i?n
Pr(?i | P?k?1(?1 . . . ?i?1))
?
?(5)
? Pr(# | P?k?1(w))
Thus, the probability assigned to a word depends
not on the observed contiguous sequences as in a
Markov model, but on observed subsequences.
Like SP languages, SP distributions can be de-
fined in terms of the product of machines very sim-
ilar to the complement of principal shuffle ideals.
Definition 4 Let w ? ?k?1 and w = ?1 ? ? ? ?k?1.
Mw = ?Q,?, q0, ?, F, T ? is a w-subsequence-
distinguishing PDFA (w-SD-PDFA) iff
Q = Pfx(w), q0 = ?, for all u ? Pfx(w)
and each ? ? ?,
?(u, ?) = u? iff u? ? Pfx(w) and
u otherwise
and F and T satisfy Equation 1.
Figure 7 shows the structure of Ma which is
almost the same as the complement of the princi-
pal shuffle ideal in Figure 4. The only difference
is the additional self-loop labeled a on the right-
most state labeled a. Ma defines a family of dis-
tributions over ??, and its states distinguish those
b
c
a
a
a
b
c
?
Figure 7: The structure of PDFA Ma. It is the
same (modulo state names) as the DFA in Figure 4
except for the self-loop labeled a on state a.
strings which contain a (state a) from those that
do not (state ?). A set of PDFAs is a k-set of SD-
PDFAs iff, for each w ? ??k?1, it contains ex-
actly one w-SD-PDFA.
In the same way that missing edges propagate
down in DFA representations of SP languages
(Lemma 2), the final and transitional probabili-
ties must propagate down in PDFA representa-
tions of SPk distributions. In other words, the fi-
nal and transitional probabilities at states further
along paths beginning at the start state must be de-
termined by final and transitional probabilities at
earlier states non-increasingly. This is captured by
defining SP distributions as a product of k-sets of
SD-PDFAs (see Definition 5 below).
While the standard product based on co-
emission probability could be used for this pur-
pose, we adopt a modified version of it defined
for k-sets of SD-PDFAs: the positive co-emission
probability. The automata product based on the
positive co-emission probability not only ensures
that the probabilities propagate as necessary, but
also that such probabilities are made on the ba-
sis of observed subsequences, and not unobserved
ones. This idea is familiar from n-gram models:
the probability of ?n given the immediately pre-
ceding sequence ?1 . . . ?n?1 does not depend on
the probability of ?n given the other (n? 1)-long
sequences which do not immediately precede it,
though this is a logical possibility.
Let A be a k-set of SD-PDFAs. For each
w ? ??k?1, let Mw = ?Qw,?, q0w, ?w, Fw, Tw?
be the w-subsequence-distinguishing PDFA in A.
The positive co-emission probability that ? is si-
multaneously emitted from states q?, . . . , qu from
the statesets Q?, . . . Qu, respectively, of each SD-
892
PDFA in A is
PCT (??, q? . . . qu?) =
?
qw??q?...qu?
qw=w
Tw(qw, ?) (6)
Similarly, the probability that a word simultane-
ously ends at n states q? ? Q?, . . . , qu ? Qu is
PCF (?q? . . . qu?) =
?
qw??q?...qu?
qw=w
Fw(qw) (7)
In other words, the positive co-emission proba-
bility is the product of the probabilities restricted
to those assigned to the maximal states in each
Mw. For example, consider a 2-set of SD-
PDFAs A with ? = {a, b, c}. A contains four
PDFAs M?,Ma,Mb,Mc. Consider state q =
??, ?, b, c? ??A (this is the state labeled ?, b, c in
Figure 6). Then
CT (a, q) = T?(?, a)? Ta(?, a)? Tb(b, a)? Tc(c, a)
but
PCT (a, q) = T?(?, a)? Tb(b, a)? Tc(c, a)
since in PDFA Ma, the state ? is not the maximal
state.
The positive co-emission product (?+) is de-
fined just as with co-emission probabilities, sub-
stituting PCT and PCF for CT and CF, respec-
tively, in Definition 1. The definition of ?+ en-
sures that the probabilities propagate on the basis
of observed subsequences, and not on the basis of
unobserved ones.
Lemma 4 Let k ? 1 and let A be a k-set of SD-
PDFAs. Then ?+S defines a well-formed proba-
bility distribution over ??.
Proof Since M? belongs to A, it is always
the case that PCT and PCF are defined. Well-
formedness follows from the normalization term
as in Definition 1. ?
Definition 5 A distribution D is k-Strictly Piece-
wise (written D ? SPDk) def?? D can be described
by a PDFA which is the positive co-emission
product of a k-set of subsequence-distinguishing
PDFAs.
By Lemma 4, SP distributions are well-formed.
Unlike PDFAs for PT distributions, which distin-
guish 2|?|k?1 states, the number of states in a k-
set of SD-PDFAs is
?
i<k(i + 1)|?|i, which is
?(|?|k+1). Furthermore, since each SD-PDFA
only has one state contributing |?|+1 probabilities
to the product, and since there are |??k| = |?|k?1|?|?1
many SD-PDFAs in a k-set, there are
|?|k ? 1
|?| ? 1 ? (|?|+ 1) =
|?|k+1 + |?|k ? |?| ? 1
|?| ? 1
parameters, which is ?(|?|k).
Lemma 5 Let D ? SPDk. Then D ? PTDk.
Proof Since D ? SPDk, there is a k-set of
subsequence-distinguishing PDFAs. The product
of this set has the same structure as the PDFA
given in Definition 3. ?
Theorem 2 A distribution D ? SPDk if D can
be described by a PDFA M = ?Q,?, q0, ?, F, T ?
satisfying Definition 3 and the following.
For all w ? ?? and all ? ? ?, let
Z(w) =
?
s?P?k?1(w)
F (P?k?1(s)) +
?
????
?
?
?
s?P?k?1(w)
T (P?k?1(s), ??)
?
? (8)
(This is the normalization term.) Then T must sat-
isfy: T (P?k?1(w), ?) =
?
s?P?k?1(w) T (P?k?1(s), ?)
Z(w) (9)
and F must satisfy: F (P?k?1(w)) =
?
s?P?k?1(w) F (P?k?1(s))
Z(w) (10)
Proof That SPDk satisfies Definition 3 Follows
directly from Lemma 5. Equations 8-10 follow
from the definition of positive co-emission proba-
bility. ?
The way in which final and transitional proba-
bilities propagate down in SP distributions is re-
flected in the conditional probability as defined by
Equations 9 and 10. In terms of conditional prob-
ability, Equations 9 and 10 mean that the prob-
ability that ?i follows a sequence ?1 . . . ?i?1 is
not only a function of P?k?1(?1 . . . ?i?1) (Equa-
tion 4) but further that it is a function of each
subsequence in ?1 . . . ?i?1 up to length k ? 1.
893
In particular, Pr(?i | P?k?1(?1 . . . ?i?1)) is ob-
tained by substituting Pr(?i | P? k?1(s)) for
T (P? k?1(s), ?) and Pr(# | P? k?1(s)) for
F (P?k?1(s)) in Equations 8, 9 and 10. For ex-
ample, for a SP2 distribution, the probability of
a given P?1(bc) (state ?, b, c in Figure 6) is the
normalized product of the probabilities of a given
P?1(?), a given P?1(b), and a given P?1(c).
To summarize, SP and PT distributions are reg-
ular deterministic. Unlike PT distributions, how-
ever, SP distributions can be modeled with only
?(|?|k) parameters and ?(|?|k+1) states. This
is true even though SP distributions distinguish
2|?|
k?1
states! Since SP distributions can be rep-
resented by a single PDFA, computing Pr(w) oc-
curs in only ?(|w|) for such PDFA. While such
PDFA might be too large to be practical, Pr(w)
can also be computed from the k-set of SD-PDFAs
in ?(|w|k) (essentially building the path in the
product machine on the fly using Equations 4, 8, 9
and 10).
6 Estimating SP Distributions
The problem of ML estimation of SPk distribu-
tions is reduced to estimating the parameters of the
SD-PDFAs. Training (counting and normaliza-
tion) occurs over each of these machines (i.e. each
machine parses the entire corpus), which gives the
ML estimates of the parameters of the distribution.
It trivially follows that this training successfully
estimates any D ? SPDk.
Theorem 3 For any D ? SPDk, let D generate
sample S. Let A be the k-set of SD-PDFAs which
describes exactly D. Then optimizing the MLE of
S with respect to each M ? A guarantees that the
distribution described by the positive co-emission
product of ?+A approaches D as |S| increases.
Proof The MLE estimate of S with respect to
SPDk returns the parameter values that maximize
the likelihood of S. The parameters of D ? SPDk
are found on the maximal states of each M ? A.
By definition, each M ? A describes a proba-
bility distribution over ??, and similarly defines
a family of distributions. Therefore finding the
MLE of S with respect to SPDk means finding the
MLE estimate of S with respect to each of the fam-
ily of distributions which each M ? A defines,
respectively.
Optimizing the ML estimate of S for each
M ? A means that as |S| increases, the estimates
T?M and F?M approach the true values TM and
FM. It follows that as |S| increases, T?N+ A and
F?N+ A approach the true values of TN+ A and
FN+ A and consequently DN+ A approaches D. ?
We demonstrate learning long-distance depen-
dencies by estimating SP2 distributions given a
corpus from Samala (Chumash), a language with
sibilant harmony.4 There are two classes of sibi-
lants in Samala: [-anterior] sibilants like [s] and
[>ts] and [+anterior] sibilants like [S] and [>tS].5
Samala words are subject to a phonological pro-
cess wherein the last sibilant requires earlier sibi-
lants to have the same value for the feature [an-
terior], no matter how many sounds intervene
(Applegate, 1972). As a consequence of this
rule, there are generally no words in Samala
where [-anterior] sibilants follow [+anterior]. E.g.
[StojonowonowaS] ?it stood upright? (Applegate
1972:72) is licit but not *[Stojonowonowas].
The results of estimating D ? SPD2 with
the corpus is shown in Table 6. The results
clearly demonstrate the effectiveness of the model:
the probability of a [? anterior] sibilant given
P?1([-? anterior]) sounds is orders of magnitude
less than given P?1(? anterior]) sounds.
x
Pr(x | P?1(y))
s
>
ts S
>
tS
s 0.0335 0.0051 0.0011 0.0002
?ts 0.0218 0.0113 0.0009 0.
y S 0.0009 0. 0.0671 0.0353
>
tS 0.0006 0. 0.0455 0.0313
Table 1: Results of SP2 estimation on the Samala
corpus. Only sibilants are shown.
7 Conclusion
SP distributions are the stochastic version of SP
languages, which model long-distance dependen-
cies. Although SP distributions distinguish 2|?|k?1
states, they do so with tractably many parameters
and states because of an assumption that distinct
subsequences do not interact. As shown, these
distributions are efficiently estimable from posi-
tive data. As previously mentioned, we anticipate
these models to find wide application in NLP.
4The corpus was kindly provided by Dr. Richard Apple-
gate and drawn from his 2007 dictionary of Samala.
5Samala actually contrasts glottalized, aspirated, and
plain variants of these sounds (Applegate, 1972). These la-
ryngeal distinctions are collapsed here for easier exposition.
894
References
R.B. Applegate. 1972. Inesen?o Chumash Grammar.
Ph.D. thesis, University of California, Berkeley.
R.B. Applegate. 2007. Samala-English dictionary : a
guide to the Samala language of the Inesen?o Chu-
mash People. Santa Ynez Band of Chumash Indi-
ans.
Eric Bakovic?. 2000. Harmony, Dominance and Con-
trol. Ph.D. thesis, Rutgers University.
D. Beauquier and Jean-Eric Pin. 1991. Languages and
scanners. Theoretical Computer Science, 84:3?21.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?566.
J. A. Brzozowski and Imre Simon. 1973. Character-
izations of locally testable events. Discrete Mathe-
matics, 4:243?271.
Noam Chomsky. 1956. Three models for the descrip-
tion of language. IRE Transactions on Information
Theory. IT-2.
J. S. Coleman and J. Pierrehumbert. 1997. Stochastic
phonological grammars and acceptability. In Com-
putational Phonology, pages 49?56. Somerset, NJ:
Association for Computational Linguistics. Third
Meeting of the ACL Special Interest Group in Com-
putational Phonology.
Colin de la Higuera. in press. Grammatical Infer-
ence: Learning Automata and Grammars. Cam-
bridge University Press.
Pedro Garc??a and Jose? Ruiz. 1990. Inference of k-
testable languages in the strict sense and applica-
tions to syntactic pattern recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 9:920?925.
Pedro Garc??a and Jose? Ruiz. 1996. Learning k-
piecewise testable languages from positive data. In
Laurent Miclet and Colin de la Higuera, editors,
Grammatical Interference: Learning Syntax from
Sentences, volume 1147 of Lecture Notes in Com-
puter Science, pages 203?210. Springer.
Pedro Garcia, Enrique Vidal, and Jose? Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325?338.
Gunnar Hansson. 2001. Theoretical and typological
issues in consonant harmony. Ph.D. thesis, Univer-
sity of California, Berkeley.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379?440.
Jeffrey Heinz. 2007. The Inductive Learning of
Phonotactic Patterns. Ph.D. thesis, University of
California, Los Angeles.
Jeffrey Heinz. to appear. Learning long distance
phonotactics. Linguistic Inquiry.
John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.
2001. Introduction to Automata Theory, Languages,
and Computation. Addison-Wesley.
Frederick Jelenik. 1997. Statistical Methods for
Speech Recognition. MIT Press.
C. Douglas Johnson. 1972. Formal Aspects of Phono-
logical Description. The Hague: Mouton.
A. K. Joshi. 1985. Tree-adjoining grammars: How
much context sensitivity is required to provide rea-
sonable structural descriptions? In D. Dowty,
L. Karttunen, and A. Zwicky, editors, Natural Lan-
guage Parsing, pages 206?250. Cambridge Univer-
sity Press.
Daniel Jurafsky and James Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. Prentice-Hall, 2nd edi-
tion.
Ronald Kaplan and Martin Kay. 1994. Regular models
of phonological rule systems. Computational Lin-
guistics, 20(3):331?378.
Gregory Kobele. 2006. Generating Copies: An In-
vestigation into Structural Identity in Language and
Grammar. Ph.D. thesis, University of California,
Los Angeles.
Leonid (Aryeh) Kontorovich, Corinna Cortes, and
Mehryar Mohri. 2008. Kernel methods for learn-
ing languages. Theoretical Computer Science,
405(3):223 ? 236. Algorithmic Learning Theory.
M. Lothaire, editor. 1997. Combinatorics on Words.
Cambridge University Press, Cambridge, UK, New
York.
A. A. Markov. 1913. An example of statistical study
on the text of ?eugene onegin? illustrating the linking
of events to a chain.
Robert McNaughton and Simon Papert. 1971.
Counter-Free Automata. MIT Press.
A. Newell, S. Langer, and M. Hickey. 1998. The
ro?le of natural language processing in alternative and
augmentative communication. Natural Language
Engineering, 4(1):1?16.
Dominique Perrin and Jean-Eric Pin. 1986. First-
Order logic and Star-Free sets. Journal of Computer
and System Sciences, 32:393?406.
Catherine Ringen. 1988. Vowel Harmony: Theoretical
Implications. Garland Publishing, Inc.
895
James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation.
James Rogers, Jeffrey Heinz, Matt Edlefsen, Dylan
Leeman, Nathan Myers, Nathaniel Smith, Molly
Visscher, and David Wellcome. to appear. On lan-
guages piecewise testable in the strict sense. In Pro-
ceedings of the 11th Meeting of the Assocation for
Mathematics of Language.
Sharon Rose and Rachel Walker. 2004. A typology of
consonant agreement as correspondence. Language,
80(3):475?531.
Jacques Sakarovitch and Imre Simon. 1983. Sub-
words. In M. Lothaire, editor, Combinatorics on
Words, volume 17 of Encyclopedia of Mathemat-
ics and Its Applications, chapter 6, pages 105?134.
Addison-Wesley, Reading, Massachusetts.
Stuart Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Phi-
losophy, 8:333?343.
Imre Simon. 1975. Piecewise testable events. In
Automata Theory and Formal Languages: 2nd
Grammatical Inference conference, pages 214?222,
Berlin ; New York. Springer-Verlag.
Howard Straubing. 1994. Finite Automata, Formal
Logic and Circuit Complexity. Birkha?user.
Wolfgang Thomas. 1982. Classifying regular events in
symbolic logic. Journal of Computer and Systems
Sciences, 25:360?376.
Enrique Vidal, Franck Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005a. Probabilistic finite-state machines-part I.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(7):1013?1025.
Enrique Vidal, Frank Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005b. Probabilistic finite-state machines-part II.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 27(7):1026?1039.
896
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 897?906,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
String Extension Learning
Jeffrey Heinz
University of Delaware
Newark, Delaware, USA
heinz@udel.edu
Abstract
This paper provides a unified, learning-
theoretic analysis of several learnable
classes of languages discussed previously
in the literature. The analysis shows that
for these classes an incremental, globally
consistent, locally conservative, set-driven
learner always exists. Additionally, the
analysis provides a recipe for constructing
new learnable classes. Potential applica-
tions include learnable models for aspects
of natural language and cognition.
1 Introduction
The problem of generalizing from examples to
patterns is an important one in linguistics and
computer science. This paper shows that many
disparate language classes, many previously dis-
cussed in the literature, have a simple, natural
and interesting (because non-enumerative) learner
which exactly identifies the class in the limit from
distribution-free, positive evidence in the sense of
Gold (Gold, 1967).1 These learners are called
String Extension Learners because each string in
the language can be mapped (extended) to an ele-
ment of the grammar, which in every case, is con-
ceived as a finite set of elements. These learners
have desirable properties: they are incremental,
globally consistent, and locally conservative.
Classes previously discussed in the litera-
ture which are string extension learnable in-
clude the Locally Testable (LT) languages, the
Locally Testable Languages in the Strict Sense
1The allowance of negative evidence (Gold, 1967) or re-
stricting the kinds of texts the learner is required to succeed
on (i.e. non-distribution-free evidence) (Gold, 1967; Horn-
ing, 1969; Angluin, 1988) admits the learnability of the class
of recursively enumerable languages. Classes of languages
learnable in the harder, distribution-free, positive-evidence-
only settings are due to structural properties of the language
classes that permit generalization (Angluin, 1980b; Blumer
et al, 1989). That is the central interest here.
(Strictly Local, SL) (McNaughton and Papert,
1971; Rogers and Pullum, to appear), the Piece-
wise Testable (PT) languages (Simon, 1975), the
Piecewise Testable languages in the Strict Sense
(Strictly Piecewise, SP) (Rogers et al, 2009), the
Strongly Testable languages (Beauquier and Pin,
1991), the Definite languages (Brzozowski, 1962),
and the Finite languages, among others. To our
knowledge, this is the first analysis which identi-
fies the common structural elements of these lan-
guage classes which allows them to be identifiable
in the limit from positive data: each language class
induces a natural partition over all logically possi-
ble strings and each language in the class is the
union of finitely many blocks of this partition.
One consequence of this analysis is a recipe
for constructing new learnable classes. One no-
table case is the Strictly Piecewise (SP) languages,
which was originally motivated for two reasons:
the learnability properties discussed here and its
ability to describe long-distance dependencies in
natural language phonology (Heinz, 2007; Heinz,
to appear). Later this class was discovered to have
several independent characterizations and form
the basis of another subregular hierarchy (Rogers
et al, 2009).
It is expected string extension learning will have
applications in linguistic and cognitive models. As
mentioned, the SP languages already provide a
novel hypothesis of how long-distance dependen-
cies in sound patterns are learned. Another exam-
ple is the Strictly Local (SL) languages which are
the categorical, symbolic version of n-gram mod-
els, which are widely used in natural language pro-
cessing (Jurafsky and Martin, 2008). Since the SP
languages also admit a probabilistic variant which
describe an efficiently estimable class of distribu-
tions (Heinz and Rogers, 2010), it is plausible to
expect the other classes will as well, though this is
left for future research.
String extension learners are also simple, mak-
897
ing them accessible to linguists without a rigorous
mathematical background.
This paper is organized as follow. ?2 goes
over basic notation and definitions. ?3 defines
string extension grammars, languages, and lan-
guage classes and proves some of their fundamen-
tal properties. ?4 defines string extension learn-
ers and proves their behavior. ?5 shows how im-
portant subregular classes are string extension lan-
guage classes. ?6 gives examples of nonregular
and infinite language classes which are string ex-
tension learnable. ?7 summarizes the results, and
discusses lines of inquiry for future research.
2 Preliminaries
This section establishes notation and recalls basic
definitions for formal languages, the paradigm of
identification in the limit from positive data (Gold,
1967). Familiarity with the basic concepts of sets,
functions, and sequences is assumed.
For some set A, P(A) denotes the set of all
subsets of A and Pfin(A) denotes the set of all
finite subsets of A. If f is a function such that
f : A ? B then let f?(a) = {f(a)}. Thus,
f? : A ? P(B) (note f? is not surjective). A
set ? of nonempty subsets of S is a partition of S
iff the elements of ? (called blocks) are pairwise
disjoint and their union equals S.
? denotes a fixed finite set of symbols, the al-
phabet. Let ?n, ??n, ??, ?+ denote all strings
formed over this alphabet of length n, of length
less than or equal to n, of any finite length, and
of any finite length strictly greater than zero, re-
spectively. The term word is used interchangeably
with string. The range of a string w is the set
of symbols which are in w. The empty string is
the unique string of length zero denoted ?. Thus
range(?) = ?. The length of a string u is de-
noted by |u|, e.g. |?| = 0. A language L is
some subset of ??. The reverse of a language
Lr = {wr : w ? L}.
Gold (1967) establishes a learning paradigm
known as identification in the limit from positive
data. A text is an infinite sequence whose ele-
ments are drawn from ?? ? {#} where # rep-
resents a non-expression. The ith element of t is
denoted t(i), and t[i] denotes the finite sequence
t(0), t(1), . . . t(i). Following Jain et al (1999),
let SEQ denote the set of all possible finite se-
quences:
SEQ = {t[i] : t is a text and i ? N}
The content of a text is defined below.
content(t) =
{w ? ?? : ?n ? N such that t(n) = w}
A text t is a positive text for a language L iff
content(t) = L. Thus there is only one text t for
the empty language: for all i, t(i) = #.
A learner is a function ? which maps ini-
tial finite sequences of texts to grammars,
i.e. ? : SEQ ? G. The elements of G (the gram-
mars) generate languages in some well-defined
way. A learner converges on a text t iff there exists
i ? N and a grammar G such that for all j > i,
?(t[j]) = G.
For any grammar G, the language it generates is
denoted L(G). A learner ? identifies a language
L in the limit iff for any positive text t for L, ?
converges on t to grammar G and L(G) = L. Fi-
nally, a learner ? identifies a class of languages L
in the limit iff for any L ? L, ? identifies L in
the limit. Angluin (1980b) provides necessary and
sufficient properties of language classes which are
identifiable in the limit from positive data.
A learner ? of language class L is globally con-
sistent iff for each i and for all texts t for some
L ? L, content(t[i]) ? L(?(t[i])). A learner ? is
locally conservative iff for each i and for all texts
t for some L ? L, whenever ?(t[i]) 6= ?(t[i? 1]),
it is the case that t(i) 6? L(?([i?1])). These terms
are from Jain et al (2007). Also, learners which
do not depend on the order of the text are called
set-driven (Jain et al, 1999, p. 99).
3 Grammars and Languages
Consider some set A. A string extension function
is a total function f : ?? ? Pfin(A). It is not
required that f be onto. Denote the class of func-
tions which have this general form SEF .
Each string extension function is naturally as-
sociated with some formal class of grammars and
languages. These functions, grammars, and lan-
guages are called string extension functions, gram-
mars, and languages, respectively.
Definition 1 Let f ? SEF .
1. A grammar is a finite subset of A.
2. The language of grammar G is
Lf (G) = {w ? ?? : f(w) ? G}
898
3. The class of languages obtained by all possi-
ble grammars is
Lf = {Lf (G) : G ? Pfin(A)}
The subscript f is omitted when it is understood
from context.
A function f ? SEF naturally induces a par-
tition ?f over ??. Strings u and v are equivalent
(u ?f v) iff f(u) = f(v).
Theorem 1 Every language L ? Lf is a finite
union of blocks of ?f .
Proof: Follows directly from the definition of ?f
and the finiteness of string extension grammars. 2
We return to this result in ?6.
Theorem 2 Lf is closed under intersection.
Proof: We show L1?L2 = L(G1?G2). Consider
any word w belonging to L1 and L2. Then f(w)
is a subset of G1 and of G2. Thus f(w) ? G1 ?
G2, and therefore w ? L(G1 ? G2). The other
inclusion follows similarly. 2
String extension language classes are not in gen-
eral closed under union or reversal (counterexam-
ples to union closure are given in ?5.1 and to re-
versal closure in ?6.)
It is useful to extend the domain of the function
f from strings to languages.
f(L) =
?
w?L
f(w) (1)
An element g of grammar G for language L =
Lf (G) is useful iff g ? f(L). An element is use-
less if it is not useful. A grammar with no useless
elements is called canonical.
Remark 1 Fix a function f ? SEF . For every
L ? Lf , there is a canonical grammar, namely
f(L). In other words, L = L(f(L)).
Lemma 1 Let L,L? ? Lf . L ? L? iff f(L) ?
f(L?)
Proof: (?) Suppose L ? L? and consider any
g ? f(L). Since g is useful, there is a w ? L such
that g ? f(w). But f(w) ? f(L?) since w ? L?.
(?) Suppose f(L) ? f(L?) and consider any
w ? L. Then f(w) ? f(L) so by transitivity,
f(w) ? f(L?). Therefore w ? L?. 2
The significance of this result is that as the gram-
mar G monotonically increases, the language
L(G) monotonically increases too. The following
result can now be proved, used in the next section
on learning.2
Theorem 3 For any finite L0 ? ??, L =
L(f(L0)) is the smallest language in Lf contain-
ing L0.
Proof: Clearly L0 ? L. Suppose L? ? Lf and
L0 ? L?. It follows directly from Lemma 1 that
L ? L? (since f(L) = f(L0) ? f(L?)). 2
4 String Extension Learning
Learning string extension classes is simple. The
initial hypothesis of the learner is the empty gram-
mar. The learner?s next hypothesis is obtained by
applying function f to the current observation and
taking the union of that set with the previous one.
Definition 2 For all f ? SEF and for all t ?
SEQ, define ?f as follows:
?f (t[i]) =
?
?
?
? if i = ?1
?f (t[i? 1]) if t(i) = #
?f (t[i? 1]) ? f(t(i)) otherwise
By convention, the initial state of the grammar
is given by ?(t[?1]) = ?. The learner ?f exem-
plifies string extension learning. Each individual
string in the text reveals, by extension with f , as-
pects of the canonical grammar for L ? Lf .
Theorem 4 ?f is globally consistent, locally con-
servative, and set-driven.
Proof: Global consistness and local conservative-
ness follow immediately from Definition 2. For
set-drivenness, witness (by Definition 2) it is the
case that for any text t and any i ? N, ?(t[i]) =
f(content(t[i])). 2
The key to the proof that ?f identifies Lf in the
limit from positive data is the finiteness of G for
all L(G) ? L. The idea is that there is a point
in the text in which every element of the grammar
has been seen because (1) there are only finitely
many useful elements of G, and (2) the learner is
guaranteed to see a word in L which yields (via f )
each element of G at some point (since the learner
receives a positive text for L). Thus at this point
2The requirement in Theorem 3 that L0 be finite can be
dropped if the qualifier ?in Lf ? be dropped as well. This
can be seen when one considers the identity function and the
class of finite languages. (The identity function is a string
extension function, see ?6.) In this case, id(??) = ??, but
?? is not a member of Lfin. However since the interest here
is learners which generalize on the basis of finite experience,
Theorem 3 is sufficient as is.
899
the learner ? is guaranteed to have converged to
the target G as no additional words will add any
more elements to the learner?s grammar.
Lemma 2 For all L ? Lf , there is a finite sample
S such that L is the smallest language in Lf con-
taining S. S is called a characteristic sample of L
in Lf (S is also called a tell-tale).
Proof: For L ? Lf , construct the sample S as
follows. For each g ? f(L), choose some word
w ? L such that g ? f(w). Since f(L) is finite
(Remark 1), S is finite. Clearly f(S) = f(L) and
thus L = L(f(S)). Therefore, by Theorem 3, L is
the smallest language in Lf containing S. 2
Theorem 5 Fix f ? SEF . Then ?f identifies Lf
in the limit.
Proof: For any L ? Lf , there is a characteristic fi-
nite sample S for L (Lemma 2). Thus for any text t
for L, there is i such that S ? content(t[i]). Thus
for any j > i, ?(t(j)) is the smallest language
in Lf containing S by Theorem 3 and Lemma 2.
Thus, ?(t(j)) = f(S) = f(L). 2
An immediate corollary is the efficiency of ?f
in the length of the sample, provided f is efficient
in the length of the string (de la Higuera, 1997).
Corollary 1 ?f is efficient in the length of the
sample iff f is efficiently computable in the length
of a string.
To summarize: string extension grammars are
finite subsets of some set A. The class of lan-
guages they generate are determined by a func-
tion f which maps strings to finite subsets of A
(chunks of grammars). Since the size of the canon-
ical grammars is finite, a learner which develops a
grammar on the basis of the observed words and
the function f identifies this class exactly in the
limit from positive data. It also follows that if f
is efficient in the length of the string then ?f is ef-
ficient in the length of the sample and that ?f is
globally consistent, locally conservative, and set-
driven. It is striking that such a natural and gen-
eral framework for generalization exists and that,
as will be shown, a variety of language classes can
be expressed given the choice of f .
5 Subregular examples
This section shows how classes which make up
the subregular hierarchies (McNaughton and Pa-
pert, 1971) are string extension language classes.
Readers are referred to Rogers and Pullum (2007)
and Rogers et al (2009) for an introduction to the
subregular hierarchies, as well as their relevance
to linguistics and cognition.
5.1 K-factor languages
The k-factors of a word are the contiguous subse-
quences of length k in w. Consider the following
string extension function.
Definition 3 For some k ? N, let
fack(w) =
{x ? ?k : ?u, v ? ??
such that w = uxv} when k ? |w| and
{w} otherwise
Following the earlier definitions, for some k, a
grammar G is a subset of ??k and a word w be-
longs to the language of G iff fack(w) ? G.
Example 1 Let ? = {a, b} and consider gram-
mars G = {?, a, aa, ab, ba}. Then L(G) =
{?, a} ? {w : |w| ? 2 and w 6? ??bb??}. The 2-
factor bb is a prohibited 2-factor for L(G). Clearly,
L(G) ? Lfac2 .
Languages in Lfack make distinctions based on
which k-factors are permitted or prohibited. Since
fack ? SEF , it follows immediately from the
results in ??3-4 that the k-factor languages are
closed under intersection, and each has a char-
acteristic sample. For example, a characteristic
sample for the 2-factor language in Example 1 is
{?, a, ab, ba, aa}; i.e. the canonical grammar it-
self. It follows from Theorem 5 that the class of
k-factor languages is identifiable in the limit by
?fack . The learner ?fac2 with a text from the lan-
guage in Example 1 is illustrated in Table 1.
The class Lfack is not closed under
union. For example for k = 2, con-
sider L1 = L({?, a, b, aa, bb, ba}) and
L2 = L({?, a, b, aa, ab, bb}). Then L1 ? L2
excludes string aba, but includes ab and ba, which
is not possible for any L ? Lfack .
K-factors are used to define other language
classes, such as the Strictly Local and Lo-
cally Testable languages (McNaughton and Pa-
pert, 1971), discussed in ?5.4 and ?5.5.
5.2 Strictly k-Piecewise languages
The Strictly k-Piecewise (SPk) languages (Rogers
et al, 2009) can be defined with a function whose
co-domain is P(??k). However unlike the func-
tion fack, the function SPk, does not require that
the k-length subsequences be contiguous.
900
i t(i) fac2(t(i)) Grammar G L(G)
-1 ? ?
0 aaaa {aa} {aa} aaa?
1 aab {aa, ab} {aa, ab} aaa? ? aaa?b
2 a {a} {a, aa, ab} aa? ? aa?b
. . .
Table 1: The learner ?fac2 with a text from the language in Example 1. Boldtype indicates newly added
elements to the grammar.
A string u = a1 . . . ak is a subsequence of
string w iff ? v0, v1, . . . vk ? ?? such that w =
v0a1v1 . . . akvk. The empty string ? is a subse-
quence of every string. When u is a subsequence
of w we write u ? w.
Definition 4 For some k ? N,
SPk(w) = {u ? ??k : u ? w}
In other words, SPk(w) returns all subse-
quences, contiguous or not, in w up to length k.
Thus, for some k, a grammar G is a subset of ??k.
Following Definition 1, a word w belongs to the
language of G only if SP2(w) ? G.3
Example 2 Let ? = {a, b} and consider the
grammar G = {?, a, b, aa, ab, ba}. Then L(G) =
??\(??b??b??).
As seen from Example 2, SP languages encode
long-distance dependencies. In Example 2, L pro-
hibits a b from following another b in a word, no
matter how distant. Table 2 illustrates ?SP2 learn-
ing the language in Example 2.
Heinz (2007,2009a) shows that consonantal
harmony patterns in natural language are describ-
able by such SP2 languages and hypothesizes
that humans learn them in the way suggested by
?SP2 . Strictly 2-Piecewise languages have also
been used in models of reading comprehension
(Whitney, 2001; Grainger and Whitney, 2004;
Whitney and Cornelissen, 2008) as well as text
classification(Lodhi et al, 2002; Cancedda et al,
2003) (see also (Shawe-Taylor and Christianini,
2005, chap. 11)).
5.3 K-Piecewise Testable languages
A language L is k-Piecewise Testable iff when-
ever strings u and v have the same subsequences
3In earlier work, the function SP2 has been described
as returning the set of precedence relations in w, and the
language class LSP2 was called the precedence languages
(Heinz, 2007; Heinz, to appear).
of length at most k and u is in L, then v is in L as
well (Simon, 1975; Simon, 1993; Lothaire, 2005).
A language L is said to be Piecewise-Testable
(PT) if it is k-Piecewise Testable for some k ? N.
If k is fixed, the k-Piecewise Testable languages
are identifiable in the limit from positive data
(Garc??a and Ruiz, 1996; Garc??a and Ruiz, 2004).
More recently, the Piecewise Testable languages
has been shown to be linearly separable with a
subsequence kernel (Kontorovich et al, 2008).
The k-Piecewise Testable languages can also
be described with the function SP ?k . Recall that
f?(a) = {f(a)}. Thus functions SP ?k define
grammars as a finite list of sets of subsequences
up to length k that may occur in words in the lan-
guage. This reflects the fact that the k-Piecewise
Testable languages are the boolean closure of the
Strictly k-Piecewise languages.4
5.4 Strictly k-Local languages
To define the Strictly k-Local languages, it is nec-
essary to make a pointwise extension to the defini-
tions in ?3.
Definition 5 For sets A1, . . . , An, suppose for
each i, fi : ?? ? Pfin(Ai), and let f =
(f1, . . . , fn).
1. A grammar G is a tuple (G1, . . . , Gn) where
G1 ? Pfin(A1), . . . , Gn ? Pfin(An).
2. If for any w ? ??, each fi(w) ? Gi for all
1 ? i ? n, then f(w) is a pointwise subset
of G, written f(w) ?? G.
3. The language of grammar G is
Lf (G) = {w : f(w) ?? G}
4. The class of languages obtained by all such
possible grammars G is Lf .
4More generally, it is not hard to show that Lf? is the
boolean closure of Lf .
901
i t(i) SP2(t(i)) Grammar G Language of G
-1 ? ?
0 aaaa {?, a, aa} {?, a, aa} a?
1 aab {?, a, b, aa, ab} {?, a, aa, b, ab} a? ? a?b
2 baa {?, a, b, aa, ba} {?, a, b, aa, ab, ba} ??\(??b??b??)
3 aba {?, a, b, ab, ba} {?, a, b, aa, ab, ba} ??\(??b??b??)
. . .
Table 2: The learner ?SP2 with a text from the language in Example 2. Boldtype indicates newly added
elements to the grammar.
These definitions preserve the learning results
of ?4. Note that the characteristic sample of L ?
Lf will be the union of the characteristic samples
of each fi and the language Lf (G) is the intersec-
tion of Lfi(Gi).
Locally k-Testable Languages in the Strict
Sense (Strictly k-Local) have been studied by sev-
eral researchers (McNaughton and Papert, 1971;
Garcia et al, 1990; Caron, 2000; Rogers and Pul-
lum, to appear), among others. We follow the
definitions from (McNaughton and Papert, 1971,
p. 14), effectively encoded in the following func-
tions.
Definition 6 Fix k ? N. Then the (left-edge) pre-
fix of length k, the (right-edge) suffix of length k,
and the interior k-factors of a word w are
Lk(w) = {u ? ?k : ?v ? ?? such that w = uv}
Rk(w) = {u ? ?k : ?v ? ?? such that w = vu}
Ik(w) = fack(w)\(Lk(w) ?Rk(w))
Example 3 Suppose w = abcba. Then L2(w) =
{ab}, R2(w) = {ba} and I2(w) = {bc, cb}.
Example 4 Suppose |w| = k. Then Lk(w) =
Rk(w) = {w} and Ik(w) = ?.
Example 5 Suppose |w| is less than k. Then
Lk(w) = Rk(w) = ? and Ik(w) = {w}.
A language L is k-Strictly Local (k-SL) iff for
all w ? L, there exist sets L,R, and I such
that w ? L iff Lk(w) ? L, Rk(w) ? R, and
Ik(w) ? I . McNaughton and Papert note that if
w is of length less than k than L may be perfectly
arbitrary about w.
This can now be expressed as the string exten-
sion function:
LRIk(w) = (Lk(w), Rk(w), Ik(w))
Thus for some k, a grammar G is triple formed
by taking subsets of ?k, ?k, and ??k, respec-
tively. A word w belongs to the language of G
only if LRIk(w) ?? G. Clearly, LLRIk = k-
SL, and henceforth we refer to this class as k-SL.
Since, for fixed k, LRIk ? SEF , all of the learn-
ing results in ?4 apply.
5.5 Locally k-Testable languages
The Locally k-testable languages (k-LT) are orig-
inally defined in McNaughton and Papert (1971)
and are the subject of several studies (Brzozowski
and Simon, 1973; McNaughton, 1974; Kim et
al., 1991; Caron, 2000; Garc??a and Ruiz, 2004;
Rogers and Pullum, to appear).
A language L is k-testable iff for all w1, w2 ?
?? such that |w1| ? k and |w2| ? k, and
LRIk(w1) = LRIk(w2) then either both w1, w2
belong to L or neither do. Clearly, every language
in k-SL belongs to k-LT. However k-LT prop-
erly include k-SL because a k-testable language
only distinguishes words whenever LRIk(w1) 6=
LRIk(w2). It is known that the k-LT languages
are the boolean closure of the k-SL (McNaughton
and Papert, 1971).
The function LRI?k exactly expresses k-testable
languages. Informally, each word w is mapped
to a set containing a single element, this element
is the triple LRIk(w). Thus a grammar G is a
subset of the triples used to define k-SL. Clearly,
LLRI?k = k-LT since it is the boolean closure of
LLRIk . Henceforth we refer to LLRI?k as the k-
Locally Testable (k-LT) languages.
5.6 Generalized subsequence languages
Here we introduce generalized subsequence func-
tions, a general class of functions to which the
SPk and fack functions belong. Like those
functions, generalized subsequence functions map
words to a set of subsequences found within the
words. These functions are instantiated by a vec-
tor whose number of coordinates determine how
many times a subsequence may be discontiguous
902
and whose coordinate values determine the length
of each contiguous part of the subsequence.
Definition 7 For some n ? N, let ~v =
?v0, v1, . . . , vn?, where each vi ? N. Let k be
the length of the subsequences; i.e. k =
?n
0 vi.
f~v(w) =
{u ? ?k : ?x0, . . . , xn, u0, . . . , un+1 ? ??
such that w = u0x0u1x1, . . . , unxnun+1
and |xi| = vi for all 0 ? i ? n}
when k ? |w|, and{w} otherwise
The following examples help make the general-
ized subsequence functions clear.
Example 6 Let ~v = ?2?. Then f?2? = fac2. Gen-
erally, f?k? = fack.
Example 7 Let ~v = ?1, 1?. Then f?1,1? = SP2.
Generally, if ~v = ?1, . . . 1? with |~v| = k. Then
f~v = SPk.
Example 8 Let ~v = ?3, 2, 1? and a, b, c, d, e, f?
?. Then Lf?3,2,1? includes languages which
prohibit strings w which contain subsequences
abcdef where abc and de must be contiguous in
w and abcdef is a subsequence of w.
Generalized subsequence languages make dif-
ferent kinds of distinctions to be made than PT and
LT languages. For example, the language in Ex-
ample 8 is neither k-LT nor k?-PT for any values
k, k?. Generalized subsequence languages prop-
erly include the k-SP and k-SL classes (Exam-
ples 6 and 7), and the boolean closure of the sub-
sequence languages (f?~v ) properly includes the LT
and PT classes.
Since for any ~v, f~v and f?~v are string extension
functions the learning results in ?4 apply. Note
that f~v(w) is computable in time O(|w|k) where k
is the length of the maximal subsequences deter-
mined by ~v.
6 Other examples
This section provides examples of infinite and
nonregular language classes that are string exten-
sion learnable. Recall from Theorem 1 that string
extension languages are finite unions of blocks of
the partition of ?? induced by f . Assuming the
blocks of this partition can be enumerated, the
range of f can be construed as Pfin(N).
grammar G Language of G
? ?
{0} anbn
{1} ??\anbn
{0, 1} ??
Table 3: The language class Lf from Example 9
In the examples considered so far, the enumera-
tion of the blocks is essentially encoded in partic-
ular substrings (or tuples of substrings). However,
much less clever enumerations are available.
Example 9 Let A = {0,1} and consider the fol-
lowing function:
f(w) =
{
0 iff w ? anbn
1 otherwise
The function f belongs to SEF because it is maps
strings to a finite co-domain. Lf has four lan-
guages shown in Table 3.
The language class in Example 9 is not regular be-
cause it includes the well-known context-free lan-
guage anbn. This collection of languages is also
not closed under reversal.
There are also infinite language classes that are
string extension language classes. Arguably the
simplest example is the class of finite languages,
denoted Lfin.
Example 10 Consider the function id which
maps words in ?? to their singleton sets, i.e.
id(w) = {w}.5 A grammar G is then a finite
subset of ??, and so L(G) is just a finite set of
words in ??; in fact, L(G) = G. It follows that
Lid = Lfin.
It can be easily seen that the function id induces
the trivial partition over ??, and languages are
just finite unions of these blocks. The learner ?id
makes no generalizations at all, and only remem-
bers what it has observed.
There are other more interesting infinite string
extension classes. Here is one relating to the
Parikh map (Parikh, 1966). For all a ? ?, let
fa(w) be the set containing n where n is the num-
ber of times the letter a occurs in the string w. For
5Strictly speaking, this is not the identity function per
se, but it is as close to the identity function as one can get
since string extension functions are defined as mappings from
strings to sets. However, once the domain of the function is
extended (Equation 1), then it follows that id is the identity
function when its argument is a set of strings.
903
example fa(babab) = {2}. Thus fa is a total func-
tion mapping strings to singleton sets of natural
numbers, so it is a string extension function. This
function induces an infinite partition of ??, where
the words in any particular block have the same
number of letters a. It is convenient to enumerate
the blocks according to how many occurrences of
the letter a may occur in words within the block.
Hence, B0 is the block whose words have no oc-
currences of a, B1 is the block whose words have
one occurrence of a, and so on.
In this case, a grammar G is a finite subset ofN,
e.g. {2, 3, 4}. L(G) is simply those words which
have either 2, 3, or 4, occurrences of the letter a.
Thus Lfa is an infinite class, which contains lan-
guages of infinite size, which is easily identified in
the limit from positive data by ?fa .
This section gave examples of nonregular and
nonfinite string extension classes by pursuing the
implications of Theorem 1, which established that
f ? SEF partition ?? into blocks of which lan-
guages are finite unions thereof. The string exten-
sion function f provides an effective way of en-
coding all languages L in Lf because f(L) en-
codes a finite set, the grammar.
7 Conclusion and open questions
One contribution of this paper is a unified way of
thinking about many formal language classes, all
of which have been shown to be identifiable in
the limit from positive data by a string extension
learner. Another contribution is a recipe for defin-
ing classes of languages identifiable in the limit
from positive data by this kind of learner.
As shown, these learners have many desirable
properties. In particular, they are globally consis-
tent, locally conservative, and set-driven. Addi-
tionally, the learner is guaranteed to be efficient
in the size of the sample, provided the function f
itself is efficient in the length of the string.
Several additional questions of interest remain
open for theoretical linguistics, theoretical com-
puter science, and computational linguistics.
For theoretical linguistics, it appears that the
string extension function f = (LRI3, P2), which
defines a class of languages which obey restric-
tions on both contiguous subsequences of length
3 and on discontiguous subsequences of length 2,
provides a good first approximation to the seg-
mental phonotactic patterns in natural languages
(Heinz, 2007). The string extension learner for
this class is essentially two learners: ?LRI3 and
?P2 , operating simultaneously.6 The learners
make predictions about generalizations, which can
be tested in artificial language learning experi-
ments on adults and infants (Rogers and Pullum, to
appear; Chambers et al, 2002; Onishi et al, 2003;
Cristia? and Seidl, 2008).7
For theoretical computer science, it remains an
open question what property holds of functions
f in SEF to ensure that Lf is regular, context-
free, or context-sensitive. For known subregular
classes, there are constructions that provide deter-
ministic automata that suggest the relevant prop-
erties. (See, for example, Garcia et al (1990) and
Garica and Ruiz (1996).)
Also, Timo Ko?tzing and Samuel Moelius (p.c.)
suggest that the results here may be generalized
along the following lines. Instead of defining the
function f as a map from strings to finite subsets,
let f be a function from strings to elements of a
lattice. A grammar G is an element of the lattice
and the language of the G are all strings w such
that f maps w to a grammar less than G. Learners
?f are defined as the least upper bound of its cur-
rent hypothesis and the grammar to which f maps
the current word.8 Kasprzik and Ko?tzing (2010)
develop this idea and demonstrate additional prop-
erties of string extension classes and learning, and
show that the pattern languages (Angluin, 1980a)
form a string extension class.9
Also, hyperplane learning (Clark et al, 2006a;
Clark et al, 2006b) and function-distinguishable
learning (Fernau, 2003) similarly associate lan-
guage classes with functions. How those analyses
relate to the current one remains open.
Finally, since the stochastic counterpart of k-
SL class is the n-gram model, it is plausible that
probabilistic string extension language classes can
form the basis of new natural language process-
ing techniques. (Heinz and Rogers, 2010) show
6This learner resembles what learning theorists call par-
allel learning (Case and Moelius, 2007) and what cognitive
scientists call modular learning (Gallistel and King, 2009).
7I conjecture that morphological and syntactic patterns
are generally not amenable to a string extension learning
analysis because these patterns appear to require a paradigm,
i.e. a set of data points, before any conclusion can be confi-
dently drawn about the generating grammar. Stress patterns
also do not appear to be amenable to a string extension learn-
ing (Heinz, 2007; Edlefsen et al, 2008; Heinz, 2009).
8See also Lange et al (2008, Theorem 15) and Case et al
(1999, pp.101-103).
9The basic idea is to consider the lattice L = ?Lfin,??.
Each element of L is a finite set of strings representing the
intersection of all pattern languages consistent with this set.
904
how to efficiently estimate k-SP distributions, and
it is conjectured that the other string extension lan-
guage classes can be recast as classes of distri-
butions, which can also be successfully estimated
from positive evidence.
Acknowledgments
This work was supported by a University of
Delaware Research Fund grant during the 2008-
2009 academic year. I would like to thank John
Case, Alexander Clark, Timo Ko?tzing, Samuel
Moelius, James Rogers, and Edward Stabler for
valuable discussion. I would also like to thank
Timo Ko?tzing for careful reading of an earlier
draft and for catching some errors. Remaining er-
rors are my responsibility.
References
Dana Angluin. 1980a. Finding patterns common to
a set of strings. Journal of Computer and System
Sciences, 21:46?62.
Dana Angluin. 1980b. Inductive inference of formal
languages from positive data. Information Control,
45:117?135.
Dana Angluin. 1988. Identifying languages from
stochastic examples. Technical Report 614, Yale
University, New Haven, CT.
D. Beauquier and J.E. Pin. 1991. Languages and scan-
ners. Theoretical Computer Science, 84:3?21.
Anselm Blumer, Andrzej Ehrenfeucht, David Haus-
sler, and Manfred K. Warmuth. 1989. Learnability
and the Vapnik-Chervonenkis dimension. J. ACM,
36(4):929?965.
J.A. Brzozowski and I. Simon. 1973. Characterization
of locally testable events. Discrete Math, 4:243?
271.
J.A. Brzozowski. 1962. Canonical regular expres-
sions and minimal state graphs for definite events. In
Mathematical Theory of Automata, pages 529?561.
New York.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean-Michel Renders. 2003. Word-sequence ker-
nels. Journal of Machine Learning Research,
3:1059?1082.
Pascal Caron. 2000. Families of locally testable lan-
guages. Theoretical Computer Science, 242:361?
376.
John Case and Sam Moelius. 2007. Parallelism
increases iterative learning power. In 18th An-
nual Conference on Algorithmic Learning Theory
(ALT07), volume 4754 of Lecture Notes in Artificial
Intelligence, pages 49?63. Springer-Verlag, Berlin.
John Case, Sanjay Jain, Steffen Lange, and Thomas
Zeugmann. 1999. Incremental concept learning for
bounded data mining. Information and Computa-
tion, 152:74?110.
Kyle E. Chambers, Kristine H. Onishi, and Cynthia
Fisher. 2002. Learning phonotactic constraints from
brief auditory experience. Cognition, 83:B13?B23.
Alexander Clark, Christophe Costa Flore?ncio, and
Chris Watkins. 2006a. Languages as hyperplanes:
grammatical inference with string kernels. In Pro-
ceedings of the European Conference on Machine
Learning (ECML), pages 90?101.
Alexander Clark, Christophe Costa Flore?ncio, Chris
Watkins, and Mariette Serayet. 2006b. Planar
languages and learnability. In Proceedings of the
8th International Colloquium on Grammatical Infer-
ence (ICGI), pages 148?160.
Alejandrina Cristia? and Amanda Seidl. 2008. Phono-
logical features in infants phonotactic learning: Ev-
idence from artificial grammar learning. Language,
Learning, and Development, 4(3):203?227.
Colin de la Higuera. 1997. Characteristic sets for poly-
nomial grammatical inference. Machine Learning,
27:125?138.
Matt Edlefsen, Dylan Leeman, Nathan Myers,
Nathaniel Smith, Molly Visscher, and David Well-
come. 2008. Deciding strictly local (SL) lan-
guages. In Jon Breitenbucher, editor, Proceedings
of the Midstates Conference for Undergraduate Re-
search in Computer Science and Mathematics, pages
66?73.
Henning Fernau. 2003. Identification of function dis-
tinguishable languages. Theoretical Computer Sci-
ence, 290:1679?1711.
C.R. Gallistel and Adam Philip King. 2009. Memory
and the Computational Brain. Wiley-Blackwell.
Pedro Garc??a and Jose? Ruiz. 1996. Learning k-
piecewise testable languages from positive data. In
Laurent Miclet and Colin de la Higuera, editors,
Grammatical Interference: Learning Syntax from
Sentences, volume 1147 of Lecture Notes in Com-
puter Science, pages 203?210. Springer.
Pedro Garc??a and Jose? Ruiz. 2004. Learning k-testable
and k-piecewise testable languages from positive
data. Grammars, 7:125?140.
Pedro Garcia, Enrique Vidal, and Jose? Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325?338.
E.M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447?474.
J. Grainger and C. Whitney. 2004. Does the huamn
mnid raed wrods as a wlohe? Trends in Cognitive
Science, 8:58?59.
905
Jeffrey Heinz and James Rogers. 2010. Estimating
strictly piecewise distributions. In Proceedings of
the ACL.
Jeffrey Heinz. 2007. The Inductive Learning of
Phonotactic Patterns. Ph.D. thesis, University of
California, Los Angeles.
Jeffrey Heinz. 2009. On the role of locality in learning
stress patterns. Phonology, 26(2):303?351.
Jeffrey Heinz. to appear. Learning long distance
phonotactics. Linguistic Inquiry.
J. J. Horning. 1969. A Study of Grammatical Infer-
ence. Ph.D. thesis, Stanford University.
Sanjay Jain, Daniel Osherson, James S. Royer, and
Arun Sharma. 1999. Systems That Learn: An In-
troduction to Learning Theory (Learning, Develop-
ment and Conceptual Change). The MIT Press, 2nd
edition.
Sanjay Jain, Steffen Lange, and Sandra Zilles. 2007.
Some natural conditions on incremental learning.
Information and Computation, 205(11):1671?1684.
Daniel Jurafsky and James Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. Prentice-Hall, Upper
Saddle River, NJ, 2nd edition.
Anna Kasprzik and Timo Ko?tzing. to appear. String
extension learning using lattices. In Proceedings of
the 4th International Conference on Language and
Automata Theory and Applications (LATA 2010),
Trier, Germany.
S.M. Kim, R. McNaughton, and R. McCloskey. 1991.
A polynomial time algorithm for the local testabil-
ity problem of deterministic finite automata. IEEE
Trans. Comput., 40(10):1087?1093.
Leonid (Aryeh) Kontorovich, Corinna Cortes, and
Mehryar Mohri. 2008. Kernel methods for learn-
ing languages. Theoretical Computer Science,
405(3):223 ? 236. Algorithmic Learning Theory.
Steffen Lange, Thomas Zeugmann, and Sandra Zilles.
2008. Learning indexed families of recursive lan-
guages from positive data: A survey. Theoretical
Computer Science, 397:194?232.
H. Lodhi, N. Cristianini, J. Shawe-Taylor, and
C. Watkins. 2002. Text classification using string
kernels. Journal of Machine Language Research,
2:419?444.
M. Lothaire, editor. 2005. Applied Combinatorics on
Words. Cmbridge University Press, 2nd edition.
Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.
R. McNaughton. 1974. Algebraic decision procedures
for local testability. Math. Systems Theory, 8:60?76.
Kristine H. Onishi, Kyle E. Chambers, and Cynthia
Fisher. 2003. Infants learn phonotactic regularities
from brief auditory experience. Cognition, 87:B69?
B77.
R. J. Parikh. 1966. On context-free languages. Journal
of the ACM, 13, 570581., 13:570?581.
James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation.
James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Visscher, David Wellcome, and Sean
Wibel. 2009. On languages piecewise testable in
the strict sense. In Proceedings of the 11th Meeting
of the Assocation for Mathematics of Language.
John Shawe-Taylor and Nello Christianini. 2005. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Imre Simon. 1975. Piecewise testable events. In Au-
tomata Theory and Formal Languages, pages 214?
222.
Imre Simon. 1993. The product of rational lan-
guages. In ICALP ?93: Proceedings of the 20th
International Colloquium on Automata, Languages
and Programming, pages 430?444, London, UK.
Springer-Verlag.
Carol Whitney and Piers Cornelissen. 2008. SE-
RIOL reading. Language and Cognitive Processes,
23:143?164.
Carol Whitney. 2001. How the brain encodes the or-
der of letters in a printed word: the SERIOL model
and selective literature review. Psychonomic Bul-
letin Review, 8:221?243.
906
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 58?64,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Tier-based Strictly Local Constraints for Phonology
Jeffrey Heinz, Chetan Rawal and Herbert G. Tanner
University of Delaware
heinz,rawal,btanner@udel.edu
Abstract
Beginning with Goldsmith (1976), the phono-
logical tier has a long history in phonological
theory to describe non-local phenomena. This
paper defines a class of formal languages, the
Tier-based Strictly Local languages, which be-
gin to describe such phenomena. Then this
class is located within the Subregular Hier-
archy (McNaughton and Papert, 1971). It is
found that these languages contain the Strictly
Local languages, are star-free, are incompa-
rable with other known sub-star-free classes,
and have other interesting properties.
1 Introduction
The phonological tier is a level of representation
where not all speech sounds are present. For ex-
ample, the vowel tier of the Finnish word pa?iva?a?
?Hello? is simply the vowels in order without the
consonants: a?ia?a?.
Tiers were originally introduced to describe tone
systems in languages (Goldsmith, 1976), and subse-
quently many variants of the theory were proposed
(Clements, 1976; Vergnaud, 1977; McCarthy, 1979;
Poser, 1982; Prince, 1984; Mester, 1988; Odden,
1994; Archangeli and Pulleyblank, 1994; Clements
and Hume, 1995). Although these theories differ in
their details, they each adopt the premise that repre-
sentational levels exist which exclude certain speech
sounds.
Computational work exists which incorporates
and formalizes phonological tiers (Kornai, 1994;
Bird, 1995; Eisner, 1997). There are also learning
algorithms which employ them (Hayes and Wilson,
2008; Goldsmith and Riggle, to appear). However,
there is no work of which the authors are aware that
addresses the expressivity or properties of tier-based
patterns in terms of formal language theory.
This paper begins to fill this gap by defining Tier-
Based Strictly Local (TSL) languages, which gen-
eralize the Strictly Local languages (McNaughton
and Papert, 1971). It is shown that TSL languages
are necessarily star-free, but are incomparable with
other known sub-star-free classes, and that natural
groups of languages within the class are string exten-
sion learnable (Heinz, 2010b; Kasprzik and Ko?tzing,
2010). Implications and open questions for learn-
ability and Optimality Theory are also discussed.
Section 2 reviews notation and key concepts. Sec-
tion 3 reviews major subregular classes and their re-
lationships. Section 4 defines the TSL languages,
relates them to known subregular classes, and sec-
tion 5 discusses the results. Section 6 concludes.
2 Preliminaries
We assume familiarity with set notation. A finite al-
phabet is denoted ?. Let ?n, ??n, ?? denote all
sequences over this alphabet of length n, of length
less than or equal to n, and of any finite length, re-
spectively. The empty string is denoted ? and |w| de-
notes the length of word w. For all strings w and all
nonempty strings u, |w|u denotes the number of oc-
currences of u in w. For instance, |aaaa|aa = 3. A
language L is a subset of ??. The concatenation of
two languages L1L2 = {uv : u ? L1 and v ? L2}.
For L ? ?? and ? ? ?, we often write L? instead
of L{?}.
We define generalized regular expressions
(GREs) recursively. GREs include ?, ? and
each letter of ?. If R and S are GREs then
RS, R + S, R ? S, R, and R? are also GREs.
The language of a GRE is defined as follows.
58
L(?) = ?. For all ? ? ? ? {?}, L(?) = {?}.
If R and S are regular expressions then
L(RS) = L(R)L(S), L(R + S) = L(R) ? L(S),
and L(R ? S) = L(R) ? L(S). Also,
L(R) = ?? ? L(R) and L(R?) = L(R)?.
For example, the GRE ? denotes the language ??.
A language is regular iff there is a GRE defin-
ing it. A language is star-free iff there is a GRE
defining it which contains no instances of the Kleene
star (*). It is well known that the star-free languages
(1) are a proper subset of the regular languages, (2)
are closed under Boolean operations, and (3) have
multiple characterizations, including logical and al-
gebraic ones (McNaughton and Papert, 1971).
String u is a factor of string w iff ?x, y ? ??
such that w = xuy. If also |u| = k then u is a k-
factor of w. For example, ab is a 2-factor of aaabbb.
The function Fk maps words to the set of k-factors
within them.
Fk(w) = {u : u is a k-factor of w}
For example, F2(abc) = {ab, bc}.
The domain Fk is generalized to languages L ?
?? in the usual way: Fk(L) = ?w?LFk(w). We
also consider the function which counts k-factors up
to some threshold t.
Fk,t(w) = {(u, n) : u is a k-factor of w and
n = |w|u iff |w|u < t else n = t}
For example F2,3(aaaaab) = {(aa, 3), (ab, 1)}.
A string u = ?1?2 ? ? ? ?k is a subsequence of a
string w iff w ? ???1???2?? ? ? ????k??. Since
|u| = k we also say u is a k-subsequence of w. For
example, ab is a 2-subsequence of caccccccccbcc.
By definition ? is a subsequence of every string in
??. The function P?k maps words to the set of sub-
sequences up to length k found in those words.
P?k(w) = {u ? ??k : u is a subsequence of w}
For example P?2(abc) = {?, a, b, c, ab, ac, bc}. As
above, the domains of Fk,t and P?k are extended to
languages in the usual way.
3 Subregular Hierarchies
Several important subregular classes of languages
have been identified and their inclusion relation-
ships have been established (McNaughton and Pa-
pert, 1971; Simon, 1975; Rogers and Pullum, to
Regular Star-Free
TSL
LTT LT
PT
SL
SP
Figure 1: Proper inclusion relationships among subreg-
ular language classes (indicated from left to right). This
paper establishes the TSL class and its place in the figure.
appear; Rogers et al, 2010). Figure 1 summarizes
those earlier results as well as the ones made in
this paper. This section defines the Strictly Local
(SL), Locally Threshold Testable (LTT) and Piece-
wise Testable (PT) classes. The Locally Testable
(LT) languages and the Strictly Piecewise (SP) lan-
guages are discussed by Rogers and Pullum (to ap-
pear) and Rogers et al (2010), respectively. Readers
are referred to these papers for additional details on
all of these classes. The Tier-based Strictly Local
(TSL) class is defined in Section 4.
Definition 1 A language L is Strictly k-Local iff
there exists a finite set S ? Fk(o??n) such that
L = {w ? ?? : Fk(own) ? S}
The symbols o and n invoke left and right word
boundaries, respectively. A language is said to be
Strictly Local iff there is some k for which it is
Strictly k-Local. For example, let ? = {a, b, c} and
L = aa?(b+ c). Then L is Strictly 2-Local because
for S = {oa, ab, ac, aa, bn, cn} and every w ? L,
every 2-factor of own belongs to S.
The elements of S can be thought of as the per-
missible k-factors and the elements in Fk(o??n)?
S are the forbidden k-factors. For example, bb and
ob are forbidden 2-factors for L = aa?(b+ c).
More generally, any SL language L excludes ex-
actly those words with any forbidden factors; i.e., L
is the intersection of the complements of sets defined
to be those words which contain a forbidden fac-
tor. Note the set of forbidden factors is finite. This
provides another characterization of SL languages
(given below in Theorem 1).
Formally, let the container of w ? o??n be
C(w) = {u ? ?? : w is a factor of o un}
For example, C(oa) = a??. Then, by the immedi-
ately preceding argument, Theorem 1 is proven.
59
Theorem 1 Consider any Strictly k-Local language
L. Then there exists a finite set of forbidden factors
S? ? Fk(o??n) such that L = ?w?S? C(w).
Definition 2 A language L is Locally t-Threshold
k-Testable iff ?t, k ? N such that ?w, v ? ??, if
Fk,t(w) = Fk,t(v) then w ? L ? v ? L.
A language is Locally Threshold Testable iff there
is some k and t for which it is Locally t-Threshold
k-Testable.
Definition 3 A language L is Piecewise k-Testable
iff ?k ? N such that ?w, v ? ??, if P?k(w) =
P?k(v) then w ? L ? v ? L.
A language is Piecewise Testable iff there is some k
for which it is Piecewise k-Testable.
4 Tier-based Strictly Local Languages
This section provides the main results of this paper.
4.1 Definition
The definition of Tier-based Strictly Local lan-
guages is similar to the one for SL languages with
the exception that forbidden k-factors only apply to
elements on a tier T ? ?, all other symbols are ig-
nored. In order to define the TSL languages, it is
necessary to introduce an ?erasing? function (some-
times called string projection), which erases sym-
bols not on the tier.
ET (?1 ? ? ? ?n) = u1 ? ? ? un
where ui = ?i iff ?i ? T and ui = ? otherwise.
For example, if ? = {a, b, c} and T = {b, c}
then ET (aabaaacaaabaa) = bcb. A string u =
?1 ? ? ? ?n ? oT ?n is a factor on tier T of a string w
iff u is a factor of ET (w).
Then the TSL languages are defined as follows.
Definition 4 A language L is Strictly k-Local on
Tier T iff there exists a tier T ? ? and finite set
S ? Fk(oT ?n) such that
L = {w ? ?? : Fk(oET (w)n) ? S}
Again, S represents the permissible k-factors on the
tier T , and elements in Fk(oT ?n) ? S represent
the forbidden k-factors on tier T . A language L is a
Tier-based Strictly Local iff it is Strictly k-Local on
Tier T for some T ? ? and k ? N.
To illustrate, let ? = {a, b, c}, T = {b, c}, and
S = {ob,oc, bc, cb, bn, cn}. Elements of S are
the permissible k-factors on tier T . Elements of
F2(oT ?n) ? S = {bb, cc} are the forbidden fac-
tors on tier T . The language this describe includes
words like aabaaacaaabaa, but excludes words like
aabaaabaaacaa since bb is a forbidden 2-factor on
tier T . This example captures the nature of long-
distance dissimilation patterns found in phonology
(Suzuki, 1998; Frisch et al, 2004; Heinz, 2010a).
Let LD stand for this particular dissimilatory lan-
guage.
Like SL languages, TSL languages can also be
characterized in terms of the forbidden factors. Let
the tier-based container of w ? oT ?n be CT (w) =
{u ? ?? : w is a factor on tier T of o un}
For example, CT (ob) = (? ? T )?b??. In general
if w = ?1 ? ? ? ?n ? T ? then CT (w) =
???1(? ? T )??2(? ? T )? ? ? ? (? ? T )??n??
In the case where w begins (ends) with a word
boundary symbol then the first (last) ?? in the pre-
vious GRE must be replaced with (? ? T )?.
Theorem 2 For any L ? TSL, let T, k, S be
the tier, length, and permissible factors, respec-
tively, and S? the forbidden factors. Then L =
?
w?S? CT (w).
Proof The structure of the proof is identical to the
one for Theorem 1. 
4.2 Relations to other subregular classes
This section establishes that TSL languages prop-
erly include SL languages and are properly star-free.
Theorem 3 shows SL languages are necessarily TSL.
Theorems 4 and 5 show that TSL languages are not
necessarily LTT nor PT, but Theorem 6 shows that
TSL languages are necessarily star-free.
Theorem 3 SL languages are TSL.
Proof Inclusion follows immediately from the defi-
nitions by setting the tier T = ?. 
The fact that TSL languages properly include SL
ones follows from the next theorem.
Theorem 4 TSL languages are not LTT.
60
Proof It is sufficient to provide an example of a TSL
language which is not LTT. Consider any threshold
t and length k. Consider the TSL language LD dis-
cussed in Section 4.1, and consider the words
w = akbakbakcak and v = akbakcakbak
Clearly w 6? LD and v ? LD. However,
Fk(own) = Fk(ovn); i.e., they have the same
k-factors. In fact for any factor f ? Fk(own),
it is the case that |w|f = |v|f . Therefore
Fk,t(own) = Fk,t(ovn). If LD were LTT,
it would follow by definition that either both
w, v ? LD or neither w, v belong to LD, which is
clearly false. Hence LD 6? LTT. 
Theorem 5 TSL languages are not PT.
Proof As above, it is sufficient to provide an exam-
ple of a TSL language which is not PT. Consider any
length k and the language LD. Let
w = ak(bakbakcakcak)k and
v = ak(bakcakbakcak)k
Clearly w 6? LD and v ? LD. But observe that
P?k(w) = P?k(v). Hence, even though the two
words have exactly the same k-subsequences (for
any k), both words are not in LD. It follows that LD
does not belong to PT. 
Although TSL languages are neither LTT nor PT,
Theorem 6 establishes that they are star-free.
Theorem 6 TSL languages are star-free.
Proof Consider any language L which is Strictly k-
Local on Tier T for some T ? ? and k ? N. By
Theorem 2, there exists a finite set S? ? Fk(oT ?n)
such that L = ?w?S? CT (w). Since the star-free lan-
guages are closed under finite intersection and com-
plement, it is sufficient to show that CT (w) is star-
free for all w ? oT ?n.
First consider any w = ?1 ? ? ? ?n ? T ?. Since
(??T )? = ??T?? and ?? = ?, the set CT (w) can
be written as
? ?T? ?1 ?T? ?2 ?T? ? ? ? ?n ?
This is a regular expression without the Kleene-star.
In the cases where w begins (ends) with a word
boundary symbol, the first (last) ? in the GRE above
should be replaced with ?T?. Since every CT (w)
can be expressed as a GRE without the Kleene-star,
every TSL language is star-free. 
Together Theorems 1-4 establish that TSL lan-
guages generalize the SL languages in a different
way than the LT and LTT languages do (Figure 1).
4.3 Other Properties
There are two other properties of TSL languages
worth mentioning. First, TSL languages are closed
under suffix and prefix. This follows immediately
because no word w of any TSL language contains
any forbidden factors on the tier and so neither does
any prefix or suffix of w. SL and SP languages?but
not LT or PT ones?also have this property, which has
interesting algebraic consequences (Fu et al, 2011).
Next, consider that the choice of T ? ? and
k ? N define systematic classes of languages which
are TSL. Let LT,k denote such a class. It follows
immediately that LT,k is a string extension class
(Heinz, 2010b). A string extension class is one
which can be defined by a function f whose do-
main is ?? and whose codomain is the set of all
finite subsets of some set A. A grammar G is a
particular finite subset of A and the language of the
grammar is all words which f maps to a subset of
G. For LT,k, the grammar can be thought of as the
set of permissible factors on tier T and the func-
tion is w 7? Fk(oET (w)n). In other words, every
word is mapped to the set of k-factors present on tier
T . (So here the codomain?the possible grammars?is
the powerset of Fk(oT ?n).)
String extension classes have quite a bit of
structure, which faciliates learning (Heinz, 2010b;
Kasprzik and Ko?tzing, 2010). They are closed un-
der intersection, and have a lattice structure under
the partial ordering given by the inclusion relation
(?). Additionally, these classes are identifiable in
the limit from positive data (Gold, 1967) by an in-
cremental learner with many desirable properties.
In the case just mentioned, the tier is known in
advance. Learners which identify in the limit a class
of TSL languages with an unknown tier but known
k exist in principle (since such a class is of finite
size), but it is unknown whether any such learner is
61
efficient in the size of the input sample.
5 Discussion
Having established the main results, this section dis-
cusses some implications for phonology in general,
Optimality Theory in particular, and future research.
There are three classes of phonotactic constraints
in phonology: local segmental patterns, long-
distance segmental patterns, and stress patterns
(Heinz, 2007). Local segmental patterns are SL
(Heinz, 2010a). Long-distance segmental phono-
tactic patterns are those derived from processes of
consonant harmony and disharmony and vowel har-
mony. Below we show each of these patterns belong
to TSL. For exposition, assume ?={l,r,i,o?,u,o}.
Phonotactic patterns derived from attested long-
distance consonantal assimilation patterns (Rose
and Walker, 2004; Hansson, 2001) are SP; on the
other hand, phonotactic patterns derived from at-
tested long-distance consonantal dissimilation pat-
terns (Suzuki, 1998) are not (Heinz, 2010a). How-
ever, both belong to TSL. Assimilation is obtained
by forbidding disagreeing factors on the tier. For
example, forbidding lr and rl on the liquid tier
T = {l, r} yields only words which do not contain
both [l] and [r]. Dissimilation is obtained by for-
bidding agreeing factors on the tier; e.g. forbidding
ll and rr on the liquid tier yields a language of the
same character as LD.
The phonological literature distinguishes three
kinds of vowel harmony patterns: those without neu-
tral vowels, those with opaque vowels and those
with transparent vowels (Bakovic?, 2000; Nevins,
2010). Formally, vowel harmony patterns without
neutral vowels are the same as assimilatory conso-
nant harmony. For example, a case of back harmony
can be described by forbidding disagreeing factors
{iu, io, o?u, o?o, ui, uo?, oi, oo?} on the vowel tier
T ={i,o?,u,o}. If a vowel is opaque, it does not har-
monize but begins its own harmony domain. For ex-
ample if [i] is opaque, this can be described by for-
bidding factors {iu, io o?u, o?o, uo?, oo?} on the vowel
tier. Thus words like lulolilo? are acceptable because
oi is a permissible factor. If a vowel is transpar-
ent, it neither harmonizes nor begins its own har-
mony domain. For example if [i] is transparent (as in
Finnish), this can be described by removing it from
the tier; i.e. by forbidding factors {o?u, o?o, uo?, oo?}
on tier T ={o?,u,o}. Thus words like lulolilu are ac-
ceptable since [i] is not on the relevant tier. The rea-
sonable hypothesis which follows from this discus-
sion is that all humanly possible segmental phono-
tactic patterns are TSL (since TSL contains SL).
Additionally, the fact that LT,k is closed under in-
tersection has interesting consequences for Optimal-
ity Theory (OT) (Prince and Smolensky, 2004). The
intersection of two languages drawn from the same
string extension class is only as expensive as the in-
tersection of finite sets (Heinz, 2010b). It is known
that the generation problem in OT is NP-hard (Eis-
ner, 1997; Idsardi, 2006) and that the NP-hardness is
due to the problem of intersecting arbitrarily many
arbitrary regular sets (Heinz et al, 2009). It is un-
known whether intersecting arbitrarily many TSL
sets is expensive, but the results here suggest that
it may only be the intersections across distinct LT,k
classes that are problematic. In this way, this work
suggests a way to factor OT constraints characteri-
zable as TSL languages in a manner originally sug-
gested by Eisner (1997).
Future work includes determining automata-
theoretic characterizations of TSL languages and
procedures for deciding whether a regular set be-
longs to TSL, and if so, for what T and k. Also,
the erasing function may be used to generalize other
subregular classes.
6 Conclusion
The TSL languages generalize the SL languages
and have wide application within phonology. Even
though virtually all segmental phonotactic con-
straints present in the phonologies of the world?s lan-
guages, both local and non-local, fall into this class,
it is striking how highly restricted (sub-star-free) and
well-structured the TSL languages are.
Acknowledgements
We thank the anonymous reviewers for carefully
checking the proofs and for their constructive crit-
icism. We also thank the participants in the Fall
2010 Formal Models in Phonology seminar at the
University of Delaware for valuable discussion, es-
pecially Jie Fu. This research is supported by grant
#1035577 from the National Science Foundation.
62
References
Diana Archangeli and Douglas Pulleyblank. 1994.
Grounded Phonology. Cambridge, MA: MIT Press.
Eric Bakovic?. 2000. Harmony, Dominance and Control.
Ph.D. thesis, Rutgers University.
Steven Bird. 1995. Computational Phonology: A
Constraint-Based Approach. Cambridge University
Press, Cambridge.
G. N. Clements and Elizabeth Hume. 1995. The internal
organization of speech sounds. In John A. Goldsmith,
editor, The Handbook of Phonological Theory, pages
245?306. Blackwell, Cambridge, Mass., and Oxford,
UK.
G. N. Clements. 1976. Neutral vowels in hungarian
vowel harmony: An autosegmental interpretation. In
David Nash Judy Kegl and Annie Zaenen, editors,
North Eastern Linguistic Society (NELS) 7, pages 49?
64, Amherst, MA. University of Massachusetts, Grad-
uate Linguistic Student Association.
Jason Eisner. 1997. Efficient generation in primitive Op-
timality Theory. In Proceedings of the 35th Annual
ACL and 8th EACL, pages 313?320, Madrid, July.
S. Frisch, J. Pierrehumbert, and M. Broe. 2004. Simi-
larity avoidance and the OCP. Natural Language and
Linguistic Theory, 22:179?228.
Jie Fu, Jeffrey Heinz, and Herbert Tanner. 2011. An
algebraic characterization of strictly piecewise lan-
guages. In The 8th Annual Conference on Theory
and Applications of Models of Computation, volume
6648 of Lecture Notes in Computer Science. Springer-
Verlag.
E.M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447?474.
John Goldsmith and Jason Riggle. to appear. Infor-
mation theoretic approaches to phonological structure:
the case of Finnish vowel harmony. Natural Language
and Linguistic Theory.
John Goldsmith. 1976. Autosegmental Phonology.
Ph.D. thesis, MIT, Cambridge, Mass. Published by
Garland Press, New York , 1979.
Gunnar Hansson. 2001. Theoretical and typological is-
sues in consonant harmony. Ph.D. thesis, University
of California, Berkeley.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learning.
Linguistic Inquiry, 39:379?440.
Jeffrey Heinz, Gregory Kobele, and Jason Riggle. 2009.
Evaluating the complexity of Optimality Theory. Lin-
guistic Inquiry, 40(2):277?288.
Jeffrey Heinz. 2007. The Inductive Learning of Phono-
tactic Patterns. Ph.D. thesis, University of California,
Los Angeles.
Jeffrey Heinz. 2010a. Learning long-distance phonotac-
tics. Linguistic Inquiry, 41(4):623?661.
Jeffrey Heinz. 2010b. String extension learning. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 897?906,
Uppsala, Sweden, July. Association for Computational
Linguistics.
William Idsardi. 2006. A simple proof that Optimality
Theory is computationally intractable. Linguistic In-
quiry, 37(2):271?275.
Anna Kasprzik and Timo Ko?tzing. 2010. String ex-
tension learning using lattices. In Henning Fer-
nau Adrian-Horia Dediu and Carlos Mart??n-Vide, ed-
itors, Proceedings of the 4th International Confer-
ence on Language and Automata Theory and Appli-
cations (LATA 2010), volume 6031 of Lecture Notes
in Computer Science, pages 380?391, Trier, Germany.
Springer.
Andras Kornai. 1994. Formal Phonology. Garland, New
York.
John J. McCarthy. 1979. Formal problems in Semitic
phonology and morphology. Ph.D. thesis, MIT. Pub-
lished by Garland Press, New York, 1985.
Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.
Armin Mester. 1988. Studies in Tier Structure. New
York: Garland Publishing, Inc.
Andrew Nevins. 2010. Locality in Vowel Harmony. The
MIT Press, Cambridge, MA.
David Odden. 1994. Adjacency parameters in phonol-
ogy. Language, 70(2):289?330.
William Poser. 1982. Phonological representation and
action-at-a-distance. In H. van der Hulst and N.R.
Smith, editors, The Structure of Phonological Repre-
sentations, pages 121?158. Dordrecht: Foris.
Alan Prince and Paul Smolensky. 2004. Optimality The-
ory: Constraint Interaction in Generative Grammar.
Blackwell Publishing.
Alan Prince. 1984. Phonology with tiers. In Mark
Aronoff and Richard T. Oehrle, editors, Language
Sound Structure, pages 234?244. MIT Press, Cam-
bridge, Mass.
James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular hi-
erarchy. Journal of Logic, Language and Information.
James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlefsen,
Molly Visscher, David Wellcome, and Sean Wibel.
2010. On languages piecewise testable in the strict
sense. In Christian Ebert, Gerhard Ja?ger, and Jens
Michaelis, editors, The Mathematics of Language, vol-
ume 6149 of Lecture Notes in Artifical Intelligence,
pages 255?265. Springer.
63
Sharon Rose and Rachel Walker. 2004. A typology of
consonant agreement as correspondence. Language,
80(3):475?531.
Imre Simon. 1975. Piecewise testable events. In Au-
tomata Theory and Formal Languages, pages 214?
222.
Keiichiro Suzuki. 1998. A Typological Investigation of
Dissimilation. Ph.D. thesis, University of Arizona,
Tucson, AZ.
Jean-Roger Vergnaud. 1977. Formal properties of
phonological rules. In R. Butts and J. Hintikka, ed-
itors, Basic Problems and Methodology and Linguis-
tics. Amsterdam: Reidel.
64
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal and Empirical Grammatical Inference
Jeffrey Heinz, Colin de la Higuera and Menno van Zaanen
heinz@udel.edu, cdlh@univ-nantes.fr, mvzaanen@uvt.nl
1
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Outline of the tutorial
I. Formal GI and learning theory (de la Higuera)
II. Empirical approaches to regular and subregular natural
language classes (Heinz)
III. Empirical approaches to nonregular natural language
classes (van Zaanen)
2
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
I Formal GI and learning theory
What is grammatical inference?
What does learning or having learnt imply?
Reasons for considering formal learning
Some criteria to study learning in a probabilistic and a non
probabilistic setting
3
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A simple definition
Grammatical inference is about learning a grammar given
information about a language
Vocabulary
Learning = building, inferring
Grammar= finite representation of a possibly infinite set of
strings, or trees, or graphs
Information=you can learn from text, from an informant, by
actively querying
Language= possibly infinite set of strings, or trees, or graphs
4
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A Dfa (Ack: Jeffrey Heinz)
The (CV)* language representing licit sequences of sounds in many
languages in the world. Consonants and vowels must alternate;
words must begin with C and must end with V. States show the
regular expression indicating its ?good tails?.
(CV )? V (CV )?
C
V
5
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A context free grammar and a parse tree
(de la Higuera 2010)
S
NP VP
John V NP
hit Det N
the ball
S ? NP VP
VP? V NP
NP? Det N
6
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A categorial dependency grammar (Be?chet et al 2011)
elle 7? [pred ],
la 7? [#(? clit ? a? obj)]?clit?a?obj ,
lui 7? [#(? clit ? 3d ? obj)]?clit?3d?obj ,
a 7? [#(? clit ? 3d ? obj)\#(?
clit ? a ? obj)\pred\S/aux ? a ? d ],
donne?e 7? [aux ? a ? d ]?clit?3d?obj?clit?a?obj
7
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A finite state transducer (Ack: Jeffrey Heinz)
A subsequential transducer illustrating a common phonological rule
of palatalization ( k ?? >tS / i). States are labelled with a
number and then the output string given by the ? function for that
state.
0,? 1,k
k:?
k:kk, C:kC, V:kV
i:>tSi
C,V,i k
? = {C ,V , k , i}
8
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
So for example:
w t(w)
kata kata
kita >tSita
tak tak
taki ta>tSi
. . .
9
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
Questions
Why grammar and not language?
Why a and not the?
10
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Why not write ?learn a language??
Because you always learn a representation of a language
Paradox
Take two learners learning a context-free language, one is learning
a quadratic normal form and the other a Greibach normal form,
they cannot agree that they have learnt the same thing
(undecidable question).
Worth thinking about. . . is it a paradox? Do two English speakers
agree they speak the same language?
11
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
How can a become the?
Ask for the grammar to be the smallest, best (re a score). ?
Combinatorial characterisation
The learning problem becomes an optimisation problem!
Then we often have theorems saying that
If our algorithm does solve the optimisation problem, what we
have learnt is correct
If we can prove that we can?t solve the optimisation problem,
then the class is not learnable
12
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Optimal with respect of some score
Score should take into account:
Simplicity
Coverage
Usefulness
What scores?
Occam argument
Compression argument
Kolmogorov complexity
MDL argument
13
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Moreover
GI is not only about building a grammar from some data. It is
concerned with saying something about:
the quality of the result,
the quality of the learning process,
the properties of the process.
14
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Naive example
Suppose you are building a random number generator.
How are you convinced that it works?
Because it follows sound principles as defined by number
theory specialists?
Because you have tested and the number 772356191 has been
produced?
Because you have proved that the series of numbers that will
be produced is incompressible?
Empirical approach
Experimental approach
Formal approach
15
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical approach: using good (safe?) ideas
For example, genetic algorithms or neural networks
Or some mathematical principle (Occam, Kolmogorov,
MDL,. . . )
Can become a principled approach
Alternative point of view
Empirical approach is about imitating what nature (or humans) do
16
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Experimental approach
Benchmarks
Competitions
Necessary but not sufficient
How do we know that all the cases are covered?
How do we know that we dont have a hidden bias?
17
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal approach: showing that the algorithm has converged
Is impossible:
Just one run
Can?t prove that 23 is random
But we can say something about the algorithm:
That in the near future, given some string, we can predict if
this string belongs to the language or not;
Choose between defining clearly ?near future? and accepting
probable truths (or error bounds) or leaving it undefined and
using identification.
18
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What else would we like to say?
That if the solution we have returned is not good, then that is
because the initial data was bad (insufficient, biased)
Idea:
Blame the data, not the algorithm
19
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Suppose we cannot say anything of the sort?
Then that means that we may be terribly wrong even in a
favourable setting
Thus there is a hidden bias
Hidden bias: the learning algorithm is supposed to be able to
learn anything inside class L1, but can really only learn things
inside class L2, with L2 ? L1
20
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Saying something about the process itself
Key idea: if there is something to learn and the data is not
corrupt, then, given enough time, we will learn it
Replace the notion of learning by that of identifying
21
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
In practise, does it make sense?
No, because we never know if we are in the ideal conditions
(something to learn + good data + enough of it)
Yes, because at least we get to blame the data, not the
algorithm
22
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Complexity issues
Complexity theory should be used: the total or update
runtime, the size of the data needed, the number of mind
changes, the number and weight of errors. . .
. . . should be measured and limited.
23
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A linguistic criterion
One argument appealing to linguists (we hope) is that if the
criteria are not met for some class of languages that a human
is supposed to know how to learn, something is wrong
somewhere
(preposterously, the maths can?t be wrong. . . )
24
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Non probabilistic settings
Identification in the limit
Resource bounded identification in the limit
Active learning (query learning)
25
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Identification in the limit
Information is presented to the learner who updates its
hypothesis after inspecting each piece of data
At some point, always, the learner will have found the correct
concept and not change from it
(Gold 1967 & 1978)
26
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example
Number Presentation Analysis of hy-
pothesis
New hypothesis
(regexp)
1 a + a
2 aaa + inconsistent a?
3 aaaa - inconsistent a(aa)?
4 aaaaaa - consistent a(aa)?
9234 aaaaaaaa - consistent a(aa)?
45623416 aaaaaaaaa + consistent a(aa)?
27
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A presentation is
a function ? : N ? X
where X is some set,
and such that ? is associated to a language L through a
function Yields : Yields(?) = L
If ?(N) = ?(N) then Yields(?) = Yields(?)
28
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
text presentation
A text presentation of a language L ? ?? is a function
? : N ? ?? such that ?(N) = L
? is an infinite succession of all the elements of L
(note : small technical difficulty with ?)
informed presentation
An informed presentation (or an informant) of L ? ?? is a
function ? : N ? ?? ?{?,+} such that
?(N) = (L,+) ? (L,?)
? is an infinite succession of all the elements of ?? labelled to
indicate if they belong or not to L
29
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Active presentation
The learner interacts with the environment (modelled as an
oracle) through queries
A membership query
Learner presents string x
Oracle answer yes or no
A correction query (Becerra-Bonache et al 2005 & 2008)
Learner presents string x
Oracle answer yes or returns a close correction
An equivalence query
Learner presents hypothesis H
Oracle answer yes or returns a counter-example
30
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example: presentations for {anbn : n ? N}
Legal presentation from text: ?, a2b2, a7b7,. . .
Illegal presentation from text: ab, ab, ab,. . .
Legal presentation from informant : (?,+), (abab,?),
(a2b2,+), (a7b7,+), (aab,?), (abab,?),. . .
31
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example: presentation for Spanish
Legal presentation from text: En un lugar de la Mancha. . .
Illegal presentation from text: Goooool
Legal presentation from informant : (en,+), (whatever,-),
(un,+), (lugar,+), (lugor,-), (xwszrrzt,-),
32
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What happens before convergence?
On two occasions I have been asked [by members of Parliament],
?Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?? I am not able rightly to apprehend
the kind of confusion of ideas that could provoke such a question.
Charles Babbage
33
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Further definitions
Given a presentation ?, ?n is the set of the first n elements in
?.
A learning algorithm (learner) A is a function that takes as
input a set ?n and returns a grammar of a language.
Given a grammar G , L(G ) is the language
generated/recognised/ represented by G .
34
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Convergence to a hypothesis
A converges to G with ? if
?n ? N : A(?n) halts and gives an answer
?n0 ? N : n ? n0 =? A(?n) = G
If furthermore L(G ) = Yields(?) then we have identified.
35
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Identification in the limit
L
G
Pres(L)
L
Yields
A
Figure: The learning setting.
from (de la Higuera 2010)
36
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Consistency and conservatism
We say that the learner A is consistent if ?n is consistent with
A(?n) ?n
A consistent learner is always consistent with the past
Consistency and conservatism
We say that the learner A is conservative if whenever ?(n + 1)
is consistent with A(?n), we have A(?n) = A(?n+1)
A conservative learner doesn?t change his mind needlessly
37
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning from data
A learner is order dependent if it learns something different
depending on the order in which it receives the data.
Usually an order independent learner is better.
38
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What about efficiency?
We can try to bound
global time
update time
errors before converging (IPE)
mind changes (MC)
queries
good examples needed (characteristic samples)
(Pitt 1989, de la Higuera et al 2008)
39
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Definition: polynomial number of implicit prediction errors
Denote by G 6|= x if G is incorrect with respect to an element
x of the presentation (i.e. the learner producing G has made
an implicit prediction error.
G is polynomially identifiable in the limit from Pres if there exists
an identification learner A and a polynomial p() such that given
any G in G, and given any presentation ? of L(G ),
]i : A(?i ) 6|= ?(i + 1) ? p(|G |).
40
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Definition: polynomial characteristic sample
G has polynomial characteristic samples for identification learner A
if there exists a polynomial p() such that: given any G in G, ?Y
correct sample for G , such that whenever Y ? ?n, A(?n) ? G and
?Y ? ? p(?G?)
As soon as the CS is in the data, the result is correct;
The CS is small.
41
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Polynomial queries
(Angluin 1987)
Algorithm A learns with a polynomial number of queries if the
number of queries made before halting with a correct
grammar is polynomial in
the size of the target,
the size of the information received.
42
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Main negative results
Cannot learn Nfa, Cfgs from an informant in most
polynomial settings (Pitt 1989, de la Higuera 1997)
Cannot learn Dfa from text (Gold 1967)
Cannot learn Dfa from membership nor equivalence queries
(Angluin 1981 & 1987).
Main positive results
Can learn Dfa from an informant with polynomial resources
(Oncina and Garc??a 1992);
Can learn Dfa from membership and equivalence queries
(Angluin 1987).
43
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Probabilistic settings
Pac learning (about learning yes-no machines with fixed but
unknown distributions)
Identification with probability 1 (about identifying
distributions)
Pac learning distributions (about approximately learning
distributions)
44
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning a language from sampling
We have a distribution over ??
We sample twice:
once to learn,
once to see how well we have learned
The Pac setting: Les Valiant, Turing award 2010
45
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Pac-learning
(Valiant 1984, Pitt 1989)
L a class of languages
G a class of grammars
 > 0 and ? > 0
m a maximal length over the strings
n a maximal size of machines
H is -AC (approximately correct)*
if
PrD [H(x) 6= G (x)] < 
46
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Polynomial Pac learning
There is a polynomial p(?, ?, ?, ?) such that
in order to learn -AC machines of size at most n with error at
most ? we require at most p(m, n, 1? , 1? ) data and time;
we want the errors to be less than  and bad luck to be less
than ?.
(French radio)
Unless there is a surprise there should be no surprise
French radio, (after the last primary elections, on 3rd of June
2008)
First surprise is ?, second surprise is 
47
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results (Kearns and Valiant 1989, Kearns and Vazirani 1994)
Using cryptographic assumptions, we cannot Pac-learn Dfa
Cannot Pac-learn Nfa, Cfgs with membership queries either
Learning can be seen as finding the encryption function from
examples (Kearns & Vazirani)
48
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alternatively
Instead of learning classifiers in a probabilistic world, learn
directly the distributions!
Learn probabilistic finite automata (deterministic or not)
49
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
No error (Angluin 1988)
This calls for identification in the limit with probability 1
Means that the probability of not converging is 0
Goal is to identify the structure and the probabilities
Mainly a (nice) theoretic setting
Results
If probabilities are computable, we can learn with probability 1
finite state automata (Carrasco and Oncina, 1994)
But not with bounded (polynomial) resources (de la Higuera
and Oncina, 2004)
50
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
With error
Pac definition applies
But error should be measured by a distance between the
target distribution and the hypothesis
How do we measure the distance: L1, L2, L?,
Kullback-Leibler?
51
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results
Too easy to learn with L?
Too hard to learn with L1
Both results hold for the same algorithm! (de la Higuera and
Oncina, 2004)
Nice algorithms for biased classes of distributions
52
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Open problems
We conclude this section on ?what is language learning about?
with some open questions:
What is a good definition of polynomial identification?
How do we deal with shifting targets? (robustness issues)
Alternative views on learnability?
Is being learnable a good indicator of being linguistically
reasonable?
Can we learn transducers? Probabilistic transducers?
53
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
II. GI of Regular Patterns
Why regular?
What are the general GI strategies?
What are the main results?
The main techniques?
The main lessons?
54
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Logically Possible Computable Patterns
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
55
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
GI Strategies
#1. Define ?learning? so that large regions can be learned
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
56
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
GI Strategies
#2. Target non-superfinite cross-cutting classes
(instructor?s bias)
Recursively Enumerable
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
57
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Common Theme
1 Different learning frameworks may better characterize the
data presentations learners actually get (strategy #1).
2 Classes of formal languages may exist which better
characterize the patterns we are interested in (strategy #2).
3 Hard problems are easier to solve with better characterizations
because the instance space of the problem is smaller.
58
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Why Begin with Regular?
Insights obtained here can be (and have been) applied fruitfully to
nonregular classes.
Angluin 1982 showed a subclass of regular languages (the
reversible languages) was identifiable in the limit from positive
data by an incremental learner.
Yokomori?s (2004) Very Simple Languages are a subclass of
the context-free languages, but draws on ideas from the
reversible languages.
Similarly, Clark and Eryaud?s (2007) substitutable languages
(also subclass of context-free) are also based on insights from
this paper.
59
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Section Outline
1 Targets of Learning
2 Learning Frameworks
3 State-merging
4 Results for learning regular languages, relations, and
distributions
60
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular Languages
Multiple grammars (i.e. representations) for regular languages:
1 Regular expressions
2 Generalized regular expressions
3 Finite state acceptors
4 Words which satisfy formulae in monadic second order logic
5 Right or left branching rewrite rules
6 . . .
61
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular Relations
Multiple grammars (i.e. representations) for regular relations:
Regular expressions (for relations)
Generalized regular expressions (for relations)
Finite state transducers
. . .
62
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Targets of Learning: Regular distributions
Multiple grammars (i.e. representations) for distributions over
regular sets and relations:
Weighted finite state automata
Hidden Markov Models
Weighted right or left branching rewrite rules
. . .
63
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
This tutorial: Finite State Automata
Acceptors and subsequential transducers admit canonical forms
1 The smallest deterministic acceptor, syntactic monoids, . . .
2 Canonical forms relate to algebraic properties (Nerode
equivalence relation, i.e. states represent sets of ?good tails?)
3 In contrast, canonical regular expressions have yet to be
determined. For example, there are no canonical (e.g.
shortest) regular expressions for regular languages.
64
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Choices
Success required on which input data streams?
All possible vs. some restricted set
i.e. ?distribution-free? vs. ?non distribution-free?
What kind of samples?
Positive data vs. postive and negative data
Other choices (e.g. query learning) are not discussed here.
65
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? w/ positive and negative data
1 The class of r.e. languages is identifiable in the limit (Gold
1967)
2 Non-enumerative algorithms for regular languages:
1 Gold (1978)
2 RPNI (Oncina and Garc??a 1992)
66
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? with positive data only
1 No superfinite class (including regular, cf, etc.) is identifiable
in the limit (Gold 1967)
2 Not even the finite class is PAC-learnable (Blumer et al 1989)
3 No superfinite class is identifiable in the limit with probability
p (p > 2/3) (Pitt 1985, Wiehagen et al 1986, Angluin 1988)
4 But many subregular classes are learnable in this difficult
setting.
67
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Distribution-free? with positive data only: learnable subregular
classes
1 reversible languages (Angluin 1982)
2 strictly local languages (Garcia et al 1990)
3 locally testable and piecewise testable (Garcia and Ruiz 2004)
4 left-to-right and right-to-left iterative languages (Heinz 2008)
5 strictly piecewise languages (Heinz 2010)
6 . . .
7 subsequential functions (Oncina et al 1993)
8 . . .
68
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning Frameworks: Main Results
?Non distribution-free? w/ positive data only
1 The class of r.e. languages are identifiable in the limit from
computable classes of r.e. texts (Gold 1967)
2 The class of r.e. distributions are identifiable from
?approximately computable? sequences (Angluin 1988, Chater
and Vitany?? 2007)
3 The class of distributions describable with Probabilistic
Deterministic FSAs (PDFAs) is learnable with probability one
(de la Higuera and Thollard 2000)
4 The class of distributions describable with PDFAs is learnable
in a modified PAC setting (Clark and Thollard, 2004)
69
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning regular languages: Key technique
State-merging
Angluin 1982 (reversible languages)
Muggleton 1990 (contextual languages)
Garcia et al 1990 (strictly local languages)
Oncina et al 1993 (subsequential functions)
Clark and Thollard 2004 (PDFA distributions)
. . .
70
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other techniques
Lattice-climbing
Heinz 2010 (strictly local languages, strictly piecewise
languages, many others)
Kasprizk and Ko?tzing 2010 (function-distinguishable
lanaguages, pattern languages, many others)
State-splitting
Tellier (2008)
71
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Only so much can be covered. . .
It?s impossible to be fair to all
those who have contributed
and to cover all the variants,
even all the algorithms in a
short tutorial. That?s why
there are books!
72
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview of State-merging
1 Builds a FSA representation of the input
2 Generalize by merging states
73
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example: Stress pattern of Pintupi
a. pa??a ?earth? ?? ?
b. tju??aya ?many? ?? ? ?
c. ma??awa`na ?through from behind? ?? ? ?` ?
d. pu??iNka`latju ?we (sat) on the hill? ?? ? ?` ? ?
e. tja?mul`?mpatju`Nku ?our relation? ?? ? ?` ? ?` ?
f. ????ir`iNula`mpatju ?the fire for our benefit
flared up?
?? ? ?` ? ?` ? ?
g. ku?ranju`lul`?mpatju`?a ?the first one who is our
relation?
?? ? ?` ? ?` ? ?` ?
h. yu?ma?`?Nkama`ratju`?aka ?because of mother-in-
law?
?? ? ?` ? ?` ? ?` ? ?
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
74
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example: Stress pattern of Pintupi
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
Minimal deterministic FSA for Pintupi Stress
0 1 2
3
4
?? ?
?
?`
?
75
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Structured representations of Input
1 Each word its own FSA (Nondeterministic)
2 Prefix Trees (deterministic)
3 Suffix Trees (reverse determinstic)
76
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Examples of Prefix and Suffix Trees
S =
?
?
?
?? ?? ?
?? ? ? ?? ? ?` ?
?? ? ?` ? ? ?? ? ?` ? ?` ?
?
?
?
PT(S)
0 1 2
3
4
5
6
7
8
?? ?
?`
?
?
?`
?
?
ST(S)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
16
??
?
??
?
?`
?`
??
?
??
?`
?
??
??
?
77
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging Informally
Eliminate redundant environments by state-merging.
States are identified as equivalent and then merged.
All transitions are preserved.
This is one way in which generalizations may occur?because
the post-merged machine accepts everything the pre-merged
machine accepts, possibly more.
Machine A Machine B
0 1 2 3a a a 0 1-2 3a a
a
The merged machine may not be deterministic.
78
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging Formally
Definition
Given an acceptor A = (Q, I ,F , ?) and a partition pi of its states
state-merging returns the acceptor A/pi = (Q ?, I ?,F ?, ??):
1 Q ? = pi (the states are the blocks of pi)
2 I ? = {B ? pi : I ? B 6= ?}
3 F ? = {B ? pi : F ? B 6= ?}
4 For all B ? pi and a ? ?,
??(B , a) = {B ? ? pi : ?q ? B , q? ? B ? such that q? ? ?(q, a)}
79
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Theorem
Theorem
Given any regular language L, let A(L) denote the minimal
deterministic acceptor recognizing L. There exists a finite sample
S ? L and a partition pi over PT (S) such that PT (S)/pi = A(L).
Notes
The finite sample need only exercise every transition in A(L).
What is pi?
80
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Illustrative Example
Let?s merge states with the same incoming paths of length 2!
PT(S)
0 1 2
3
4
5
6
7
8
?? ?
?`
?
?
?`
?
?
81
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Result of State Merging
0 1 2
3-6
4-7
5-8
??
?`
?
?`
?
?
?
This acceptor is not the canonical acceptor we saw earlier but it
recognizes the same language.
Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)):
Primary stress falls on the initial syllable
Secondary stress falls on alternating nonfinal syllables
82
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary of Algorithm
1 States in the prefix tree are merged if they have the same
k-length suffix.
u ? v def?? ?x , y ,w such that |w | = k , u = xw , v = yw
2 The algorithm then is simply:
G = PT (S)/pi?
3 This algorithm provably identifies in the limit from positive
data the Strictly (k + 1)-Local class of languages (Garcia et
al. 1990).
83
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Back to the Illustrative Example
Results for stress patterns more generally
Out of 109 distinct stress patterns in the world?s languages
(encoded as FSAs), this state-merging strategy works for only
44 of them
If we merge states with the same paths up to length 5(!), only
81 are learned.
This is the case even permitting very generous input samples.
In other words, 44 attested stress patterns are Strictly 3-Local and
81 are Strictly 6-Local. 28 are not Strictly 6-Local In fact those 28
are not Strictly k-Local for any k (Edlefsen et al 2008).
84
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other ways to merge states
If the current structure is ?ill-formed? then merge states to
eliminate source of ill-formedness
State equivalence relations
1 merge state with same incoming paths of length k (Garcia et. al 1990)
2 recursively eliminate reverse non-determinism (Angluin 1982)
3 merge states with same ?contexts? (Muggleton 1990, Clark and Eryaud
2007)
4 merge final states (Heinz 2008)
5 merge states with same ?neighborhood? (Heinz 2009)
6 . . .
7 merge states to maximize posterior probability (for HMMs, Stolcke 1994)
8 . . .
85
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Other ways to merge states
Merge states indiscriminately unless ?ill-formedness? arises
Merge unless something tells us not to
1 unless ?onward subsequentiality? is lost (for transducers,
Oncina et al 1993)
2 unless they are ??-distinguishable? (Clark and Thollard 2004)
3 . . .
86
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging as inference rules
Strictly k-Local languages (Garcia et al 1990)
merge states with same incoming paths of length k
?u, v ,w ? ?? : uv ,wv ,? Prefix(L) and |v | = k
?
TailsL(uv) = TailsL(wv) ? L
87
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging as inference rules
0-Reversible languages (Angluin 1982)
recursively eliminate reverse non-determinism
?u, v ,w , y ? ?? : uv ,wv , uy ? L ? wy ? L
88
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
State-merging summary
1 Distinctions maintained in the prefix tree are lost by state
merging, which results in generalizations.
2 The choice of partition corresponds to the generalization
strategy (i.e. which distinctions will be maintained and which
will be lost)
Gleitman (1990:12):
The trouble is that an observer who notices everything
can learn nothing for there is no end of categories known
and constructible to describe a situation [emphasis in
original].
89
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results for regular languages
Distribution-free with positive data
Identification in the limit from positive data
1 strictly k-local languages (each state corresponds to suffixes of
up to length k) (Garcia et al 1990)
2 reversible languages (acceptors are both forward and reverse
k-deterministic for some k) (Angluin 1982)
3 k-contextual languages (Muggleton 1990)
4 . . .
90
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Regular relations
Regular relations in CL
1 transliteration
2 translation
3 . . .
4 anything with finite state transducers
91
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA (Oncina et al 1993)
distribution-free with positive data
OSTIA
1 identifies subsequential functions in the limit from positive
data.
2 Merges states greedily unless subsequentiality is violated
3 If the function is partial, exactness is guaranteed only where
the function is defined.
92
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA (Oncina et al 1993)
Subsequential relations
1 are a subclass of the regular relations, recognizing functions.
2 are those which are recognized by subsequential transducers,
which are determinstic on the input and which have an
?output? string associated with every state.
3 have a canonical form.
4 have been generalized to permit up to p outputs for each
input (Mohri 1997).
93
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
OSTIA for learning phonological rules
Gildea and Jurafsky 1996
1 Show that OSTIA doesn?t learn the English tapping rule or
German word-final devoicing rule from data present in
adapted dictionaries of English or German
2 Applied additional phonologically motivated heuristics to
improve state-merging choices.
What about well-defined subclasses of subsequential relations?
94
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Weighted finite-state automata
non-distribution-free with positive data
The problem
Given a finite multiset of words drawn independently from the
target distribution, what grammar accurately describes the
distribution?
Theorem
The class of distributions describable with Non-deterministic
Probabilistic Finite-State Automata (NPFA) exactly matches the
class of distributions describable with Hidden Markov Models
(Vidal et al 2005).
95
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Maximum Likelihood Estimation
A : 13
a : 0
b : 13
c : 13
M
A : 15
a : 15
b : 15
c : 15
M?
?
{bc}
M represents a family of
distributions with 4 parameters.
M? represents a particular
distribution in this family.
Theorem
For a sample S and deterministic finite-state acceptor M, counting the
parse of S through M and normalizing at each state optimizes the
maximum-likelihood estimate.
(Vidal et. al 2005, de la Higuera 2010) 96
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly 2-Local Distributions are bigram models
?
a?
b?
c ?
a
b
c
a
b
c
a
b
c
a
b
c
Figure: The structure of a bigram model. The 16 parameters of this
model are given by associating probabilities to each transition and to
?ending? at each state.
97
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subregular distributions
RegularFinite
Some well-defined
subregular class
1 When the structure of a Deterministic FSA is known in
advance, MLE is easy to do.
2 The DFA represents a subregular class of distributions.
98
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise Distributions
1 N-gram models can?t describe long-distance dependencies.
Long-distance dependencies in phonology
1 Consonantal harmony
(Jensen 1974, Odden 1994, Hansson 2001, Rose and Walker
2004, and many others)
2 Vowel harmony
(Ringen 1988, Bakovic? 2000, and many others)
99
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Sibilant Harmony example from Samala (Inesen?o
Chumash)
[StojonowonowaS] ?it stood upright? (Applegate 1972:72)
cf. *[stojonowonowaS] and
cf. *[Stojonowonowas]
Hypothesis: *[stojonowonowaS] and *[Stojonowonowas] are
ill-formed because the discontiguous subsequences sS and Ss are
ill-formed.
100
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise languages
Rogers et al 2010
1 solely make distinctions on the basis of potentially
discontiguous subsequences up to some length k
2 are mathematically natural. They have several chacterizations
in terms of formal language theory, automata theory, logic,
model theory, and the
3 algebraic theory of automata (Fu et al 2011)
101
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Strictly Piecewise Distributions
Heinz and Rogers 2010
1 are defined in terms of the factored automata-theoretic
representations (Rogers et al 2010)
2 along with the co-emission probability as the product (Vidal et
al. 2005)
3 Estimation over the factors permits learnability of the patterns
like the ones in Samala.
Example with ? = {a, b, c} and k = 2.
A0 A1 B0 B1 C0 C1? ?a b c
a
b
c
a
b
c
a
b
c
b
c
a
c
a
b
102
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
SP2 learning results for Chumash
Training corpus 4800 words from a dictionary of Samala
x
P(x | y <)
s >ts S >tS
y
s 0.0325 0.0051 0.0013 0.0002
ts 0.0212 0.0114 0.0008 0.
S 0.0011 0. 0.067 0.0359
>
tS 0.0006 0. 0.0458 0.0314
Table: SP2 probabilities of sibilant occuring sometime after another one
(collapsing laryngeal distinctions)
103
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning larger classes of regular distributions
More non-distribution-free with positive data
The class of distributions describable with PDFA
1 are identifiable in the limit with probability one (de la Higuera
and Thollard 2000).
2 are learnable in modified-PAC setting (Clark and Thollard
2004).
3 The algorithms presented employ state-merging methods.
1 This is a (much!) larger class than that which is describable
with n-gram distributions or with SP distributions.
2 To my knowledge these approaches have not been applied to
tasks in CL.
104
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary
#1. Define ?learning? so that large regions can be learned
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
Oncina et al 1993, de la Higuera and Thollard 2000, Clark and
Thollard 2004, . . .
105
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summary
#2. Target non-superfinite cross-cutting classes
Recursively Enumerable
Context-
Sensitive
Mildly
Context-
Sensitive
Context-FreeRegularFinite
Yoruba copying
Kobele 2006
Swiss German
Shieber 1985
English nested embedding
Chomsky 1957
English consonant clusters
Clements and Keyser 1983 Kwakiutl stress
Bach 1975
Chumash sibilant harmony
Applegate 1972
Angluin 1982, Muggleton 1990, Garcia et al 1990, Heinz 2010, . . .
106
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Have we put the cart before the horse?
1 So far we have discussed algorithms that learn various classes
of languages.
2 But shouldn?t we first know which classes are relevant for our
goals?
3 E.g. for phonology, while ?being regular? may be a necessary
property of phonological patterns, it certainly is not sufficient.
107
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Have we put the cart before the horse?
Research strategy
Patterns ? Characterizations ? Learning algorithms
1 Identify the range and kind of patterns (linguistics).
2 Characterize the range and kind of patterns (computational
linguistics).
3 Create learning algorithms for these classes, prove their
success in a variety of settings, and otherwise demonstrate
their success (grammatical inference, formal learning theory,
computational linguistics)
108
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subregular classes of regular sets
Regular
Star-Free=NonCounting
TSL LTT
LT PT
SL SP
Proper inclusion
relationships among
subregular language
classes.
instructor?s hunch for
phonology
TSL Tier-based Strictly Local PT Piecewise Testable
LTT Locally Threshold Testable SL Strictly Local
LT Locally Testable SP Strictly Piecewise
(McNaughton and Papert 1971, Simon 1975, Rogers and Pullum 2007, in
press, Rogers et al 2010, Heinz et al 2011)
109
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Conclusion to section 2 part 1
1 State-merging is a well-studied strategy for inferring
automata, including acceptors, transducers, and weighted
acceptors and transducers.
2 It has yielded theoretical results in many learning frameworks
including both distribution-free and non-distribution-free
learning frameworks.
110
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Conclusion to section 2 part 2
1 Many subclasses of regular languages are learnable even in the
hardest learning settings.
2 Recent advances yield algorithms for large classes
(probabilistic DFAs)
3 Computational linguists can explore which are relevant to
natural language and consequently which are useful for NLP!
4 There is a rich literature in GI which speaks to these classes,
and how such patterns in these classes can be learned.
111
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview
Empirical grammatical inference
Family of languages
Information contained in input
Overview of systems
Evaluation issues
From empirical to formal GI
112
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Introduction
Language learning
Starting from family of languages
Given set of samples
Identify language that is used to generate samples
Formal grammatical inference
Identify family of languages that can be learned efficiently
Under certain restrictions
Empirical grammatical inference
Exact underlying family of languages is unknown
Target language is approximation
113
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical GI
Try to identify language given samples
E.g. sentences (syntax), words (morphology), . . .
Underlying language class is unknown
For algorithm we still need to make a choice
If identification is impossible, provide approximation
Evaluation of empirical GI is different from formal GI
114
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Family of languages
What is the underlying family of languages?
Choice has impact on learning algorithm
Many possibilities
Use simple, fixed structures (n-grams)
Find probabilities
Extract structure from treebanks
Slightly more flexible structure
Find probabilities
Learn structure
Flexible structure
Find probabilities
115
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
N-grams
1 Starting from a plain text or collection of texts (corpus)
2 Extract all subsequences of length n (n-grams)
3 Count occurrences of n-grams in texts
4 Assign probabilities to each n-gram based on counts
Issues
Unseen n-grams
Back-off: use n-grams with smaller n
Smoothing: adjust probabilities for unseen n-grams
116
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Using n-gram models
How likely is the sentence ?John likes Mary??
Unigram language model
P(John likes Mary) ? P(John)P(likes)P(Mary)
Bigram language model
P(John likes Mary) ? P(John|?s?)P(likes|John)P(Mary|likes)
Trigram language model
P(John likes Mary) ?
P(John|?s??s?)P(likes|?s?John)P(Mary|John likes)
N-gram language model
P(wn1 ) ?
?n
k=1 P(wk |wk?1k?N+1)
N-grams provide a probability for each sequence
Probability describes how well sequence fits language
117
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Extract structure from treebanks
1 Starting from a treebank (sentences with structure)
2 Extract grammar rules that are used to create tree structures
For instance, context-free grammars (Charniak 1993)
or sub-trees (Data-Oriented Parsing) (Bod 1998)
3 Count occurrences of grammar rules in treebank
4 Assign probabilities to grammar rules based on counts
Issues
Over-generalization, ?incorrect? probabilities
Add information on applicability of grammar rules
(Johnson 1998)
Reestimate probabilities (EM)
(Dempster et al1977, Lari and Young 1990)
118
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Extract structure from tree
VB
PRP
He
VB1
adores
VB2
VB
listening
TO
TO
to
NN
music
VB ?PRP VB1 VB2
PRP?He
VB1?adores
VB2?VB TO
VB ?listening
TO ?TO NN
TO ?to
NN ?music
Extract counts from treebank ? probabilities
Reestimate probabilities
Improve fit of grammar and sentences
119
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learn structure
1 Starting from a corpus
2 Identify regularities that may serve as grammar rules
3 Output:
Structure assigned to sentences ? extract grammar
Extracted grammar rules (and probabilities) ? parse
Issues
Learning system has to deal with both
flexibility in structure
probabilities of structure
120
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Summarizing fixed versus flexible structure
Fixed versus flexible is really a sliding scale
Language modelling using n-grams
Structure is very simple and very rigid
Requires plain sequences as input
Corresponds to k-testable languages (Garc??a 1990)
Language modelling using extracted grammar rules
Structure is more flexible, but restricted by treebank
Requires structured sequences as input
Corresponds to e.g. (limited) context-free languages
?Learning structure?
Structure is flexible, restricted by learning algorithm
Requires plain sequences as input
Corresponds to e.g. context-free languages
121
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical grammatical inference
Choices:
What type of grammar are we learning?
Regular language
K -testable language (n-grams)
Context-free language
. . .
What kind of input do we require?
Sequence of words (sentence)
Sequence of part-of-speech tags
(Partial) tree structures
. . .
What kind of output do we want?
Structured version of input
Explicit grammar
Binary or n-ary (context-free rules)
. . .
122
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Overview of systems
EMILE
Alignment-Based Learning (ABL)
ADIOS
CCM+DMV
U-DOP
. . .
123
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Underlying approach
Given a collection of plain sentences
On what basis are we going to assign structure?
Should structure be linguistically motivated?
or similar to what linguists would assign?
Perhaps we can use tests for constituency to find structure
124
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Substitutability
Elements of the same type are substitutable
Test for constituency (Harris, 1951)
What is (a family fare)NP
Replace noun phrase with another noun phrase
What is (the payload of an African Swallow)NP
Learning by reversing test
What is (a family fare)X
What is (the payload of an African Swallow)X
125
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
EMILE
Learns context-free grammars
Using plain sentences
Originally used to show formal learnability
of (a form of) Categorial Grammars in a PAC learning setting
(Adriaans 1992, Adriaans and Vervoort 2002, Vervoort 2000)
Approach
1 Starting from simple sentences
identify recurring subsequences
2 Store recurring subsequences and contexts
3 Introduce grammar rules when there is enough evidence
Practical implementation allows for several constraints
Context length, subsequence length, . . .
126
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Example matrix
John walks
Mary walks
John sees Mary
(.) walks John (.) (.) sees Mary . . . contexts
John x x . . .
walks x . . .
Mary x . . .
sees . . .
... ... ... ... . . .
terms
127
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learn grammar rules
Terms that share (approximately) same context are clustered
?John? and ?Mary? are grouped together
Occurrences of terms in cluster are replaced by new symbol
Modified sequences may again contain terms/contexts
Terms may consist of multiple words
Example
John walks ?X walks
Mary walks ?X walks
John sees Mary ?X sees X
Mary slaps John?X slaps X
?sees? and ?slaps? now also share the same context
128
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Based on substitutability test
Using plain sentences
Similar to EMILE, but
Clustered terms are not explicitly replaced by symbol
Terms and contexts are always separated
All terms are considered (and only selected afterwards)
Output is structured version of input or grammar
(van Zaanen 2000a, b, 2002)
129
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Corpus Alignment
Learning
Hypothesis
Space
Hypothesis
Space
Selection
Learning
Structured
Corpus
Structured
Corpus
Grammar
Extraction
Grammar
130
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment-Based Learning (ABL)
Alignment learning
Align pairs of sentences
Unequal parts of sentences are stored as hypotheses
(Clustering)
Group hypotheses in same context together
Selection learning
Remove overlapping hypotheses
131
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Alignment learning
Align pairs of sentences
using edit distance (Wagner and Fischer 1974)
or suffixtrees (Geertzen and van Zaanen 2004, Ukkonen 1995)
Unequal parts of sentences are stored as hypotheses
Align all sentences in a corpus to all others
Example
(Y1 I need (X1a dinner during the flight)X1)Y1
(Z1 I need)Z1 (X1to return on (Z2tuesday)Z2)X1
(Y1(Z1he wants)Z1 to return on (Z2wednesday)Z2)Y1
132
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Selection Learning
Alignment learning can generate overlapping brackets
Underlying grammar is considered context-free
Structure describes parse according to underlying grammar
?Wrong? brackets have to be removed
Based on e.g. chronological order or statistics
Example
from (Y1Tilburg (X2to)Y1 Portland)X2
from (X1Portland (Y2to)X1 Tilburg)Y2
133
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
ADIOS
Automatic Distillation of Structure (ADIOS) (Solan 2005)
Idea
1 Represent language as a graph
2 Compress graph
3 As long as possible, find significant patterns in paths
Using substitutability and significance tests
4 (Recursion may be added as a post-processing step)
134
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Graph
sees Mary
S John walks E
Mary slaps John
135
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Phases
1 Initialization
Load all sentences (as paths) in the graph
2 Pattern distilation
Find sub-paths
shared by significant number of partially-aligned paths
using motif-extraction (MEX) algorithm
3 Generalization
Group all nodes that occur in same pattern together
Cluster words/subsequences similarly to EMILE
4 Repeat 2 and 3 until no new patterns are found
136
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Graph
S e1 e2 e3 e4 e5 E
If e2 e3 e4 is a significant pattern
S e1 e2 e3 e4 e5 E
137
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
MEX
Compute probabilities depending on in-/out-degree of nodes
PR(e1; e2) =
# paths from e1 to e2
# paths to e1
PR(e1; e3) =
# paths from e1 to e3
# paths to e1
DR(e1; e3) =
PR(e1; e4)
PR(e1; e3)
PR describes path to the right
similarly PL describes path to the left
Significance is computed based on DR and DL wrt parameter
Informally: find significant changes in number of paths
Pick most significant pattern
138
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Constituent-Context Model (CCM)
Consider all possible binary tree structures on POS sequences
Define a probability distribution over the possible bracketings
A bracketing is a particular structure on a sequence
P(s,B) = Pbin(B)P(s|B)
P(s|B) = ?i ,j :i?jPspan(sij |Bij)Pctx(si?1, sj |Bij)
Run (iterative) Expectation-Maximization (EM) algorithm
to maximize likelihood ?s?SP(s)
(Klein 2002)
139
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Dependency Model with Valence (DMV)
DMV aims to learn dependency relations
in contrast to CCM which learns context-free grammar rules
Dependency parse links words in a head-dependent relation
Model describes likelihood of
left dependencies
right dependencies
stop condition (no more dependencies)
Again, iterative EM is used to maximize likelihood of corpus
140
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
CCM+DMV
CCM and DMV can be combined
Both models have different view on structure
Results of combined system are better than either systems
Strengths of both systems are combined
(Klein 2004)
141
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
U-DOP
Similar to CCM in that it
finds probability distribution over ?all? structures
uses POS sequences
U-DOP uses Data-Oriented Parsing (DOP) as formalism
Extends probabilistic model of context-free grammars
Requires practical implementation choices
Random sampling due to huge size of search space
(Bod 2006a, b)
Procedure
1 Generate all possible binary trees on example sentences
2 Extract all subtrees
3 Estimate probabilities on subtrees using EM
142
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Subtrees
S
NP
PN
VP
V NP
S
NP VP
V NP
S
NP
PN
VP
S
NP VP
VP
V NP
NP
PN
Remove either all or no elements on a level
Leads to many subtrees
Each subtree receives a probability
Longer distance dependencies may be modeled
143
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Parsing
Subtrees can be recombined into a larger tree
Similar to context-free grammar rules
Same parse may be created using different derivations
Statistical model has to take this into account
Example
S
NP VP
V NP
? NP
PN
? NP
PN
= S
NP
PN
VP
V NP
PN
144
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Underlying idea
U-DOP works because span of subtrees reoccur in a corpus
Likelihood of ?useful? spans increase
Hence, likelihood of contexts (also subtrees) increase
Essentially, U-DOP uses implied substitutability
while system leans heavily on probabilities
145
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation
Base
treebank
Extract
sentences
Compare
treebanks
Results
Plain
corpus
Learning
system
Learned
treebank
Recall (completeness)
Precision (correctness)
F-Score (combination of Precision and Recall)
(van Zaanen and Adriaans 2001)
146
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation settings
Air Travel Information System (ATIS)
Taken from Penn Treebank II
568 English sentences
Example
list the flights from baltimore to seattle that stop in minneapolis
does this flight serve dinner
the flight should arrive at eleven a.m. tomorrow
what airline is this
147
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results on ATIS
Micro Macro Macro2
Precision 47.01 46.18 46.18
Recall 44.94 50.98 50.98
F-Score 44.60 47.10 48.46
Explanation
Micro Count constituents, weighted average per sentence
Macro Count constituents and average per sentence
Macro2 Compute Macro Precision/Recall, average at end
148
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Results on ATIS
remove remove remove
sentence empty both
Micro Precision 47.01 47.67 77.10 79.07
Micro Recall 44.94 45.30 44.95 45.29
Micro F-Score 44.60 45.09 55.31 56.13
Macro Precision 46.18 47.66 77.08 81.18
Macro Recall 50.98 52.96 51.07 52.80
Macro F-Score 47.10 48.62 60.00 62.47
Macro2 F-Score 48.46 50.17 61.43 63.99
Example
(bla bla bla)?bla bla bla
bla () bla ?bla bla
(bla () bla) ?bla bla
149
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Evaluation insights
No standard evaluation exists
but de facto evaluation datasets arise
ATIS (van Zaanen and Adriaans 2001)
WSJ10, WSJ40 (WSJ with sentence length limitations)
NEGRA10 (German)
CTB10 (Chinese)
Systems have different input/output
Evaluation settings influence results
Different metrics (micro/macro/macro2)
Included constituents (sentence/empty)
Formal grammatical inference does not have this problem
Evaluation performed through formal proofs
150
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Context-sensitive grammars
Learning context-free grammars is hard
Is learning context-sensitive grammars impossible?
That depends
To what degree is the grammar context-sensitive?
We may not need ?full? context-sensitiveness
Grammar rules: ?A? ? ???
Mildly context-sensitive grammars may be enough for NL
(Huybrechts 1984, Shieber 1985)
Perhaps the full power of context-freeness is not needed
151
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Family of languages
RegCFCSUnres b
Family to learn
152
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning context-sensitive languages
Open research area
Some work has already been done
Augmented Regular Expressions (Alque?zar 1997)
Variants of substitutability (Yoshinaka 2009)
Distributional Lattice Grammars (Clark 2010)
153
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Relationship between empirical and formal GI
Is there a relationship between empirical GI and formal GI?
Example: consider the case of substitutability
There are situations in which substitutability breaks:
John eats meat
John eats much
This suggests that learning based on substitutability
learns a different family of languages (not CFG)
Non-terminally separated (NTS) languages
Subclass of deterministic context-free grammars
154
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning NTS grammars
Grammar G=??,V ,P ,S? is NTS
? is vocabulary
V is set of non-terminals
P is set of production rules
S ? V is the start symbol
Additional restriction:
If N ? V
N ?? ???
M ?? ?
then N ?? ?M?
In other words:
non-terminals correspond exactly with substitutability
(Clark and Eyraud 2005, Clark 2006, Clark and Eyraud 2007)
155
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Learning NTS grammars
It can be shown that NTS grammars are
identifiable in the limit
PAC learnable
Unfortunately, natural language is not an NTS language
Ultimate goal:
Find family of languages that fits natural language
and is learnable in the right learning setting
156
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal GI and empirical GI
Relation between formal GI and empirical GI
Formal GI can show learnability
Under certain conditions
Emprical GI tries to learn structure from real data
Practically shows possibilities and limitations
Ultimate aim: Find family of languages that is
learnable under different conditions
fits natural languages
157
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
CONCLUSIONS
1 There have been new strong positive results in a recent past
for all the cases mentioned (subclasses of regular, PFA,
transducers, CFGs, MCSGs)
2 Look for ICGI! It?s the conference where these exciting results
happen (as well as exciting challenges, competitions,
benchmarks etc.)
3 The use of GI techniques both in computational linguistics
and natural language processing is taking place.
4 The future is bright!
158
References
P. W. Adriaans and M. van Zaanen. 2004. Computational
grammar induction for linguists. Grammars, 7:57?68.
Special issue with the theme ?Grammar Induction?.
P. W. Adriaans and M. van Zaanen. 2006. Computa-
tional grammatical inference. In D. E. Holmes and
L. C. Jain, editors, Innovations in Machine Learning,
volume 194 of Studies in Fuzziness and Soft Com-
puting, chapter 7. Springer-Verlag, Berlin Heidelberg,
Germany. To be published. ISBN: 3-540-30609-9.
P. W. Adriaans and M. Vervoort. 2002. The EMILE 4.1
grammar induction toolbox. In P. W. Adriaans, H. Fer-
nau, and M. van Zaanen, editors, Grammatical Infer-
ence: Algorithms and Applications (ICGI); Amster-
dam, the Netherlands, volume 2482 of Lecture Notes
in AI, pages 293?295, Berlin Heidelberg, Germany,
September 23?25. Springer-Verlag.
P. W. Adriaans. 1992. Language Learning from a Cat-
egorial Perspective. Ph.D. thesis, University of Ams-
terdam, Amsterdam, the Netherlands, November.
R. Alque?zar and A. Sanfeliu. 1997. Recognition and
learning of a class of context-sensitive languages de-
scribed by augmented regular expressions. Pattern
Recognition, 30(1):163?182.
D. Angluin and M. Kharitonov. 1991. When won?t mem-
bership queries help? In Proceedings of 24th ACM
Symposium on Theory of Computing, pages 444?454,
New York. ACM Press.
D. Angluin. 1981. A note on the number of queries
needed to identify regular languages. Information and
Control, 51:76?87.
D. Angluin. 1982. Inference of reversible languages.
Journal for the Association of Computing Machinery,
29(3):741?765.
D. Angluin. 1987a. Learning regular sets from
queries and counterexamples. Information and Con-
trol, 39:337?350.
D. Angluin. 1987b. Queries and concept learning. Ma-
chine Learning Journal, 2:319?342.
D. Angluin. 1988. Identifying languages from stochas-
tic examples. Technical Report YALEU/DCS/RR-614,
Yale University, March.
R. B. Applegate. 1972. Inesen?o Chumash Grammar.
Ph.D. thesis, University of California, Berkeley.
L. Beccera-Bonache, C. Bibire, and A. Horia Dediu.
2005. Learning DFA from corrections. In Henning
Fernau, editor, Proceedings of the Workshop on The-
oretical Aspects of Grammar Induction (TAGI), WSI-
2005-14, pages 1?11. Technical Report, University of
Tu?bingen.
L. Becerra-Bonache, C. de la Higuera, J. C. Janodet, and
F. Tantini. 2008. Learning balls of strings from edit
corrections. Journal of Machine Learning Research,
9:1841?1870.
D. Be?chet, A. Dikovsky, and A. Fore?t. 2011. Sur
les ite?rations disperse?es et les choix itr?e?s pour
l?apprentissage incre?mental des types dans les gram-
maires de de?pendances. In Proceedings of Confe?rence
d?Apprentissage.
A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K.
Warmuth. 1989. Learnability and the Vapnik-
Chervonenkis dimension. J. ACM, 36(4):929?965.
R. Bod. 1998. Beyond Grammar?An Experience-
Based Theory of Language, volume 88 of CSLI Lec-
ture Notes. Center for Study of Language and Infor-
mation (CSLI) Publications, Stanford:CA, USA.
R. Bod. 2006a. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of the 21st International
Conference on Computational Linguistics (COLING)
and 44th Annual Meeting of the Association of Com-
putational Linguistics (ACL); Sydney, Australia, pages
865?872. Association for Computational Linguistics.
R. Bod. 2006b. Unsupervised parsing with u-dop. In
CoNLL-X ?06: Proceedings of the Tenth Conference
on Computational Natural Language Learning, pages
85?92, Morristown, NJ, USA. Association for Com-
putational Linguistics.
R. C. Carrasco and J. Oncina. 1994. Learning stochastic
regular grammars by means of a state merging method.
In R. C. Carrasco and J. Oncina, editors, Grammatical
Inference and Applications, Proceedings of ICGI ?94,
number 862 in LNAI, pages 139?150. Springer-Verlag.
E. Charniak. 1993. Statistical Language Learning.
Massachusetts Institute of Technology Press, Cam-
bridge:MA, USA and London, UK.
N. Chater and P. Vita?nyi. 2007. ?ideal learning? of natu-
ral language: Positive results about learning from pos-
itive evidence. Journal of Mathematical Psychology,
51(3):135?163.
N. Chomsky. 1957. Syntactic Structures. Mouton & Co.,
Printers, The Hague.
A. Clark and R. Eyraud. 2005. Identification in the limit
of substitutable context-free languages. In S. Jain,
H. U. Simon, and E. Tomita, editors, Algorithmic
Learning Theory: 16th International Conference, ALT
2005, volume 3734 of Lecture Notes in Computer Sci-
ence, pages 283?296, Berlin Heidelberg, Germany.
Springer-Verlag.
A. Clark and R. Eyraud. 2007. Polynomial identification
in the limit of substitutable context-free languages.
Journal of Machine Learning Research, 8:1725?1745.
A. Clark and F. Thollard. 2004. Pac-learnability of prob-
abilistic deterministic finite state automata. Journal of
Machine Learning Research, 5:473?497.
A. Clark. 2006. PAC-learning unambiguous NTS lan-
guages. In Y. Sakakibara, S. Kobayashi, K. Sato,
T. Nishino, and E. Tomita, editors, Eighth Interna-
tional Colloquium on Grammatical Inference, (ICGI);
Tokyo, Japan, number 4201 in Lecture Notes in AI,
pages 59?71, Berlin Heidelberg, Germany. Springer-
Verlag.
A. Clark. 2010. Efficient, correct, unsupervised learn-
ing of context-sensitive languages. In CoNLL ?10:
Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 28?37,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
C. de la Higuera and J. Oncina. 2004. Learning proba-
bilistic finite automata. In G. Paliouras and Y. Sakak-
ibara, editors, Grammatical Inference: Algorithms and
Applications, Proceedings of ICGI ?04, volume 3264
of LNAI, pages 175?186. Springer-Verlag.
C. de la Higuera and F. Thollard. 2000. Identification in
the limit with probability one of stochastic determinis-
tic finite automata. In A.L. de Oliveira, editor, Gram-
matical Inference: Algorithms and Applications, Pro-
ceedings of ICGI ?00, volume 1891 of Lecture Notes
in Computer Science, pages 15?24. Springer-Verlag.
C. de la Higuera, J.-C. Janodet, and F. Tantini. 2008.
Learning languages from bounded resources: the case
of the DFA and the balls of strings. In A. Clark,
F. Coste, and L. Miclet, editors, Grammatical In-
ference: Algorithms and Applications, Proceedings
of ICGI ?08, volume 5278 of LNCS, pages 43?56.
Springer-Verlag.
C. de la Higuera. 1997. Characteristic sets for polyno-
mial grammatical inference. Machine Learning Jour-
nal, 27:125?138.
C. de la Higuera. 2010. Grammatical inference: learn-
ing automata and grammars. Cambridge University
Press, Cambridge, UK.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Matt Edlefsen, Dylan Leeman, Nathan Myers, Nathaniel
Smith, Molly Visscher, and David Wellcome. 2008.
Deciding strictly local (SL) languages. In Jon Breit-
enbucher, editor, Proceedings of the Midstates Con-
ference for Undergraduate Research in Computer Sci-
ence and Mathematics, pages 66?73.
Jie Fu, J. Heinz, and Herbert Tanner. 2011. An alge-
braic characterization of strictly piecewise languages.
In The 8th Annual Conference on Theory and Applica-
tions of Models of Computation, volume 6648 of Lec-
ture Notes in Computer Science. Springer-Verlag.
P. Garc??a and J. Ruiz. 2004. Learning k-testable
and k-piecewise testable languages from positive data.
Grammars, 7:125?140.
P. Garcia and E. Vidal. 1990. Inference of k-testable
languages in the strict sense and application to syntac-
tic pattern recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 12:920?925.
P. Garcia, E. Vidal, and J. Oncina. 1990. Learning lo-
cally testable languages in the strict sense. In Proceed-
ings of the Workshop on Algorithmic Learning Theory,
pages 325?338.
G.Clements and J. Keyser. 1983. CV phonology: a gen-
erative theory of the syllable. Cambridge, MA: MIT
Press.
J. Geertzen and M. van Zaanen. 2004. Grammati-
cal inference using suffix trees. In G. Paliouras and
Y. Sakakibara, editors, Grammatical Inference: Algo-
rithms and Applications: Seventh International Collo-
quium, (ICGI); Athens, Greece, volume 3264 of Lec-
ture Notes in AI, pages 163?174, Berlin Heidelberg,
Germany, October 11?13. Springer-Verlag.
D. Gildea and D. Jurafsky. 1996. Learning bias and
phonological-rule induction. Computational Linguis-
tics, 24(4).
L. Gleitman. 1990. The structural sources of verb mean-
ings. Language Acquisition, 1(1):3?55.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10(5):447?474.
E. M. Gold. 1978. Complexity of automaton identi-
fication from given data. Information and Control,
37:302?320.
K. C. Hansen and L. E. Hansen. 1969. Pintupi phonol-
ogy. Oceanic Linguistics, 8:153?170.
Z. S. Harris. 1951. Structural Linguistics. University of
Chicago Press, Chicago:IL, USA and London, UK, 7th
(1966) edition. Formerly Entitled: Methods in Struc-
tural Linguistics.
B. Hayes. 1995. Metrical Stress Theory. Chicago Uni-
versity Press.
J. Heinz and J. Rogers. 2010. Estimating strictly piece-
wise distributions. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 886?896, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
J. Heinz. 2008. Left-to-right and right-to-left iterative
languages. In Alexander Clark, Franc?ois Coste, and
Lauren Miclet, editors, Grammatical Inference: Al-
gorithms and Applications, 9th International Collo-
quium, volume 5278 of Lecture Notes in Computer
Science, pages 84?97. Springer.
J. Heinz. 2009. On the role of locality in learning stress
patterns. Phonology, 26(2):303?351.
J. Heinz. 2010. String extension learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 897?906, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
R. M. A. C. Huybrechts. 1984. The weak adequacy
of context-free phrase structure grammar. In G. J.
de Haan, M. Trommelen, and W. Zonneveld, editors,
Van periferie naar kern, pages 81?99. Foris, Dor-
drecht, the Netherlands.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632, December.
A. Kasprzik and T. Ko?tzing. 2010. String extension
learning using lattices. In Henning Fernau Adrian-
Horia Dediu and Carlos Mart??n-Vide, editors, Pro-
ceedings of the 4th International Conference on Lan-
guage and Automata Theory and Applications (LATA
2010), volume 6031 of Lecture Notes in Computer Sci-
ence, pages 380?391, Trier, Germany. Springer.
M. Kearns and L. Valiant. 1989. Cryptographic lim-
itations on learning boolean formulae and finite au-
tomata. In 21st ACM Symposium on Theory of Com-
puting, pages 433?444.
M. J. Kearns and U. Vazirani. 1994. An Introduction to
Computational Learning Theory. MIT press.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In 40th Annual Meeting of the Association
for Computational Linguistics; Philadelphia:PA, USA,
pages 128?135. Association for Computational Lin-
guistics, July. yes.
D. Klein. 2004. Corpus-based induction of syntactic
structure: Models of dependency and constituency. In
42th Annual Meeting of the Association for Computa-
tional Linguistics; Barcelona, Spain, pages 479?486.
G. Kobele. 2006. Generating Copies: An Investigation
into Structural Identity in Language and Grammar.
Ph.D. thesis, University of California, Los Angeles.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4(35?56).
R. McNaughton and S. Papert. 1971. Counter-Free Au-
tomata. MIT Press.
M. Mohri. 1997. Finite-state transducers in language
and speech processing. Computational Linguistics,
23(2):269?311.
S. Muggleton. 1990. Inductive Acquisition of Expert
Knowledge. Addison-Wesley.
J. Oncina and P. Garc??a. 1992. Identifying regular lan-
guages in polynomial time. In H. Bunke, editor, Ad-
vances in Structural and Syntactic Pattern Recogni-
tion, volume 5 of Series in Machine Perception and
Artificial Intelligence, pages 99?108. World Scientific.
J. Oncina, P. Garc??a, and E. Vidal. 1993. Learning sub-
sequential transducers for pattern recognition tasks.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15:448?458, May.
L. Pitt. 1985. Probabilistic Inductive Inference. Ph.D.
thesis, Yale University. Computer Science Depart-
ment, TR-400.
L. Pitt. 1989. Inductive inference, DFA?s, and compu-
tational complexity. In Analogical and Inductive In-
ference, number 397 in LNAI, pages 18?44. Springer-
Verlag.
J. Rogers and G. Pullum. to appear. Aural pattern recog-
nition experiments and the subregular hierarchy. Jour-
nal of Logic, Language and Information.
J. Rogers, J. Heinz, Gil Bailey, Matt Edlefsen, Molly
Visscher, David Wellcome, and Sean Wibel. 2010.
On languages piecewise testable in the strict sense. In
Christian Ebert, Gerhard Ja?ger, and Jens Michaelis,
editors, The Mathematics of Language, volume 6149
of Lecture Notes in Artifical Intelligence, pages 255?
265. Springer.
S. M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
I. Simon. 1975. Piecewise testable events. In Automata
Theory and Formal Languages, pages 214?222.
Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2005.
Unsupervised learning of natural languages. Proceed-
ings of the National Academy of Sciences of the United
States of America, 102(33):11629?11634, August.
A. Stolcke. 1994. Bayesian Learning of Probabilistic
Language Models. Ph.D. thesis, University of Califor-
nia, Berkeley.
I. Tellier. 2008. How to split recursive automata. In
ICGI, pages 200?212.
E. Ukkonen. 1995. On-line construction of suffix trees.
Algorithmica, 14:249?260.
L. G. Valiant. 1984. A theory of the learnable. Commu-
nications of the Association for Computing Machinery,
27(11):1134?1142.
M. van Zaanen and P. W. Adriaans. 2001. Alignment-
Based Learning versus EMILE: A comparison. In
Proceedings of the Belgian-Dutch Conference on Ar-
tificial Intelligence (BNAIC); Amsterdam, the Nether-
lands, pages 315?322, October.
M. van Zaanen. 2000a. ABL: Alignment-Based
Learning. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING);
Saarbru?cken, Germany, pages 961?967. Association
for Computational Linguistics, July 31?August 4.
M. van Zaanen. 2000b. Bootstrapping syntax and recur-
sion using Alignment-Based Learning. In P. Langley,
editor, Proceedings of the Seventeenth International
Conference on Machine Learning; Stanford:CA, USA,
pages 1063?1070, June 29?July 2.
M. van Zaanen. 2002. Bootstrapping Structure into Lan-
guage: Alignment-Based Learning. Ph.D. thesis, Uni-
versity of Leeds, Leeds, UK, January.
Marco R. Vervoort. 2000. Games, Walks and Grammars.
Ph.D. thesis, University of Amsterdam, Amsterdam,
the Netherlands, September.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005a. Probabilistic finite-state
machines-part I. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1013?1025.
E. Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,
and R. C. Carrasco. 2005b. Probabilistic finite-state
machines-part II. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 27(7):1026?1039.
R. A. Wagner and M. J. Fischer. 1974. The string-to-
string correction problem. Journal of the Association
for Computing Machinery, 21(1):168?173.
R. Wiehagen, R. Frievalds, and E. Kinber. 1984. On the
power of probabilistic strategies in inductive inference.
Theoretical Computer Science, 28:111?133.
T. Yokomori. 2003. Polynomial-time identification of
very simple grammars from positive data. Theoretical
Computer Science, 298(1):179?206.
R. Yoshinaka. 2009. Learning mildly context-sensitive
languages with multidimensional substitutability from
positive data. In R. Gavalda`, G. Lugosi, T. Zeugmann,
and S. Zilles, editors, Proceedings of the Workshop on
Algorithmic Learning Theory, volume 5809 of Lecture
Notes in Computer Science, pages 278?292. Springer
Berlin / Heidelberg.
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 65?72
Manchester, August 2008
Improving Word Segmentation by Simultaneously Learning Phonotactics
Daniel Blanchard
Computer & Information Sciences
University of Delaware
dsblanch@udel.edu
Jeffrey Heinz
Linguistics & Cognitive Science
University of Delaware
heinz@udel.edu
Abstract
The most accurate unsupervised word seg-
mentation systems that are currently avail-
able (Brent, 1999; Venkataraman, 2001;
Goldwater, 2007) use a simple unigram
model of phonotactics. While this sim-
plifies some of the calculations, it over-
looks cues that infant language acquisition
researchers have shown to be useful for
segmentation (Mattys et al, 1999; Mattys
and Jusczyk, 2001). Here we explore the
utility of using bigram and trigram phono-
tactic models by enhancing Brent?s (1999)
MBDP-1 algorithm. The results show
the improved MBDP-Phon model outper-
forms other unsupervised word segmenta-
tion systems (e.g., Brent, 1999; Venkatara-
man, 2001; Goldwater, 2007).
1 Introduction
How do infants come to identify words in the
speech stream? As adults, we break up speech
into words with such ease that we often think
that there are audible pauses between words in the
same sentence. However, unlike some written lan-
guages, speech does not have any completely reli-
able markers for the breaks between words (Cole
and Jakimik, 1980). In fact, languages vary on how
they signal the ends of words (Cutler and Carter,
1987), which makes the task even more daunting.
Adults at least have a lexicon they can use to rec-
ognize familiar words, but when an infant is first
born, they do not have a pre-existing lexicon to
consult. In spite of these challenges, by the age of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
six months infants can begin to segment words out
of speech (Bortfeld et al, 2005). Here we present
an efficient word segmentation system aimed to
model how infants accomplish the task.
While an algorithm that could reliably extract
orthographic representations of both novel and fa-
miliar words from acoustic data is something we
would like to see developed, following earlier re-
searchers, we simplify the problem by using a text
that does not contain any word boundary markers.
Hereafter, we use the phrase ?word segmentation?
to mean some process which adds word boundaries
to a text that does not contain them.
This paper?s focus is on unsupervised, incre-
mental word segmentation algorithms; i.e., those
that do not rely on preexisting knowledge of a par-
ticular language, and those that segment the cor-
pus one utterance at a time. This is in contrast
to supervised word segmentation algorithms (e.g.,
Teahan et al, 2000), which are typically used for
segmenting text in documents written in languages
that do not put spaces between their words like
Chinese. (Of course, unsupervised word segmen-
tation algorithms also have this application.) This
also differs from batch segmentation algorithms
(Goldwater, 2007; Johnson, 2008b; Fleck, 2008),
which process the entire corpus at least once be-
fore outputting a segmentation of the corpus. Un-
supervised incremental algorithms are of interest
to some psycholinguists and acquisitionists inter-
ested in the problem of language learning, as well
as theoretical computer scientists who are inter-
ested in what unsupervised, incremental models
are capable of achieving.
Phonotactic patterns are the rules that deter-
mine what sequences of phonemes or allophones
are allowable within words. Learning the phono-
tactic patterns of a language is usually modeled
65
separately from word segmentation; e.g., current
phonotactic learners such as Coleman and Pierre-
humbert (1997), Heinz (2007), or Hayes and Wil-
son (2008) are given word-sized units as input.
However, infants appear to simultaneously learn
which phoneme combinations are allowable within
words and how to extract words from the input. It
is reasonable that the two processes feed into one
another, and when infants acquire a critical mass of
phonotactic knowledge, they use it to make judge-
ments about what phoneme sequences can occur
within versus across word boundaries (Mattys and
Jusczyk, 2001). We use this insight, also suggested
by Venkataraman (2001) and recently utilized by
Fleck (2008) in a different manner, to enhance
Brent?s (1999) model MBDP-1, and significantly
increase segmentation accuracy. We call this mod-
ified segmentation model MBDP-Phon.
2 Related Work
2.1 Word Segmentation
The problem of unsupervised word segmentation
has attracted many earlier researchers over the
past fifty years (e.g., Harris, 1954; Olivier, 1968;
de Marcken, 1995; Brent, 1999). In this section,
we describe the base model MBDP-1, along with
two other segmentation approaches, Venkataraman
(2001) and Goldwater (2007). In ?4, we compare
MBDP-Phon to these models in more detail. For
a thorough review of word segmentation literature,
see Brent (1999) or Goldwater (2007).
2.1.1 MBDP-1
Brent?s (1999) MBDP-1 (Model Based Dy-
namic Programming) algorithm is an implemen-
tation of the INCDROP framework (Brent, 1997)
that uses a Bayesian model of how to generate an
unsegmented text to insert word boundaries. The
generative model consists of five steps:
1. Choose a number of word types, n.
2. Pick n distinct strings from ?
+
#, which will
make up the lexicon, L. Entries in L are la-
beled W
1
. . .W
n
. W
0
= $, where $ is the
utterance boundary marker.
3. Pick a function, f , which maps word types to
their frequency in the text.
4. Choose a function, s, to map positions in the
text to word types.
5. Concatenate the words in the order specified
by s, and remove the word delimiters (#).
It is important to note that this model treats the
generation of the text as a single event in the prob-
ability space, which allows Brent to make a num-
ber of simplifying assumptions. As the values for
n,L, f, and s completely determine the segmenta-
tion, the probability of a particular segmentation,
w
m
, can be calculated as:
P (w
m
) = P (n,L, f, s) (1)
To allow the model to operate on one utterance at
a time, Brent states the probability of each word in
the text as a recursive function, R(w
k
), where w
k
is the text up to and including the word at position
k, w
k
. Furthermore, there are two specific cases
for R: familiar words and novel words. If w
k
is
familiar, the model already has the word in its lex-
icon, and its score is calculated as in Equation 2.
R(w
k
) =
f(w
k
)
k
?
(
f(w
k
)? 1
f(w
k
)
)
2
(2)
Otherwise, the word is novel, and its score is cal-
culated using Equation 3
1
(Brent and Tao, 2001),
R(w
k
) =
6
pi
2
?
n
k
?
P
?
(a
1
)...P
?
(a
q
)
1?P
?
(#)
?
(
n?1
n
)
2
(3)
where P
?
is the probability of a particular
phoneme occurring in the text. The third term of
the equation for novel words is where the model?s
unigram phonotactic model comes into play. We
detail how to plug a more sophisticated phonotac-
tic learning model into this equation in ?3. With
the generative model established, MBDP-1 uses a
Viterbi-style search algorithm to find the segmen-
tation for each utterance that maximizes the R val-
ues for each word in the segmentation.
Venkataraman (2001) notes that considering the
generation of the text as a single event is un-
likely to be how infants approach the segmenta-
tion problem. However, MBDP-1 uses an incre-
mental search algorithm to segment one utterance
at a time, which is more plausible as a model of
infants? word segmentation.
1
Brent (1999) originally described the novel word score
as R(w
k
) =
6
pi
2
?
n
k
k
?
P
?
(W
n
k
)
1?
n
k
?1
n
k
?
?
n
k
j=1
P
?
(W
j
)
?
(
n
k
?1
n
k
)
2
,
where P
?
is the probability of all the phonemes in the word
occurring together, but the denominator of the third term was
dropped in Brent and Tao (2001). This change drastically
speeds up the model, and only reduces segmentation accuracy
by ? 0.5%.
66
2.1.2 Venkataraman (2001)
MBDP-1 is not the only incremental unsuper-
vised segmentation model that achieves promis-
ing results. Venkataraman?s (2001) model tracks
MBDP-1?s performance so closely that Batchelder
(2002) posits that the models are performing the
same operations, even though the authors describe
them differently.
Venkataraman?s model uses a more traditional,
smoothed n-gram model to describe the distribu-
tion of words in an unsegmented text.
2
The most
probable segmentation is retrieved via a dynamic
programming algorithm, much like Brent (1999).
We use MBDP-1 rather than Venkataraman?s
approach as the basis for our model only because it
was more transparent how to plug in a phonotactic
learning module at the time this project began.
2.1.3 Goldwater (2007)
We also compare our results to a segmenter put
forward by Goldwater (2007). Goldwater?s seg-
menter uses an underlying generative model, much
like MBDP-1 does, only her language model is
described as a Dirichlet process (see also John-
son, 2008b). While this model uses a unigram
model of phoneme distribution, as did MBDP-1, it
implements a bigram word model like Venkatara-
man (2001). A bigram word model is useful in
that it prevents the segmenter from assuming that
frequent word bigrams are not simply one word,
which Goldwater observes happen with a unigram
version of her model.
Goldwater uses a Gibbs sampler augmented
with simulated annealing to sample from the pos-
terior distribution of segmentations and deter-
mine the most likely segmentation of each utter-
ance.
3
This approach requires non-incremental
learning.
4
We include comparison with Goldwa-
ter?s segmenter because it outperforms MBDP-1
and Venkataraman (2001) in both precision and
recall, and we are interested in whether an incre-
mental algorithm supplemented with phonotactic
learning can match its performance.
2.2 Phonotactic Learning
Phonotactic acquisition models have seen a surge
in popularity recently (e.g., Coleman and Pierre-
2
We refer the reader to Venkataraman (2001) for the de-
tails of this approach.
3
We direct the reader to Goldwater (2007) for details.
4
In our experiments and those in Goldwater (2007), the
segmenter runs through the corpus 1000 times before out-
putting the final segmentation.
humbert, 1997; Heinz, 2007; Hayes and Wilson,
2008). While Hayes and Wilson present a more
complex Maximum Entropy phonotactic model in
their paper than the one we add to MBDP-1, they
also evaluate a simple n-gram phonotactic learner
operating over phonemes. The input to the mod-
els is a list of English onsets and their frequency
in the lexicon, and the basic trigram learner simply
keeps track of the trigrams it has seen in the cor-
pus. They test the model on novel words with ac-
ceptable rhymes?some well-formed (e.g., [kIp]),
and some less well-formed (e.g., [stwIk])?so any
ill-formedness is attributable to onsets. This ba-
sic trigram model explains 87.7% of the variance
in the scores that Scholes (1966) reports his 7th
grade students gave when subjected to the same
test. When Hayes and Wilson run their Maximum
Entropy phonotactic learning model with n-grams
over phonological features, the r-score increases
substantially to 95.6%.
Given the success and simplicity of the basic n-
gram phonotactic model, we choose to integrate
this with MBDP-1.
3 Extending MBDP-1 with Phonotactics
The main contribution of our work is adding
a phonotactic learning component to MBDP-1
(Brent, 1999). As we mention in ?2.1.1, the third
term of Equation 3 is where MBDP-1?s unigram
phonotactic assumption surfaces. The original
model simply multiplies the probabilities of all the
phonemes in the word together and divides by one
minus the probability of a particular phoneme be-
ing the word boundary to come up with probabil-
ity of the phoneme combination. The order of the
phonemes in the word has no effect on its score.
The only change we make to MBDP-1 is to the
third term of Equation 3. In MBDP-Phon this be-
comes
q
?
i=0
P
MLE
(a
i
. . . a
j
) (4)
where a
i
. . . a
j
is an n-gram inside a proposed
word, and a
0
and a
q
are both the word boundary
symbol, #
5
.
It is important to note that probabilities calcu-
lated in Equation 4 are maximum likelihood esti-
mates of the joint probability of each n-gram in the
word. The maximum likelihood estimate (MLE)
5
The model treats word boundary markers like a phoneme
for the purposes of storing n-grams (i.e., a word boundary
marker may occur anywhere within the n-grams).
67
for a particular n-gram inside a word is calculated
by dividing the total number of occurrences of that
n-gram (including in the word we are currently ex-
amining) by the total number of n-grams (includ-
ing those in the current word). The numbers of
n-grams are computed with respect to the obtained
lexicon, not the corpus, and thus the frequency of
lexical items in the corpus does not affect the n-
gram counts, just like Brent?s unigram phonotactic
model and other phonotactic learning models (e.g.,
Hayes and Wilson, 2008).
We use the joint probability instead of the con-
ditional probability which is often used in compu-
tational linguistics (Manning and Sch?utze, 1999;
Jurafsky and Martin, 2000), because of our intu-
ition that the joint probability is truer to the idea
that a phonotactically well-formed word is made
up of n-grams that occur frequently in the lexicon.
On the other hand, the conditional probability is
used when one tries to predict the next phoneme
that will occur in a word, rather than judging the
well-formedness of the word as a whole.
6
We are able to drop the denominator that was
originally in Equation 3, because P
?
(#) is zero
for an n-gram model when n > 1. This sim-
ple modification allows the model to learn what
phonemes are more likely to occur at the begin-
nings and ends of words, and what combinations
of phonemes rarely occur within words.
What is especially interesting about this mod-
ification is that the phonotactic learning compo-
nent estimates the probabilities of the n-grams by
using their relative frequencies in the words the
segmenter has extracted. The phonotactic learner
is guaranteed to see at least two valid patterns in
every utterance, as the n-grams that occur at the
beginnings and ends of utterances are definitely
at the beginnings and ends of words. This al-
lows the learner to provide useful information to
the segmenter even early on, and as the segmenter
correctly identifies more words, the phonotactic
learner has more correct data to learn from. Not
only is this mutually beneficial process supported
by evidence from language acquisitionists (Mat-
tys et al, 1999; Mattys and Jusczyk, 2001), it also
resembles co-training (Blum and Mitchell, 1998).
We refer to the extended version of Brent?s model
6
This intuition is backed up by preliminary results sug-
gesting MBDP-Phon performs better when usingMLEs of the
joint probability as opposed to conditional probability. There
is an interesting question here, which is beyond the scope of
this paper, so we leave it for future investigation.
described above as MBDP-Phon.
4 Evaluation
4.1 The Corpus
We run all of our experiments on the Bernstein-
Ratner (1987) infant-directed speech corpus from
the CHILDES database (MacWhinney and Snow,
1985). This is the same corpus that Brent (1999),
Goldwater (2007), and Venkataraman (2001) eval-
uate their models on, and it has become the de
facto standard for segmentation testing, as unlike
other corpora in CHILDES, it was phonetically
transcribed.
We examine the transcription system Brent
(1999) uses and conclude some unorthodox
choices were made when transcribing the corpus.
Specifically, some phonemes that are normally
considered distinct are combined into one symbol,
which we call a bi-phone symbol. These phonemes
combinations include diphthongs and vowels fol-
lowed by /?/. Another seemingly arbitrary deci-
sion is the distinction between stressed and un-
stressed syllabic /?/ sound (i.e., there are differ-
ent symbols for the /?/ in ?butter? and the /?/ in
?bird?) since stress is not marked elsewhere in the
corpus. To see the effect of these decisions, we
modified the corpus so that the bi-phone symbols
were split into two
7
and the syllabic /?/ symbols
were collapsed into one.
4.2 Accuracy
We ran MBDP-1 on the original corpus, and the
modified version of the corpus. As illustrated by
Figures 1 and 2, MBDP-1 performs worse on the
modified corpus with respect to both precision and
recall. As MBDP-1 and MBDP-Phon are both iter-
ative learners, we calculate segmentation precision
and recall values over 500-utterance blocks. Per
Brent (1999) and Goldwater (2007), precision and
recall scores reflect correctly segmented words,
not correctly identified boundaries.
We also test to see how the addition of an n-gram
phonotactic model affects the segmentation accu-
racy of MBDP-Phon by comparing it to MBDP-
1 on our modified corpus.
8
As seen in Figure 3,
MBDP-Phon using bigrams (henceforth MBDP-
Phon-Bigrams) is consistently more precise in its
7
We only split diphthongs whose first phoneme can occur
in isolation in English, so the vowels in ?bay? and ?boat? were
not split.
8
We also compare MBDP-Phon to MBDP-1 on the origi-
nal corpus. The results are given in Tables 1 and 2.
68
0.45
0.50
0.55
0.60
0.65
0.70
0.75
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
P
r
e
c
i
s
i
o
n
Utterances Processed
Modified Original
Figure 1: Precision of MBDP-1 on both corpora.
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
R
e
c
a
l
l
Utterances Processed
Modified Original
Figure 2: Recall of MBDP-1 on both corpora.
segmentation thanMBDP-1, and bests it by? 18%
in the last block. Furthermore, MBDP-Phon-
Bigrams significantly outpaces MBDP-1 with re-
spect to recall only after seeing 1000 utterances,
and finishes the corpus ? 10% ahead of MBDP-
1 (see Figure 4). MBDP-Phon-Trigrams does not
fair as well in our tests, falling behind MBDP-1
and MBDP-Phon-Bigrams in recall, and MBDP-
Phon-Bigrams in precision. We attribute this poor
performance to the fact that we are not currently
smoothing the n-gram models in any way, which
leads to data sparsity issues when using trigrams.
We discuss a potential solution to this problem in
?5.
Having established that MBDP-Phon-Bigrams
significantly outperforms MBDP-1, we compare
its segmentation accuracy to those of Goldwater
(2007) and Venkataraman (2001).
9
As before, we
9
We only examine Venkataraman?s unigram model, as his
bigram and trigram models perform better on precision, but
worse on recall.
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
P
r
e
c
i
s
i
o
n
Utterances Processed
MBDP-1 MBDP-Bigrams MBDP-Trigrams
Figure 3: Precision of MBDP-1 and MBDP-Phon
on modified corpus.
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
R
e
c
a
l
l
Utterances Processed
MBDP-1 MBDP-Bigrams MBDP-Trigrams
Figure 4: Recall of MBDP-1 and MBDP-Phon on
modified corpus.
run the models on the entire corpus, and then mea-
sure their performance over 500-utterance blocks.
MBDP-Phon-Bigrams edges out Goldwater?s
model in precision on our modified corpus, with
an average precision of 72.79% vs. Goldwa-
ter?s 70.73% (Table 1). If we drop the first 500-
utterance block for MBDP-Phon-Bigrams because
the model is still in the early learning stages,
whereas Goldwater?s has seen the entire corpus, its
average precision increases to 73.21% (Table 1).
When considering the recall scores in Table 2,
it becomes clear that MBDP-Phon-Bigrams has a
clear advantage over the other models. Its aver-
age recall is higher than or nearly equal to both
of the other models? maximum scores. Since
Venkataraman?s (2001) model performs similarly
to MBDP-1, it is no surprise that MBDP-Phon-
Bigrams achieves higher precision and recall.
69
MBDP-
Phon-
Bigrams
Venkataraman Goldwater
Original: Utterances 0 to 9790
Avg. 72.84% 67.46% 67.87%
Max. 79.91% 71.79% 71.98%
Min. 63.97% 61.77% 61.87%
Modified: Utterances 0 to 9790
Avg. 72.79% 59.64% 70.73%
Max. 80.60% 66.84% 74.61%
Min. 64.78% 52.54% 65.29%
Modified: Utterances 500 to 9790
Avg. 73.21% 59.54% 70.59%
Max. 80.60% 66.84% 74.61%
Min. 67.40% 52.54% 65.29%
Table 1: Precision statistics for MBDP-Phon-
Bigrams, Goldwater, and Venkataraman on both
corpora over 500-utterance blocks.
The only metric by which MBDP-Phon-
Bigrams does not outperform the other algorithms
is lexical precision, as shown in Table 3. Lexi-
cal precision is the ratio of the number of correctly
identified words in the lexicon to the total number
of words in the lexicon (Brent, 1999; Venkatara-
man, 2001).
10
The relatively poor performance
of MBDP-Phon-Bigrams is due to the incremental
nature of the MBDP algorithm. Initially, it makes
numerous incorrect guesses that are added to the
lexicon, and there is no point at which the lexi-
con is purged of earlier erroneous guesses (c.f. the
improved lexical precision when omitting the first
block in Table 3). On the other hand, Goldwater?s
algorithm runs over the corpus multiple times, and
only produces output when it settles on a final seg-
mentation.
In sum, MBDP-Phon-Bigrams significantly im-
proves the accuracy of MBDP-1, and achieves
better performance than the models described in
Venkataraman (2001) and Goldwater (2007).
5 Future Work
There are many ways to implement phonotactic
learning. One idea is to to use n-grams over phono-
logical features, as per Hayes and Wilson (2008).
Preliminary results have shown that we need to add
smoothing to our n-grammodel, and we plan to use
10
See Brent (1999) for a discussion of the meaning of this
statistic.
MBDP-
Phon-
Bigrams
Venkataraman Goldwater
Original: Utterances 0 to 9790
Avg. 72.03% 70.02% 71.02%
Max. 79.31% 75.59% 76.79%
Min. 44.71% 42.57% 64.32%
Modified: Utterances 0 to 9790
Avg. 74.63% 66.24% 70.48%
Max. 82.45% 70.47% 74.79%
Min. 47.63% 44.71% 63.74%
Modified: Utterances 500 to 9790
Avg. 76.05% 67.37% 70.28%
Max. 82.45% 70.47% 74.79%
Min. 71.92% 63.86% 63.74%
Table 2: Recall statistics for MBDP-Phon-
Bigrams, Goldwater, and Venkataraman on both
corpora over 500-utterance blocks.
Modified Kneser-Ney smoothing (Chen and Good-
man, 1998).
Another approach would be to develop a
syllable-based phonotactic model (Coleman and
Pierrehumbert, 1997). Johnson (2008b) achieves
impressive segmentation results by adding a sylla-
ble level with Adaptor grammars.
Some languages (e.g., Finnish, and Navajo)
contain long-distance phonotactic constraints that
cannot be learned by n-gram learners (Heinz,
2007). Heinz (2007) shows that precedence-based
learners?which work like a bigram model, but
without the restriction that the elements in the bi-
gram be adjacent?can handle many long-distance
agreement patterns (e.g., vowel and consonantal
harmony) in the world?s languages. We posit that
adding such a learner to MBDP-Phon would allow
it to handle a greater variety of languages.
Since none of these approaches to phonotactic
learning depend on MBDP-1, it is also of interest
to integrate phonotactic learners with other word
segmentation strategies.
In addition to evaluating segmentation models
integrated with phonotactic learning on their seg-
mentation performance, it would be interesting to
evaluate the quality of the phonotactic grammars
obtained. A good point of comparison for English
are the constraints obtained by Hayes and Wilson
(2008), since the data with which they tested their
phonotactic learner is publicly available.
Finally, we are looking forward to investigat-
70
MBDP-
Phon-
Bigrams
Venkataraman Goldwater
Original: Utterances 0 to 9790
Avg. 47.69% 49.78% 56.50%
Max. 49.71% 52.95% 63.09%
Min. 46.30% 41.83% 55.33%
Modified: Utterances 0 to 9790
Avg. 48.31% 45.98% 58.03%
Max. 50.42% 48.90% 65.58%
Min. 41.74% 36.57% 56.43%
Modified: Utterances 500 to 9790
Avg. 54.34% 53.06% 57.95%
Max. 63.76% 54.35% 62.30%
Min. 51.31% 51.95% 56.52%
Table 3: Lexical precision statistics for MBDP-
Phon-Bigrams, Goldwater, and Venkataraman on
both corpora over 500-utterance blocks.
ing the abilities of these segmenters on corpora
of different languages. Fleck (2008) tests her seg-
menter on a number of corpora, including Arabic
and Spanish, and Johnson (2008a) applies his seg-
menter to a corpus of Sesotho.
6 Conclusion
From the results established in ?4, we can con-
clude that MBDP-Phon using a bigram phonotac-
tic model is more accurate than the models de-
scribed in Brent (1999), Venkataraman (2001), and
Goldwater (2007). The n-gram phonotactic model
improves overall performance, and is especially
useful for corpora that do not encode diphthongs
with bi-phone symbols. The main reason there
is such a marked improvement with MBDP-Phon
vs. MBDP-1 when the bi-phone symbols were re-
moved from the original corpus is that these bi-
phone symbols effectively allow MBDP-1 to have
a select few bigrams in the cases where it would
otherwise over-segment.
The success of MBDP-Phon is not clear evi-
dence that the INCDROP framework (Brent, 1997)
is superior to Venkataraman or Goldwater?s mod-
els. We imagine that adding a phonotactic learning
component to either of their models would also im-
prove their performance.
We also tentatively conclude that phonotactic
patterns can be learned from unsegmented text.
However, the phonotactic patterns learned by our
model ought to be studied in detail to see how well
they match the phonotactic patterns of English.
MBDP-Phon?s performance reinforces the the-
ory put forward by language acquisition re-
searchers that phonotactic knowledge is a cue for
word segmentation (Mattys et al, 1999; Mattys
and Jusczyk, 2001). Furthermore, our results in-
dicate that learning phonotactic patterns can oc-
cur simultaneously with word segmentation. Fi-
nally, further investigation of the simultaneous ac-
quisition of phonotactics and word segmentation
appears fruitful for theoretical and computational
linguists, as well as acquisitionists.
Acknoledgements
We are grateful to Roberta Golinkoff who inspired
this project. We also thank Vijay Shanker for
valuable discussion, Michael Brent for the corpus,
and Sharon Goldwater for the latest version of her
code.
References
Batchelder, Eleanor Olds. 2002. Bootstrapping the
lexicon: a computational model of infant speech
segmentation. Cognition, 83(2):167?206.
Bernstein-Ratner, Nan. 1987. The phonology of
parent child speech, volume 6. Erlbaum, Hills-
dale, NJ.
Blum, Avrim and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Workshop on Computational Learning Theory,
pages 92?100.
Bortfeld, Heather, James Morgan, Roberta
Golinkoff, and Karen Rathbun. 2005. Mommy
and me: Familiar names help launch babies into
speech-stream segmentation. Psychological
Science, 16(4):298?304.
Brent, Michael R. 1997. Towards a unified model
of lexical acquisition and lexical access. Journal
of Psycholinguistic Research, 26(3):363?375.
Brent, Michael R. 1999. An efficient, probabilis-
tically sound algorithm for segmentation and
word discovery. Machine Learning, 34:71?105.
Brent, Michael R and Xiaopeng Tao. 2001. Chi-
nese text segmentation with mbdp-1: Making
the most of training corpora. In 39th Annual
Meeting of the ACL, pages 82?89.
Chen, Stanley F and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98,
71
Center for Research in Computing Technology,
Harvard University.
Cole, Ronald and Jola Jakimik. 1980. A model of
speech perception, pages 136?163. Lawrence
Erlbaum Associates, Hillsdale, NJ.
Coleman, John and Janet Pierrehumbert. 1997.
Stochastic phonological grammars and accept-
ability. In Third Meeting of the ACL SIGPHON,
pages 49?56. ACL, Somerset, NJ.
Cutler, Anne and David Carter. 1987. The predom-
inance of strong initial syllables in the english
vocabulary. Computer Speech and Language,
2(3-4):133?142.
de Marcken, Carl. 1995. Acquiring a lexicon from
unsegmented speech. In 33rd Annual Meeting
of the ACL, pages 311?313.
Fleck, Margaret M. 2008. Lexicalized phonotactic
word segmentation. In 46th Annual Meeting of
the ACL, pages 130?138. ACL, Morristown, NJ.
Goldwater, Sharon. 2007. Nonparametric
Bayesian Models of Lexical Acquisition. Ph.D.
thesis, Brown University, Department of Cogni-
tive and Linguistic Sciences.
Harris, Zellig. 1954. Distributional structure.
Word, 10(2/3):146?62.
Hayes, Bruce and Colin Wilson. 2008. A maxi-
mum entropy model of phonotactics and phono-
tactic learning. Linguistic Inquiry.
Heinz, Jeffrey. 2007. Inductive Learning of Phono-
tactic Patterns. Ph.D. thesis, University of Cali-
fornia, Los Angeles, Department of Linguistics.
Johnson, Mark. 2008a. Unsupervised word seg-
mentation for sesotho using adaptor grammars.
In Tenth Meeting of ACL SIGMORPHON, pages
20?27. ACL, Morristown, NJ.
Johnson, Mark. 2008b. Using adaptor grammars
to identify synergies in the unsupervised acqui-
sition of linguistic structure. In 46th Annual
Meeting of the ACL, pages 398?406. ACL, Mor-
ristown, NJ.
Jurafsky, Daniel and James Martin. 2000. Speech
and Language Processing. Prentice-Hall.
MacWhinney, Brian and Catherine Snow. 1985.
The child language data exchange system. Jour-
nal of child language, 12(2):271?95.
Manning, Christopher and Hinrich Sch?utze. 1999.
Foundations of Statistical Natural Language
Processing. MIT Press.
Mattys, Sven and Peter Jusczyk. 2001. Phonotac-
tic cues for segmentation of fluent speech by in-
fants. Cognition, 78:91?121.
Mattys, Sven, Peter Jusczyk, Paul Luce, and James
Morgan. 1999. Phonotactic and prosodic effects
on word segmentation in infants. Cognitive Psy-
chology, 38:465?494.
Olivier, Donald. 1968. Stochastic Grammars and
Language Acquisition Mechanisms. Ph.D. the-
sis, Harvard Univerity.
Scholes, Robert. 1966. Phonotactic Grammatical-
ity. Mouton, The Hague.
Teahan, W. J., Rodger McNab, Yingying Wen, and
Ian H. Witten. 2000. A compression-based al-
gorithm for chinese word segmentation. Com-
putational Linguistics, 26(3):375?393.
Venkataraman, Anand. 2001. A statistical model
for word discovery in transcribed speech. Com-
putational Linguistics, 27(3):352?372.
72
Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 28?37,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Maximum Likelihood Estimation of Feature-based Distributions
Jeffrey Heinz and Cesar Koirala
University of Delaware
Newark, Delaware, USA
{heinz,koirala}@udel.edu
Abstract
Motivated by recent work in phonotac-
tic learning (Hayes and Wilson 2008, Al-
bright 2009), this paper shows how to de-
fine feature-based probability distributions
whose parameters can be provably effi-
ciently estimated. The main idea is that
these distributions are defined as a prod-
uct of simpler distributions (cf. Ghahra-
mani and Jordan 1997). One advantage
of this framework is it draws attention to
what is minimally necessary to describe
and learn phonological feature interactions
in phonotactic patterns. The ?bottom-up?
approach adopted here is contrasted with
the ?top-down? approach in Hayes and
Wilson (2008), and it is argued that the
bottom-up approach is more analytically
transparent.
1 Introduction
The hypothesis that the atomic units of phonology
are phonological features, and not segments, is one
of the tenets of modern phonology (Jakobson et
al., 1952; Chomsky and Halle, 1968). Accord-
ing to this hypothesis, segments are essentially
epiphenomenal and exist only by virtue of being
a shorthand description of a collection of more
primitive units?the features. Incorporating this
hypothesis into phonological learning models has
been the focus of much influential work (Gildea
and Jurafsky, 1996; Wilson, 2006; Hayes and Wil-
son, 2008; Moreton, 2008; Albright, 2009).
This paper makes three contributions. The first
contribution is a framework within which:
1. researchers can choose which statistical in-
dependence assumptions to make regarding
phonological features;
2. feature systems can be fully integrated into
strictly local (McNaughton and Papert, 1971)
(i.e. n-gram models (Jurafsky and Martin,
2008)) and strictly piecewise models (Rogers
et al, 2009; Heinz and Rogers, 2010) in
order to define families of provably well-
formed, feature-based probability distribu-
tions that are provably efficiently estimable.
The main idea is to define a family of distribu-
tions as the normalized product of simpler distri-
butions. Each simpler distribution can be repre-
sented by a Probabilistic Deterministic Finite Ac-
ceptor (PDFA), and the product of these PDFAs
defines the actual distribution. When a family of
distributions F is defined in this way, F may have
many fewer parameters than if F is defined over
the product PDFA directly. This is because the pa-
rameters of the distributions are defined in terms
of the factors which combine in predictable ways
via the product. Fewer parameters means accurate
estimation occurs with less data and, relatedly, the
family contains fewer distributions.
This idea is not new. It is explicit in Facto-
rial Hidden Markov Models (FHMMs) (Ghahra-
mani and Jordan, 1997; Saul and Jordan, 1999),
and more recently underlies approaches to de-
scribing and inferring regular string transductions
(Dreyer et al, 2008; Dreyer and Eisner, 2009).
Although HMMs and probabilistic finite-state au-
tomata describe the same class of distributions
(Vidal et al, 2005a; Vidal et al, 2005b), this paper
presents these ideas in formal language-theoretic
and automata-theoretic terms because (1) there are
no hidden states and is thus simpler than FHMMs,
(2) determinstic automata have several desirable
properties crucially used here, and (3) PDFAs
add probabilities to structure whereas HMMs add
structure to probabilities and the authors are more
comfortable with the former perspective (for fur-
ther discussion, see Vidal et al (2005a,b)).
The second contribution illustrates the main
idea with a feature-based bigram model with a
28
strong statistical independence assumption: no
two features interact. This is shown to capture ex-
actly the intuition that sounds with like features
have like distributions. Also, the assumption of
non-interacting features is shown to be too strong
because like sounds do not have like distributions
in actual phonotactic patterns. Four kinds of fea-
tural interactions are identified and possible solu-
tions are discussed.
Finally, we compare this proposal with Hayes
and Wilson (2008). Essentially, the model here
represents a ?bottom-up? approach whereas theirs
is ?top-down.? ?Top-down? models, which con-
sider every set of features as potentially interact-
ing in every allowable context, face the difficult
problem of searching a vast space and often re-
sort to heuristic-based methods, which are diffi-
cult to analyze. To illustrate, we suggest that the
role played by phonological features in the phono-
tactic learner in Hayes and Wilson (2008) is not
well-understood. We demonstrate that classes of
all segments but one (i.e. the complement classes
of single segments) play a significant role, which
diminishes the contribution provided by natural
classes themselves (i.e. ones made by phonologi-
cal features). In contrast, the proposed model here
is analytically transparent.
This paper is organized as follows. ?2 reviews
some background. ?3 discusses bigram models
and ?4 defines feature systems and feature-based
distributions. ?5 develops a model with a strong
independence assumption and ?6 discusses feat-
ural interaction. ?7 dicusses Hayes and Wilson
(2008) and ?8 concludes.
2 Preliminaries
We start with mostly standard notation. P(A) is
the powerset of A. ? denotes a finite set of sym-
bols and a string over ? is a finite sequence of
these symbols. ?+ and ?? denote all strings over
this alphabet of nonzero but finite length, and of
any finite length, respectively. A function f with
domain A and codomain B is written f : A ? B.
When discussing partial functions, the notation ?
and ? indicate for particular arguments whether
the function is undefined and defined, respectively.
A language L is a subset of ??. A stochastic
language D is a probability distribution over ??.
The probability p of word w with respect to D is
written Pr
D
(w) = p. Recall that all distributions
D must satisfy
?
w??
?
Pr
D
(w) = 1. If L is lan-
guage then Pr
D
(L) =
?
w?L
Pr
D
(w). Since all
distributions in this paper are stochastic languages,
we use the two terms interchangeably.
A Probabilistic Deterministic Finite-
state Automaton (PDFA) is a tuple
M = ?Q,?, q
0
, ?, F, T ? where Q is the state
set, ? is the alphabet, q
0
is the start state, ? is
a deterministic transition function, F and T are
the final-state and transition probabilities. In
particular, T : Q ? ? ? R+ and F : Q ? R+
such that
for all q ? Q, F (q) +
?
???
T (q, ?) = 1. (1)
PDFAs are typically represented as labeled di-
rected graphs (e.g. M? in Figure 1).
A PDFA M generates a stochastic language
D
M
. If it exists, the (unique) path for a word w =
a
0
. . . a
k
belonging to ?? through a PDFA is a
sequence ?(q
0
, a
0
), (q
1
, a
1
), . . . , (q
k
, a
k
)?, where
q
i+1
= ?(q
i
, a
i
). The probability a PDFA assigns
tow is obtained by multiplying the transition prob-
abilities with the final probability along w?s path if
it exists, and zero otherwise.
Pr
D
M
(w) =
(
k
?
i=0
T (q
i
, a
i
)
)
?F (q
k+1
) (2)
if ?d(q
0
, w)? and 0 otherwise
A stochastic language is regular deterministic iff
there is a PDFA which generates it.
The structural components of a PDFAM is the
deterministic finite-state automata (DFA) given by
the states Q, alphabet ?, transitions ?, and initial
state q
0
of M. By the structure of a PDFA, we
mean its structural components.1 Each PDFA M
defines a family of distributions given by the pos-
sible instantiations of T and F satisfying Equa-
tion 1. These distributions have at most |Q|? (|?|+
1) parameters (since for each state there are |?|
possible transitions plus the possibility of finality.)
These are, for all q ? Q and ? ? ?, the proba-
bilities T (q, ?) and F (q). To make the connection
to probability theory, we sometimes write these as
Pr(? | q) and Pr(# | q), respectively.
We define the product of PDFAs in terms of
co-emission probabilities (Vidal et al, 2005a).
Let M
1
= ?Q
1
,?
1
, q
01
, ?
1
, F
1
, T
1
? and M
2
=
1This is up to the renaming of states so PDFA with iso-
morphic structural components are said to have the same
structure.
29
?Q
2
,?
2
, q
02
, ?
2
, F
2
, T
2
? be PDFAs. The proba-
bility that ?
1
is emitted from q
1
? Q
1
at the
same moment ?
2
is emitted from q
2
? Q
2
is
CT (?
1
, ?
2
, q
1
, q
2
) = T
1
(q
1
, ?
1
)?T
2
(q
2
, ?
2
). Sim-
ilarly, the probability that a word simultaneously
ends at q
1
? Q
1
and at q
2
? Q
2
is CF (q
1
, q
2
) =
F
1
(q
1
)?F
2
(q
2
).
Definition 1 The normalized co-emission product
of PDFAs M
1
and M
2
is M = M
1
? M
2
=
?Q,?, q
0
, ?, F, T ? where
1. Q, q
0
, and F are defined in terms of the
standard DFA product over the state space
Q
1
?Q
2
(Hopcroft et al, 2001).
2. ? = ?
1
? ?
2
3. For all ?q
1
, q
2
? ? Q and ??
1
, ?
2
? ?
?, ?(?q
1
, q
2
?, ??
1
, ?
2
?) = ?q
?
1
, q
?
2
? iff
?
1
(q
1
, ?
1
) = q
?
1
and ?
2
(q
2
, ?
2
) = q
?
2
.2
4. For all ?q
1
, q
2
? ? Q,
(a) let Z(?q
1
, q
2
?) = CF (?q
1
, q
2
?) +
?
??
1
,?
2
???
CT (?
1
, ?
2
, q
1
, q
2
) be the
normalization term; and
(b) F (?q
1
, q
2
?) =
CF (q
1
,q
2
)
Z
; and
(c) for all ??
1
, ?
2
? ? ?,
T (?q
1
, q
2
?, ??
1
, ?
2
?) =
CT (??
1
,?
2
,q
1
,q
2
?)
Z
In other words, the numerators of T and F are
defined to be the co-emission probabilities, and
division by Z ensures that M defines a well-
formed probability distribution.3 The normalized
co-emission product effectively adopts a statisti-
cal independence assumption between the states
of M
1
and M
2
. If S is a list of PDFAs, we write
?
S for their product (note order of product is ir-
relevant up to renaming of the states).
The maximum likelihood (ML) estimation of
regular deterministic distributions is a solved
problem when the structure of the PDFA is known
(Vidal et al, 2005a; Vidal et al, 2005b; de la
Higuera, 2010). Let S be a finite sample of words
drawn from a regular deterministic distribution D.
The problem is to estimate parameters T and F of
2Note that restricting ? to cases when ?
1
= ?
2
obtains
the standard definition of ? = ?
1
? ?
2
(Hopcroft et al, 2001).
The reason we maintain two alphabets becomes clear in ?4.
3
Z(?q
1
, q
2
?) is less than one whenever either F
1
(q
1
) or
F
2
(q
2
) are neither zero nor one.
M so that D
M
approaches D using the widely-
adopted ML criterion (Equation 3).
(
?
T ,
?
F ) = argmax
T,F
(
?
w?S
Pr
M
(w)
)
(3)
It is well-known that if D is generated by some
PDFAM? with the same structural components as
M, then the ML estimate of S with respect to M
guarantees that D
M
approaches D as the size of
S goes to infinity (Vidal et al, 2005a; Vidal et al,
2005b; de la Higuera, 2010).
Finding the ML estimate of a finite sample S
with respect to M is simple provided M is de-
terministic with known structural components. In-
formally, the corpus is passed through the PDFA,
and the paths of each word through the corpus are
tracked to obtain counts, which are then normal-
ized by state. Let M = ?Q,?, ?, q
0
, F, T ? be the
PDFA whose parameters F and T are to be esti-
mated. For all states q ? Q and symbols ? ? ?,
The ML estimation of the probability of T (q, ?)
is obtained by dividing the number of times this
transition is used in parsing the sample S by the
number of times state q is encountered in the pars-
ing of S. Similarly, the ML estimation of F (q) is
obtained by calculating the relative frequency of
state q being final with state q being encountered
in the parsing of S. For both cases, the division is
normalizing; i.e. it guarantees that there is a well-
formed probability distribution at each state. Fig-
ure 1 illustrates the counts obtained for a machine
M with sample S = {abca}.4 Figure 1 shows
a DFA with counts and the PDFA obtained after
normalizing these counts.
3 Strictly local distributions
In formal language theory, strictly k-local lan-
guages occupy the bottom rung of a subregular
hierarchy which makes distinctions on the basis
of contiguous subsequences (McNaughton and Pa-
pert, 1971; Rogers and Pullum, to appear; Rogers
et al, 2009). They are also the categorical coun-
terpart to stochastic languages describable with n-
gram models (where n = k) (Garcia et al, 1990;
Jurafsky and Martin, 2008). Since stochastic lan-
guages are distributions, we refer to strictly k-
local stochastic languages as strictly k-local distri-
4Technically,M is neither a simple DFA or PDFA; rather,
it has been called a Frequency DFA. We do not formally de-
fine them here, see de la Higuera (2010).
30
A:1
a :2
b:1
c:1
A:1/5
a:2/5
b:1/5
c:1/5
M M
?
Figure 1: M shows the counts obtained by parsing
it with sample S = {abca}. M? shows the proba-
bilities obtained after normalizing those counts.
butions (SLD
k
). We illustrate with SLD
2
(bigram
models) for ease of exposition.
For an alphabet ?, SL
2
distributions have
(|?| + 1)
2 parameters. These are, for all ?, ? ?
? ? {#}, the probabilities Pr(? | ?). The proba-
bility of w = ?
1
. . . ?
n
is given in Equation 4.
Pr(w)
def
= Pr(?
1
| #)? Pr(?
2
| ?
1
)
? . . .? Pr(# | ?
n
)
(4)
PDFA representations of SL
2
distributions have
the following structure: Q = ? ? {#}, q
0
= #,
and for all q ? Q and ? ? ?, it is the case that
?(q, ?) = ?.
As an example, the DFA in Figure 2 provides
the structure of PDFAs which recognize SL
2
dis-
tributions with ? = {a, b, c}. Plainly, the param-
eters of the model are given by assigning proba-
bilities to each transition and to the ending at each
state. In fact, for all ? ? ? and ? ? ? ? {#},
Pr(? | ?) is T (?, ?) and Pr(# | ?) is F (?).
It follows that the probability of a particular path
through the model corresponds to Equation 4. The
structure of a SL
2
distribution for alphabet ? is
given byMSL2(?).
Additionally, given a finite sample S ? ??, the
ML estimate of S with respect to the family of
distributions describable with MSL2(?) is given
by counting the parse of S through MSL2(?) and
then normalizing as described in ?2. This is equiv-
alent to the procedure described in Jurafsky and
Martin (2008, chap. 4).
4 Feature-based distributions
This section first introduces feature systems. Then
it defines feature-based SL
2
distributions which
make the strong independence assumption that no
two features interact. It explains how to find
b
a
c
 b
a
 c
 b
a
c
 b
a
 c
#
a
b
c
Figure 2: MSL2({a, b, c}) represents the structure
of SL
2
distributions when ? = {a, b, c}.
F G
a + -
b + +
c - +
Table 1: An example of a feature system with ? =
{a, b, c} and two features F and G.
the ML estimate of samples with respect to such
distributions. This section closes by identifying
kinds of featural interactions in phonotactic pat-
terns, and discusses how such interactions can be
addressed within this framework.
4.1 Feature systems
Assume the elements of the alphabet share prop-
erties, called features. For concreteness, let each
feature be a total function F : ? ? V
F
, where
the codomain V
F
is a finite set of values. A fi-
nite vector of features F = ?F
1
, . . . , F
n
? is called
a feature system. Table 1 provides an example
of a feature system with F = ?F,G? and values
V
F
= V
G
= {+,?}.
We extend the domain of all features F ? F
to ?+, so that F (?
1
. . . ?
n
) = F (?
1
) . . . F (?
n
).
For example, using the feature system in Table 1,
F (abc) = + + ? and G(abc) = ? + +. We
also extend the domain of F to all languages:
F (L) = ?
w?L
f(w). We also extend the notation
so that F(?) = ?F
1
(?), . . . , F
n
(?)?. For example,
F(c) = ??F,+G? (feature indices are included
for readability).
For feature F : ? ? V
F
, let F?1 be the inverse
function with domain V
F
and codomain P(?).
For example in Table 1, G?1(+) = {b, c}. F?1
is similarly defined, i.e. F?1(??F,+G?) = {c}.
31
If, for all arguments ~v, F?1(~v) is nonempty then
the feature system is exhaustive. If, for all argu-
ments ~v such that F?1(~v) is nonempty, it is the
case that |F?1(~v)| = 1 then the feature system is
distinctive. E.g. the feature system in Table 1 in
not exhaustive since F?1(??F,?G?) = ?, but it is
distinctive since where F?1 is nonempty, it picks
out exactly one element of the alphabet.
Generally, phonological feature systems for a
particular language are distinctive but not exhaus-
tive. Any feature system F can be made exhaustive
by adding finitely many symbols to the alphabet
(since F is finite). Let ?? denote an alphabet ob-
tained by adding to ? the fewest symbols which
make F exhaustive.
Each feature system also defines a set of indi-
cator functions VF =
?
f?F
(V
f
? {f}) with do-
main ? such that ?v, f?(?) = 1 iff f(?) = v and
0 otherwise. In the example in Table 1, VF =
{+F,?F,+G,?G} (omitting angle braces for
readability). For all f ? F, the set VF
f
is the
VF restricted to f . So continuing our example,
VF
F
= {+F,?F}.
4.2 Feature-based distributions
We now define feature-based SL
2
distributions un-
der the strong independence assumption that no
two features interact. For feature system F =
?F
1
. . . F
n
?, there are n PDFAs, one for each fea-
ture. The normalized co-emission product of these
PDFAs essentially defines the distribution. For
each F
i
, the structure of its PDFA is given by
MSL2(VF
i
). For example, M
F
= MSL2(VF )
andM
G
= MSL2(VG) in figures 3 and 4 illustrate
the finite-state representation of feature-based SL
2
distributions given the feature system in Table 1.5
The states of each machine make distinctions ac-
cording to features F and G, respectively. The pa-
rameters of these distributions are given by assign-
ing probabilities to each transition and to the end-
ing at each state (except for Pr(# | #)).6
Thus there are 2|VF| +
?
F?F
|VF
F
|
2
+ 1 pa-
rameters for feature-based SL
2
distributions. For
example, the feature system in Table 1 defines a
distribution with 2? 4 + 22 + 22 + 1 = 17 param-
5For readability, featural information in the states and
transitions is included in these figures. By definition, the
states and transitions are only labeled with elements of V
F
and V
G
, respectively. In this case, that makes the structures
of the two machines identical.
6It is possible to replace Pr(# | #) with two parameters,
Pr(# | #
F
) Pr(# | #
G
), but for ease of exposition we do
not pursue this further.
-F
-F
+F
+F
-F
+F
-F
+F
#
Figure 3: M
F
represents a SL
2
distribution with
respect to feature F.
-G
-G
+G
+G
-G
+G
-G
+G
#
Figure 4: M
G
represents a SL
2
distribution with
respect to feature G.
eters, which include Pr(# | +F ), Pr(+F | #),
Pr(+F | +F ), Pr(+F | ?F ), . . . , the G equiva-
lents, and Pr(# | #). Let SLD2
F
be the family of
distributions given by all possible parameter set-
tings (i.e. all possible probability assignments for
eachMSL2(VF
i
) in accordance with Equation 1.)
The normalized co-emission product defines the
feature-based distribution. For example, the struc-
ture of the product of M
F
and M
G
is shown in
Figure 5.
As defined, the normalized co-emission product
can result in states and transitions that cannot be
interpreted by non-exhaustive feature systems. An
example of this is in Figure 5 since ??F,?G? is
not interpretable by the feature system in Table 1.
We make the system exhaustive by letting ?? =
? ? {d} and setting F(d) = ??F,?G?.
What is the probability of a given b in the
feature-based model? According to the normal-
ized co-emission product (Defintion 1), it is
Pr(a | b) = Pr(?+F,?G? | ?+F,+G?) =
Pr(+F | +F )?Pr(?G | +G)
Z
where Z = Z(?+F,+G?) equals
?
???
?
Pr(F (?) | +F )?Pr(G(?) | +G)
+ (Pr(# | +F )?Pr(# | +G)
Generally, for an exhuastive distinctive feature
system F = ?F
1
, . . . , F
n
?, and for all ?, ? ? ?,
32
#+F,-G
+F,-G
+F ,+G
+F ,+G
-F,+G
-F,+G
-F,-G
-F,-G
+F,-G
+F ,+G
-F,+G
-F,-G
+F,-G
+F ,+G
-F,+G
-F,-G
+F,-G
+F ,+G
-F,+G
-F,-G
+F,-G
+F ,+G
-F,+G-F,-G
Figure 5: The structure of the product ofM
F
andM
G
.
the Pr(? | ?) is given by Equation 5. First, the
normalization term is provided. Let
Z(?) =
?
???
?
?
?
1?i?n
Pr(F
i
(?) | F
i
(?))
?
?
+
?
1?i?n
Pr(# | F
i
(?))
Then
Pr(? | ?) =
?
1?i?n
Pr(F
i
(?) | F
i
(?))
Z(?)
(5)
The probabilities Pr(? | #) and Pr(# | ?)
are similarly decomposed into featural parameters.
Finally, like SL
2
distributions, the probability of a
word w ? ?? is given by Equation 4. We have
thus proved the following.
Theorem 1 The parameters of a feature-based
SL
2
distribution define a well-formed probability
distribution over ??.
Proof It is sufficient to show for all ? ? ? ? {#}
that
?
????{#}
Pr(? | ?) = 1 since in this
case, Equation 4 yields a well-formed probability
distribution over ??. This follows directly from
the definition of the normalized co-emission
product (Definition 1). 
The normalized co-emission product adopts a
statistical independence assumption, which here is
between features since each machine represents a
single feature. For example, consider Pr(a | b) =
Pr(??F,+G? | ?+F,+G?). The probability
Pr(??F,+G? | ?+F,+G?) cannot be arbitrar-
ily different from the probabilities Pr(?F | +F )
and Pr(+G | +G); it is not an independent pa-
rameter. In fact, because Pr(a | b) is computed
directly as the normalized product of parameters
Pr(?F | +F ) and Pr(+G | +G), the assump-
tion is that the features F and G do not interact. In
other words, this model describes exactly the state
of affairs one expects if there is no statistical in-
teraction between phonological features. In terms
of inference, this means if one sound is observed
to occur in some context (at least contexts dis-
tinguishable by SL
2
models), then similar sounds
(i.e. those that share many of its featural values)
are expected to occur in this context as well.
4.3 ML estimation
The ML estimate of feature-based SL
2
distribu-
tions is obtained by counting the parse of a sample
through each feature machine, and normalizing the
results. This is because the parameters of the dis-
tribution are the probabilities on the feature ma-
chines, whose product determines the actual dis-
tribution. The following theorem follows imme-
diately from the PDFA representation of feature-
based SL
2
distributions.
Theorem 2 Let F = ?F
1
, . . . F
n
? and let D be de-
scribed by M =
?
1?i?n
MSL2(VF i). Consider
a finite sample S drawn from D. Then the ML es-
timate of S with respect to SLD2
F
is obtained by
finding, for each F
i
? F, the ML estimate of F
i
(S)
with respect toMSL2(VF i).
Proof The ML estimate of S with respect to
SLD2
F
returns the parameter values that maxi-
mize the likelihood of S within the family SLD2
F
.
The parameters of D ?SLD2
F
are found on the
33
states of each MSL2(VF
i
). By definition, each
MSL2(VF
i
) describes a probability distribution
over F
i
(?
?
), as well as a family of distributions.
Therefore finding the MLE of S with respect to
SLD2
F
means finding the MLE estimate of F
i
(S)
with respect to eachMSL2(VF
i
).
Optimizing the ML estimate of F
i
(S) for
each M
i
= MSL2(VF
i
) means that as |F
i
(S)|
increases, the estimates ?T
M
i
and ?F
M
i
approach
the true values T
M
i
and F
M
i
. It follows that
as |S| increases, ?T
M
and ?F
M
approach the true
values of T
M
and F
M
and consequently D
M
approaches D. 
4.4 Discussion
Feature-based models can have significantly fewer
parameters than segment-based models. Con-
sider binary feature systems, where |VF| = 2|F|.
An exhaustive feature system with 10 binary fea-
tures describes an alphabet with 1024 symbols.
Segment-based bigram models have (1024+1)2 =
1, 050, 625 parameters, but the feature-based one
only has 40 + 40 + 1 = 81 parameters! Con-
sequently, much less training data is required to
accurately estimate the parameters of the model.
Another way of describing this is in terms of ex-
pressivity. For given feature system, feature-based
SL
2
distributions are a proper subset of SL
2
dis-
tributions since, as the the PDFA representations
make clear, every feature-based distribution can be
described by a segmental bigram model, but not
vice versa. The fact that feature-based distribu-
tions have potentially far fewer parameters is a re-
flection of the restrictive nature of the model. The
statistical independence assumption constrains the
system in predictable ways. The next section
shows exactly what feature-based generalization
looks like under these assumptions.
5 Examples
This section demonstrates feature-based gener-
alization by comparing it with segment-based
generalization, using a small corpus S =
{aaab, caca, acab, cbb} and the feature system
in Table 1. Tables 2 and 3 show the results of
ML estimation of S with respect to segment-based
SL
2
distributions (unsmoothed bigram model)
and feature-based SL
2
distributions, respectively.
Each table shows the Pr(? | ?) for all ?, ? ?
{a, b, c, d,#} (where F(d) = ??F,?G?), for
?
P(? | ? )
a b c d #
a 0.29 0.29 0.29 0. 0.14
b 0. 0.25 0. 0. 0.75
? c 0.75 0.25 0. 0. 0.
d 0. 0. 0. 0. 0.
# 0.5 0. 0.5 0. 0.
Table 2: ML estimates of parameters of segment-
based SL
2
distributions.
?
P(? | ? )
a b c d #
a 0.22 0.43 0.17 0.09 0.09
b 0.32 0.21 0.09 0.13 0.26
? c 0.60 0.40 0. 0 0.
d 0.33 0.67 0 0 0
# 0.25 0.25 0.25 0.25 0.
Table 3: ML estimates of parameters of feature-
based SL
2
distributions.
ease of comparison.
Observe the sharp divergence between the two
models in certain cells. For example, no words be-
gin with b in the sample. Hence the segment-based
ML estimates of Pr(b | #) is zero. Conversely,
the feature-based ML estimate is nonzero because
b, like a, is +F, and b, like c, is +G, and both a
and c begin words. Also, notice nonzero probabil-
ities are assigned to d occuring after a and b. This
is because F(d) = ??F,?G? and the following
sequences all occur in the corpus: [+F][-F] (ac),
[+G][-G] (ca), and [-G][-G] (aa). On the other
hand, zero probabilities are assigned to d ocurring
after c and d because there are no cc sequences in
the corpus and hence the probability of [-F] occur-
ing after [-F] is zero.
This simple example demonstrates exactly how
the model works. Generalizations are made on the
basis of individual features, not individual sym-
bols. In fact, segments are truly epiphenomenal in
this model, as demonstrated by the nonzero prob-
abilties assigned to segments outside the original
feature system (here, this is d). To sum up, this
model captures exactly the idea that the distribu-
tion of segments is conditioned on the distribu-
tions of its features.
34
6 Featural interaction
In many empirical cases of interest, features do
interact, which suggests the strong independence
assumption is incorrect for modeling phonotactic
learning.
There are at least four kinds of featural inter-
action. First, different features may be prohib-
ited from occuring simultaneously in certain con-
texts. As an example of the first type consider
the fact that both velars and nasal sounds occur
word-initially in English, but the velar nasal may
not. Second, specific languages may prohibit dif-
ferent features from simultaneously occuring in all
contexts. In English, for example, there are syl-
labic sounds and obstruents but no syllabic obstru-
ents. Third, different features may be universally
incompatible: e.g. no vowels are both [+high] and
[+low]. The last type of interaction is that different
features may be prohibited from occuring syntag-
matically. For example, some languages prohibit
voiceless sounds from occuring after nasals.
Although the independence assumption is too
strong, it is still useful. First, it allows researchers
to quantify the extent to which data can be ex-
plained without invoking featural interaction. For
example, following Hayes and Wilson (2008), we
may be interested in how well human acceptabil-
ity judgements collected by Scholes (1966) can be
explained if different features do not interact. Af-
ter training the feature-based SL
2
model on a cor-
pus of word initial onsets adapted from the CMU
pronouncing dictionary (Hayes and Wilson, 2008,
395-396) and using a standard phonological fea-
ture system (Hayes, 2009, chap. 4), it achieves
a correlation (Spearman?s r) of 0.751.7 In other
words, roughly three quarters of the acceptability
judgements are explained without relying on feat-
ural interaction (or segments).
Secondly, the incorrect predictions of the model
are in principle detectable. For example, recall
that English has word-inital velars and nasals, but
no word-inital velar nasals. A one-cell chi-squared
test can determine whether the observed number
of [#N] is significantly below the expected number
according to the feature-based distribution, which
could lead to a new parameter being adopted to
describe the interaction of the [dorsal] and [nasal]
7We use the feature chart in Hayes (2009) because it con-
tains over 150 IPA symbols (and not just English phonemes).
Featural combinations not in the chart were assumed to be
impossible (e.g. [+high,+low]) and were zeroed out.
features word-initially. The details of these proce-
dures are left for future research and are likely to
draw from the rich literature on Bayesian networks
(Pearl, 1989; Ghahramani, 1998).
More important, however, is this framework al-
lows researchers to construct the independence as-
sumptions they want into the model in at least two
ways. First, universally incompatible features can
be excluded. For example, suppose [-F] and [-G]
in the feature system in Table 1 are anatomically
incompatible like [+low] and [+high]. If desired,
they can be excluded from the model essentially
by zeroing out any probability mass assigned to
such combinations and re-normalizing.
Second, models can be defined where multiple
features are permitted to interact. For example,
suppose features F and G from Table 1 are em-
bedded in a larger feature system. The machine
in Figure 5 can be defined to be a factor of the
model, and now interactions between F and G will
be learned, including syntagmatic ones. The flex-
ibility of the framework and the generality of the
normalized co-emission product allow researchers
to consider feature-based distributions which al-
low any two features to interact but which pro-
hibit three-feature interactions, or which allow any
three features to interact but which prohibit four-
feature interactions, or models where only certain
features are permitted to interact but not others
(perhaps because they belong to the same node in a
feature geometry (Clements, 1985; Clements and
Hume, 1995).8
7 Hayes and Wilson (2008)
This section introduces the Hayes and Wilson
(2008) (henceforth HW) phonotactic learner and
shows that the contribution features play in gener-
alization is not as clear as previously thought.
HW propose an inductive model which ac-
quires a maxent grammar defined by weighted
constraints. Each constraint is described as a se-
quence of natural classes using phonological fea-
tures. The constraint format also allows reference
to word boundaries and at most one complement
class. (The complement class of S ? ? is ?/S.)
For example, the constraint
*#[? -voice,+anterior,+strident][-approximant]
means that in word-initial C
1
C
2
clusters, if C
2
is a
nasal or obstruent, then C
1
must be [s].
8Note if all features are permitted to interact, this yields
the segmental bigram model.
35
Hayes and Wilson maxent models r
features & complement classes 0.946
no features & complement classes 0.937
features & no complement classes 0.914
no features & no complement classes 0.885
Table 4: Correlations of different settings versions
of HW maxent model with Scholes data.
HW report that the model obtains a correlation
(Spearman?s r) of 0.946 with blick test data from
Scholes (1966). HW and Albright (2009) attribute
this high correlation to the model?s use of natural
classes and phonological features. HW also report
that when the model is run without features, the
grammar obtained scores an r value of only 0.885,
implying that the gain in correlation is due specif-
ically to the use of phonological features.
However, there are two relevant issues. The first
is the use of complement classes. If features are
not used but complement classes are (in effect only
allowing the model to refer to single segments and
the complements of single segments, e.g. [t] and
[?t]) then in fact the grammar obtained scores an
r value of 0.936, a result comparable to the one
reported.9 Table 4 shows the r values obtained by
the HW learner under different conditions. Note
we replicate the main result of r = 0.946 when
using both features and complement classes.10
This exercise reveals that phonological features
play a smaller role in the HW phonotactic learner
than previously thought. Features are helpful, but
not as much as complement classes of single seg-
ments (though features with complement classes
yields the best result by this measure).
The second issue relates to the first: the question
of whether additional parameters are worth the
gain in empirical coverage. Wilson and Obdeyn
(2009) provide an excellent discussion of the
model comparison literature and provide a rigor-
ous comparative analysis of computational mod-
eleling of OCP restrictions. Here we only raise the
questions and leave the answers to future research.
Compare the HW learners in the first two rows
in Table 4. Is the ? 0.01 gain in r score worth
the additional parameters which refer to phono-
9Examination of the output grammar reveals heavy re-
liance on the complement class [?s], which is not surprising
given the discussion of [sC] clusters in HW.
10This software is available on Bruce Hayes? webpage:
http://www.linguistics.ucla.edu/
people/hayes/Phonotactics/index.htm.
logically natural classes? Also, the feature-based
SL
2
model in ?4 only receives an r score of 0.751,
much lower than the results in Table 4. Yet this
model has far fewer parameters not only because
the maxent models in Table 4 keep track of tri-
grams, but also because of its strong independence
assumption. As mentioned, this result is infor-
mative because it reveals how much can be ex-
plained without featural interaction. In the con-
text of model comparison, this particular model
provides an inductive baseline against which the
utility of additional parameters invoking featural
interaction ought to be measured.
8 Conclusion
The current proposal explicitly embeds the Jakob-
sonian hypothesis that the primitive unit of
phonology is the phonological feature into a
phonotactic learning model. While this paper
specifically shows how to integrate features into
n-gram models to describe feature-based strictly
n-local distributions, these techniques can be ap-
plied to other regular deterministic distributions,
such as strictly k-piecewise models, which de-
scribe long-distance dependencies, like the ones
found in consonant and vowel harmony (Heinz, to
appear; Heinz and Rogers, 2010).
In contrast to models which assume that all
features potentially interact, a baseline model
was specifically introduced under the assumption
that no two features interact. In this way, the
?bottom-up? approach to feature-based general-
ization shifts the focus of inquiry to the featural
interactions necessary (and ultimately sufficient)
to describe and learn phonotactic patterns. The
framework introduced here shows how researchers
can study feature interaction in phonotactic mod-
els in a systematic, transparent way.
Acknowledgments
We thank Bill Idsardi, Tim O?Neill, Jim Rogers,
Robert Wilder, Colin Wilson and the U. of
Delaware?s phonology/phonetics group for valu-
able discussion. Special thanks to Mark Ellison
for helpful comments, to Adam Albright for illu-
minating remarks on the types of featural interac-
tion in phonotactic patterns, and to Jason Eisner
for bringing to our attention FHMMs and other re-
lated work.
36
References
Adam Albright. 2009. Feature-based generalisation
as a source of gradient acceptability. Phonology,
26(1):9?41.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper & Row, New York.
G.N. Clements and Elizabeth V. Hume. 1995. The
internal organization of speech sounds. In John A.
Goldsmith, editor, The handbook of phonological
theory, chapter 7. Blackwell, Cambridge, MA.
George N. Clements. 1985. The geometry of phono-
logical features. Phonology Yearbook, 2:225?252.
Colin de la Higuera. 2010. Grammatical Inference:
Learning Automata and Grammars. Cambridge
University Press.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 101?110, Singa-
pore, August.
Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with finite-state methods. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1080?1089,
Honolulu, October.
Pedro Garcia, Enrique Vidal, and Jose? Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325?338.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2):245?273.
Zoubin Ghahramani. 1998. Learning dynamic
bayesian networks. In Adaptive Processing of
Sequences and Data Structures, pages 168?197.
Springer-Verlag.
Daniel Gildea and Daniel Jurafsky. 1996. Learn-
ing bias and phonological-rule induction. Compu-
tational Linguistics, 24(4).
Bruce Hayes and ColinWilson. 2008. Amaximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379?440.
Bruce Hayes. 2009. Introductory Phonology. Wiley-
Blackwell.
Jeffrey Heinz and James Rogers. 2010. Estimating
strictly piecewise distributions. In Proceedings of
the 48th AnnualMeeting of the Association for Com-
putational Linguistics, Uppsala, Sweden.
Jeffrey Heinz. to appear. Learning long-distance
phonotactics. Linguistic Inquiry, 41(4).
John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.
2001. Introduction to Automata Theory, Languages,
and Computation. Boston, MA: Addison-Wesley.
Roman Jakobson, C. Gunnar, M. Fant, and Morris
Halle. 1952. Preliminaries to Speech Analysis.
MIT Press.
Daniel Jurafsky and James Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. Prentice-Hall, Upper
Saddle River, NJ, 2nd edition.
Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.
Elliot Moreton. 2008. Analytic bias and phonological
typology. Phonology, 25(1):83?127.
Judea Pearl. 1989. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kauffman.
James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation.
James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Visscher, David Wellcome, and Sean
Wibel. 2009. On languages piecewise testable in
the strict sense. In Proceedings of the 11th Meeting
of the Assocation for Mathematics of Language.
Lawrence K. Saul and Michael I. Jordan. 1999. Mixed
memory markov models: Decomposing complex
stochastic processes as mixtures of simpler ones.
Machine Learning, 37(1):75?87.
Robert J. Scholes. 1966. Phonotactic grammaticality.
Mouton, The Hague.
Enrique Vidal, Franck Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005a. Probabilistic finite-state machines-part I.
IEEE Transactions on Pattern Analysis andMachine
Intelligence, 27(7):1013?1025.
Enrique Vidal, Frank Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005b. Probabilistic finite-state machines-part II.
IEEE Transactions on Pattern Analysis andMachine
Intelligence, 27(7):1026?1039.
Colin Wilson and Marieke Obdeyn. 2009. Simplifying
subsidiary theory: statistical evidence from arabic,
muna, shona, and wargamay. Johns Hopkins Uni-
versity.
Colin Wilson. 2006. Learning phonology with sub-
stantive bias: An experimental and computational
study of velar palatalization. Cognitive Science,
30(5):945?982.
37
Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 42?51,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Bounded copying is subsequential: Implications for metathesis and
reduplication?
Jane Chandlee
Linguistics and Cognitive Science
University of Delaware
Newark, DE
janemc@udel.edu
Jeffrey Heinz
Linguistics and Cognitive Science
University of Delaware
Newark, DE
heinz@udel.edu
Abstract
This paper first defines the conditions under
which copying and deletion processes are sub-
sequential: specifically this is the case when
the process is bounded in the right ways.
Then, if we analyze metathesis as the compo-
sition of copying and deletion, it can be shown
that the set of attested metathesis patterns fall
into the subsequential or reverse subsequential
classes. The implications of bounded copying
are extended to partial reduplication, which is
also shown to be either subsequential or re-
verse subsequential.
1 Introduction
This paper presents a computational analysis of
copying and deletion in metathesis and partial redu-
plication and establishes the necessary conditions
for such patterns to be subsequential. More specif-
ically, it is shown that such patterns fall into the
subsequential or reverse subsequential classes if the
copying (for both cases) and deletion (for the case of
metathesis only) are bounded in the right ways.
The classification of natural language patterns by
the Chomsky Hierarchy (Chomsky, 1956) is one
means of distinguishing the complexity of the pat-
terns found in various linguistic domains. Syn-
tactic patterns, for example, may be context-free
(e.g. English nested embedding, (Chomsky, 1956))
or context-sensitive (e.g. Swiss German crossing
?We thank the anonymous reviewers for useful questions
and suggestions.This research is supported by grant #1035577
from the National Science Foundation.
dependencies (Schieber, 1985)), while phonologi-
cal patterns (i.e. patterns that can be described
with rewrite rules of the form A ? B / C D,
where A, B, C, and D are regular expressions) have
been shown by Johnson (1972) and Kaplan and Kay
(1994) to be regular.
The regular class of patterns, however, is in fact
too large to correspond exactly to phonology (Heinz,
2007; Heinz, 2009; Heinz, 2010). Rather, it seems
that phonological patterns fit into a subclass of the
regular patterns. Since the subsequential class is
a proper subset of the regular class (Oncina et al,
1993; Mohri, 1997), it is therefore a useful candi-
date, especially because of its attractive computa-
tional properties (Mohri, 1997). Restricting the class
of phonological patterns in this way has implications
for learning, since the subsequential but not the reg-
ular class is identifiable in the limit from positive
data (Oncina et al, 1993).
Using the formalism of finite state transducers
(FSTs), we will show that metathesis and partial
reduplication patterns can be described with subse-
quential FSTs. Subsequential FSTs are determinis-
tic weighted transducers in which the weights are
strings and multiplication is concatenation.
The analysis defines generally the conditions nec-
essary for metathesis and partial reduplication to
be subsequential. Representative examples of the
empirical phenomena that can be so classified are
shown in (1). (1-a) is an example of local metathe-
sis, (1-b) is an example of metathesis around an in-
tervening segment, and (1-c) is an example of partial
reduplication.
42
(1) a. Rotuman:
hosa? hoas ?flower?
b. Cuzco Quechua:
yuraq? ruyaq ?white?
c. Tagalog:
sulat? susulat ?will write?
This kind of analysis sheds light on the nature
of the relation itself independent of the particular
theory used to account for it. It does not matter
whether the metathesized or reduplicated forms ex-
emplified here are derived via a series of SPE-style
rules or with ranked constraints in OT. The mapping
(e.g. <hosa, hoas> for Rotuman) remains the same
in either case. Additionally, the analysis has im-
plications for any model beyond the phonological
domain that uses finite-state methodology, includ-
ing (but not limited to) artificial intelligence (Rus-
sell and Norvig, 2009), bioinformatics (Durbin et
al., 1998), natural language processing (Jurafsky and
Martin, 2008), and robotics (Belta et al, 2007; Tan-
ner et al, 2012).
The structure of the paper is as follows. Section
two provides the formal definitions necessary for the
analysis. Section three presents an analysis of subse-
quential copying, and section four presents an anal-
ysis of subsequential deletion. Section five turns
to the analysis of metathesis as the composition of
copying and deletion and proves the conditions for
metathesis to be subsequential. Section six extends
this analysis to partial reduplication. Section seven
discusses the implications of the distinctions drawn
by the computational analysis for both typology and
learning. Section eight concludes.
2 Preliminaries
If ? is a fixed finite set of symbols (an alphabet),
then ?* is the set of all finite length strings formed
over this alphabet, and ??k is the set of all strings of
length less than or equal to k. A language is a subset
of ??.  is the empty string. The length of a string s
is |s|; thus || = 0. The prefixes of a string s, written
Pr(s), are {u ? ?? : ?v ? ?? such that s = uv}. The
suffixes of a string s, written Suf (s), are {u ? ?? :
?v ? ?? such that s = vu}. Sufn(s) is a suffix of s of
length n. The nonempty, proper prefixes of a string
s is written Prprop(s).
If L is a language then the prefixes of L are
Pr(L) =
?
s?L Pr(s) and the nonempty proper
prefixes of L are Prprop(L) =
?
s?L Prprop(s). A
language L is finite iff there exists some k such that
L ? ??k. For any w ? ??, the good tails of w
in L is TL(w) = {v ? ??|wv ? L}. Two pre-
fixes u1 and u2 are Nerode equivalent with respect
to some language L iff they share the same good
tails: u1 ?L u2 ? TL(u1) = TL(u2). In fact, a lan-
guage L is regular iff the partition induced over ??
by ?L has finite cardinality. Note that every finite
language is regular. If L1 and L2 are languages then
L1L2 = {uv | u ? L1 and v ? L2}.
Definition 1. (Oncina et al, 1993) A subsequential
finite state transducer (SFST) is a six-tuple (Q, ?,
?, q0, ?, ?), where Q is a finite set of states, ? is the
input alphabet, ? is the output alphabet, q0 ? Q is
the initial state, ? ? (Q ?????? Q) is the transi-
tion function, and ?: Q? ?? is a partial function
that assigns strings to the states in Q. The edges, E,
of the SFST are a finite subset of (Q??????? Q).
SFSTs are deterministic, meaning they are subject to
the condition (q,a,u,r),(q,a,v,s) ? E? (u=v ? r=s).
Definition 2. (Oncina et al, 1993) A path in
an SFST ? is a sequence of edges in ? , pi =
(q0, x1, y1, q1)(q1, x2, y2, q2) ? ? ? (qn?1, xn, yn, qn).
?? is the set of all possible paths over ? . A path
pi can also be expressed as (q0, x, y, qn) where x
= x1x2...xn and y = y1y2 ? ? ? yn. The transduction
? realizes is the function t: ?? ? ?? such that
?(q0,x,y,q)??? and t(x) = y?(q).
A relation describable with an SFST is a subse-
quential relation. If f and g are relations, ? denotes
the composition, where (g ? f )(x) = g(f (x)). Subse-
quential relations are closed under composition:
Theorem 1 ((Mohri, 1997) Theorem 1). Let f : ??
? ?? and g : ?? ? ?? be subsequential functions,
then g ? f is subsequential.
Let R be a relation. The reverse relation Rr =
{<xr,yr>: <x,y> ? R }. A relation is reverse sub-
sequential if its reverse relation is subsequential.
3 Subsequential copying
This paper will ultimately prove the conditions un-
der which metathesis and partial reduplication, two
processes which can be analyzed as involving copy-
ing, are subsequential relations. To do this it is
43
first necessary to define a copy relation in gen-
eral. A copy can be place either before or after the
original?these two processes can be distinguished
as pre-pivot or post-pivot copying (where the pivot
is an intervening string from the set U).
Definition 3. Let L,U,X,R be languages.
1. The rule ? ? X / LXU R is a post-pivot copy
relation.
2. The rule ? ? X / L UXR is a pre-pivot copy
relation.
Kaplan and Kay (1994) show that if L,X,U, and
R are regular languages, then the copy relations
above are regular relations. One goal of this paper is
to identify the conditions on L,X,U, and R which
make the above relations subsequential.
Further distinctions can be drawn among regu-
lar copy relations based on which of the surround-
ing contexts of the copy and original are of bounded
length.
Definition 4. Let L,X,U, and R be regular lan-
guages.
1. A pre-pivot or post-pivot copy relation is I-
bounded iff U is a finite language (I is for ?in-
tervening?).
2. A pre-pivot copy relation is L-bounded iff L is
a finite language (L is for ?left-context?).
3. A post-pivot copy relation is R-bounded iff R is
a finite language (R is for ?right-context?).
4. A copy relation is T-bounded iff X is a finite
language (T is for ?target?).
Theorem 2 below states that a pre-pivot, T-
bounded, I-bounded, and R-bounded copy relation
is subsequential. To understand the idea behind
the proof, consider the abstract pre-pivot relation
schematized in the following SFST for a particular
l ? L, u ? U , x ? X , and r ? R.1 Note that each
1Following (Beesley and Karttunen, 2003), in this and all
other FSTs in the paper, ??? represents any symbol or string
except for those for which other transitions out of that state are
defined. The states of the machine are labeled with the string
mapped to them by the ? function.
transition in the figure represents a series of tran-
sitions and states (depending on the lengths of the
strings involved).
?
? ?l? u
u:?
?:u?
ux
x:??:ux?r:xuxr
Figure 1: An SFST schematizing a pre-pivot T-bounded,
I-bounded, and R-bounded copy relation.
The transitions for which the output is the empty
string can be thought of as the machine withhold-
ing the output until it verifies that it has found the
context for copying. Thus, out of the state labelled
?ux? the output on ?r? is ?xuxr?, which is the copy fol-
lowed by the segments for which there was no output
(i.e. the segments in u and x that were being ?held?).
This mechanism of holding is why the bounds on the
lengths of the strings are necessary. If there were
no upper bound on the length of the words in U,X,
or R, the machine would have to hold a potentially
infinite number of strings, which would in turn re-
quire infinitely many states. Without these bounds,
no SFST can be constructed.
Theorem 2. A regular pre-pivot copy relation that
is T-bounded, I-bounded, and R-bounded is subse-
quential.
Proof. Let C be a pre-pivot, T-bounded, I-bounded,
and R-bounded regular copy relation. Then there ex-
ists a regular language L and finite languages U , X ,
and R such that C is described by the rewrite rule
? ? X / L UXR.
An SFST is constructed for C as follows. The
states Q are the set of good tails of L and the non-
empty proper prefixes of UXR. Formally, let piL =
{TL(w) | w ? Pr(L)}. Then
Q = (piL ? Prprop(UXR))
SinceL is a regular language, there are finitely many
elements of piL. Since U,X, and R are finite lan-
44
guages, Prprop(UXR) is also finite. Therefore Q is
finite.
The initial state q0 = TL().
The sigma function is defined as follows. ?q ? Q,
?(q) =
{
 iff q ? piL
q otherwise
The transition function is defined in two parts.
First, for all s ? Pr(L) and a ? ?:
(TL(s), a, a, TL(sa)) ? E iff s, sa ? Pr(L)
and s 6? L
(TL(s), a, , a) ? E iff s ? L and a ? Pr(UXR)
(TL(s), a, a, TL()) ? E otherwise
Second, for all s in the nonempty proper prefixes
of UXR and a ? ?:
(s, a, , sa) ? E iff s, sa ? Prprop(UXR)
(s, a, xuxra, TL()) ? E iff (?x ? X)
(?u ? U)(?r ? Prprop(R))[s = uxr, ra ? R]
(s, a, ?(s)a, TL()) ? E otherwise
It follows directly from this construction that the
SFST recognizes the specified copy relation.
Theorem 3 below states that a post-pivot copy re-
lation need only be T- and R-bounded to be subse-
quential. The idea behind the proof is demonstrated
by the abstract post-pivot copy relation schematized
in the following SFST for a particular x, u, and r.
?
? ?l? ?
u
?
?
:?xrux
Figure 2: An SFST for a post-pivot T-bounded and R-
bounded copy relation.
Since the machine finds the original x before it has
to produce the copy, the segments in u do not have to
be held. The bounding is only necessary for x itself,
and for the right context of the copy r. Thus when
the original precedes the copy, the bounding on u is
no longer a necessary condition for subsequentiality.
Theorem 3. A regular post-pivot copy relation that
is T-bounded and R-bounded is subsequential.
The proof of the Theorem 3 (omitted) is similar to
the one for Theorem 2 but slightly more complicated
by the fact that U can be any regular language.
The reverse of the relation in the proof of The-
orem 3 would be a pre-pivot copy relation that is
T-bounded and L-bounded. This would reverse the
pattern in Figure 2, except the left context and not
the right context would be bounded. Such a pattern
is not subsequential, but it is reverse subsequential.
Corollary 1. A regular pre-pivot copy relation that
is T-bounded and L-bounded is reverse subsequen-
tial.
4 Subsequential deletion
As with copying, the deletion relations relevant
for metathesis come in two flavors, depending on
whether the deleted string precedes or follows the
one that remains.
Definition 5. Let L,U,X,R be languages.
1. The rule X? ? / LXU R is a post-pivot dele-
tion relation.
2. The rule X? ? / L UXR is a pre-pivot dele-
tion relation.
Kaplan and Kay (1994) show that if L,X,U, and
R are regular languages, then the deletion relations
above are regular relations. Another goal of this pa-
per is to identify the conditions on L,X,U, and R
which make the relations above subsequential.
We can thus provide parallel definitions for T-
bounded, I-bounded, and R-bounded deletion rela-
tions.
Definition 6. Let L,X,U,R be regular languages.
1. A pre-pivot or post-pivot deletion relation is I-
bounded iff U is a finite language.
2. A pre-pivot deletion relation is L-bounded iff L
is a finite language.
3. A post-pivot deletion relation is R-bounded iff
R is a finite language.
4. A deletion relation is T-bounded iff X is a finite
language.
45
Figure 3 schematizes a pre-pivot regular dele-
tion relation that is T-bounded, I-bounded, and R-
bounded.
?
? ?l? u
u:?
?:u? uxx:??:ux?
uxuu:??:uxu?
r:xur
Figure 3: An SFST for a pre-pivot T-bounded, I-bounded,
and R-bounded deletion relation.
As with the copying, due to the need for -
transitions, the possibility of constructing this ma-
chine depends on the bounding of the length of the
deleted string, the intervening string, and the string
that makes up the right context.
Theorem 4. A regular pre-pivot deletion relation
that is T-bounded, I-bounded, and R-bounded is sub-
sequential.
Proof. Let D be a pre-pivot, T-bounded, I-bounded,
and R-bounded regular deletion relation. Then there
exists a regular language L and finite languages X ,
R, and U such that D is defined by the rewrite rule
X? ? / L UXR.
An SFST is constructed for D as follows. The
states Q are the set of good tails of L and the non-
empty proper prefixes of XUXR. Formally, let piL =
{TL(w) | w ? Pr(L)}. Then
Q = (piL ? Prprop(XUXR))
Since L is regular, piL is finite. Since U,X, and R
are finite languages, Prprop(XUXR) is also finite.
Therefore Q is finite.
The initial state q0 = TL(). The sigma function
is defined as in Theorem 2.
Also as in Theorem 2, the transition function is
defined in two parts. The first part, where all s ?
Pr(L) and a ? ? are considered, is the same as in
Theorem 2 except for one case below.
(TL(s), a, , a) ? E iff s ? L and
a ? Pr(XUXR)
The second part, where all s in the nonempty
proper prefixes of XUXR and a ? ? are considered,
is constructed as follows.
(s, a, , sa) ? E iff s, sa ? Prprop(XUXR)
(s, a, uxra, TL()) ? E iff(?x, x? ? X)
(?u ? U)(?r ? Prprop(R))[s = xux?r, ra ? R]
(s, a, ?(s)a, TL()) ? E otherwise
It follows directly from this construction that the
SFST recognizes the deletion relation D.
As for a post-pivot deletion relation, the proper-
ties of T-bounding and R-bounding are sufficient for
subsequentiality since the intervening set U occurs
before the deletion. As with Theorem 3, the proof of
Theorem 5 is omitted.
Theorem 5. A regular post-pivot deletion relation
that is T-bounded and R-bounded is subsequential.
Lastly, if u is not bounded in a pre-pivot deletion
relation, but l and x are, the relation is reverse sub-
sequential.
Corollary 2. A regular pre-pivot deletion relation
that is T-bounded and L-bounded is reverse subse-
quential.
5 Metathesis as the composition of copying
and deletion
Metathesis has traditionally been viewed as an op-
eration of transposition, in which segments switch
positions. Under another view, metathesis can be
considered as the result of two separate processes,
a copy process followed by deletion of the original
segment the copy was made from (Blevins and Gar-
rett, 1998; Blevins and Garrett, 2004). Take, for ex-
ample, the metathesis process in Najdi Arabic (Ab-
boud, 1979) in which a word with a CaCCat tem-
plate surfaces as CCaCat:
(2) /naQ?at/? [nQa?at] ?ewe?
An independent process of deletion (CaCaC ?
CCaC) is also observed in Arabic dialects. So the
change in (2) could be achieved via the two pro-
cesses in (3). The result after both processes cor-
responds to metathesis (4).
(3) a. Copy: CV1CC? CV1CV1C
b. Delete: CV1CV1C? CCV1C
46
(4) Metathesis: CV1CC? CCV1C
This analysis provides a way of classifying attested
patterns according to the type of copying and dele-
tion involved. Cross-linguistic surveys (Blevins and
Garrett, 1998; Blevins and Garrett, 2004; Hume,
2000; Buckley, 2011; Chandlee et al, to appear) re-
veal that in a large number of metathesis patterns
there is some bound on the length of the string that
intervenes between the copied segment and its orig-
inal. A classic example is found in the Rotuman
language, in which the incomplete form of a word
is derived from the complete form via word-final
consonant-vowel metathesis (Churchward, 1940).
The general rule for the example in (5-a) is in (5-b).
(5) a. hosa? hoas ?flower?
b. CV? VC / V #
If we decompose this metathesis into its compo-
nent copy and deletion operations, the copy portion
would be as in (6):2
(6) V1CV2#? V1V2CV2#
Applying Definitions 3 and 4 to this example, we
first classify the Rotuman pattern as pre-pivot copy-
ing. Since the length of the string between the orig-
inal segment and the copy (u = C) is bounded by
1, the copying is I-bounded. The copying is also T-
bounded, since a single vowel is copied (x = V2).
And it is R-bounded, since the original vowel is
word-final (r = ). An FST for this pattern is shown
in Figure 4. Note that when the right context is the
empty string, the copying is achieved via the ? func-
tion rather than a transition.
!
C !V
V
C
C:!
C:CC VCVV:!
C:CVC
V:CVV
Figure 4: An SFST for the copy process of Rotuman CV-
metathesis.
2This analysis assumes that it is the vowel that metathesizes.
The same surface form would obtain if the consonant metathe-
sized (C1V?C1VC1?VC1). For evidence that the vowel is in-
deed the segment involved in CV metathesis, see (Heinz, 2005).
Another example of I-bounded copying is in an
optional metathesis process in Cuzco Quechua, in
which sonorants metathesize across an intervening
vowel (Davidson, 1977):
(7) yuraq? ruyaq ?white?
Under a copy+deletion analysis of metathesis, this
pattern would involve two copy processes followed
by two deletions, one for the ?r? and one for the ?y?.
Schematizing just the process for ?r?, we can see in
(8) that the length of the intervening string is again
bounded by 1 (the ?y? is removed for clarity).3 The
copied string x (the liquid) is also bound by 1.
(8) uraq? ruraq
L-bounded copying is exemplified in a diachronic
metathesis pattern found in a South Italian dialect of
Greek (Rohlfs, 1950):
(9) Classical South Italian Greek
gambros? grambo4 ?son-in-law?
In this pattern, a non-initial liquid surfaces in the ini-
tial onset cluster. The original position of the liquid
varies, which means there is no bound on the length
of the intervening string. However, the location of
the copy is always the initial cluster, which means
the left context of the copy is bounded by the length
of the maximum onset.
The copying in metathesis is only the first step -
the second is deletion of the original. The deletion
rule must be complementary to the copying, in the
sense that if the copying is pre-pivot, the deletion
will be post-pivot (10), and vice versa (11).
(10) a. Copy: ? ? x / v uxw
b. Delete: x? ? / vxu w
(11) a. Copy: ? ? x / vxu w
b. Delete: x? ? / v uxw
As stated in section 2, if two relations are subse-
quential, then the composition of these relations will
also be subsequential. Thus, whether or not the
metathesis is subsequential depends on its compo-
nent copy and deletion processes. This result is es-
3This assumes the two copy-deletions occur simultaneously.
If one were to precede the other, then the bound would be 2.
4The [s] is deleted in an unrelated process.
47
tablished in the following theorem.
Theorem 6. If a copy relation f is either (1)
post-pivot, T-bounded, I-bounded and R-bounded
or (2) pre-pivot, T-bounded, and R-bounded, and
a deletion relation g is either (1) post-pivot, T-
bounded, I-bounded and R-bounded or (2) pre-pivot,
T-bounded, and R-bounded, then the metathesis re-
lation g ? f is subsequential.
Proof. By Theorems 2 and 3, f is subsequential. By
Theorems 4 and 5, g is subsequential. By Theorem
1, g ? f is subsequential.
6 Partial Reduplication
Unlike metathesis, reduplication is analyzed by pho-
nologists of all stripes as involving copying. Tradi-
tionally, two categories of reduplication have been
described: full (or total) and partial. Full reduplica-
tion involves copying the entire string and affixing
it to the original. A classic example is found in In-
donesian (Sneddon, 1996), to express the plural:
(12) a. buku ?book?
b. buku-buku ?books?
In contrast, partial reduplication involves copying a
designated portion of the string and affixing it as ei-
ther a prefix, suffix, or infix. These options can be
schematized as in (13)(Riggle, 2003), in which local
means the reduplicant attaches adjacent to the ma-
terial it copies and nonlocal means the reduplicant
copies a non-adjacent portion of the string:5
(13) a. local prefixation: CV-CVZ
b. nonlocal prefixation: CV-ZCV
c. local suffixation: ZCV-CV
d. nonlocal suffixation: CVZ-CV
e. infixation: C1VC1Z
An example of local prefixation is found in Tagalog,
in which the future of a verb is derived from the stem
with a CV reduplicative prefix (Blake, 1917):
(14) sulat? susulat ?will write?
An example of the infixation shown in (13-e) can
be found in Pima, in which the plural is derived by
5These schemas assume CV reduplication, but the analy-
sis would proceed the same for any reduplicant template, CC,
CVC, etc.
infixing a copy of the initial consonant after the first
vowel (Riggle, 2006):
(15) sipuk? sispuk ?cardinals?
Since under the current analysis of metathesis as
copy+delete, the only difference between a metathe-
sis pattern and partial reduplication is that the latter
does not involve the second process of deletion, we
should predict that partial reduplication patterns will
also be subsequential if the copying is bounded as
per the definitions given above. We can clearly see
that in the local patterns (13-a) and (13-c), the orig-
inal and the copy are adjacent and therefore (vac-
uously) they are I-bounded. For the infixation, the
copy likewise appears at a fixed distance from the
original. In all three cases, the amount of material
to be copied fits a template and is thus bounded by
the length of the template (i.e. they are T-bounded).
Therefore by Theorem 2, these partial reduplication
patterns are subsequential. The SFST for local pre-
fixation (13-a) is shown in Figure 5.
!
!C !
V
V:VCV
C
?
Figure 5: An SFST for CV-CVZ partial reduplication.
The SFST for local suffixation (13-c) is shown in
Figure 6.
!
V
!C
C CV
V
V C
Figure 6: An SFST for ZCV-CV partial reduplication.
And the SFST for infixation (13-e) is shown in Fig-
ure 7.
!
!C !
V
V:VC
C
?
Figure 7: An SFST for C1VC1Z partial reduplication.
As for the nonlocal patterns, first consider the case
of suffixation, (13-d). The string represented by Z
48
is not bounded, and therefore this pattern is not I-
bounded. It is, however, R-bounded, since the copy
is always affixed to the end of the word. The right
context is the empty string, which is (vacuously)
bounded. Thus, by Theorem 3 this pattern is also
subsequential; the FST is presented in Figure 8.
!
!C
!
V
C
CVV
?
?
Figure 8: An SFST for a CVZ-CV pattern.
As for the last pattern, nonlocal prefixation, we
have the opposite situation: the machine does have
to hold a potentially infinite number of segments
while it searches for the original, which means the
pattern is not subsequential. However, this par-
tial reduplication pattern is reverse subsequential
by Corollary 1: reversing the string CV-ZCV gives
VCZ-VC, which is R-bounded and identical to the
nonlocal suffixation that was already argued to be
subsequential.
To summarize, the attested partial reduplication
patterns all appear to be subsequential or reverse
subsequential.
This leaves us with full reduplication. Full redu-
plication is non-regular?although the position of
the copy is fixed, the amount of material that is
copied is not: full reduplication is not T-bounded.
Again, with no principled upper limit on the length
of words, a machine that copies an entire string can-
not be finite state, much less subsequential. This
distinction separates full and partial reduplication
in terms of computational complexity?the implica-
tion being that these processes may be better viewed
as distinct phenomena rather than subclasses of a
single process.
7 Discussion
The analyses of subsequential copying and deletion
presented above have revealed the conditions under
which metathesis and partial reduplication patterns
are subsequential. Under a copy+deletion analysis
of metathesis, all metathesis patterns are T-bounded,
since only one segment is copied (and then deleted)
at a time. Partial reduplication is also T-bounded, as-
suming what is copied fits a certain template. Thus
the T-bounded requirement on subsequential copy-
ing excludes only full reduplication from the subse-
quential class.
The analysis of metathesis relied on generalized
representations of such patterns that appear to be
typologically justified. A large number of attested
metathesis patterns are considered ?local?, which
amounts to being I-bounded by 1. Metathesis pat-
terns described as ?long distance? are also either I-
bounded or else L-bounded - patterns such as Ro-
mance liquid movement (Vennemann, 1988) and
Romani aspiration displacement (Matras, 2002) are
striking in that they all affect the initial onset of the
word.6 A type of logically possible pattern that ap-
pears to be unattested is one in which no context of
the copying - left, right, or intervening - is bounded.
Such a pattern would not be subsequential, and in
fact could not be described with any FST (i.e. it is
non-regular). In this way the restriction of metathe-
sis to the subsequential class finds support in the ty-
pological evidence. Also (apparently) unattested is
an R-bounded metathesis pattern - one which targets
the end of the word - though this is readily found in
the typology of partial reduplication. It remains for
future work whether such a metathesis pattern does
exist, and if not, whether further distinctions need
to be drawn to account for why R-bounded copying
only appears as partial reduplication.
Narrowing the computational bound of possible
phonological patterns from the regular class to the
subsequential class also has implications for learn-
ing. It is known that the class of regular languages is
not identifiable in the limit from positive data (Gold,
1967), but the subsequential class is: Oncina et al
(1993) have shown this class to be learnable by the
OSTIA algorithm. Although (without modification)
OSTIA does not do so well in practice on real data
sets (Gildea and Jurafsky, 1996), future work may
reveal algorithms that fare better if the hypothesis
space can be restricted even further (i.e. to a sub-
6The L-bounded patterns also appear to be restricted to the
diachronic domain. See (Chandlee et al, to appear)
49
class of the subsequential relations).
8 Conclusion
This paper has argued for an analysis of metathesis
as the composition of copying and deletion. Such
an analysis provides a computational link between
metathesis and partial reduplication, which extends
into a classification of these patterns as subsequen-
tial based on the bounded nature of the copying and
(in the case of metathesis) deletion. The typology
of attested patterns aligns well with the classifica-
tions proposed here, suggesting a tighter computa-
tional bound on phonological patterns than the one
established by Johnson (1972) and Kaplan and Kay
(1994).
Thus we can add metathesis and partial redupli-
cation to the phonological processes that have previ-
ously been shown to be subsequential - see (Koirala,
2010) for substitution, insertion, and deletion, and
Gainor et al (to appear) for vowel harmony. It
will be interesting to see to what extent morpholog-
ical processes, including templatic morphology, and
prosodic circumscription, also fit into this class.
References
P. F. Abboud. 1979. The verb in Northern Najdi Arabic.
Bulletin of the School of Oriental and African Studies,
42:467-499.
K.R. Beesley and L. Karttunen. 2003. Finite State Mor-
phology. Stanford: CSLI Publications.
C. Belta, A. Bicchi, M. Egerstedt, E. Frazzoli, E. Klavins
and G. J. Pappas. 2007 Symbolic planning and con-
trol of robot motion. IEEE Robotics and Automation
Magazine, 14(1): 61?71.
F. R. Blake. 1917. Reduplication in Tagalog. The Amer-
ican Journal of Philology, 38: 425-431.
J. Blevins and A. Garrett. 1998. The origins of
consonant-vowel metathesis. Language, 74(3):508-
556.
J. Blevins and A. Garrett. 2004. The evolution of
metathesis. In B. Hayes, R. Kirchner, and D. Steri-
ade (eds.) Phonetically Based Phonology. Cambridge:
Cambridge UP, 117-156.
E. Buckley. 2011. Metathesis. In M. van Oostendorp,
C.J. Ewen, E. Hume, and K. Rice (eds.) The Black-
well Companion to Phonology, Volume 3. Wiley-
Blackwell.
J. Chandlee, A. Athanasopoulou, and J. Heinz. to appear
Evidence for classifying metathesis patterns as subse-
quential. Proceedings of the 29th West Coast Confer-
ence on Formal Linguistics Somerville: Cascadilla.
N. Chomsky. 1956. Three models for the description
of language. IRE Transactions on Information Theory
113124. IT-2.
C. M. Churchward. 1940. Rotuman grammar and dictio-
nary. Sydney: Methodist Church of Australasia, De-
partment of Overseas Missions.
J. O. Davidson, Jr. 1977. A Contrastive Study of
the Grammatical Structures of Aymara and Cuzco
Quechua Ph.D. dissertation. University of California,
Berkeley.
R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison 1998
Biological Sequence Analysis: Probabilistic Models
of Proteins and Nucleic Acids Cambridge University
Press.
B. Gainor, R. Lai, and J. Heinz. to appear Compu-
tational characterizations of vowel harmony patterns
and pathologies. Proceedings of the 29th West Coast
Conference on Formal Linguistics. Somerville: Cas-
cadilla.
D. Gildea and D. Jurafsky. 1996. Learning bias and
phonological-rule induction. Computational Linguis-
tics22, 497-530.
E.M. Gold. 1967. Language identification in the limit.
Infomation and Control10, 447-474.
J. Heinz. 2005. Optional partial metathesis in Kwara?ae.
Proceedings of AFLA 12, UCLA Working Papers, 91-
102.
J. Heinz. 2007. The inductive learning of phonotactic
patterns. Doctoral dissertation, University of Califor-
nia, Los Angeles.
J. Heinz. 2009. On the role of locality in learning stress
patterns. Phonology 26: 303-351.
J. Heinz. 2010. Learning long-distance phonotactics.
Linguistic Inquiry 41, 623-661.
E. Hume. 2000. Metathesis Website. www.ling.ohio-
state.edu/ ehume/metathesis.
C. D. Johnson. 1972. Formal Aspects of Phonological
Description The Hague: Mouton.
D. Jurafsky and J.H. Martin 2008 Speech and Language
Processing, 2nd edition. Pearson Prentice Hall.
R. Kaplan and M. Kay. 1994. Regular models of phono-
logical rules systems. Computational Linguistics 20:
331-378.
C. Koirala. 2012. Strictly Local Relations. Ms. Univer-
sity of Delaware.
Y. Matras. 2002. Romani: a linguistic introduction.
Cambridge: Cambridge UP.
M. Mohri. 1997. Finite-state transducers in language
and speech processing. Computational Linguistics,
23: 269-311.
50
J. Oncina, P. Garcia, and E. Vidal. 1993. Learning subse-
quential transducers for pattern recognition interpreta-
tion tasks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 15(5): 448-457.
J. Riggle. 2003. Nonlocal reduplication. Proceedings of
the 34th annual meeting of the North Eastern Linguis-
tic Society.
J. Riggle. 2006. Infixing reduplication in Pima and its
theoretical consequences. Natural Language and Lin-
guistic Theory, 24: 857-891.
G. Rohlfs. 1950. Historische grammatik der unterital-
ienischen Gra?zita?t, Mu?nchen: Verlag der Bayerischen
Akademie der Wissenchaften.
S. Russell and P. Norvig. 2009 Artificial Intelligence: a
modern approach. Upper Saddle River, NJ: Prentice
Hall
S. Schieber. 1985 Evidence against the context-freeness
of natural language. Linguistics and Philosophy 8:
333-343.
J. N. Sneddon. 1996. Indonesian: a comprehensive
grammar, Routledge.
H. G. Tanner, C. Rawal, J. Fu, J. L. Piovesan, and C. T.
Abdallah. 2012 Finite abstractions for hybrid systems
with stable continuous dynamics. Discrete Event Dy-
namic Systems ,22(1):83-99.
T. Vennemann. 1988. Preference laws for syllable struc-
ture and the explanation of sound change. Berlin:
Mouton de Gruyter.
51
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 52?63,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Vowel Harmony and Subsequentiality
Jeffrey Heinz and Regine Lai
University of Delaware
{heinz,rlai}@udel.edu
Abstract
Attested and ?pathological? vowel har-
mony patterns are studied in the context of
subclasses of regular functions. The anal-
ysis suggests that the computational com-
plexity of phonology can be reduced from
regular to weakly deterministic.
1 Introduction
The expressivity of ordered rewrite-rule grammars
for phonology (Chomsky and Halle, 1968, hence-
forth SPE) and two-level phonology (Kosken-
niemi, 1983) are exactly the class of regular re-
lations (Johnson, 1972; Kaplan and Kay, 1994;
Beesley and Kartunnen, 2003). Since SPE-style
grammars can express virtually any phonologi-
cal generalization, it follows that the generaliza-
tions themselves are regular, even if they are rep-
resented with other formalisms (such as OT gram-
mars).
This result can be interpreted cognitively as
establishing a universal property of phonological
patterns: humanly possible phonological patterns
are regular. If correct, this would mean, for ex-
ample, that humanly possible syntactic patterns
which are nonregular are not humanly possible
phonological patterns (Heinz and Idsardi, 2011;
Lai, 2012; Heinz and Idsardi, 2013).
Recent research suggests that stronger univer-
sals than ?being regular? can be established for
phonology. It has been shown that segmental
phonotactic patterns are star-free (Heinz et al,
2011), as are virtually all stress patterns (Rogers
et al, to appear), and the semantics of two-level
rules appear to ensure that these mappings have
star-free-like properties, provided the contexts to
the rules are star-free (Yli-Jyra? and Koskenniemi,
2006).1
1There are multiple ways to generalize the class of star-
free regular sets to regular relations (Benedikt et al, 2001) so
This paper examines the hypothesis that subse-
quentiality is a necessary property of phonologi-
cal patterns by studying theories of iterative vowel
harmony. Informally, a function is left (right) sub-
sequential if there is a finite-state transducer de-
scribing the function which processes strings from
left to right (right to left) deterministically on the
input. We use the term ?subsequentiality? to mean
either left or right subsequential.
Previous work has found that synchronically at-
tested metathesis patterns and partial reduplica-
tion patterns are either left or right subsequen-
tial (Chandlee et al, 2012; Chandlee and Heinz,
2012). Also Gainor et al (2012) show that the
vowel harmony generalizations in Nevins (2010)
are also left or right subsequential mappings.
Gainor et al?s analysis, while insightful, is in-
complete since Nevin?s theory of vowel harmony
invokes underspecification and other theories of
vowel harmony do not. Phonological underspec-
ification is explained in section 3. The linguis-
tic generalizations examined in this paper come
from two types of theories of vowel harmony
patterns in linguistics which do not use under-
specification: traditional directional theories and
dominant/recessive/stem-control theories.
We prove that subsequentiality separates di-
rectional theories from logically possible but
?pathological? vowel harmony patterns (Wil-
son 2003). (This claim was also made by
Gainor et al without proof.) It is also
shown that dominant/recessive/stem-control theo-
ries posit generalizations which are neither left nor
right subsequential, but which are weakly deter-
ministic. Informally, this means that these gen-
eralizations can be decomposed into a left sub-
sequential and right subsequential function with-
out the left-subsequential function marking up its
output in any special way. We conjecture this is
it would be interesting to determine more exactly the nature
of such two-level rules.
52
not the case for the pathological patterns. Since
subsequential and weakly deterministic functions
are proper subclasses of regular relations, these
results suggest concretely how the computational
complexity of phonology established by earlier re-
searchers can be improved.
Mathematical and phonological preliminaries
are given in sections 2 and 3, respectively. Sec-
tions 4, 5, and 6 consider the vowel harmony pat-
terns with respect to the regular and subsequential
boundaries. and weakly deterministic boundaries,
respectively. Section 7 concludes.
2 Preliminaries
2.1 Regular relations and functions
If X denotes a finite alphabet then X? and Xn
denotes the sets of all finite strings and the set of
all strings of length n over X, respectively. The
length of a string w is |w|. The unique string of
length zero is denoted ?. A string w of length k
can be written w1w2 ? ? ?wk, where wi is the ith
letter of w. The reverse of a string w = w1 ? ? ?wk
is wr = wk ? ? ?w1. For finite alphabets X and Y ,
a relation is a subset ofX??Y ?. IfR is a relation,
the reverse relation Rr = {(xr, yr) | (x, y) ? R}.
Note the reverse relation is not the inverse rela-
tion. A relation R is length-preserving iff for all
(x, y) ? R it is the case that |x| = |y|. It is
length-increasing iff there exists (x, y) ? R such
that |x| < |y|.
Any relation R ? X? ? Y ? is a function iff
for all x ? X?, there is at most y ? Y ? such
that (x, y) ? R. In this case, we often write R :
X? ? Y ?. For two functions f : X? ? Y ? and
g : Y ? ? Z?, the composition of f and g is a
function h : X? ? Z? such that h(x) = g(f(x)).
We write h = g ? f .
For all x ? X?, the prefixes of x are Pr(x) =
{u ? X? | ?v ? X? such that x = uv}. For any
set L ? X?, the longest common prefix of L is
lcp(L) = w ? w ?
?
x?L
Pr(x) ?
(
?w? ?
?
x?L
Pr(x)
)[
|w?| ? |w|
]
(1)
Regular relations are those describable by
finite-state transducers (FSTs).2 A finite-state
transducer T is a tuple (Q,X, Y, I, F, ?) where
2In the algebraic theory of automata, these are called ra-
tional relations (Berstel, 1979; Sakarovitch, 2009).
Q is a finite set of states, X and Y are finite al-
phabets, I, F ? Q are the initial and final states,
respectively, and ? ? Q ? X? ? Y ? ? Q is
the transition function. For all FSTs, the transi-
tion function ? is recursively extended to ?? in the
usual way. The relation that a finite state trans-
ducer T = (Q,X, Y, I, F, ?) recognizes/accepts/-
generates is
R(T ) =
{
(x, y) ? X? ? Y ? | (?qi ? I)
(qf ? F )
[
(qi, x, y, qf ) ? ??
]
}
.
(2)
Let RR denote the class of regular relations. This
class is closed under concatenation, Kleene clo-
sure, union, composition, and inversion, but not
intersection or complement.
A subsequential transducer is a tuple
(Q, q0,X, Y, ?, ?), where Q is a finite set of
states, X and Y are finite alphabets, q0 ? Q is
the initial state, and ? ? Q ? Y ? is the output
function. Informally, subsequential transducers
are weighted acceptors that are deterministic on
the input, and where the weights are strings and
multiplication is concatenation.
Formally, the transition function ? ? Q?X ?
Y ? ?Q is deterministic:
(q, a, u, r), (q, a, v, s) ? ? ? u = v ? r = s .
The transition function ? is also recursively ex-
tended to ??. The relation that a subsequen-
tial transducer T = (Q, q0,X, Y, ?, ?) recog-
nizes/accepts/generates is
R(t) =
{
(x, yz) ? X? ? Y ? | (?q ? F )
[
(q0, x, y, q) ? ?? ? z = ?(q)
]
}
.
(3)
Since subsequential transducers are deterministic,
the relations they recognize are functions.
Functions recognized by subsequential trans-
ducers are called left subsequential. A function
f is right subsequential iff its reverse f r is left
subsequential. Observe that for all w ? X?, the
image of a right subsequential function f of w can
be calculated by reversing w, processing the result
with the subsequential transducer T recognizing
f r, and then reversing the result. Formally,
(?w ? X?)[f(w) = T (wr)r]. (4)
However, there is another way to state the above
without the reversing function (?r). This is to rec-
ognize that f(w) can be computed by applying T
53
to w from right to left, instead of from left to right.
When T processes w right to left, we write??T (w)
and when T processes w left to right, we write??T (w). Then we can restate Equation 4 as
(?w ? X?)[f(w) =??T (w)]. (5)
Let LSF and RSF denote the class of left and
right subsequential functions, respectively.
Theorem 1 (Mohri 1997) The following hold:
1. LSF,RSF ( RR.
2. RSF r = LSF .
3. LSF and RSF are incomparable.
There are some relevant subclasses and gen-
eralizations of subsequentiality. A subsequential
transduction T is sequential iff for all q ? Q, it
is the case that ?(q) = ?.3 Mohri (1997) gen-
eralizes subsequentiality to p-subsequentiality (al-
lowing up to p outputs for each input), preserving
many important properties. Mohri?s generaliza-
tions are important here because there are likely
to be a bounded number of exceptions, or optional
forms, in actual vowel harmony systems that fall
outside the purview of the 1-subsequential anal-
ysis presented here, but which would presumably
not fall outside a p-subsequential analysis (not pre-
sented here).
Elgot and Mezei proved the following.
Theorem 2 (Elgot and Mezei 1965) Let
T : X? ? Y ? be a function. Then T ? RR
iff there exists L : X? ? Z? ? LSF , and
R : Z? ? Y ? ? RSF with X ? Z such that
T = R ? L.
What this decomposition means is that the com-
putation of T (x) = y can be accomplished by
(1) reading x sequentially from left to right with a
subsequential transducer, which transforms it into
a word z possibly marking it up with additional
symbols; (2) reading the resulting word z from
right to left with another subsequential transducer
and writing from right to left the final output y. As
their proof makes clear, this decomposition of T
is possible because the alphabet Z may be strictly
3Sakarovitch (2009) prefers the term ?sequential? for sub-
sequential functions and the term ?pure sequential? for se-
quential functions. While his arguments are reasonable (pp.
651-2), we adopt the more widely adopted terminology.
larger than X, and so z can be marked-up with ad-
ditional symbols which carry additional informa-
tion.4
Finally, we review one important property of
subsequential transducers and regular sets. For
any function f : X? ? Y ? and x ? X?, let the
tails of x in f be defined as
TLf (x) =
{
(y, v) | f(xy) = uv ?
u = lcp(f(xX?))
}
.
(6)
Every subsequential transducer T computing a
function f admits a canonical form, where the
states of T are in one-to-one correspondence with
TLf (x) for all x ? X?.
Theorem 3 (Oncina et al 1993) f ? LSF ?
{TLf (x) | x ? X?} has finite cardinality.
This theorem is the functional counterpart to the
Myhill/Nerode relation. Recall that for any set of
strings L, the tails of a word w with respect to L
is defined as TLL(w) = {u | wu ? L}. This
relation partitions the set of all logically possible
strings into a finite set of equivalence classes iff
the set L is regular. These equivalence classes are
the basis for constructing the smallest determinis-
tic acceptor for a regular language.
Similarly, in the construction of the canonical
subsequential transducer for a left subsequential
function, the states correspond to the sets of tails
defined in (6) above. There is a rich literature on
subsequential functions (Elgot and Mezei, 1965;
Berstel, 1979; Oncina et al, 1993; Mohri, 1997;
Roche and Schabes, ; Sakarovitch, 2009).
2.2 Weak Determinism
Here we introduce the notion of weak determin-
ism. Informally, these are regular functions which
decompose into left and right subsequential func-
tions as in Elgot and Mezei?s theorem but without
the mark-up given by the intermediate, larger al-
phabet. Thus, they are not necessarily determinis-
tic, but they are ?more? deterministic than regular
functions where Elgot and Mezei decomposition
requires the intermediate mark-up.
While the mark-up can be accomplished by
introducing new symbols (as done in Elgot and
4Berstel (1979) provides an updated proof. This
book is out of print but the first four chapters are
available for download at http://www-igm.
univ-mlv.fr/?berstel/LivreTransductions/
LivreTransductions.html. The theorem and proof
begin on page 117 in the online version and on page 126 in
the printed version.
54
X?
X?
Z? X?
T
L? = h ? L
L
R
R? = R ? h?1
h
h?1
Figure 1: Decompositions of regular function T
with X ? Z .
Mezei?s proof), for any alphabet with at least two
symbols, the mark-up can also be accomplished
by coding these new symbols as strings formed
over the original alphabet.5 Figure 1 illustrates.
Let X be a finite alphabet containing the symbols
{a, b}. Consider any alphabet-preserving regular
function T : X? ? X?. By Elgot and Mezei?s
theorem, there exists left subsequential L : X? ?
Z? and right subsequential R : Z? ? X? with
X ? Z such that T = R ? L. Let h : Z? ? X?
be a function which encodes each word w in Z?
by coding each symbol in w as follows. Assume
some enumeration of the symbols in Z and the
rewrite the nth symbol of Z as abna. For exam-
ple if Z = {a, b, c} and w = cab then h(w) =
abbaaaaba. It is not difficult to verify that h is
length-increasing and that both h and h?1 are sub-
sequential functions. Letting R? = R ? h?1 and
L? = h ? L, it follows that T = R? ? L? and that
both R? and L? have domain and co-domain X?.
For this reason, it is not sufficient to require that
the decomposition be alphabet-preserving (i.e.
Z = X) to avoid any mark-up. It is also neces-
sary that the first factor L not be length-increasing.
This is because the only way to unambiguously en-
code a larger alphabet into a smaller one is with
length-increasing functions (like h in the above
example).
Definition 1 A regular function T is weakly de-
terministic iff there exists L : X? ? X? ? LSF ,
and R : X? ? X? ? RSF such that L is not
length-increasing and T = R ? L. The class of
weakly deterministic functions is denoted WD.
The corollary below is immediate from this def-
inition and Elgot and Mezei?s theorem.
Corollary 1 LSF,RSF ?WD ? RR.
5We are indebted to an anonymous reviewer for this im-
portant observation.
noun genitive gloss
a. ip ip-in rope
b. el el-in and
c. son son-un end
d. pul pul-un stamp
Table 1: Examples illustrating a fragment of the
Vowel harmony from Turkish (Nevins 2010:32).
(a)
w f(w)
/ip-un/ [ip-in]
/el-un/ [el-in]
/son-un/ [son-un]
/pul-un/ [pul-un]
. . .
(b)
w f(w)
/?C+C/ [?C?C]
/C+C+C/ [C+C+C]
. . .
Table 2: Examples showing fragments of the
phonological function describing Turkish back
harmony assuming the underlying genitive mor-
pheme is /-un/.
The vowel harmony analysis below is sufficient to
go a step further and demonstrate a separation be-
tween LSF, RSF on one side and WD on the other.
We conjecture that one unattested ?pathological?
vowel harmony patterns separates WD from RR.
3 Vowel Harmony
Vowel harmony is a pattern wherein vowels assim-
ilate with respect to some feature. Table 1 shows
two allomorphs of the genitive suffix, [-in] and [-
un]. The allomorph is predictable based on the
front/back dimension of the preceding vowel: if
the preceding vowel is front then [-in] occurs, but
if it is back then [-un] occurs. (Turkish also has
rounding harmony, which is not shown here.)
Phonological analysis conceives of the phono-
logical grammar as a function which maps an
abstract lexical representation (the ?underlying
form?) to a more concrete?but still abstract?
phonological representation (the ?surface form?)
(Hyman, 1975; Kenstowicz, 1994; Hayes, 2009).
Phonological transcriptions, like the ones in Ta-
ble 1, represent surface forms.
To illustrate, consider a simple phonological
analysis of the Turkish forms above, which posits
the underlying form of the genitive suffix to be
/-un/ and a mapping f which derives the surface
forms as shown in Table 2(a). Table 2(b) describes
the mapping only in terms of the relevant details
where [+] indicates [+ back] vowels, [?] indi-
55
cates [?back] vowels, and C consonants.
Nevins? (2010) analysis of vowel harmony uti-
lizes underspecification. We illustrate this concept
ostensively with the Turkish example above. In-
stead of positing the underlying form of the suffix
to be either /-in/ or /-un/, underspecification the-
ory would posit it to be /-Vn/ where V is high,
unrounded vowel unspecified for backness. In
Nevins? theory, underlying vowels which have fea-
ture specifications can spread those features only
to vowels unspecified for those features. Under-
specification is not congruent with research in Op-
timality Theory (Prince and Smolensky, 2004),
which, by the principle of the rich base, requires
every underlying form to be considered (includ-
ing those where every vowel is fully specified).
Gainor et al (2012) show that the iterative map-
pings Nevins describes are subsequential, but this
does not address those theories (like OT) which
may not consider underlying forms to permit un-
derspecification. All the vowel harmony patterns
considered in this paper do not admit any under-
specification whatsoever.
A traditional view of vowel harmony analyzes
vowel harmony patterns as either instances of
progressive harmony (PH) or regressive harmony
(RH). Informally, progressive harmony means the
value of a feature can be thought to spread from
left to the right (as in the Turkish example above).
Conversely, regressive harmony can be thought of
as spreading from right to left. This is illustrated
with examples (a-d) in Table 3.
Other theories of vowel harmony reject that di-
rectionality is a primitive of the theory and argue
that vowel harmony is either dominant/recessive
(DR) or stem-controlled (SC) (Bakovic?, 2000;
Kra?mer, 2003) (see also (Archangeli and Pulley-
blank, 1994)). Dominant/recessive theories ana-
lyze vowel harmony patterns by postulating that a
particular feature value of a harmonizing feature
is the dominant one. The DR function in Table 3
identifies the [+] value as the dominant one; so
any underlying representation containing the har-
monizing feature with the value [+] will surface
so that the harmonizing feature in all vowels will
also be [+]. Stem-controlled analyses are similar
to dominant-recessive theories, however the fea-
ture that spreads is determined not by its value but
instead by the morphological position of the vowel
to which the feature belongs (for instance, is the
vowel in a stem or affix?).
An additional complication is that variations
of the above functions are introduced by neu-
tral vowels, which never undergo harmony. They
come in two kinds: transparent vowels which per-
mit features to spread through them, and opaque
vowels, which block the spread of harmony, but
trigger their own harmony domain. Some effects
of neutral vowels are shown in rows (e-f) in Ta-
ble 3. (Symbols [?] and [?] are [?F] vowels that
are opaque and transparent, respectively. Likewise
we use [?] and [?] to denote opaque and transpar-
ent vowels which are [+F].)
Additionally, the phonological literature in-
cludes discussion of logically possible, unattested
and unnatural vowel harmony patterns that are
predicted by classical approaches to vowel har-
mony in OT. These patterns include sour grapes
(SG) (Padgett, 1995; Wilson, 2003) and major-
ity rules (MR) (Lombardi, 1999; Bakovic?, 2000).
Informally, SG is like progressive harmony ex-
cept that vowels only harmonize if every vowel
is guaranteed to harmonize. For example, if an
opaque vowel occurs after the trigger, no non-
neutral vowel harmonizes with the trigger. Ma-
jority Rules instantiates the following rule: If the
number of segments with ?F is greater than the
number of segments with ??F , then segments
with ?F are the triggers of harmony and segments
with ??F are the targets and undergo change.
Because phonologists consider SG and MR to be
bizarre, they are referred to as ?pathologies? (Wil-
son, 2003; Wilson, 2004; Finley, 2008) and it is a
strike against a theory if it predicts the existence
of either SG or MR.
Henceforth, let X = Y = {+,?, C,?,?}.
These symbols represent equivalence classes of a
partition of the phonemic inventory of any lan-
guage which exhibits progressive harmony for the
feature F . The symbols + and ? represent the
classes of all harmonizing vowels which are +F
and ?F , respectively. Phonemes invisible to har-
mony are in the C class; this includes consonants
and transparent vowels. The symbol ? (?) refers
to opaque vowels which are +F (?F ), which
block the spread of ?F (+F ), and which begins a
new harmonic domain spreading +F (?F ).
The vowel harmony mappings in this paper are
all, in fact, same-length relations. Furthermore,
they are sequential. These additional properties
do not appear to be shared by other phonological
processes. Epenthesis and deletion are common
56
w PH(w) RH(w) DR(w) SG(w) MR(w)
a. /+ ??/ [+ + +] [???] [+ + +] [+ + +] [???]
b. /? + +/ [???] [+ + +] [+ + +] [???] [+ + +]
c. /? ??/ [???] [???] [? ??] [???] [???]
d. /? +?/ [???] [???] [+ + +] [???] [???]
e. /+ ??/ [+ + ?] [???] [+ + ?] [+??] [???]
f. /+ ??/ [+?+] [???] [+ ?+] [+?+] [???]
Table 3: Example mappings of underlying forms (w) given by progressive harmony (PH), regressive har-
mony (RH), dominant/recessive harmony (DR), sour grapes harmony (SG), and majority rules harmony
(MR). Symbols [+] indicates a [+F] vowel and [?] indicates a [?F] vowel where ?F? is the feature
harmonizing. Symbols [?] and [?] are [?F] vowels that are opaque and transparent, respectively.
cross-linguistically, and the metathesis patterns
analyzed by Chandlee and Heinz (2012) are not
sequential (though they are subsequential). For
this reason, we keep the analysis focused at the
level of subsequentiality.
4 The regular boundary
In this section we show that the regular boundary
is sufficient to distinguish the pathological Major-
ity Rules pattern from the attested progressive and
regressive harmony patterns.
Formally, MR functions can be defined as fol-
lows. Let |w|+F and |w|?F denote the number of
participating vowels (i.e. non transparent vowels)
which are +F and ?F , respectively, in w ? X?.
Then we define a Majority Rules Harmony pattern
as any same-length function which at a minimum
obeys the following:
MR(w) =
{
+|w| if |w|+F > |w|?F
?|w| if |w|?F > |w|+F
(7)
The result below seems to be widely known (see
Riggle (2004, chapter 7, section 5)) though we
have not been able to find a proof in print.
Theorem 4 Majority Rules is not regular.
Proof By way of contradiction, suppose that MR
is a regular relation. Since regular relations are
closed under inverse, so is MR?1. The image of a
regular set under a regular relation is also a regular
set (see Roche and Schabes (, pp. 41-43)). There-
fore, MR?1(+?) is a regular set. Since regular
sets are closed under intersection, it follows that
MR?1(+?)? (+???) is regular as well. Call this
set S.
However, S is in fact not a regular set. Since
MR is length preserving, for all odd k ? N, it
0,?
1,?
2,?
?,?
+,?
??
C C, ?, ?, + :?
C, +, ?, ? :+
Figure 2: A subsequential transducer which rec-
ognizes iterative, progressive harmony.
is the case that MR(+k) ? (+???) = +m?n
where 0 ? m,n ? k and m + n = k and m > n.
Furthermore, for all for all odd k, it is the case
that TLS(+k), includes ?n for all n < k ? 1
but excludes ?n for all n > k + 1. Thus there is
a distinct Nerode-equivalence class for each odd
k, and hence S is not a regular set, and therefore
MR is not a regular relation. 
On the other hand, progressive and regressive
harmony are regular; in fact, subsequential. For
concreteness we analyze a canonical progressive
harmony pattern which includes neutral vowels.
The subsequential transducer TPH in Figure 2
faithfully captures the PH function. Labels on the
transitions are interpreted as in Beesley and Kar-
tunnen (2003): commas delimit multiple transi-
tions; the label x:y means x is the input and y the
output; absence of a colon means the input and
output are identical. The rightmost symbol inte-
rior to a state is the output of the ? function.
While TPH presupposes languages have opaque
vowels, it can be modified as needed to remove
this assumption without losing subsequentiality. If
a language has no opaque vowels, those transi-
tions can be removed. Since any subgraph of the
transducer shown in Figure 2 is also subsequen-
57
tial, this establishes the left subsequentiality of it-
erative, progressive harmony patterns without un-
derspecification.6
As for iterative regressive harmony patterns,
they are simply the reverse of iterative progres-
sive patterns. In other words, for all w ? ??,
RH(w) =???TPH .
5 The subsequential boundary
In this section we show that while the regular
boundary is not sufficient to separate the patho-
logical Sour Grapes pattern from attested harmony
patterns, the subsequential boundary is sufficient.
Padgett (1995) defines Sour Grapes Harmony
as ?Either all features must spread, or none
will. . . ? For concreteness, consider a progressive
Sour Grapes pattern. The form +??? would be
mapped to ++++ as in progressive harmony, but
the form + ? ?? is mapped to + ? ?? because
the opaque vowel will not become +F , and so the
spreading process grumpily chooses not to spread
at all. Therefore, a progressive Sour Grapes Har-
mony pattern is defined as any length-preserving
function which at a minimum includes the follow-
ing mappings for all n ? N:
SG(+?n) = ++n?SG(+?n?) = +?n? (8)
There is a finite state transducer which describes
this fragment of the SG function, shown in Fig-
ure 3. As a total function, for SG to be regular, it
is important that the image of the complement of
?n?N{+?n} ?n?N {+ ?n ?} under SG also be
regular. Pictorially, this would mean that the frag-
ment shown in Figure 3 is a subgraph of the full
SG pattern. Crucially, however, there can also be
no transition from state 2 bearing the input symbol
? that can lead (even eventually) to a final state.
We now prove the main theorem of this paper.
Theorem 5 SG is neither left nor right subse-
quential.
Proof We show that, for all distinct n,m ? N,
the tails of +?n is not the same as the tails of
+?m. This immediately implies that the canon-
ical left subsequential transducer would have in-
finitely many states, and hence that any SG pattern
meeting Equation 8 is not left subsequential.
6It is true that TPH does not model progressive harmony
patterns where transparent vowels can trigger harmony. It
is not difficult to modify TPH to accommodate this without
sacrificing subsequentiality.
0 1 2
3 4
+ ?:+
?
?
?:+
?
Figure 3: A non-deterministic transducer which
recognizes a fragment of SG harmony.
To illustrate, consider x = +?. Since SG(+?
X?) includes elements +++ and +?? (mapped
from e.g. + ? ? and + ? ?, resp.), it fol-
lows that lcp(SG(+ ? X?)) = +. Therefore,
(?,++) ? TLSG(+?). Observe that (?,+n) 6?
TLSG(+?) for all n 6= 2 since SG is length-
preserving.
More generally it is the case that (?,+n+1) ?
TLSG(+?n) and (?,+m) 6? TLSG(+?n) for all
m 6= n + 1. Therefore there are infinitely many
distinct sets of tails for functions conforming to
(8), and thus no SG pattern is subsequential.
A similar argument (omitted) establishes that
any SG pattern is not right subsequential. 
Consequently, the subsequential boundary sepa-
rates SG and MR from PH and RH.
6 The weakly deterministic boundary
As mentioned earlier, the dominant/recessive and
stem-control theories of vowel harmony reject the
directionality generalizations inherent in the PH
and RH mappings. If these theories are correct,
then it is important to see what boundary (if any)
separates MR and SG from vowel harmony pat-
terns described with dominant/recessive and stem-
control based generalizations. We show that the
mappings these theories posit are, like SG, not
subsequential. However, we believe there is an
interesting complexity difference between SG on
the one hand and DR and SC on the other. In par-
ticular, we show that DR and SC are weakly de-
terministic. We conjecture that SG is not weakly
deterministic and provide the intuition behind this
conjecture, though its proof currently escapes us.
A dominant/recessive analysis of vowel har-
mony says if the word contains the dominant value
of the harmonizing feature, then other vowels in
the word take on the dominant value for this fea-
ture. For example, if the [+] value for harmoniz-
ing feature F is the dominant one and an under-
58
lying representation contains a vowel specified as
+F, then all other non-neutral vowels in the word
will be realized as +F as well.
Therefore, we can define a function as dom-
inant/recessive as any length-preserving function
which includes the following mappings:
?w ? {+,?}?,
DR(w) =
{
+|w| if (? 0 ? i ? |w|)[wi = +]
?|w| otherwise
(9)
The next two theorems establish that DR har-
mony is properly weakly deterministic.
Theorem 6 DR is neither left nor right subse-
quential.
Proof The proof is similar to the one for SG. We
show that, for all distinct n,m ? N, the tails of?n
is not the same as the tails of ?m.
Consider first x = ??. To find its tails we must
know lcp(DR(? ? X?)). Since DR(? ? X?)
includes elements ? ? ? and + + + (mapped
from e.g. ? ? ? and ? ? +, resp.), it fol-
lows that lcp(DR(+ ? X?)) = ?. Therefore,
(?,? ? ?) ? TLDR(??). Observe that for all
n 6= 2, it is the case that (?,+n) 6? TLDR(??)
since DR is length-preserving.
Next consider x = ? ? ?. To find its tails we
must know lcp(DR(???X?)). DR(???X?)
includes elements ???? and ++++ (mapped
from e.g. ? ? ?? and ? ? ?+, resp.), and
so again the longest common prefix is ?. There-
fore, (?,++++) belongs to TLDR(???) and
(?,+n) 6? TLDR(+?) for all n 6= 3 since DR
preserves string lengths.
More generally for all distinct n,m ? N it
is the case that (?,+n+1) ? TLDR(?n) and
(?,+m) 6? TLDR(+?n) for all m 6= n + 1.
Therefore there are infinitely many distinct sets of
tails for functions conforming to (9), and thus no
DR pattern is subsequential.
A similar argument (omitted) establishes that
any DR pattern is not right subsequential. 
On the other hand, DR is weakly determinis-
tic. We establish this for the case when the al-
phabet contains only {+,?}. In fact, DR is sim-
ply the composition of progressive harmony and
regressive harmony where only the dominant fea-
ture value spreads. Figure 4 shows a subsequen-
tial transducer TPHP describing a progressive har-
mony function where only [+] spreads. Observe
0,?
1,?
2,?
?
+
+
C ?
+, ? :+
Figure 4: The subsequential transducer TPHP
which recognizes iterative, progressive harmony
where only the + value spreads.
that the transducer in Figure 4 is nearly identical
to TPH in Figure 2 without the opaque vowels and
the C symbol. The important difference is that in
Figure 2 there is a transition from state 1 to itself
which reads a [+] input and outputs a [?], but in
Figure 4, the transducer in state 1, upon reading
input [+] transitions to state 2 and writes out [+].
Let PHP denote the function TPHP computes.
Theorem 7 DR is weakly deterministic.
Proof We show that for all w ? {+,?}?,
DR(w) =????TPHP ?
????TPHP (w).
Case 1. There exists 0 ? i ? |w| such that wi =
+. Then DR(w) = +|w|. It follows from the
definition of PHP that for all j ? i, wj =
+. Letting u = PHP (w)r , it follows that
u1 = +. Thus PHP (u) = +|w|. The reverse
of +|w| is clearly itself. Therefore, PHP r ?
PHP (w) = +|w|.
Case 2. Case 1 does not hold. Then DR(w) =
?|w|. By definition, PHP (w) = ?|w|.
Clearly then PHP r ? PHP (w) = ?|w|.
Since DR = ????TPHP ?
????TPHP , since
????TPHP and????TPHP are alphabet-preserving, and since
????TPHP is
not length-increasing, the theorem is proved. 
While, the proof of theorem 7 is limited to words
in {+,?}?, we believe the extension to words in
{+,?, C,?,?}? is only challenging technically,
and not conceptually. For example, the definition
of DR harmony above in Equation 9 should be
more articulated so that, for instance, DR(????
?+?????) = ??? + + + + + ???.
Stem-controlled analyses are similar to
dominant/recessive theories. Unlike domi-
nant/recessive theories, however, the vowels
which trigger harmony are the ones which belong
to the morphological stem. Table 4 illustrates
with stem boundaries indicated with #. What
59
w SC(w)
a. +#?#? ???
b. ?# + #+ + + +
c. ?#?#? ???
d. ?# + #? + + +
Table 4: Example mappings of underlying forms
(w) with three vowels given stem control theories
of vowel harmony. In each example, the middle
vowel is the only vowel in the stem.
happens when there is more than one stem vowel?
The stem precedence generalization (Bakovic?,
2003) states that ?an alternating affix vowel
always agrees with the adjacent vowel in the
stem to which the affix is attached.? Therefore
stem vowels themselves are not targets of the
harmony process. Consequently, underlying
/ ? ?# + ? + ?# + +/ would surface as
[+ + # +?+?#??].
Since every underlying form is assumed
to contain a stem, the domain of SC har-
mony is X?#X+#X?. Then ?w#u#v ?
X?#X+#X?, it is the case that
SC(w) =
?
?
?
?
?
?
?
+|w|#u# +|v| if u1 = + ? u|u| = +
+|w|#u#?|v| if u1 = + ? u|u| = ?
?|w|#u# +|v| if u1 = ? ? u|u| = +
?|w|#u#?|v| if u1 = ? ? u|u| = ?
(10)
The analysis of SC is the same as DR; and so
the proofs are omitted.
Theorem 8 SC is neither left nor right subsequen-
tial.
Theorem 9 SC is weakly deterministic.
We believe there is a difference between DR
and SG: we conjecture that SG is not weakly de-
terministic. To get an intuition why, consider
the two subsequential transducers A and B in
Figure 5. Limiting our attention to the domain
{+?n}?{+?n ?}?{?}, the composition??B ???A
equals SG. This is possible because these func-
tions make use of an additional symbol [ ??], indi-
cating a minus value whose left context matches
the environment to become [+].
Table 5 illustrates the role the additional symbol
plays in the derivation. We are doubtful that it is
possible to decompose SG into a left and right de-
terministic function where the left function is pro-
0,? 1,? 2,?+ ?
?: ??
A
0,?
1,+
2,+
?
?? :+
?? :?
?? :+
B
Figure 5: Two subsequential transducers such that??B ? ??A = SG. The symbol [ ??] indicates a
[?] which would undergo harmony provided no
opaque vowel occurs downstream.
w +???? +???
??A (w) + ?? ?? ??? + ?? ?? ????B ? ??A (w) +???? + + ++
Table 5: Illustrations of the role of [ ??] in the de-
terministic decomposition of SG=
??B ? ??A .
hibited from marking up its output in any way (ei-
ther with extra symbols or with a length-increasing
coding trick).
7 Conclusion
The first suggestion that phonological processes
have a tighter computational bound than ?being
regular? may come from (Mohri, 1997), buried on
page 279. He writes without elaboration or cita-
tion ?Most phonological and morphological rules
correspond to p-subsequential relations.? This
study suggests that Mohri?s assessment is largely
correct, though the complete picture is more com-
plicated than Mohri?s offhand comment indicates.
The more complicated picture with respect
to vowel harmony is expressed in Figure 6,
which summarizes this paper?s contributions.
Traditional directional theories of vowel har-
mony express simpler generalizations than
dominant/recessive/stem-control theories. It is
our opinion that future work will likely show
that even the weakly deterministic boundary
surely separates the pathological patterns from the
attested ones.
60
Non-regular Regular
Weakly deterministic
Left
Subsequential
Right
Subsequential
? PH ? RH
? DR
? SC
? SG
? MR
??
Figure 6: Hierarchies of transductions with the results of this paper shown. PH=progressive harmony,
RH=regressive harmony, DR=dominant/recessive harmony, SC=stem control harmony, SG=sour grapes
harmony, and MR=majority rules harmony.
Although the harmony patterns in this paper are
all describable with same-length relations, we de-
liberately chose not to focus on the special prop-
erties same-length relations engender. This is
largely because there are phonological processes
such as epenthesis and deletion which are not
same-length, and we would like our conclusions
to hold for all phonological patterns. Nonetheless,
future work which explores the same-lengthness
aspect may lead to some interesting insights. One
reviewer of this paper conjectured, for example,
that if the same-length relations were coded as lan-
guages that they would then be k-reversible (An-
gluin, 1982).
With respect to learnability, total subsequential
functions are identifiable in the limit from posi-
tive data (Oncina et al, 1993), though this algo-
rithm appears to require data points unavailable
in natural language corpora (Gildea and Jurafsky,
1996). Investigating subclasses of subsequential
functions which cover attested phonological pat-
terns may thus not only better characterize possi-
ble phonologies, but may also provide insights for
learning (Chandlee and Koirala, 2013).
Finally, we believe Elgot and Mezei?s theorem
can shed new light on the old problem of abstract-
ness in phonology (Hyman, 1970), and suspect a
hierarchy of complexity depending on how much
markup (either new symbols or with a length-
increasing function) needs to be introduced in the
intermediate alphabet to order to decompose a reg-
ular function into left and right subsequential ones.
Computational work whose results should be more
carefully investigated with this in mind include
Kempe (2000) and Crespi Reghizzi and San Pietro
(2012).
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. We also thank the 2012-
2013 computational linguistics group at the Uni-
versity of Delaware, especially Jim Rogers, for
useful discussion.
References
Dana Angluin. 1982. Inference of reversible lan-
guages. Journal for the Association of Computing
Machinery, 29(3):741?765.
Diana Archangeli and Douglas Pulleyblank. 1994.
Grounded Phonology. Cambridge, MA: MIT Press.
Eric Bakovic?. 2000. Harmony, Dominance and Con-
trol. Ph.D. thesis, Rutgers University.
Eric Bakovic?. 2003. Vowel harmony and stem
identity. In San Diego Linguistic Papers, pages
1?42. Department of Linguistics, UCSD, UC San
Diego. http://escholarship.org/uc/
item/7zw206pt.
61
Kenneth Beesley and Lauri Kartunnen. 2003. Finite
State Morphology. CSLI Publications.
Michael Benedikt, Leonid Libkin, Thomas
Schwentick, and Luc Segoufin. 2001. A model-
theoretic approach to regular string relations. Logic
in Computer Science, Symposium on, 0:0431.
Jean Berstel. 1979. Transductions and Context-Free
languages. Teubner-Verlag.
Jane Chandlee and Jeffrey Heinz. 2012. Bounded
copying is subsequential: Implications for metathe-
sis and reduplication. In Proceedings of the 12th
Meeting of the ACL Special Interest Group on Com-
putational Morphology and Phonology, pages 42?
51, Montreal, Canada, June. Association for Com-
putational Linguistics.
Jane Chandlee and Cesar Koirala. 2013. Learning
local phonological rules. Penn Linguistics Collo-
quium, March.
Jane Chandlee, Angeliki Athanasopoulou, and Jeffrey
Heinz. 2012. Evidence for classifying metathesis
patterns as subsequential. In The Proceedings of the
29th West Coast Conference on Formal Linguistics,
pages 303?309. Cascillida Press.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper & Row, New York.
Stefano Crespi Reghizzi and Pierluigi San Pietro.
2012. From regular to strictly locally testable lan-
guages. International Journal of Foundations of
Computer Science, 23(08):1711?1727.
C. C. Elgot and J. E. Mezei. 1965. On relations de-
fined by generalized finite automata. IBM Journal
of Research and Development, 9(1):47?68.
Sara Finley. 2008. The formal and cognitive restric-
tions on vowel harmony. Ph.D. thesis, Johns Hop-
kins University, Baltimore, MD.
Brian Gainor, Regine Lai, and Jeffrey Heinz. 2012.
Computational characterizations of vowel harmony
patterns and pathologies. In The Proceedings of the
29th West Coast Conference on Formal Linguistics,
pages 63?71.
Daniel Gildea and Daniel Jurafsky. 1996. Learn-
ing bias and phonological-rule induction. Compu-
tational Linguistics, 24(4).
Bruce Hayes. 2009. Introductory Phonology. Wiley-
Blackwell.
Jeffrey Heinz and William Idsardi. 2011. Sentence
and word complexity. Science, 333(6040):295?297,
July.
Jeffrey Heinz and William Idsardi. 2013. What
complexity differences reveal about domains in lan-
guage. Topics in Cognitive Science, 5(1):111?131.
Jeffrey Heinz, Chetan Rawal, and Herbert G. Tan-
ner. 2011. Tier-based strictly local constraints for
phonology. In Proceedings of the 49th AnnualMeet-
ing of the Association for Computational Linguis-
tics, pages 58?64, Portland, Oregon, USA, June. As-
sociation for Computational Linguistics.
Larry Hyman. 1970. How concrete is phonology?
Language, 46(1):58?76.
Larry Hyman. 1975. Phonology: Theory and Analysis.
Holt, Rinehart and Winston.
C. Douglas Johnson. 1972. Formal Aspects of Phono-
logical Description. The Hague: Mouton.
Ronald Kaplan and Martin Kay. 1994. Regular models
of phonological rule systems. Computational Lin-
guistics, 20(3):331?378.
Andre? Kempe. 2000. Reduction of intermediate al-
phabets in finite-state transducer cascades. CoRR,
cs.CL/0010030.
Michael Kenstowicz. 1994. Phonology in Generative
Grammar. Blackwell Publishers.
Kimmo Koskenniemi. 1983. Two-level morphology.
Publication no. 11, Department of General Linguis-
tics. Helsinki: University of Helsinki.
Martin Kra?mer. 2003. Vowel Harmony and Corre-
spondence Theory. Berlin: Mouton de Gruyter.
Regine Lai. 2012. Domain Specificity in Phonology.
Ph.D. thesis, University of Delaware.
Linda Lombardi. 1999. Positional faithfulness and
voicing assimilation in Optimality Theory. Natural
Language and Linguistic Theory, 17:267?302.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269?311.
Jose? Oncina, Pedro Garc??a, and Enrique Vidal.
1993. Learning subsequential transducers for pat-
tern recognition tasks. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 15:448?
458, May.
Jaye Padgett. 1995. Partial class behavior and nasal
place assimilation. In K. Suzuki and D. Elzinga, ed-
itors, Proceedings of the 1995 Southwestern Work-
shop on Optimality Theory.
Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in Generative Gram-
mar. Blackwell Publishing.
Jason Riggle. 2004. Generation, Recognition, and
Learning in Finite State Optimality Theory. Ph.D.
thesis, University of California, Los Angeles.
Emmanuel Roche and Yves Schabes. Finite-State Lan-
guage Processing. MIT Press.
62
James Rogers, Jeffrey Heinz, Margaret Fero, Jeremy
Hurst, Dakotah Lambert, and Sean Wibel. to appear.
Cognitive and sub-regular complexity. In Proceed-
ings of the 17th Conference on Formal Grammar.
Jaques Sakarovitch. 2009. Elements of Automata The-
ory. Cambridge University Press. Translated by
Reuben Thomas from the 2003 edition published by
Vuibert, Paris.
Colin Wilson. 2003. Analyzing unbounded spread-
ing with constraints: marks, targets, and derivations.
Unpublished manuscript, UCLA.
Colin Wilson. 2004. Experimental investigation of
phonological naturalness. In Proceedings of WC-
CFL 22, pages 534?546.
Anssi Yli-Jyra? and Kimmo Koskenniemi. 2006. Com-
piling generalized two-level rules and grammars. In
FinTAL, volume 4139 of Lecture Notes in Artifi-
cial Intelligence, pages 174?185. Springer-Verlag,
Berlin.
63
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 64?71,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Learning Subregular Classes of Languages with Factored Deterministic
Automata
Jeffrey Heinz
Dept. of Linguistics and Cognitive Science
University of Delaware
heinz@udel.edu
James Rogers
Dept. of Computer Science
Earlham College
jrogers@cs.earlham.edu
Abstract
This paper shows how factored finite-
state representations of subregular lan-
guage classes are identifiable in the limit
from positive data by learners which are
polytime iterative and optimal. These rep-
resentations are motivated in two ways.
First, the size of this representation for
a given regular language can be expo-
nentially smaller than the size of the
minimal deterministic acceptor recogniz-
ing the language. Second, these rep-
resentations (including the exponentially
smaller ones) describe actual formal lan-
guages which successfully model natural
language phenomenon, notably in the sub-
field of phonology.
1 Introduction
In this paper we show how to define certain sub-
regular classes of languages which are identifiable
in the limit from positive data (ILPD) by efficient,
well-behaved learners with a lattice-structured hy-
pothesis space (Heinz et al, 2012). It is shown
that every finite set of DFAs defines such an ILPD
class. In this case, each DFA can be viewed as
one factor in the description of every language in
the class. This factoring of language classes into
multiple DFA can provide a compact, canonical
representation of the grammars for every language
in the class. Additionally, many subregular classes
of languages can be learned by the above methods
including the Locally k-Testable, Strictly k-Local,
Piecewise k-Testable, and Strictly k-Piecewise
languages (McNaughton and Papert, 1971; Rogers
and Pullum, 2011; Rogers et al, 2010). From a
linguistic (and cognitive) perspective, these sub-
regular classes are interesting because they appear
to be sufficient for modeling phonotactic patterns
in human language (Heinz, 2010; Heinz et al,
2011; Rogers et al, to appear).
2 Preliminaries
For any function f and element a in the domain of
f , we write f(a)? if f(a) is defined, f(a)?= x if
it is defined for a and its value is x, and f(a) ?
otherwise. The range of f , the set of values f
takes at elements for which it is defined, is denoted
range(f).
?? and ?k denote all sequences of any finite
length, and of length k, over a finite alphabet ?.
The empty string is denoted ?. A language L is a
subset of ??.
For all x, y belonging to a partially-ordered set
(S,?), if x ? z and y ? z then z is an upper
bound of x and y. For all x, y ? S, the least upper
bound (lub) x? y = z iff x ? z, y ? z, and for all
z? which upper bound x and y, it is the case that
z ? z?. An upper semi-lattice is a partially ordered
set (S,?) such that every subset of S has a lub. If
S is finite, this is equivalent to the existence of
x ? y for all x, y ? S.
A deterministic finite-state automaton (DFA) is
a tuple (Q,?, Q0, F, ?). The states of the DFA are
Q; the input alphabet is ?; the set of initial states
is Q0; the final states are F ; and ? : Q? ? ? Q
is the transition function.
We admit a set of initial states solely to accom-
modate the empty DFA, which has none. Deter-
ministic automata never have more than one ini-
tial state. We will assume that, if the automaton is
non-empty, then Q0 = {q0};
The transition function?s domain is extended to
Q? ?? in the usual way.
The language of a DFA A is
L(A) def= {w ? ?? | ?(q0, w)?? F}.
A DFA is trim iff it has no useless states:
(?q ? Q)[ ?w, v ? ?? |
?(q0, w)?= q and ?(q, v)?? F ].
64
Every DFA can be trimmed by eliminating useless
states from Q and restricting the remaining com-
ponents accordingly.
The empty DFA isA? = (?,?,?,?,?). This
is the minimal trim DFA such that L(A?) = ?.
The DFA product of A1 = (Q1,?, Q01, F1, ?1)
and A2 = (Q2,?, Q02, F2, ?2) is
?(A1,A2) = (Q,?, Q0, F, ?)
where Q = Q1 ? Q2, Q0 = Q01 ? Q02,
F = F1 ? F2 and
(?q ? Q)(?? ? ?)
[
?
(
(q1, q2), ?
) def= (?1(q1, ?), ?2(q2, ?))
]
The DFA product of two DFA is also a DFA. It
is not necessarily trim, but we will generally as-
sume that in taking the product the result has been
trimmed, as well.
The product operation is associative and com-
mutative (up to isomorphism), and so it can be ap-
plied to a finite set S of DFA, in which case we
write
?
S = ?A?S A (letting
?{A} = A). In
this paper, grammars are finite sequences of DFAs
~A = ?A1 ? ? ? An? and we also use the
?
nota-
tion for the product of a finite sequence of DFAs:
? ~A def= ?A? ~AA and L( ~A)
def= L
(
? ~A
)
. Se-
quences are used instead of sets in order to match
factors in two grammars. Let DFA denote the
collection of finite sequences of DFAs.
Theorem 1 is well-known.
Theorem 1 Consider a finite set S of DFA. Then
L
(
?
A?S A
)
= ?A?S L(A).
An important consequence of Theorem 1 is that
some languages are exponentially more com-
pactly represented by their factors. The grammar
~A = ?A1 ? ? ? An? has
?
1?i?n card(Qi) states,
whereas the trimmed
? ~A can have as many as
?
1?i?n card(Qi) ? ?(max1?i?n(card(Qi))n)
states. An example of such a language is given
in Section 4, Figures 1 and 2.
2.1 Identification in the limit
A positive text T for a language L is a total
function T : N ? L ? {#} (# is a ?pause?)
such that range(T ) = L (i.e., for every w ? L
there is at least one n ? N for which w =
T (n)). Let T [i] denote the initial finite sequence
T (0), T (1) . . . T (i ? 1). Let SEQ denote the set
of all finite initial portions of all positive texts for
all possible languages. The content of an element
T [i] of SEQ is
content(T [i]) def=
{w ? ?? | (?j ? i? 1)[T (j) = w]}.
In this paper, learning algorithms are programs:
? : SEQ ? DFA. A learner ? identifies in the
limit from positive texts a collection of languages
L if and only if for all L ? L, for all positive texts
T for L, there exists an n ? N such that
(?m ? n)[?(T [m]) = ?(T [n])] and L(T [n]) = L
(see Gold (1967) and Jain et al (1999)). A class of
languages is ILPD iff it is identifiable in the limit
by such a learner.
3 Classes of factorable-DFA languages
In this section, classes of factorable-DFA lan-
guages are introduced. The notion of sub-DFA is
central to this concept. Pictorially, a sub-DFA is
obtained from a DFA by removing zero or more
states, transitions, and/or revoking the final status
of zero or more final states.
Definition 1 For any DFA A = (Q,?, Q0, F, ?),
a DFA A? = (Q?,??, Q?0, F ?, ??) is sub-DFA of A,
written A? ? A, if and only if Q? ? Q, ? ? ??,
Q?0 ? Q0, F ? ? F , ?? ? ?.
The sub-DFA relation is extended to grammars
(sequences of DFA). Let ~A = ?A1 ? ? ? An? and
~A? = ?A?1 ? ? ? A?n?.
Then ~A? ? ~A ? (?0 ? i ? n)[A?i ? Ai].
Clearly, if A? ? A then L(A?) ? L(A).
Every grammar ~A determines a class of lan-
guages: those recognized by a sub-grammar of ~A.
Our interest is not in L( ~A), itself. Indeed, this will
generally be ??. Rather, our interest is in identi-
fying languages relative to the class of languages
recognizable by sub-grammars of ~A.
Definition 2 Let G( ~A) def= {~B | ~B ? ~A}, the class
of grammars that are sub-grammars of ~A.
Let L( ~A) def= {L( ~B) | ~B ? ~A}, the class of lan-
guages recognized by sub-grammars of ~A.
A class of languages is a factorable-DFA class
iff it is L( ~A) for some ~A.
The set G( ~A) is necessarily finite, since ~A is, so
every class L( ~A) is trivially ILPD by a learning
algorithm that systematically rules out grammars
that are incompatible with the text, but this na??ve
algorithm is prohibitively inefficient. Our goal is
65
to establish that the efficient general learning algo-
rithm given by Heinz et al (2012) can be applied
to every class of factorable-DFA languages, and
that this class includes many of the well-known
sub-regular language classes as well as classes that
are, in a particular sense, mixtures of these.
4 A motivating example
This section describes the Strictly 2-Piecewise lan-
guages, which motivate the factorization that is
at the heart of this analysis. Strictly Piecewise
(SP) languages are characterized in Rogers et al
(2010) and are a special subclass of the Piecewise
Testable languages (Simon, 1975).
Every SP language is the intersection of a finite
set of complements of principal shuffle ideals:
L ? SP def?? L =
?
w?S
[SI(w)], S finite
where
SI(w) def= {v ? ?? | w = ?1 ? ? ? ?k and
(?v0, . . . , vk ? ??)[v = v0 ? ?1 ? v1 ? ? ? ?k ? vk]}
So v ? SI(w) iff w occurs as a subsequence of v
and L ? SP iff there is a finite set of strings for
which L includes all and only those strings that
do not include those strings as subsequences. We
say that L is generated by S. It turns out that SP is
exactly the class of languages that are closed under
subsequence.
A language is SPk iff it is generated by a set of
strings each of which is of length less than or equal
to k. Clearly, every SP language is SPk for some
k and SP = ?1?k?N[SPk].
If w ? ?? and |w| = k, then SI(w) = L(Aw)
for a DFA Aw with no more than k states. For
example, if k = 2 and ? = {a, b, c} and, hence,
w ? {a, b, c}2, then the minimal trim DFA recog-
nizing SI(w) will be a sub-DFA (in which one of
the transitions from the ?1 state has been removed)
of one of the three DFA of Figure 1.
Figure 1 shows ~A = ?Aa, Ab, Ac?, where ? =
{a, b, c} and each A? is a DFA accepting ??
whose states distinguish whether ? has yet oc-
curred. Figure 2 shows
? ~A.
Note that every SP2 language over {a, b, c} is
L( ~B) for some ~B ? ~A. The class of grammars
of G( ~A) recognize a slight extension of SP2 over
{a, b, c} (which includes 1-Reverse Definite lan-
guages as well).
Observe that 6 states are required to describe ~A
but 8 states are required to describe
? ~A. Let ~A?
be the sequence of DFA with one DFA for each
letter in ?, as in Figure 1. As card(?) increases
the number of states of ~A? is 2 ? card(?) but
the number of states in
? ~A? is 2card(?). The
number of states in the product, in this case, is ex-
ponential in the number of its factors.
The Strictly 2-Piecewise languages are cur-
rently the strongest computational characteriza-
tion1 of long-distance phonotactic patterns in hu-
man languages (Heinz, 2010). The size of the
phonemic inventories2 in the world?s languages
ranges from 11 to 140 (Maddieson, 1984). English
has about 40, depending on the dialect. With an al-
phabet of that size ~A? would have 80 states, while
? ~A? would have 240 ? 1 ? 1012 states. The
fact that there are about 1011 neurons in human
brains (Williams and Herrup, 1988) helps moti-
vate interest in the more compact, parallel repre-
sentation given by ~A? as opposed to the singular
representation of the DFA
? ~A?.
5 Learning factorable classes of
languages
In this section, classes of factorable-DFA lan-
guages are shown to be analyzable as finite lattice
spaces. By Theorem 6 of Heinz et al (2012), ev-
ery such class of languages can be identified in the
limit from positive texts.
Definition 3 (Joins) Let
A = (Q,?, Q0, F, ?),
A1 = (Q1,?, Q01, F1, ?1) ? A
and
A2 = (Q2,?, Q02, F2, ?2) ? A.
The join of A1 and A2 is
A1?A2
def= (Q1?Q2,?, Q01?Q02, F1?F2, ?1??2).
Similarly, for all ~A = ?A1 ? ? ? An? and ~B =
?B1 ? ? ? Bn? ? ~A, ~C2 = ?C1 ? ? ? Cn? ? ~A, the join
of and ~B and ~C is ~B ? ~C def= ?B1 ? C1 ? ? ? Bn ? Cn?.
Note that the join of two sub-DFA of A is also a
sub-DFA of A. Since G( ~A) is finite, binary join
suffices to define join of any set of sub-DFA of a
given DFA (as iterated binary joins). Let
?[S] be
the join of S, a set of sub-DFAs of some A (or ~A).
1See Heinz et al (2011) for competing characterizations.
2The mental representations of speech sounds are called
phonemes, and the phonemic inventory is the set of these rep-
resentations (Hayes, 2009).
66
a0 a1 b0 b1 c0 c1
b, c
a
a, b, c a, c
b
a, b, c a, b
c
a, b, c
Figure 1: The sequence of DFA ~A = ?Aa, Ab, Ac?, where ? = {a, b, c} and each A? accepts ?? and
whose states distinguish whether ? has yet occurred.
a0b0c0
a1b0c0
a0b1c0
a0b0c1
a1b1c0
a0b1c1
a1b0c1
a1b1c1
a
b
c
b
c
a
a
c
b
a
b
c
c
a, b
ab, c
b
a, c
a, b, c
Figure 2: The product
??Aa, Ab, Ac?.
67
Lemma 1 The set of sub-DFA of a DFA A, or-
dered by ?, ({B | B ? A},?), is an upper semi-
lattice with the least upper bound of a set of S sub-
DFA of A being their join.
Similarly the set of sub-grammars of a grammar
~A, ordered again by?, ({~B ? ~A},?), is an upper
semi-lattice with the least upper bound of a set of
sub-grammars of ~A being their join.3
This follows from the fact that Q1 ?Q2 (similarly
F1 ?F2 and ?1 ? ?2) is the lub of Q1 and Q2 (etc.)
in the lattice of sets ordered by subset.
5.1 Paths and Chisels
Definition 4 LetA = (Q,?, {q0}, F, ?) be a non-
empty DFA and w = ?0?1 ? ? ? ?n ? ??.
If ?(q0, w)?, the path of w in A is the sequence
?(A, w) def=
?
(q0, ?0), . . . , (qn, ?n), (qn+1, ?)
?
where (?0 ? i ? n)[qi+1 = ?(qi, ?i)].
If ?(q0, w)? then ?(A, w)?.
If ?(A, w)?, let Q?(A,w) denote set of states it
traverses, ??(A,w) denote the the transitions it tra-
verses, and let F?(A,w) = {qn+1}.
Next, for any DFA A, and any w ? L(A), we
define the chisel of w given A to be the sub-DFA
of A that exactly encompasses the path etched out
in A by w.
Definition 5 For any non-empty DFA A =
(Q,?, {q0}, F, ?) and all w ? ??, if w ? L(A),
then the chisel of w given A is the sub-DFA
CA(w) = (Q?(A,w),?, {q0}, F?(A,w), ??(A,w)).
If w 6? L(A), then CA(w) = A?.
Consider any ~A = ?A1 ? ? ? An? and any word
w ? ??. The chisel of w given ~A is C ~A(w) =
?CA1(w) ? ? ?CAn(w)?.
Observe that CA(w) ? A for all words w and all
A, and that CA(w) is trim.
Using the join, the domain of the chisel is ex-
tended to sets of words: C ~A(S) =
?
w?S C ~A(w).
Note that {C ~A(w) | w ? ??} is finite, since
{~B | ~B ? ~A} is.
Theorem 2 For any grammar ~A, let C( ~A) =
{C ~A(S) | S ? ??}. Then (C( ~A),?) is an up-
per semi-lattice with the lub of two elements given
by the join ?.
3These are actually complete finite lattices, but we are in-
terested primarily in the joins.
Proof This follows immediately from the finite-
ness of {C ~A(w) | w ? ??} and Lemma 1. 
Lemma 2 For all A = (Q,?, Q0, F, ?), there is
a finite set S ? ?? such that
?
w?S CA(w) = A.
Similarly, for all ~A = ?A1 ? ? ? An?, there is a finite
set S ? ?? such that C ~A(S) = ~A.
Proof If A is empty, then clearly S = ? suffices.
Henceforth consider only nonempty A.
For the first statement, let S be the set of u?v
where, for each q ? Q and for each ? ? ?,
?(q0, u) ?= q and ?(?(q, ?), v) ?? F such that
u?v has minimal length. By construction, S is fi-
nite. Furthermore, for every state and every transi-
tion in A, there is a word in S whose path touches
that state and transition. By definition of ? it fol-
lows that CA(S) = A.
For proof of the second statement, for each Ai
in ~A, construct Si as stated and take their union. 
Heinz et al (2012) define lattice spaces. For an
upper semi-lattice V and a function f : ?? ? V
such that f and ? are (total) computable, (V, f) is
called a Lattice Space (LS) iff, for each v ? V ,
there exists a finite D ? range(f) with?D = v.
Theorem 3 For all grammars ~A = ?A1 ? ? ? An?,
(C( ~A), C ~A) is a lattice space.
Proof For all ~A? ? C( ~A), by Lemma 2, there is a
finite S ? ?? such that ?w?S C ~A(w) = ~A?. 
For Heinz et al (2012), elements of the lat-
tice are grammars. Likewise, here, each grammar
~A = ?A1 ? ? ? An? defines a lattice whose elements
are its sub-grammars. Heinz et al (2012) associate
the languages of a grammar v in a lattice space
(V, f) with {w ? ?? | f(w) ? v}. This definition
coincides with ours: for any element ~A? of C( ~A)
(note ~A? ? ~A), a word w belongs to L( ~A?) if and
only if C ~A(w) is a sub-DFA of ~A?. The class of
languages of a LS is the collection of languages
obtained by every element in the lattice. For ev-
ery LS (C( ~A), C ~A), we now define a learner ? ac-
cording to the construction in Heinz et al (2012):
?T ? SEQ, ?(T ) = ?w?content(T ) C ~A(w).
Let L(C( ~A),C ~A) denote the class of languages
associated with the LS in Theorem 3. Accord-
ing to Heinz et al (2012, Theorem 6), the learner
? identifies L(C( ~A),CvA) in the limit from posi-
tive data. Furthermore, ? is polytime iterative,
68
i.e can compute the next hypothesis in polytime
from the previous hypothesis alone, and opti-
mal in the sense that no other learner converges
more quickly on languages in L(C( ~A),CG). In ad-
dition, this learner is globally-consistent (every
hypothesis covers the data seen so far), locally-
conservative (the hypothesis never changes unless
the current datum is not consistent with the cur-
rent hypothesis), strongly-monotone (the current
hypothesis is a superset of all prior hypotheses),
and prudent (it never hypothesizes a language that
is not in the target class). Formal definitions of
these terms are given in Heinz et al (2012) and can
also be found elsewhere, e.g. Jain et al (1999).
6 Complexity considerations
The space of sub-grammars of a given sequence of
DFAs is necessarily finite and, thus, identifiable in
the limit from positive data by a na??ve learner that
simply enumerates the space of grammars. The
lattice learning algorithm has better efficiency be-
cause it works bottom-up, extending the grammar
minimally, at each step, with the chisel of the cur-
rent string of the text. The lattice learner never
explores any part of the space of grammars that
is not a sub-grammar of the correct one and, as it
never moves down in the lattice, it will skip much
of the space of grammars that are sub-grammars of
the correct one. The space it explores will be mini-
mal, given the text it is running on. Generalization
is a result of the fact that in extending the gram-
mar for a string the learner adds its entire Nerode
equivalence class to the language.
The time complexity of either learning or recog-
nition with the factored automata may actually be
somewhat worse than the complexity of doing so
with its product. Computing the chisel of a string
w in the product machine of Figure 2 is ?(|w|),
while in the factored machine of Figure 1 one must
compute the chisel in each factor and its complex-
ity is, thus, ?(|w| card(?)k?1). But ? and k are
fixed for a given factorization, so this works out to
be a constant factor.
Where the factorization makes a substantial dif-
ference is in the number of features that must
be learned. In the factored grammar of the
example, the total number of states plus edges
is ?(kcard(?)k?1), while in its product it is
?(2(card(?)k?1)). This represents an exponential
improvement in the space complexity of the fac-
tored grammar.
Every DFA can be factored in many ways, but
the factorizations do not necessarily provide an
asymptotically significant improvement in space
complexity. The canonical contrast is between
sequences of automata ?A1, . . . ,An? that count
modulo some sequence of mi ? N. If the
mi are pairwise prime, the product will require
?
1?i?n[mi] = ?((maxi[mi])n) states. If on the
other hand, they are all multiples of each other it
will require just ?(maxi[mi]).
7 Examples
The fact that the class of SP2 languages is effi-
ciently identifiable in the limit from positive data
is neither surprising or new. The obvious ap-
proach to learning these languages simply accu-
mulates the set of pairs of symbols that occur as
subsequences of the strings in the text and builds a
machine that accepts all and only those strings in
which no other such pairs occur. This, in fact, is
essentially what the lattice learner is doing.
What is significant is that the lattice learner pro-
vides a general approach to learning any language
class that can be captured by a factored grammar
and, more importantly, any class of languages that
are intersections of languages that are in classes
that can be captured this way.
Factored grammars in which each factor recog-
nizes ??, as in the case of Figure 1, are of par-
ticular interest. Every sub-Star-Free class of lan-
guages in which the parameters of the class (k, for
example) are fixed can be factored in this way.4 If
the parameters are not fixed and the class of lan-
guages is not finite, none of these classes can be
identified in the limit from positive data at all.5 So
this approach is potentially useful at least for all
sub-Star-Free classes. The learners for non-strict
classes are practical, however, only for small val-
ues of the parameters. So that leaves the Strictly
Local SLk and Strictly Piecewise SPk languages
as the obvious targets.
The SLk languages are those that are deter-
mined by the substrings of length no greater than
k that occur within the string (including endmark-
4We conjecture that there is a parameterized class of lan-
guages that is equivalent to the Star-Free languages, which
would make that class learnable in this way as well.
5For most of these classes, including the Definite,
Reverse-Definite and Strictly Local classes and their super
classes, this is immediate from the fact that they are super-
finite. SP, on the other hand, is not super-finite (since it
does not include all finite languages) but nevertheless, it is
not IPLD.
69
ers). These can be factored on the basis of those
substrings, just as the SPk languages can, although
the construction is somewhat more complex. (See
the Knuth-Morris-Pratt algorithm (Knuth et al,
1977) for a way of doing this.) But SLk is a case in
which there is no complexity advantage in factor-
ing the DFA. This is because every SLk language
is recognized by a DFA that is a Myhill graph:
with a state for each string of ?<k (i.e., of length
less than k). Such a graph has ?(card(?)k?1)
states, asymptotically the same as the number of
states in the factored grammar, which is actually
marginally worse.
Therefore, factored SLk grammars are not, in
themselves, interesting. But they are interesting as
factors of other grammars. Let (SL+ SP)k,l (resp.
(LT + SP)k,l, (SL + PT)k,l) be the class of lan-
guages that are intersections of SLk and SPl (resp.
LTk and SPl, SLk and PTl) languages. Where
LT (PT) languages are determined by the set of
substrings (subsequences) that occur in the string
(see Rogers and Pullum (2011) and Rogers et al
(2010)).
These classes capture co-occurrence of lo-
cal constraints (based on adjacency) and long-
distance constraints (based on precedence). These
are of particular interest in phonotactics, as they
are linguistically well-motivated approaches to
modeling phonotactics and they are sufficiently
powerful to model most phonotactic patterns. The
results of Heinz (2007) and Heinz (2010) strongly
suggest that nearly all segmental patterns are
(SL+ SP)k,l for small k and l. Moreover, roughly
72% of the stress patterns that are included in
Heinz?s database (Heinz, 2009; Phonology Lab,
2012) of patterns that have been attested in nat-
ural language can be modeled with SLk grammars
with k ? 6. Of the rest, all but four are LT1 + SP4
and all but two are LT2 + SP4. Both of these last
two are properly regular (Wibel et al, in prep).
8 Conclusion
We have shown how subregular classes of lan-
guages can be learned over factored representa-
tions, which can be exponentially more compact
than representations with a single DFA. Essen-
tially, words in the data presentation are passed
through each factor, ?activating? the parts touched.
This approach immediately allows one to natu-
rally ?mix? well-characterized learnable subreg-
ular classes in such a way that the resulting lan-
guage class is also learnable. While this mixing is
partly motivated by the different kinds of phono-
tactic patterns in natural language, it also suggests
a very interesting theoretical possibility. Specifi-
cally, we anticipate that the right parameterization
of these well-studied subregular classes will cover
the class of star-free languages. Future work could
also include extending the current analysis to fac-
toring stochastic languages, perhaps in a way that
connects with earlier research on factored HMMs
(Ghahramani and Jordan, 1997).
Acknowledgments
This paper has benefited from the insightful com-
ments of three anonymous reviewers, for which
the authors are grateful. The authors also thank
Jie Fu and Herbert G. Tanner for useful discus-
sion. This research was supported by NSF grant
1035577 to the first author, and the work was com-
pleted while the second author was on sabbatical at
the Department of Linguistics and Cognitive Sci-
ence at the University of Delaware.
References
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2):245?273.
E.M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447?474.
Bruce Hayes. 2009. Introductory Phonology. Wiley-
Blackwell.
Jeffrey Heinz, Chetan Rawal, and Herbert G. Tan-
ner. 2011. Tier-based strictly local constraints for
phonology. In Proceedings of the 49th AnnualMeet-
ing of the Association for Computational Linguis-
tics, pages 58?64, Portland, Oregon, USA, June. As-
sociation for Computational Linguistics.
Jeffrey Heinz, Anna Kasprzik, and Timo Ko?tzing.
2012. Learning with lattice-structured hypothesis
spaces. Theoretical Computer Science, 457:111?
127, October.
Jeffrey Heinz. 2007. The Inductive Learning of
Phonotactic Patterns. Ph.D. thesis, University of
California, Los Angeles.
Jeffrey Heinz. 2009. On the role of locality in learning
stress patterns. Phonology, 26(2):303?351.
Jeffrey Heinz. 2010. Learning long-distance phono-
tactics. Linguistic Inquiry, 41(4):623?661.
70
Sanjay Jain, Daniel Osherson, James S. Royer, and
Arun Sharma. 1999. Systems That Learn: An In-
troduction to Learning Theory (Learning, Develop-
ment and Conceptual Change). The MIT Press, 2nd
edition.
Donald Knuth, James H Morris, and Vaughn Pratt.
1977. Fast pattern matching in strings. SIAM Jour-
nal on Computing, 6(2):323?350.
Ian Maddieson. 1984. Patterns of Sounds. Cambridge
University Press, Cambridge, UK.
Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.
UD Phonology Lab. 2012. UD phonology lab
stress pattern database. http://phonology.
cogsci.udel.edu/dbs/stress. Accessed
December 2012.
James Rogers and Geoffrey Pullum. 2011. Aural pat-
tern recognition experiments and the subregular hi-
erarchy. Journal of Logic, Language and Informa-
tion, 20:329?342.
James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Visscher, David Wellcome, and Sean
Wibel. 2010. On languages piecewise testable in the
strict sense. In Christian Ebert, Gerhard Ja?ger, and
Jens Michaelis, editors, The Mathematics of Lan-
guage, volume 6149 of Lecture Notes in Artifical In-
telligence, pages 255?265. Springer.
James Rogers, Jeffrey Heinz, Margaret Fero, Jeremy
Hurst, Dakotah Lambert, and Sean Wibel. to appear.
Cognitive and sub-regular complexity. In Proceed-
ings of the 17th Conference on Formal Grammar.
Imre Simon. 1975. Piecewise testable events. In
Automata Theory and Formal Languages: 2nd
Grammatical Inference conference, pages 214?222,
Berlin. Springer-Verlag.
Sean Wibel, James Rogers, and Jeffery Heinz. Factor-
ing of stress patterns. In preparation.
R.W. Williams and K. Herrup. 1988. The control of
neuron number. Annual Review of Neuroscience,
11:423?453.
71
