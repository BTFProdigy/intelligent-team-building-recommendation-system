Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 208?215,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Humor: Prosody Analysis and Automatic Recognition
for F * R * I * E * N * D * S *
Amruta Purandare and Diane Litman
Intelligent Systems Program
University of Pittsburgh
famruta,litmang@cs.pitt.edu
Abstract
We analyze humorous spoken conversa-
tions from a classic comedy television
show, FRIENDS, by examining acoustic-
prosodic and linguistic features and their
utility in automatic humor recognition.
Using a simple annotation scheme, we au-
tomatically label speaker turns in our cor-
pus that are followed by laughs as hu-
morous and the rest as non-humorous.
Our humor-prosody analysis reveals sig-
nificant differences in prosodic character-
istics (such as pitch, tempo, energy etc.)
of humorous and non-humorous speech,
even when accounted for the gender and
speaker differences. Humor recognition
was carried out using standard supervised
learning classifiers, and shows promising
results significantly above the baseline.
1 Introduction
As conversational systems are becoming preva-
lent in our lives, we notice an increasing need for
adding social intelligence in computers. There has
been a considerable amount of research on incor-
porating affect (Litman and Forbes-Riley, 2004)
(Alm et al, 2005) (D?Mello et al, 2005) (Shroder
and Cowie, 2005) (Klein et al, 2002) and person-
ality (Gebhard et al, 2004) in computer interfaces,
so that, for instance, user frustrations can be rec-
ognized and addressed in a graceful manner. As
(Binsted, 1995) correctly pointed out, one way to
alleviate user frustrations, and to make human-
computer interaction more natural, personal and
interesting for the users, is to model HUMOR.
Research in computational humor is still in
very early stages, partially because humorous lan-
guage often uses complex, ambiguous and incon-
gruous syntactic and semantic expressions (At-
tardo, 1994) (Mulder and Nijholt, 2002) which re-
quire deep semantic interpretation. Nonetheless,
recent studies have shown a feasibility of auto-
matically recognizing (Mihalcea and Strapparava,
2005) (Taylor and Mazlack, 2004) and generating
(Binsted and Ritchie, 1997) (Stock and Strappar-
ava, 2005) humor in computer systems. The state
of the art research in computational humor (Bin-
sted et al, 2006) is, however, limited to text (such
as humorous one-liners, acronyms or wordplays),
and to our knowledge, there has been no work to
date on automatic humor recognition in spoken
conversations.
Before we can model humor in real application
systems, we must first analyze features that char-
acterize humor. Computational approaches to hu-
mor recognition so far primarily rely on lexical
and stylistic cues such as alliteration, antonyms,
adult slang (Mihalcea and Strapparava, 2005). The
focus of our study is, on the other hand, on ana-
lyzing acoustic-prosodic cues (such as pitch, in-
tensity, tempo etc.) in humorous conversations
and testing if these cues can help us to auto-
matically distinguish between humorous and non-
humorous (normal) utterances in speech. We hy-
pothesize that not only the lexical content but
also the prosody (or how the content is expressed)
makes humorous expressions humorous.
The following sections describe our data collec-
tion and pre-processing, followed by the discus-
sion of various acoustic-prosodic as well as other
types of features used in our humorous-speech
analysis and classification experiments. We then
present our experiments, results, and finally end
with conclusions and future work.
208
2 FRIENDS Corpus
(Scherer, 2003) discuss a number of pros and cons
of using real versus acted data, in the context of
emotional speech analysis. His main argument is
that while real data offers natural expressions of
emotions, it is not only hard to collect (due to eth-
ical issues) but also very challenging to annotate
and analyze, as there are very few instances of
strong expressions and the rest are often very sub-
tle. Acted data (also referred to as portrayed or
simulated), on the other hand, offers ample of pro-
totypical examples, although these are criticized
for not being natural at times. To achieve some
balance between naturalness and strength/number
of humorous expressions, we decided to use di-
alogs from a comedy television show FRIENDS,
which provides classical examples of casual, hu-
morous conversations between friends who often
discuss very real-life issues, such as job, career,
relationships etc.
We collected a total of 75 dialogs (scenes) from
six episodes of FRIENDS, four from Season I
(Monica Gets a New Roommate, The One with
Two Parts: Part 1 and 2, All the Poker) and two
from Season II (Ross Finds Out, The Prom Video),
all available on The Best of Friends Volume I
DVD. This gave us approximately 2 hrs of audio.
Text transcripts of these episodes were obtained
from: http://www.friendscafe.org/scripts.shtml,
and were used to extract lexical features (used later
in classification).
Figure 1 shows an excerpt from one of the di-
alogs in our corpus.
3 Audio Segmentation and Annotation
We segmented each audio file (manually) by mark-
ing speaker turn boundaries, using Wavesurfer
(http://www.speech.kth.se/wavesurfer). We apply
a fairly straightforward annotation scheme to au-
tomatically identify humorous and non-humorous
turns in our corpus. Speaker turns that are fol-
lowed by artificial laughs are labeled as Humor-
ous, and all the rest as Non-Humorous. For ex-
ample, in the dialog excerpt shown in figure 1,
turns 3, 7, 9, 11 and 16 are marked as humor-
ous, whereas turns 1, 2, 5, 6, 13, 14, 15 are
marked as non-humorous. Artificial laughs, si-
lences longer than 1 second and segments of au-
dio that contain purely non-verbal sounds (such
as phone rings, door bells, music etc.) were ex-
cluded from the analysis. By considering only
[1] Rachel: Guess what?
[2] Ross: You got a job?
[3] Rachel: Are you kidding? I am trained for
nothing!
[4] <Laughter>
[5] Rachel: I was laughed out of twelve inter-
views today.
[6] Chandler: And yet you?re surprisingly up-
beat.
[7] Rachel: You would be too if you found John
and David boots on sale, fifty percent off!
[8] <Laughter>
[9] Chandler: Oh, how well you know me...
[10] <Laughter>
[11] Rachel: They are my new, I don?t need a job,
I don?t need my parents, I got great boots, boots!
[12] <Laughter>
[13] Monica: How?d you pay for them?
[14] Rachel: Uh, credit card.
[15] Monica: And who pays for that?
[16] Rachel: Um... my... father.
[17] <Laughter>
Figure 1: Dialog Excerpt
speaker turns that are followed by laughs as hu-
morous, we also automatically eliminate cases of
pure visual comedy where humor is expressed us-
ing only gestures or facial expressions. In short,
non-verbal sounds or silences followed by laughs
are not treated as humorous. Henceforth, by
turn, we mean proper speaker turns (and not non-
verbal turns). We currently do not apply any spe-
cial filters to remove non-verbal sounds or back-
ground noise (other than laughs) that overlap with
speaker turns. However, if artificial laughs overlap
with a speaker turn (there were only few such in-
stances), the speaker turn is chopped by marking a
turn boundary exactly before/after the laughs be-
gin/end. This is to ensure that our prosody anal-
ysis is fair and does not catch any cues from the
laughs. In other words, we make sure that our
speaker turns are clean and not garbled by laughs.
After segmentation, we got a total of 1629
speaker turns, of which 714 (43.8%) are humor-
ous, and 915 (56.2%) are non-humorous. We also
made sure that there is a 1-to-1 correspondence be-
tween speaker turns in text transcripts that were
obtained online and our audio segments, and cor-
rected few cases where there was a mis-match (due
to turn-chopping or errors in online transcripts).
209
Figure 2: Audio Segmentation, Transcription and Feature Extraction using Wavesurfer
4 Speaker Distributions
There are 6 main actors/speakers (3 male and 3 fe-
male) in this show, along with a number of (in our
data 26) guest actors who appear briefly and rarely
in some of our dialogs. As the number of guest
actors is quite large, and their individual contribu-
tion is less than 5% of the turns in our data, we
decided to group all the guest actors together in
one GUEST class.
As these are acted (not real) conversations,
there were only few instances of speaker turn-
overlaps, where multiple speakers speak together.
These turns were given a speaker label MULTI. Ta-
ble 1 shows the total number of turns and humor-
ous turns for each speaker, along with their per-
centages in braces. Percentages for the Humor col-
umn show, out of the total (714) humorous turns,
how many are by each speaker. As one can notice,
the distribution of turns is fairly balanced among
the six main speakers. We also notice that even
though each guest actors? individual contribution
is less than 5% in our data, their combined contri-
bution is fairly large, almost 16% of the total turns.
Table 2 shows that the six main actors together
form a total of 83% of our data. Also, of the to-
tal 714 humorous turns, 615 (86%) turns are by
the main actors. To study if prosody of humor dif-
fers across males and females, we also grouped
the main actors into two gender classes. Table
2 shows that the gender distribution is fairly bal-
Speaker #Turns(%) #Humor (%)
Chandler (M) 244 (15) 163 (22.8)
Joey (M) 153 (9.4) 57 (8)
Monica (F) 219 (13.4) 74 (10.4)
Phoebe (F) 180 (11.1) 104 (14.6)
Rachel (F) 273 (16.8) 90 (12.6)
Ross (M) 288 (17.7) 127 (17.8)
GUEST (26) 263 (16.1) 95 (13.3)
MULTI 9 (0.6) 4 (0.6)
Table 1: Speaker Distribution
anced among the main actors, with 50.5% male
and 49.5% female turns. We also see that of the
685 male turns, 347 turns (almost 50%) are hu-
morous, and of the 672 female turns, 268 (ap-
proximately 40%) are humorous. Guest actors and
multi-speaker turns are not considered in the gen-
der analysis.
Speaker #Turns #Humor
Male 685 347
(50.5% of Main) (50.6% of Male)
Female 672 268
(49.5% Of Main) (39.9% of Female)
Total 1357 615
Main (83.3% of Total) (86.1% of Humor)
Table 2: Gender Distribution for Main Actors
210
5 Features
Literature in emotional speech analysis (Liscombe
et al, 2003)(Litman and Forbes-Riley, 2004)
(Scherer, 2003)(Ang et al, 2002) has shown that
prosodic features such as pitch, energy, speak-
ing rate (tempo) are useful indicators of emotional
states, such as joy, anger, fear, boredom etc. While
humor is not necessarily considered as an emo-
tional state, we noticed that most humorous ut-
terances in our corpus (and also in general) often
make use of hyper-articulations, similar to those
found in emotional speech.
For this study, we use a number of acoustic-
prosodic as well as some non acoustic-prosodic
features as listed below:
Acoustic-Prosodic Features:
 Pitch (F0): Mean, Max, Min, Range, Stan-
dard Deviation
 Energy (RMS): Mean, Max, Min, Range,
Standard Deviation
 Temporal: Duration, Internal Silence, Tempo
Non Acoustic-Prosodic Features:
 Lexical
 Turn Length (#Words)
 Speaker
Our acoustic-prosodic features make use of
the pitch, energy and temporal information in
the speech signal, and are computed using
Wavesurfer. Figure 2 shows Wavesurfer?s energy
(dB), pitch (Hz), and transcription (.lab) panes.
The transcription interface shows text correspond-
ing to the dialog turns, along with the turn bound-
aries. All features are computed at the turn level,
and essentially measure the mean, maximum, min-
imum, range (maximum-minimum) and standard
deviation of the feature value (F0 or RMS) over
the entire turn (ignoring zeroes). Duration is mea-
sured in terms of time in seconds, from the be-
ginning to the end of the turn including pauses
(if any) in between. Internal silence is measured
as the percentage of zero F0 frames, and essen-
tially account for the amount of silence in the turn.
Tempo is computed as the total number of sylla-
bles divided by the duration of the turn. For com-
puting the number of syllables per word, we used
the General Inquirer database (Stone et al, 1966).
Our lexical features are simply all words (alpha-
numeric strings including apostrophes and stop-
words) in the turn. The value of these features is
integral and essentially counts the number of times
a word is repeated in the turn. Although this indi-
rectly accounts for alliterations, in the future stud-
ies, we plan to use more stylistic lexical features
like (Mihalcea and Strapparava, 2005).
Turn length is measured as the number of words
in the turn. For our classification study, we con-
sider eight speaker classes (6 Main actors, 1 for
Guest and Multi) as shown in table 1, whereas for
the gender study, we consider only two speaker
categories (male and female) as shown in table 2.
6 Humor-Prosody Analysis
Feature Humor Non-Humor
Mean-F0 206.9 208.9
Max-F0* 299.8 293.5
Min-F0* 121.1 128.6
Range-F0* 178.7 164.9
StdDev-F0 41.5 41.1
Mean-RMS* 58.3 57.2
Max-RMS* 76.4 75
Min-RMS* 44.2 44.6
Range-RMS* 32.16 30.4
StdDev-RMS* 7.8 7.5
Duration* 3.18 2.66
Int-Sil* 0.452 0.503
Tempo* 3.21 3.03
Length* 10.28 7.97
Table 3: Humor Prosody: Mean feature values for
Humor and Non-Humor groups
Table 3 shows mean values of various acoustic-
prosodic features over all speaker turns in our data,
across humor and non-humor groups. Features
that have statistically (p<=0.05 as per indepen-
dent samples t-test) different values across the two
groups are marked with asterisks. As one can
see, all features except Mean-F0 and StdDev-F0
show significant differences across humorous and
non-humorous speech. Table 3 shows that humor-
ous turns in our data are longer, both in terms of
the time duration and the number of words, than
non-humorous turns. We also notice that humor-
ous turns have smaller internal silence, and hence
rapid tempo. Pitch (F0) and energy (RMS) fea-
tures have higher maximum, but lower minimum
211
values, for humorous turns. This in turn gives
higher values for range and standard deviation for
humor compared to the non-humor group. This re-
sult is somewhat consistent with previous findings
of (Liscombe et al, 2003) who found that most of
these features are largely associated with positive
and active emotional states such as happy, encour-
aging, confident etc. which are likely to appear in
our humorous turns.
7 Gender Effect on Humor-Prosody
To analyze prosody of humor across two genders,
we conducted a 2-way ANOVA test, using speaker
gender (male/female) and humor (yes/no) as our
fixed factors, and each of the above acoustic-
prosodic features as a dependent variable. The
test tells us the effect of humor on prosody ad-
justed for gender, the effect of gender on prosody
adjusted for humor and also the effect of interac-
tion between gender and humor on prosody (i.e.
if the effect of humor on prosody differs accord-
ing to gender). Table 4 shows results of 2-way
ANOVA, where Y shows significant effects, and
N shows non-significant effects. For example, the
result for tempo shows that tempo differs signifi-
cantly only across humor and non-humor groups,
but not across the two gender groups, and that
there is no effect of interaction between humor
and gender on tempo. As before, all features ex-
cept Mean-F0 and StdDev-F0 show significant dif-
ferences across humor and no-humor conditions,
even when adjusted for gender differences. The
table also shows that all features except inter-
nal silence and tempo show significant differences
across two genders, although only pitch features
(Max-F0, Min-F0, and StdDev-F0) show the ef-
fect of interaction between gender and humor. In
other words, the effect of humor on these pitch fea-
tures is dependent on gender. For instance, if male
speakers raise their pitch while expressing humor,
female speakers might lower. To confirm this,
we computed means values of various features for
males and females separately (See Tables 5 and
6). These tables indeed suggest that male speak-
ers show higher values for pitch features (Mean-
F0, Min-F0, StdDev-F0), while expressing humor,
whereas females show lower. Also for male speak-
ers, differences in Min-F0 and Min-RMS values
are not statistically significant across humor and
non-humor groups, whereas for female speakers,
features Mean-F0, StdDev-F0 and tempo do not
show significant differences across the two groups.
One can also notice that the differences in the
mean pitch feature values (specifically Mean-F0,
Max-F0 and Range-F0) between humor and non-
humor groups are much higher for males than for
females.
In summary, our gender analysis shows that al-
though most acoustic-prosodic features are differ-
ent for males and females, the prosodic style of ex-
pressing humor by male and female speakers dif-
fers only along some pitch-features (both in mag-
nitude and direction).
Feature Humor Gender Humor
x Gender
Mean-F0 N Y N
Max-F0 Y Y Y
Min-F0 Y Y Y
Range-F0 Y Y N
StdDev-F0 N Y Y
Mean-RMS Y Y N
Max-RMS Y Y N
Min-RMS Y Y N
Range-RMS Y Y N
StdDev-RMS Y Y N
Duration Y Y N
Int-Sil Y N N
Tempo Y N N
Length Y Y N
Table 4: Gender Effect on Humor Prosody: 2-Way
ANOVA Results
8 Speaker Effect on Humor-Prosody
We then conducted similar ANOVA test to account
for the speaker differences, i.e. by considering hu-
mor (yes/no) and speaker (8 groups as shown in ta-
ble 1) as our fixed factors and each of the acoustic-
prosodic features as a dependent variable for a 2-
Way ANOVA. Table 7 shows results of this analy-
sis. As before, the table shows the effect of humor
adjusted for speaker, the effect of speaker adjusted
for humor and also the effect of interaction be-
tween humor and speaker, on each of the acoustic-
prosodic features. According to table 7, we no
longer see the effect of humor on features Min-
F0, Mean-RMS and Tempo (in addition to Mean-
F0 and StdDev-F0), in presence of the speaker
variable. Speaker, on the other hand, shows sig-
nificant effect on prosody for all features. But
212
Feature Humor Non-Humor
Mean-F0* 188.14 176.43
Max-F0* 276.94 251.7
Min-F0 114.54 113.56
Range-F0* 162.4 138.14
StdDev-F0* 37.83 34.27
Mean-RMS* 57.86 56.4
Max-RMS* 75.5 74.21
Min-RMS 44.04 44.12
Range-RMS* 31.46 30.09
StdDev-RMS* 7.64 7.31
Duration* 3.1 2.57
Int-Sil* 0.44 0.5
Tempo* 3.33 3.1
Length* 10.27 8.1
Table 5: Humor Prosody for Male Speakers
surprisingly, again only pitch features Mean-F0,
Max-F0 and Min-F0 show the interaction effect,
suggesting that the effect of humor on these pitch
features differs from speaker to speaker. In other
words, different speakers use different pitch varia-
tions while expressing humor.
9 Humor Recognition by Supervised
Learning
We formulate our humor-recognition experiment
as a classical supervised learning problem, by
automatically classifying spoken turns into hu-
mor and non-humor groups, using standard ma-
chine learning classifiers. We used the decision
tree algorithm ADTree from Weka, and ran a
10-fold cross validation experiment on all 1629
turns in our data1. The baseline for these ex-
periments is 56.2% for the majority class (non-
humorous). Table 8 reports classification results
for six feature categories: lexical alone, lexical +
speaker, prosody alone, prosody + speaker, lexical
+ prosody and lexical + prosody + speaker (all).
Numbers in braces show the number of features
in each category. There are total 2025 features
which include 2011 lexical (all word types plus
turn length), 13 acoustic-prosodic and 1 for the
speaker information. Feature Length was included
in the lexical feature group, as it counts the num-
ber of lexical items (words) in the turn.
1We also tried other classifiers like Naive Bayes and Ad-
aBoost, although since the results were equivalent to ADTree,
we do not report those here.
Feature Humor Non-Humor
Mean-F0 235.79 238.75
Max-F0* 336.15 331.14
Min-F0* 133.63 143.14
Range-F0* 202.5 188
StdDev-F0 46.33 46.6
Mean-RMS* 58.44 57.64
Max-RMS* 77.33 75.57
Min-RMS* 44.08 44.74
Range-RMS* 33.24 30.83
StdDev-RMS* 8.18 7.59
Duration* 3.35 2.8
Int-Sil* 0.47 0.51
Tempo 3.1 3.1
Length* 10.66 8.25
Table 6: Humor Prosody for Female Speakers
All results are significantly above the baseline
(as measured by a pair-wise t-test) with the best
accuracy of 64% (8% over the baseline) obtained
using all features. We notice that the classifica-
tion accuracy improves on adding speaker infor-
mation to both lexical and prosodic features. Al-
though these results do not show a strong evidence
that prosodic features are better than lexical, it is
interesting to note that the performance of just a
few (13) prosodic features is comparable to that
of 2011 lexical features. Figure 3 shows the deci-
sion tree produced by the classifier in 10 iterations.
Numbers indicate the order in which the nodes are
created, and indentations mark parent-child rela-
tions. We notice that the classifier primarily se-
lected speaker and prosodic features in the first
10 iterations, whereas lexical features were se-
lected only in the later iterations (not shown here).
This seems consistent with our original hypothe-
sis that speech features are better at discriminating
between humorous and non-humorous utterances
in speech than lexical content.
Although (Mihalcea and Strapparava, 2005) ob-
tained much higher accuracies using lexical fea-
tures alone, it might be due to the fact that our data
is homogeneous in the sense that both humorous
and non-humorous turns are extracted from the
same source, and involve same speakers, which
makes the two groups highly alike and hence chal-
lenging to distinguish. To make sure that the
lower accuracy we get is not simply due to using
smaller data compared to (Mihalcea and Strappar-
213
Feature Humor Speaker Humor
x Speaker
Mean-F0 N Y Y
Max-F0 Y Y Y
Min-F0 N Y Y
Range-F0 Y Y N
StdDev-F0 N Y N
Mean-RMS N Y N
Max-RMS Y Y N
Min-RMS Y Y N
Range-RMS Y Y N
StdDev-RMS Y Y N
Duration Y Y N
Int-Sil Y Y N
Tempo N Y N
Length Y Y N
Table 7: Speaker Effect on Humor Prosody: 2-
Way ANOVA Results
Feature -Speaker +Speaker
Lex 61.14 (2011) 63.5 (2012)
Prosody 60 (13) 63.8 (14)
Lex + Prosody 62.6 (2024) 64 (2025)
Table 8: Humor Recognition Results (% Correct)
ava, 2005), we looked at the learning curve for the
classifier (see figure 4) and found that the classi-
fier performance is not sensitive to the amount of
data.
Table 9 shows classification results by gender,
using all features. For the male group, the base-
line is 50.6%, as the majority class humor is 50.6%
(See Table 2). For females, the baseline is 60%
(for non-humorous) as only 40% of the female
turns are humorous.
Gender Baseline Classifier
Male 50.6 64.63
Female 60.1 64.8
Table 9: Humor Recognition Results by Gender
As Table 9 shows, the performance of the classi-
fier is somewhat consistent cross-gender, although
for male speakers, the relative improvement is
much higher (14% above the baseline), than for
females (only 5% above the baseline). Our earlier
observation (from tables 5 and 6) that differences
in pitch features between humor and non-humor
j (1)SPEAKER = chandler: 0.469
j (1)SPEAKER != chandler: -0.083
j j (4)SPEAKER = phoebe: 0.373
j j (4)SPEAKER != phoebe: -0.064
j (2)DURATION < 1.515: -0.262
j j (5)SILENCE < 0.659: 0.115
j j (5)SILENCE >= 0.659: -0.465
j j (8)SD F0 < 9.919: -1.11
j j (8)SD F0 >= 9.919: 0.039
j (2)DURATION >= 1.515: 0.1
j j (3)MEAN RMS < 56.117: -0.274
j j (3)MEAN RMS >= 56.117: 0.147
j j j (7)come < 0.5: -0.056
j j j (7)come >= 0.5: 0.417
j j (6)SD F0 < 57.333: 0.076
j j (6)SD F0 >= 57.333: -0.285
j j (9)MAX RMS < 86.186: 0.011
j j j (10)MIN F0 < 166.293: 0.047
j j j (10)MIN F0 >= 166.293: -0.351
j j (9)MAX RMS >= 86.186: -0.972
Legend: +ve = humor, -ve = non-humor
Figure 3: Decision Tree (only the first 10 iterations
are shown)
groups are quite higher for males than for females,
may explain why we see higher improvement for
male speakers.
10 Conclusions
In this paper, we presented our experiments on
humor-prosody analysis and humor recognition
in spoken conversations, collected from a clas-
sic television comedy, FRIENDS. Using a sim-
ple automated annotation scheme, we labeled
speaker turns in our corpus that are followed
by artificial laughs as humorous, and the rest as
non-humorous. We then examined a number of
acoustic-prosodic features based on pitch, energy
and temporal information in the speech signal,
that have been found useful by previous studies in
emotion recognition.
Our prosody analysis revealed that humorous
and non-humorous turns indeed show significant
differences in most of these features, even when
accounted for the speaker and gender differences.
Specifically, we found that humorous turns tend
to have higher tempo, smaller internal silence, and
higher peak, range and standard deviation for pitch
and energy, compared to non-humorous turns.
On the humor recognition task, our classifier
214
Figure 4: Learning Curve: %Accuracy versus
%Fraction of Data
achieved the best performance when acoustic-
prosodic features were used in conjunction with
lexical and other types of features, and in all ex-
periments attained the accuracy statistically signif-
icant over the baseline. While prosody of humor
shows some differences due to gender, the perfor-
mance on the humor recognition task is equiva-
lent for males and females, although the relative
improvement over the baseline is much higher for
males than for females.
Our current study focuses only on lexical and
speech features, primarily because these features
can be computed automatically. In the future, we
plan to explore more sophisticated semantic and
pragmatic features such as incongruity, ambiguity,
expectation-violation etc. We also like to inves-
tigate if our findings generalize to other types of
corpora besides TV-show dialogs.
References
C. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion pre-
diction. In Proceedings of HLT/EMNLP, Vancou-
ver, CA.
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and
A. Stolcke. 2002. Prosody-based automatic de-
tection of annoyance and frustration in human-
computer dialog. In Proceedings of ICSLP.
S. Attardo. 1994. Linguistic Theory of Humor. Moun-
ton de Gruyter, Berlin.
K. Binsted and G. Ritchie. 1997. Computational rules
for punning riddles. Humor, 10(1).
K. Binsted, B. Bergen, S. Coulson, A. Nijholt,
O. Stock, C. Strapparava, G. Ritchie, R. Manurung,
H. Pain, A. Waller, and D. O?Mara. 2006. Com-
putational humor. IEEE Intelligent Systems, March-
April.
K. Binsted. 1995. Using humour to make natural lan-
guage interfaces more friendly. In Proceedings of
the AI, ALife and Entertainment Workshop, Mon-
treal, CA.
S. D?Mello, S. Craig, G. Gholson, S. Franklin, R. Pi-
card, and A. Graesser. 2005. Integrating affect sen-
sors in an intelligent tutoring system. In Proceed-
ings of Affective Interactions: The Computer in the
Affective Loop Workshop.
P. Gebhard, M. Klesen, and T. Rist. 2004. Color-
ing multi-character conversations through the ex-
pression of emotions. In Proceedings of Affective
Dialog Systems.
J. Klein, Y. Moon, and R. Picard. 2002. This computer
responds to user frustration: Theory, design, and re-
sults. Interacting with Computers, 14.
J. Liscombe, J. Venditti, and J. Hirschberg. 2003.
Classifying subject ratings of emotional speech us-
ing acoustic features. In Proceedings of Eurospeech,
Geneva, Switzerland.
D. Litman and K. Forbes-Riley. 2004. Predicting
student emotions in computer-human tutoring dia-
logues. In Proceedings of ACL, Barcelona, Spain.
R. Mihalcea and C. Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of HLT/EMNLP, Van-
couver, CA.
M. Mulder and A. Nijholt. 2002. Humor research:
State of the art. Technical Report 34, CTIT Techni-
cal Report Series.
Scherer. 2003. Vocal communication of emotion: A
review of research paradigms. Speech Communica-
tion, 40(1-2):227?256.
M. Shroder and R. Cowie. 2005. Toward emotion-
sensitive multimodal interfaces: the challenge of the
european network of excellence humaine. In Pro-
ceedings of User Modeling Workshop on Adapting
the Interaction Style to Affective Factors.
O. Stock and C. Strapparava. 2005. Hahaacronym:
A computational humor system. In Proceedings of
ACL Interactive Poster and Demonstration Session,
pages 113?116, Ann Arbor, MI.
P. Stone, D. Dunphy, M. Smith, and D. Ogilvie. 1966.
The General Inquirer: A Computer Approach to
Content Analysis. MIT Press, Cambridge, MA.
J. Taylor and L. Mazlack. 2004. Computationally rec-
ognizing wordplay in jokes. In Proceedings of the
CogSci 2004, Chicago, IL.
215
Discriminating Among Word Senses Using McQuitty?s Similarity Analysis
Amruta Purandare
Department of Computer Science
University of Minnesota
Duluth, MN 55812
pura0010@d.umn.edu
Abstract
This paper presents an unsupervised method
for discriminating among the senses of a given
target word based on the context in which it oc-
curs. Instances of a word that occur in similar
contexts are grouped together via McQuitty?s
Similarity Analysis, an agglomerative cluster-
ing algorithm. The context in which a target
word occurs is represented by surface lexical
features such as unigrams, bigrams, and sec-
ond order co-occurrences. This paper summa-
rizes our approach, and describes the results of
a preliminary evaluation we have carried out
using data from the SENSEVAL-2 English lexi-
cal sample and the line corpus.
1 Introduction
Word sense discrimination is the process of grouping or
clustering together instances of written text that include
similar usages of a given target word. The instances that
form a particular cluster will have used the target word in
similar contexts and are therefore presumed to represent a
related meaning. This view follows from the strong con-
textual hypothesis of (Miller and Charles, 1991), which
states that two words are semantically similar to the ex-
tent that their contextual representations are similar.
Discrimination is distinct from the more common
problem of word sense disambiguation in at least two
respects. First, the number of possible senses a target
word may have is usually not known in discrimination,
while disambiguation is often viewed as a classification
problem where a word is assigned to one of several pre?
existing possible senses. Second, discrimination utilizes
features and information that can be easily extracted from
raw corpora, whereas disambiguation often relies on su-
pervised learning from sense?tagged training examples.
However, the creation of sense?tagged data is time con-
suming and results in a knowledge acquisition bottleneck
that severely limits the portability and scalability of tech-
niques that employ it. Discrimination does not suffer
from this problem since there is no expensive preprocess-
ing, nor are any external knowledge sources or manually
annotated data required.
The objective of this research is to extend previous
work in discrimination by (Pedersen and Bruce, 1997),
who developed an approach using agglomerative cluster-
ing. Their work relied on McQuitty?s Similarity Anal-
ysis using localized contextual features. While the ap-
proach in this paper also adopts McQuitty?s method, it
is distinct in that it uses a larger number of features that
occur both locally and globally in the instance being dis-
criminated. It also incorporates several ideas from later
work by (Schu?tze, 1998), including the reliance on a sep-
arate ?training? corpus of raw text from which to iden-
tify contextual features, and the use of second order co?
occurrences (socs) as feature for discrimination.
Our near term objectives for this research include de-
termining to what extent different types of features im-
pact the accuracy of unsupervised discrimination. We
are also interested in assessing how different measures
of similarity such as the matching coefficient or the co-
sine affect overall performance. Once we have refined
our clustering techniques, we will incorporate them into a
method that automatically assigns sense labels to discov-
ered clusters by using information from a machine read-
able dictionary.
This paper continues with a more detailed discussion
of the previous work that forms the foundation for our re-
search. We then present an overview of the features used
to represent the context of a target word, and go on to de-
scribe an experimental evaluation using the SENSEVAL-2
lexical sample data. We close with a discussion of our re-
sults, a summary of related work, and an outline of our
future directions.
                                                               Edmonton, May-June 2003
                                                 Student Research Workshop , pp. 19-24
                                                         Proceedings of HLT-NAACL 2003
2 Previous Work
The work in this paper builds upon two previous ap-
proaches to word sense discrimination, those of (Peder-
sen and Bruce, 1997) and (Schu?tze, 1998). Pedersen and
Bruce developed a method based on agglomerative clus-
tering using McQuitty?s Similarity Analysis (McQuitty,
1966), where the context of a target word is represented
using localized contextual features such as collocations
and part of speech tags that occur within one or two po-
sitions of the target word. Pedersen and Bruce demon-
strated that despite it?s simplicity, McQuitty?s method
was more accurate than Ward?s Method of Minimum
Variance and the EM Algorithm for word sense discrimi-
nation.
McQuitty?s method starts by assuming that each in-
stance is a separate cluster. It merges together the pair
of clusters that have the highest average similarity value.
This continues until a specified number of clusters is
found, or until the similarity measure between every pair
of clusters is less than a predefined cutoff. Pedersen and
Bruce used a relatively small number of features, and em-
ployed the matching coefficient as the similarity measure.
Since we use a much larger number of features, we are ex-
perimenting with the cosine measure, which scales simi-
larity based on the number of non?zero features in each
instance.
By way of contrast, (Schu?tze, 1998) performs discrim-
ination through the use of two different kinds of context
vectors. The first is a word vector that is based on co?
occurrence counts from a separate training corpus. Each
word in this corpus is represented by a vector made up of
the words it co-occurs with. Then, each instance in a test
or evaluation corpus is represented by a vector that is the
average of all the vectors of all the words that make up
that instance. The context in which a target word occurs
is thereby represented by second order co?occurrences,
which are words which co?occur with the co?occurrences
of the target word. Discrimination is carried out by clus-
tering instance vectors using the EM Algorithm.
The approach described in this paper proceeds as fol-
lows. Surface lexical features are identified in a training
corpus, which is made up of instances that consists of a
sentence containing a given target word, plus one or two
sentences to the left or right of it. Similarly defined in-
stances in the test data are converted into vectors based
on this feature set, and a similarity matrix is constructed
using either the matching coefficient or the cosine. There-
after McQuitty?s Similarity Analysis is used to group to-
gether instances based on the similarity of their context,
and these are evaluated relative to a manually created
gold standard.
3 Discrimination Features
We carry out discrimination based on surface lexical fea-
tures that require little or no preprocessing to identify.
They consist of unigrams, bigrams, and second order co?
occurrences.
Unigrams are single words that occur in the same con-
text as a target word. Bag?of?words feature sets made
up of unigrams have had a long history of success in text
classification and word sense disambiguation (Mooney,
1996), and we believe that despite creating quite a bit of
noise can provide useful information for discrimination.
Bigrams are pairs of words which occur together in
the same context as the target word. They may include
the target word, or they may not. We specify a win-
dow of size five for bigrams, meaning that there may be
up to three intervening words between the first and last
word that make up the bigram. As such we are defining
bigrams to be non?consecutive word sequences, which
could also be considered a kind of co?occurrence feature.
Bigrams have recently been shown to be very successful
features in supervised word sense disambiguation (Peder-
sen, 2001). We believe this is because they capture mid-
dle distance co?occurrence relations between words that
occur in the context of the target word.
Second order co?occurrences are words that occur with
co-occurrences of the target word. For example, suppose
that line is the target word. Given telephone line and tele-
phone bill, bill would be considered a second order co?
occurrence of line since it occurs with telephone, a first
order co?occurrence of line.
We define a window size of five in identifying sec-
ond order co?occurrences, meaning that the first order
co?occurrence must be within five positions of the tar-
get word, and the second order co?occurrence must be
within five positions of the first order co?occurrence. We
only select those second order co?occurrences which co?
occur more than once with the first order co-occurrences
which in turn co-occur more than once with the target
word within the specified window.
We employ a stop list to remove high frequency non?
content words from all of these features. Unigrams that
are included in the stop list are not used as features. A bi-
gram is rejected if any word composing it is a stop word.
Second order co?occurrences that are stop words or those
that co?occur with stop words are excluded from the fea-
ture set.
After the features have been identified in the training
data, all of the instances in the test data are converted
into binary feature vectors
 	

that repre-
sent whether the features found in the training data have
occurred in a particular test instance. In order to clus-
ter these instances, we measure the pair?wise similarities
between them using matching and cosine coefficients.
These values are formatted in a    similarity ma-
trix such that cell
   
contains the similarity measure
between instances

and

. This information serves as the
input to the clustering algorithm that groups together the
most similar instances.
4 Experimental Methodology
We evaluate our method using two well known sources of
sense?tagged text. In supervised learning sense?tagged
text is used to induce a classifier that is then applied to
held out test data. However, our approach is purely un-
supervised and we only use the sense tags to carry out an
automatic evaluation of the discovered clusters. We fol-
low Schu?tze?s strategy and use a ?training? corpus only
to extract features and ignore the sense tags.
In particular, we use subsets of the line data (Leacock
et al, 1993) and the English lexical sample data from the
SENSEVAL-2 comparative exercise among word sense
disambiguation systems (Edmonds and Cotton, 2001).
The line data contains 4,146 instances, where each
consists of two to three sentences where a single oc-
currence of line has been manually tagged with one of
six possible senses. We randomly select 100 instances
of each sense for test data, and 200 instances of each
sense for training. This gives a total of 600 evaluation
instances, and 1200 training instances. This is done to
test the quality of our discrimination method when senses
are uniformly distributed and where no particular sense is
dominant.
The standard distribution of the SENSEVAL-2 data
consists of 8,611 training instances and 4,328 test in-
stances. Each instance is made up of two to three sen-
tences where a single target word has been manually
tagged with a sense (or senses) appropriate for that con-
text. There are 73 distinct target words found in this
data; 29 nouns, 29 verbs, and 15 adjectives. Most of
these words have less than 100 test instances, and ap-
proximately twice that number of training examples. In
general these are relatively small samples for an unsu-
pervised approach, but we are developing techniques to
increase the amount of training data for this corpus auto-
matically.
We filter the SENSEVAL-2 data in three different ways
to prepare it for processing and evaluation. First, we in-
sure that it only includes instances whose actual sense is
among the top five most frequent senses as observed in
the training data for that word. We believe that this is an
aggressive number of senses for a discrimination system
to attempt, considering that (Pedersen and Bruce, 1997)
experimented with 2 and 3 senses, and (Schu?tze, 1998)
made binary distinctions.
Second, instances may have been assigned more than
one correct sense by the human annotator. In order to
simplify the evaluation process, we eliminate all but the
most frequent of multiple correct answers.
Third, the SENSEVAL-2 data identifies target words
that are proper nouns. We have elected not to use that in-
formation and have removed these P tags from the data.
After carrying out these preprocessing steps, the number
of training and test instances is 7,476 and 3,733.
5 Evaluation Technique
We specify an upper limit on the number of senses that
McQuitty?s algorithm can discover. In these experiments
this value is five for the SENSEVAL-2 data, and six for
line. In future experiments we will specify even higher
values, so that the algorithm is forced to create larger
number of clusters with very few instances when the ac-
tual number of senses is smaller than the given cutoff.
About a third of the words in the SENSEVAL-2 data have
fewer than 5 senses, so even now the clustering algorithm
is not always told the correct number of clusters it should
find.
Once the clusters are formed, we access the actual cor-
rect sense of each instance as found in the sense?tagged
text. This information is never utilized prior to evalua-
tion. We use the sense?tagged text as a gold standard by
which we can evaluate the discovered sense clusters. We
assign sense tags to clusters such that the resulting accu-
racy is maximized.
For example, suppose that five clusters (C1 ? C5) have
been discovered for a word with 100 instances, and that
the number of instances in each cluster is 25, 20, 10, 25,
and 20. Suppose that there are five actual senses (S1 ?
S5), and the number of instances for each sense is 20, 20,
20, 20, and 20. Figure 1 shows the resulting confusion
matrix if the senses are assigned to clusters in numeric
order. After this assignment is made, the accuracy of the
clustering can be determined by finding the sum of the
diagonal, and dividing by the total number of instances,
which in this case leads to accuracy of 10% (10/100).
However, clearly there are assignments of senses to clus-
ters that would lead to better results.
Thus, the problem of assigning senses to clusters be-
comes one of reordering the columns of the confusion
such that the diagonal sum is maximized. This corre-
sponds to several well known problems, among them the
Assignment Problem in Operations Research, and deter-
mining the maximal matching of a bipartite graph. Figure
2 shows the maximally accurate assignment of senses to
clusters, which leads to accuracy of 70% (70/100).
During evaluation we assign one cluster to at most one
sense, and vice versa. When the number of discovered
clusters is the same as the number of senses, then there
is a 1 to 1 mapping between them. When the number
of clusters is greater than the number of actual senses,
then some clusters will be left unassigned. And when the
S1 S2 S3 S4 S5
C1: 5 20 0 0 0 25
C2: 10 0 5 0 5 20
C3: 0 0 0 0 10 10
C4: 0 0 15 5 5 25
C5: 5 0 0 15 0 20
20 20 20 20 20 100
Figure 1: Numeric Assignment
S2 S1 S5 S3 S4
C1: 20 5 0 0 0 25
C2: 0 10 5 5 0 20
C3: 0 0 10 0 0 10
C4: 0 0 5 15 5 25
C5: 0 5 0 0 15 20
20 20 20 20 20 100
Figure 2: Maximally Accurate Assignment
number of senses is greater than the number of clusters,
some senses will not be assigned to any cluster.
We determine the precision and recall based on this
maximally accurate assignment of sense tags to clusters.
Precision is defined as the number of instances that are
clustered correctly divided by the number of instances
clustered, while recall is the number of instances clus-
tered correctly over the total number of instances.
To be clear, we do not believe that word sense discrim-
ination must be carried out relative to a pre?existing set
of senses. In fact, one of the great advantages of an un-
supervised approach is that it need not be relative to any
particular set of senses. We carry out this evaluation tech-
nique in order to improve the performance of our cluster-
ing algorithm, which we will then apply on text where
sense?tagged data is not available.
An alternative means of evaluation is to have a hu-
man inspect the discovered clusters and judge them based
on the semantic coherence of the instances that populate
each cluster, but this is a more time consuming and sub-
jective method of evaluation that we will pursue in future.
6 Experimental Results
For each word in the SENSEVAL-2 data and line, we con-
ducted various experiments, each of which uses a differ-
ent combination of measure of similarity and features.
Features are identified from the training data. Our fea-
tures consist of unigrams, bigrams, or second order co?
occurrences. We employ each of these three types of fea-
tures separately, and we also create a mixed set that is the
union of all three sets. We convert each evaluation in-
stance into a feature vector, and then convert those into a
similarity matrix using either the matching coefficient or
the cosine.
Table 1 contains overall precision and recall for the
nouns, verbs, and adjectives overall in the SENSEVAL-
2 data, and for line. The SENSEVAL-2 values are de-
rived from 29 nouns, 28 verbs, and 15 adjectives from
the SENSEVAL-2 data. The first column lists the part of
speech, the second shows the feature, the third lists the
measure of similarity, the fourth and the fifth show pre-
cision and recall, the sixth shows the percentage of the
majority sense, and the final column shows the number
of words in the given part of speech that gave accuracy
greater than the percentage of the majority sense. The
value of the majority sense is derived from the sense?
tagged data we use in evaluation, but this is not infor-
mation that we would presume to have available during
actual clustering.
Table 1: Experimental Results
pos feat meas prec rec maj   maj
noun soc cos 0.49 0.48 0.57 6/29
mat 0.54 0.52 0.57 7/29
big cos 0.53 0.50 0.57 5/29
mat 0.52 0.49 0.57 3/29
uni cos 0.50 0.49 0.57 7/29
mat 0.52 0.50 0.57 8/29
mix cos 0.50 0.48 0.57 6/29
mat 0.54 0.51 0.57 5/29
verb soc cos 0.51 0.49 0.51 11/28
mat 0.50 0.47 0.51 6/28
big cos 0.54 0.45 0.51 5/28
mat 0.53 0.43 0.51 5/28
uni cos 0.42 0.41 0.51 13/28
mat 0.43 0.41 0.51 9/28
mix cos 0.43 0.41 0.51 12/28
mat 0.42 0.41 0.51 7/28
adj soc cos 0.59 0.54 0.64 1/15
mat 0.59 0.55 0.64 1/15
big cos 0.56 0.51 0.64 0/15
mat 0.55 0.50 0.64 0/15
uni cos 0.55 0.50 0.64 1/15
mat 0.58 0.53 0.64 0/15
mix cos 0.50 0.44 0.64 0/15
mat 0.59 0.54 0.64 2/15
line soc cos 0.25 0.25 0.17 1/1
mat 0.23 0.23 0.17 1/1
big cos 0.19 0.18 0.17 1/1
mat 0.18 0.17 0.17 1/1
uni cos 0.21 0.21 0.17 1/1
mat 0.20 0.20 0.17 1/1
mix cos 0.21 0.21 0.17 1/1
mat 0.20 0.20 0.17 1/1
For the SENSEVAL-2 data, on average the precision
and recall of the clustering as determined by our evalu-
ation method is less than that of the majority sense, re-
gardless of which features or measure are used. How-
ever, for nouns and verbs, a relatively significant num-
ber of individual words have precision and recall values
higher than that of the majority sense. The adjectives are
an exception to this, where words are very rarely dis-
ambiguated more accurately than the percentage of the
majority sense. However, many of the adjectives have
very high frequency majority senses, which makes this
a difficult standard for an unsupervised method to reach.
When examining the distribution of instances in clusters,
we find that the algorithm tends to seek more balanced
distributions, and is unlikely to create a single long clus-
ter that would result in high accuracy for a word whose
true distribution of senses is heavily skewed towards a
single sense.
We also note that the precision and recall of the clus-
tering of the line data is generally better than that of the
majority sense regardless of the features or measures em-
ployed. We believe there are two explanations for this.
First, the number of training instances for the line data is
significantly higher (1200) than that of the SENSEVAL-2
words, which typically have 100?200 training instances
per word. The number and quality of features identified
improves considerably with an increase in the amount of
training data. Thus, the amount of training data avail-
able for feature identification is critically important. We
believe that the SENSEVAL-2 data could be augmented
with training data taken from the World Wide Web, and
we plan to pursue such approaches and see if our perfor-
mance on the evaluation data improves as a result.
At this point we do not observe a clear advantage to
using the cosine measure or matching coefficient. This
surprises us somewhat, as the number of features em-
ployed is generally in the thousands, and the number of
non?zero features can be quite large. It would seem that
simply counting the number of matching features would
be inferior to the cosine measure, but this is not the case.
This remains an interesting issue that we will continue to
explore, with these and other measures of similarity.
Finally, there is not a single feature that does best in
all parts of speech. Second order co?occurrences seem to
do well with nouns and adjectives, while bigrams result
in accurate clusters for verbs. We also note that second
order co?occurrences do well with the line data. As yet
we have drawn no conclusions from these results, but it
is clearly a vital issue to investigate further.
7 Related Work
Unsupervised approaches to word sense discrimination
have been somewhat less common in the computational
linguistics literature, at least when compared to super-
vised approaches to word sense disambiguation.
There is a body of work at the intersection of super-
vised and unsupervised approaches, which involves using
a small amount of training data in order to automatically
create more training data, in effect bootstrapping from the
small sample of sense?tagged data. The best example of
such an approach is (Yarowsky, 1995), who proposes a
method that automatically identifies collocations that are
indicative of the sense of a word, and uses those to itera-
tively label more examples.
While our focus has been on Pedersen and Bruce, and
on Schu?tze, there has been other work in purely unsuper-
vised approaches to word sense discrimination.
(Fukumoto and Suzuki, 1999) describe a method for
discriminating among verb senses based on determining
which nouns co?occur with the target verb. Collocations
are extracted which are indicative of the sense of a verb
based on a similarity measure they derive.
(Pantel and Lin, 2002) introduce a method known as
Committee Based Clustering that discovers word senses.
The words in the corpus are clustered based on their dis-
tributional similarity under the assumption that semanti-
cally similar words will have similar distributional char-
acteristics. In particular, they use Pointwise Mutual In-
formation to find how close a word is to its context and
then determine how similar the contexts are using the co-
sine coefficient.
8 Future Work
Our long term goal is to develop a method that will as-
sign sense labels to clusters using information found in
machine readable dictionaries. This is an important prob-
lem because clusters as found in discrimination have no
sense tag or label attached to them. While there are cer-
tainly applications for unlabeled sense clusters, having
some indication of the sense of the cluster would bring
discrimination and disambiguation closer together. We
will treat glosses as found in a dictionary as vectors that
we project into the same space that is populated by in-
stances as we have already described. A cluster could be
assigned the sense of the gloss whose vector it was most
closely located to.
This idea is based loosely on work by (Niwa and Nitta,
1994), who compare word co?occurrence vectors derived
from large corpora of text with co?occurrence vectors
based on the definitions or glosses of words in a ma-
chine readable dictionary. A co?occurrence vector indi-
cates how often words are used with each other in a large
corpora or in dictionary definitions. These vectors can be
projected into a high dimensional space and used to mea-
sure the distance between concepts or words. Niwa and
Nitta show that while the co?occurrence data from a dic-
tionary has different characteristics that a co?occurrence
vector derived from a corpus, both provide useful infor-
mation about how to categorize a word based on its mean-
ing. Our future work will mostly attempt to merge clus-
ters found from corpora with meanings in dictionaries
where presentation techniques like co?occurrence vectors
could be useful.
There are a number of smaller issues that we are inves-
tigating. We are also exploring a number of other types
of features, as well as varying the formulation of the fea-
tures we are currently using. We have already conducted
a number of experiments that vary the window sizes em-
ployed with bigrams and second order co?occurrences,
and will continue in this vein. We are also considering
the use of other measures of similarity beyond the match-
ing coefficient and the cosine. We do not stem the train-
ing data prior to feature identification, nor do or employ
fuzzy matching techniques when converting evaluation
instances into feature vectors. However, we believe both
might lead to increased numbers of useful features being
identified.
9 Conclusions
We have presented an unsupervised method of word
sense discrimination that employs a range of surface lexi-
cal features, and relies on similarity based clustering. We
have evaluated this method in an extensive experiment
that shows that our method can achieve precision and re-
call higher than the majority sense of a word for a reason-
ably large number of cases. We believe that increases in
the amount of training data employed in this method will
yield to considerably improved results, and have outlined
our plans to address this and several other issues.
10 Acknowledgments
This research is being conducted as a part of my M.S. the-
sis in Computer Science at the University of Minnesota,
Duluth. I am grateful to my thesis advisor, Dr. Ted Ped-
ersen, for his help and guidance.
I have been fully supported by a National Science
Foundation Faculty Early CAREER Development Award
(#0092784) during the 2002?2003 academic year.
I would like to thank the Director of Computer Science
Graduate Studies, Dr. Carolyn Crouch, and the Associate
Vice Chancellor, Dr. Stephen Hedman, for their support
in providing a travel award to attend the Student Research
Workshop at HLT-NAACL 2003.
References
P. Edmonds and S. Cotton, editors. 2001. Proceedings
of the Senseval?2 Workshop. Association for Compu-
tational Linguistics, Toulouse, France.
F. Fukumoto and Y. Suzuki. 1999. Word sense disam-
biguation in untagged text based on term weight learn-
ing. In Proceedings of the Ninth Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 209?216, Bergen.
C. Leacock, G. Towell, and E. Voorhees. 1993. Corpus-
based statistical sense resolution. In Proceedings of
the ARPA Workshop on Human Language Technology,
pages 260?265, March.
L. McQuitty. 1966. Similarity analysis by reciprocal
pairs for discrete and continuous data. Educational
and Psychological Measurement, 26:825?831.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
R. Mooney. 1996. Comparative experiments on disam-
biguating word senses: An illustration of the role of
bias in machine learning. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 82?91, May.
Y. Niwa and Y. Nitta. 1994. Co-occurrence vectors from
corpora versus distance vectors from dictionaries. In
Proceedings of the Fifteenth International Conference
on Computational Linguistics, pages 304?309, Kyoto,
Japan.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining-2002.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
T. Pedersen. 2001. A decision tree of bigrams is an ac-
curate predictor of word sense. In Proceedings of the
Second Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 79?86, Pittsburgh, July.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA.
SenseClusters - Finding Clusters that Represent Word Senses
Amruta Purandare and Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
{pura0010,tpederse}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
SenseClusters is a freely available word sense
discrimination system that takes a purely unsu-
pervised clustering approach. It uses no knowl-
edge other than what is available in a raw un-
structured corpus, and clusters instances of a
given target word based only on their mutual
contextual similarities. It is a complete sys-
tem that provides support for feature selec-
tion from large corpora, several different con-
text representation schemes, various clustering
algorithms, and evaluation of the discovered
clusters.
1 Introduction
Most words in natural language have multiple possible
meanings that can only be determined by considering the
context in which they occur. Given instances of a tar-
get word used in a number of different contexts, word
sense discrimination is the process of grouping these in-
stances into clusters that refer to the same word mean-
ing. Approaches to this problem are often based on
the strong contextual hypothesis of (Miller and Charles,
1991), which states that two words are semantically re-
lated to the extent that their contextual representations
are similar. Hence the problem of word sense discrimi-
nation reduces to that of determining which contexts of a
given target word are related or similar.
SenseClusters creates clusters made up of the contexts
in which a given target word occurs. All the instances in
a cluster are contextually similar to each other, making it
more likely that the given target word has been used with
the same meaning in all of those instances. Each instance
normally includes 2 or 3 sentences, one of which contains
the given occurrence of the target word.
SenseClusters was originally intended to discriminate
among word senses. However, the methodology of clus-
tering contextually (and hence semantically) similar in-
stances of text can be used in a variety of natural language
processing tasks such as synonymy identification, text
summarization and document classification. SenseClus-
ters has also been used for applications such as email sort-
ing and automatic ontology construction.
In the sections that follow we will describe the basic
functionality supported by SenseClusters. In general pro-
cessing starts by selecting features from a corpus of text.
Then these features are used to create an appropriate rep-
resentation of the contexts that are to be clustered. There-
after the actual clustering takes place, followed by an op-
tional evaluation stage that compares the discovered clus-
ters to an existing gold standard (if available).
2 Feature Selection
SenseClusters distinguishes among the different contexts
in which a target word occurs based on a set of features
that are identified from raw corpora. SenseClusters uses
the Ngram Statistics Package (Banerjee and Pedersen,
2003), which is able to extract surface lexical features
from large corpora using frequency cutoffs and various
measures of association, including the log?likelihood ra-
tio, Pearson?s Chi?Squared test, Fisher?s Exact test, the
Dice Coefficient, Pointwise Mutual Information, etc.
SenseClusters currently supports the use of unigram,
bigram, and co-occurrence features. Unigrams are indi-
vidual words that occur above a certain frequency cutoff.
These can be effective discriminating features if they are
shared by a minimum of 2 contexts, but not shared by all
contexts. Very common non-content words are excluded
by providing a stop?list.
Bigrams are pairs of words that occur above a given
frequency cutoff and that have a statistically significant
score on a test of association. There may optionally be
intervening words between them that are ignored. Co?
occurrences are bigrams that include the target word. In
effect co?occurrences localize the scope of the unigram
features by selecting only those words that occur within
some number of positions from the target word.
SenseClusters allows for the selection of lexical fea-
tures either from a held out corpus of training data, or
from the same data that is to be clustered, which we refer
to as the test data. Selecting features from separate train-
ing data is particularly useful when the amount of the test
data to be clustered is too small to identify interesting
features.
The following is a summary of some of the options
provided by SenseClusters that make it possible for a user
to customize feature selection to their needs:
?training FILE A held out file of training data to be
used to select features. Otherwise, features will be se-
lected from the data to be clustered.
?token FILE A file containing Perl regular expressions
that defines the tokenization scheme.
?stop FILE A file containing a user provided stoplist.
?feature STRING The feature type to be selected.
Valid options include unigrams, bigrams, and co-
occurrences.
?remove N Ignore features that occur less N times.
?window M Allow up to M-2 words to intervene be-
tween pairs of words when identifying bigram and co-
occurrence features.
?stat STRING The statistical test of association to
identify bigram and co?occurrence features. Valid values
include any of the tests supported by the Ngram Statistics
Package.
3 Context Representation
Once features are selected, SenseClusters creates a vector
for each test instance to be discriminated where each se-
lected feature is represented by an entry/index. Each vec-
tor shows if the feature represented by the corresponding
index occurs or not in the context of the instance (binary
vectors), or how often the feature occurs in the context
(frequency vectors). This is referred to as a first order
context vector, since this representation directly indicates
which features make up the contexts. Here we are follow-
ing (Pedersen and Bruce, 1997), who likewise took this
approach to feature representation.
(Schu?tze, 1998) utilized second order context vectors
that represent the context of a target word to be discrim-
inated by taking the average of the first order vectors as-
sociated with the unigrams that occur in that context. In
SenseClusters we have extended this idea such that these
first order vectors can also be based on co?occurrence or
bigram features from the training corpus.
Both the first and second order context vectors repre-
sent the given instances as vectors in a high dimensional
word space. This approach suffers from two limitations.
First, there may be synonyms represented by separate di-
mensions in the space. Second, and conversely, a single
dimension in the space might be polysemous and associ-
ated with several different underlying concepts. To com-
bat these problems, SenseClusters follows the lead of LSI
(Deerwester et al, 1990) and LSA (Landauer et al, 1998)
and allows for the conversion of word level feature spaces
into a concept level semantic space by carrying out di-
mensionality reduction with Singular Value Decomposi-
tion (SVD). In particular, the package SVDPACK (Berry
et al, 1993) is integrated into SenseClusters to allow for
fast and efficient SVD.
4 Clustering
Clustering can be carried out using either a first or sec-
ond order vector representation of instances. SenseClus-
ters provides a seamless interface to CLUTO, a Cluster-
ing Toolkit (Karypis, 2002), which implements a range
of clustering techniques suitable for both representations,
including repeated bisections, direct, nearest neighbor,
agglomerative, and biased agglomerative.
The first or second order vector representations of con-
texts can be directly clustered using vector space meth-
ods provided in CLUTO. As an alternative, each context
vector can be represented as a point in similarity space
such that the distance between it and any other context
vector reflects the pairwise similarity of the underlying
instances.
SenseClusters provides support for a number of simi-
larity measures, such as simple matching, the cosine, the
Jaccard coefficient, and the Dice coefficient. A similar-
ity matrix created by determining all pairwise measures
of similarity between contexts can be used as an input
to CLUTO?s clustering algorithms, or to SenseClusters?
own agglomerative clustering implementation.
5 Evaluation
SenseClusters produces clusters of instances where each
cluster refers to a particular sense of the given target
word. SenseClusters supports evaluation of these clus-
ters in two ways. First, SenseClusters provides external
evaluation techniques that require knowledge of correct
senses or clusters of the given instances. Second, there
are internal evaluation methods provided by CLUTO that
report the intra-cluster and inter-cluster similarity.
5.1 External Evaluation
When a gold standard clustering of the instances is avail-
able, SenseClusters builds a confusion matrix that shows
S1 S2 S3 S4 S5 S6
C0: 2 3 3 1 99 3 111
C1: 11 5 43 11 11 8 89
C2: 1 19 7 19 208 7 261
C3: 3 15 13 7 37 12 87
C4: 6 5 8 16 143 8 186
C5: 37 18 8 18 186 20 287
C6: 17 7 11 59 14 13 121
C7: 4 9 13 14 163 12 215
C8: 54 20 15 6 16 35 146
C9: 29 51 12 18 11 35 156
164 152 133 169 888 153 1659
Figure 1: Confusion Matrix: Prior to Mapping
S3 S5 S6 S4 S1 S2
C1: 43 11 8 11 11 5 89
C2: 7 208 7 19 1 19 261
C5: 8 186 20 18 37 18 287
C6: 11 14 13 59 17 7 121
C8: 15 16 35 6 54 20 146
C9: 12 11 35 18 29 51 156
C0:* 3 99 3 1 2 3 111
C3:* 13 37 12 7 3 15 87
C4:* 8 143 8 16 6 5 186
C7:* 13 163 12 14 4 9 215
133 888 153 169 164 152 1659
Figure 2: Confusion Matrix: After Mapping
the distribution of the known senses in each of the dis-
covered clusters. A gold standard most typically exists in
the form of sense?tagged text, where each sense tag can
be considered to represent a different cluster that could
be discovered.
In Figure 1, the rows C0 ? C9 represent ten discovered
clusters while the columns represent six gold-standard
senses. The value of cell (i,j) shows the number of in-
stances in the ith discovered cluster that actually belong
to the gold standard sense represented by the jth column.
Note that the bottom row represents the true distribution
of the instances across the senses, while the right hand
column shows the distribution of the discovered clusters.
To carry out evaluation of the discovered clusters,
SenseClusters finds the mapping of gold standard senses
to discovered clusters that would result in maximally ac-
curate discrimination. The problem of assigning senses
to clusters becomes one of re-ordering the columns of the
confusion matrix to maximize the diagonal sum. Thus,
each possible re-ordering shows one assignment scheme
and the sum of the diagonal entries indicates the total
number of instances in the discovered clusters that would
be in their correct sense given that alignment. This corre-
sponds to several well known problems, among them the
Assignment Problem in Operations Research and finding
the maximal matching of a bipartite graph.
Figure 2 shows that cluster C1 maps most closely to
sense S3, while discovered cluster C2 corresponds best
to sense S5, and so forth. The clusters marked with *
are not assigned to any sense. The accuracy of discrim-
ination is simply the sum of the diagonal entries of the
row/column re-ordered confusion matrix divided by the
total number of instances clustered (435/1659 = 26%).
Precision can also be computed by dividing the total num-
ber of correctly discriminated instances by the number
of instances in the six clusters mapped to gold standard
senses (435/1060 = 41%).
5.2 Internal Evaluation
When gold?standard sense tags of the test instances are
not available, SenseClusters relies on CLUTO?s internal
evaluation metrics to report the intra-cluster and inter-
cluster similarity. There is also a graphical component
to CLUTO known as gCLUTO that provides a visualiza-
tion tool. An example of gCLUTO?s output is provided in
Figure 3, which displays a mountain view of the clusters
shown in tables 1 and 2.
This particular visualization illustrates the case when
the gold?standard data has fewer senses (6) than the ac-
tual number requested (10). CLUTO and SenseClusters
both require that the desired number of clusters be speci-
fied prior to clustering. In this example we requested 10,
and the mountain view reveals that there were really only
5 to 7 actual distinct senses. In unsupervised word sense
discrimination, the user will usually not know the actual
number of senses ahead of time. One possible solution
to this problem is to request an arbitrarily large number
of clusters and rely on such visualizations to discover the
true number of senses. In future work, we plan to sup-
port mechanisms that automatically determine the opti-
mal number of clusters/senses to be found.
6 Summary of Unique Features
The following are some of the distinguishing characteris-
tics of SenseClusters.
Feature Types SenseClusters supports the flexible se-
lection of a variety of lexical features, including uni-
grams, bigrams, co-occurrences. These are selected by
the Ngram Statistics Package using statistical tests of as-
sociation or frequency cutoffs.
Context Representations SenseClusters supports two
different representations of context, first order context
vectors as used by (Pedersen and Bruce, 1997) and
second order context vectors as suggested by (Schu?tze,
1998). The former is a direct representation of the in-
stances to be clustered in terms of their features, while
Figure 3: Mountain View from gCLUTO
the latter uses an indirect representation that averages the
first order vector representations of the features that make
up the context.
Clustering SenseClusters seamlessly integrates
CLUTO, a clustering package that provides a wide
range of clustering algorithms and criteria functions.
CLUTO also provides evaluation functions that report
the inter-cluster and intra-cluster similarity, the most
discriminating features characterizing each cluster,
a dendogram tree view, and a 3D mountain view of
clusters. SenseClusters also provides a native imple-
mentation of single link, complete link, and average link
clustering.
Evaluation SenseClusters supports the evaluation of
discovered clusters relative to an existing gold standard.
If sense?tagged text is available, this can be immediately
used as such a gold standard. This evaluation reports pre-
cision and recall relative to the gold standard.
LSA Support SenseClusters provides all of the func-
tionality needed to carry out Latent Semantic Analysis.
LSA converts a word level feature space into a concept
level semantic space that smoothes over differences due
to polysemy and synonymy among words.
Efficiency SenseClusters is optimized to deal with a
large amount of data both in terms of the number of text
instances being clustered and the number of features used
to represent the contexts.
Integration SenseClusters transparently incorporates
several specialized tools, including CLUTO, the Ngram
Statistics Package, and SVDPACK. This provides a wide
number of options and high efficiency at various steps
like feature selection, feature space dimensionality reduc-
tion, clustering and evaluation.
Availability SenseClusters is an open source software
project that is freely distributed under the GNU Public
License (GPL) via http://senseclusters.sourceforge.net/
SenseClusters is an ongoing project, and there are al-
ready a number of published papers based on its use (e.g.,
(Purandare, 2003), (Purandare and Pedersen, 2004)).
7 Acknowledgments
This work has been partially supported by a National Sci-
ence Foundation Faculty Early CAREER Development
award (Grant #0092784).
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistics Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
M. Berry, T. Do, G. O?Brien, V. Krishna, and S. Varad-
han. 1993. SVDPACK (version 1.0) user?s guide.
Technical Report CS-93-194, University of Tennessee
at Knoxville, Computer Science Department, April.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41:391?407.
G. Karypis. 2002. CLUTO - a clustering toolkit. Tech-
nical Report 02-017, University of Minnesota, Depart-
ment of Computer Science, August.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, Boston,
MA.
A. Purandare. 2003. Discriminating among word senses
using mcquitty?s similarity analysis. In Proceedings
of the HLT-NAACL 2003 Student Research Workshop,
pages 19?24, Edmonton, Alberta, Canada, May 27 -
June 1.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
The SENSEVAL?3 Multilingual English?Hindi Lexical Sample Task
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
timc@isi.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX 76203
rada@cs.unt.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Amruta Purandare
Department of Computer Science
University of Minnesota
Duluth, MN 55812
pura0010@d.umn.edu
Abstract
This paper describes the English?Hindi Multilingual
lexical sample task in SENSEVAL?3. Rather than
tagging an English word with a sense from an En-
glish dictionary, this task seeks to assign the most
appropriate Hindi translation to an ambiguous tar-
get word. Training data was solicited via the Open
Mind Word Expert (OMWE) from Web users who
are fluent in English and Hindi.
1 Introduction
The goal of the MultiLingual lexical sample task
is to create a framework for the evaluation of sys-
tems that perform Machine Translation, with a fo-
cus on the translation of ambiguous words. The
task is very similar to the lexical sample task, ex-
cept that rather than using the sense inventory from
a dictionary we follow the suggestion of (Resnik and
Yarowsky, 1999) and use the translations of the tar-
get words into a second language. In this task for
SENSEVAL-3, the contexts are in English, and the
?sense tags? for the English target words are their
translations in Hindi.
This paper outlines some of the major issues that
arose in the creation of this task, and then describes
the participating systems and summarizes their re-
sults.
2 Open Mind Word Expert
The annotated corpus required for this task was
built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted for mul-
tilingual annotations 1.
To overcome the current lack of tagged data and
the limitations imposed by the creation of such data
using trained lexicographers, the Open Mind Word
1Multilingual Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/english-hindi
Expert system enables the collection of semantically
annotated corpora over the Web. Tagged examples
are collected using a Web-based application that al-
lows contributors to annotate words with their mean-
ings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, together with all
possible translations for the given target word. Users
are asked to select the most appropriate translation
for the target word in each sentence. The selection
is made using check-boxes, which list all possible
translations, plus two additional choices, ?unclear?
and ?none of the above.? Although users are encour-
aged to select only one translation per word, the se-
lection of two or more translations is also possible.
The results of the classification submitted by other
users are not presented to avoid artificial biases.
3 Sense Inventory Representation
The sense inventory used in this task is the set of
Hindi translations associated with the English words
in our lexical sample. Selecting an appropriate
English-Hindi dictionary was a major decision early
in the task, and it raised a number of interesting is-
sues.
We were unable to locate any machine readable
or electronic versions of English-Hindi dictionaries,
so it became apparent that we would need to manu-
ally enter the Hindi translations from printed mate-
rials. We briefly considered the use of Optical Char-
acter Recognition (OCR), but found that our avail-
able tools did not support Hindi. Even after deciding
to enter the Hindi translations manually, it wasn?t
clear how those words should be encoded. Hindi is
usually represented in Devanagari script, which has
a large number of possible encodings and no clear
standard has emerged as yet.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
We decided that Romanized or transliterated
Hindi text would be the the most portable encoding,
since it can be represented in standard ASCII text.
However, it turned out that the number of English?
Hindi bilingual dictionaries is much less than the
number of Hindi?English, and the number that use
transliterated text is smaller still.
Still, we located one promising candidate, the
English?Hindi Hippocrene Dictionary (Raker and
Shukla, 1996), which represents Hindi in a translit-
erated form. However, we found that many English
words only had two or three translations, making it
too coarse grained for our purposes2 .
In the end we selected the Chambers English?
Hindi dictionary (Awasthi, 1997), which is a high
quality bilingual dictionary that uses Devanagari
script. We identified 41 English words from the
Chambers dictionary to make up our lexical sam-
ple. Then one of the task organizers, who is
fluent in English and Hindi, manually transliter-
ated the approximately 500 Hindi translations of
the 41 English words in our lexical sample from
the Chambers dictionary into the ITRANS format
(http://www.aczone.com/itrans/). ITRANS software
was used to generate Unicode for display in the
OMWE interfaces, although the sense tags used in
the task data are the Hindi translations in transliter-
ated form.
4 Training and Test Data
The MultiLingual lexical sample is made up of 41
words: 18 nouns, 15 verbs, and 8 adjectives. This
sample includes English words that have varying de-
grees of polysemy as reflected in the number of pos-
sible Hindi translations, which range from a low of
3 to a high of 39.
Text samples made up of several hundred in-
stances for each of 31 of the 41 words were drawn
from the British National Corpus, while samples for
the other 10 words came from the SENSEVAL-2 En-
glish lexical sample data. The BNC data is in a
?raw? text form, where the part of speech tags have
been removed. However, the SENSEVAL-2 data in-
cludes the English sense?tags as determined by hu-
man taggers.
After gathering the instances for each word in
the lexical sample, we tokenized each instance and
removed those that contain collocations of the tar-
get word. For example, the training/test instances
for arm.n do not include examples for contact arm,
2We have made available transcriptions of the entries for
approximately 70 Hippocrene nouns, verbs, and adjectives
at http://www.d.umn.edu/?pura0010/hindi.html, although these
were not used in this task.
pickup arm, etc., but only examples that refer to arm
as a single lexical unit (not part of a collocation). In
our experience, disambiguation accuracy on collo-
cations of this sort is close to perfect, and we aimed
to concentrate the annotation effort on the more dif-
ficult cases.
The data was then annotated with Hindi transla-
tions by web volunteers using the Open Mind Word
Expert (bilingual edition). At various points in time
we offered gift certificates as a prize for the most
productive tagger in a given day, in order to spur
participation. A total of 40 volunteers contributed to
this task.
To create the test data we collected two indepen-
dent tags per instance, and then discarded any in-
stances where the taggers disagreed. Thus, each
instance that remains in the test data has complete
agreement between two taggers. For the training
data, we only collected one tag per instance, and
therefore this data may be noisy. Participating sys-
tems could choose to apply their own filtering meth-
ods to identify and remove the less reliably anno-
tated examples.
After tagging by the Web volunteers, there were
two data sets provided to task participants: one
where the English sense of the target word is un-
known, and another where it is known in both the
training and test data. These are referred to as the
translation only (t) data and the translation and sense
(ts) data, respectively. The t data is made up of in-
stances drawn from the BNC as described above,
while the ts data is made up of the instances from
SENSEVAL-2. Evaluations were run separately for
each of these two data sets, which we refer to as the
t and ts subtasks.
The t data contains 31 ambiguous words: 15
nouns, 10 verbs, and 6 adjectives. The ts data con-
tains 10 ambiguous words: 3 nouns, 5 verbs, and 2
adjectives, all of which have been used in the En-
glish lexical sample task of SENSEVAL-2. These
words, the number of possible translations, and the
number of training and test instances are shown in
Table 1. The total number of training instances in
the two sub-tasks is 10,449, and the total number of
test instances is 1,535.
5 Participating Systems
Five teams participated in the t subtask, submitting
a total of eight systems. Three teams (a subset of
those five) participated in the ts subtask, submitting
a total of five systems. All submitted systems em-
ployed supervised learning, using the training ex-
amples provided. Some teams used additional re-
sources as noted in the more detailed descriptions
Table 1: Target words in the SENSEVAL-3 English-Hindi task
Lexical Unit Translations Train Test Lexical Unit Translations Train Test Lexical Unit Translations Train Test
TRANSLATION ONLY (T?DATA)
band.n 8 224 91 bank.n 21 332 52 case.n 13 348 42
different.a 4 320 25 eat.v 3 271 48 field.n 14 300 100
glass.n 8 379 13 hot.a 18 348 32 line.n 39 360 11
note.v 11 220 12 operate.v 9 280 50 paper.n 8 264 73
plan.n 8 210 35 produce.v 7 265 67 rest.v 14 172 10
rule.v 8 160 18 shape.n 8 320 32 sharp.a 16 248 48
smell.v 5 210 17 solid.a 16 327 37 substantial.a 15 250 100
suspend.v 4 370 28 table.n 21 378 16 talk.v 6 341 35
taste.n 6 350 40 terrible.a 4 200 99 tour.n 5 240 9
vision.n 14 318 20 volume.n 9 309 54 watch.v 10 300 100
way.n 16 331 22 TOTAL 348 8945 1336
TRANSLATION AND SENSE ONLY (TS?DATA)
bar.n 19 278 39 begin.v 6 360 15 channel.n 6 92 16
green.a 9 175 26 nature.n 15 71 14 play.v 14 152 10
simple.a 9 166 19 treat.v 7 100 32 wash.v 16 10 11
work.v 24 100 17 TOTAL 125 1504 199
below.
5.1 NUS
The NUS team from the National University of Sin-
gapore participated in both the t and ts subtasks. The
t system (nusmlst) uses a combination of knowledge
sources as features, and the Support Vector Machine
(SVM) learning algorithm. The knowledge sources
used include part of speech of neighboring words,
single words in the surrounding context, local col-
locations, and syntactic relations. The ts system
(nusmlsts) does the same, but adds the English sense
of the target word as a knowledge source.
5.2 LIA-LIDILEM
The LIA-LIDILEM team from the Universite? d?
Avignon and the Universite? Stendahl Grenoble had
two systems which participated in both the t and ts
subtasks. In the ts subtask, only the English sense
tags were used, not the Hindi translations.
The FL-MIX system uses a combination of three
probabilistic models, which compute the most prob-
able sense given a six word window of context. The
three models are a Poisson model, a Semantic Clas-
sification Tree model, and a K nearest neighbors
search model. This system also used a part of speech
tagger and a lemmatizer.
The FC-MIX system is the same as the FL-MIX
system, but replaces context words by more gen-
eral synonym?like classes computed from a word
aligned English?French corpus which number ap-
proximately 850,000 words in each language.
5.3 HKUST
The HKUST team from the Hong Kong University
of Science and Technology had three systems that
participated in both the t and ts subtasks
The HKUST me t and HKUST me ts sys-
tems are maximum entropy classifiers. The
HKUST comb t and HKUST comb ts systems
are voted classifiers that combine a new Kernel
PCA model with a maximum entropy model and
a boosting?based model. The HKUST comb2 t
and HKUST comb2 ts are voted classifiers that
combine a new Kernel PCA model with a maximum
entropy model, a boosting?based model, and a
Naive Bayesian model.
5.4 UMD
The UMD team from the University of Maryland en-
tered (UMD?SST) in the t task. UMD?SST is a su-
pervised sense tagger based on the Support Vector
Machine learning algorithm, and is described more
fully in (Cabezas et al, 2001).
5.5 Duluth
The Duluth team from the University of Minnesota,
Duluth had one system (Duluth-ELSS) that partici-
pated in the t task. This system is an ensemble of
three bagged decision trees, each based on a differ-
ent type of lexical feature. This system was known
as Duluth3 in SENSEVAL-2, and it is described more
fully in (Pedersen, 2001).
6 Results
All systems attempted all of the test instances, so
precision and recall are identical, hence we report
Table 2: t Subtask Results
System Accuracy
nusmlst 63.4
HKUST comb t 62.0
HKUST comb2 t 61.4
HKUST me t 60.6
FL-MIX 60.3
FC-MIX 60.3
UMD-SST 59.4
Duluth-ELSS 58.2
Baseline (majority) 51.9
Table 3: ts Subtask Results
System Accuracy
nusmlsts 67.3
FL-MIX 64.1
FC-MIX 64.1
HKUST comb ts 63.8
HKUST comb2 ts 63.8
HKUST me ts 60.8
Baseline (majority) 55.8
the single Accuracy figure. Tables 2 and 3 show re-
sults for the t and ts subtasks, respectively.
We note that the participating systems all ex-
ceeded the baseline (majority) classifier by some
margin, suggesting that the sense distinctions made
by the translations are clear and provide sufficient
information for supervised methods to learn effec-
tive classifiers.
Interestingly, the average results on the ts data are
higher than the average results on the t data, which
suggests that sense information is likely to be helpful
for the task of targeted word translation. Additional
investigations are however required to draw some fi-
nal conclusions.
7 Conclusion
The Multilingual Lexical Sample task in
SENSEVAL-3 featured English ambiguous words
that were to be tagged with their most appropriate
Hindi translation. The objective of this task is to
determine feasibility of translating words of various
degrees of polysemy, focusing on translation of
specific lexical items. The results of five teams
that participated in this event tentatively suggest
that machine learning techniques can significantly
improve over the most frequent sense baseline.
Additionally, this task has highlighted creation
of testing and training data by leveraging the
knowledge of bilingual Web volunteers. The
training and test data sets used in this exercise are
available online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the Mul-
tilingual Open Mind Word Expert project, making
this task possible. We are also grateful to all the par-
ticipants in this task, for their hard work and involve-
ment in this evaluation exercise. Without them, all
these comparative analyses would not be possible.
We are particularly grateful to a research grant
from the University of North Texas that provided the
funding for contributor prizes, and to the National
Science Foundation for their support of Amruta Pu-
randare under a Faculty Early CAREER Develop-
ment Award (#0092784).
References
S. Awasthi, editor. 1997. Chambers English?Hindi
Dictionary. South Asia Books, Columbia, MO.
C. Cabezas, P. Resnik, and J. Stevens. 2001. Su-
pervised sense tagging using Support Vector Ma-
chines. In Proceedings of the Senseval-2 Work-
shop, Toulouse, July.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with the Open Mind Word
Expert. In Proceedings of the ACL Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions, Philadelphia.
T. Pedersen. 2001. Machine learning with lexical
features: The Duluth approach to Senseval-2. In
Proceedings of the Senseval-2 Workshop, pages
139?142, Toulouse, July.
J. Raker and R. Shukla, editors. 1996. Hip-
pocrene Standard Dictionary English-Hindi
Hindi-English (With Romanized Pronunciation).
Hippocrene Books, New York, NY.
P. Resnik and D. Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New eval-
uation methods for word sense disambiguation.
Natural Language Engineering, 5(2):113?133.
Word Sense Discrimination by Clustering Contexts
in Vector and Similarity Spaces
Amruta Purandare and Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
{pura0010,tpederse}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
This paper systematically compares unsuper-
vised word sense discrimination techniques
that cluster instances of a target word that oc-
cur in raw text using both vector and similarity
spaces. The context of each instance is repre-
sented as a vector in a high dimensional fea-
ture space. Discrimination is achieved by clus-
tering these context vectors directly in vector
space and also by finding pairwise similarities
among the vectors and then clustering in sim-
ilarity space. We employ two different repre-
sentations of the context in which a target word
occurs. First order context vectors represent
the context of each instance of a target word
as a vector of features that occur in that con-
text. Second order context vectors are an indi-
rect representation of the context based on the
average of vectors that represent the words that
occur in the context. We evaluate the discrim-
inated clusters by carrying out experiments us-
ing sense?tagged instances of 24 SENSEVAL-
2 words and the well known Line, Hard and
Serve sense?tagged corpora.
1 Introduction
Most words in natural language have multiple possible
meanings that can only be determined by considering
the context in which they occur. Given a target word
used in a number of different contexts, word sense dis-
crimination is the process of grouping these instances of
the target word together by determining which contexts
are the most similar to each other. This is motivated by
(Miller and Charles, 1991), who hypothesize that words
with similar meanings are often used in similar contexts.
Hence, word sense discrimination reduces to the problem
of finding classes of similar contexts such that each class
represents a single word sense. Put another way, contexts
that are grouped together in the same class represent a
particular word sense.
While there has been some previous work in sense dis-
crimination (e.g., (Schu?tze, 1992), (Pedersen and Bruce,
1997), (Pedersen and Bruce, 1998), (Schu?tze, 1998),
(Fukumoto and Suzuki, 1999)), by comparison it is much
less than that devoted to word sense disambiguation,
which is the process of assigning a meaning to a word
from a predefined set of possibilities. However, solutions
to disambiguation usually require the availability of an
external knowledge source or manually created sense?
tagged training data. As such these are knowledge inten-
sive methods that are difficult to adapt to new domains.
By contrast, word sense discrimination is an unsuper-
vised clustering problem. This is an attractive methodol-
ogy because it is a knowledge lean approach based on ev-
idence found in simple raw text. Manually sense tagged
text is not required, nor are specific knowledge rich re-
sources like dictionaries or ontologies. Instances are clus-
tered based on their mutual contextual similarities which
can be completely computed from the text itself.
This paper presents a systematic comparison of dis-
crimination techniques suggested by Pedersen and Bruce
((Pedersen and Bruce, 1997), (Pedersen and Bruce,
1998)) and by Schu?tze ((Schu?tze, 1992), (Schu?tze,
1998)). This paper also proposes and evaluates several
extensions to these techniques.
We begin with a summary of previous work, and then
a discussion of features and two types of context vec-
tors. We summarize techniques for clustering in vector
versus similarity spaces, and then present our experimen-
tal methodology, including a discussion of the data used
in our experiments. Then we describe our approach to
the evaluation of unsupervised word sense discrimina-
tion. Finally we present an analysis of our experimental
results, and conclude with directions for future work.
2 Previous Work
(Pedersen and Bruce, 1997) and (Pedersen and Bruce,
1998) propose a (dis)similarity based discrimination ap-
proach that computes (dis)similarity among each pair of
instances of the target word. This information is recorded
in a (dis)similarity matrix whose rows/columns repre-
sent the instances of the target word that are to be dis-
criminated. The cell entries of the matrix show the de-
gree to which the pair of instances represented by the
corresponding row and column are (dis)similar. The
(dis)similarity is computed from the first order context
vectors of the instances which show each instance as a
vector of features that directly occur near the target word
in that instance.
(Schu?tze, 1998) introduces second order context vec-
tors that represent an instance by averaging the feature
vectors of the content words that occur in the context of
the target word in that instance. These second order con-
text vectors then become the input to the clustering algo-
rithm which clusters the given contexts in vector space,
instead of building the similarity matrix structure.
There are some significant differences in the ap-
proaches suggested by Pedersen and Bruce and by
Schu?tze. As yet there has not been any systematic study
to determine which set of techniques results in better
sense discrimination. In the sections that follow, we high-
light some of the differences between these approaches.
2.1 Context Representation
Pedersen and Bruce represent the context of each test in-
stance as a vector of features that directly occur near the
target word in that instance. We refer to this representa-
tion as the first order context vector. Schu?tze, by contrast,
uses the second order context representation that averages
the first order context vectors of individual features that
occur near the target word in the instance. Thus, Schu?tze
represents each feature as a vector of words that occur
in its context and then computes the context of the target
word by adding the feature vectors of significant content
words that occur near the target word in that context.
2.2 Features
Pedersen and Bruce use a small number of local features
that include co?occurrence and part of speech informa-
tion near the target word. They select features from the
same test data that is being discriminated, which is a com-
mon practice in clustering in general. Schu?tze represents
contexts in a high dimensional feature space that is cre-
ated using a separate large corpus (referred to as the train-
ing corpus). He selects features based on their frequency
counts or log-likelihood ratios in this corpus.
In this paper, we adopt Schu?tze?s approach and select
features from a separate corpus of training data, in part
because the number of test instances may be relatively
small and may not be suitable for selecting a good feature
set. In addition, this makes it possible to explore varia-
tions in the training data while maintaining a consistent
test set. Since the training data used in unsupervised clus-
tering does not need to be sense tagged, in future work we
plan to develop methods of collecting very large amounts
of raw corpora from the Web and other online sources
and use it to extract features.
Schu?tze represents each feature as a vector of words
that co?occur with that feature in the training data. These
feature vectors are in fact the first order context vectors
of the feature words (and not target word). The words
that co?occur with the feature words form the dimensions
of the feature space. Schu?tze reduces the dimensional-
ity of this feature space using Singular Value Decompo-
sition (SVD), which is also employed by related tech-
niques such as Latent Semantic Indexing (Deerwester et
al., 1990) and Latent Semantic Analysis (Landauer et al,
1998). SVD has the effect of converting a word level
feature space into a concept level semantic space that
smoothes the fine distinctions between features that rep-
resent similar concepts.
2.3 Clustering Space
Pedersen and Bruce represent instances in a
(dis)similarity space where each instance can be seen as
a point and the distance between any two points is a func-
tion of their mutual (dis)similarities. The (dis)similarity
matrix showing the pair-wise (dis)similarities among
the instances is given as the input to the agglomerative
clustering algorithm. The context group discrimination
method used by Schu?tze, on the other hand, operates on
the vector representations of instances and thus works
in vector space. Also he employs a hybrid clustering
approach which uses both an agglomerative and the
Estimation Maximization (EM) algorithm.
3 First Order Context Vectors
First order context vectors directly indicate which fea-
tures make up a context. In all of our experiments, the
context of the target word is limited to 20 surrounding
content words on either side. This is true both when we
are selecting features from a set of training data, or when
we are converting test instances into vectors for cluster-
ing. The particular features we are interested in are bi-
grams and co?occurrences.
Co-occurrences are words that occur within five po-
sitions of the target word (i.e., up to three intervening
words are allowed). Bigrams are ordered pairs of words
that co?occur within five positions of each other. Thus,
co?occurrences are unordered word pairs that include the
target word, whereas bigrams are ordered pairs that may
or may not include the target. Both the co?occurrences
and the bigrams must occur in at least two instances in
the training data, and the two words must have a log?
likelihood ratio in excess of 3.841, which has the effect
of removing co?occurrences and bigrams that have more
than 95% chance of being independent of the target word.
After selecting a set of co-occurrences or bigrams from
a corpus of training data, a first order context representa-
tion is created for each test instance. This shows how
many times each feature occurs in the context of the tar-
get word (i.e., within 20 positions from the target word)
in that instance.
4 Second Order Context Vectors
A test instance can be represented by a second order con-
text vector by finding the average of the first order context
vectors that are associated with the words that occur near
the target word. Thus, the second order context represen-
tation relies on the first order context vectors of feature
words. The second order experiments in this paper use
two different types of features, co?occurrences and bi-
grams, defined as they are in the first order experiments.
Each co?occurrence identified in training data is as-
signed a unique index and occupies the corresponding
row/column in a word co?occurrence matrix. This is
constructed from the co?occurrence pairs, and is a sym-
metric adjacency matrix whose cell values show the log-
likelihood ratio for the pair of words representing the
corresponding row and column. Each row of the co?
occurrence matrix can be seen as a first order context vec-
tor of the word represented by that row. The set of words
forming the rows/columns of the co?occurrence matrix
are treated as the feature words.
Bigram features lead to a bigram matrix such that
for each selected bigram WORDi<>WORDj, WORDi
represents a single row, say the ith row, and WORDj
represents a single column, say the jth column, of
the bigram matrix. Then the value of cell (i,j) indi-
cates the log?likelihood ratio of the words in the bigram
WORDi<>WORDj. Each row of the bigram matrix can
be seen as a bigram vector that shows the scores of all
bigrams in which the word represented by that row oc-
curs as the first word. Thus, the words representing the
rows of the bigram matrix make the feature set while the
words representing the columns form the dimensions of
the feature space.
5 Clustering
The objective of clustering is to take a set of instances
represented as either a similarity matrix or context vec-
tors and cluster together instances that are more like each
other than they are to the instances that belong to other
clusters.
Clustering algorithms are classified into three main
categories, hierarchical, partitional, and hybrid methods
that incorporate ideas from both. The algorithm acts as a
search strategy that dictates how to proceed through the
instances. The actual choice of which clusters to split
or merge is decided by a criteria function. This section
describes the clustering algorithms and criteria functions
that have been employed in our experiments.
5.1 Hierarchical
Hierarchical algorithms are either agglomerative or divi-
sive. They both proceed iteratively, and merge or divide
clusters at each step. Agglomerative algorithms start with
each instance in a separate cluster and merge a pair of
clusters at each iteration until there is only a single clus-
ter remaining. Divisive methods start with all instances
in the same cluster and split one cluster into two during
each iteration until all instances are in their own cluster.
The most widely known criteria functions used with hi-
erarchical agglomerative algorithms are single link, com-
plete link, and average link, also known as UPGMA.
(Schu?tze, 1998) points out that single link clustering
tends to place all instances into a single elongated clus-
ter, whereas (Pedersen and Bruce, 1997) and (Purandare,
2003) show that hierarchical agglomerative clustering
using average link (via McQuitty?s method) fares well.
Thus, we have chosen to use average link/UPGMA as our
criteria function for the agglomerative experiments.
In similarity space, each instance can be viewed as a
node in a weighted graph. The weights on edges joining
two nodes indicate their pairwise similarity as measured
by the cosine between the context vectors that represent
the pair of instances.
When agglomerative clustering starts, each node is in
its own cluster and is considered to be the centroid of that
cluster. At each iteration, average link selects the pair
of clusters whose centroids are most similar and merges
them into a single cluster. For example, suppose the clus-
ters I and J are to be merged into a single cluster IJ . The
weights on all other edges that connect existing nodes to
the new node IJ must now be revised. Suppose that Q is
such a node. The new weight in the graph is computed by
averaging the weight on the edge between nodes I and Q
and that on the edge between J and Q. In other words:
W
?
(IJ,Q) =
W (I,Q) +W (J,Q)
2
(1)
In vector space, average link starts by assigning each
vector to a single cluster. The centroid of each cluster is
found by calculating the average of all the context vec-
tors that make up the cluster. At each iteration, average
link selects the pair of clusters whose centroids are clos-
est with respect to their cosines. The selected pair of clus-
ters is merged and a centroid is computed for this newly
created cluster.
5.2 Partitional
Partitional algorithms divide an entire set of instances
into a predetermined number of clusters (K) without go-
ing through a series of pairwise comparisons. As such
these methods are somewhat faster than hierarchical al-
gorithms.
For example, the well known K-means algorithm is
partitional. In vector space, each instance is represented
by a context vector. K-means initially selects K random
vectors to serve as centroids of these initial K clusters. It
then assigns every other vector to one of the K clusters
whose centroid is closest to that vector. After all vectors
are assigned, it recomputes the cluster centroids by av-
eraging all of the vectors assigned to that cluster. This
repeats until convergence, that is until no vector changes
its cluster across iterations and the centroids stabilize.
In similarity space, each instance can be viewed as a
node of a fully connected weighted graph whose edges in-
dicate the similarity between the instances they connect.
K-means will first select K random nodes that represent
the centroids of the initial K clusters. It will then assign
every other node I to one of the K clusters such that the
edge joining I and the centroid of that cluster has maxi-
mum weight among the edges joining I to all centroids.
5.3 Hybrid Methods
It is generally believed that the quality of clustering by
partitional algorithms is inferior to that of the agglom-
erative methods. However, a recent study (Zhao and
Karypis, 2002) has suggested that these conclusions are
based on experiments conducted with smaller data sets,
and that with larger data sets partitional algorithms are
not only faster but lead to better results.
In particular, Zhao and Karypis recommend a hybrid
approach known as Repeated Bisections. This overcomes
the main weakness with partitional approaches, which is
the instability in clustering solutions due to the choice of
the initial random centroids. Repeated Bisections starts
with all instances in a single cluster. At each iteration it
selects one cluster whose bisection optimizes the chosen
criteria function. The cluster is bisected using standard
K-means method with K=2, while the criteria function
maximizes the similarity between each instance and the
centroid of the cluster to which it is assigned. As such this
is a hybrid method that combines a hierarchical divisive
approach with partitioning.
6 Experimental Data
We use 24 of the 73 words in the SENSEVAL-2 sense?
tagged corpus, and the Line, Hard and Serve sense?
tagged corpora. Each of these corpora are made up of
instances that consist of 2 or 3 sentences that include a
single target word that has a manually assigned sense tag.
However, we ignore the sense tags at all times except
during evaluation. At no point do the sense tags enter into
the clustering or feature selection processes. To be clear,
we do not believe that unsupervised word sense discrim-
ination needs to be carried out relative to a pre-existing
set of senses. In fact, one of the great advantages of un-
supervised technique is that it doesn?t need a manually
annotated text. However, here we employ sense?tagged
text in order to evaluate the clusters that we discover.
The SENSEVAL-2 data is already divided into training
and test sets, and those splits were retained for these ex-
periments. The SENSEVAL-2 data is relatively small, in
that each word has approximately 50-200 training and
test instances. The data is particularly challenging for
unsupervised algorithms due to the large number of fine
grained senses, generally 8 to 12 per word. The small
volume of data combined with large number of possible
senses leads to very small set of examples for most of the
senses.
As a result, prior to clustering we filter the training
and test data independently such that any instance that
uses a sense that occurs in less than 10% of the available
instances for a given word is removed. We then elimi-
nate any words that have less than 90 training instances
after filtering. This process leaves us with a set of 24
SENSEVAL-2 words, which includes the 14 nouns, 6 ad-
jectives and 4 verbs that are shown in Table 1.
In creating our evaluation standard, we assume that
each instance will be assigned to at most a single clus-
ter. Therefore if an instance has multiple correct senses
associated with it, we treat the most frequent of these as
the desired tag, and ignore the others as possible correct
answers in the test data.
The Line, Hard and Serve corpora do not have a stan-
dard training?test split, so these were randomly divided
into 60?40 training?test splits. Due to the large number
of training and test instances for these words, we filtered
out instances associated with any sense that occurred in
less than 5% of the training or test instances.
We also randomly selected five pairs of words from
the SENSEVAL-2 data and mixed their instances together
(while retaining the training and test distinction that al-
ready existed in the data). After mixing, the data was
filtered such that any sense that made up less than 10%
in the training or test data of the new mixed sample was
removed; this is why the total number of instances for the
mixed pairs is not the same as the sum of those for the
individual words. These mix-words were created in order
to provide data that included both fine grained and coarse
grained distinctions.
Table 1 shows all words that were used in our exper-
iments along with their parts of speech. Thereafter we
show the number of training (TRN) and test instances
(TST) that remain after filtering, and the number of
senses found in the test data (S). We also show the per-
centage of the majority sense in the test data (MAJ). This
is particularly useful, since this is the accuracy that would
be attained by a baseline clustering algorithm that puts all
test instances into a single cluster.
7 Evaluation Technique
When we cluster test instances, we specify an upper limit
on the number of clusters that can be discovered. In these
experiments that value is 7. This reflects the fact that
we do not know a?priori the number of possible senses a
word will have. This also allows us to verify the hypothe-
sis that a good clustering approach will automatically dis-
cover approximately same number of clusters as senses
for that word, and the extra clusters (7?#actual senses)
will contain very few instances. As can be seen from col-
umn S in Table 1, most of the words have 2 to 4 senses on
an average. Of the 7 clusters created by an algorithm, we
detect the significant clusters by ignoring (throwing out)
clusters that contain less than 2% of the total instances.
The instances in the discarded clusters are counted as un-
clustered instances and are subtracted from the total num-
ber of instances.
Our basic strategy for evaluation is to assign available
sense tags to the discovered clusters such that the assign-
ment leads to a maximally accurate mapping of senses to
clusters. The problem of assigning senses to clusters be-
comes one of reordering the columns of a confusion ma-
trix that shows how senses and clusters align such that the
diagonal sum is maximized. This corresponds to several
well known problems, among them the Assignment Prob-
lem in Operations Research, or determining the maximal
matching of a bipartite graph in Graph Theory.
During evaluation we assign one sense to at most one
cluster, and vice versa. When the number of discovered
clusters is the same as the number of senses, then there
is a one to one mapping between them. When the num-
ber of clusters is greater than the number of actual senses,
then some clusters will be left unassigned. And when the
number of senses is greater than the number of clusters,
some senses will not be assigned to any cluster. The rea-
son for not assigning a single sense to multiple clusters
or multiple senses to one cluster is that, we are assuming
one sense per instance and one sense per cluster.
We measure the precision and recall based on this max-
imally accurate assignment of sense tags to clusters. Pre-
cision is defined as the number of instances that are clus-
tered correctly divided by the number of instances clus-
tered, while recall is the number of instances clustered
correctly over the total number of instances. From that we
compute the F?measure, which is two times the precision
and recall, divided by the sum of precision and recall.
8 Experimental Results
We present the discrimination results for six configura-
tions of features, context representations and clustering
algorithms. These were run on each of the 27 target
words, and also on the five mixed words. What follows is
a concise description of each configuration.
 PB1 : First order context vectors, using co?
occurrence features, are clustered in similarity space
using the UPGMA technique.
 PB2 : Same as PB1, except that the first order con-
text vectors are clustered in vector space using Re-
peated Bisections.
 PB3: Same as PB1, except the first order con-
text vectors used bigram features instead of co?
occurrences.
All of the PB experiments use first order context repre-
sentations that correspond to the approach suggested by
Pedersen and Bruce.
 SC1: Second order context vectors of instances were
clustered in vector space using the Repeated Bisec-
tions technique. The context vectors were created
from the word co?occurrence matrix whose dimen-
sions were reduced using SVD.
 SC2: Same as SC1 except that the second order con-
text vectors are converted to a similarity matrix and
clustered using the UPGMA method.
 SC3: Same as SC1, except the second order context
vectors were created from the bigram matrix.
All of the SC experiments use second order context
vectors and hence follow the approach suggested by
Schu?tze.
Experiment PB2 clusters the Pedersen and Bruce style
(first order) context vectors using the Schu?tze like cluster-
ing scheme, while SC2 tries to see the effect of using the
Pedersen and Bruce style clustering method on Schu?tze
style (second order) context vectors. The motivation be-
hind experiments PB3 and SC3 is to try bigram features
in both PB and SC style context vectors.
The F?measure associated with the discrimination of
each word is shown in Table 1. Any score that is sig-
nificantly greater than the majority sense (according to a
paired t?test) is shown in bold face.
9 Analysis and Discussion
We employ three different types of data in our experi-
ments. The SENSEVAL-2 words have a relatively small
number of training and test instances (around 50-200).
However, the Line, Hard and Serve data is much larger,
word.pos TRN TST S PB1 SC1 PB2 SC2 PB3 SC3 MAJ
art.n 159 83 4 37.97 45.52 45.46 46.15 43.03 55.34 46.32
authority.n 168 90 4 38.15 51.25 43.93 53.01 41.86 34.94 37.76
bar.n 220 119 5 34.63 37.23 50.66 40.87 41.05 58.26 45.93
channel.n 135 67 6 40.63 37.21 40.31 41.54 36.51 39.06 31.88
child.n 116 62 2 45.04 46.85 51.32 50.00 55.17 53.45 56.45
church.n 123 60 2 57.14 49.09 48.21 55.36 52.73 46.43 59.02
circuit.n 129 75 8 25.17 34.72 32.17 33.33 27.97 25.35 30.26
day.n 239 128 3 60.48 46.15 55.65 45.76 62.65 55.65 62.94
facility.n 110 56 3 40.00 58.00 38.09 58.00 38.46 64.76 48.28
feeling.n 98 45 2 58.23 51.22 52.50 56.10 46.34 53.66 61.70
grip.n 94 49 5 45.66 43.01 58.06 53.76 49.46 49.46 46.67
material.n 111 65 5 32.79 40.98 41.32 47.54 32.79 47.54 42.25
mouth.n 106 55 4 54.90 47.53 60.78 43.14 43.14 47.06 46.97
post.n 135 72 5 32.36 37.96 48.17 30.88 30.88 32.36 32.05
blind.a 97 53 3 53.06 61.18 63.64 58.43 76.29 79.17 82.46
cool.a 102 51 5 35.42 39.58 38.71 34.78 33.68 38.71 42.86
fine.a 93 59 5 47.27 47.71 47.71 33.93 38.18 47.71 41.10
free.a 105 64 3 48.74 49.54 52.54 55.46 45.00 52.99 49.23
natural.a 142 75 4 34.72 35.21 33.56 30.99 32.40 38.03 35.80
simple.a 126 64 4 38.33 50.00 47.06 38.33 38.33 47.06 50.75
begin.v 507 255 3 59.36 40.46 40.40 43.66 70.12 42.55 64.31
leave.v 118 54 5 43.14 38.78 27.73 40.00 46.00 53.47 38.18
live.v 112 59 4 37.83 40.00 48.21 45.45 36.37 41.82 57.63
train.v 116 56 5 28.57 33.96 28.57 34.28 26.67 32.08 33.93
line.n 1615 1197 3 72.67 26.77 62.00 55.47 68.40 37.97 72.10
hard.a 2365 1592 2 86.75 67.42 41.18 73.22 87.06 63.41 87.44
serve.v 2365 1752 4 40.50 33.20 36.82 34.37 45.66 31.46 40.53
cool.a-train.v 197 102 8 22.34 39.00 25.25 40.61 22.57 41.00 22.86
fine.a-cool.a 185 104 7 27.86 42.36 33.83 47.72 35.00 42.05 24.79
fine.a-grip.n 177 99 7 36.84 49.48 33.50 45.02 31.41 49.48 24.19
leave.v-post.n 204 113 8 29.36 48.18 32.11 41.44 23.85 41.82 21.01
post.n-grip.n 208 117 8 28.44 43.67 28.44 41.05 26.55 34.21 20.90
Table 1: F-measures
where each contains around 4200 training and test in-
stances combined. Mixed word are unique because they
combined the instances of multiple target words and
thereby have a larger number of senses to discriminate.
Each type of data brings with it unique characteristics,
and sheds light on different aspects of our experiments.
9.1 Senseval-2 data
Table 2 compares PB1 against PB3, and SC1 against
SC3, when these methods are used to discriminate the 24
SENSEVAL-2 words. Our objective is to study the effect
of using bigram features against co?occurrences in first
(PB) and second (SC) order context vectors while using
relatively small amounts of training data per word. Note
that PB1 and SC1 use co?occurrence features, while PB3
and SC3 rely on bigram features.
This table shows the number of nouns (N), adjec-
tives (A) and verbs (V) where bigrams were more effec-
tive than co-occurrences (bigram>co-occur), less effec-
tive (bigram<co-occur), and had no effect (bigram=co-
occur).
Table 2 shows that there is no clear advantage to us-
ing either bigrams or co?occurrence features in first or-
der context vectors (PB). However, bigram features show
clear improvement in the results of second order context
vectors (SC).
Our hypothesis is that first order context vectors (PB)
represent a small set of bigram features since they are
selected from the relatively small SENSEVAL-2 words.
These features are very sparse, and as such most instances
do not share many common features with other instances,
making first order clustering difficult.
N A V
7 1 2 bigram>co-occur
PB 6 4 2 bigram<co-occur
1 1 0 bigram=co-occur
9 3 3 bigram>co-occur
SC 4 1 1 bigram<co-occur
1 2 0 bigram=co-occur
Table 2: Bigrams vs. Co-occurrences
N A V
PB 9 4 1 rbr>upgma
4 0 2 rbr<upgma
1 2 1 rbr=upgma
SC 8 1 3 rbr>upgma
2 5 0 rbr<upgma
4 0 1 rbr=upgma
Table 3: Repeated Bisections vs. UPGMA
However, second order context vectors indirectly rep-
resent bigram features, and do not require an exact match
between vectors in order to establish similarity. Thus,
the poor performance of bigrams in the case of first or-
der context vectors suggests that when dealing with small
amounts of data, we need to boost or enrich our bigram
feature set by using some other larger training source like
a corpus drawn from the Web.
Table 3 shows the results of using the Repeated Bi-
sections algorithm in vector space (PB) against that of
using UPGMA method in similarity space. This ta-
ble shows the number of Nouns, Adjectives and Verbs
SENSEVAL-2 words that performed better (rbr>upgma),
worse (rbr<upgma), and equal (rbr=upgma) when using
Repeated Bisections clustering versus the UPGMA tech-
nique, on first (PB) and second (SC) order vectors.
In short, Table 3 compares PB1 against PB2 and SC1
against SC2. From this, we observe that with both first
order and second order context vectors Repeated Bisec-
tions is more effective than UPGMA. This suggests that it
is better suited to deal with very small amounts of sparse
data.
Table 4 summarizes the overall performance of each of
these experiments compared with the majority class. This
table shows the number of words for which an experi-
ment performed better than the the majority class, broken
down by part of speech. Note that SC3 and SC1 are most
often better than the majority class, followed closely by
PB2 and SC2. This suggests that the second order con-
text vectors (SC) have an advantage over the first order
vectors for small training data as is found among the 24
SENSEVAL-2 words.
We believe that second order methods work better on
N A V TOTAL
SC3 > MAJ 8 3 1 12
SC1 > MAJ 6 2 2 10
PB2 > MAJ 7 2 0 9
SC2 > MAJ 6 1 2 9
PB1 > MAJ 4 1 1 6
PB3 > MAJ 3 0 2 5
Table 4: All vs. Majority Class
smaller amounts of data, in that the feature spaces are
quite small, and are not able to support the degree of ex-
act matching of features between instances that first order
vectors require. Second order context vectors succeed in
such cases because they find indirect second order co?
occurrences of feature words and hence describe the con-
text more extensively than the first order representations.
With smaller quantities of data, there is less possibil-
ity of finding instances that use exactly the same set of
words. Semantically related instances use words that are
conceptually the same but perhaps not lexically. Sec-
ond order context vectors are designed to identify such
relationships, in that exact matching is not required, but
rather words that occur in similar contexts will have sim-
ilar vectors.
9.2 Line, Hard and Serve data
The comparatively good performance of PB1 and PB3 in
the case of the Line, Hard and Serve data (see Table 1)
suggests that first order context vectors when clustered
with UPGMA perform relatively well on larger samples
of data.
Moreover, among the SC experiments on this data, the
performance of SC2 is relatively high. This further sug-
gests that UPGMA performs much better than Repeated
Bisections with larger amounts of training data.
These observations correspond with the hypothesis
drawn from the SENSEVAL-2 results. That is, a large
amount of training data will lead to a larger feature space
and hence there is a greater chance of matching more fea-
tures directly in the context of the test instances. Hence,
the first order context vectors that rely on the immedi-
ate context of the target word succeed as the contexts are
more likely to use similar sets of words that in turn are
selected from a large feature collection.
9.3 Mix-Word Results
Nearly all of the experiments carried out with the 6 dif-
ferent methods perform better than the majority sense in
the case of the mix-words. This is partially due to the fact
that these words have a large number of senses, and there-
fore have low majority classifiers. In addition, recall that
this data is created by mixing instances of distinct target
words, which leads to a subset of coarse grained (distinct)
senses within the data that are easier to discover than the
senses of a single word.
Table 1 shows that the top 3 experiments for each of
the mixed-words are all second order vectors (SC). We
believe that this is due to the sparsity of the feature spaces
of this data. Since there are so many different senses, the
number of first order features that would be required to
correctly discriminate them is very high, leading to better
results for second order vectors.
10 Future Directions
We plan to conduct experiments that compare the ef-
fect of using very large amounts of training data versus
smaller amounts where each instance includes the tar-
get word (as is the case in this paper). We will draw
our large corpora from a variety of sources, including
the British National Corpus, the English GigaWord Cor-
pus, and the Web. Our motivation is that the larger cor-
pora will provide more generic co?occurrence informa-
tion about words without regard to a particular target
word. However, the data specific to a given target word
will capture the word usages in the immediate context of
the target word. Thus, we will test the hypothesis that
a smaller sample of data where each instance includes
the target word is more effective for sense discrimination
than a more general corpus of training data.
We are also planning to automatically attach descrip-
tive labels to the discovered clusters that capture the un-
derlying word sense. These labels will be created from
the most characteristic features used by the instances be-
longing to the same cluster. By comparing such descrip-
tive features of each cluster with the words that occur in
actual dictionary definitions of the target word, we plan
to carry out fully automated word sense disambiguation
that does not rely on any manually annotated text.
11 Conclusions
We present an extensive comparative analysis of word
sense discrimination techniques using first order and sec-
ond order context vectors, where both can be employed in
similarity and vector space. We conclude that for larger
amounts of homogeneous data such as the Line, Hard and
Serve data, the first order context vector representation
and the UPGMA clustering algorithm are the most effec-
tive at word sense discrimination. We believe this is the
case because in a large sample of data, it is very likely that
the features that occur in the training data will also occur
in the test data, making it possible to represent test in-
stances with fairly rich feature sets. When given smaller
amounts of data like SENSEVAL-2, second order context
vectors and a hybrid clustering method like Repeated Bi-
sections perform better. This occurs because in small and
sparse data, direct first order features are seldom observed
in both the training and the test data. However, the in-
direct second order co?occurrence relationships that are
captured by these methods provide sufficient information
for discrimination to proceed.
12 Acknowledgments
This research is supported by a National Science Foun-
dation Faculty Early CAREER Development Award
(#0092784).
All of the experiments in this paper were carried out
with version 0.47 of the SenseClusters package, freely
available from the URL shown on the title page.
References
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41:391?407.
F. Fukumoto and Y. Suzuki. 1999. Word sense disam-
biguation in untagged text based on term weight learn-
ing. In Proceedings of the Ninth Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 209?216, Bergen.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
T. Pedersen and R. Bruce. 1998. Knowledge lean word
sense disambiguation. In Proceedings of the Fifteenth
National Conference on Artificial Intelligence, pages
800?805, Madison, WI, July.
A. Purandare. 2003. Discriminating among word senses
using McQuitty?s similarity analysis. In Proceedings
of the HLT-NAACL 2003 Student Research Workshop,
pages 19?24, Edmonton, Alberta, Canada, May.
H. Schu?tze. 1992. Dimensions of meaning. In Pro-
ceedings of Supercomputing ?92, pages 787?796, Min-
neapolis, MN.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets. In
Proceedings of the 11th Conference of Information and
Knowledge Management (CIKM), pages 515?524.
