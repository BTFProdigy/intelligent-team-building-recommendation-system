A Machine Learning Approach to Extract Temporal Information from
Texts in Swedish and Generate Animated 3D Scenes
Anders Berglund Richard Johansson Pierre Nugues
Department of Computer Science, LTH
Lund University
SE-221 00 Lund, Sweden
d98ab@efd.lth.se, {richard, pierre}@cs.lth.se
Abstract
Carsim is a program that automatically
converts narratives into 3D scenes. Carsim
considers authentic texts describing road
accidents, generally collected from web
sites of Swedish newspapers or transcribed
from hand-written accounts by victims of
accidents. One of the program?s key fea-
tures is that it animates the generated scene
to visualize events.
To create a consistent animation, Carsim
extracts the participants mentioned in a
text and identifies what they do. In this
paper, we focus on the extraction of tem-
poral relations between actions. We first
describe how we detect time expressions
and events. We then present a machine
learning technique to order the sequence
of events identified in the narratives. We
finally report the results we obtained.
1 Extraction of Temporal Information
and Scene Visualization
Carsim is a program that generates 3D scenes from
narratives describing road accidents (Johansson et
al., 2005; Dupuy et al, 2001). It considers au-
thentic texts, generally collected from web sites
of Swedish newspapers or transcribed from hand-
written accounts by victims of accidents.
One of Carsim?s key features is that it animates
the generated scene to visualize events described
in the narrative. The text below, a newspaper arti-
cle with its translation into English, illustrates the
goals and challenges of it. We bracketed the enti-
ties, time expressions, and events and we anno-
tated them with identifiers, denoted respectively
oi, tj , and ek:
En {bussolycka}e1 i s?dra Afghanistan
kr?vdee2 {p? torsdagen}t1 {20
d?dsoffer}o1 . Ytterligare {39
personer}o2 skadadese3 i olyckane4.
Busseno3 {var p? v?g}e5 fr?n Kanda-
har mot huvudstaden Kabul n?r deno4
under en omk?rninge6 k?rdee7
av v?gbanano5 och voltadee8,
meddeladee9 general Salim Khan,
bitr?dande polischef i Kandahar.
TT-AFP & Dagens Nyheter, July 8,
2004
{20 persons}o1 diede2 in a {bus
accident}e1 in southern Afghanistan
{on Thursday}t1. In addition, {39
persons}o2 {were injured}e3 in the
accidente4.
The buso3 {was on its way}e5 from
Kandahar to the capital Kabul when
ito4 {drove off}e7 the roado5 while
overtakinge6 and {flipped over}e8,
saide9 General Salim Khan, assistant
head of police in Kandahar.
The text above, our translation.
To create a consistent animation, the program
needs to extract and understand who the partici-
pants are and what they do. In the case of the ac-
cident above, it has to:
1. Detect the involved physical entities o3, o4,
and o5.
2. Understand that the pronoun o4 refers to o3.
3. Detect the events e6, e7, and e8.
385
4. Link the participants to the events using se-
mantic roles or grammatical functions and in-
fer the unmentioned vehicle that is overtaken.
5. Understand that the order of the events is e6-
e7-e8.
6. Detect the time expression t1 to anchor tem-
porally the animation.
In this paper, we describe how we address tasks
3, 5, and 6 within the Carsim program, i.e., how
we detect, interpret, and order events and how we
process time expressions.
2 Previous Work
Research on the representation of time, events,
and temporal relations dates back the beginning
of logic. It resulted in an impressive number of
formulations and models. In a review of contem-
porary theories and an attempt to unify them, Ben-
nett and Galton (2004) classified the most influen-
tial formalisms along three lines. A first approach
is to consider events as transitions between states
as in STRIPS (Fikes and Nilsson, 1971). A sec-
ond one is to map events on temporal intervals
and to define relations between pairs of intervals.
Allen?s (1984) 13 temporal relations are a widely
accepted example of this. A third approach is to
reify events, to quantify them existentially, and
to connect them to other objects using predicates
based on action verbs and their modifiers (David-
son, 1967). The sentence John saw Mary in Lon-
don on Tuesday is then translated into the logical
form: ?[Saw(, j,m)?Place(, l)?T ime(, t)].
Description of relations between time, events,
and verb tenses has also attracted a considerable
interest, especially in English. Modern work on
temporal event analysis probably started with Re-
ichenbach (1947), who proposed the distinction
between the point of speech, point of reference,
and point of event in utterances. This separation
allows for a systematic description of tenses and
proved to be very powerful.
Many authors proposed general principles to
extract automatically temporal relations between
events. A basic observation is that the tempo-
ral order of events is related to their narrative or-
der. Dowty (1986) investigated it and formulated a
Temporal Discourse Interpretation Principle to in-
terpret the advance of narrative time in a sequence
of sentences. Lascarides and Asher (1993) de-
scribed a complex logical framework to deal with
events in simple past and pluperfect sentences.
Hitzeman et al (1995) proposed a constraint-
based approach taking into account tense, aspect,
temporal adverbials, and rhetorical structure to an-
alyze a discourse.
Recently, groups have used machine learn-
ing techniques to determine temporal relations.
They trained automatically classifiers on hand-
annotated corpora. Mani et al (2003) achieved
the best results so far by using decision trees to
order partially events of successive clauses in En-
glish texts. Boguraev and Ando (2005) is another
example of it for English and Li et al (2004) for
Chinese.
3 Annotating Texts with Temporal
Information
Several schemes have been proposed to anno-
tate temporal information in texts, see Setzer and
Gaizauskas (2002), inter alia. Many of them were
incompatible or incomplete and in an effort to rec-
oncile and unify the field, Ingria and Pustejovsky
(2002) introduced the XML-based Time markup
language (TimeML).
TimeML is a specification language whose
goal is to capture most aspects of temporal rela-
tions between events in discourses. It is based
on Allen?s (1984) relations and a variation of
Vendler?s (1967) classification of verbs. It de-
fines XML elements to annotate time expressions,
events, and ?signals?. The SIGNAL tag marks sec-
tions of text indicating a temporal relation. It
includes function words such as later and not.
TimeML also features elements to connect entities
using different types of links, most notably tem-
poral links, TLINKs, that describe the temporal re-
lation holding between events or between an event
and a time.
4 A System to Convert Narratives of
Road Accidents into 3D Scenes
4.1 Carsim
Carsim is a text-to-scene converter. From a nar-
rative, it creates a complete and unambiguous 3D
geometric description, which it renders visually.
Carsim considers authentic texts describing road
accidents, generally collected from web sites of
Swedish newspapers or transcribed from hand-
written accounts by victims of accidents. One of
the program?s key features is that it animates the
generated scene to visualize events.
386
The Carsim architecture is divided into two
parts that communicate using a frame representa-
tion of the text. Carsim?s first part is a linguistic
module that extracts information from the report
and fills the frame slots. The second part is a vir-
tual scene generator that takes the structured rep-
resentation as input, creates the visual entities, and
animates them.
4.2 Knowledge Representation in Carsim
The Carsim language processing module reduces
the text content to a frame representation ? a tem-
plate ? that outlines what happened and enables a
conversion to a symbolic scene. It contains:
? Objects. They correspond to the physical en-
tities mentioned in the text. They also include
abstract symbols that show in the scene. Each
object has a type, that is selected from a pre-
defined, finite set. An object?s semantics is
a separate geometric entity, where its shape
(and possibly its movement) is determined by
its type.
? Events. They correspond intuitively to an ac-
tivity that goes on during a period in time
and here to the possible object behaviors. We
represent events as entities with a type taken
from a predefined set, where an event?s se-
mantics will be a proposition paired with a
point or interval in time during which the
proposition is true.
? Relations and Quantities. They describe spe-
cific features of objects and events and how
they are related to each other. The most obvi-
ous examples of such information are spatial
information about objects and temporal in-
formation about events. Other meaningful re-
lations and quantities include physical prop-
erties such as velocity, color, and shape.
5 Time and Event Processing
We designed and implemented a generic com-
ponent to extract temporal information from the
texts. It sits inside the natural language part of
Carsim and proceeds in two steps. The first step
uses a pipeline of finite-state machines and phrase-
structure rules that identifies time expressions, sig-
nals, and events. This step also generates a feature
vector for each element it identifies. Using the
vectors, the second step determines the temporal
relations between the extracted events and orders
them in time. The result is a text annotated using
the TimeML scheme.
We use a set of decision trees and a machine
learning approach to find the relations between
events. As input to the second step, the decision
trees take sequences of events extracted by the
first step and decide the temporal relation, possi-
bly none, between pairs of them. To run the learn-
ing algorithm, we manually annotated a small set
of texts on which we trained the trees.
5.1 Processing Structure
We use phrase-structure rules and finite state ma-
chines to mark up events and time expressions. In
addition to the identification of expressions, we of-
ten need to interpret them, for instance to com-
pute the absolute time an expression refers to. We
therefore augmented the rules with procedural at-
tachments.
We wrote a parser to control the processing flow
where the rules, possibly recursive, apply regular
expressions, call procedures, and create TimeML
entities.
5.2 Detection of Time Expressions
We detect and interpret time expressions with a
two-level structure. The first level processes in-
dividual tokens using a dictionary and regular ex-
pressions. The second level uses the results from
the token level to compute the meaning of multi-
word expressions.
Token-Level Rules. In Swedish, time expres-
sions such as en tisdagseftermiddag ?a Tuesday
afternoon? use nominal compounds. To decode
them, we automatically generate a comprehensive
dictionary with mappings from strings onto com-
pound time expressions. We decode other types
of expressions such as 2005-01-14 using regular
expressions
Multiword-Level Rules. We developed a
grammar to interpret the meaning of multiword
time expressions. It includes instructions on how
to combine the values of individual tokens for ex-
pressions such as {vid lunchtid}t1 {en tisdagefter-
middag}t2 ?{at noon}t1 {a Tuesday afternoon}t2?.
The most common case consists in merging the to-
kens? attributes to form a more specific expression.
However, relative time expressions such as i tors-
dags ?last Tuesday? are more complex. Our gram-
mar handles the most frequent ones, mainly those
387
that need the publishing date for their interpreta-
tion.
5.3 Detection of Signals
We detect signals using a lexicon and na?ve string
matching. We annotate each signal with a sense
where the possible values are: negation, before, af-
ter, later, when, and continuing. TimeML only de-
fines one attribute for the SIGNAL tag, an identifier,
and encodes the sense as an attribute of the LINKs
that refer to it. We found it more appropriate to
store the sense directly in the SIGNAL element, and
so we extended it with a second attribute.
We use the sense information in decision trees
as a feature to determine the order of events. Our
strategy based on string matching results in a lim-
ited overdetection. However, it does not break the
rest of the process.
5.4 Detection of Events
We detect the TimeML events using a part-of-
speech tagger and phrase-structure rules. We con-
sider that all verbs and verb groups are events. We
also included some nouns or compounds, which
are directly relevant to Carsim?s application do-
main, such as bilolycka ?car accident? or krock
?collision?. We detect these nouns through a set
of six morphemes.
TimeML annotates events with three features:
aspect, tense, and ?class?, where the class corre-
sponds to the type of the event. The TimeML spec-
ifications define seven classes. We kept only the
two most frequent ones: states and occurrences.
We determine the features using procedures at-
tached to each grammatical construct we extract.
The grammatical features aspect and tense are
straightforward and a direct output of the phrase-
structure rules. To infer the TimeML class, we use
heuristics such as these ones: predicative clauses
(copulas) are generally states and verbs in preterit
are generally occurrences.
The domain, reports of car accidents, makes
this approach viable. The texts describe sequences
of real events. They are generally simple, to the
point, and void of speculations and hypothetical
scenarios. This makes the task of feature identifi-
cation simpler than it is in more general cases.
In addition to the TimeML features, we extract
the grammatical properties of events. Our hypoth-
esis is that specific sequences of grammatical con-
structs are related to the temporal order of the de-
scribed events. The grammatical properties con-
sist of the part of speech, noun (NOUN) or verb
(VB). Verbs can be finite (FIN) or infinitive (INF).
They can be reduced to a single word or part of a
group (GR). They can be a copula (COP), a modal
(MOD), or a lexical verb. We combine these prop-
erties into eight categories that we use in the fea-
ture vectors of the decision trees (see ...EventStruc-
ture in Sect. 6.2).
6 Event Ordering
TimeML defines three different types of links:
subordinate (SLINK), temporal (TLINK), and aspec-
tual (ALINK). Aspectual links connect two event in-
stances, one being aspectual and the other the ar-
gument. As its significance was minor in the visu-
alization of car accidents, we set aside this type of
link.
Subordinate links generally connect signals to
events, for instance to mark polarity by linking a
not to its main verb. We identify these links simul-
taneously with the event detection. We augmented
the phrase-structure rules to handle subordination
cases at the same time they annotate an event. We
restricted the cases to modality and polarity and
we set aside the other ones.
6.1 Generating Temporal Links
To order the events in time and create the tempo-
ral links, we use a set of decision trees. We apply
each tree to sequences of events where it decides
the order between two of the events in each se-
quence. If e1, ..., en are the events in the sequence
they appear in the text, the trees correspond to the
following functions:
fdt1(ei, ei+1) ? trel(ei, ei+1)
fdt2(ei, ei+1, ei+2) ? trel(ei, ei+1)
fdt3(ei, ei+1, ei+2) ? trel(ei+1, ei+2)
fdt4(ei, ei+1, ei+2) ? trel(ei, ei+2)
fdt5(ei, ei+1, ei+2, ei+3) ? trel(ei, ei+3)
The possible output values of the trees are: si-
multaneous, after, before, is_included, includes,
and none. These values correspond to the relations
described by Setzer and Gaizauskas (2001).
The first decision tree should capture more gen-
eral relations between two adjacent events with-
out the need of a context. Decision trees dt2 and
dt3 extend the context by one event to the left re-
spectively one event to the right. They should cap-
ture more specific phenomena. However, they are
not always applicable as we never apply a decision
388
tree when there is a time expression between any
of the events involved. In effect, time expressions
?reanchor? the narrative temporally, and we no-
ticed that the decision trees performed very poorly
across time expressions.
We complemented the decision trees with a
small set of domain-independent heuristic rules
that encode common-sense knowledge. We as-
sume that events in the present tense occur after
events in the past tense and that all mentions of
events such as olycka ?accident? refer to the same
event. In addition, the Carsim event interpreter
recognizes some semantically motivated identity
relations.
6.2 Feature Vectors
The decision trees use a set of features correspond-
ing to certain attributes of the considered events,
temporal signals between them, and some other
parameters such as the number of tokens separat-
ing the pair of events to be linked. We list below
the features of fdt1 together with their values. The
first event in the pair is denoted by a mainEvent pre-
fix and the second one by relatedEvent:
? mainEventTense: none, past, present, future,
NOT_DETERMINED.
? mainEventAspect: progressive, perfective, per-
fective_progressive, none, NOT_DETERMINED.
? mainEventStructure: NOUN, VB_GR_COP_INF,
VB_GR_COP_FIN, VB_GR_MOD_INF,
VB_GR_MOD_FIN, VB_GR, VB_INF, VB_FIN,
UNKNOWN.
? relatedEventTense: (as mainEventTense)
? relatedEventAspect: (as mainEventAspect)
? relatedEventStructure: (as mainEventStructure)
? temporalSignalInbetween: none, before, after,
later, when, continuing, several.
? tokenDistance: 1, 2 to 3, 4 to 6, 7 to 10, greater
than 10.
? sentenceDistance: 0, 1, 2, 3, 4, greater than 4.
? punctuationSignDistance: 0, 1, 2, 3, 4, 5, greater
than 5.
The four other decision trees consider more
events but use similar features. The values for the
...Distance features are of course greater.
6.3 Temporal Loops
The process described above results in an overgen-
eration of temporal links. As some of them may be
conflicting, a post-processing module reorganizes
them and discards the temporal loops.
The initial step of the loop resolution assigns
each link with a score. This score is created by the
decision trees and is derived from the C4.5 metrics
(Quinlan, 1993). It reflects the accuracy of the leaf
as well as the overall accuracy of the decision tree
in question. The score for links generated from
heuristics is rule dependent.
The loop resolution algorithm begins with an
empty set of orderings. It adds the partial order-
ings to the set if their inclusion doesn?t introduce
a temporal conflict. It first adds the links with the
highest scores, and thus, in each temporal loop, the
ordering with the lowest score is discarded.
7 Experimental Setup and Evaluation
As far as we know, there is no available time-
annotated corpus in Swedish, which makes the
evaluation more difficult. As development and
test sets, we collected approximately 300 reports
of road accidents from various Swedish newspa-
pers. Each report is annotated with its publishing
date. Analyzing the reports is complex because
of their variability in style and length. Their size
ranges from a couple of sentences to more than a
page. The amount of details is overwhelming in
some reports, while in others most of the informa-
tion is implicit. The complexity of the accidents
described ranges from simple accidents with only
one vehicle to multiple collisions with several par-
ticipating vehicles and complex movements.
We manually annotated a subset of our corpus
consisting of 25 texts, 476 events and 1,162 tem-
poral links. We built the trees automatically from
this set using the C4.5 program (Quinlan, 1993).
Our training set is relatively small and the num-
ber of features we use relatively large for the set
size. This can produce a training overfit. However,
C4.5, to some extent, makes provision for this and
prunes the decision trees.
We evaluated three aspects of the temporal in-
formation extraction modules: the detection and
interpretation of time expressions, the detection
and interpretation of events, and the quality of the
final ordering. We report here the detection of
events and the final ordering.
389
Feature Ncorrect Nerroneous Correct
Tense 179 1 99.4%
Aspect 161 19 89.4%
Class 150 30 83.3%
Table 1: Feature detection for 180 events.
7.1 Event Detection
We evaluated the performance of the event detec-
tion on a test corpus of 40 previously unseen texts.
It should be noted that we used a simplified defi-
nition of what an event is, and that the manual an-
notation and evaluation were both done using the
same definition (i.e. all verbs, verb groups, and a
small number of nouns are events). The system
detected 584 events correctly, overdetected 3, and
missed 26. This gives a recall of 95.7%, a preci-
sion of 99.4%, and an F -measure of 97.5%.
The feature detection is more interesting and
Table 1 shows an evaluation of it. We carried out
this evaluation on the first 20 texts of the test cor-
pus.
7.2 Evaluation of Final Ordering
We evaluated the final ordering with the method
proposed by Setzer and Gaizauskas (2001). Their
scheme is comprehensive and enables to compare
the performance of different systems.
Description of the Evaluation Method. Set-
zer and Gaizauskas carried out an inter-annotator
agreement test for temporal relation markup.
When evaluating the final ordering of a text, they
defined the set E of all the events in the text and
the set T of all the time expressions. They com-
puted the set (E ? T )? (E ? T ) and they defined
the sets S`, I`, and B` as the transitive closures
for the relations simultaneous, includes, and be-
fore, respectively.
If S`k and S`r represent the set S` for the an-
swer key (?Gold Standard?) and system response,
respectively, the measures of precision and recall
for the simultaneous relation are:
R = |S
`
k ? S`r |
|S`k |
P = |S
`
k ? S`r |
|S`r |
For an overall measure of recall and precision,
Setzer and Gaizauskas proposed the following for-
mulas:
R = |S
`
k ? S`r | + |B`k ?B`r | + |I`k ? I`r |
|S`k | + |B`k | + |I`k |
P = |S
`
k ? S`r | + |B`k ?B`r | + |I`k ? I`r |
|S`r | + |B`r | + |I`r |
They used the classical definition of the F -
measure: the harmonic means of precision and re-
call. Note that the precision and recall are com-
puted per text, not for all relations in the test set
simultaneously.
Results. We evaluated the output of the Car-
sim system on 10 previously unseen texts against
our Gold Standard. As a baseline, we used a sim-
ple algorithm that assumes that all events occur in
the order they are introduced in the narrative. For
comparison, we also did an inter-annotator evalu-
ation on the same texts, where we compared the
Gold Standard, annotated by one of us, with the
annotation produced by another member in our
group.
As our system doesn?t support comparisons of
time expressions, we evaluated the relations con-
tained in the set E ? E. We only counted the
reflexive simultaneous relation once per tuples
(ex, ey) and (ey, ex) and we didn?t count relations
(ex, ex).
Table 2 shows our results averaged over the
10 texts. As a reference, we also included Set-
zer and Gaizauskas? averaged results for inter-
annotator agreement on temporal relations in six
texts. Their results are not directly comparable
however as they did the evaluation over the set
(E ? T ) ? (E ? T ) for English texts of another
type.
Comments. The computation of ratios on the
transitive closure makes Setzer and Gaizauskas?
evaluation method extremely sensitive. Missing a
single link often results in a loss of scores of gener-
ated transitive links and thus has a massive impact
on the final evaluation figures.
As an example, one of our texts contains six
events whose order is e4 < e5 < e6 < e1 < e2 <
e3. The event module automatically detects the
chains e4 < e5 < e6 and e1 < e2 < e3 correctly,
but misses the link e6 < e1. This gives a recall of
6/15 = 0.40. When considering evaluations per-
formed using the method above, it is meaningful
to have this in mind.
8 Carsim Integration
The visualization module considers a subset of the
detected events that it interprets graphically. We
390
Evaluation Average nwords Average nevents Pmean Rmean Fmean
Gold vs. Baseline 98.5 14.3 49.42 29.23 35.91
Gold vs. Automatic " " 54.85 37.72 43.97
Gold vs. Other Annotator " " 85.55 58.02 68.01
Setzer and Gaizauskas 312.2 26.7 67.72 40.07 49.13
Table 2: Evaluation results for final ordering averaged per text (with P , R, and F in %).
call this subset the Carsim events. Once the event
processing has been done, Carsim extracts these
specific events from the full set using a small do-
main ontology and inserts them into the template.
We use the event relations resulting from temporal
information extraction module to order them. For
all pairs of events in the template, Carsim queries
the temporal graph to determine their relation.
Figure 1 shows a part of the template represent-
ing the accident described in Section 1. It lists
the participants, with the unmentioned vehicle in-
ferred to be a car. It also shows the events and
their temporal order. Then, the visualization mod-
ule synthesizes a 3D scene and animates it. Fig-
ure 2 shows four screenshots picturing the events.
Figure 1: Representation of the accident in the ex-
ample text.
9 Conclusion and Perspectives
We have developed a method for detecting time
expressions, events, and for ordering these events
temporally. We have integrated it in a text-to-
scene converter enabling the animation of generic
actions.
The module to detect time expression and inter-
pret events performs significantly better than the
baseline technique used in previous versions of
Carsim. In addition, it should to be easy to sep-
arate it from the Carsim framework and reuse it in
other domains.
The central task, the ordering of all events,
leaves lots of room for improvement. The accu-
racy of the decision trees should improve with a
larger training set. It would result in a better over-
all performance. Switching from decision trees to
other training methods such as Support Vector Ma-
chines or using semantically motivated features, as
suggested by Mani (2003), could also be sources
of improvements.
More fundamentally, the decision tree method
we have presented is not able to take into account
long-distance links. Investigation into new strate-
gies to extract such links directly without the com-
putation of a transitive closure would improve re-
call and, given the evaluation procedure, increase
the performance.
References
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23(2):123?
154.
Brandon Bennett and Antony P. Galton. 2004. A uni-
fying semantics for time and events. Artificial Intel-
ligence, 153(1-2):13?48.
Branimir Boguraev and Rie Kubota Ando. 2005.
TimeML-compliant text analysis for temporal rea-
soning. In IJCAI-05, Proceedings of the Nineteenth
International Joint Conference on Artificial Intelli-
gence, pages 997?1003, Edinburgh, Scotland.
Donald Davidson. 1967. The logical form of action
sentences. In N. Rescher, editor, The Logic of Deci-
sion and Action. University of Pittsburgh Press.
David R. Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: Semantics or
pragmatics? Linguistics and Philosophy, 9:37?61.
391
Figure 2: Animation of the scene and event visualization.
Sylvain Dupuy, Arjan Egges, Vincent Legendre, and
Pierre Nugues. 2001. Generating a 3D simulation
of a car accident from a written description in nat-
ural language: The Carsim system. In ACL 2001,
Workshop on Temporal and Spatial Information Pro-
cessing, pages 1?8, Toulouse, France.
Richard Fikes and Nils J. Nilsson. 1971. Strips: A
new approach to the application of theorem proving
to problem solving. Artificial Intelligence, 2:189?
208.
Janet Hitzeman, Marc Noels Moens, and Clare Grover.
1995. Algorithms for analyzing the temporal struc-
ture of discourse. In Proceedings of the Annual
Meeting of the European Chapter of the Associa-
tion of Computational Linguistics, pages 253?260,
Dublin, Ireland.
Bob Ingria and James Pustejovsky. 2002. Specification
for TimeML 1.0.
Richard Johansson, Anders Berglund, Magnus
Danielsson, and Pierre Nugues. 2005. Automatic
text-to-scene conversion in the traffic accident
domain. In IJCAI-05, Proceedings of the Nineteenth
International Joint Conference on Artificial Intelli-
gence, pages 1073?1078, Edinburgh, Scotland.
Alex Lascarides and Nicholas Asher. 1993. Tem-
poral interpretation, discourse relations, and com-
mon sense entailment. Linguistics & Philosophy,
16(5):437?493.
Wenjie Li, Kam-Fai Wong, Guihong Cao, and Chunfa
Yuan. 2004. Applying machine learning to Chinese
temporal relation resolution. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), pages 582?588, Barcelona.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in
news. In Human Language Technology Conference
(HLT?03), Edmonton, Canada.
Inderjeet Mani. 2003. Recent developments in tempo-
ral information extraction. In Nicolas Nicolov and
Ruslan Mitkov, editors, Proceedings of RANLP?03.
John Benjamins.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kauffman.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Academic Press, New York.
Andrea Setzer and Robert Gaizauskas. 2001. A pi-
lot study on annotating temporal relations in text. In
ACL 2001, Workshop on Temporal and Spatial Infor-
mation Processing, pages 73?80, Toulouse, France.
Andrea Setzer and Robert Gaizauskas. 2002. On the
importance of annotating temporal event-event rela-
tions in text. In LREC 2002, Workshop on Annota-
tion Standards for Temporal Information in Natural
Language.
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press, Ithaca, New York.
392
Carsim: A System to Visualize Written Road Accident Reports as Animated
3D Scenes
Richard Johansson David Williams Anders Berglund Pierre Nugues
LUCAS, Department of Computer Science, Lund University
Box 118
SE-221 00 Lund, Sweden
{richard, pierre}@cs.lth.se, {d98dw, d98ab}@efd.lth.se
Abstract
This paper describes a system to create animated
3D scenes of car accidents from reports written in
Swedish. The system has been developed using
news reports of varying size and complexity. The
text-to-scene conversion process consists of two
stages. An information extraction module creates
a structured representation of the accident and a vi-
sual simulator generates and animates the scene.
We first describe the overall structure of the text-
to-scene conversion and the structure of the repre-
sentation. We then explain the information extrac-
tion and visualization modules. We show snapshots
of the car animation output and we conclude with
the results we obtained.
1 Text-to-Scene Conversion
As noted by Michel Denis, language and images are
two different representation modes whose cooper-
ation is needed in many forms of cognitive opera-
tions. The description of physical events, mathe-
matical theorems, or structures of any kind using
language is sometimes difficult to understand. Im-
ages and graphics can then help understand ideas or
situations and realize their complexity. They have
an indisputable capacity to represent and to commu-
nicate knowledge and are an effective means to rep-
resent and explain things, see (Kosslyn, 1983; Tufte,
1997; Denis, 1991).
Narratives of a car accidents, for instance, often
make use of space descriptions, movements, and
directions that are sometimes difficult to grasp for
most readers. We believe that forming consistent
mental images are necessary to understand them
properly. However, some people have difficulties in
imagining situations and may need visual aids pre-
designed by professional analysts.
In this paper, we will describe Carsim, a text-to-
scene converter that automates the generation of im-
ages from texts.
2 Related Work
The conversion of natural language texts into graph-
ics has been investigated in a few projects. NALIG
(Adorni et al, 1984; Manzo et al, 1986) is an early
example of them that was aimed at recreating static
2D scenes. One of its major goals was to study rela-
tionships between space and prepositions. NALIG
considered simple phrases in Italian of the type sub-
ject, preposition, object that in spite of their simplic-
ity can have ambiguous interpretations. From what
is described in the papers, NALIG has not been ex-
tended to process sentences and even less to texts.
WordsEye (Coyne and Sproat, 2001) is an im-
pressive system that recreates 3D animated scenes
from short descriptions. The number of 3D objects
WordsEye uses ? 12,000 ? gives an idea of its am-
bition. WordsEye integrates resources such as the
Collins? dependency parser and the WordNet lexical
database. The narratives cited as examples resemble
imaginary fairy tales and WordsEye does not seem
to address real world stories.
CogViSys is a last example that started with the
idea of generating texts from a sequence of video
images. The authors found that it could also be
useful to reverse the process and generate synthetic
video sequences from texts. The logic engine be-
hind the text-to-scene converter (Arens et al, 2002)
is based on the Discourse Representation Theory.
The system is limited to the visualization of single
vehicle maneuvers at an intersection as the one de-
scribed in this two-sentence narrative: A car came
from Kriegstrasse. It turned left at the intersection.
The authors give no further details on the text cor-
pus and no precise description of the results.
3 Carsim
Carsim (Egges et al, 2001; Dupuy et al, 2001) is
a program that analyzes texts describing car acci-
dents and visualizes them in a 3D environment. It
has been developed using real-world texts.
The Carsim architecture is divided into two parts
that communicate using a formal representation of
Input Text
Linguistic
Component
Formal
Description
Visualizer
Component
Output
Animation
Figure 1: The Carsim architecture.
the accident. Carsim?s first part is a linguistic mod-
ule that extracts information from the report and fills
the frame slots. The second part is a virtual scene
generator that takes the structured representation as
input, creates the visual entities, and animates them
(Figure 1).
4 A Corpus of Traffic Accident
Descriptions
As development and test sets, we have collected ap-
proximately 200 reports of road accidents from vari-
ous Swedish newspapers. The task of analyzing the
news reports is made more complex by their vari-
ability in style and length. The size of the texts
ranges from a couple of sentences to more than a
page. The amount of details is overwhelming in
some reports, while in others most of the informa-
tion is implicit. The complexity of the accidents de-
scribed ranges from simple accidents with only one
vehicle to multiple collisions with several partici-
pating vehicles and complex movements.
Although our work has concentrated on the press
clippings, we also have access to accident reports
from the STRADA database (Swedish TRaffic Ac-
cident Data Acquisition) of Va?gverket, the Swedish
traffic authority. STRADA registers nearly all the
accidents that occur in Sweden (Karlberg, 2003).
(All the accidents where there are casualties.) Af-
ter an accident, the victims describe the location
and conditions of it in a standardized form col-
lected in hospitals. The corresponding reports are
transcribed in a computer-readable format in the
STRADA database. This source contains two kinds
of reports: the narratives written by the victims of
the accident and their transcriptions by traffic ex-
perts. The original texts contain spelling mistakes,
abbreviations, and grammatical errors. The tran-
scriptions often simplify, interpret the original texts,
and contain jargon.
The next text is an excerpt from our development
corpus. This report is an example of a press wire
describing an accident.
En do?dsolycka intra?ffade inatt so?der
om Vissefja?rda pa? riksva?g 28. Det var en
bil med tva? personer i som kom av va?gen i
en va?nsterkurva och ko?rde i ho?g hastighet
in i en gran. Passageraren, som var fo?dd
-84, dog. Fo?raren som var 21 a?r gam-
mal va?rdas pa? sjukhus med sva?ra skador.
Polisen missta?nker att bilen de fa?rdades i,
en ny Saab, var stulen i Emmaboda och
det ska under dagen underso?kas.
Sveriges Radio, November 9, 2002
A fatal accident took place tonight south
of Vissefja?rda on Road 28. A car carry-
ing two persons departed from the road
in a left-hand curve and crashed at a high
speed into a spruce. The passenger, who
was born in 1984, died. The driver, who
was 21 years old, is severely injured and
is taken care of in a hospital. The police
suspects that the car they were traveling
in, a new Saab, was stolen in Emmaboda
and will investigate it today.
The text above, our translation.
5 Knowledge Representation
The Carsim language processing module reduces
the text content to a formal representation that out-
lines what happened and enables a conversion to a
symbolic scene. It uses information extraction tech-
niques to map a text onto a structure that consists of
three main elements:
? A scene object, which describes the static pa-
rameters of the environment, such as weather,
light, and road configuration.
? A list of road objects, for example cars, trucks,
and trees, and their associated sequences of
movements.
? A list of collisions between road objects.
The structure of the formalism, which sets the
limit of what information can be expressed, was de-
signed with the help of traffic safety experts at the
Department of Traffic and Road at Lund University.
It contains the information necessary to reproduce
and animate the accident entities in our visualiza-
tion model. We used an iterative process to design
it. We started from a first incomplete model (Dupuy
et al, 2001) and we manually constructed the rep-
resentation of about 50 texts until we had reached a
sufficient degree of expressivity.
The representation we use is a typical example of
frames a` la Minsky, where the objects in the rep-
resentation consist of a number of attribute/values
slots which are to be filled by the information ex-
traction module. Each object in the representation
Figure 2: Representation of the accident in the ex-
ample above.
belongs to a concept in a domain ontology we have
developed. The concepts are ordered in an inheri-
tance hierarchy.
Figure 2 shows how Carsim?s graphical user in-
terface presents the representation of the accident
in the example above. The scene element contains
the location of the accident and the configuration of
roads, in this case a left-hand bend. The list of road
objects contains one car and one tree. The event
chain for the car describes the movements: the car
leaves the road. Finally, the collision list describes
one collision between the car and the tree.
6 The Information Extraction Module
The information extraction subsystem fills the frame
slots. Its processing flow consists in analyzing the
text linguistically using the word groups obtained
from the linguistic modules and a sequence of se-
mantic modules. The information extraction sub-
system uses the literal content of certain phrases it
finds in the text or infers the environment and the
actions.
We use a pipeline of modules in the first stages
of the natural language processing chain. The
tasks consists of tokenizing, part-of-speech tagging,
splitting into sentences, detecting the noun groups,
clause boundaries, and domain-specific multiwords.
We use the Granska part-of-speech tagger (Carl-
berger and Kann, 1999) and Ejerhed?s algorithm
(Ejerhed, 1996) to detect clause boundaries.
6.1 Named Entity Recognition
Carsim uses a domain-specific named entity recog-
nition module, which detects names of persons,
places, roads, and car makes (Persson and Daniels-
son, 2004).
The recognition is based on a small database of
2,500 entries containing person names, city and re-
gion names, and car names. It applies a cascade
of regular expressions that takes into account the
morphology of Swedish proper noun formation and
the road nomenclature. The recall/precision perfor-
mance of the detector is 0.89/0.97.
6.2 Finding the Participants
The system uses the detected noun groups to iden-
tify the physical objects, which are involved in the
accident. It extracts the headword of each group and
associates it to an entity in the ontology. We used
parts of the Swedish WordNet as a resource to de-
velop this dictionary (A?ke Viberg et al, 2002).
We track the entities along the text with a sim-
ple coreference resolution algorithm. It assumes
that each definite expression corefers with the last
sortally consistent (according to the ontology) en-
tity which was mentioned. Indefinite expressions
are assumed to be references to previously unmen-
tioned entities. This is similar to the algorithm men-
tioned in (Appelt and Israel, 1999). Although this
approach is relatively simple, we get reasonable re-
sults with it and could use it as a baseline when in-
vestigating other approaches.
Figure 3 shows an excerpt from a text with the
annotation of the participants as well as their coref-
erences.
Olyckan intra?ffade na?r [bilen]1 som de fem
fa?rdades i ko?rde om [en annan personbil]2 . Na?r
[den]1 sva?ngde tillbaka in framfo?r [den omko?rda
bilen]2 fick [den]1 sladd och for med sidan rakt
mot fronten pa? [den mo?tande lastbilen]3 .
The accident took place when [the car]1 where the
five people were traveling overtook [another car]2.
When [it]1 pulled in front of [the overtaken car]2,
[it]1 skidded and hit with its side the front of [the
facing truck]3.
Figure 3: A sentence where references to road ob-
jects have been marked.
6.3 Resolution of Metonymy
Use of metonymy, such as alternation between the
driver and his vehicle, is frequent in the Swedish
press clippings. An improper resolution of it intro-
duces errors in the templates and in the visualiza-
tion. It can create independently moving graphic
entities i.e. the vehicle and its driver, that should be
represented as one single object, a moving vehicle,
or stand together.
We detect the metonymic relations between
drivers and their vehicles. We use either cue phrases
like lastbilschauffo?ren (?the truck driver?) or the lo-
cation or instrument semantic roles in phrases like
Mannen som fa?rdades i lastbilen (?The man who
was traveling in the truck?). We then apply con-
straints on the detected events and directions to ex-
clude wrong candidates. For example, given the
phrase Mannen krockade med en traktor (?The man
collided with a tractor?), we know that the man can-
not be the driver of the tractor.
We do not yet handle the metonymic relations be-
tween parts of vehicles and the vehicles themselves.
They are less frequent in the texts we have exam-
ined.
6.4 Marking Up the Events
Events in car accident reports correspond to vehicle
motions and collisions. We detect them to be able
to visualize and animate the scene actions. To carry
out the detection, we created a dictionary of words
? nouns and verbs ? depicting vehicle activity and
maneuvers. We use these words to anchor the event
identification as well as the semantic roles of the
dependents to determine the event arguments.
6.4.1 Detecting the Semantic Roles
Figure 4 shows a sentence that we translated from
our corpus of news texts, where the groups have
been marked up and labeled with semantic roles.
[En personbil]Actor ko?rde [vid femtiden]T ime
[pa? torsdagseftermiddagen]T ime [in i ett rad-
hus]V ictim [i ett a?ldreboende]Loc [pa? Alva?gen]Loc
[i Enebyberg]Loc [norr om Stockholm]Loc .
[About five]T ime [on Thursday afternoon]T ime , [a
car]Actor crashed [into a row house]V ictim [in an
old people?s home]Loc [at Alva?gen street]Loc [in
Enebyberg]Loc [north of Stockholm]Loc.
Figure 4: A sentence tagged with semantic roles.
Gildea and Jurafsky (2002) describe an algorithm
to label automatically semantic roles in a general
context. They use the semantic frames and associ-
ated roles defined in FrameNet (Baker et al, 1998)
and train their classifier on the FrameNet corpus.
They report a performance of 82 percent.
Carsim uses a classification algorithm similar to
the one described in this paper. However, as there is
no lexical resource such as FrameNet for Swedish
and no widely available parser, we adapted it. Our
classifier uses a more local strategy as well as a dif-
ferent set of attributes.
The analysis starts from the words in our dictio-
nary for which we designed a specific set of frames
and associated roles. The classifier limits the scope
of each event to the clause where it appears. It iden-
tifies the verb and nouns dependents: noun groups,
prepositional groups, and adverbs that it classifies
according to semantic roles.
The attributes of the classifier are:
? Target word: the keyword denoting the event.
? Head word: the head word of the group to be
classified.
? Syntactic class of head word: noun group,
prepositional group, or adverb.
? Voice of the target word: active or passive.
? Domain-specific semantic type: Dynamic ob-
ject, static object, human, place, time, cause,
or speed.
The classifier chooses the role, which maximizes
the estimated probability of a role given the values
of the target, head, and semantic type attributes:
P? (r|t, head, sem) = C(r, t, head, sem)C(t, head, sem) .
If a particular combination of target, head, and
semantic type is not found in the training set, the
classifier uses a back-off strategy, taking the other
attributes into account.
We annotated manually a set of 819 examples on
which we trained and tested our classifier. We used
a random subset of 100 texts as a test set and the
rest as a learning set. On the test set, the classi-
fier achieved an accuracy of 90 percent. A classi-
fier based on decision trees built using the ID3 algo-
rithm with gain ratio measure yielded roughly the
same performance.
The value of the semantic type attribute is set us-
ing domain knowledge. Removing this attribute de-
graded the performance of the classifier to 80 per-
cent.
6.4.2 Interpreting the Events
When the events have been detected in the text, they
can be represented and interpreted in the formal de-
scription of the accidents.
We observed that event coreferences are very fre-
quent in longer texts: A same action like a colli-
sion is repeated in several places in the text. As
for metonymy, duplicated events in the template en-
tails a wrong visualization. We solve it through the
unification of as many events as possible, taking
metonymy relations into account, and we remove
the duplicates.
6.5 Time Processing and Event Ordering
In some texts, the order in which events are men-
tioned does not correspond to their chronological
order. To address this issue and order the events cor-
rectly, we developed a module based on the generic
TimeML framework (Pustejovsky et al, 2002). We
use a machine learning approach to annotate the
whole set of events contained in a text and from this
set, we extract events used specifically by the Car-
sim template ? the Carsim events.
TimeML has tags for time expressions (today),
?signals? indicating the polarity (not), the modal-
ity (could), temporal prepositions and connectives
such as for, during, before, after, events (crashed,
accident), and tags that indicate relations between
entities. Amongst the relations, the TLINKs are
the most interesting for our purposes. They ex-
press temporal relations between time expressions
and events as well as temporal relations between
pairs of events.
We developed a comprehensive phrase-structure
grammar to detect the time expressions, signals, and
TimeML events and to assign values to the enti-
ties? attributes. The string den tolfte maj (?May
12th?) is detected as a time expression with the
attribute value=?YYYY-05-12?. We extended the
TimeML attributes to store the events? syntactic fea-
tures. They include the part-of-speech annotation
and verb group structure, i.e. auxiliary + participle,
etc.
We first apply the PS rules to detect the time ex-
pressions, signals, and events. Let e1, e2, e3, ...,
en be the events in the order they are mentioned
in a text. We then generate TLINKs to relate these
events together using a set of decision trees.
We apply three decision trees on se-
quences of two to four consecutive events
(ei, ei+1, [, ei+2[, ei+3]]), with the constraint
that there is no time expression between them,
as they might change the temporal ordering sub-
stantially. The output of each tree is the temporal
relation holding between the first and last event
of the considered sequence, i.e. respectively:
adjacent pairs (ei, ei+1), pairs separated by one
event (ei, ei+2), and by two events (ei, ei+3). The
possible output values are simultaneous, after,
before, is included, includes, and none. As a result,
each event is linked by TLINKs to the three other
events immediately after and before it.
We built automatically the decision trees using
the ID3 algorithm (Quinlan, 1986). We trained them
on a set of hand-annotated examples, which consists
of 476 events and 1,162 TLINKs.
As a set of features, the decision trees use certain
attributes of the events considered, temporal signals
between them, and some other parameters such as
the number of tokens separating the pair of events
to be linked. The complete list of features with x
ranging from 0 to 1, 0 to 2, and 0 to 3 for each tree
respectively, and their possible values is:
? Eventi+xTense: none, past, present, future,
NOT DETERMINED.
? Eventi+xAspect: progressive, per-
fective, perfective progressive, none,
NOT DETERMINED.
? Eventi+xStructure: NOUN,
VB GR COP INF, VB GR COP FIN,
VB GR MOD INF, VB GR MOD FIN,
VB GR, VB INF, VB FIN.
? temporalSignalInbetween: none, before, after,
later, when, still, several.
? tokenDistance: 1, 2 to 3, 4 to 6, 7 to 10, greater
than 10.
? sentenceDistance: 0, 1, 2, 3, 4, greater than 4.
? punctuationSignDistance: 0, 1, 2, 3, 4, 5,
greater than 5.
The process results in an overgeneration of links.
The reason for doing this is to have a large set of
TLINKs to ensure a fine-grained ordering of the
events. As the generated TLINKs can be conflict-
ing, we assign each of them a score, which is de-
rived from the C4.5 metrics (Quinlan, 1993).
We complement the decision trees with heuris-
tics and hints from the event interpreter that events
are identical. Heuristics represent common-sense
knowledge and are encoded as nine production
rules. An example of them is that an event in the
present tense is after an event in the past tense.
Event identity and heuristics enable to connect
events across the time expressions. The TLINKs
generated by the rules also have a score that is rule
dependent.
When all TLINKs are generated, we resolve tem-
poral loops by removing the TLINK with the lowest
score within the loop. Finally, we extract the Carsim
events from the whole set of TimeML events and we
order them using the relevant TLINKs.
6.6 Detecting the Roads
The configuration of roads is inferred from the in-
formation in the detected events. When one of the
involved vehicles makes a turn, this indicates that
the configuration is probably a crossroads.
Additional information is provided using key-
word spotting in the text. Examples of relevant key-
words are korsning (?crossing?), ?rondell? (?round-
about?) and kurva (?bend?), which are very likely
indicators of the road configuration if seen in the
text.
These methods are very simple, but the cases
where they fail are quite rare. During the evalua-
tion described below, we found no text where the
road configuration was misclassified.
7 Evaluation of the Information
Extraction Module
To evaluate the performance of the information ex-
traction component, we applied it to 50 previously
unseen texts, which were collected from newspaper
sources on the web. The size of the texts ranged
from 31 to 459 words. We calculated precision and
recall measures for detection of road objects and for
detection of events. A road object was counted as
correctly detected if there was a corresponding ob-
ject in the text, and the type of the object was cor-
rect. The same criteria apply to the detection of
events, but here we also add the criterion that the
actor (and victim, where this applies) must be cor-
rect. The performance figures are shown in Tables 1
and 2.
Total number of objects in the texts 105
Number of detected objects 110
Number of correctly detected objects 94
Precision 0.85
Recall 0.90
F-measure (? = 1) 0.87
Table 1: Statistics for the detection of road objects
in the test set.
Total number of events in the texts 92
Number of detected events 91
Number of correctly detected events 71
Precision 0.78
Recall 0.77
F-measure (? = 1) 0.78
Table 2: Statistics for the detection of events in the
test set.
The system was able to extract or infer all rele-
vant information correctly in 23 of the 50 texts. In
order to find out the causes of the errors, we investi-
gated what simplifications of the texts needed to be
Figure 5: Planning the trajectories.
made to make the system produce a correct analysis.
The result of this investigation is shown in Table 3.
Object coreference 6
Role labeling 5
Metonymy 5
Clause segmentation 3
Representational expressivity 3
Unknown objects 2
Event detection 2
Unknown event 1
Tagger error 1
PP attachment 1
Table 3: Causes of errors.
8 Scene Synthesis and Visualization
The visualizer reads its input from the formal de-
scription. It synthesizes a symbolic 3D scene and
animates the vehicles. We designed the graphic el-
ements in the scene with the help of traffic safety
experts.
The scene generation algorithm positions the
static objects and plans the vehicle motions. It uses
rule-based modules to check the consistency of the
description and to estimate the 3D start and end co-
ordinates of the vehicles.
The visualizer uses a planner to generate the vehi-
cle trajectories. A first module determines the start
and end positions of the vehicles from the initial di-
rections, the configuration of the other objects in the
scene, and the chain of events as if they were no ac-
cident. Then, a second module alters these trajecto-
ries to insert the collisions according to the accident
slots in the accident representation (Figure 5).
This two-step procedure can be justified by the
descriptions found in most reports. The car drivers
generally start the description of their accident as if
it were a normal movement, which is subsequently
been modified by the abnormal conditions of the ac-
cident.
Finally, the temporal module of the planner as-
signs time intervals to all the segments of the trajec-
tories.
Figure 6 shows two screenshots that the Carsim
visualizer produces for the text above. It should be
noted that the graphic representation is intended to
be iconic in order not to convey any meaning which
is not present in the text.
9 Conclusion and Perspectives
We have presented an architecture and a strategy
based on information extraction and a symbolic vi-
sualization that enable to convert real texts into 3D
scenes. We have obtained promising results that val-
idate our approach. They show that the Carsim ar-
chitecture is applicable to Swedish and other lan-
guages. As far as we know, Carsim is the only
text-to-scene conversion system working on non-
invented narratives.
We are currently improving Carsim and we hope
in future work to obtain better results in the reso-
lution of coreferences. We are implementing and
adapting algorithms such as the one described in
(Soon et al, 2001) to handle this. We also intend
to improve the visualizer to handle more complex
scenes and animations.
The current aim of the Carsim project is to visu-
alize the content of a text as accurately as possible,
with no external knowledge. In the future, we would
like to integrate additional knowledge sources in or-
der to make the visualization more realistic and un-
derstandable. Geographical and meteorological in-
formation systems are good examples of this, which
could be helpful to improve the realism. Another
topic, which has been prominent in our discussions
with traffic safety experts, is how to reconcile dif-
ferent narratives that describe a same accident.
In our work on the information extraction mod-
ule, we have concentrated on the extraction of data
which are relevant for the visual reconstruction of
the scene. We believe that the information extrac-
tion component could be interesting in itself to ex-
tract other relevant data, for example casualty statis-
tics or traffic conditions.
Acknowledgements
We are very grateful to Karin Brundell-Freij, A?se
Svensson, and Andra?s Va?rhelyi, traffic safety ex-
perts at LTH, for helping us in the design the Carsim
template and advising us with the 3D graphic repre-
sentation.
This work is partly supported by grant num-
ber 2002-02380 from the Spra?kteknologi program
of Vinnova, the Swedish Agency of Innovation
Systems.
References
Giovanni Adorni, Mauro Di Manzo, and Fausto
Giunchiglia. 1984. Natural language driven im-
age generation. In Proceedings of COLING 84,
pages 495?500, Stanford, California.
Douglas E. Appelt and David Israel. 1999. In-
troduction to information extraction technology.
Tutorial Prepared for IJCAI-99. Artificial Intelli-
gence Center, SRI International.
Michael Arens, Artur Ottlik, and Hans-Hellmut
Nagel. 2002. Natural language texts for a cogni-
tive vision system. In Frank van Harmelen, edi-
tor, ECAI2002, Proceedings of the 15th European
Conference on Artificial Intelligence, Lyon, July
21-26.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet Project. In
Proceedings of COLING-ACL?98, pages 86?90,
Montre?al, Canada.
Johan Carlberger and Viggo Kann. 1999. Imple-
menting an efficient part-of-speech tagger. Soft-
ware Practice and Experience, 29:815?832.
Bob Coyne and Richard Sproat. 2001. Wordseye:
An automatic text-to-scene conversion system.
In Proceedings of the Siggraph Conference, Los
Angeles.
Michel Denis. 1991. Imagery and thinking. In Ce-
sare Cornoldi and Mark A. McDaniel, editors,
Imagery and Cognition, pages 103?132. Springer
Verlag.
Sylvain Dupuy, Arjan Egges, Vincent Legendre,
and Pierre Nugues. 2001. Generating a 3D simu-
lation of a car accident from a written descrip-
tion in natural language: The Carsim system.
In Proceedings of The Workshop on Temporal
and Spatial Information Processing, pages 1?8,
Toulouse, July 7. Association for Computational
Linguistics.
Arjan Egges, Anton Nijholt, and Pierre Nugues.
2001. Generating a 3D simulation of a car ac-
cident from a formal description. In Venetia Gi-
agourta and Michael G. Strintzis, editors, Pro-
ceedings of The International Conference on
Augmented, Virtual Environments and Three-
Dimensional Imaging (ICAV3D), pages 220?223,
Mykonos, Greece, May 30-June 01.
Eva Ejerhed. 1996. Finite state segmentation of
discourse into clauses. In Proceedings of the 12th
European Conference on Artificial Intelligence
(ECAI-96) Workshop on Extended Finite State
Models of Language, Budapest, Hungary.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
Figure 6: Screenshots from the animation of the text above.
matic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Nils-Olof Karlberg. 2003. Field results from
STRADA ? a traffic accident data system telling
the truth. In ITS World Congress, Madrid, Spain,
November 16-20.
Stephen Michael Kosslyn. 1983. Ghosts in the
Mind?s Machine. Norton, New York.
Mauro Di Manzo, Giovanni Adorni, and Fausto
Giunchiglia. 1986. Reasoning about scene de-
scriptions. IEEE Proceedings ? Special Issue on
Natural Language, 74(7):1013?1025.
Lisa Persson and Magnus Danielsson. 2004. Name
extraction in car accident reports for Swedish.
Technical report, LTH, Department of Computer
science, Lund, January.
James Pustejovsky, Roser Saur??, Andrea Setzer, Rob
Gaizauskas, and Bob Ingria. 2002. TimeML An-
notation Guidelines. Technical report.
John Ross Quinlan. 1986. Induction of decision
trees. Machine Learning, 1(1):81?106.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kauffman.
A?ke Viberg, Kerstin Lindmark, Ann Lindvall, and
Ingmarie Mellenius. 2002. The Swedish Word-
Net project. In Proceedings of Euralex 2002,
pages 407?412, Copenhagen.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Edward R. Tufte. 1997. Visual Explanations: Im-
ages and Quantities, Evidence and Narrative.
Graphic Press.
