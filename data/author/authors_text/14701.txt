First Joint Conference on Lexical and Computational Semantics (*SEM), pages 506?513,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Zhou qiaoli: A divide-and-conquer strategy for  
semantic dependency parsing 
 
 
Qiaoli Zhou Ling Zhang Fei Liu Dongfeng 
Cai 
Guiping 
Zhang 
Knowledge Engineering  
Research Center Shenyang Aerospace University 
No.37 Daoyi South Avenue 
Shenyang, Liaoning, China 
Zhou_qiao_li@
hotmail.com 
710138892@qq.
com 
fei_l2011@
163.com 
caidf@vip.16
3.com 
zgp@ge-
soft.com 
 
 
 
 
 
 
Abstract 
We describe our SemEval2012 shared Task 5 
system in this paper. The system includes 
three cascaded components: the tagging se-
mantic role phrase, the identification of se-
mantic role phrase, phrase and frame semantic 
dependency parsing. In this paper, semantic 
role phrase is tagged automatically based on 
rules, and takes Conditional Random Fields 
(CRFs) as the statistical identification model 
of semantic role phrase. A projective graph-
based parser is used as our semantic depend-
ency parser. Finally, we gain Labeled At-
tachment Score (LAS) of 61.84%, which 
ranked the first position. At present, we gain 
the LAS of 62.08%, which is 0.24% higher 
than that ranked the first position in the task 5. 
1 System Architecture  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. Firstly, Semantic Role (SR) phrase in 
a sentence are identified; next, SR phrase can be 
replaced by their head or SR of head. Therefore, 
the original sentence is divided into two kinds of 
parts, which can be parsed separately. The first 
kind is SR phrase parsing; the second kind is  
parsing the sentence in which the SR phrases are 
replaced by their head or SR of head. Finally, the 
paper takes graph-based parser as the semantic de-
pendency parser for all parts. They are described in 
Section 2 and Section 4. Their experimental results 
are shown in Section5. Section 6 gives our conclu-
sion and future work. 
2 SR Phrase Tagging and Frame  
To identify SR phrase, SR phrase of train corpus 
are tagged. SR phrase is tagged automatically 
based on rules in this paper. A phrase of the sen-
tence is called Semantic Role phrase (SR phrase) 
when the parent of only one word of this phrase is 
out of this phrase. The word with the parent out of 
the phrase is called Head of Phrase (HP). The 
shortest SR phrase is one word, while the longest 
SR phrase is a part of the sentence. In this paper, 
the new sequence in which phrases are replaced by 
their head or SR of head is defined as the frame. In 
this paper, firstly, SR phrases of the sentence are 
identified; secondly, the whole sentence is divided 
into SR phrases and frame; thirdly, SR phrase and 
frame semantic dependency are parsed; finally, the 
dependency parsing results of all components are 
combined into the dependency parsing result of the 
whole sentence. 
SR of HP is used as the type of this phrase. Only 
parts of types of SR phrases are tagged. In this pa-
per, the tagged SR phrases are divided into two 
506
types: Main Semantic Role (MSR) phrase and 
Preposition Semantic Role (PSR) phrase. 
2.1 MSR Phrase Tagging  
In this paper, MSR phrase includes: OfPart, agent, 
basis, concerning, content, contrast, cost, existent, 
experiencer, isa, partner, patient, possession, pos-
sessor, relevant, scope and whole. MSR phrase 
tagging rules are shown in figure1&2. 
  
Figure1: Tagging Rule of the Last Word of MSR Phrase 
Figure 1 shows the rule for identification of the 
last word of MSR phrase. If the SR of the current 
word is MSR and its POS is not VV, VE, VC or 
VA, it is the last word of phrase. 
As shown in the figure 2, the first word of 
phrase is found based on the last word of phrase. 
The child with the longest distance from the last 
word of phrase is used as the current word, and if 
the current word has no child, it is the first word of 
phrase; otherwise, the child of the current word is 
found recursively. If the first word of phrase POS 
is preposition and punctuation, and its parent is the 
last word, the word following the first word serves 
as the first word of phrase. 
 
Figure2: Tagging Rule of the First Word of MSR Phrase 
 
 
Figure3: Example of the Tagging MSR Phrase 
As shown in the figure 3, the first column is 
word ID and the seventh column is parent ID of 
word. SR of ID40 is content, so ID40 is the last 
word of phrase. Its children include ID39 and ID37, 
thus ID37 with the longest distance from ID40 is 
the current word. The child of ID37 is ID33, the 
child of ID33 is ID32, ID32 has no child, and ID32 
is the first word of SR phrase. 
The tagged result in the above figure 3 is as fol-
lows: ?/CC ?/VC ??/VV content[ ??/JJ ?
? /NN ? /CC ?? /NR ? /ETC ?? /NN ?
/DEG ??/NN ??/NN ]  
Input: wi: word index (ID) in a given sentence. 
           N: the number of words. 
          Mi: MSR list. 
          Vi: POS tags list 
Output: the last word ID of MSR phrase 
Function: Findmainsemanticword(wi): return word 
ID when wi of semantic belongs to Mi. 
Otherwise return 0. 
Function: FindPOSword(wi): return true when wi 
of POS tagging not belongs to Vi. Oth-
erwise return 0. 
Function Findlastword(wi) 
For i?1 to N do begin 
             If (Findmainsemanticword(wi)&& 
FindPOSword(wi)) 
               { 
                   return wi; 
} 
else { 
                          i++; 
} 
       end 
return 0; 
29  ?  ?  CC  CC  _  30  aux-depend  _  _ 
30  ?  ?  VC  VC  _  58 s-succession  _  _ 
31  ?? ??  VV  VV  _  54  s-succession _  _ 
32  ??  ??  JJ   JJ  _  33  d-attribute  _  _ 
33  ??  ??  NN  NN  _  37  s-coordinate  _  _ 
34  ?  ?  CC  CC  _  37  aux-depend  _  _ 
35  ??  ??  NR  NR  _  37  d-member  _  _ 
36  ?  ?  ETC  ETC  _  35  aux-depend  _  _ 
37  ??  ??  NN  NN  _  40  d-genetive  _  _ 
38  ?  ?  DEG  DEG  _  37  aux-depend  _  _ 
39  ??  ??  NN  NN  _  40  s-coordinate  _ _ 
40  ?? ??  NN  NN  _  31  content  _  _ 
Input: Lword: the last word ID of MSR phrase. 
Output: Fword: the first word ID of MSR phrase. 
Function: Findmaxlenchild (w): return child ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Fuction: FindPOSword(w): return POS of w. 
Fuction:Findparent(w): return parent ID of w. 
Function Findfirstword(Lword) 
     If(Findmaxlenchild (Lword)= =0) 
      { 
         return Lword; 
} 
Else { 
Fword=Findmaxlenchildword(Lword); 
If(findPOSword(Fword)==P||  
findPOSword(Fword)= =PU) 
{ 
    If (findparent(Fword)= =Lword) 
        Return Fword +1; 
} 
Findfirstword(Fword); 
} 
507
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/CC ?/VC ??/VV ??/NN  
Example of sentences with nested phrases: 
?/P ??/JJ ??/NN ?/PU ??/NT exis-
tent[ ? /P ?? /NR ?? /NN ?? /VV con-
tent[ ??/NN ] ?/DEC ??/NN ???/NN ] 
?/AD ?/VE ?????/CD ?/M  
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/P ??/JJ ??/NN ?/PU ??
/NT???/NN ?/AD ?/VE ?????/CD ?
/M 
2.2 PSR Phrase Tagging  
In this paper, SR phrase containing preposition is 
defined as PSR phrase. If the POS tags of the cur-
rent word is Preposition (P), the first word and the 
last word of PSR phrase are found based on the 
current word. PSR phrase tagging rule as figure 4 
& 5. 
 
Figure 4: Tagging Rule of the First Word of PSR Phrase 
As shown in the figure 4, the child with the 
longest distance from the current word is the first 
word of phrase. If the prep has no child, then it is 
PSR phrase. 
As shown in the figure 5, firstly, the parent of 
the prep is found; next, the parent is taken as the 
current word, and the child with the longest dis-
tance from the current word is found recursively. If 
no child is found, the current word is the last word 
of PSR phrase. If preposition of SR is root or par-
ent of preposition is root, and proposition is PSR. 
If ID of preposition is larger than ID of parent of 
preposition, and preposition is PSR. 
 
Figure5: Tagging Rule of the Last Word of PSR Phrase 
 
 
Figure6: Example of the Tagging PSR Phrase 
As shown in the figure6, ID4 is prep, and it has 
no child, so the first word is ID4. The parent of 
Input: Pword: the word ID that word POS tags is P. 
Output: Fword: the first word ID of PSR phrase. 
Function: Findmaxlenchildword(w): return word ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Function Findfirstword(Pword) 
        If(Findmaxlenchildword(Pword)= =0) 
          { 
             return Pword; 
} 
Else { 
return Fwrod= 
 Findmaxlenchildword(Pword); 
} 
Input: Pword: the word ID that word POS tags is P. 
Output: Lword: the last word ID of PSR phrase. 
Function: Findmaxchild (w): return word ID that 
length is max with w when w has child. 
Otherwise return 0. 
Function: Findparent (w): return word ID when w of 
parent is not root. Otherwise return 0.  
Function: Findroot(w): return 1 when w of semantic 
role is root. Other wise return 0. 
Function Findlastword(Pword) 
Var cword: parent ID 
     If(Findparentsword(Pword)= =0|| 
 findroot(Pword)= =1)  { 
             return Pword; 
} 
else { cword=Findparent (Pword) ) 
 If(Pword>cword){ 
return Pword; 
} 
else { 
                   if(Findmaxchild (cword)= =0) { 
                               return cword; 
} 
else{  
Lword= 
Findmaxchild (cword); 
Findlastword(Lword); 
} 
                           } 
}
1  ??  ??  NN  NN  _  2  j-agent  _  _ 
2  ??  ??  NN  NN  _  3  r-patient  _  _ 
3  ??  ??  NN  NN  _  11  agent  _  _ 
4  ?  ?  P P  _ 5  prep-depend  _ first word 
5  ??  ??  VV  VV  _  11 duration _ head_ 
6  ?? ?? NR  NR _ 8  d-genetive  _ _ 
7  ?? ?? NN  NN _  8 r-patient _ _ 
8  ??  ?? NN  NN _ 9 d-host _  _ 
9  ??  ?? NN  NN _ 5 patient  _  _ 
10  ?  ? LC  LC  _ 5  aux-depend _ last word_ 
11  ??  ?? VV VV  _  0  ROOT _  _ 
12  ?  ?  AS  AS  _ 11 aspect  _  _ 
13 ??  ?? JJ  JJ  _ 14 d-attribute  _  _ 
14  ?? ??  NN NN  _  11 content  _  _ 
15  ?  ?  PU  PU  _ 11  PU  _  _ 
508
ID4 is ID5, the child with the longest distance from 
ID5 is ID10, and ID10 with no child is the last 
word of phrase. 
The tagged result in the above figure 6 is as fol-
lows: ??/NN ??/NN ??/NN duration[?/P 
??/VV ??/NR ??/NN ??/NN ?/LC] ?
?/VV ?/AS ??/JJ ??/NN ?/PU 
The position of HP in PSR phrase is not fixed. 
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with SR of HP is 
called PSR frame. 
PSR frame: ??/NN ??/NN ??/NN dura-
tion/duration ?? /VV ? /AS ?? /JJ ??
/NN ?/PU 
Examples of sentences with nested phrases: 
s-cause[ ??/P ??/NR s-purpose[ ?/P ?
?/VV ???/NT ]  ?/MSP ??/VV ??/VV 
?/DT ?/M ??/NN ??/NN ],/PU ??/AD 
?? /NN ?? /NN ? /VV ? /VV ????
/VV ?/PU 
PSR frame: s-cause/s-cause ,/PU ??/AD ??
/NN ??/NN ?/VV ?/VV ????/VV ?/PU 
2.3 SR Phrase Tagging Performance 
If the parent of only one word of the tagged phrase 
is out of this phrase, this phrase is tagged correctly. 
If each word in the generated frame has one parent 
(i.e. words out of the phrase are dependent on HP 
instead of other words of the phrase), the frame is 
correct. 
 Phrase Frame 
MSR 99.99% 100% 
PSR 99.98% 99.70% 
Table 1. Tagging Performance (P-score) 
 
As shown in the table 1, tagging results were of 
very high accuracy. The wrong results were not 
contained in phrase and frame train corpus of de-
pendency parsing. 
3 SR Phrase Identification  
In this paper, we divide SR phrase into two classes: 
Max SR phrase and Base SR phrase. Max SR 
phrase refers to SR phrase is not included in any 
other SR phrase in a sentence. Base SR phrase re-
fers to SR phrase does not include any other SR 
phrase in a SR phrase. Therefore, MSR phrase is 
divided into two classes: Max MSR (MMSR) 
phrase and Base MSR (BMSR) phrase. PSR phrase 
was divided into two classes: Max PSR (MPSR) 
phrase and Base PSR (BPSR) phrase. 
3.1 MMSR Phrase Identification based on 
Cascaded Conditional Random Fields 
Reference (Qiaoli Zhou, 2010) is selected as our 
approach of MMSR phrase identification. The 
MMSR identifying process is conceptually very 
simple. The MMSR identification first performs 
identifying BMSR phrase, and converts the identi-
fied phrase to head. It then performs identifying for 
the updated sequence and converts the newly rec-
ognized phrases into head. The identification re-
peats this process until the whole sequence has no 
phrase, and the top-level phrase are the MMSR 
phrases. A common approach to the phrase identi-
fication problem is to convert the problem into a 
sequence tagging task by using the ?BIEO? (B for 
beginning, I for inside, E for ending, and O for 
outside) representation. If the phrase has one word, 
the tag is E. This representation enables us to use 
the linear chain CRF model to perform identifying, 
since the task is simply assigning appropriate la-
bels to sequence. 
There are two differences between our feature 
set and Qiaoli (2010)?s: 
1) We use dependency direction of word as iden-
tification feature, while Qiaoli (2010) did not 
use. 
2) We do not use scoring algorithm which is used 
by Qiaoli (2010). 
Direction Unigrams D-3,D-2 ,D-1 , D0 , D+1 ,D+2 ,D+3
Direction Bigrams D-2D-1, D-1D0, D0D+1, D+1D+2,  
Word & Direction W0D0
Table 2. Feature Templates of MMSR Phrase 
 
Table 2 is additional new feature templates 
based on Qiaoli (2010). W represents a word, and 
D represents dependency direction of the word. 
With this approach, nested MSR phrases are identi-
fied, and the top-level MSR phrase is the MMSR 
that we obtained. 
corpus P R F 
dev 81.41% 75.40% 78.29% 
test 81.23% 73.04% 76.92% 
Table 3.  MMSR Identification Performance 
509
3.2 BMSR Phrase Identification based on 
CRFs  
We use the tag set ?BIEO? the same as that used 
for MMSR identification. 
Word Unigrams W-3, W-2, W-1, W0, W+1, W+2, W+3
Word Bigrams 
W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
POS Unigrams P-3 , P-2, P-1, P0, P+1, P+2, P+3
POS Bigrams 
P-3P-2, P-2P-1, P-1P0, P0P+1,  
P+1P+2, P+2P+3
Word_X X0
Word_Y Y0
Word_D D0
Word_S S-3, S-2 , S-1 , S0, S+1, S+2, S+3
Word & POS W-1P-1, W0P0, W+1P+1
Word & Word_X W-3X0
Word & Word_D 
W0D0, W-3W-2D0, W-2W-1D0,  
W-1W0D0, W0W+1D0, W+1W+2D0, 
W+2W+3D0
Word & Word_S W-1S-1, W0S0, W+1S+1, W+2S+2
Word_X & Word_Y X0Y0
POS & Word_D 
P0D0, P-3P-2D0, P-2P-1D0, P-1P0D0, 
P0P+1D0, P+1P+2D0, P+2P+3D0
POS & Word_S 
P-1S-1, P-2S-2, P-3S-3, P0S0, 
 P+1S+1, P+2S+2, P+3S+3
Word_D & Word_S 
D-1S-1, D-2S-2, D-3S-3, D0S0, 
 D+1S+1, D+2S+2, D+3S+3
Word & POS & 
Word_D 
W-1P-1D0, W0P0D0, W+1P+1D0
Word & POS & 
Word_D & Word_S 
W-3P-3D-3S-3, W-2P-2D-2S-2,  
W-1P-1D-1S-1, W0P0D0S0, W1P1D1S1, 
W2P2D2S2, W3P3D3S3
Table 4. Feature Templates of BMSR Phrase 
 
In table 4, ?W? represents a word, ?P? repre-
sents the part-of-speech of the word, ?X? repre-
sents the fourth word following the current word, 
?Y? represents the fifth word following the current 
word, ?D? represents the dependency direction of 
the current word, and ?S? represents the paired 
punctuation feature. ?S? consists of ?RLIO? (R for 
the right punctuation, L for the left punctuation, I 
for the part between the paired punctuation and O 
for outside). 
 
corpus P R F 
dev 79.32% 80.65% 79.98% 
test 79.22% 79.96% 79.59% 
Table 5.  BMSR Identification Performance (F-score) 
3.3 MPSR Phrase Identification Based on 
Collection  
Reference (Dongfeng, 2011) is selected as our ap-
proach of MPSR phrase identification. The posi-
tion of HP in PSR phrase is not fixed. Not only 
PSR phrase is identified, but also PSR phrase type 
is identified.  
There are two major differences between our 
feature set and Dongfeng (2011)?s: 
1) We take the PSR phrase type (the SR of HP) 
as tag.  
2)  We use ?S-type? represents that the PSR 
phrase is the single preposition. ?Type? represents 
SR of the preposition. 
For example: ???/NN location [?/P ??
/NR ??/NR] ??/VV 
O|W POS
Dongfeng 
(2011) Tag 
Our Tag 
*|??? NN O O 
*|? P O O 
?|?? NR I I 
?|?? NR E Location-E
?|?? VV N N 
Table 6. Example of PSR Phrase Tag Set  
 
In table 6, Dongfeng(2011) takes ?E? as the tag 
of last word of PSR phrase, but we take ?Location-
E? as the tag of last word of PSR phrase  (Location 
is type of  PSR phrase). 
With this approach, nested PSR phrases are 
identified, and the top-level PSR phrase is the 
MPSR that we obtained. 
corpus MPSR phrase MPSR phrase & type
dev 84.00% 54.23% 
test 83.78% 51.60% 
Table 7. MPSR Identification Performance (F-score) 
3.4 Combined Identification of MSR Phrase 
and PSR Phrase 
Identification process: MSR phrase and PSR 
phrase are respectively identified in one sentence, 
and the results are combined in accordance with 
this rule: if phrases are nested, only the top-level 
phrase is tagged; if phrases are same, only the PSR 
510
phrase is tagged; if phrases are overlapped, only 
PSR phrase is tagged. 
There are two combinations in this paper:  
1) MMSR phrase and MPSR phrase combined 
result is defined as MMMP phrase. For exam-
ple as follow (?[ ]?represents MMSR, 
?{}?represents MPSR): 
Example A: [ ??/NN ] ?/VC [ ??/VV ?
?/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ] ?/PU ??/DT ?/M ?/VE [ ??/CD 
?/M ??/NN ??/NN ?/PU ???/CD ?/M 
??/NN ??/NN ] ??/VV location{ ?/P ?
/DT ?/M ??/NN ?/LC } ?/PU  
MMMP  frame: [ ??/NN ] ?/VC ??/NN ?
/PU ??/DT ?/M ?/VE ??/NN ??/VV 
location/location ?/PU 
2) BMSR phrase and MPSR phrase combined 
result is defined as BMMP phrase. 
Example B: [ ??/NN ] ?/VC ??/VV [ ??
/NR ] ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ?/PU ??/DT ?/M ?/VE [ ??/CD ?
/M ??/NN ??/NN ?/PU ???/CD ?/M ?
?/NN ??/NN ] ??/VV location{ ?/P ?/DT 
?/M ??/NN ?/LC } ?/PU 
BMMP  frame: ??/NN ?/VC ??/VV ??
/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ??
/NN ?/PU ??/DT ?/M ?/VE ??/NN ??
/VV location/location ?/PU 
corpus phrase P R F 
BMMP 79.48% 81.60% 80.53%
dev 
MMMP 80.00% 76.79% 78.36%
BMMP 80.14% 82.48% 81.30%
test 
MMMP 80.19% 78.53% 79.35%
Table 8.  Combination Phrase Identification 
Performance 
3.5 Phrase and Frame Length Distribution   
We count phrases, frame and Original Sentence 
(OS) length distribution in training set and dev set. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 80.07% 71.36% 75.36% 85.74% 9.07%
[5,10) 16.15% 21.63% 18.93% 12.33% 8.30%
[10,20) 3.35% 6.13% 5.05% 1.80% 17.23%
20? 0.43% 0.88% 0.66% 0.13% 65.40%
Table 9.  Length Distribution of Phrases and OS 
 
Table 9 shows, about 95% of phrases have less 
than 10 words, but about 65% of OS has more than 
20 words. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 16.00% 18.70% 16.43% 14.36% 9.07%
[5,10) 18.87% 24.91% 19.41% 14.11% 8.30%
[10,20) 34.26% 35.42% 33.94% 30.68% 17.23%
20? 30.87% 20.97% 30.22% 40.85% 65.40%
Table 10.  Length Distribution of Frames and OS 
 
Table 10 shows, about 70% of frames have less 
than 20 words, especially 80% of MMMP frame 
has less than 20 words, but about 65% of OS has 
more than 20 words. 
 BMMP MMMP BMSR MMSR OS 
phrase 3.07 3.83 2.53 3.44 30.07
frame 16.00 13.21 19.16 15.79 30.07
Table 11. Average Length 
 
We count phrases, frame and Original Sentence 
(OS) Average Length (AL) in training set and dev 
set. Table 11 shows phrase of AL accounted for 
10% of OS of AL, and frame of AL accounted for 
50% of OS of AL. The AL shows that the semantic 
dependency paring unit length of OS is greatly re-
duced after dividing an original sentence into SR 
phrases and frame.  
As shown in tables 9, 10 and 11, the length dis-
tribution indicates that the divide-and-conquer 
strategy reduces the complexity of sentences sig-
nificantly. 
4 Semantic Dependency Parsing  
Graph-based parser is selected as our basic seman-
tic dependency parser. It views the semantic de-
pendency parsing as problem of finding maximum 
spanning trees (McDonald, 2006) in directed 
graphs. In this paper, phrase and frame semantic 
dependency parsing result was obtained by Graph-
based parser. Training set of phrase comes from 
phrases, and training set of frame comes from 
frames. 
5 Experiments  
5.1 Direction of Identification  
511
Dependency direction serves as feature of SR 
phrase identification, so we need to identify de-
pendency direction of word. We use tag set is {B, 
F}, B represents backward dependence, F repre-
sents forward dependence. The root?s dependency 
direction in sentence is B. Dependency direction 
identification p-score has reached 94.87%. 
Word Unigrams W-4, W-3, W-2, W-1, W0, W+1,  
W+ 2, W+ 3, W+ 4
Word Bigrams W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
Word Trigrams W-1W 0W+1
Word Four-grams W-2W-1W0 W +1, W0W+1W+2W+3
Word Five-grams W- 4W-3W-2W-1W0,  
W0W+1W+2W+3W+ 4
POS Unigrams P-4, P-3, P-2, P-1, P0, P+1, P+2, P+3, P+ 4
POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P+1, 
 P+1P+2, P +2P+3
POS Trigrams P-1P0P+1
POS Four-grams P-2P-1P0P+1, P0P+1P+2P+3
POS Five-grams P-4P-3P-2P-1P0, P0P+1P+2P+3P+4
Word & POS W-2 P-2, W-1P-1, W0P0, W+1P+1, 
W+2P+2
Table 12.  Feature Templates of Dependency Direction 
In table12, w represents word, p represents POS. 
5.2 System and Model  
For a sentence for which phrases has been identi-
fied, if phrases can be identified, then the whole 
sentence semantic dependency parsing result is 
obtained by phrase parsing model and frame pars-
ing model. Therefore, in this paper, the sentence is 
divided into the following types based on the 
phrase identification results: (1) SentMMMP indi-
cates MMSR phrase and MPSR phrase identified 
in a sentence; (2) SentBMMP indicates BMSR 
phrase and MPSR phrase identified in a sentence; 
(3) SentMMSR indicates only MMSR phrase iden-
tified in a sentence; (4) SentMPSR indicates only 
MPSR phrase identified in a sentence; (5) 
SentBMSR indicates only BMSR phrase identified 
in a sentence; (6) SentNone indicates no phrase 
identified in a sentence. 
Sentence type Phrase parsing Model 
Frame parsing
Model 
SentMMMP MMMP phrase MMMP frame
SentBMMP BMMP phrase BMMP frame
SentMMSR MMSR phrase MMSR frame
SentMPSR MPSR phrase MPSR frame 
SentBMSR BMSR phrase BMSR frame
SentNone Sentence model 
Table 13.  Type of Sentence and Parsing Model 
Table 13 shows types of sentence, and parsing 
models for every type of sentence. For example, 
parsing SentMMMP needs MMMP phrase parsing 
model and MMMP frame paring model 
The corpus contains the sentence type deter-
mined by the phrase identification strategy. 
Strategy of phrase 
identification Sentence type in the corpus
Strategy MMMP SentMMMP, SentMMSR, SentMPSR, SentNone 
Strategy BMMP SentBMMP, SentMPSR, SentBMSR, SentNone 
Strategy BMSR SentBMSR, SentNone 
Table 14.  Sentence Types in the Corpus 
 
As shown in table 14, Strategy MMMP indicates 
that MMMP phrase in the corpus was identified, 
and sentences in the corpus were divided into 
SentMMMP, SentMMSR, SentMPSR and Sent-
None. Strategy BMMP indicates that BMMP 
phrase in the corpus was identified, and sentences 
in the corpus were divided into SentBMMP, 
SentBMSR, SentMPSR and SentNone. Strategy 
BMSR indicates that BMSR phrase in the corpus 
was identified, and sentences in the corpus were 
divided into SentBMSR and SentNone. 
5.3 Comparative Experiments  
In this paper, we carry out comparative experi-
ments of parsing for the test set by 3 systems. 
1) System1 represents strategy MMMP in the 
table 14. 
2) System2 represents strategy BMMP in the ta-
ble 14. 
3) System3 represents strategy BMSR in the table 
14. 
 Dev Test 
G-parser 62.31% 61.68% 
System1(MMMP) 61.98% 61.84% 
System2(BMMP) 62.7% 62.08% 
System3(BMSR) 62.22% 61.15% 
Table 15.  Comparative Experiments 
 
As shown in the table 15, system2 result is more 
accurate than system1, because BMMP phrase 
identification is more accurate than MMMP as 
shown in the table 8. Although, BMSR phrase 
identification is more accurate than MMMP phrase 
as shown in the table 5 & 8, system 3 result is less 
accurate than systm1. Compared with BMSR iden-
512
tification, MMMP identification reduces the com-
plexity of sentences significantly, because the table 
11 shows that the AL of MMMP frame is about 
30% less than that of BMSR frame. G-parser is 
graph-based parser (Wangxiang Che, 2008). 
6 Conclusion and Future Work  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. We present our SemEval2012 shared 
Task 5 system which is composed of three cas-
caded components: the tagging of SR phrase, the 
identification of Semantic-role- phrase and seman-
tic dependency parsing.  
Divide-and-conquer strategy is influenced by 
two factors: one is identifying the type of phrase 
will greatly reduce the sentence complexity; the 
other is phrase identifying precision results in cas-
caded errors. The topic of this evaluation is seman-
tic dependency parsing, and word and POS contain 
less semantic information. If we can make seman-
tic label on words, then it will be more helpful for 
semantic dependency parsing. In the future, we 
will study how to solve the long distance depend-
ency parsing problem. 
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. This work was supported 
by National Natural Science Foundation of China 
(NSFC) via grant 61073123 and Natural Science 
Foundation of Liaoning province via grant 
20102174. 
References  
Dongfeng Cai, Ling Zhang, Qiaoli Zhou and Yue Zhao. 
A Collocation Based Approach for Prepositional 
Phrase Identification. IEEE NLPKE, 2011. 
McDonald, Ryan. 2006. Discriminative Learning and 
Spanning Tree Algorithms for Dependency Parsing. 
Ph.D. thesis, University of Pennsylvania. 
Guiping Zhang, Wenjing Lang, Qiaoli Zhou and Dong-
feng Cai. 2010. Identification of Maximal-Length 
Noun Phrases Based on Maximal-Length Preposition 
Phrases in Chinese, 2010 International Conference 
on Asian Language Processing, pages 65-68. 
Qiaoli Zhou, Wenjing Lang, Yingying Wang, Yan 
Wang, Dongfeng Cai. 2010.  The SAU Report for the 
1st CIPS-SIGHAN-ParsEval-2010, Proceedings of 
the First CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pp:304-311. 
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang 
Li,Bing Qin, Ting Liu, and Sheng Li. 2008. A cas-
caded syntactic and semantic dependency parsing 
system. In CoNLL-2008. 
513
Automatic Identification of Predicate Heads in Chinese Sentences 
 
Xiaona Rena   Qiaoli Zhoua   Chunyu Kitb   Dongfeng Caia 
Knowledge Engineering Research Centera 
Shenyang Aerospace University 
Department of Chinese, Translation and Linguisticsb 
City University of Hong Kong 
rxn_nlp@163.com    ctckit@cityu.edu.hk 
 
  
Abstract 
We propose an effective approach to auto-
matically identify predicate heads in Chinese 
sentences based on statistical pre-processing 
and rule-based post-processing. In the pre-
processing stage, the maximal noun phrases in 
a sentence are recognized and replaced by 
?NP? labels to simplify the sentence structure. 
Then a CRF model is trained to recognize the 
predicate heads of this simplified sentence. In 
the post-processing stage, a rule base is built 
according to the grammatical features of 
predicate heads. It is then utilized to correct 
the preliminary recognition results. Experi-
mental results show that our approach is feasi-
ble and effective, and its accuracy achieves 
89.14% on Tsinghua Chinese Treebank. 
1 Introduction 
It is an important issue to identify predicates in 
syntactic analysis. In general, a predicate is con-
sidered the head of a sentence. In Chinese, it 
usually organizes two parts into a well-formed 
sentence, one with a subject and its adjunct, and 
the other with an object and/or complement (Luo 
et al, 1994). Accurate identification of predicate 
head is thus critical in determining the syntactic 
structure of a sentence. Moreover, a predicate 
head splitting a long sentence into two shorter 
parts can alleviate the complexity of syntactic 
analysis to a certain degree. This is particularly 
useful when long dependency relations are in-
volved. Without doubt, this is also a difficult task 
in Chinese dependency parsing (Cheng et al, 
2005). 
Predicate head identification also plays an im-
portant role in facilitating various tasks of natural 
language processing. For example, it enhances 
shallow parsing (Sun et al, 2000) and head-
driven parsing (Collins, 1999), and also improves 
the precision of sentence similarity computation 
(Sui et al, 1998a). There is reason to expect it to 
be more widely applicable to other tasks, e.g. 
machine translation, information extraction, and 
question answering. 
In this paper, we propose an effective ap-
proach to automatically recognize predicate 
heads of Chinese sentences based on a preproc-
essing step for maximal noun phrases 1(MNPs). 
MNPs usually appear in the location of subject 
and object in a sentence. The proper identifica-
tion of them is thus expected to assist the analy-
sis of sentence structure and/or improve the ac-
curacy of predicate head recognition. 
In the next section, we will first review some 
related works and discuss their limitations, fol-
lowed by a detailed description of the task of 
recognizing predicate heads in Section 3. Section 
4 illustrates our proposed approach and Section 5 
presents experiments and results. Finally we 
conclude the paper in Section 6. 
2 Related Works 
There exist various approaches to identify predi-
cate heads in Chinese sentences. Luo and Zheng 
(1994) and Tan (2000) presented two rule-based 
methods based on contextual features and part of 
speeches. A statistical approach was presented in 
Sui and Yu (1998b), which utilizes a decision 
tree model. Gong et al (2003) presented their 
hybrid method combining both rules and statis-
tics. These traditional approaches only make use 
of the static and dynamic grammatical features of 
the quasi-predicates to identify the predicate 
heads. On this basis, Li and Meng (2005) pro-
posed a method to further utilize syntactic rela-
tions between the subject and the predicate in a 
sentence. Besides the above monolingual pro-
posals, Sui and Yu (1998a) discussed a bilingual 
strategy to recognize predicate heads in Chinese 
                                                 
1 Maximal noun phrase is the noun phrase which is not con-
tained by any other noun phrases.  
sentences with reference to those in their coun-
terpart English sentences. 
Nevertheless, these methods have their own 
limitations. The rule-based methods require ef-
fective linguistic rules to be formulated by lin-
guists according to their own experience. Cer-
tainly, this is impossible to cover all linguistic 
situations concerned, due to the complexity of 
language and the limitations of human observa-
tion. In practice, we also should not underesti-
mate the complexity of feature application, the 
computing power demanded and the difficulties 
in handing irregular sentence patterns. For in-
stance, a sentence without subject may lead to an 
incorrect recognition of predicate head. For cor-
pus-based approaches, they rely on language data 
in huge size but the available data may not be 
adequate. Those bilingual methods may first en-
counter the difficulty of determining correct sen-
tence alignment in the case that the parallel data 
consist of much free translation. 
Our method proposed here focuses on a simple 
but effective means to help identify predicate 
heads, i.e., MNP pre-processing. At present, 
there has some substantial progress in automatic 
recognition of MNP. Zhou et al (2000) proposed 
an efficient algorithm for identifying Chinese 
MNPs by using their structure combination, 
achieving an 85% precision and an 82% recall. 
Dai et al (2008) presented another method based 
on statistics and rules, reaching a 90% F-score on 
HIT Chinese Treebank. Jian et al (2009) em-
ployed both left-right and right-left sequential 
labeling and developed a novel ?fork position? 
based probabilistic algorithm to fuse bidirec-
tional results, obtaining an 86% F-score on the 
Penn Chinese Treebank. Based on these previous 
works, we have developed an approach that first 
identifies the MNPs in a sentence, which are then 
used in determining the predicate heads in the 
next stage. 
3 Task  Description 
The challenge of accurate identification of predi-
cate heads is to resolve the problem of quasi-
predicate heads in a sentence. On the one hand, 
the typical POSs of predicate heads in Chinese 
sentences are verbs, adjectives and descriptive 
words 2 . Each of them may have multiple in-
stances in a sentence. On the other hand, while a 
simple sentence has only one predicate head, a 
complex sentence may have multiple ones. The 
                                                 
2 We only focus on Verbs and adjectives in this work. 
latter constitutes 8.25% in our corpus. Thus, the 
real difficulty lies in how to recognize the true 
predicate head of a sentence among so many 
possibilities. 
Take a simple sentence as example: 
?/rN ?/qN ?/v ??/a ??/n ?
/uJDE ?/a ?/n ??/v ??/aD ?
/uJDE ??/v ?/n ?/cC ??/v ?
/n ?/wE 
The quasi-predicate heads (verbs and adjectives) 
include ?/v, ??/a, ?/a, ??/v, ??/v, 
and ??/v. However, there are two MNPs in 
this sentence, namely, ??/rN ?/qN ?/v ??
/a ??/n ?/uJDE ?/a ?/n? and ???/aD 
?/uJDE ??/v ?/n ?/cC ??/v ?/n?. 
These two MNPs cover most quasi-predicate 
heads in the sentence, except ??/v, the true 
predicate head that we want. 
An MNP is a complete semantic unit, and its 
internal structure may include different kinds of 
constituents (Jian et al, 2009). Therefore, the 
fundamental structure of a sentence can be made 
clear after recognizing its MNPs. This can help 
filter out those wrong quasi-predicates for a bet-
ter shortlist of good candidates for the true predi-
cate head in a sentence. 
In practice, the identification of predicate head 
begins with recognizing MNPs in the same sen-
tence. It turns the above example sentence into: 
[ ?/rN ?/qN ?/v ??/a ??/n ?
/uJDE ?/a ?/n ] ??/v [ ??/aD 
?/uJDE ??/v ?/n ?/cC ??/v ?
/n ] ?/wE 
These MNPs are then replaced with the conven-
tional label ?NP? for noun phrase, resulting in a 
simplified sentence structure as follows. 
NP/NP  ??/v  NP/NP ?/wE 
This basic sentence structure can largely allevi-
ates the complexity of the original sentence and 
narrows down the selection scope of quasi-
predicates for the true head. In this particular 
example, the only verb left in the sentence after 
MNP recognition is the true predicate head. 
4 Predicate Head Identification  
This section describes the process of identifying 
predicate heads in sentences. As illustrated in 
Figure 1 below, it can be divided into three steps: 
Step 1: recognize the MNPs in a sentence and 
replace the MNPs with ?NP? label to simplify 
the sentence structure. 
Step 2: recognize the predicate heads in the 
resulted simplified structure. 
Step 3: post-process the preliminary results to 
correct the wrong predicate heads according to 
heuristics in a rule base. 
4.1 MNP Recognition 
The MNP recognition is performed via a trained 
CRF model on unlabeled data. We adopt the 
method in Dai et al (2008), with modified tem-
plates for the different corpus. Each feature is 
composed of the words and POS tags surround-
ing the current word i, as well as different com-
bination of them. The context window of tem-
plate is set to size 3. Table 1 shows the feature 
template we use.  
Type Features 
Unigram Wordi Posi 
Bigram Wordi/Posi  
Surrounding Wordi-1/Wordi Posi-1/Posi 
 Wordi/Wordi+1 Posi/Posi+1 
 Wordi-2/Posi-2 Posi-2/Posi-1 
 Posi-2/Posi-1/Posi Posi-3/Posi-2 
 Posi-1/Posi/Posi+1 Wordi+3/Posi+3 
 Posi+1/Posi+2/Posi+3 Wordi+2/Wordi+3
 
Table 1: Feature Template 
 
Test data Final results 
Preliminar
 
 
Figure 1: Flow Chart of Predicate Head Identification 
 
The main effective factors for MNPs recogni-
tion are the lengths of MNPs and the complexity 
of sentence in question. We analyze the length 
distribution of MNPs in TCT 3  corpus, finding 
that their average length is 6.24 words and the 
longest length is 119 words. Table 2 presents this 
distribution in detail. 
 
Length of MNP Occurrences Percentage (%)
len?5 3260 48.82 
5?len?10 2348 35.17 
len?10 1069 16.01 
 
Table 2: Length Distribution of MNPs in TCT Corpus 
 
The MNPs longer than 5 words cover 50% of 
total occurrences, indicating the relatively high 
complexity of sentences. We trained a CRF 
model using this data set, which achieves an F-
score of 83.7% on MNP recognition. 
4.2 Predicate Head Identification 
After the MNPs in a sentence are recognized, 
they are replaced by ?NP? label to rebuild a sim-
plified sentence structure. It largely reduces the 
difficulty in identifying predicate heads from this 
simplified structure.  
We evaluate our models by their precision in 
the test set, which is formulated as 
                                                 
3 Tsinghua Chinese Treebank ver1.0. 
_
100%
_
right sentences
Precision
Sum sentences
= ?      (1) 
The right_sentences refer to the number of sen-
tences whose predicate heads are successfully 
identified, and the sum_sentences to the total 
number of sentences in the test set. We count a 
sentence as right_sentence if and only if all its 
predicate heads are successfully identified, in-
cluding those with multiple predicate heads. 
For each predicate head, we need an appropri-
ate feature representation f (i, j). We test the 
model performance with different context win-
dow sizes of template. The results are shown in 
Table 3 as follows. 
 
Template Context window size Precision (%) 
Temp1 2 79.27 
Temp2 3 82.59 
Temp3 4 81.37 
 
Table 3: Precisions of Predicate Heads Recognition under 
Different Context Window Sizes 
 
It shows that the window size of 3 words gives 
the highest precision (82.59%). Therefore we 
apply this window size, together with other fea-
tures in our CRF model, including words, POSs, 
phrase tags and their combinations. There are 24 
template types in total. 
4.3 Post-processing 
The post-processing stage is intended to correct 
errors in the preliminary identification results of 
MNP recognition MNP replacement Predicate head recognition y results 
Predicate head recognition model Rule base MNP recognition model 
predicate heads, by applying linguistic rules for-
mulated heuristically. We test each rule to see if 
it improves the recognition accuracy, so as to 
retrieve a validated rule base. The labeling of 
predicate heads follows the standard of TCT and 
a wrong labeling is treated as an error. 
There are three main types of error, according 
to our observation. The first is that no predicate 
head is identified. The second is that the whole 
sentence is recognized as an MNP, such that no 
predicate head is recognized. The third is that the 
predicate head is incorrectly identified, such as  
??? in the expression ???????, where the 
correct answer is ???? according to the TCT 
standard.  
 
Error types Percentage Improved  percentage 
No predicate head 17.50% 2.44% 
a sentence as an MNP 10.63% 1.11% 
??????? 8.75% 0.56% 
Others 63.12% 2.77% 
 
Table 4: Types of Error  
 
Table 4 lists different types of error, together 
with their percentage in all sentences whose 
predicate heads have been mistakenly identified, 
and the improvement in percentage after the 
post-processing. To correct these errors, a num-
ber of rules for post-processing are formulated. 
The main rules are the followings: 
? If no predicate head is recognized in a sen-
tence, we label the first verb as the predi-
cate head. 
Error sample??/p [ ????/m ?/qT ?
???/nR ] ?/f ?/wP [ ??/nS ??/d 
??/v ????/b ???/b ??/n ] ?
/wE 
Corrected??/p [ ????/m ?/qT ??
??/nR ] ?/f ?/wP [ ??/nS ??/d 
??/v ????/b ???/b ??/n ] ?
/wE 
? If the whole sentence is recognized as an 
MNP, such that no predicate head is identi-
fied, we label the first verb as the predicate 
head. 
 Error sample?[ ??/n ??/v ?/n ?
/cC ?/n ?/m ??/n ] ?/wE 
Corrected?[ ??/n ??/v ?/n ?/cC ?
/n ?/m ??/n ] ?/wE 
? For expression ???????, we label ??
?? as the predicate head. 
Error sample?[ ?/rB ?/m ?/qN ??/n ] 
??/v ???/n ?/vC [ ?/d ?/v ??
??/n ?/cC ????/n ??/n ?/uJDE 
???/b ??/n ] ?/wE 
Corrected?[ ?/rB ?/m ?/qN ??/n ] 
??/v ???/n ?/vC [ ?/d ?/v ??
??/n ?/cC ????/n ??/n ?/uJDE 
???/b ??/n ] ?/wE 
There are also other rules in the rule base be-
sides the above ones. For example, if the first 
word of a sentences is ??? or ????, it is la-
beled as the predicate head. 
5 Experiments 
5.1 Data Sets 
Our experiments are carried out on the Tsinghua 
Chinese Treebank (TCT). Every constituent of a 
sentence in TCT is labeled by human expert. We 
randomly extract 5000 sentences from TCT and 
remove those sentences that do not have predi-
cate head. Finally, our data set contains 4613 
sentences, in which 3711 sentences are randomly 
chosen as training data and 902 sentences as test-
ing data. The average length of these sentences 
in training set is 20 words. 
The number of quasi-predicate heads in a sen-
tence is a critical factor to determine the per-
formance of predicate head recognition. Reduc-
ing the number of quasi-predicate heads can im-
prove the recognition precision. Table 5 shows 
the percentage of quasi-predicate heads in train-
ing data before and after MNP replacement. 
 
Number of 
quasi-
predicates 
Percentage before 
MNP replace-
ment(%) 
Percentage after 
MNP replace-
ment(%) 
1 12.50 49.69 
2 19.62 27.22 
3 20.37 12.37 
>3 47.51 10.72 
 
Table 5: The Percentage of Quasi-predicate Heads Before 
and After MNP Replacement 
 
From Table 5, we can see that almost half sen-
tences contain more than three quasi-predicate 
heads. Only 12.5% of sentences have only one 
quasi-predicate head before MNP replacement. 
However, after MNPs are replaced with the ?NP? 
label, only 10.72% contain more than three 
quasi-predicate heads and nearly 50% contain 
only one quasi-predicate head. We have evidence 
that MNP pre-processing can reduce the number 
of quasi-predicate heads and lower the complex-
ity of sentence structures. 
5.2 Results and Discussion 
For comparison purpose, we developed four dif-
ferent models for predicate head recognition. 
Models 1 and 2 are CRF models, the former rec-
ognizing predicate heads directly and the later 
recognizing MNPs at the same time. Model 3 
recognizes predicate heads based on MNP pre-
processing. Model 4 is based on model 3, includ-
ing the post-processing stage. Table 6 shows the 
recognition performance of each model using the 
best context window size. 
 
Model Context window size 
Number of cor-
rect sentences 
Preci-
sion(%) 
model 1 4 680 75.39 
model 2 4 687 76.16 
model 3 3 745 82.59 
model 4 3 804 89.14 
 
Table 6: Performance of Different Models 
 
Comparing these models, we can see that the 
additional feature in model 2 leads to 1% im-
provement in precision over model 1. Moreover, 
the MNP pre-processing in model 3 results in a 
large increase in accuracy, compared to model 1. 
It indicates that the MNP pre-processing does 
improve the precision of recognition. Compared 
with model 3, model 4 achieves a precision even 
6.55% higher, indicating that the post-processing 
is also an effective step for recognition. 
As shown, the performance is affected by the 
effect of MNP recognition. There are three kinds 
of relation between the predicate heads and the 
types of MNP recognition error: 
Relation 1: The whole sentence is recognized 
as an MNP. 
Relation 2: The boundaries of an MNP are in-
correctly recognized and the MNP does not con-
tain the predicate head. 
Relation 3: The boundaries of an MNP are in-
correctly recognized and the MNP contains the 
predicate head. Table 7 shows the distribution of 
these three relations in the recognition errors. 
 
Relation Number of sentences Percentage(%)
Relation 1 17 5.47 
Relation 2  281 90.35 
Relation 3 13 4.18 
 
Table 7: Distribution of the Three Relations in 
Recognition Errors 
In our approach, the errors of relation 1 and 
relation 3 can be solved by the post-processing, 
as presented in Section 4.3. Relation 2 holds the 
largest proportion among the three. But the error 
rate of predicate head recognition only reaches 
31.67% in this case. That is to say, although the 
MNP boundaries are incorrectly recognized, the 
accuracy of predicate head recognition can still 
reach 68.33%. 
Chen (2007) proposed a probabilistic model 
(model 5) for recognizing predicate heads in Chi-
nese sentences. The probabilities of quasi-
predicates are estimated by maximum likelihood 
estimation. A discounted model is used to 
smooth parameters. We compare his model with 
our model 3 using different contextual features 
on TCT corpus. Table 8 shows the comparison 
results.  
The highest precision of model 3 is 82.59% 
when the context window size is set to 3. For 
model 5, it is 70.62% at a context window size of 
4. Experimental results show that the precision of 
our method is about 12% higher than Chen?s. 
 
Context window size Model Precision (%) 
model 5 69.18 2 
model 3 79.27 
model 5 70.18 
3 
model 3 82.59 
model 5 70.62 
4 
model 3 81.37 
 
Table 8: Comparison between model 3 and Chen?s model 
 
Beside Chen?s method, the Stanford Parser 
can also recognize the predicate heads in simple 
Chinese sentences. The root node of dependency 
tree is the predicate head. For a comparison, we 
randomly extract two hundred simple sentences 
in our test data to compare it with the outputs of 
our model 3. We also train a model of predicate 
head recognition (model 6), which assumes that 
all MNPs are successfully identified. The com-
parison is shown in Table 9. We can see that the 
precision of model 6 is 8.35% higher than model 
3. This means that our method still has a certain 
room for further improvement. 
 
Stanford Parser model 3 model6 
78.17% 83.15% 91.5% 
 
Table 9: Comparison between model 3 and Stanford 
Parser 
5.3 Error Analysis 
As shown above, the post-processing can correct 
most errors in the recognition of predicate heads. 
But we also observe some errors that cannot be 
corrected this way. For example, 
???/n?/p ???/n ??/v [ ??/n 
??/n ] ??/v ?/wE 
The predicate head here is ????, but usually 
???? is recognized as the predicate head. This 
is because ???? can be used either as a verb or 
a noun. There are many verbs of this kind in Chi-
nese, such as ??? ? and ??? ?. Mistakes 
caused by the flexibility of Chinese verb and the 
ambiguity of sentence structure appear to deserve 
more of our effort. Meanwhile, there are also 
some other unusual cases that cannot be properly 
solved with statistical methods. 
6 Conclusion 
Identification of predicate heads is important to 
syntactic parsing. In this paper, we have pre-
sented a novel method that combines both statis-
tical and rule-based approaches to identify predi-
cate heads based on MNP pre-processing and 
rule-based post-processing. We have had a series 
of experiments to show that this method achieves 
a significant improvement over some state-of-
the-art approaches. Furthermore, it also provides 
a simple structure of sentence that can be utilized 
for parsing. 
In the future, we will study how semantic in-
formation can be applied to further improve the 
precision of MNP recognition and predicate head 
identification. It is also very interesting to ex-
plore how this approach can facilitate parsing, 
including shallow parsing. 
Acknowledgments 
We would like to thank the anonymous review-
ers for their helpful comments and suggestions. 
We also thank Billy Wong of City University of 
Hong Kong for his much-appreciated input dur-
ing the writing process. 
References  
Zhiqun Chen. 2007. Study on recognizing predicate of 
Chinese sentences. Computer Engineering and 
Applications, 43(17): 176-178. 
Yuchang Cheng, Asahara Masayuki, and Matsumoto 
Yuji. 2005. Chinese deterministic dependency ana-
lyzer: examining effects of global features and root 
node finder. In Proceedings of the Fourth 
SIGHAN Wordshop on Chinese Language 
Processing, pp. 17-24. 
Cui Dai, Qiaoli Zhou, and Dongfeng Cai. 2008. 
Automatic recognition of Chinese maximal-length 
noun phrase based on statistics and rules. Journal 
of Chinese Information Processing, 22(6): 110-
115. 
Xiaojin Gong, Zhensheng Luo, and Weihua Luo. 
2003. Recognizing the predicate head of Chinese 
sentences. Journal of Chinese Information 
Processing, 17(2): 7-13. 
Ping Jian, and Chengqing Zong. 2009. A new ap-
proach to identifying Chinese maximal-length 
phrase using bidirectional labeling. CAAI Trans-
actions on Intelligent Systems, 4(5): 406-413. 
Guochen Li, and Jing Meng. 2005. A method of iden-
tifying the predicate head based on the correpon-
dence between the subject and the predicate. Jour-
nal of Chinese Information Processing, 19(1): 
1-7. 
Zhensheng Luo, and Bixia Zheng. 1994. An approach 
to the automatic analysis and frequency statistics of 
Chinese sentence patterns. Journal of Chinese 
Information Processing, 8(2): 1-9. 
Zhifang Sui, and Shiwen Yu. 1998a. The research on 
recognizing the predicate head of a Chinese simple 
sentence in EBMT. Journal of Chinese Informa-
tion Processing, 12(4): 39-46. 
Zhifang Sui, and Shiwen Yu. 1998b. The acquisition 
and application of the knowledge for recognizing 
the predicate head of a Chinese simple sentence. 
Journal of Peking University (Science Edition), 
34(2-3): 221-229. 
Honglin Sun, and Shiwen Yu. 2000. Shallow parsing: 
an overview. Contemporary Linguistics, 2(2): 
74-83. 
Hui Tan. 2000. Center predicate recognization for 
scientific article. Journal of WuHan University 
(Natural Science Edition), 46(3): 1-3. 
Qiang Zhou, Maosong Sun, and Changning Huang. 
2000. Automatically identify Chinese maximal 
noun phrase. Journal of Software, 11(2): 195-201. 
Michael Collins. 1999. Head-driven statistical 
models for natural language parsing. Ph. D. 
Thesis, University of Pennsylvania. 
The SAU Report for the 1st CIPS-SIGHAN-ParsEval-2010 
Qiaoli Zhou Wenjing 
Lang 
Yingying 
Wang 
Yan Wang Dongfeng Cai
Knowledge Engineering Research Center,Shenyang Aerospace 
University,Shenyang,China 
Qiaoli_z@yahoo.com.cn 
 
Abstract 
This paper presents our work for 
participation in the 2010 CIPS-SIGHAN 
evaluation on two tasks which are Event 
Description Sub-sentence (EDSs) 
Analysis and Complete Sentence (CS) 
Parsing in Chinese Parsing. The paper 
describes the implementation of our 
system as well as the results we have 
achieved and the analysis. 
1 Introduction 
The paper describes the parsing system of SAU 
in 1st CLPS-SIGHAN evaluation task 2. We 
participate in two tasks - EDS Analysis and CS 
Parsing. The testing set only provides 
segmentation results, therefore, we divide our 
system into the following subsystems: (1) Part-
of-Speech (POS) tagging system, we mainly 
make use of Conditional Random Fields (CRFs) 
model for POS tagging; (2) parsing system, the 
paper adopts divide-and-conquer strategy to 
parsing, which uses CCRFs model for parsing 
and adopts searching algorithm to build trees in 
decoding; (3) head recognition system, which 
also makes use of CCRFs model. 
The rest of the paper is organized as follows: 
Section 2 describes the POS tagging system; 
Section 3 describes the structure of our parsing 
system; Section 4 describes head recognition 
system in parsing tree; Section 5 presents the 
results of our system and the analysis; Section 6 
concludes the paper. 
2 Part-of-Speech Tagging 
We use CRFs model and post-processing 
method for POS tagging. In the first step, we tag 
POS based on CRFs. The second step is the 
post-processing after tagging, which is 
correcting by using dictionary drawn from 
training set. The system architecture of POS 
tagging is shown in Figure 1. 
2.1 Features 
Feature selection significantly influences the 
performance of CRFs. We use the following 
features in our system. 
Atom Template 
word(-2) , word(-1) , word(0) , word(1) , word(2) 
prefix( word (0) ) ,suffix( word(0) ) 
includeDot1(word ( 0 )) 
includeDot2(word ( 0 )) 
Complex Template 
word(-1)& word(0) ? word(0)& word(1) 
word(0)& prefix( word (0) ) 
word(0)& suffix( word(0) ) 
word(0)& includeDot1(word ( 0 )) 
word(0)& includeDot2(word ( 0 )) 
Table 1: Feature templates used in POS tagger. 
word(i) represents the ith word, prefix( word (i) ) 
represents the first character of the ith word, 
suffix( word (i) ) represents the last character of  
the ith word, ncludeDot1(word ( i)) represents 
the ith word containing ?? ? or not, and 
includeDot2(word ( i)) represnts the ith word 
containing ?.? or not. 
2.2 Post-processing 
The post-processing module adopts the 
following processing by analyzing the errors 
from tagging result based on CRFs. We firstly 
need to build two dictionaries which are single 
class word dictionary and ambiguity word 
dictionary before the post-processing. The 
single class word dictionary and ambiguity 
word dictionary are built by drawing from 
training set. 
 
The single class word is the word having 
single POS in training set, and the ambiguity 
word is the word having multi POS in training 
set. Besides, we build rules for words with 
distinctive features aiming at correcting errors, 
such as ???, numbers and English characters, 
etc. 
Figure 2 shows the post-processing step after 
POS tagging by CRFs model. As shown in 
Figure 2, we respectively post-process single 
class words and ambiguity words according to 
CRF score. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1) Single class word processing module 
The post-processing of single class words 
consults the single class word dictionary and 
CRFs score. When the score from CRFs is 
higher than 0.9, we take the POS from CRFs as 
the final POS; otherwise, POS of the word is 
corrected by the POS in the single class word 
dictionary. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2 
3 
1 
N 
CRF Primary result 
Word class? 
Ambiguity word 
Single class word 
Unknown word 
End 
Rule base 
Figure2: Post-processing architecture after CRF labeling 
Single class word 
processing module 
Ambiguity word 
processing module 
Unknown word 
processing module 
Training 
corpus 
Features selection 
Parameter estimation 
CRF model
Testing 
corpus 
POS tagger based 
on CRF 
Primary reco- 
gnition result 
Post-processing 
POS 
Result 
Figure 1: System architecture of POS tagging
(2) Ambiguity word processing module 
The post-processing of ambiguity words 
consults the ambiguity word dictionary and 
CRFs score. When the POS from CRFs belongs 
to the POS of the word in the ambiguity word 
dictionary, we take the POS from CRFs as the 
final POS; otherwise, we examine the score of 
CRF, if the score is less than 0.4, the final POS 
of the word is the POS who has the highest 
score (has highest frequency), or else taking 
POS from CRF as the final POS. 
(3) Unknown word processing module 
The unknown words are the words not in 
training set. By analyzing the examples, we find 
that there are great deals of person names, 
location names, organization names and 
numbers, etc. And the words have 
characteristics when building word, therefore, 
we set up rules for processing. 
2.3 Experiment results 
 Table 2 shows the comparative experimental 
results of POS tagging using two methods. 
Table 2: Comparative POS tagging results 
3 Parsing system 
The paper uses divide-and-conquer strategy 
(Shiuan 1996 et al, Braun 2000 et al, Lyon 
1997 et al)for parsing. Firstly, we recognize 
MNP for an input sentence, which divide the 
sentence into two kinds of parts. One kind is 
MNPs, and the other one is frame which is a 
new sentence generating by replacing MNP 
using its head word. Secondly, we use parsing 
approach based on chunking (Abney, 1991, Erik 
Tjong and Kim Sang, 2001) and a searching 
algorithm in decoding. Thirdly, we combine the 
parsing trees of MNPs and frame, which obtains 
the full parsing tree of the original sentence. 
Figure 3 shows the architecture of paring 
system. 
3.1 MNP recognition 
Maximal Noun Phrase (MNP) is the noun 
phrase which is not contained by any other noun 
phrases. We use Berkeley parser (2009 1.0) for 
MNP recognition. We first use Berkeley parser 
to parse sentences after POS tagging, and then 
we tag MNPs from the parsing results. As the 
following example: 
Berkeley parser result: dj[ ??/nS vp[ ??/v 
vp[ ??/v np[ pp[ ?/p np[ ??/nS ??/n ] ] ?
/uJDE ??/n ] ] ] ] 
MNP recognition result: ??/nS ??/v ??
/v np[ ?/p ??/nS ??/n ?/uJDE ??/n ]  
The results of MNP recognition EDSs 
analysis and CS parsing are as table3: 
 
 P R F 
EDSs 85.3202% 85.998% 85.6578% 
CS 77.7102% 79.2782% 78.4864% 
Table 3: Results of MNP recognition 
3.2 Head recognition of MNP and 
generation of frame 
 In this paper, the new sentence in which MNPs 
are replaced by their head word is defined as the 
sentence?s frame. The head of MNPs is 
identified after MNP recognition and then they 
are used to replace the original MNP, and 
finally the sentence?s frame is formed. We use 
the rules to recognize the head of MNP. Usually, 
the last word of MNP is the head of the phrase, 
which can represent the MNP in function. For 
example: ?[?/r ??/n] ??/ad ??/v ??/v 
[?? /v ?? /v ? /u ?? /n]? ? In this 
sentence? ? /r?? /n? and ? ??/v ??/v 
?/u ??/n? are MNPs. If we omit the 
modifier in MNP, for example ?[??/n] ??
/ad ??/v ??/v [??/n]??, the meaning of 
the sentence will not be changed. Because the 
head can represent the syntax function of MNP, 
we can use the head for parsing, which can 
avoid the effect of the modifier of MNP on 
parsing and reduce the complexity of parsing. 
Method EDSs precision 
CS 
precision
CRF 92.83% 89.42% 
CRF +  
post-processing 93.96% 91.05% 
However, the components of MNP are 
complicated, not all of the last word of MNP 
can be the head of MNP. The paper shows that 
if MNP has parentheses, we can use the last 
word before parentheses as the head. When the 
last word of MNP is ???, we use the second last 
word as the head.  
3.3 Chunking with CRFs 
The accuracy of chunk parsing is highly 
dependent on the accuracy of each level of  
 
chunking. This section describes our approach 
to the chunking task. A common approach to 
the chunking problem is to convert the problem 
into a sequence tagging task by using the 
?BIEO? (B for beginning, I for inside, E for 
ending, and O for outside) representation. 
This representation enables us to use the 
linear chain CRF model to perform chunking, 
since the task is simply assigning appropriate 
labels to sequence. 
3.3.1 Features 
Table 4 shows feature templates used in the 
whole levels of chunking. In the whole levels of 
chunking, we can use a rich set of features 
because the chunker has access to the 
information about the partial trees that have 
been already created (Yoshimasa et al, 2009). It 
uses the words and POS tags around the edges 
of the covered by the current non-terminal 
symbol. 
Table 4: Feature templates used in parsing system.  
W represents a word, P represents the part-of-speech 
of the word, C represents the sum of the chunk 
containing the word, F represents the first word of 
the chunk containing the word, L represents the last 
word of the chunk containing the word, S represents 
that the word is a non-terminal symbol or not. Wj is 
the current word; Wj-1 is the word preceding Wj, Wj+1 
is the word following Wj. 
 
 
 
 
 
 
 
 
3.4 Searching for the Best Parse 
The probability for an entire parsing tree is 
computed as the product of the probabilities 
output by the individual CRF chunkers: 
0
(y / )
h
i i
i
score p x
=
=?  
We use a searching algorithm to find the highest 
probability derivation. CRF can score each 
chunker result by A* search algorithm, 
therefore, we use the score as the probability of 
each chunker. We do not give pseudo code, but 
the basic idea is as figure 4. 
 
 
1: inti parser(sent) 
2: Parse(sent, 1, 0) 
    3: 
    4: function Parse(sent, m, n) 
    5:  if sent is chunked as a complete sentence 
    6:     return m 
    7:  H = Chunking(sent, m/n) 
    8:   for h?H do 
    9:    r = m * h.probability 
    10:     if r?n then 
    11:        sent2 = Update(sent, h) 
    12:        s = Parse(sent2, r, n) 
    13:        if s?n then n = s 
    14:    return n 
15: function Chunking(sent, t) 
    16: perform chunking with a CRF chunker and 
return a set of chunking hypotheses whose  
17: probabilities are greater than t. 
18: function Update(sent, h) 
19:  update sequence sent according to chunking 
hypothesis h and return the updated sequence. 
Figure 4: Searching algorithm for the best parse  
 
It is straightforward to introduce beam search 
in this search algorithm?we simply limit the 
number of hypotheses generated by the CRF 
chunker. We examine how the width of the 
beam affects the parsing performance in the 
Word Unigrams W-2 , W-1, W0, W1, W2,
Word Bigrams W-2W-1, W-1W0, W0W1, 
W1W2, W0W-2, W0W2,  
Word Trigrams W0W-1W-2, W0W1W2
POS Unigrams P-3, P-2 , P-1 , P0 , P1, P2, P3,
POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P1, 
P1P2, P2P3, P0P-2, P0P2,
POS Trigrams P-3P-2P-1, P-2P-1P0, P-1P0P1, 
P0P1P2, P1P2P3
Word & POS W0P0, W0P-1, W0P1,
Word & WordCount W0C0
Word & FirstWord W0F0 , W-1F0
Word & LastWord W0L0, W1L0
Word & Symbol W0S0
Chunk Model
 frame 
MNPs
sentence 
MNP Recognition parsing tree
Search
CRF Chunker
Figure3: Parsing system architecture 
experiments. We experiment beam width and 
we adopt the beam width of 4 at last. 
3.5 Head Finding 
Head finding is a post process after parsing in 
our system. The paper uses method combining 
statistics and rules to find head. The selected 
statistical method is CRF model. The first step 
is to train a CRF classifier to classify each 
context-free production into several categories. 
Then a rule-based method is used to post 
process the identification results and gets the 
final recognition results. The rule-based post-
processing module mainly uses rule base and 
case base to carry out post-processing. 
3.6 Head finding based on CRFs 
The head finding procedure proceeds in the 
bottom-up fashion, so that the head words of 
productions in lower layers could be used as 
features for the productions of higher layers 
(Xiao chen et al 2009). 
 
Atom template Definition 
CurPhraseTag The label of the current word 
LCh_Word The left most child 
RCh_Word The right most child 
LCh_Pos The POS of the left most child
MCh_Pos The POS of the middle child 
RCh_Pos The POS of the right most child
NumCh The number of children 
CurPhraseTag 1 ? The labels of the former phrase and the latter 
Table 5: Atom templates for Head finding 
 
Table 6: Complex templates for Head finding 
 
The atom templates are not sufficient for 
labeling context; therefore, we use some 
complex templates by combining the upper 
atom templates for more effectively describing 
context. When the feature function is fixed, the 
atom templates in complex templates are 
instantiated, which will generate features. 
The final feature templates are composed of 
the atom templates and the complex templates. 
The feature templates of the head recognition in 
phrases contain 24 types. 
3.7 Head Finding based on rules 
Through the analysis of error examples, we 
found that some CRFs recognition results are 
clearly inconsistent with the actual situation; we 
can use rules to correct these errors, thus 
forming a rule base. Example-base is a chunk-
based library built through analysis and 
processing on the training corpus. The 
Example-base is composed of all the bottom 
chunk and high-level chunk in training corpus. 
High-level phrases are the bottom chunk 
replaced by heads. 
3.8 Experiment results of head finding 
Table 7 shows the comparative experiment 
results of head recognition. 
 
Table7: Comparative results of head recognition 
4 Experiment of parsing system 
We perform experiments on the training set and 
testing set of Tsinghua Treebank provided by 
CIPS-SIGHAN-ParsEval-2010. For the direct 
fluence of parsing result by the length of 
sentence, we count the length distribution of 
corpus. 
in
Table 8 shows that the length of training set 
and testing set of EDSs is mostly less than 20 
words. The length of training set of CS is evenly 
distributed, while the length of testing set is 
between 30 and 40 words. 
Complex Template 
CurPhraseTag/ NumCh, CurPhraseTag/ LCh_Word, 
CurPhraseTag/LCh_Pos, 
CurPhraseTag/LCh_Pos/RCh_Pos, 
CurPhraseTag/NumCh/LCh_Pos/ RCh_Pos, 
CurPhraseTag/NumCh/LCh_Word/LCh_Pos/MCh_
Pos/RCh_Word/RCh_Pos,  
LCh_Word/LCh_Pos, CurPhraseTag/MCh_Pos, 
NumCh/LCh_Pos/ MCh_Pos/ RCh_Pos, 
 CurPhraseTag/ NumCh/ MCh_Pos, 
CurPhraseTag/LCh_Word/LCh_Pos/MCh_Pos/RCh
_Word/RCh_Pos,  
LCh_Word/ LCh_Pos, LCh_Pos/ MCh_Pos, 
 CurPhraseTag/NumCh, RCh_Word/RCh_Pos,  
NumCh/LCh_Word/LCh_Pos/MCh_Pos/RCh_Word
/RCh_Pos 
 Total Num 
Wrong 
Num Precision 
CRFs 7035 93 98.68% 
CRFs + 
rule-base+ 
case-base 
7035 74 98.95% 
The paper adopts divide-and-conquer strategy 
to parsing; therefore, we conduct the 
frame whose length is less than 5 words, the frame 
length distribution of training set is 9.17% higher 
than the testing set; for the frame whose length is 
more than 5 words and less than 10 words, the 
training set is 7.65% lower than testing; and for the 
frame whose length is between 10 words and 20 
words, the testing set is 20.09% higher compared 
with the training set. From another aspect, in 
testing set, CS is 46.2% lower compared with 
EDSs for frame whose length is less than 5. 
Therefore, the complexity of frame in CS is higher 
than in EDSs. 
comparative experiment of MNP parsing and 
frame parsing. In addition, the results of MNP 
parsing and frame parsing depend on the length 
largely, so we list the length distribution of 
MNP and frame of EDSs and CS as table 9 and 
table 10. 
 
As shown in Table 8, 9 and 10, the length 
distribution of testing set shows that the paring unit 
length of EDSs is reduced to less than 10 from less 
than 20 in original sentence and CS is reduced to 
less than 20 from between 30 and 40 after dividing 
an original sentence into MNPs parts and frame 
part. The above data indicate the divide-and-
conquer strategy reduces the complexity of 
sentences significantly. 
Table 8: Length distribution of EDSs and CS 
 EDSs CS 
length training set 
testing 
set 
training 
set 
testing 
set 
[0, 10) 50.68% 64.30% 10.59% 0 
[10,20) 37.27% 29.50% 27.55% 0 
[20,30) 8.64% 5.40% 26.37% 79.9%
[30,40) 2.31% 0.60% 16.63% 20.1%
40? 1.10% 0.20% 18.86% 0 
 
We define Simple MNP (SMNP) whose 
length is less than 5 words and Complete MNP 
(CMNP) whose length is more than 5 words. 
 We can conclude that the parsing result of CS 
is lower than EDSs from Table 11, which is due 
to the higher complexity of MNP and frame in CS 
compared with EDSs from the results of Table 9 
and Table 10. In addition, we obtain about 1% 
improvement compared with Berkeley parser in 
MNP and Frame parsing result in EDSs from 
Table 11 and Table 12, which indicates that our 
method is effective for short length parsing units. In 
particular, Table 12 shows that our result is 1.8% 
higher than Berkeley parser in the frame parsing of 
CS. Due to the non-consistent frame length 
distribution of training set and testing set in CS 
from Table 10, we find that Berkeley parser largely 
depends on training set compared with our method. 
Table 9: Length distribution of MNP  
 EDSs CS 
length training set 
testing 
set 
training 
set 
testing 
set 
[0,5) 55.30% 62.46% 55.42% 59.45%
[5,10) 32.66% 29.69% 32.57% 30.77%
[10,20) 10.03% 6.75% 10.03% 8.65%
20? 2.00% 1.09% 1.98% 1.12%
 
Table 9 shows the length distribution of MNP 
in training set and testing set of sub-sentence is 
consistent in basic, but the SMNP distribution 
of EDSs is 3.01% less than CS, which 
illuminates the complexity of MNP in CS is 
higher than in EDSs. 
 
 EDSs CS 
length training set 
testing 
set 
training 
set 
testing 
set 
[0,5) 45.84% 47.20% 10.17% 1.00%
[5,10) 43.58% 44.00% 24.14% 10.80%
[10,20) 9.98% 8.70% 41.31% 62.20%
20? 0.60% 0.10% 24.38% 26.00%
To more fairly compare the performance of 
our proposed method, the comparative results 
are shown as Table 13, the first one (Model01) 
is combination method of MNP pre-processing 
and chunk-based, and the chunk-based result 
which adopts CCRFs method with searching 
algorithm; the second one (Berkeley) is the 
parsing result of Berkeley parser; the third one 
(Model02) also is combination method of MNP 
pre-processing and chunk-based, and the chunk-
based result which adopts CCRFs method only; 
and the lase one (Model03) is the chunk-based 
result which adopts CCRFs method with 
searching algorithm. 
Table 10: Length distribution of frame 
 
Table 10 shows the length distribution of frame 
in training set and testing set of EDSs is consistent 
in basic, while the CS is non-consistent. For the 
  
    
 method P R F 
Berkeley 87.5746% 87.8365% 87.7053% 
EDSs 
Proposed Method 88.5752% 88.6341% 88.6047% 
Berkeley 84.4755% 84.9182% 84.6963% CS 
Proposed Method 84.7535% 85.046% 84.8995% 
Table 11: Comparative results of MNP parsing 
 
 method P R F 
Berkeley 91.3411% 91.1823% 91.2617% 
EDSs 
Proposed Method 92.4669% 92.0765% 92.2713% 
Berkeley 85.4388% 85.3023% 85.3705% 
CS 
Proposed Method 87.3357% 87.0357% 87.1854% 
Table12: Comparative results of Frame parsing 
 
 P R F 
Model 01 85.42% 85.35% 85.39%
Berkeley 84.56% 84.62% 84.59%
Models 02 85.31% 85.30% 85.31%
Models 03 83.99% 83.77% 83.88%
Table13: Comparative results of EDSs 
 
dj constituent fj constituent overall F 
P R P R F F F 
Model 01 78.64% 78.73% 78.69% 70.22% 71.62% 70.91% 74.80% 
Berkeley 78.37% 78.16% 78.26% 69.43% 72.42% 70.89% 74.58% 
Models 02 78.18% 78.30% 78.24% 70.20% 70.98% 70.59% 74.41% 
Models 03 77.38% 77.41% 77.39% 70.39% 70.01% 70.24% 73.82% 
Table14: Comparative results of CS 
 
From Table 13, we can see that Model01 
performance in EDSs is improved by 0.08% 
than Model02, and the searching algorithm 
helps little in EDSs analysis. From Table 14, we 
can see that Model01 performance in CS is 
improved by 0.4% than Model02, better than 
Berkeley parser result with search algorism. 
Overall, in EDSs analysis, Model01 
performance is improved by 0.8% than 
Berkeley parser, and in overall F-measure of CS, 
Model01 performance is 0.22% higher than 
Berkeley parser. From Table 13 and 14, We can 
see that Model01 performance in EDSs is 
improved by 1.51% than Model03 and the 
Model01 in CS is improved by 0.98% than 
Model03, and the MNP pre-processing helps. 
5 Conclusions 
We participate in two tasks - EDS Analysis 
and CS Parsing in CLPS-SIGHAN- ParsEval-
2010. We use divide-and-conquer strategy for 
parsing and a chunking-based discriminative 
approach to full parsing by using CRF for 
chunking. As we all know, CRF is effective for  
chunking task. However, the chunking result in 
the current level is based on the upper level in 
the chunking-based parsing approach, which 
will enhance ambiguity problems when the 
input of the current level contains non-terminal 
symbols, therefore, the features used in 
chunking is crucial. This paper, for effectively 
using the information of partial trees that have 
been already created, keeps the terminal 
symbols in the node containing non-terminal 
symbols for features. Our experiments show 
that these features are effective for ambiguity 
problems. 
We suppose that MNP pre-processing before 
statistical model can significantly simplify the 
analysis of complex sentences, which will have 
more satisfatory results compared with using 
statistical model singly. The current results 
show that the MNP pre-processing does 
simplify the complex sentences. However, the 
performance of MNP recognition and the 
parsing of MNP need to be improved, which 
will be our next work. 
References 
Yoshimasa Tsuruoka, Jun?ichi Tsujii, Sophia 
Anaiakou. 2009. Fast Full Parsing by Linear-
Chain Conditional Random Fields. In 
Proceedings of EACL?09, pages 790-798.  
Xiao chen, Changning Huang, Mu li, Chunyu Kit. 
2009. Better Parser Combination. In CIPS-
ParsEval-2009, pages 81-90. 
Abney, S.. 1991. Parsing by chunks, Principle-Based 
Parsing, Kluwer Academic Publishers. 
Erik Tjong, Kim Sang. 2000. Transforming a 
chunker to a parser. In J.Veenstra W.daelemans, 
K Sima? an and J. Zavrek, editors, Computational 
Linguistics in the Netherlands 2000, Rodopi, page 
177-188.  
P.L. Shiuan, C.T.H. Ann. 1996. A Divided-and-
Conquer Strategy for Parsing. In Proc. of the 
ACL/SIGPARSE 5th International Workshop on 
Parsing Technologies. Santa Cruz, USA, 1996, 
pages 57-66 
C. Braun, G. Neumann, J, Piskorski. 2000. A Divide-
and-Conquer Strategy for Shallow Parsing of 
German Free Texts. In Proc. of ANLP-2000. 
Seattle, Washington, 2000, pages 239-246. 
C.Lyon, B.Dickerson. 1997. Reducing the 
Complexity of Parsing by a Method of 
Decomposition International Workshop on 
Parsing Technology, 1997, pages 215-222. 
Qiaoli Zhou, Xin Liu, Xiaona Ren, Wenjing Lang, 
Dongfeng Cai. 2009. Statistical parsing based on 
Maximal Noun Phrase pre-processing. In CIPS-
ParsEval-2209. 
P.L. Shiuan, C.T.H. Ann. A Divide-and-Conquer 
Strategy for Parsing. In: Proc. of the 
ACL/SIGPARSE 5th International Workshop on 
Parsing Technologies. Santa Cruz, USA, 1996. 
57-66. 
C. Braun, G. Neumann, J. Piskorski. A Divide-and-
Conquer Strategy for Shallow Parsing of German 
Free Texts. In: Proc. of ANLP-2000. Seattle, 
Washington, 2000. 239-246. 
C. Lyon, B. Dickerson. Reducing the Complexity of 
Parsing by a Method of Decomposition. 
International Workshop on Parsing Technology. 
1997. 215-222. 
