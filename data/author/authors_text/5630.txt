Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 724?731, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Shortest Path Dependency Kernel for Relation Extraction
Razvan C. Bunescu and Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan,mooney@cs.utexas.edu
Abstract
We present a novel approach to relation
extraction, based on the observation that
the information required to assert a rela-
tionship between two named entities in
the same sentence is typically captured
by the shortest path between the two en-
tities in the dependency graph. Exper-
iments on extracting top-level relations
from the ACE (Automated Content Ex-
traction) newspaper corpus show that the
new shortest path dependency kernel out-
performs a recent approach based on de-
pendency tree kernels.
1 Introduction
One of the key tasks in natural language process-
ing is that of Information Extraction (IE), which is
traditionally divided into three subproblems: coref-
erence resolution, named entity recognition, and
relation extraction. Consequently, IE corpora are
typically annotated with information corresponding
to these subtasks (MUC (Grishman, 1995), ACE
(NIST, 2000)), facilitating the development of sys-
tems that target only one or a subset of the three
problems. In this paper we focus exclusively on ex-
tracting relations between predefined types of en-
tities in the ACE corpus. Reliably extracting re-
lations between entities in natural-language docu-
ments is still a difficult, unsolved problem, whose
inherent difficulty is compounded by the emergence
of new application domains, with new types of nar-
rative that challenge systems developed for previous
well-studied domains. The accuracy level of cur-
rent syntactic and semantic parsers on natural lan-
guage text from different domains limit the extent
to which syntactic and semantic information can be
used in real IE systems. Nevertheless, various lines
of work on relation extraction have shown experi-
mentally that the use of automatically derived syn-
tactic information can lead to significant improve-
ments in extraction accuracy. The amount of syntac-
tic knowledge used in IE systems varies from part-
of-speech only (Ray and Craven, 2001) to chunking
(Ray and Craven, 2001) to shallow parse trees (Ze-
lenko et al, 2003) to dependency trees derived from
full parse trees (Culotta and Sorensen, 2004). Even
though exhaustive experiments comparing the per-
formance of a relation extraction system based on
these four levels of syntactic information are yet to
be conducted, a reasonable assumption is that the ex-
traction accuracy increases with the amount of syn-
tactic information used. The performance however
depends not only on the amount of syntactic infor-
mation, but also on the details of the exact models
using this information. Training a machine learn-
ing system in a setting where the information used
for representing the examples is only partially rele-
vant to the actual task often leads to overfitting. It is
therefore important to design the IE system so that
the input data is stripped of unnecessary features as
much as possible. In the case of the tree kernels
from (Zelenko et al, 2003; Culotta and Sorensen,
2004), the authors reduce each relation example to
the smallest subtree in the parse or dependency tree
that includes both entities. We will show in this
paper that increased extraction performance can be
724
obtained by designing a kernel method that uses an
even smaller part of the dependency structure ? the
shortest path between the two entities in the undi-
rected version of the dependency graph.
2 Dependency Graphs
Let e1 and e2 be two entities mentioned in the same
sentence such that they are observed to be in a re-
lationship R, i.e R(e1, e2) = 1. For example, R
can specify that entity e1 is LOCATED (AT) entity
e2. Figure 1 shows two sample sentences from ACE,
with entity mentions in bold. Correspondingly, the
first column in Table 1 lists the four relations of type
LOCATED that need to be extracted by the IE sys-
tem. We assume that a relation is to be extracted
only between entities mentioned in the same sen-
tence, and that the presence or absence of a relation
is independent of the text preceding or following the
sentence. This means that only information derived
from the sentence including the two entities will be
relevant for relation extraction. Furthermore, with
each sentence we associate its dependency graph,
with words figured as nodes and word-word depen-
dencies figured as directed edges, as shown in Fig-
ure 1. A subset of these word-word dependencies
capture the predicate-argument relations present in
the sentence. Arguments are connected to their tar-
get predicates either directly through an arc point-
ing to the predicate (?troops ? raided?), or indirectly
through a preposition or infinitive particle (?warning
? to ? stop?). Other types of word-word dependen-
cies account for modifier-head relationships present
in adjective-noun compounds (?several ? stations?),
noun-noun compounds (?pumping ? stations?), or
adverb-verb constructions (?recently ? raided?). In
Figure 1 we show the full dependency graphs for two
sentences from the ACE newspaper corpus.
Word-word dependencies are typically catego-
rized in two classes as follows:
? [Local Dependencies] These correspond to lo-
cal predicate-argument (or head-modifier) con-
structions such as ?troops ? raided?, or ?pump-
ing ? stations? in Figure 1.
? [Non-local Dependencies] Long-distance de-
pendencies arise due to various linguistic con-
structions such as coordination, extraction,
raising and control. In Figure 1, among non-
local dependencies are ?troops ? warning?, or
?ministers ? preaching?.
A Context Free Grammar (CFG) parser can be
used to extract local dependencies, which for each
sentence form a dependency tree. Mildly context
sensitive formalisms such as Combinatory Catego-
rial Grammar (CCG) (Steedman, 2000) model word-
word dependencies more directly and can be used to
extract both local and long-distance dependencies,
giving rise to a directed acyclic graph, as illustrated
in Figure 1.
3 The Shortest Path Hypothesis
If e1 and e2 are two entities mentioned in the same
sentence such that they are observed to be in a rela-
tionship R, our hypothesis stipulates that the con-
tribution of the sentence dependency graph to es-
tablishing the relationship R(e1, e2) is almost exclu-
sively concentrated in the shortest path between e1
and e2 in the undirected version of the dependency
graph.
If entities e1 and e2 are arguments of the same
predicate, then the shortest path between them will
pass through the predicate, which may be con-
nected directly to the two entities, or indirectly
through prepositions. If e1 and e2 belong to different
predicate-argument structures that share a common
argument, then the shortest path will pass through
this argument. This is the case with the shortest path
between ?stations? and ?workers? in Figure 1, pass-
ing through ?protesters?, which is an argument com-
mon to both predicates ?holding? and ?seized?. In
Table 1 we show the paths corresponding to the four
relation instances encoded in the ACE corpus for the
two sentences from Figure 1. All these paths sup-
port the LOCATED relationship. For the first path, it
is reasonable to infer that if a PERSON entity (e.g.
?protesters?) is doing some action (e.g. ?seized?) to
a FACILITY entity (e.g. ?station?), then the PERSON
entity is LOCATED at that FACILITY entity. The sec-
ond path captures the fact that the same PERSON
entity (e.g. ?protesters?) is doing two actions (e.g.
?holding? and ?seized?) , one action to a PERSON en-
tity (e.g. ?workers?), and the other action to a FACIL-
ITY entity (e.g. ?station?). A reasonable inference in
this case is that the ?workers? are LOCATED at the
725
S1 =
=S2
Protesters stations workers
Troops churches ministers
seized   several   pumping , holding   127   Shell hostage .
recently   have   raided , warning to   stop   preaching .
Figure 1: Sentences as dependency graphs.
Relation Instance Shortest Path in Undirected Dependency Graph
S1: protesters AT stations protesters ?? seized ?? stations
S1: workers AT stations workers ?? holding ?? protesters ?? seized ?? stations
S2: troops AT churches troops ?? raided ?? churches
S2: ministers AT churches ministers ?? warning ?? troops ?? raided ?? churches
Table 1: Shortest Path representation of relations.
?station?.
In Figure 2 we show three more examples of the
LOCATED (AT) relationship as dependency paths
created from one or two predicate-argument struc-
tures. The second example is an interesting case,
as it illustrates how annotation decisions are accom-
modated in our approach. Using a reasoning similar
with that from the previous paragraph, it is reason-
able to infer that ?troops? are LOCATED in ?vans?,
and that ?vans? are LOCATED in ?city?. However,
because ?vans? is not an ACE markable, it cannot
participate in an annotated relationship. Therefore,
?troops? is annotated as being LOCATED in ?city?,
which makes sense due to the transitivity of the rela-
tion LOCATED. In our approach, this leads to short-
est paths that pass through two or more predicate-
argument structures.
The last relation example is a case where there ex-
ist multiple shortest paths in the dependency graph
between the same two entities ? there are actually
two different paths, with each path replicated into
three similar paths due to coordination. Our current
approach considers only one of the shortest paths,
nevertheless it seems reasonable to investigate using
all of them as multiple sources of evidence for rela-
tion extraction.
There may be cases where e1 and e2 belong
to predicate-argument structures that have no argu-
ment in common. However, because the depen-
dency graph is always connected, we are guaran-
teed to find a shortest path between the two enti-
ties. In general, we shall find a shortest sequence of
predicate-argument structures with target predicates
P1, P2, ..., Pn such that e1 is an argument of P1, e2 is
an argument of Pn, and any two consecutive predi-
cates Pi and Pi+1 share a common argument (where
by ?argument? we mean both arguments and com-
plements).
4 Learning with Dependency Paths
The shortest path between two entities in a depen-
dency graph offers a very condensed representation
of the information needed to assess their relation-
ship. A dependency path is represented as a se-
quence of words interspersed with arrows that in-
726
(1) He had no regrets for his actions in Brcko.
his? actions? in? Brcko
(2) U.S. troops today acted for the first time to capture an
alleged Bosnian war criminal, rushing from unmarked vans
parked in the northern Serb-dominated city of Bijeljina.
troops? rushing? from? vans? parked? in? city
(3) Jelisic created an atmosphere of terror at the camp by
killing, abusing and threatening the detainees.
detainees? killing? Jelisic? created? at? camp
detainees? abusing? Jelisic? created? at? camp
detainees? threatning? Jelisic? created? at? camp
detainees? killing? by? created? at? camp
detainees? abusing? by? created? at? camp
detainees? threatening? by? created? at? camp
Figure 2: Relation examples.
dicate the orientation of each dependency, as illus-
trated in Table 1. These paths however are com-
pletely lexicalized and consequently their perfor-
mance will be limited by data sparsity. We can al-
leviate this by categorizing words into classes with
varying degrees of generality, and then allowing
paths to use both words and their classes. Examples
of word classes are part-of-speech (POS) tags and
generalizations over POS tags such as Noun, Active
Verb or Passive Verb. The entity type is also used for
the two ends of the dependency path. Other poten-
tially useful classes might be created by associating
with each noun or verb a set of hypernyms corre-
sponding to their synsets in WordNet.
The set of features can then be defined as a
Cartesian product over these word classes, as illus-
trated in Figure 3 for the dependency path between
?protesters? and ?station? in sentence S1. In this rep-
resentation, sparse or contiguous subsequences of
nodes along the lexicalized dependency path (i.e.
path fragments) are included as features simply by
replacing the rest of the nodes with their correspond-
ing generalizations.
The total number of features generated by this de-
pendency path is 4?1?3?1?4, and some of them
are listed in Table 2.
?
?
?
protesters
NNS
Noun
PERSON
?
?
?
? [?]?
[
seized
VBD
Verb
]
? [?]?
?
?
?
stations
NNS
Noun
FACILITY
?
?
?
Figure 3: Feature generation from dependency path.
protesters ? seized ? stations
Noun ? Verb ? Noun
PERSON ? seized ? FACILITY
PERSON ? Verb ? FACILITY
... (48 features)
Table 2: Sample Features.
For verbs and nouns (and their respective word
classes) occurring along a dependency path we also
use an additional suffix ?(-)? to indicate a negative
polarity item. In the case of verbs, this suffix is used
when the verb (or an attached auxiliary) is modi-
fied by a negative polarity adverb such as ?not? or
?never?. Nouns get the negative suffix whenever
they are modified by negative determiners such as
?no?, ?neither? or ?nor?. For example, the phrase ?He
never went to Paris? is associated with the depen-
dency path ?He ? went(-) ? to ? Paris?.
Explicitly creating for each relation example a
vector with a position for each dependency path fea-
ture is infeasible, due to the high dimensionality of
the feature space. Here we can exploit dual learn-
ing algorithms that process examples only via com-
puting their dot-products, such as the Support Vec-
tor Machines (SVMs) (Vapnik, 1998; Cristianini
and Shawe-Taylor, 2000). These dot-products be-
tween feature vectors can be efficiently computed
through a kernel function, without iterating over all
the corresponding features. Given the kernel func-
tion, the SVM learner tries to find a hyperplane that
separates positive from negative examples and at the
same time maximizes the separation (margin) be-
tween them. This type of max-margin separator has
been shown both theoretically and empirically to re-
sist overfitting and to provide good generalization
performance on unseen examples.
Computing the dot-product (i.e. kernel) between
two relation examples amounts to calculating the
727
number of common features of the type illustrated
in Table 2. If x = x1x2...xm and y = y1y2...yn are
two relation examples, where xi denotes the set of
word classes corresponding to position i (as in Fig-
ure 3), then the number of common features between
x and y is computed as in Equation 1.
K(x, y) =
{
0, m 6= n
?n
i=1 c(xi, yi), m = n
(1)
where c(xi, yi) = |xi?yi| is the number of common
word classes between xi and yi.
This is a simple kernel, whose computation takes
O(n) time. If the two paths have different lengths,
they correspond to different ways of expressing a re-
lationship ? for instance, they may pass through a
different number of predicate argument structures.
Consequently, the kernel is defined to be 0 in this
case. Otherwise, it is the product of the number of
common word classes at each position in the two
paths. As an example, let us consider two instances
of the LOCATED relationship:
1. ?his actions in Brcko?, and
2. ?his arrival in Beijing?.
Their corresponding dependency paths are:
1. ?his ? actions ? in ? Brcko?, and
2. ?his ? arrival ? in ? Beijing?.
Their representation as a sequence of sets of word
classes is given by:
1. x = [x1 x2 x3 x4 x5 x6 x7], where x1 =
{his, PRP, PERSON}, x2 = {?}, x3 = {actions,
NNS, Noun}, x4 = {?}, x5 = {in, IN}, x6 =
{?}, x7 = {Brcko, NNP, Noun, LOCATION}
2. y = [y1 y2 y3 y4 y5 y6 y7], where y1 = {his,
PRP, PERSON}, y2 = {?}, y3 = {arrival, NN,
Noun}, y4 = {?}, y5 = {in, IN}, y6 = {?}, y7
= {Beijing, NNP, Noun, LOCATION}
Based on the formula from Equation 1, the kernel is
computed as K(x, y) = 3?1?1?1?2?1?3 = 18.
We use this relation kernel in conjunction with
SVMs in order to find decision hyperplanes that best
separate positive examples from negative examples.
We modified the LibSVM1 package for SVM learn-
ing by plugging in the kernel described above, and
used its default one-against-one implementation for
multiclass classification.
5 Experimental Evaluation
We applied the shortest path dependency kernel to
the problem of extracting top-level relations from
the ACE corpus (NIST, 2000), the version used
for the September 2002 evaluation. The training
part of this dataset consists of 422 documents, with
a separate set of 97 documents allocated for test-
ing. This version of the ACE corpus contains three
types of annotations: coreference, named entities
and relations. Entities can be of the type PERSON,
ORGANIZATION, FACILITY, LOCATION, and GEO-
POLITICAL ENTITY. There are 5 general, top-level
relations: ROLE, PART, LOCATED, NEAR, and SO-
CIAL. The ROLE relation links people to an organi-
zation to which they belong, own, founded, or pro-
vide some service. The PART relation indicates sub-
set relationships, such as a state to a nation, or a sub-
sidiary to its parent company. The AT relation indi-
cates the location of a person or organization at some
location. The NEAR relation indicates the proxim-
ity of one location to another. The SOCIAL rela-
tion links two people in personal, familial or profes-
sional relationships. Each top-level relation type is
further subdivided into more fine-grained subtypes,
resulting in a total of 24 relation types. For exam-
ple, the LOCATED relation includes subtypes such
as LOCATED-AT, BASED-IN, and RESIDENCE. In
total, there are 7,646 intra-sentential relations, of
which 6,156 are in the training data and 1,490 in the
test data.
We assume that the entities and their labels are
known. All preprocessing steps ? sentence segmen-
tation, tokenization, and POS tagging ? were per-
formed using the OpenNLP2 package.
5.1 Extracting dependencies using a CCG
parser
CCG (Steedman, 2000) is a type-driven theory of
grammar where most language-specific aspects of
the grammar are specified into lexicon. To each lex-
1URL:http://www.csie.ntu.edu.tw/?cjlin/libsvm/
2URL: http://opennlp.sourceforge.net
728
ical item corresponds a set of syntactic categories
specifying its valency and the directionality of its
arguments. For example, the words from the sen-
tence ?protesters seized several stations? are mapped
in the lexicon to the following categories:
protesters : NP
seized : (S\NP )/NP
several : NP/NP
stations : NP
The transitive verb ?seized? expects two arguments:
a noun phrase to the right (the object) and another
noun phrase to the left (the subject). Similarly, the
adjective ?several? expects a noun phrase to its right.
Depending on whether its valency is greater than
zero or not, a syntactic category is called a functor
or an argument. In the example above, ?seized? and
?several? are functors, while ?protesters? and ?sta-
tions? are arguments.
Syntactic categories are combined using a small
set of typed combinatory rules such as functional ap-
plication, composition and type raising. In Table 3
we show a sample derivation based on three func-
tional applications.
protesters seized several stations
NP (S\NP )/NP NP/NP NP
NP (S\NP )/NP NP
NP S\NP
S
Table 3: Sample derivation.
In order to obtain CCG derivations for all sen-
tences in the ACE corpus, we used the CCG
parser introduced in (Hockenmaier and Steedman,
2002)3. This parser also outputs a list of dependen-
cies, with each dependency represented as a 4-tuple
?f, a, wf , wa?, where f is the syntactic category of
the functor, a is the argument number, wf is the head
word of the functor, and wa is the head word of the
argument. For example, the three functional appli-
cations from Table 3 result in the functor-argument
dependencies enumerated below in Table 4.
3URL:http://www.ircs.upenn.edu/?juliahr/Parser/
f a wf wa
NP/NP 1 ?several? ?stations?
(S\NP )/NP 2 ?seized? ?stations?
(S\NP )/NP 1 ?seized? ?protesters?
Table 4: Sample dependencies.
Because predicates (e.g. ?seized?) and adjuncts
(e.g. ?several?) are always represented as func-
tors, while complements (e.g. ?protesters? and ?sta-
tions?) are always represented as arguments, it is
straightforward to transform a functor-argument de-
pendency into a head-modifier dependency. The
head-modifier dependencies corresponding to the
three functor-argument dependencies in Table 4 are:
?protesters ? seized?, ?stations ? seized?, and ?sev-
eral ? stations?.
Special syntactic categories are assigned in CCG
to lexical items that project unbounded dependen-
cies, such as the relative pronouns ?who?, ?which?
and ?that?. Coupled with a head-passing mechanism,
these categories allow the extraction of long-range
dependencies. Together with the local word-word
dependencies, they create a directed acyclic depen-
dency graph for each parsed sentence, as shown in
Figure 1.
5.2 Extracting dependencies using a CFG
parser
Local dependencies can be extracted from a CFG
parse tree using simple heuristic rules for finding
the head child for each type of constituent. Alter-
natively, head-modifier dependencies can be directly
output by a parser whose model is based on lexical
dependencies. In our experiments, we used the full
parse output from Collins? parser (Collins, 1997), in
which every non-terminal node is already annotated
with head information. Because local dependencies
assemble into a tree for each sentence, there is only
one (shortest) path between any two entities in a de-
pendency tree.
5.3 Experimental Results
A recent approach to extracting relations is de-
scribed in (Culotta and Sorensen, 2004). The au-
thors use a generalized version of the tree kernel
from (Zelenko et al, 2003) to compute a kernel over
729
relation examples, where a relation example consists
of the smallest dependency tree containing the two
entities of the relation. Precision and recall values
are reported for the task of extracting the 5 top-level
relations in the ACE corpus under two different sce-
narios:
? [S1] This is the classic setting: one multi-class
SVM is learned to discriminate among the 5 top-
level classes, plus one more class for the no-relation
cases.
? [S2] Because of the highly skewed data distribu-
tion, the recall of the SVM approach in the first sce-
nario is very low. In (Culotta and Sorensen, 2004)
the authors propose doing relation extraction in two
steps: first, one binary SVM is trained for rela-
tion detection, which means that all positive rela-
tion instances are combined into one class. Then the
thresholded output of this binary classifier is used as
training data for a second multi-class SVM, which is
trained for relation classification. The same kernel
is used in both stages.
We present in Table 5 the performance of our
shortest path (SP) dependency kernel on the task of
relation extraction from ACE, where the dependen-
cies are extracted using either a CCG parser (SP-
CCG), or a CFG parser (SP-CFG). We also show
the results presented in (Culotta and Sorensen, 2004)
for their best performing kernel K4 (a sum between
a bag-of-words kernel and their dependency kernel)
under both scenarios.
Method Precision Recall F-measure
(S1) SP-CCG 67.5 37.2 48.0
(S1) SP-CFG 71.1 39.2 50.5
(S1) K4 70.3 26.3 38.0
(S2) SP-CCG 63.7 41.4 50.2
(S2) SP-CFG 65.5 43.8 52.5
(S2) K4 67.1 35.0 45.8
Table 5: Extraction Performance on ACE.
The shortest-path dependency kernels outperform
the dependency kernel from (Culotta and Sorensen,
2004) in both scenarios, with a more significant dif-
ference for SP-CFG. An error analysis revealed that
Collins? parser was better at capturing local depen-
dencies, hence the increased accuracy of SP-CFG.
Another advantage of our shortest-path dependency
kernels is that their training and testing are very fast
? this is due to representing the sentence as a chain
of dependencies on which a fast kernel can be com-
puted. All the four SP kernels from Table 5 take
between 2 and 3 hours to train and test on a 2.6GHz
Pentium IV machine.
To avoid numerical problems, we constrained the
dependency paths to pass through at most 10 words
(as observed in the training data) by setting the ker-
nel to 0 for longer paths. We also tried the alterna-
tive solution of normalizing the kernel, however this
led to a slight decrease in accuracy. Having longer
paths give larger kernel scores in the unnormalized
version does not pose a problem because, by defi-
nition, paths of different lengths correspond to dis-
joint sets of features. Consequently, the SVM algo-
rithm will induce lower weights for features occur-
ring in longer paths, resulting in a linear separator
that works irrespective of the size of the dependency
paths.
6 Related Work
In (Zelenko et al, 2003), the authors do relation
extraction using a tree kernel defined over shallow
parse tree representations of sentences. The same
tree kernel is slightly generalized in (Culotta and
Sorensen, 2004) and used in conjunction with de-
pendency trees. In both approaches, a relation in-
stance is defined to be the smallest subtree in the
parse or dependency tree that includes both entities.
In this paper we argued that the information relevant
to relation extraction is almost entirely concentrated
in the shortest path in the dependency tree, leading to
an even smaller representation. Another difference
between the tree kernels above and our new kernel
is that the tree kernels used for relation extraction
are opaque i.e. the semantics of the dimensions in
the corresponding Hilbert space is not obvious. For
the shortest-path kernels, the semantics is known by
definition: each path feature corresponds to a dimen-
sion in the Hilbert space. This transparency allows
us to easily restrict the types of patterns counted by
the kernel to types that we deem relevant for relation
extraction. The tree kernels are also more time con-
suming, especially in the sparse setting, where they
count sparse subsequences of children common to
nodes in the two trees. In (Zelenko et al, 2003), the
730
tree kernel is computed in O(mn) time, where m
and n are the number of nodes in the two trees. This
changes to O(mn3) in the sparse setting.
Our shortest-path intuition bears some similar-
ity with the underlying assumption of the relational
pathfinding algorithm from (Richards and Mooney,
1992) : ?in most relational domains, important con-
cepts will be represented by a small number of fixed
paths among the constants defining a positive in-
stance ? for example, the grandparent relation is de-
fined by a single fixed path consisting of two parent
relations.? We can see this happening also in the task
of relation extraction from ACE, where ?important
concepts? are the 5 types of relations, and the ?con-
stants? defining a positive instance are the 5 types of
entities.
7 Future Work
Local and non-local (deep) dependencies are equally
important for finding relations. In this paper we tried
extracting both types of dependencies using a CCG
parser, however another approach is to recover deep
dependencies from syntactic parses, as in (Camp-
bell, 2004; Levy and Manning, 2004). This may
have the advantage of preserving the quality of lo-
cal dependencies while completing the representa-
tion with non-local dependencies.
Currently, the method assumes that the named en-
tities are known. A natural extension is to automati-
cally extract both the entities and their relationships.
Recent research (Roth and Yih, 2004) indicates that
integrating entity recognition with relation extrac-
tion in a global model that captures the mutual influ-
ences between the two tasks can lead to significant
improvements in accuracy.
8 Conclusion
We have presented a new kernel for relation extrac-
tion based on the shortest-path between the two rela-
tion entities in the dependency graph. Comparative
experiments on extracting top-level relations from
the ACE corpus show significant improvements over
a recent dependency tree kernel.
9 Acknowledgements
This work was supported by grants IIS-0117308 and
IIS-0325116 from the NSF.
References
Richard Campbell. 2004. Using linguistic principles to recover
empty categories. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics (ACL-
04), pages 645?652, Barcelona, Spain, July.
Michael J. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the 35th An-
nual Meeting of the Association for Computational Linguis-
tics (ACL-97), pages 16?23.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-04), Barcelona, Spain, July.
Ralph Grishman. 1995. Message Understanding Conference 6.
http://cs.nyu.edu/cs/faculty/grishman/muc6.html.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with combinatory categorial
grammar. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-2002),
pages 335?342, Philadelphia, PA.
Roger Levy and Christopher Manning. 2004. Deep dependen-
cies from context-free statistical parsers: Correcting the sur-
face dependency approximation. In Proceedings of the 42nd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-04), pages 327?334, Barcelona, Spain, July.
NIST. 2000. ACE ? Automatic Content Extraction.
http://www.nist.gov/speech/tests/ace.
Soumya Ray and Mark Craven. 2001. Representing sentence
structure in hidden Markov models for information extrac-
tion. In Proceedings of the Seventeenth International Joint
Conference on Artificial Intelligence (IJCAI-2001), pages
1273?1279, Seattle, WA.
Bradley L. Richards and Raymond J. Mooney. 1992. Learning
relations by pathfinding. In Proceedings of the Tenth Na-
tional Conference on Artificial Intelligence (AAAI-92), pages
50?55, San Jose, CA, July.
D. Roth and W. Yih. 2004. A linear programming formula-
tion for global inference in natural language tasks. In Pro-
ceedings of the Annual Conference on Computational Natu-
ral Language Learning (CoNLL), pages 1?8, Boston, MA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. John
Wiley & Sons.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning
Research, 3:1083?1106.
731
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 439?446,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning for Semantic Parsing with Statistical Machine Translation
Yuk Wah Wong and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
{ywwong,mooney}@cs.utexas.edu
Abstract
We present a novel statistical approach to
semantic parsing, WASP, for construct-
ing a complete, formal meaning represen-
tation of a sentence. A semantic parser
is learned given a set of sentences anno-
tated with their correct meaning represen-
tations. The main innovation of WASP
is its use of state-of-the-art statistical ma-
chine translation techniques. A word
alignment model is used for lexical acqui-
sition, and the parsing model itself can be
seen as a syntax-based translation model.
We show that WASP performs favorably
in terms of both accuracy and coverage
compared to existing learning methods re-
quiring similar amount of supervision, and
shows better robustness to variations in
task complexity and word order.
1 Introduction
Recent work on natural language understanding has
mainly focused on shallow semantic analysis, such
as semantic role labeling and word-sense disam-
biguation. This paper considers a more ambi-
tious task of semantic parsing, which is the con-
struction of a complete, formal, symbolic, mean-
ing representation (MR) of a sentence. Seman-
tic parsing has found its way in practical applica-
tions such as natural-language (NL) interfaces to
databases (Androutsopoulos et al, 1995) and ad-
vice taking (Kuhlmann et al, 2004). Figure 1 shows
a sample MR written in a meaning-representation
language (MRL) called CLANG, which is used for
((bowner our {4})
(do our {6} (pos (left (half our)))))
If our player 4 has the ball, then our player 6 should
stay in the left side of our half.
Figure 1: A meaning representation in CLANG
encoding coach advice given to simulated soccer-
playing agents (Kuhlmann et al, 2004).
Prior research in semantic parsing has mainly fo-
cused on relatively simple domains such as ATIS
(Air Travel Information Service) (Miller et al, 1996;
Papineni et al, 1997; Macherey et al, 2001), in
which a typcial MR is only a single semantic frame.
Learning methods have been devised that can gen-
erate MRs with a complex, nested structure (cf.
Figure 1). However, these methods are mostly
based on deterministic parsing (Zelle and Mooney,
1996; Kate et al, 2005), which lack the robustness
that characterizes recent advances in statistical NLP.
Other learning methods involve the use of fully-
annotated augmented parse trees (Ge and Mooney,
2005) or prior knowledge of the NL syntax (Zettle-
moyer and Collins, 2005) in training, and hence re-
quire extensive human efforts when porting to a new
domain or language.
In this paper, we present a novel statistical ap-
proach to semantic parsing which can handle MRs
with a nested structure, based on previous work on
semantic parsing using transformation rules (Kate et
al., 2005). The algorithm learns a semantic parser
given a set of NL sentences annotated with their
correct MRs. It requires no prior knowledge of
the NL syntax, although it assumes that an unam-
biguous, context-free grammar (CFG) of the target
MRL is available. The main innovation of this al-
439
answer(count(city(loc 2(countryid(usa)))))
How many cities are there in the US?
Figure 2: A meaning representation in GEOQUERY
gorithm is its integration with state-of-the-art statis-
tical machine translation techniques. More specif-
ically, a statistical word alignment model (Brown
et al, 1993) is used to acquire a bilingual lexi-
con consisting of NL substrings coupled with their
translations in the target MRL. Complete MRs are
then formed by combining these NL substrings and
their translations under a parsing framework called
the synchronous CFG (Aho and Ullman, 1972),
which forms the basis of most existing statisti-
cal syntax-based translation models (Yamada and
Knight, 2001; Chiang, 2005). Our algorithm is
called WASP, short for Word Alignment-based Se-
mantic Parsing. In initial evaluation on several
real-world data sets, we show that WASP performs
favorably in terms of both accuracy and coverage
compared to existing learning methods requiring the
same amount of supervision, and shows better ro-
bustness to variations in task complexity and word
order.
Section 2 provides a brief overview of the do-
mains being considered. In Section 3, we present
the semantic parsing model of WASP. Section 4 out-
lines the algorithm for acquiring a bilingual lexicon
through the use of word alignments. Section 5 de-
scribes a probabilistic model for semantic parsing.
Finally, we report on experiments that show the ro-
bustness of WASP in Section 6, followed by the con-
clusion in Section 7.
2 Application Domains
In this paper, we consider two domains. The first do-
main is ROBOCUP. ROBOCUP (www.robocup.org)
is an AI research initiative using robotic soccer as its
primary domain. In the ROBOCUP Coach Competi-
tion, teams of agents compete on a simulated soccer
field and receive coach advice written in a formal
language called CLANG (Chen et al, 2003). Fig-
ure 1 shows a sample MR in CLANG.
The second domain is GEOQUERY, where a func-
tional, variable-free query language is used for
querying a small database on U.S. geography (Zelle
and Mooney, 1996; Kate et al, 2005). Figure 2
shows a sample query in this language. Note that
both domains involve the use of MRs with a com-
plex, nested structure.
3 The Semantic Parsing Model
To describe the semantic parsing model of WASP,
it is best to start with an example. Consider the
task of translating the sentence in Figure 1 into its
MR in CLANG. To achieve this task, we may first
analyze the syntactic structure of the sentence us-
ing a semantic grammar (Allen, 1995), whose non-
terminals are the ones in the CLANG grammar. The
meaning of the sentence is then obtained by com-
bining the meanings of its sub-parts according to
the semantic parse. Figure 3(a) shows a possible
partial semantic parse of the sample sentence based
on CLANG non-terminals (UNUM stands for uni-
form number). Figure 3(b) shows the corresponding
CLANG parse from which the MR is constructed.
This process can be formalized as an instance of
synchronous parsing (Aho and Ullman, 1972), orig-
inally developed as a theory of compilers in which
syntax analysis and code generation are combined
into a single phase. Synchronous parsing has seen a
surge of interest recently in the machine translation
community as a way of formalizing syntax-based
translation models (Melamed, 2004; Chiang, 2005).
According to this theory, a semantic parser defines a
translation, a set of pairs of strings in which each
pair is an NL sentence coupled with its MR. To
finitely specify a potentially infinite translation, we
use a synchronous context-free grammar (SCFG) for
generating the pairs in a translation. Analogous to
an ordinary CFG, each SCFG rule consists of a sin-
gle non-terminal on the left-hand side (LHS). The
right-hand side (RHS) of an SCFG rule is a pair of
strings, ??, ??, where the non-terminals in ? are a
permutation of the non-terminals in ?. Below are
some SCFG rules that can be used for generating the
parse trees in Figure 3:
RULE ? ?if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 )?
CONDITION ? ?TEAM 1 player UNUM 2 has the ball ,
(bowner TEAM 1 {UNUM 2 })?
TEAM ? ?our , our?
UNUM ? ?4 , 4?
440
RULE
If CONDITION
TEAM
our
player UNUM
4
has the ball
...
(a) English
RULE
( CONDITION
(bowner TEAM
our
{ UNUM
4
})
...)
(b) CLANG
Figure 3: Partial parse trees for the CLANG statement and its English gloss shown in Figure 1
Each SCFG rule X ? ??, ?? is a combination of a
production of the NL semantic grammar, X ? ?,
and a production of the MRL grammar, X ? ?.
Each rule corresponds to a transformation rule in
Kate et al (2005). Following their terminology,
we call the string ? a pattern, and the string ? a
template. Non-terminals are indexed to show their
association between a pattern and a template. All
derivations start with a pair of associated start sym-
bols, ?S 1 , S 1 ?. Each step of a derivation involves
the rewriting of a pair of associated non-terminals
in both of the NL and MRL streams. Below is a
derivation that would generate the sample sentence
and its MR simultaneously: (Note that RULE is the
start symbol for CLANG)
?RULE 1 , RULE 1 ?
? ?if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 )?
? ?if TEAM 1 player UNUM 2 has the ball, DIR 3 . ,
((bowner TEAM 1 {UNUM 2 }) DIR 3 )?
? ?if our player UNUM 1 has the ball, DIR 2 . ,
((bowner our {UNUM 1 }) DIR 2 )?
? ?if our player 4 has the ball, DIRECTIVE 1 . ,
((bowner our {4}) DIRECTIVE 1 )?
? ...
? ?if our player 4 has the ball, then our player 6
should stay in the left side of our half. ,
((bowner our {4})
(do our {6} (pos (left (half our)))))?
Here the MR string is said to be a translation of the
NL string. Given an input sentence, e, the task of
semantic parsing is to find a derivation that yields
?e, f?, so that f is a translation of e. Since there may
be multiple derivations that yield e (and thus mul-
tiple possible translations of e), a mechanism must
be devised for discriminating the correct derivation
from the incorrect ones.
The semantic parsing model of WASP thus con-
sists of an SCFG, G, and a probabilistic model, pa-
rameterized by ?, that takes a possible derivation, d,
and returns its likelihood of being correct given an
input sentence, e. The output translation, f?, for a
sentence, e, is defined as:
f? = m
(
arg max
d?D(G|e)
Pr?(d|e)
)
(1)
where m(d) is the MR string that a derivation d
yields, and D(G|e) is the set of all possible deriva-
tions of G that yield e. In other words, the output
MR is the yield of the most probable derivation that
yields e in the NL stream.
The learning task is to induce a set of SCFG rules,
which we call a lexicon, and a probabilistic model
for derivations. A lexicon defines the set of deriva-
tions that are possible, so the induction of a proba-
bilistic model first requires a lexicon. Therefore, the
learning task can be separated into two sub-tasks:
(1) the induction of a lexicon, followed by (2) the
induction of a probabilistic model. Both sub-tasks
require a training set, {?ei, fi?}, where each training
example ?ei, fi? is an NL sentence, ei, paired with
its correct MR, fi. Lexical induction also requires
an unambiguous CFG of the MRL. Since there is no
lexicon to begin with, it is not possible to include
correct derivations in the training data. This is un-
like most recent work on syntactic parsing based on
gold-standard treebanks. Therefore, the induction of
a probabilistic model for derivations is done in an
unsupervised manner.
4 Lexical Acquisition
In this section, we focus on lexical learning, which
is done by finding optimal word alignments between
441
RULE ? (CONDITION DIRECTIVE)
TEAM ? our
UNUM ? 4
If
our
player
4
has
the
ball
CONDITION ? (bowner TEAM {UNUM})
Figure 4: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1
NL sentences and their MRs in the training set. By
defining a mapping of words from one language to
another, word alignments define a bilingual lexicon.
Using word alignments to induce a lexicon is not a
new idea (Och and Ney, 2003). Indeed, attempts
have been made to directly apply machine transla-
tion systems to the problem of semantic parsing (Pa-
pineni et al, 1997; Macherey et al, 2001). However,
these systems make no use of the MRL grammar,
thus allocating probability mass to MR translations
that are not even syntactically well-formed. Here we
present a lexical induction algorithm that guarantees
syntactic well-formedness of MR translations by us-
ing the MRL grammar.
The basic idea is to train a statistical word align-
ment model on the training set, and then form a
lexicon by extracting transformation rules from the
K = 10 most probable word alignments between
the training sentences and their MRs. While NL
words could be directly aligned with MR tokens,
this is a bad approach for two reasons. First, not all
MR tokens carry specific meanings. For example, in
CLANG, parentheses and braces are delimiters that
are semantically vacuous. Such tokens are not sup-
posed to be aligned with any words, and inclusion of
these tokens in the training data is likely to confuse
the word alignment model. Second, MR tokens may
exhibit polysemy. For instance, the CLANG pred-
icate pt has three meanings based on the types of
arguments it is given: it specifies the xy-coordinates
(e.g. (pt 0 0)), the current position of the ball (i.e.
(pt ball)), or the current position of a player (e.g.
(pt our 4)). Judging from the pt token alone, the
word alignment model would not be able to identify
its exact meaning.
A simple, principled way to avoid these diffi-
culties is to represent an MR using a sequence of
productions used to generate it. Specifically, the
sequence corresponds to the top-down, left-most
derivation of an MR. Figure 4 shows a partial word
alignment between the sample sentence and the lin-
earized parse of its MR. Here the second produc-
tion, CONDITION ? (bowner TEAM {UNUM}), is
the one that rewrites the CONDITION non-terminal
in the first production, RULE ? (CONDITION DI-
RECTIVE), and so on. Note that the structure of a
parse tree is preserved through linearization, and for
each MR there is a unique linearized parse, since the
MRL grammar is unambiguous. Such alignments
can be obtained through the use of any off-the-shelf
word alignment model. In this work, we use the
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 5 (Brown et al, 1993).
Assuming that each NL word is linked to at most
one MRL production, transformation rules are ex-
tracted in a bottom-up manner. The process starts
with productions whose RHS is all terminals, e.g.
TEAM ? our and UNUM ? 4. For each of these
productions, X ? ?, a rule X ? ??, ?? is ex-
tracted such that ? consists of the words to which
the production is linked, e.g. TEAM ? ?our, our?,
UNUM ? ?4, 4?. Then we consider productions
whose RHS contains non-terminals, i.e. predicates
with arguments. In this case, an extracted pattern
consists of the words to which the production is
linked, as well as non-terminals showing where the
arguments are realized. For example, for the bowner
predicate, the extracted rule would be CONDITION
? ?TEAM 1 player UNUM 2 has (1) ball, (bowner
TEAM 1 {UNUM 2 })?, where (1) denotes a word
gap of size 1, due to the unaligned word the that
comes between has and ball. A word gap, (g), can
be seen as a non-terminal that expands to at most
g words in the NL stream, which allows for some
flexibility in pattern matching. Rule extraction thus
proceeds backward from the end of a linearized MR
442
our
left
penalty
area
REGION ? (left REGION)
REGION ? (penalty-area TEAM)
TEAM ? our
Figure 5: A word alignment from which no rules can be extracted for the penalty-area predicate
parse (so that a predicate is processed only after its
arguments have all been processed), until rules are
extracted for all productions.
There are two cases where the above algorithm
would not extract any rules for a production r. First
is when no descendants of r in the MR parse are
linked to any words. Second is when there is a
link from a word w, covered by the pattern for r,
to a production r? outside the sub-parse rooted at
r. Rule extraction is forbidden in this case be-
cause it would destroy the link between w and r?.
The first case arises when a component of an MR
is not realized, e.g. assumed in context. The sec-
ond case arises when a predicate and its arguments
are not realized close enough. Figure 5 shows an
example of this, where no rules can be extracted
for the penalty-area predicate. Both cases can be
solved by merging nodes in the MR parse tree, com-
bining several productions into one. For example,
since no rules can be extracted for penalty-area,
it is combined with its parent to form REGION ?
(left (penalty-area TEAM)), for which the pat-
tern TEAM left penalty area is extracted.
The above algorithm is effective only when words
linked to an MR predicate and its arguments stay
close to each other, a property that we call phrasal
coherence. Any links that destroy this property
would lead to excessive node merging, a major cause
of overfitting. Since building a model that strictly
observes phrasal coherence often requires rules that
model the reordering of tree nodes, our goal is to
bootstrap the learning process by using a simpler,
word-based alignment model that produces a gen-
erally coherent alignment, and then remove links
that would cause excessive node merging before rule
extraction takes place. Given an alignment, a, we
count the number of links that would prevent a rule
from being extracted for each production in the MR
parse. Then the total sum for all productions is ob-
tained, denoted by v(a). A greedy procedure is em-
ployed that repeatedly removes a link a ? a that
would maximize v(a) ? v(a\{a}) > 0, until v(a)
cannot be further reduced. A link w ? r is never
removed if the translation probability, Pr(r|w), is
greater than a certain threshold (0.9). To replenish
the removed links, links from the most probable re-
verse alignment, a? (obtained by treating the source
language as target, and vice versa), are added to a, as
long as a remains n-to-1, and v(a) is not increased.
5 Parameter Estimation
Once a lexicon is acquired, the next task is to learn a
probabilistic model for the semantic parser. We pro-
pose a maximum-entropy model that defines a con-
ditional probability distribution over derivations (d)
given the observed NL string (e):
Pr?(d|e) =
1
Z?(e)
exp
?
i
?ifi(d) (2)
where fi is a feature function, and Z?(e) is a nor-
malizing factor. For each rule r in the lexicon there
is a feature function that returns the number of times
r is used in a derivation. Also for each word w there
is a feature function that returns the number of times
w is generated from word gaps. Generation of un-
seen words is modeled using an extra feature whose
value is the total number of words generated from
word gaps. The number of features is quite modest
(less than 3,000 in our experiments). A similar fea-
ture set is used by Zettlemoyer and Collins (2005).
Decoding of the model can be done in cubic time
with respect to sentence length using the Viterbi al-
gorithm. An Earley chart is used for keeping track
of all derivations that are consistent with the in-
put (Stolcke, 1995). The maximum conditional like-
lihood criterion is used for estimating the model pa-
rameters, ?i. A Gaussian prior (?2 = 1) is used for
regularizing the model (Chen and Rosenfeld, 1999).
Since gold-standard derivations are not available in
the training data, correct derivations must be treated
as hidden variables. Here we use a version of im-
443
proved iterative scaling (IIS) coupled with EM (Rie-
zler et al, 2000) for finding an optimal set of param-
eters.1 Unlike the fully-supervised case, the condi-
tional likelihood is not concave with respect to ?,
so the estimation algorithm is sensitive to initial pa-
rameters. To assume as little as possible, ? is initial-
ized to 0. The estimation algorithm requires statis-
tics that depend on all possible derivations for a sen-
tence or a sentence-MR pair. While it is not fea-
sible to enumerate all derivations, a variant of the
Inside-Outside algorithm can be used for efficiently
collecting the required statistics (Miyao and Tsujii,
2002). Following Zettlemoyer and Collins (2005),
only rules that are used in the best parses for the
training set are retained in the final lexicon. All
other rules are discarded. This heuristic, commonly
known as Viterbi approximation, is used to improve
accuracy, assuming that rules used in the best parses
are the most accurate.
6 Experiments
We evaluated WASP in the ROBOCUP and GEO-
QUERY domains (see Section 2). To build a cor-
pus for ROBOCUP, 300 pieces of coach advice were
randomly selected from the log files of the 2003
ROBOCUP Coach Competition, which were manu-
ally translated into English (Kuhlmann et al, 2004).
The average sentence length is 22.52. To build a
corpus for GEOQUERY, 880 English questions were
gathered from various sources, which were manu-
ally translated into the functional GEOQUERY lan-
guage (Tang and Mooney, 2001). The average sen-
tence length is 7.48, much shorter than ROBOCUP.
250 of the queries were also translated into Spanish,
Japanese and Turkish, resulting in a smaller, multi-
lingual data set.
For each domain, there was a minimal set of ini-
tial rules representing knowledge needed for trans-
lating basic domain entities. These rules were al-
ways included in a lexicon. For example, in GEO-
QUERY, the initial rules were: NUM ? ?x, x?, for
all x ? R; CITY ? ?c, cityid(?c?, )?, for all
city names c (e.g. new york); and similar rules for
other types of names (e.g. rivers). Name transla-
tions were provided for the multilingual data set (e.g.
1We also implemented limited-memory BFGS (Nocedal,
1980). Preliminary experiments showed that it typically reduces
training time by more than half with similar accuracy.
CITY ? ?nyuu yooku, cityid(?new york?, )? for
Japanese).
Standard 10-fold cross validation was used in our
experiments. A semantic parser was learned from
the training set. Then the learned parser was used
to translate the test sentences into MRs. Translation
failed when there were constructs that the parser did
not cover. We counted the number of sentences that
were translated into anMR, and the number of trans-
lations that were correct. For ROBOCUP, a trans-
lation was correct if it exactly matched the correct
MR. For GEOQUERY, a translation was correct if it
retrieved the same answer as the correct query. Us-
ing these counts, we measured the performance of
the parser in terms of precision (percentage of trans-
lations that were correct) and recall (percentage of
test sentences that were correctly translated). For
ROBOCUP, it took 47 minutes to learn a parser us-
ing IIS. For GEOQUERY, it took 83 minutes.
Figure 6 shows the performance of WASP com-
pared to four other algorithms: SILT (Kate et al,
2005), COCKTAIL (Tang and Mooney, 2001), SCIS-
SOR (Ge and Mooney, 2005) and Zettlemoyer and
Collins (2005). Experimental results clearly show
the advantage of extra supervision in SCISSOR and
Zettlemoyer and Collins?s parser (see Section 1).
However, WASP performs quite favorably compared
to SILT and COCKTAIL, which use the same train-
ing data. In particular, COCKTAIL, a determinis-
tic shift-reduce parser based on inductive logic pro-
gramming, fails to scale up to the ROBOCUP do-
main where sentences are much longer, and crashes
on larger training sets due to memory overflow.
WASP also outperforms SILT in terms of recall,
where lexical learning is done by a local bottom-up
search, which is much less effective than the word-
alignment-based algorithm in WASP.
Figure 7 shows the performance of WASP on
the multilingual GEOQUERY data set. The lan-
guages being considered differ in terms of word or-
der: Subject-Verb-Object for English and Spanish,
and Subject-Object-Verb for Japanese and Turkish.
WASP?s performance is consistent across these lan-
guages despite some slight differences, most proba-
bly due to factors other than word order (e.g. lower
recall for Turkish due to a much larger vocabulary).
Details can be found in a longer version of this pa-
per (Wong, 2005).
444
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250  300
Pr
ec
is
io
n 
(%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
(a) Precision for ROBOCUP
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250  300
R
ec
al
l (%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
(b) Recall for ROBOCUP
 0
 20
 40
 60
 80
 100
 0  100  200  300  400  500  600  700  800
Pr
ec
is
io
n 
(%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
Zettlemoyer et al (2005)
(c) Precision for GEOQUERY
 0
 20
 40
 60
 80
 100
 0  100  200  300  400  500  600  700  800
R
ec
al
l (%
)
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
Zettlemoyer et al (2005)
(d) Recall for GEOQUERY
Figure 6: Precision and recall learning curves comparing various semantic parsers
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
Pr
ec
is
io
n 
(%
)
Number of training examples
English
Spanish
Japanese
Turkish
(a) Precision for GEOQUERY
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
R
ec
al
l (%
)
Number of training examples
English
Spanish
Japanese
Turkish
(b) Recall for GEOQUERY
Figure 7: Precision and recall learning curves comparing various natural languages
7 Conclusion
We have presented a novel statistical approach to
semantic parsing in which a word-based alignment
model is used for lexical learning, and the parsing
model itself can be seen as a syntax-based trans-
lation model. Our method is like many phrase-
based translation models, which require a simpler,
word-based alignment model for the acquisition of a
phrasal lexicon (Och and Ney, 2003). It is also sim-
ilar to the hierarchical phrase-based model of Chi-
ang (2005), in which hierarchical phrase pairs, es-
sentially SCFG rules, are learned through the use of
a simpler, phrase-based alignment model. Our work
shows that ideas from compiler theory (SCFG) and
machine translation (word alignment models) can be
successfully applied to semantic parsing, a closely-
related task whose goal is to translate a natural lan-
guage into a formal language.
Lexical learning requires word alignments that are
phrasally coherent. We presented a simple greedy
algorithm for removing links that destroy phrasal co-
herence. Although it is shown to be quite effective in
the current domains, it is preferable to have a more
principled way of promoting phrasal coherence. The
problem is that, by treating MRL productions as
atomic units, current word-based alignment models
have no knowledge about the tree structure hidden
in a linearized MR parse. In the future, we would
like to develop a word-based alignment model that
445
is aware of the MRL syntax, so that better lexicons
can be learned.
Acknowledgments
This research was supported by Defense Advanced
Research Projects Agency under grant HR0011-04-
1-0007.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice Hall, Engle-
wood Cliffs, NJ.
J. F. Allen. 1995. Natural Language Understanding (2nd
Ed.). Benjamin/Cummings, Menlo Park, CA.
I. Androutsopoulos, G. D. Ritchie, and P. Thanisch.
1995. Natural language interfaces to databases: An
introduction. Journal of Natural Language Engineer-
ing, 1(1):29?81.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?312, June.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models. Technical re-
port, Carnegie Mellon University, Pittsburgh, PA.
M. Chen et al 2003. Users manual: RoboCup soc-
cer server manual for soccer server version 7.07 and
later. Available at http://sourceforge.net/
projects/sserver/.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL-05,
pages 263?270, Ann Arbor, MI, June.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of CoNLL-05, pages 9?16, Ann Arbor, MI, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learn-
ing to transform natural to formal languages. In Proc.
of AAAI-05, pages 1062?1068, Pittsburgh, PA, July.
G. Kuhlmann, P. Stone, R. J. Mooney, and J. W. Shavlik.
2004. Guiding a reinforcement learner with natural
language advice: Initial results in RoboCup soccer. In
Proc. of the AAAI-04 Workshop on Supervisory Con-
trol of Learning and Adaptive Systems, San Jose, CA,
July.
K. Macherey, F. J. Och, and H. Ney. 2001. Natural lan-
guage understanding using statistical machine transla-
tion. In Proc. of EuroSpeech-01, pages 2205?2208,
Aalborg, Denmark.
I. D. Melamed. 2004. Statistical machine translation
by parsing. In Proc. of ACL-04, pages 653?660,
Barcelona, Spain.
S. Miller, D. Stallard, R. Bobrow, and R. Schwartz. 1996.
A fully statistical approach to natural language inter-
faces. In Proc. of ACL-96, pages 55?61, Santa Cruz,
CA.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estima-
tion for feature forests. In Proc. of HLT-02, San Diego,
CA, March.
J. Nocedal. 1980. Updating quasi-Newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782, July.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In Proc. of
EuroSpeech-97, pages 1435?1438, Rhodes, Greece.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000.
Lexicalized stochastic modeling of constraint-based
grammars using log-linear measures and EM training.
In Proc. of ACL-00, pages 480?487, Hong Kong.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165?201.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In Proc. of ECML-01, pages 466?
477, Freiburg, Germany.
Y. W. Wong. 2005. Learning for semantic parsing us-
ing statistical machine translation techniques. Techni-
cal Report UT-AI-05-323, Artificial Intelligence Lab,
University of Texas at Austin, Austin, TX, October.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of ACL-01, pages
523?530, Toulouse, France.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proc. of AAAI-96, pages 1050?1055, Portland, OR,
August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Proc.
of UAI-05, Edinburgh, Scotland, July.
446
Proceedings of NAACL HLT 2007, pages 172?179,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Generation by Inverting a Semantic Parser That Uses
Statistical Machine Translation
Yuk Wah Wong and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
{ywwong,mooney}@cs.utexas.edu
Abstract
This paper explores the use of statisti-
cal machine translation (SMT) methods
for tactical natural language generation.
We present results on using phrase-based
SMT for learning to map meaning repre-
sentations to natural language. Improved
results are obtained by inverting a seman-
tic parser that uses SMT methods to map
sentences into meaning representations.
Finally, we show that hybridizing these
two approaches results in still more accu-
rate generation systems. Automatic and
human evaluation of generated sentences
are presented across two domains and four
languages.
1 Introduction
This paper explores the use of statistical machine
translation (SMT) methods in natural language gen-
eration (NLG), specifically the task of mapping
statements in a formal meaning representation lan-
guage (MRL) into a natural language (NL), i.e. tacti-
cal generation. Given a corpus of NL sentences each
paired with a formal meaning representation (MR),
it is easy to use SMT to construct a tactical gener-
ator, i.e. a statistical model that translates MRL to
NL. However, there has been little, if any, research
on exploiting recent SMT methods for NLG.
In this paper we present results on using a re-
cent phrase-based SMT system, PHARAOH (Koehn
et al, 2003), for NLG.1 Although moderately effec-
1We also tried IBM Model 4/REWRITE (Germann, 2003), a
word-based SMT system, but it gave much worse results.
tive, the inability of PHARAOH to exploit the for-
mal structure and grammar of the MRL limits its ac-
curacy. Unlike natural languages, MRLs typically
have a simple, formal syntax to support effective au-
tomated processing and inference. This MRL struc-
ture can also be used to improve language genera-
tion.
Tactical generation can also be seen as the inverse
of semantic parsing, the task of mapping NL sen-
tences to MRs. In this paper, we show how to ?in-
vert? a recent SMT-based semantic parser, WASP
(Wong and Mooney, 2006), in order to produce a
more effective generation system. WASP exploits
the formal syntax of the MRL by learning a trans-
lator (based on a statistical synchronous context-
free grammar) that maps an NL sentence to a lin-
earized parse-tree of its MR rather than to a flat MR
string. In addition to exploiting the formal MRL
grammar, our approach also allows the same learned
grammar to be used for both parsing and genera-
tion, an elegant property that has been widely ad-
vocated (Kay, 1975; Jacobs, 1985; Shieber, 1988).
We present experimental results in two domains pre-
viously used to test WASP?s semantic parsing abil-
ity: mapping NL queries to a formal database query
language, and mapping NL soccer coaching instruc-
tions to a formal robot command language. WASP?1
is shown to produce a more accurate NL generator
than PHARAOH.
We also show how the idea of generating from
linearized parse-trees rather than flat MRs, used
effectively in WASP?1, can also be exploited in
PHARAOH. A version of PHARAOH that exploits
this approach is experimentally shown to produce
more accurate generators that are more competi-
tive with WASP?1?s. Finally, we also show how
172
((bowner our {4})
(do our {6} (pos (left (half our)))))
If our player 4 has the ball, then our player 6
should stay in the left side of our half.
(a) CLANG
answer(state(traverse 1(riverid(?ohio?))))
What states does the Ohio run through?
(b) GEOQUERY
Figure 1: Sample meaning representations
aspects of PHARAOH?s phrase-based model can be
used to improve WASP?1, resulting in a hybrid sys-
tem whose overall performance is the best.
2 MRLs and Test Domains
In this work, we consider input MRs with a hi-
erarchical structure similar to Moore (2002). The
only restriction on the MRL is that it be defined
by an available unambiguous context-free grammar
(CFG), which is true for almost all computer lan-
guages. We also assume that the order in which MR
predicates appear is relevant, i.e. the order can affect
the meaning of the MR. Note that the order in which
predicates appear need not be the same as the word
order of the target NL, and therefore, the content
planner need not know about the target NL grammar
(Shieber, 1993).
To ground our discussion, we consider two ap-
plication domains which were originally used to
demonstrate semantic parsing. The first domain is
ROBOCUP. In the ROBOCUP Coach Competition
(www.robocup.org), teams of agents compete in a
simulated soccer game and receive coach advice
written in a formal language called CLANG (Chen
et al, 2003). The task is to build a system that trans-
lates this formal advice into English. Figure 1(a)
shows a piece of sample advice.
The second domain is GEOQUERY, where a func-
tional, variable-free query language is used for
querying a small database on U.S. geography (Kate
et al, 2005). The task is to translate formal queries
into NL. Figure 1(b) shows a sample query.
3 Generation using SMT Methods
In this section, we show how SMT methods can be
used to construct a tactical generator. This is in con-
trast to existing work that focuses on the use of NLG
in interlingual MT (Whitelock, 1992), in which the
roles of NLG and MT are switched. We first con-
sider using a phrase-based SMT system, PHARAOH,
for NLG. Then we show how to invert an SMT-based
semantic parser, WASP, to produce a more effective
generation system.
3.1 Generation using PHARAOH
PHARAOH (Koehn et al, 2003) is an SMT system
that uses phrases as basic translation units. Dur-
ing decoding, the source sentence is segmented into
a sequence of phrases. These phrases are then re-
ordered and translated into phrases in the target lan-
guage, which are joined together to form the output
sentence. Compared to earlier word-based methods
such as IBM Models (Brown et al, 1993), phrase-
based methods such as PHARAOH are much more
effective in producing idiomatic translations, and
are currently the best performing methods in SMT
(Koehn and Monz, 2006).
To use PHARAOH for NLG, we simply treat the
source MRL as an NL, so that phrases in the MRL
are sequences of MR tokens. Note that the grammat-
icality of MRs is not an issue here, as they are given
as input.
3.2 WASP: The Semantic Parsing Algorithm
Before showing how generation can be performed
by inverting a semantic parser, we present a brief
overview of WASP (Wong and Mooney, 2006), the
SMT-based semantic parser on which this work is
based.
To describe WASP, it is best to start with an ex-
ample. Consider the task of translating the English
sentence in Figure 1(a) into CLANG. To do this,
we may first generate a parse tree of the input sen-
tence. The meaning of the sentence is then ob-
tained by combining the meanings of the phrases.
This process can be formalized using a synchronous
context-free grammar (SCFG), originally developed
as a grammar formalism that combines syntax anal-
ysis and code generation in compilers (Aho and Ull-
man, 1972). It has been used in syntax-based SMT
to model the translation of one NL to another (Chi-
ang, 2005). A derivation for a SCFG gives rise to
multiple isomorphic parse trees. Figure 2 shows a
partial parse of the sample sentence and its corre-
173
RULE
If CONDITION
TEAM
our
player UNUM
4
has the ball
...
(a) English
RULE
( CONDITION
(bowner TEAM
our
{ UNUM
4
})
...)
(b) CLANG
Figure 2: Partial parse trees for the CLANG statement and its English gloss shown in Figure 1(a)
sponding CLANG parse from which an MR is con-
structed. Note that the two parse trees are isomor-
phic (ignoring terminals).
Each SCFG rule consists of a non-terminal, X ,
on the left-hand side (LHS), and a pair of strings,
??, ??, on the right-hand side (RHS). The non-
terminals in ? are a permutation of the non-terminals
in ? (indices are used to show their correspondence).
In WASP, ? denotes an NL phrase, and X ? ? is
a production of the MRL grammar. Below are the
SCFG rules that generate the parses in Figure 2:
RULE ? ?if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 )?
CONDITION ? ?TEAM 1 player UNUM 2 has the
ball , (bowner TEAM 1 {UNUM 2 })?
TEAM ? ?our , our?
UNUM ? ?4 , 4?
All derivations start with a pair of co-indexed start
symbols of the MRL grammar, ?S 1 , S 1 ?, and each
step involves the rewriting of a pair of co-indexed
non-terminals (by ? and ?, respectively). Given an
input sentence, e, the task of semantic parsing is to
find a derivation that yields ?e, f?, so that f is an MR
translation of e.
Parsing with WASP requires a set of SCFG rules.
These rules are learned using a word alignment
model, which finds an optimal mapping from words
to MR predicates given a set of training sentences
and their correct MRs. Word alignment models have
been widely used for lexical acquisition in SMT
(Brown et al, 1993; Koehn et al, 2003). To use
a word alignment model in the semantic parsing
scenario, we can treat the MRL simply as an NL,
and MR tokens as words, but this often leads to
poor results. First, not all MR tokens carry spe-
cific meanings. For example, in CLANG, parenthe-
ses and braces are delimiters that are semantically
vacuous. Such tokens can easily confuse the word
alignment model. Second, MR tokens may exhibit
polysemy. For example, the CLANG predicate pt
has three meanings based on the types of arguments
it is given (Chen et al, 2003). Judging from the pt
token alone, the word alignment model would not be
able to identify its exact meaning.
A simple, principled way to avoid these difficul-
ties is to represent an MR using a list of productions
used to generate it. This list is used in lieu of the
MR in a word alignment. Figure 3 shows an exam-
ple. Here the list of productions corresponds to the
top-down, left-most derivation of an MR. For each
MR there is a unique linearized parse-tree, since
the MRL grammar is unambiguous. Note that the
structure of the parse tree is preserved through lin-
earization. This allows us to extract SCFG rules in a
bottom-up manner, assuming the alignment is n-to-1
(each word is linked to at most one production). Ex-
traction starts with productions whose RHS is all ter-
minals, followed by those with non-terminals. (De-
tails can be found in Wong and Mooney (2006).)
The rules extracted from Figure 3 would be almost
the same as those used in Figure 2, except the one for
bowner: CONDITION ? ?TEAM 1 player UNUM 2
has (1) ball, (bowner TEAM 1 {UNUM 2 })?. The
token (1) denotes a word gap of size 1, due to the un-
aligned word the that comes between has and ball.
It can be seen as a non-terminal that expands to at
most one word, allowing for some flexibility in pat-
tern matching.
In WASP, GIZA++ (Och and Ney, 2003) is used
to obtain the best alignments from the training ex-
amples. Then SCFG rules are extracted from these
alignments. The resulting SCFG, however, can be
174
RULE ? (CONDITION DIRECTIVE)
TEAM ? our
UNUM ? 4
If
our
player
4
has
the
ball
CONDITION ? (bowner TEAM {UNUM})
Figure 3: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1(a)
ambiguous. Therefore, a maximum-entropy model
that defines the conditional probability of deriva-
tions (d) given an input sentence (e) is used for dis-
ambiguation:
Pr?(d|e) =
1
Z?(e)
exp
?
i
?ifi(d) (1)
The feature functions, fi, are the number of times
each rule is used in a derivation. Z?(e) is the
normalizing factor. The model parameters, ?i, are
trained using L-BFGS (Nocedal, 1980) to maxi-
mize the conditional log-likelihood of the training
examples (with a Gaussian prior). The decoding
task is thus to find a derivation d? that maximizes
Pr?(d?|e), and the output MR translation, f?, is the
yield of d?. This can be done in cubic time with re-
spect to the length of e using an Earley chart parser.
3.3 Generation by Inverting WASP
Now we show how to invert WASP to produce
WASP?1, and use it for NLG. We can use the same
grammar for both parsing and generation, a partic-
ularly appealing aspect of using WASP. Since an
SCFG is fully symmetric with respect to both gen-
erated strings, the same chart used for parsing can
be easily adapted for efficient generation (Shieber,
1988; Kay, 1996).
Given an input MR, f , WASP?1 finds a sentence
e that maximizes Pr(e|f). It is difficult to directly
model Pr(e|f), however, because it has to assign
low probabilities to output sentences that are not
grammatical. There is no such requirement for pars-
ing, because the use of the MRL grammar ensures
the grammaticality of all output MRs. For genera-
tion, we need an NL grammar to ensure grammati-
cality, but this is not available a priori.
This motivates the noisy-channel model for
WASP?1, where Pr(e|f) is divided into two smaller
components:
arg max
e
Pr(e|f) = arg max
e
Pr(e) Pr(f |e) (2)
Pr(e) is the language model, and Pr(f |e) is the
parsing model. The generation task is to find a sen-
tence e such that (1) e is a good sentence a priori,
and (2) its meaning is the same as the input MR. For
the language model, we use an n-grammodel, which
is remarkably useful in ranking candidate generated
sentences (Knight and Hatzivassiloglou, 1995; Ban-
galore et al, 2000; Langkilde-Geary, 2002). For the
parsing model, we re-use the one from WASP (Equa-
tion 1). Hence computing (2) means maximizing the
following:
max
e
Pr(e) Pr(f |e)
? max
d?D(f)
Pr(e(d)) Pr?(d|e(d))
= max
d?D(f)
Pr(e(d)) ? exp?i ?ifi(d)
Z?(e(d))
(3)
where D(f) is the set of derivations that are con-
sistent with f , and e(d) is the output sentence that
a derivation d yields. Compared to most exist-
ing work on generation, WASP?1 has the following
characteristics:
1. It does not require any lexical information in
the input MR, so lexical selection is an integral
part of the decoding algorithm.
2. Each predicate is translated to a phrase. More-
over, it need not be a contiguous phrase (con-
sider the SCFG rule for bowner in Section 3.2).
For decoding, we use an Earley chart generator
that scans the input MR from left to right. This im-
plies that each chart item covers a certain substring
of the input MR, not a subsequence in general. It
175
requires the order in which MR predicates appear
to be fixed, i.e. the order determines the meaning
of the MR. Since the order need not be identical to
the word order of the target NL, there is no need for
the content planner to know the target NL grammar,
which is learned from the training data.
Overall, the noisy-channel model is a weighted
SCFG, obtained by intersecting the NL side of the
WASP SCFG with the n-gram language model. The
chart generator is very similar to the chart parser, ex-
cept for the following:
1. To facilitate the calculation of Pr(e(d)), chart
items now include a list of (n?1)-grams that encode
the context in which output NL phrases appear. The
size of the list is 2N + 2, where N is the number of
non-terminals to be rewritten in the dotted rule.
2. Words are generated from word gaps through
special rules (g) ? ??, ??, where the word gap,
(g), is treated as a non-terminal, and ? is the NL
string that fills the gap (|?| ? g). The empty set
symbol indicates that the NL string does not carry
any meaning. There are similar constructs in Car-
roll et al (1999) that generate function words. Fur-
thermore, to improve efficiency, our generator only
considers gap fillers that have been observed during
training.
3. The normalizing factor in (3), Z?(e(d)), is not
a constant and varies across the output string, e(d).
(Note that Z?(e) is fixed for parsing.) This is un-
fortunate because the calculation of Z?(e(d)) is ex-
pensive, and it is not easy to incorporate it into the
chart generation algorithm. Normalization is done
as follows. First, compute the k-best candidate out-
put strings based on the unnormalized version of (3),
Pr(e(d)) ? exp?i ?ifi(d). Then re-rank the list by
normalizing the scores using Z?(e(d)), which is ob-
tained by running the inside-outside algorithm on
each output string. This results in a decoding al-
gorithm that is approximate?the best output string
might not be in the k-best list?and takes cubic time
with respect to the length of each of the k candidate
output strings (k = 100 in our experiments).
Learning in WASP?1 involves two steps. First, a
back-off n-gram language model with Good-Turing
discounting and no lexical classes2 is built from all
2This is to ensure that the same language model is used in
all systems that we tested.
training sentences using the SRILM Toolkit (Stolcke,
2002). We use n = 2 since higher values seemed to
cause overfitting in our domains. Next, the parsing
model is trained as described in Section 3.2.
4 Improving the SMT-based Generators
The SMT-based generation algorithms, PHARAOH
and WASP?1, while reasonably effective, can be
substantially improved by borrowing ideas from
each other.
4.1 Improving the PHARAOH-based Generator
A major weakness of PHARAOH as an NLG sys-
tem is its inability to exploit the formal structure of
the MRL. Like WASP?1, the phrase extraction al-
gorithm of PHARAOH is based on the output of a
word alignment model such as GIZA++ (Koehn et
al., 2003), which performs poorly when applied di-
rectly to MRLs (Section 3.2).
We can improve the PHARAOH-based generator
by supplying linearized parse-trees as input rather
than flat MRs. As a result, the basic translation units
are sequences of MRL productions, rather than se-
quences of MR tokens. This way PHARAOH can
exploit the formal grammar of the MRL to produce
high-quality phrase pairs. The same idea is used in
WASP?1 to produce high-quality SCFG rules. We
call the resulting hybrid NLG system PHARAOH++.
4.2 Improving the WASP-based Generator
There are several aspects of PHARAOH that can be
used to improve WASP?1. First, the probabilistic
model of WASP?1 is less than ideal as it requires
an extra re-ranking step for normalization, which is
expensive and prone to over-pruning. To remedy this
situation, we can borrow the probabilistic model of
PHARAOH, and define the parsing model as:
Pr(d|e(d)) =
?
d?d
w(r(d)) (4)
which is the product of the weights of the rules used
in a derivation d. The rule weight, w(X ? ??, ??),
is in turn defined as:
P (?|?)?1P (?|?)?2Pw(?|?)?3Pw(?|?)?4 exp(?|?|)?5
where P (?|?) and P (?|?) are the relative frequen-
cies of ? and ?, and Pw(?|?) and Pw(?|?) are
176
the lexical weights (Koehn et al, 2003). The word
penalty, exp(?|?|), allows some control over the
output sentence length. Together with the language
model, the new formulation of Pr(e|f) is a log-
linear model with ?i as parameters. The advantage
of this model is that maximization requires no nor-
malization and can be done exactly and efficiently.
The model parameters are trained using minimum
error-rate training (Och, 2003).
Following the phrase extraction phase in
PHARAOH, we eliminate word gaps by incorpo-
rating unaligned words as part of the extracted
NL phrases (Koehn et al, 2003). The reason is
that while word gaps are useful in dealing with
unknown phrases during semantic parsing, for
generation, using known phrases generally leads to
better fluency. For the same reason, we also allow
the extraction of longer phrases that correspond to
multiple predicates (but no more than 5).
We call the resulting hybrid system WASP?1++.
It is similar to the syntax-based SMT system of Chi-
ang (2005), which uses both SCFG and PHARAOH?s
probabilistic model. The main difference is that we
use the MRL grammar to constrain rule extraction,
so that significantly fewer rules are extracted, mak-
ing it possible to do exact inference.
5 Experiments
We evaluated all four SMT-based NLG systems in-
troduced in this paper: PHARAOH, WASP?1, and the
hybrid systems, PHARAOH++ and WASP?1++.
We used the ROBOCUP and GEOQUERY corpora
in our experiments. The ROBOCUP corpus consists
of 300 pieces of coach advice taken from the log files
of the 2003 ROBOCUP Coach Competition. The ad-
vice was written in CLANG and manually translated
to English (Kuhlmann et al, 2004). The average
MR length is 29.47 tokens, or 12.82 nodes for lin-
earized parse-trees. The average sentence length is
22.52. The GEOQUERY corpus consists of 880 En-
glish questions gathered from various sources. The
questions were manually translated to the functional
GEOQUERY language (Kate et al, 2005). The av-
erage MR length is 17.55 tokens, or 5.55 nodes for
linearized parse-trees. The average sentence length
is 7.57.
Reference: If our player 2, 3, 7 or 5 has the ball
and the ball is close to our goal line ...
PHARAOH++: If player 3 has the ball is in 2 5 the
ball is in the area near our goal line ...
WASP?1++: If players 2, 3, 7 and 5 has the ball
and the ball is near our goal line ...
Figure 4: Sample partial system output in the
ROBOCUP domain
ROBOCUP GEOQUERY
BLEU NIST BLEU NIST
PHARAOH 0.3247 5.0263 0.2070 3.1478
WASP?1 0.4357 5.4486 0.4582 5.9900
PHARAOH++ 0.4336 5.9185 0.5354 6.3637
WASP?1++ 0.6022 6.8976 0.5370 6.4808
Table 1: Results of automatic evaluation; bold type
indicates the best performing system (or systems)
for a given domain-metric pair (p < 0.05)
5.1 Automatic Evaluation
We performed 4 runs of 10-fold cross validation, and
measured the performance of the learned generators
using the BLEU score (Papineni et al, 2002) and the
NIST score (Doddington, 2002). Both MT metrics
measure the precision of a translation in terms of the
proportion of n-grams that it shares with the refer-
ence translations, with the NIST score focusing more
on n-grams that are less frequent and more informa-
tive. Both metrics have recently been used to eval-
uate generators (Langkilde-Geary, 2002; Nakanishi
et al, 2005; Belz and Reiter, 2006).
All systems were able to generate sentences for
more than 97% of the input. Figure 4 shows some
sample output of the systems. Table 1 shows the
automatic evaluation results. Paired t-tests were
used to measure statistical significance. A few
observations can be made. First, WASP?1 pro-
duced a more accurate generator than PHARAOH.
Second, PHARAOH++ significantly outperformed
PHARAOH, showing the importance of exploiting
the formal structure of the MRL. Third, WASP?1++
significantly outperformed WASP?1. Most of the
gain came from PHARAOH?s probabilistic model.
Decoding was also 4?11 times faster, despite ex-
act inference and a larger grammar due to extrac-
tion of longer phrases. Lastly, WASP?1++ signifi-
cantly outperformed PHARAOH++ in the ROBOCUP
177
ROBOCUP GEOQUERY
Flu. Ade. Flu. Ade.
PHARAOH++ 2.5 2.9 4.3 4.7
WASP?1++ 3.6 4.0 4.1 4.7
Table 2: Results of human evaluation
domain. This is because WASP?1++ allows dis-
contiguous NL phrases and PHARAOH++ does not.
Such phrases are commonly used in ROBOCUP
for constructions like: players 2 , 3 , 7 and 5;
26.96% of the phrases generated during testing were
discontiguous. When faced with such predicates,
PHARAOH++ would consistently omit some of the
words: e.g. players 2 3 7 5, or not learn any phrases
for those predicates at all. On the other hand, only
4.47% of the phrases generated during testing for
GEOQUERY were discontiguous, so the advantage of
WASP?1++ over PHARAOH++ was not as obvious.
Our BLEU scores are not as high as those re-
ported in Langkilde-Geary (2002) and Nakanishi et
al. (2005), which are around 0.7?0.9. However,
their work involves the regeneration of automati-
cally parsed text, and the MRs that they use, which
are essentially dependency parses, contain extensive
lexical information of the target NL.
5.2 Human Evaluation
Automatic evaluation is only an imperfect substitute
for human assessment. While it is found that BLEU
and NIST correlate quite well with human judgments
in evaluating NLG systems (Belz and Reiter, 2006),
it is best to support these figures with human evalu-
ation, which we did on a small scale. We recruited 4
native speakers of English with no previous experi-
ence with the ROBOCUP and GEOQUERY domains.
Each subject was given the same 20 sentences for
each domain, randomly chosen from the test sets.
For each sentence, the subjects were asked to judge
the output of PHARAOH++ and WASP?1++ in terms
of fluency and adequacy. They were presented with
the following definition, adapted from Koehn and
Monz (2006):
Score Fluency Adequacy
5 Flawless English All meaning
4 Good English Most meaning
3 Non-native English Some meaning
PHARAOH++ WASP?1++
BLEU NIST BLEU NIST
English 0.5344 5.3289 0.6035 5.7133
Spanish 0.6042 5.6321 0.6175 5.7293
Japanese 0.6171 4.5357 0.6585 4.6648
Turkish 0.4562 4.2220 0.4824 4.3283
Table 3: Results of automatic evaluation on the mul-
tilingual GEOQUERY data set
Score Fluency Adequacy
2 Disfluent English Little meaning
1 Incomprehensible No meaning
For each generated sentence, we computed the av-
erage of the 4 human judges? scores. No score
normalization was performed. Then we compared
the two systems using a paired t-test. Table 2
shows that WASP?1++ produced better generators
than PHARAOH++ in the ROBOCUP domain, con-
sistent with the results of automatic evaluation.
5.3 Multilingual Experiments
Lastly, we describe our experiments on the mul-
tilingual GEOQUERY data set. The 250-example
data set is a subset of the larger GEOQUERY cor-
pus. All English questions in this data set were
manually translated into Spanish, Japanese and
Turkish, while the corresponding MRs remain un-
changed. Table 3 shows the results, which are sim-
ilar to previous results on the larger GEOQUERY
corpus. WASP?1++ outperformed PHARAOH++
for some language-metric pairs, but otherwise per-
formed comparably.
6 Related Work
Numerous efforts have been made to unify the tasks
of semantic parsing and tactical generation. One of
the earliest espousals of the notion of grammar re-
versability can be found in Kay (1975). Shieber
(1988) further noted that not only a single gram-
mar can be used for parsing and generation, but the
same language-processing architecture can be used
for both tasks. Kay (1996) identified parsing charts
as such an architecture, which led to the develop-
ment of various chart generation algorithms: Car-
roll et al (1999) for HPSG, Bangalore et al (2000)
for LTAG, Moore (2002) for unification grammars,
178
White and Baldridge (2003) for CCG. More re-
cently, statistical chart generators have emerged, in-
cluding White (2004) for CCG, Carroll and Oepen
(2005) and Nakanishi et al (2005) for HPSG. Many
of these systems, however, focus on the task of sur-
face realization?inflecting and ordering words?
which ignores the problem of lexical selection. In
contrast, our SMT-based methods integrate lexical
selection and realization in an elegant framework
and automatically learn all of their linguistic knowl-
edge from an annotated corpus.
7 Conclusion
We have presented four tactical generation systems
based on various SMT-based methods. In particular,
the hybrid system produced by inverting the WASP
semantic parser shows the best overall results across
different application domains.
Acknowledgments
We would like to thank Kevin Knight, Jason
Baldridge, Razvan Bunescu, and the anonymous re-
viewers for their valuable comments. We also sin-
cerely thank the four annotators who helped us eval-
uate the SMT-based generators. This research was
supported by DARPA under grant HR0011-04-1-
0007 and a gift from Google Inc.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation,
and Compiling. Prentice Hall, Englewood Cliffs, NJ.
S. Bangalore, O. Rambow, and S. Whittaker. 2000. Evaluation metrics
for generation. In Proc. INLG-00, pages 1?8, Mitzpe Ramon, Israel,
July.
A. Belz and E. Reiter. 2006. Comparing automatic and human evalu-
ation of NLG systems. In Proc. EACL-06, pages 313?320, Trento,
Italy, April.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?312, June.
J. Carroll and S. Oepen. 2005. High efficiency realization for a wide-
coverage unification grammar. In Proc. IJCNLP-05, pages 165?176,
Jeju Island, Korea, October.
J. Carroll, A. Copestake, D. Flickinger, and V. Poznan?ski. 1999. An
efficient chart generator for (semi-)lexicalist grammars. In Proc.
EWNLG-99, pages 86?95, Toulouse, France.
M. Chen et al 2003. Users manual: RoboCup soccer server man-
ual for soccer server version 7.07 and later. Available at http:
//sourceforge.net/projects/sserver/.
D. Chiang. 2005. A hierarchical phrase-based model for statistical
machine translation. In Proc. ACL-05, pages 263?270, Ann Arbor,
MI, June.
G. Doddington. 2002. Automatic evaluation of machine translation
quality using n-gram co-occurrence statistics. In Proc. ARPA Work-
shop on Human Language Technology, pages 128?132, San Diego,
CA.
U. Germann. 2003. Greedy decoding for statistical machine translation
in almost linear time. In Proc. HLT/NAACL-03, Edmonton, Canada.
P. S. Jacobs. 1985. PHRED: A generator for natural language inter-
faces. Computational Linguistics, 11(4):219?242.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform
natural to formal languages. In Proc. AAAI-05, pages 1062?1068,
Pittsburgh, PA, July.
M. Kay. 1975. Syntactic processing and functional sentence per-
spective. In Theoretical Issues in Natural Language Processing?
Supplement to the Proceedings, pages 12?15, Cambridge, MA,
June.
M. Kay. 1996. Chart generation. In Proc. ACL-96, pages 200?204, San
Francisco, CA.
K. Knight and V. Hatzivassiloglou. 1995. Two-level, many-paths gen-
eration. In Proc. ACL-95, pages 252?260, Cambridge, MA.
P. Koehn and C. Monz. 2006. Manual and automatic evaluation of
machine translation between European languages. In Proc. SMT-06
Workshop, pages 102?121, New York City, NY, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based
translation. In Proc. HLT/NAACL-03, Edmonton, Canada.
G. Kuhlmann, P. Stone, R. J. Mooney, and J. W. Shavlik. 2004. Guiding
a reinforcement learner with natural language advice: Initial results
in RoboCup soccer. In Proc. of the AAAI-04 Workshop on Supervi-
sory Control of Learning and Adaptive Systems, San Jose, CA, July.
I. Langkilde-Geary. 2002. An empirical verification of coverage
and correctness for a general-purpose sentence generator. In Proc.
INLG-02, pages 17?24, Harriman, NY, July.
R. C. Moore. 2002. A complete, efficient sentence-realization algo-
rithm for unification grammar. In Proc. INLG-02, pages 41?48,
Harriman, NY, July.
H. Nakanishi, Y. Miyao, and J. Tsujii. 2005. Probabilistic models for
disambiguation of an HPSG-based chart generator. In Proc. IWPT-
05, pages 93?102, Vancouver, Canada, October.
J. Nocedal. 1980. Updating quasi-Newton matrices with limited stor-
age. Mathematics of Computation, 35(151):773?782, July.
F. J. Och and H. Ney. 2003. A systematic comparison of various statis-
tical alignment models. Computational Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in statistical machine
translation. In Proc. ACL-03, pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In Proc.
ACL-02, pages 311?318, Philadelphia, PA, July.
S. M. Shieber. 1988. A uniform architecture for parsing and generation.
In Proc. COLING-88, pages 614?619, Budapest, Hungary.
S. M. Shieber. 1993. The problem of logical-form equivalence. Com-
putational Linguistics, 19(1):179?190.
A. Stolcke. 2002. SRILM?an extensible language modeling toolkit.
In Proc. ICSLP-02, pages 901?904, Denver, CO.
M. White and J. Baldridge. 2003. Adapting chart realization to CCG.
In Proc. EWNLG-03, Budapest, Hungary, April.
M. White. 2004. Reining in CCG chart realization. In Proc. INLG-04,
New Forest, UK, July.
P. Whitelock. 1992. Shake-and-bake translation. In Proc. COLING-92,
pages 784?791, Nantes, France.
Y. W. Wong and R. J. Mooney. 2006. Learning for semantic parsing
with statistical machine translation. In Proc. HLT/NAACL-06, pages
439?446, New York City, NY, June.
179
Proceedings of NAACL HLT 2007, Companion Volume, pages 81?84,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Learning for Semantic Parsing
using Support Vector Machines
Rohit J. Kate and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
frjkate,mooneyg@cs.utexas.edu
Abstract
We present a method for utilizing unan-
notated sentences to improve a semantic
parser which maps natural language (NL)
sentences into their formal meaning rep-
resentations (MRs). Given NL sentences
annotated with their MRs, the initial su-
pervised semantic parser learns the map-
ping by training Support Vector Machine
(SVM) classifiers for every production in
the MR grammar. Our new method ap-
plies the learned semantic parser to the
unannotated sentences and collects unla-
beled examples which are then used to
retrain the classifiers using a variant of
transductive SVMs. Experimental results
show the improvements obtained over
the purely supervised parser, particularly
when the annotated training set is small.
1 Introduction
Semantic parsing is the task of mapping a natu-
ral language (NL) sentence into a complete, for-
mal meaning representation (MR) which a computer
program can execute to perform some task, like
answering database queries or controlling a robot.
These MRs are expressed in domain-specific unam-
biguous formal meaning representation languages
(MRLs). Given a training corpus of NL sentences
annotated with their correct MRs, the goal of a learn-
ing system for semantic parsing is to induce an ef-
ficient and accurate semantic parser that can map
novel sentences into their correct MRs.
Several learning systems have been developed for
semantic parsing, many of them recently (Zelle and
Mooney, 1996; Zettlemoyer and Collins, 2005; Ge
and Mooney, 2005; Kate and Mooney, 2006). These
systems use supervised learning methods which
only utilize annotated NL sentences. However, it
requires considerable human effort to annotate sen-
tences. In contrast, unannotated NL sentences are
usually easily available. Semi-supervised learning
methods utilize cheaply available unannotated data
during training along with annotated data and of-
ten perform better than purely supervised learning
methods trained on the same amount of annotated
data (Chapelle et al, 2006). In this paper we present,
to our knowledge, the first semi-supervised learning
system for semantic parsing.
We modify KRISP, a supervised learning sys-
tem for semantic parsing presented in (Kate and
Mooney, 2006), to make a semi-supervised system
we call SEMISUP-KRISP. Experiments on a real-
world dataset show the improvements SEMISUP-
KRISP obtains over KRISP by utilizing unannotated
sentences.
2 Background
This section briefly provides background needed for
describing our approach to semi-supervised seman-
tic parsing.
2.1 KRISP: The Supervised Semantic Parsing
Learning System
KRISP (Kernel-based Robust Interpretation for Se-
mantic Parsing) (Kate and Mooney, 2006) is a su-
pervised learning system for semantic parsing which
81
takes NL sentences paired with their MRs as train-
ing data. The productions of the formal MRL
grammar are treated like semantic concepts. For
each of these productions, a Support-Vector Ma-
chine (SVM) (Cristianini and Shawe-Taylor, 2000)
classifier is trained using string similarity as the ker-
nel (Lodhi et al, 2002). Each classifier can then
estimate the probability of any NL substring rep-
resenting the semantic concept for its production.
During semantic parsing, the classifiers are called to
estimate probabilities on different substrings of the
sentence to compositionally build the most probable
meaning representation (MR) of the sentence.
KRISP trains the classifiers used in semantic pars-
ing iteratively. In each iteration, for every produc-
tion  in the MRL grammar, KRISP collects pos-
itive and negative examples. In the first iteration,
the set of positive examples for production  con-
tains all sentences whose corresponding MRs use
the production  in their parse trees. The set of neg-
ative examples includes all of the other training sen-
tences. Using these positive and negative examples,
an SVM classifier is trained for each production 
using a string kernel. In subsequent iterations, the
parser learned from the previous iteration is applied
to the training examples and more refined positive
and negative examples, which are more specific sub-
strings within the sentences, are collected for train-
ing. Iterations are continued until the classifiers con-
verge, analogous to iterations in EM (Dempster et
al., 1977). Experimentally, KRISP compares favor-
ably to other existing semantic parsing systems and
is particularly robust to noisy training data (Kate and
Mooney, 2006).
2.2 Transductive SVMs
SVMs (Cristianini and Shawe-Taylor, 2000) are
state-of-the-art machine learning methods for clas-
sification. Given positive and negative training ex-
amples in some vector space, an SVM finds the
maximum-margin hyperplane which separates them.
Maximizing the margin prevents over-fitting in very
high-dimensional data which is typical in natural
language processing and thus leads to better general-
ization performance on test examples. When the un-
labeled test examples are also available during train-
ing, a transductive framework for learning (Vapnik,
1998) can further improve the performance on the
test examples.
Transductive SVMs were introduced in
(Joachims, 1999). The key idea is to find the
labeling of the test examples that results in the
maximum-margin hyperplane that separates the
positive and negative examples of both the training
and the test data. This is achieved by including
variables in the SVM?s objective function repre-
senting labels of the test examples. Finding the
exact solution to the resulting optimization problem
is intractable, however Joachims (1999) gives an
approximation algorithm for it. One drawback of
his algorithm is that it requires the proportion of
positive and negative examples in the test data be
close to the proportion in the training data, which
may not always hold, particularly when the training
data is small. Chen et al (2003) present another
approximation algorithm which we use in our
system because it does not require this assumption.
More recently, new optimization methods have been
used to scale-up transductive SVMs to large data
sets (Collobert et al, 2006), however we did not
face scaling problems in our current experiments.
Although transductive SVMs were originally de-
signed to improve performance on the test data by
utilizing its availability during training, they can also
be directly used in a semi-supervised setting (Ben-
nett and Demiriz, 1999) where unlabeled data is
available during training that comes from the same
distribution as the test data but is not the actual data
on which the classifier is eventually to be tested.
This framework is more realistic in the context of se-
mantic parsing where sentences must be processed
in real-time and it is not practical to re-train the
parser transductively for every new test sentence. In-
stead of using an alternative semi-supervised SVM
algorithm, we preferred to use a transductive SVM
algorithm (Chen et al, 2003) in a semi-supervised
manner, since it is easily implemented on top of an
existing SVM system.
3 Semi-Supervised Semantic Parsing
We modified the existing supervised system KRISP,
described in section 2.1, to incorporate semi-
supervised learning. Supervised learning in KRISP
involves training SVM classifiers on positive and
negative examples that are substrings of the anno-
82
function TRAIN SEMISUP KRISP(Annotated corpus A = f(s
i
;m
i
)ji = 1::Ng, MRL grammar G,
Unannotated sentences T = ft
i
ji = 1::Mg)
C  fC

j 2 Gg = TRAIN KRISP(A,G) // classifiers obtained by training KRISP
Let
P = fp

= Set of positive examples used in training C

j 2 Gg
N = fn

= Set of negative examples used in training C

j 2 Gg
U = fu

= j 2 Gg // set of unlabeled examples for each production, initially all empty
for i = 1 to M do
fu
i

j 2 Gg =COLLECT CLASSIFIER CALLS(PARSE(t
i
; C))
U = fu

= u

[ u
i

j 2 Gg
for each  2 G do
C

=TRANSDUCTIVE SVM TRAIN(p

; n

; u

) // retrain classifiers utilizing unlabeled examples
return classifiers C = fC

j 2 Gg
Figure 1: SEMISUP-KRISP?s training algorithm
tated sentences. In order to perform semi-supervised
learning, these classifiers need to be given appropri-
ate unlabeled examples. The key question is: Which
substrings of the unannotated sentences should be
given as unlabeled examples to which productions?
classifiers? Giving all substrings of the unannotated
sentences as unlabeled examples to all of the clas-
sifiers would lead to a huge number of unlabeled
examples that would not conform to the underly-
ing distribution of classes each classifier is trying to
separate. SEMISUP-KRISP?s training algorithm, de-
scribed below and shown in Figure 1, addresses this
issue.
The training algorithm first runs KRISP?s exist-
ing training algorithm and obtains SVM classifiers
for every production in the MRL grammar. Sets of
positive and negative examples that were used for
training the classifiers in the last iteration are col-
lected for each production. Next, the learned parser
is applied to the unannotated sentences. During the
parsing of each sentence, whenever a classifier is
called to estimate the probability of a substring rep-
resenting the semantic concept for its production,
that substring is saved as an unlabeled example for
that classifier. These substrings are representative of
the examples that the classifier will actually need to
handle during testing. Note that the MRs obtained
from parsing the unannotated sentences do not play
a role during training since it is unknown whether
or not they are correct. These sets of unlabeled ex-
amples for each production, along with the sets of
positive and negative examples collected earlier, are
then used to retrain the classifiers using transductive
SVMs. The retrained classifiers are finally returned
and used in the final semantic parser.
4 Experiments
We compared the performance of SEMISUP-KRISP
and KRISP in the GEOQUERY domain for semantic
parsing in which the MRL is a functional language
used to query a U.S. geography database (Kate et
al., 2005). This domain has been used in most of
the previous work. The original corpus contains 250
NL queries collected from undergraduate students
and annotated with their correct MRs (Zelle and
Mooney, 1996). Later, 630 additional NL queries
were collected from real users of a web-based inter-
face and annotated (Tang and Mooney, 2001). We
used this data as unannotated sentences in our cur-
rent experiments. We also collected an additional
407 queries from the same interface, making a total
of 1; 037 unannotated sentences.
The systems were evaluated using standard 10-
fold cross validation. All the unannotated sentences
were used for training in each fold. Performance
was measured in terms of precision (the percent-
age of generated MRs that were correct) and recall
(the percentage of all sentences for which correct
MRs were obtained). An output MR is considered
correct if and only if the resulting query retrieves
the same answer as the correct MR when submit-
ted to the database. Since the systems assign confi-
dences to the MRs they generate, the entire range of
the precision-recall trade-off can be obtained for a
system by measuring precision and recall at various
confidence levels. We present learning curves for the
best F-measure (harmonic mean of precision and re-
83
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100  120  140  160  180  200  220  240
Be
st
 F
-m
ea
su
re
No. of annotated training sentences
SEMISUP-KRISP
KRISP
GEOBASE
Figure 2: Learning curves for the best F-measures
on the GEOQUERY corpus.
call) obtained across the precision-recall trade-off as
the amount of annotated training data is increased.
Figure 2 shows the results for both systems.
The results clearly show the improvement
SEMISUP-KRISP obtains over KRISP by utilizing
unannotated sentences, particularly when the num-
ber of annotated sentences is small. We also show
the performance of a hand-built semantic parser
GEOBASE (Borland International, 1988) for com-
parison. From the figure, it can be seen that, on
average, KRISP achieves the same performance as
GEOBASE when it is given 126 annotated examples,
while SEMISUP-KRISP reaches this level given only
94 annotated examples, a 25:4% savings in human-
annotation effort.
5 Conclusions
This paper has presented a semi-supervised ap-
proach to semantic parsing. Our method utilizes
unannotated sentences during training by extracting
unlabeled examples for the SVM classifiers it uses to
perform semantic parsing. These classifiers are then
retrained using transductive SVMs. Experimental
results demonstrated that this exploitation of unla-
beled data significantly improved the accuracy of the
resulting parsers when only limited supervised data
was provided.
Acknowledgments
This research was supported by a Google research
grant. The experiments were run on the Mastodon
cluster provided by NSF grant EIA-0303609.
References
K. Bennett and A. Demiriz. 1999. Semi-supervised support
vector machines. Advances in Neural Information Process-
ing Systems, 11:368?374.
Borland International. 1988. Turbo Prolog 2.0 Reference
Guide. Borland International, Scotts Valley, CA.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006. Semi-
Supervised Learning. MIT Press, Cambridge, MA.
Y. Chen, G. Wang, and S. Dong. 2003. Learning with progres-
sive transductive support vector machine. Pattern Recogni-
tion Letters, 24:1845?1855.
R. Collobert, F. Sinz, J. Weston, and L. Bottou. 2006. Large
scale transductive SVMs. Journal of Machine Learning Re-
search, 7(Aug):1687?1712.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduction to
Support Vector Machines and Other Kernel-based Learning
Methods. Cambridge University Press.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Proc. of CoNLL-05,
pages 9?16, Ann Arbor, MI, July.
T. Joachims. 1999. Transductive inference for text classifica-
tion using support vector machines. In Proc. of ICML-99,
pages 200?209, Bled, Slovenia, June.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels for
learning semantic parsers. In Proc. of COLING/ACL-06,
pages 913?920, Sydney, Australia, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to
transform natural to formal languages. In Proc. of AAAI-05,
pages 1062?1068, Pittsburgh, PA, July.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and
C. Watkins. 2002. Text classification using string kernels.
Journal of Machine Learning Research, 2:419?444.
L. R. Tang and R. J. Mooney. 2001. Using multiple clause con-
structors in inductive logic programming for semantic pars-
ing. In Proc. of ECML-01, pages 466?477, Freiburg, Ger-
many.
V. N. Vapnik. 1998. Statistical Learning Theory. John Wiley
& Sons.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of
AAAI-96, pages 1050?1055, Portland, OR, August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map sen-
tences to logical form: Structured classification with proba-
bilistic categorial grammars. In Proc. of UAI-05, Edinburgh,
Scotland, July.
84
 	
ffProceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913?920,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using String-Kernels for Learning Semantic Parsers
Rohit J. Kate
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
rjkate@cs.utexas.edu
Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
mooney@cs.utexas.edu
Abstract
We present a new approach for mapping
natural language sentences to their for-
mal meaning representations using string-
kernel-based classifiers. Our system learns
these classifiers for every production in the
formal language grammar. Meaning repre-
sentations for novel natural language sen-
tences are obtained by finding the most
probable semantic parse using these string
classifiers. Our experiments on two real-
world data sets show that this approach
compares favorably to other existing sys-
tems and is particularly robust to noise.
1 Introduction
Computational systems that learn to transform nat-
ural language sentences into formal meaning rep-
resentations have important practical applications
in enabling user-friendly natural language com-
munication with computers. However, most of the
research in natural language processing (NLP) has
been focused on lower-level tasks like syntactic
parsing, word-sense disambiguation, information
extraction etc. In this paper, we have considered
the important task of doing deep semantic parsing
to map sentences into their computer-executable
meaning representations.
Previous work on learning semantic parsers
either employ rule-based algorithms (Tang and
Mooney, 2001; Kate et al, 2005), or use sta-
tistical feature-based methods (Ge and Mooney,
2005; Zettlemoyer and Collins, 2005; Wong and
Mooney, 2006). In this paper, we present a
novel kernel-based statistical method for learn-
ing semantic parsers. Kernel methods (Cristianini
and Shawe-Taylor, 2000) are particularly suitable
for semantic parsing because it involves mapping
phrases of natural language (NL) sentences to se-
mantic concepts in a meaning representation lan-
guage (MRL). Given that natural languages are so
flexible, there are various ways in which one can
express the same semantic concept. It is difficult
for rule-based methods or even statistical feature-
based methods to capture the full range of NL con-
texts which map to a semantic concept because
they tend to enumerate these contexts. In contrast,
kernel methods allow a convenient mechanism to
implicitly work with a potentially infinite number
of features which can robustly capture these range
of contexts even when the data is noisy.
Our system, KRISP (Kernel-based Robust In-
terpretation for Semantic Parsing), takes NL sen-
tences paired with their formal meaning represen-
tations as training data. The productions of the for-
mal MRL grammar are treated like semantic con-
cepts. For each of these productions, a Support-
Vector Machine (SVM) (Cristianini and Shawe-
Taylor, 2000) classifier is trained using string sim-
ilarity as the kernel (Lodhi et al, 2002). Each
classifier then estimates the probability of the pro-
duction covering different substrings of the sen-
tence. This information is used to compositionally
build a complete meaning representation (MR) of
the sentence.
Some of the previous work on semantic pars-
ing has focused on fairly simple domains, primar-
ily, ATIS (Air Travel Information Service) (Price,
1990) whose semantic analysis is equivalent to fill-
ing a single semantic frame (Miller et al, 1996;
Popescu et al, 2004). In this paper, we have
tested KRISP on two real-world domains in which
meaning representations are more complex with
richer predicates and nested structures. Our exper-
iments demonstrate that KRISP compares favor-
913
NL: ?If the ball is in our goal area then our player 1 should
intercept it.?
CLANG: ((bpos (goal-area our))
(do our {1} intercept))
Figure 1: An example of an NL advice and its
CLANG MR.
ably to other existing systems and is particularly
robust to noise.
2 Semantic Parsing
We call the process of mapping natural language
(NL) utterances into their computer-executable
meaning representations (MRs) as semantic pars-
ing. These MRs are expressed in formal languages
which we call meaning representation languages
(MRLs). We assume that all MRLs have deter-
ministic context free grammars, which is true for
almost all computer languages. This ensures that
every MR will have a unique parse tree. A learn-
ing system for semantic parsing is given a training
corpus of NL sentences paired with their respec-
tive MRs from which it has to induce a semantic
parser which can map novel NL sentences to their
correct MRs.
Figure 1 shows an example of an NL sentence
and its MR from the CLANG domain. CLANG
(Chen et al, 2003) is the standard formal coach
language in which coaching advice is given to soc-
cer agents which compete on a simulated soccer
field in the RoboCup 1 Coach Competition. In the
MR of the example, bpos stands for ?ball posi-
tion?.
The second domain we have considered is the
GEOQUERY domain (Zelle and Mooney, 1996)
which is a query language for a small database of
about 800 U.S. geographical facts. Figure 2 shows
an NL query and its MR form in a functional query
language. The parse of the functional query lan-
guage is also shown along with the involved pro-
ductions. This example is also used later to illus-
trate how our system does semantic parsing. The
MR in the functional query language can be read
as if processing a list which gets modified by vari-
ous functions. From the innermost expression go-
ing outwards it means: the state of Texas, the list
containing all the states next to the state of Texas
and the list of all the rivers which flow through
these states. This list is finally returned as the an-
swer.
1http://www.robocup.org/
NL: ?Which rivers run through the states bordering Texas??
Functional query language:
answer(traverse(next to(stateid(?texas?))))
Parse tree of the MR in functional query language:
ANSWER
answer RIVER
TRAVERSE
traverse
STATE
NEXT TO
next to
STATE
STATEID
stateid ?texas?
Productions:
ANSWER ? answer(RIVER) RIVER ? TRAVERSE(STATE)
STATE ? NEXT TO(STATE) STATE ? STATEID
TRAVERSE ? traverse NEXT TO ? next to
STATEID ? stateid(?texas?)
Figure 2: An example of an NL query and its MR
in a functional query language with its parse tree.
KRISP does semantic parsing using the notion
of a semantic derivation of an NL sentence. In
the following subsections, we define the seman-
tic derivation of an NL sentence and its probabil-
ity. The task of semantic parsing then is to find
the most probable semantic derivation of an NL
sentence. In section 3, we describe how KRISP
learns the string classifiers that are used to obtain
the probabilities needed in finding the most prob-
able semantic derivation.
2.1 Semantic Derivation
We define a semantic derivation, D, of an NL sen-
tence, s, as a parse tree of an MR (not necessarily
the correct MR) such that each node of the parse
tree also contains a substring of the sentence in
addition to a production. We denote nodes of the
derivation tree by tuples (pi, [i..j]), where pi is its
production and [i..j] stands for the substring s[i..j]
of s (i.e. the substring from the ith word to the jth
word), and we say that the node or its production
covers the substring s[i..j]. The substrings cov-
ered by the children of a node are not allowed to
overlap, and the substring covered by the parent
must be the concatenation of the substrings cov-
ered by its children. Figure 3 shows a semantic
derivation of the NL sentence and the MR parse
which were shown in figure 2. The words are num-
bered according to their position in the sentence.
Instead of non-terminals, productions are shown
in the nodes to emphasize the role of productions
in semantic derivations.
Sometimes, the children of an MR parse tree
914
(ANSWER? answer(RIVER), [1..9])
(RIVER? TRAVERSE(STATE), [1..9])
(TRAVERSE?traverse, [1..4])
which1 rivers2 run3 through4
(STATE? NEXT TO(STATE), [5..9])
(NEXT TO? next to, [5..7])
the5 states6 bordering7
(STATE? STATEID, [8..9])
(STATEID? stateid ?texas?, [8..9])
Texas8 ?9
Figure 3: Semantic derivation of the NL sentence ?Which rivers run through the states bordering Texas??
which gives MR as answer(traverse(next to(stateid(texas)))).
node may not be in the same order as are the sub-
strings of the sentence they should cover in a se-
mantic derivation. For example, if the sentence
was ?Through the states that border Texas which
rivers run??, which has the same MR as the sen-
tence in figure 3, then the order of the children of
the node ?RIVER ? TRAVERSE(STATE)? would
need to be reversed. To accommodate this, a se-
mantic derivation tree is allowed to contain MR
parse tree nodes in which the children have been
permuted.
Note that given a semantic derivation of an NL
sentence, it is trivial to obtain the corresponding
MR simply as the string generated by the parse.
Since children nodes may be permuted, this step
also needs to permute them back to the way they
should be according to the MRL productions. If a
semantic derivation gives the correct MR of the
NL sentence, then we call it a correct semantic
derivation otherwise it is an incorrect semantic
derivation.
2.2 Most Probable Semantic Derivation
Let Ppi(u) denote the probability that a production
pi of the MRL grammar covers the NL substring
u. In other words, the NL substring u expresses
the semantic concept of a production pi with prob-
ability Ppi(u). In the next subsection we will de-
scribe how KRISP obtains these probabilities using
string-kernel based SVM classifiers. Assuming
these probabilities are independent of each other,
the probability of a semantic derivationD of a sen-
tence s is then:
P (D) =
?
(pi,[i..j])?D
Ppi(s[i..j])
The task of the semantic parser is to find the
most probable derivation of a sentence s. This
task can be recursively performed using the no-
tion of a partial derivation En,s[i..j], which stands
for a subtree of a semantic derivation tree with n
as the left-hand-side (LHS) non-terminal of the
root production and which covers s from index
i to j. For example, the subtree rooted at the
node ?(STATE ? NEXT TO(STATE),[5..9])? in
the derivation shown in figure 3 is a partial deriva-
tion which would be denoted as ESTATE,s[5..9].
Note that the derivation D of sentence s is then
simply Estart,s[1..|s|], where start is the start sym-
bol of the MRL?s context free grammar, G.
Our procedure to find the most probable par-
tial derivation E?n,s[i..j] considers all possible sub-
trees whose root production has n as its LHS non-
terminal and which cover s from index i to j.
Mathematically, the most probable partial deriva-
tion E?n,s[i..j] is recursively defined as:
E?n,s[i..j] =
makeTree( argmax
pi = n ? n1..nt ? G,
(p1, .., pt) ?
partition(s[i..j], t)
(Ppi(s[i..j])
?
k=1..t
P (E?nk,pk )))
where partition(s[i..j], t) is a function which re-
turns the set of all partitions of s[i..j] with t el-
ements including their permutations. A parti-
tion of a substring s[i..j] with t elements is a
t?tuple containing t non-overlapping substrings
of s[i..j] which give s[i..j] when concatenated.
For example, (?the states bordering?, ?Texas ??)
is a partition of the substring ?the states bor-
dering Texas ?? with 2 elements. The proce-
duremakeTree(pi, (p1, .., pt)) constructs a partial
derivation tree by making pi as its root production
and making the most probable partial derivation
trees found through the recursion as children sub-
trees which cover the substrings according to the
partition (p1, .., pt).
The most probable partial derivation E?n,s[i..j]
is found using the above equation by trying all
productions pi = n ? n1..nt in G which have
915
n as the LHS, and all partitions with t elements
of the substring s[i..j] (n1 to nt are right-hand-
side (RHS) non-terminals of pi, terminals do not
play any role in this process and are not shown
for simplicity). The most probable partial deriva-
tion E?STATE,s[5..9] for the sentence shown in fig-
ure 3 will be found by trying all the productions
in the grammar with STATE as the LHS, for ex-
ample, one of them being ?STATE ? NEXT TO
STATE?. Then for this sample production, all parti-
tions, (p1, p2), of the substring s[5..9] with two el-
ements will be considered, and the most probable
derivations E?NEXT TO,p1 and E
?
STATE,p2 will be
found recursively. The recursion reaches base
cases when the productions which have n on the
LHS do not have any non-terminal on the RHS or
when the substring s[i..j] becomes smaller than
the length t.
According to the equation, a production pi ? G
and a partition (p1, .., pt) ? partition(s[i..j], t)
will be selected in constructing the most probable
partial derivation. These will be the ones which
maximize the product of the probability of pi cov-
ering the substring s[i..j] with the product of prob-
abilities of all the recursively found most proba-
ble partial derivations consistent with the partition
(p1, .., pt).
A naive implementation of the above recursion
is computationally expensive, but by suitably ex-
tending the well known Earley?s context-free pars-
ing algorithm (Earley, 1970), it can be imple-
mented efficiently. The above task has some re-
semblance to probabilistic context-free grammar
(PCFG) parsing for which efficient algorithms are
available (Stolcke, 1995), but we note that our task
of finding the most probable semantic derivation
differs from PCFG parsing in two important ways.
First, the probability of a production is not inde-
pendent of the sentence but depends on which sub-
string of the sentence it covers, and second, the
leaves of the tree are not individual terminals of
the grammar but are substrings of words of the NL
sentence. The extensions needed for Earley?s al-
gorithm are straightforward and are described in
detail in (Kate, 2005) but due to space limitation
we do not describe them here. Our extended Ear-
ley?s algorithm does a beam search and attempts
to find the ? (a parameter) most probable semantic
derivations of an NL sentence s using the probabil-
ities Ppi(s[i..j]). To make this search faster, it uses
a threshold, ?, to prune low probability derivation
trees.
3 KRISP?s Training Algorithm
In this section, we describe how KRISP learns
the classifiers which give the probabilities Ppi(u)
needed for semantic parsing as described in the
previous section. Given the training corpus of
NL sentences paired with their MRs {(si,mi)|i =
1..N}, KRISP first parses the MRs using the MRL
grammar, G. We represent the parse of MR, mi,
by parse(mi).
Figure 4 shows pseudo-code for KRISP?s train-
ing algorithm. KRISP learns a semantic parser it-
eratively, each iteration improving upon the parser
learned in the previous iteration. In each itera-
tion, for every production pi of G, KRISP collects
positive and negative example sets. In the first
iteration, the set P(pi) of positive examples for
production pi contains all sentences, si, such that
parse(mi) uses the production pi. The set of nega-
tive examples,N (pi), for production pi includes all
of the remaining training sentences. Using these
positive and negative examples, an SVM classi-
fier 2, Cpi, is trained for each production pi using
a normalized string subsequence kernel. Follow-
ing the framework of Lodhi et al (2002), we de-
fine a kernel between two strings as the number of
common subsequences they share. One difference,
however, is that their strings are over characters
while our strings are over words. The more the
two strings share, the greater the similarity score
will be.
Normally, SVM classifiers only predict the class
of the test example but one can obtain class proba-
bility estimates by mapping the distance of the ex-
ample from the SVM?s separating hyperplane to
the range [0,1] using a learned sigmoid function
(Platt, 1999). The classifier Cpi then gives us the
probabilities Ppi(u). We represent the set of these
classifiers by C = {Cpi|pi ? G}.
Next, using these classifiers, the extended
Earley?s algorithm, which we call EX-
TENDED EARLEY in the pseudo-code, is invoked
to obtain the ? best semantic derivations for each
of the training sentences. The procedure getMR
returns the MR for a semantic derivation. At this
point, for many training sentences, the resulting
most-probable semantic derivation may not give
the correct MR. Hence, next, the system collects
more refined positive and negative examples
to improve the result in the next iteration. It
2We use the LIBSVM package available at: http://
www.csie.ntu.edu.tw/?cjlin/libsvm/
916
function TRAIN KRISP(training corpus {(si,mi)|i = 1..N}, MRL grammar G)
for each pi ?G // collect positive and negative examples for the first iteration
for i = 1 to N do
if pi is used in parse(mi) then
include si in P(pi)
else include si in N (pi)
for iteration = 1 to MAX ITR do
for each pi ?G do
Cpi = trainSVM(P(pi),N (pi)) // SVM training
for each pi ?G P(pi) = ? // empty the positive examples, accumulate negatives though
for i = 1 to N do
D =EXTENDED EARLEY(si, G, P ) // obtain best derivations
if 6 ? d ? D such that parse(mi) = getMR(d) then
D = D ? EXTENDED EARLEY CORRECT(si, G, P,mi) // if no correct derivation then force to find one
d? = argmaxd?D&getMR(d)=parse(mi) P (d)
COLLECT POSITIVES(d?) // collect positives from maximum probability correct derivation
for each d ? D do
if P (d) > P (d?) and getMR(d) 6= parse(mi) then
// collect negatives from incorrect derivation with larger probability than the correct one
COLLECT NEGATIVES(d, d?)
return classifiers C = {Cpi|pi ? G}
Figure 4: KRISP?s training algorithm
is also possible that for some sentences, none
of the obtained ? derivations give the correct
MR. But as will be described shortly, the most
probable derivation which gives the correct MR is
needed to collect positive and negative examples
for the next iteration. Hence in these cases, a
version of the extended Earley?s algorithm, EX-
TENDED EARLEY CORRECT, is invoked which
also takes the correct MR as an argument and
returns the best ? derivations it finds, all of
which give the correct MR. This is easily done by
making sure all subtrees derived in the process are
present in the parse of the correct MR.
From these derivations, positive and negative
examples are collected for the next iteration. Pos-
itive examples are collected from the most prob-
able derivation which gives the correct MR, fig-
ure 3 showed an example of a derivation which
gives the correct MR. At each node in such a
derivation, the substring covered is taken as a pos-
itive example for its production. Negative exam-
ples are collected from those derivations whose
probability is higher than the most probable cor-
rect derivation but which do not give the cor-
rect MR. Figure 5 shows an example of an in-
correct derivation. Here the function ?next to?
is missing from the MR it produces. The fol-
lowing procedure is used to collect negative ex-
amples from incorrect derivations. The incorrect
derivation and the most probable correct deriva-
tion are traversed simultaneously starting from the
root using breadth-first traversal. The first nodes
where their productions differ is detected, and all
of the words covered by the these nodes (in both
derivations) are marked. In the correct and incor-
rect derivations shown in figures 3 and 5 respec-
tively, the first nodes where the productions differ
are ?(STATE ? NEXT TO(STATE), [5..9])? and
?(STATE ? STATEID, [8..9])?. Hence, the union
of words covered by them: 5 to 9 (?the states
bordering Texas??), will be marked. For each
of these marked words, the procedure considers
all of the productions which cover it in the two
derivations. The nodes of the productions which
cover a marked word in the incorrect derivation
but not in the correct derivation are used to col-
lect negative examples. In the example, the node
?(TRAVERSE?traverse,[1..7])? will be used
to collect a negative example (i.e. the words 1
to 7 ??which rivers run through the states border-
ing? will be a negative example for the produc-
tion TRAVERSE?traverse) because the pro-
duction covers the marked words ?the?, ?states?
and ?bordering? in the incorrect derivation but
not in the correct derivation. With this as a neg-
ative example, hopefully in the next iteration, the
probability of this derivation will decrease signif-
icantly and drop below the probability of the cor-
rect derivation.
In each iteration, the positive examples from
the previous iteration are first removed so that
new positive examples which lead to better cor-
rect derivations can take their place. However,
negative examples are accumulated across iter-
ations for better accuracy because negative ex-
amples from each iteration only lead to incor-
rect derivations and it is always good to include
them. To further increase the number of nega-
tive examples, every positive example for a pro-
duction is also included as a negative example for
all the other productions having the same LHS.
After a specified number of MAX ITR iterations,
917
(ANSWER? answer(RIVER), [1..9])
(RIVER? TRAVERSE(STATE), [1..9])
(TRAVERSE?traverse, [1..7])
Which1 rivers2 run3 through4 the5 states6 bordering7
(STATE? STATEID, [8..9])
(STATEID? stateid texas, [8..9])
Texas8 ?9
Figure 5: An incorrect semantic derivation of the NL sentence ?Which rivers run through the states
bordering Texas?? which gives the incorrect MR answer(traverse(stateid(texas))).
the trained classifiers from the last iteration are
returned. Testing involves using these classifiers
to generate the most probable derivation of a test
sentence as described in the subsection 2.2, and
returning its MR.
The MRL grammar may contain productions
corresponding to constants of the domain, for e.g.,
state names like ?STATEID ? ?texas??, or river
names like ?RIVERID ? ?colorado?? etc. Our
system allows the user to specify such produc-
tions as constant productions giving the NL sub-
strings, called constant substrings, which directly
relate to them. For example, the user may give
?Texas? as the constant substring for the produc-
tion ?STATEID ? ?texas?. Then KRISP does
not learn classifiers for these constant productions
and instead decides if they cover a substring of the
sentence or not by matching it with the provided
constant substrings.
4 Experiments
4.1 Methodology
KRISP was evaluated on CLANG and GEOQUERY
domains as described in section 2. The CLANG
corpus was built by randomly selecting 300 pieces
of coaching advice from the log files of the 2003
RoboCup Coach Competition. These formal ad-
vice instructions were manually translated into
English (Kate et al, 2005). The GEOQUERY cor-
pus contains 880 English queries collected from
undergraduates and from real users of a web-based
interface (Tang and Mooney, 2001). These were
manually translated into their MRs. The average
length of an NL sentence in the CLANG corpus
is 22.52 words while in the GEOQUERY corpus it
is 7.48 words, which indicates that CLANG is the
harder corpus. The average length of the MRs is
13.42 tokens in the CLANG corpus while it is 6.46
tokens in the GEOQUERY corpus.
KRISP was evaluated using standard 10-fold
cross validation. For every test sentence, only the
best MR corresponding to the most probable se-
mantic derivation is considered for evaluation, and
its probability is taken as the system?s confidence
in that MR. Since KRISP uses a threshold, ?, to
prune low probability derivation trees, it some-
times may fail to return any MR for a test sen-
tence. We computed the number of test sentences
for which KRISP produced MRs, and the number
of these MRs that were correct. For CLANG, an
output MR is considered correct if and only if it
exactly matches the correct MR. For GEOQUERY,
an output MR is considered correct if and only if
the resulting query retrieves the same answer as
the correct MR when submitted to the database.
Performance was measured in terms of precision
(the percentage of generated MRs that were cor-
rect) and recall (the percentage of all sentences for
which correct MRs were obtained).
In our experiments, the threshold ? was fixed
to 0.05 and the beam size ? was 20. These pa-
rameters were found through pilot experiments.
The maximum number of iterations (MAX ITR) re-
quired was only 3, beyond this we found that the
system only overfits the training corpus.
We compared our system?s performance with
the following existing systems: the string and tree
versions of SILT (Kate et al, 2005), a system that
learns transformation rules relating NL phrases
to MRL expressions; WASP (Wong and Mooney,
2006), a system that learns transformation rules
using statistical machine translation techniques;
SCISSOR (Ge and Mooney, 2005), a system that
learns an integrated syntactic-semantic parser; and
CHILL (Tang and Mooney, 2001) an ILP-based
semantic parser. We also compared with the
CCG-based semantic parser by Zettlemoyer et al
(2005), but their results are available only for the
GEO880 corpus and their experimental set-up is
also different from ours. Like KRISP, WASP and
SCISSOR also give confidences to the MRs they
generate which are used to plot precision-recall
curves by measuring precisions and recalls at vari-
918
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
KRISP
WASP
SCISSOR
SILT-tree
SILT-string
Figure 6: Results on the CLANG corpus.
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
KRISP
WASP
SCISSOR
SILT-tree
SILT-string
CHILL
Zettlemoyer et al (2005)
Figure 7: Results on the GEOQUERY corpus.
ous confidence levels. The results of the other sys-
tems are shown as points on the precision-recall
graph.
4.2 Results
Figure 6 shows the results on the CLANG cor-
pus. KRISP performs better than either version
of SILT and performs comparable to WASP. Al-
though SCISSOR gives less precision at lower re-
call values, it gives much higher maximum recall.
However, we note that SCISSOR requires more su-
pervision for the training corpus in the form of se-
mantically annotated syntactic parse trees for the
training sentences. CHILL could not be run be-
yond 160 training examples because its Prolog im-
plementation runs out of memory. For 160 training
examples it gave 49.2% precision with 12.67% re-
call.
Figure 7 shows the results on the GEOQUERY
corpus. KRISP achieves higher precisions than
WASP on this corpus. Overall, the results show
that KRISP performs better than deterministic
rule-based semantic parsers like CHILL and SILT
and performs comparable to other statistical se-
mantic parsers like WASP and SCISSOR.
4.3 Experiments with Other Natural
Languages
We have translations of a subset of the GEOQUERY
corpus with 250 examples (GEO250 corpus) in
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Recall
English
Japanese
Spanish
Turkish
Figure 8: Results of KRISP on the GEO250 corpus
for different natural languages.
three other natural languages: Spanish, Turkish
and Japanese. Since KRISP?s learning algorithm
does not use any natural language specific knowl-
edge, it is directly applicable to other natural lan-
guages. Figure 8 shows that KRISP performs com-
petently on other languages as well.
4.4 Experiments with Noisy NL Sentences
Any real world application in which semantic
parsers would be used to interpret natural language
of a user is likely to face noise in the input. If the
user is interacting through spontaneous speech and
the input to the semantic parser is coming form
the output of a speech recognition system then
there are many ways in which noise could creep
in the NL sentences: interjections (like um?s and
ah?s), environment noise (like door slams, phone
rings etc.), out-of-domain words, grammatically
ill-formed utterances etc. (Zue and Glass, 2000).
As opposed to the other systems, KRISP?s string-
kernel-based semantic parsing does not use hard-
matching rules and should be thus more flexible
and robust to noise. We tested this hypothesis by
running experiments on data which was artificially
corrupted with simulated speech recognition er-
rors.
The interjections, environment noise etc. are
likely to be recognized as real words by a speech
recognizer. To simulate this, after every word in
a sentence, with some probability Padd, an ex-
tra word is added which is chosen with proba-
bility proportional to its word frequency found in
the British National Corpus (BNC), a good rep-
resentative sample of English. A speech recog-
nizer may sometimes completely fail to detect
words, so with a probability of Pdrop a word is
sometimes dropped. A speech recognizer could
also introduce noise by confusing a word with a
high frequency phonetically close word. We sim-
919
 0
 20
 40
 60
 80
 100
 0  1  2  3  4  5
F-
m
ea
su
re
Noise level
KRISP
WASP
SCISSOR
Figure 9: Results on the CLANG corpus with in-
creasing amounts of noise in the test sentences.
ulate this type of noise by substituting a word in
the corpus by another word, w, with probability
ped(w)?P (w), where p is a parameter, ed(w) isw?s
edit distance (Levenshtein, 1966) from the original
word and P (w) is w?s probability proportional to
its word frequency. The edit distance which calcu-
lates closeness between words is character-based
rather than based on phonetics, but this should not
make a significant difference in the experimental
results.
Figure 9 shows the results on the CLANG cor-
pus with increasing amounts of noise, from level
0 to level 4. The noise level 0 corresponds to no
noise. The noise parameters, Padd and Pdrop, were
varied uniformly from being 0 at level 0 and 0.1 at
level 4, and the parameter p was varied uniformly
from being 0 at level 0 and 0.01 at level 4. We
are showing the best F-measure (harmonic mean
of precision and recall) for each system at differ-
ent noise levels. As can be seen, KRISP?s perfor-
mance degrades gracefully in the presence of noise
while other systems? performance degrade much
faster, thus verifying our hypothesis. In this exper-
iment, only the test sentences were corrupted, we
get qualitatively similar results when both training
and test sentences are corrupted. The results are
also similar on the GEOQUERY corpus.
5 Conclusions
We presented a new kernel-based approach to
learn semantic parsers. SVM classifiers based on
string subsequence kernels are trained for each of
the productions in the meaning representation lan-
guage. These classifiers are then used to com-
positionally build complete meaning representa-
tions of natural language sentences. We evaluated
our system on two real-world corpora. The re-
sults showed that our system compares favorably
to other existing systems and is particularly robust
to noise.
Acknowledgments
This research was supported by Defense Ad-
vanced Research Projects Agency under grant
HR0011-04-1-0007.
References
Mao Chen et al 2003. Users manual: RoboCup soccer server manual for soc-
cer server version 7.07 and later. Available at http://sourceforge.
net/projects/sserver/.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support
Vector Machines and Other Kernel-based Learning Methods. Cambridge
University Press.
Jay Earley. 1970. An efficient context-free parsing algorithm. Communica-
tions of the Association for Computing Machinery, 6(8):451?455.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates
syntax and semantics. In Proc. of 9th Conf. on Computational Natural
Language Learning (CoNLL-2005), pages 9?16, Ann Arbor, MI, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural
to formal languages. In Proc. of 20th Natl. Conf. on Artificial Intelligence
(AAAI-2005), pages 1062?1068, Pittsburgh, PA, July.
Rohit J. Kate. 2005. A kernel-based approach to learning semantic parsers.
Technical Report UT-AI-05-326, Artificial Intelligence Lab, University of
Texas at Austin, Austin, TX, November.
V. I. Levenshtein. 1966. Binary codes capable of correcting insertions and
reversals. Soviet Physics Doklady, 10(8):707?710, February.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris
Watkins. 2002. Text classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Scott Miller, David Stallard, Robert Bobrow, and Richard Schwartz. 1996. A
fully statistical approach to natural language interfaces. In Proc. of the 34th
Annual Meeting of the Association for Computational Linguistics (ACL-
96), pages 55?61, Santa Cruz, CA.
John C. Platt. 1999. Probabilistic outputs for support vector machines and
comparisons to regularized likelihood methods. In Alexander J. Smola, Pe-
ter Bartlett, Bernhard Scho?lkopf, and Dale Schuurmans, editors, Advances
in Large Margin Classifiers, pages 185?208. MIT Press.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander
Yates. 2004. Modern natural language interfaces to databases: Composing
statistical parsing with semantic tractability. In Proc. of 20th Intl. Conf. on
Computational Linguistics (COLING-04), Geneva, Switzerland, August.
Patti J. Price. 1990. Evaluation of spoken language systems: The ATIS do-
main. In Proc. of 3rd DARPA Speech and Natural Language Workshop,
pages 91?95, June.
Andreas Stolcke. 1995. An efficient probabilistic context-free parsing al-
gorithm that computes prefix probabilities. Computational Linguistics,
21(2):165?201.
L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in
inductive logic programming for semantic parsing. In Proc. of the 12th
European Conf. on Machine Learning, pages 466?477, Freiburg, Germany.
Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic pars-
ing with statistical machine translation. In Proc. of Human Language Tech-
nology Conf. / North American Association for Computational Linguistics
Annual Meeting (HLT/NAACL-2006), New York City, NY. To appear.
John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of 13th Natl. Conf. on
Artificial Intelligence (AAAI-96), pages 1050?1055, Portland, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to
logical form: Structured classification with probabilistic categorial gram-
mars. In Proc. of 21th Conf. on Uncertainty in Artificial Intelligence (UAI-
2005), Edinburgh, Scotland, July.
Victor W. Zue and James R. Glass. 2000. Conversational interfaces: Advances
and challenges. In Proc. of the IEEE, volume 88(8), pages 1166?1180.
920
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 263?270,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Reranking for Semantic Parsing
Ruifang Ge Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712
{grf,mooney}@cs.utexas.edu
Abstract
Semantic parsing is the task of mapping
natural language sentences to complete
formal meaning representations. The per-
formance of semantic parsing can be po-
tentially improved by using discrimina-
tive reranking, which explores arbitrary
global features. In this paper, we investi-
gate discriminative reranking upon a base-
line semantic parser, SCISSOR, where the
composition of meaning representations is
guided by syntax. We examine if features
used for syntactic parsing can be adapted
for semantic parsing by creating similar
semantic features based on the mapping
between syntax and semantics. We re-
port experimental results on two real ap-
plications, an interpreter for coaching in-
structions in robotic soccer and a natural-
language database interface. The results
show that reranking can improve the per-
formance on the coaching interpreter, but
not on the database interface.
1 Introduction
A long-standing challenge within natural language
processing has been to understand the meaning of
natural language sentences. In comparison with
shallow semantic analysis tasks, such as word-
sense disambiguation (Ide and Jeane?ronis, 1998)
and semantic role labeling (Gildea and Jurafsky,
2002; Carreras and Ma`rquez, 2005), which only
partially tackle this problem by identifying the
meanings of target words or finding semantic roles
of predicates, semantic parsing (Kate et al, 2005;
Ge and Mooney, 2005; Zettlemoyer and Collins,
2005) pursues a more ambitious goal ? mapping
natural language sentences to complete formal
meaning representations (MRs), where the mean-
ing of each part of a sentence is analyzed, includ-
ing noun phrases, verb phrases, negation, quanti-
fiers and so on. Semantic parsing enables logic
reasoning and is critical in many practical tasks,
such as speech understanding (Zue and Glass,
2000), question answering (Lev et al, 2004) and
advice taking (Kuhlmann et al, 2004).
Ge and Mooney (2005) introduced an approach,
SCISSOR, where the composition of meaning rep-
resentations is guided by syntax. First, a statis-
tical parser is used to generate a semantically-
augmented parse tree (SAPT), where each internal
node includes both a syntactic and semantic label.
Once a SAPT is generated, an additional meaning-
composition process guided by the tree structure is
used to translate it into a final formal meaning rep-
resentation.
The performance of semantic parsing can be po-
tentially improved by using discriminative rerank-
ing, which explores arbitrary global features.
While reranking has benefited many tagging and
parsing tasks (Collins, 2000; Collins, 2002c;
Charniak and Johnson, 2005) including semantic
role labeling (Toutanova et al, 2005), it has not
yet been applied to semantic parsing. In this paper,
we investigate the effect of discriminative rerank-
ing to semantic parsing.
We examine if the features used in reranking
syntactic parses can be adapted for semantic pars-
ing, more concretely, for reranking the top SAPTs
from the baseline model SCISSOR. The syntac-
tic features introduced by Collins (2000) for syn-
tactic parsing are extended with similar semantic
features, based on the coupling of syntax and se-
mantics. We present experimental results on two
corpora: an interpreter for coaching instructions
263
in robotic soccer (CLANG) and a natural-language
database interface (GeoQuery). The best rerank-
ing model significantly improves F-measure on
CLANG from 82.3% to 85.1% (15.8% relative er-
ror reduction), however, it fails to show improve-
ments on GEOQUERY.
2 Background
2.1 Application Domains
2.1.1 CLANG: the RoboCup Coach Language
RoboCup (www.robocup.org) is an inter-
national AI research initiative using robotic soccer
as its primary domain. In the Coach Competition,
teams of agents compete on a simulated soccer
field and receive advice from a team coach in
a formal language called CLANG. In CLANG,
tactics and behaviors are expressed in terms of
if-then rules. As described in Chen et al (2003),
its grammar consists of 37 non-terminal symbols
and 133 productions. Negation and quantifiers
like all are included in the language. Below is a
sample rule with its English gloss:
((bpos (penalty-area our))
(do (player-except our {4})
(pos (half our))))
?If the ball is in our penalty area, all our players
except player 4 should stay in our half.?
2.1.2 GEOQUERY: a DB Query Language
GEOQUERY is a logical query language for
a small database of U.S. geography containing
about 800 facts. The GEOQUERY language
consists of Prolog queries augmented with several
meta-predicates (Zelle and Mooney, 1996). Nega-
tion and quantifiers like all and each are included
in the language. Below is a sample query with its
English gloss:
answer(A,count(B,(city(B),loc(B,C),
const(C,countryid(usa))),A))
?How many cities are there in the US??
2.2 SCISSOR: the Baseline Model
SCISSOR is based on a fairly standard approach
to compositional semantics (Jurafsky and Martin,
2000). First, a statistical parser is used to con-
struct a semantically-augmented parse tree that
captures the semantic interpretation of individual
NP-PLAYER
PRP$-TEAM
our
NN-PLAYER
player
CD-UNUM
2
Figure 1: A SAPT for describing a simple CLANG
concept PLAYER .
words and the basic predicate-argument structure
of a sentence. Next, a recursive deterministic pro-
cedure is used to compose the MR of a parent
node from the MR of its children following the
tree structure.
Figure 1 shows the SAPT for a simple natural
language phrase describing the concept PLAYER
in CLANG. We can see that each internal node
in the parse tree is annotated with a semantic la-
bel (shown after dashes) representing concepts in
an application domain; when a node is semanti-
cally vacuous in the application domain, it is as-
signed with the semantic label NULL. The seman-
tic labels on words and non-terminal nodes repre-
sent the meanings of these words and constituents
respectively. For example, the word our repre-
sents a TEAM concept in CLANG with the value
our, whereas the constituent OUR PLAYER 2 rep-
resents a PLAYER concept. Some type concepts
do not take arguments, like team and unum (uni-
form number), while some concepts, which we
refer to as predicates, take an ordered list of ar-
guments, like player which requires both a TEAM
and a UNUM as its arguments.
SAPTs are given to a meaning composition
process to compose meaning, guided by both
tree structures and domain predicate-argument re-
quirements. In figure 1, the MR of our and 2
would fill the arguments of PLAYER to generate
the MR of the whole constituent PLAYER(OUR,2)
using this process.
SCISSOR is implemented by augmenting
Collins? (1997) head-driven parsing model II to
incorporate the generation of semantic labels on
internal nodes. In a head-driven parsing model,
a tree can be seen as generated by expanding
non-terminals with grammar rules recursively.
To deal with the sparse data problem, the expan-
sion of a non-terminal (parent) is decomposed
into primitive steps: a child is chosen as the
head and is generated first, and then the other
children (modifiers) are generated independently
264
BACK-OFFLEVEL PL1(Li|...)
1 P,H,w,t,?,LC
2 P,H,t,?,LC
3 P,H,?,LC
4 P,H
5 P
Table 1: Extended back-off levels for the semantic
parameter PL1(Li|...), using the same notation as
in Ge and Mooney (2005). The symbols P , H and
Li are the semantic label of the parent , head, and
the ith left child, w is the head word of the parent,
t is the semantic label of the head word, ? is the
distance between the head and the modifier, and
LC is the left semantic subcat.
constrained by the head. Here, we only describe
changes made to SCISSOR for reranking, for a
full description of SCISSOR see Ge and Mooney
(2005).
In SCISSOR, the generation of semantic labels
on modifiers are constrained by semantic subcat-
egorization frames, for which data can be very
sparse. An example of a semantic subcat in Fig-
ure 1 is that the head PLAYER associated with NN
requires a TEAM as its modifier. Although this
constraint improves SCISSOR?s precision, which
is important for semantic parsing, it also limits
its recall. To generate plenty of candidate SAPTs
for reranking, we extended the back-off levels for
the parameters generating semantic labels of mod-
ifiers. The new set is shown in Table 1 using the
parameters for the generation of the left-side mod-
ifiers as an example. The back-off levels 4 and 5
are newly added by removing the constraints from
the semantic subcat. Although the best SAPTs
found by the model may not be as precise as be-
fore, we expect that reranking can improve the re-
sults and rank correct SAPTs higher.
2.3 The Averaged Perceptron Reranking
Model
Averaged perceptron (Collins, 2002a) has been
successfully applied to several tagging and parsing
reranking tasks (Collins, 2002c; Collins, 2002a),
and in this paper, we employed it in reranking
semantic parses generated by the base semantic
parser SCISSOR. The model is composed of three
parts (Collins, 2002a): a set of candidate SAPTs
GEN , which is the top n SAPTs of a sentence
from SCISSOR; a function ? that maps a sentence
Inputs: A set of training examples (xi, y?i ), i = 1...n, where xi
is a sentence, and y?i is a candidate SAPT that has the highest
similarity score with the gold-standard SAPT
Initialization: Set W? = 0
Algorithm:
For t = 1...T, i = 1...n
Calculate yi = arg maxy?GEN(xi) ?(xi, y) ? W?
If (yi 6= y?i ) then W? = W? + ?(xi, y?i ) ? ?(xi, yi)
Output: The parameter vector W?
Figure 2: The perceptron training algorithm.
x and its SAPT y into a feature vector ?(x, y) ?
Rd; and a weight vector W? associated with the set
of features. Each feature in a feature vector is a
function on a SAPT that maps the SAPT to a real
value. The SAPT with the highest score under a
parameter vector W? is outputted, where the score
is calculated as:
score(x, y) = ?(x, y) ? W? (1)
The perceptron training algorithm for estimat-
ing the parameter vector W? is shown in Fig-
ure 2. For a full description of the algorithm,
see (Collins, 2002a). The averaged perceptron, a
variant of the perceptron algorithm is often used in
testing to decrease generalization errors on unseen
test examples, where the parameter vectors used
in testing is the average of each parameter vector
generated during the training process.
3 Features for Reranking SAPTs
In our setting, reranking models discriminate be-
tween SAPTs that can lead to correct MRs and
those that can not. Intuitively, both syntactic and
semantic features describing the syntactic and se-
mantic substructures of a SAPT would be good in-
dicators of the SAPT?s correctness.
The syntactic features introduced by Collins
(2000) for reranking syntactic parse trees have
been proven successfully in both English and
Spanish (Cowan and Collins, 2005). We exam-
ine if these syntactic features can be adapted for
semantic parsing by creating similar semantic fea-
tures. In the following section, we first briefly de-
scribe the syntactic features introduced by Collins
(2000), and then introduce two adapted semantic
feature sets. A SAPT in CLANG is shown in Fig-
ure 3 for illustrating the features throughout this
section.
265
VP-ACTION.PASS
VB
be
VP-ACTION.PASS
VBN-ACTION.PASS
passed
PP-POINT
TO
to
NP-POINT
PRN-POINT
-LRB?POINT
(
NP-NUM1
CD-NUM
36
COMMA
,
NP-NUM2
CD-NUM
10
-RRB-
)
Figure 3: A SAPT for illustrating the reranking features, where the syntactic label ?,? is replaced by
COMMA for a clearer description of features, and the NULL semantic labels are not shown. The head
of the rule ?PRN-POINT? -LRB?POINT NP-NUM1 COMMA NP-NUM2 -RRB-? is -LRB?POINT. The
semantic labels NUM1 and NUM2 are meta concepts in CLANG specifying the semantic role filled since
NUM can fill multiple semantic roles in the predicate POINT.
3.1 Syntactic Features
All syntactic features introduced by Collins (2000)
are included for reranking SAPTs. While the full
description of all the features is beyond the scope
of this paper, we still introduce several feature
types here for the convenience of introducing se-
mantic features later.
1. Rules. These are the counts of unique syntac-
tic context-free rules in a SAPT. The example
in Figure 3 has the feature f (PRN? -LRB- NP
COMMA NP -RRB-)=1.
2. Bigrams. These are the counts of unique
bigrams of syntactic labels in a constituent.
They are also featured with the syntactic la-
bel of the constituent, and the bigram?s rel-
ative direction (left, right) to the head of the
constituent. The example in Figure 3 has the
feature f (NP COMMA, right, PRN)=1.
3. Grandparent Rules. These are the same as
Rules, but also include the syntactic label
above a rule. The example in Figure 3 has
the feature f ([PRN? -LRB- NP COMMA NP
-RRB-], NP)=1, where NP is the syntactic la-
bel above the rule ?PRN? -LRB- NP COMMA
NP -RRB-?.
4. Grandparent Bigrams. These are the same
as Bigrams, but also include the syntactic
label above the constituent containing a bi-
gram. The example in Figure 3 has the
feature f ([NP COMMA, right, PRN], NP)=1,
where NP is the syntactic label above the con-
stituent PRN.
3.2 Semantic Features
3.2.1 Semantic Feature Set I
A similar semantic feature type is introduced for
each syntactic feature type used by Collins (2000)
by replacing syntactic labels with semantic ones
(with the semantic label NULL not included). The
corresponding semantic feature types for the fea-
tures in Section 3.1 are:
1. Rules. The example in Figure 3 has the fea-
ture f (POINT? POINT NUM1 NUM2)=1.
2. Bigrams. The example in Figure 3 has the
feature f (NUM1 NUM2, right, POINT)=1,
where the bigram ?NUM1 NUM2?appears to
the right of the head POINT.
3. Grandparent Rules. The example in Figure 3
has the feature f ([POINT? POINT NUM1
NUM2], POINT)=1, where the last POINT is
266
ACTION.PASS
ACTION.PASS
passed
POINT
POINT
(
NUM1
NUM
36
NUM2
NUM
10
Figure 4: The tree generated by removing purely-
syntactic nodes from the SAPT in Figure 3 (with
syntactic labels omitted.)
the semantic label above the semantic rule
?POINT? POINT NUM1 NUM2?.
4. Grandparent Bigrams. The example in Fig-
ure 3 has the feature f ([NUM1 NUM2, right,
POINT], POINT)=1, where the last POINT is
the semantic label above the POINT associ-
ated with PRN.
3.2.2 Semantic Feature Set II
Purely-syntactic structures in SAPTs exist with
no meaning composition involved, such as the ex-
pansions from NP to PRN, and from PP to ?TO NP?
in Figure 3. One possible drawback of the seman-
tic features derived directly from SAPTs as in Sec-
tion 3.2.1 is that they could include features with
no meaning composition involved, which are in-
tuitively not very useful. For example, the nodes
with purely-syntactic expansions mentioned above
would trigger a semantic rule feature with mean-
ing unchanged (from POINT to POINT). Another
possible drawback of these features is that the fea-
tures covering broader context could potentially
fail to capture the real high-level meaning compo-
sition information. For example, the Grandparent
Rule example in Section 3.2.1 has POINT as the
semantic grandparent of a POINT composition, but
not the real one ACTION.PASS.
To address these problems, another semantic
feature set is introduced by deriving semantic fea-
tures from trees where purely-syntactic nodes of
SAPTs are removed (the resulting tree for the
SAPT in Figure 3 is shown in Figure 4). In this
tree representation, the example in Figure 4 would
have the Grandparent Rule feature f ([POINT?
POINT NUM1 NUM2], ACTION.PASS)=1, with the
correct semantic grandparent ACTION.PASS in-
cluded.
4 Experimental Evaluation
4.1 Experimental Methodology
Two corpora of natural language sentences paired
with MRs were used in the reranking experiments.
For CLANG, 300 pieces of coaching advice were
randomly selected from the log files of the 2003
RoboCup Coach Competition. Each formal in-
struction was translated into English by one of
four annotators (Kate et al, 2005). The average
length of an natural language sentence in this cor-
pus is 22.52 words. For GEOQUERY, 250 ques-
tions were collected by asking undergraduate stu-
dents to generate English queries for the given
database. Queries were then manually translated
into logical form (Zelle and Mooney, 1996). The
average length of a natural language sentence in
this corpus is 6.87 words.
We adopted standard 10-fold cross validation
for evaluation: 9/10 of the whole dataset was used
for training (training set), and 1/10 for testing (test
set). To train a reranking model on a training set,
a separate ?internal? 10-fold cross validation over
the training set was employed to generate n-best
SAPTs for each training example using a base-
line learner, where each training set was again
separated into 10 folds with 9/10 for training the
baseline learner, and 1/10 for producing the n-
best SAPTs for training the reranker. Reranking
models trained in this way ensure that the n-best
SAPTs for each training example are not gener-
ated by a baseline model that has already seen that
example. To test a reranking model on a test set, a
baseline model trained on a whole training set was
used to generate n-best SAPTs for each test ex-
ample, and then the reranking model trained with
the above method was used to choose a best SAPT
from the candidate SAPTs.
The performance of semantic parsing was mea-
sured in terms of precision (the percentage of com-
pleted MRs that were correct), recall (the percent-
age of all sentences whose MRs were correctly
generated) and F-measure (the harmonic mean of
precision and recall). Since even a single mistake
in an MR could totally change the meaning of an
example (e.g. having OUR in an MR instead of OP-
PONENT in CLANG), no partial credit was given
for examples with partially-correct SAPTs.
Averaged perceptron (Collins, 2002a), which
has been successfully applied to several tag-
ging and parsing reranking tasks (Collins, 2002c;
Collins, 2002a), was employed for training rerank-
267
CLANG GEOQUERY
P R F P R F
SCISSOR 89.5 73.7 80.8 98.5 74.4 84.8
SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4
Table 2: The performance of the baseline model SCISSOR+ compared with SCISSOR (with the best result in
bold), where P = precision, R = recall, and F = F-measure.
n 1 2 5 10 20 50
CLANG 78.0 81.3 83.0 84.0 85.0 85.3
GEOQUERY 77.2 77.6 80.0 81.2 81.6 81.6
Table 3: Oracle recalls on CLANG and GEOQUERY as a function of number n of n-best SAPTs.
ing models. To choose the correct SAPT of a
training example required for training the aver-
aged perceptron, we selected a SAPT that results
in the correct MR; if multiple such SAPTs exist,
the one with the highest baseline score was cho-
sen. Since no partial credit was awarded in evalua-
tion, a training example was discarded if it had no
correct SAPT. Rerankers were trained on the 50-
best SAPTs provided by SCISSOR, and the num-
ber of perceptron iterations over the training exam-
ples was limited to 10. Typically, in order to avoid
over-fitting, reranking features are filtered by re-
moving those occurring in less than some mini-
mal number of training examples. We only re-
moved features that never occurred in the training
data since experiments with higher cut-offs failed
to show any improvements.
4.2 Results
4.2.1 Baseline Results
Table 2 shows the results comparing the base-
line learner SCISSOR using both the back-off pa-
rameters in Ge and Mooney (2005) (SCISSOR) and
the revised parameters in Section 2.2 (SCISSOR+).
As we expected, SCISSOR+ has better recall and
worse precision than SCISSOR on both corpora
due to the additional levels of back-off. SCISSOR+
is used as the baseline model for all reranking ex-
periments in the next section.
Table 3 gives oracle recalls for CLANG and
GEOQUERY where an oracle picks the correct
parse from the n-best SAPTs if any of them are
correct. Results are shown for increasing values
of n. The trends for CLANG and GEOQUERY are
different: small values of n show significant im-
provements for CLANG, while a larger n is needed
to improve results for GEOQUERY.
4.2.2 Reranking Results
In this section, we describe the experiments
with reranking models utilizing different feature
sets. All models include the score assigned to a
SAPT by the baseline model as a special feature.
Table 4 shows results using different feature sets
derived directly from SAPTs. In general, rerank-
ing improves the performance of semantic parsing
on CLANG, but not on GEOQUERY. This could
be explained by the different oracle recall trends of
CLANG and GEOQUERY. We can see that in Ta-
ble 3, even a small n can increase the oracle score
on CLANG significantly, but not on GEOQUERY.
With the baseline score included as a feature, cor-
rect SAPTs closer to the top are more likely to
be reranked to the top than the ones in the back,
thus CLANG is more likely to have more sentences
reranked correct than GEOQUERY. On CLANG,
using the semantic feature set alne achieves the
best improvements over the baseline with 2.8%
absolute improvement in F-measure (15.8% rel-
ative error reduction), which is significant at the
95% confidence level using a paired Student?s t-
test. Nevertheless, the difference between SEM1
and SYN+SEM1 is very small (only one example).
Using syntactic features alone only slightly im-
proves the results because the syntactic features
do not directly discriminate between correct and
incorrect meaning representations. To put this
in perspective, Charniak and Johnson (2005) re-
ported that reranking improves the F-measure of
syntactic parsing from 89.7% to 91.0% with a 50-
best oracle F-measure score of 96.8%.
Table 5 compares results using semantic fea-
tures directly derived from SAPTs (SEM1), and
from trees with purely-syntactic nodes removed
(SEM2). It compares reranking models using these
268
CLANG GEOQUERY
P R F P R F
SCISSOR+ 87.0 78.0 82.3 95.5 77.2 85.4
SYN 87.7 78.7 83.0 95.5 77.2 85.4
SEM1 90.0(23.1) 80.7(12.3) 85.1(15.8) 95.5 76.8 85.1
SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9
Table 4: Reranking results on CLANG and GEOQUERY using different feature sets derived directly from
SAPTs (with the best results in bold and relative error reduction in parentheses). The reranking model
SYN uses the syntactic feature set in Section 3.1, SEM1 uses the semantic feature set in Section 3.2.1, and
SYN+SEM1 uses both.
CLANG GEOQUERY
P R F P R F
SEM1 90.0 80.7 85.1 95.5 76.8 85.1
SEM2 88.1 79.0 83.3 96.0 77.2 85.6
SEM1+SEM2 88.5 79.3 83.7 95.5 76.4 84.9
SYN+SEM1 89.6 80.3 84.7 95.5 76.4 84.9
SYN+SEM2 88.1 79.0 83.3 95.5 76.8 85.1
SYN+SEM1+SEM2 88.9 79.7 84.0 95.5 76.4 84.9
Table 5: Reranking results on CLANG and GEOQUERY comparing semantic features derived directly from
SAPTs, and semantic features from trees with purely-syntactic nodes removed. The symbol SEM1 and SEM2
refer to the semantic feature sets in Section 3.2.1 and 3.2.1 respectively, and SYN refers to the syntactic
feature set in Section 3.1.
feature sets alone and together, and using them
along with the syntactic feature set (SYN) alone
and together. Overall, SEM1 provides better results
than SEM2 on CLANG and slightly worse results
on GEOQUERY (only in one sentence), regard-
less of whether or not syntactic features are in-
cluded. Using both semantic feature sets does not
improve the results over just using SEM1. On one
hand, the better performance of SEM1 on CLANG
contradicts our expectation because of the reasons
discussed in Section 3.2.2; the reason behind this
needs to be investigated. On the other hand, how-
ever, it also suggests that the semantic features de-
rived directly from SAPTs can provide good evi-
dence for semantic correctness, even with redun-
dant purely syntactically motivated features.
We have also informally experimented with
smoothed semantic features utilizing domain on-
tology given by CLANG, which did not show im-
provements over reranking models not using these
features.
5 Conclusion
We have applied discriminative reranking to se-
mantic parsing, where reranking features are de-
veloped from features for reranking syntactic
parses based on the coupling of syntax and se-
mantics. The best reranking model significantly
improves F-measure on a Robocup coaching task
(CLANG) from 82.3% to 85.1%, while it fails to
improve the performance on a geography database
query task (GEOQUERY).
Future work includes further investigation of
the reasons behind the different utility of rerank-
ing for the CLANG and GEOQUERY tasks. We
also plan to explore other types of reranking
features, such as the features used in semantic
role labeling (SRL) (Gildea and Jurafsky, 2002;
Carreras and Ma`rquez, 2005), like the path be-
tween a target predicate and its argument, and
kernel methods (Collins, 2002b). Experimenting
with other effective reranking algorithms, such as
SVMs (Joachims, 2002) and MaxEnt (Charniak
and Johnson, 2005), is also a direction of our fu-
ture research.
6 Acknowledgements
We would like to thank Rohit J. Kate and anony-
mous reviewers for their insightful comments.
This research was supported by Defense Ad-
269
vanced Research Projects Agency under grant
HR0011-04-1-0007.
References
Xavier Carreras and Lu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proc. of 9th Conf. on Computational
Natural Language Learning (CoNLL-2005), pages
152?164, Ann Arbor, MI, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 43nd Annual Meeting
of the Association for Computational Linguistics
(ACL-05), pages 173?180, Ann Arbor, MI, June.
Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros
Kapetanakis, Kostas Kostiadis, Johan Kummeneje,
Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-
fens, Yi Wang, and Xiang Yin. 2003. Users
manual: RoboCup soccer server manual for soccer
server version 7.07 and later. Available at http://
sourceforge.net/projects/sserver/.
Michael J. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. of the 35th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-97), pages 16?23.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. of 17th Intl. Conf.
on Machine Learning (ICML-2000), pages 175?182,
Stanford, CA, June.
Michael Collins. 2002a. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conf. on Empirical Methods in Natural Lan-
guage Processing (EMNLP-02), Philadelphia, PA,
July.
Michael Collins. 2002b. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proc. of the
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 263?270,
Philadelphia, PA, July.
Michael Collins. 2002c. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2002), pages 489?496, Philadelphia, PA.
Brooke Cowan and Michael Collins. 2005. Mor-
phology and reranking for the statistical parsing of
Spanish. In Proc. of the Human Language Technol-
ogy Conf. and Conf. on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP-05), Van-
couver, B.C., Canada, October.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proc. of 9th Conf. on Computational
Natural Language Learning (CoNLL-2005), pages
9?16, Ann Arbor, MI, July.
Daniel Gildea and Daniel Jurafsky. 2002. Automated
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Nancy A. Ide and Jeane?ronis. 1998. Introduction to
the special issue on word sense disambiguation: The
state of the art. Computational Linguistics, 24(1):1?
40.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proc. of 8th ACM
SIGKDD Intl. Conf. on Knowledge Discovery and
Data Mining (KDD-2002), Edmonton, Canada.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall, Upper
Saddle River, NJ.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages.
In Proc. of 20th Natl. Conf. on Artificial Intelli-
gence (AAAI-2005), pages 1062?1068, Pittsburgh,
PA, July.
Gregory Kuhlmann, Peter Stone, Raymond J. Mooney,
and Jude W. Shavlik. 2004. Guiding a reinforce-
ment learner with natural language advice: Initial
results in RoboCup soccer. In Proc. of the AAAI-04
Workshop on Supervisory Control of Learning and
Adaptive Systems, San Jose, CA, July.
Iddo Lev, Bill MacCartney, Christopher D. Manning,
and Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proc. of
2nd Workshop on Text Meaning and Interpretation,
ACL-04, Barcelona, Spain.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proc. of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), Ann Arbor, MI, June.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of 13th Natl. Conf. on Artifi-
cial Intelligence (AAAI-96), pages 1050?1055, Port-
land, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of 21th Conf. on Uncertainty in
Artificial Intelligence (UAI-2005), Edinburgh, Scot-
land, July.
Victor W. Zue and James R. Glass. 2000. Conversa-
tional interfaces: Advances and challenges. In Proc.
of the IEEE, volume 88(8), pages 1166?1180.
270
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 576?583,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning to Extract Relations from the Web
using Minimal Supervision
Razvan C. Bunescu
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan@cs.utexas.edu
Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
mooney@cs.utexas.edu
Abstract
We present a new approach to relation ex-
traction that requires only a handful of train-
ing examples. Given a few pairs of named
entities known to exhibit or not exhibit a
particular relation, bags of sentences con-
taining the pairs are extracted from the web.
We extend an existing relation extraction
method to handle this weaker form of su-
pervision, and present experimental results
demonstrating that our approach can reliably
extract relations from web documents.
1 Introduction
A growing body of recent work in information
extraction has addressed the problem of relation
extraction (RE), identifying relationships between
entities stated in text, such as LivesIn(Person,
Location) or EmployedBy(Person, Company).
Supervised learning has been shown to be effective
for RE (Zelenko et al, 2003; Culotta and Sorensen,
2004; Bunescu and Mooney, 2006); however, anno-
tating large corpora with examples of the relations
to be extracted is expensive and tedious.
In this paper, we introduce a supervised learning
approach to RE that requires only a handful of
training examples and uses the web as a corpus.
Given a few pairs of well-known entities that
clearly exhibit or do not exhibit a particular re-
lation, such as CorpAcquired(Google, YouTube)
and not(CorpAcquired(Yahoo, Microsoft)), a
search engine is used to find sentences on the web
that mention both of the entities in each of the pairs.
Although not all of the sentences for positive pairs
will state the desired relationship, many of them
will. Presumably, none of the sentences for negative
pairs state the targeted relation. Multiple instance
learning (MIL) is a machine learning framework
that exploits this sort of weak supervision, in
which a positive bag is a set of instances which is
guaranteed to contain at least one positive example,
and a negative bag is a set of instances all of which
are negative. MIL was originally introduced to
solve a problem in biochemistry (Dietterich et
al., 1997); however, it has since been applied to
problems in other areas such as classifying image
regions in computer vision (Zhang et al, 2002), and
text categorization (Andrews et al, 2003; Ray and
Craven, 2005).
We have extended an existing approach to rela-
tion extraction using support vector machines and
string kernels (Bunescu and Mooney, 2006) to han-
dle this weaker form of MIL supervision. This ap-
proach can sometimes be misled by textual features
correlated with the specific entities in the few train-
ing pairs provided. Therefore, we also describe a
method for weighting features in order to focus on
those correlated with the target relation rather than
with the individual entities. We present experimen-
tal results demonstrating that our approach is able to
accurately extract relations from the web by learning
from such weak supervision.
2 Problem Description
We address the task of learning a relation extrac-
tion system targeted to a fixed binary relationship
R. The only supervision given to the learning algo-
576
rithm is a small set of pairs of named entities that are
known to belong (positive) or not belong (negative)
to the given relationship. Table 1 shows four posi-
tive and two negative example pairs for the corpo-
rate acquisition relationship. For each pair, a bag of
sentences containing the two arguments can be ex-
tracted from a corpus of text documents. The corpus
is assumed to be sufficiently large and diverse such
that, if the pair is positive, it is highly likely that the
corresponding bag contains at least one sentence that
explicitly asserts the relationship R between the two
arguments. In Section 6 we describe a method for
extracting bags of relevant sentences from the web.
+/? Arg a1 Arg a2
+ Google YouTube
+ Adobe Systems Macromedia
+ Viacom DreamWorks
+ Novartis Eon Labs
? Yahoo Microsoft
? Pfizer Teva
Table 1: Corporate Acquisition Pairs.
Using a limited set of entity pairs (e.g. those in
Table 1) and their associated bags as training data,
the aim is to induce a relation extraction system that
can reliably decide whether two entities mentioned
in the same sentence exhibit the target relationship
or not. In particular, when tested on the example
sentences from Figure 1, the system should classify
S1, S3,and S4 as positive, and S2 and S5 as negative.
+/S1: Search engine giant Google has bought video-
sharing website YouTube in a controversial $1.6 billion
deal.
?/S2: The companies will merge Google?s search ex-
pertise with YouTube?s video expertise, pushing what
executives believe is a hot emerging market of video
offered over the Internet.
+/S3: Google has acquired social media company,
YouTube for $1.65 billion in a stock-for-stock transaction
as announced by Google Inc. on October 9, 2006.
+/S4: Drug giant Pfizer Inc. has reached an agreement
to buy the private biotechnology firm Rinat Neuroscience
Corp., the companies announced Thursday.
?/S5: He has also received consulting fees from Al-
pharma, Eli Lilly and Company, Pfizer, Wyeth Pharmaceu-
ticals, Rinat Neuroscience, Elan Pharmaceuticals, and For-
est Laboratories.
Figure 1: Sentence examples.
As formulated above, the learning task can be
seen as an instance of multiple instance learning.
However, there are important properties that set it
apart from problems previously considered in MIL.
The most distinguishing characteristic is that the
number of bags is very small, while the average size
of the bags is very large.
3 Multiple Instance Learning
Since its introduction by Dietterich (1997), an ex-
tensive and quite diverse set of methods have been
proposed for solving the MIL problem. For the task
of relation extraction, we consider only MIL meth-
ods where the decision function can be expressed in
terms of kernels computed between bag instances.
This choice was motivated by the comparatively
high accuracy obtained by kernel-based SVMs when
applied to various natural language tasks, and in par-
ticular to relation extraction. Through the use of ker-
nels, SVMs (Vapnik, 1998; Scho?lkopf and Smola,
2002) can work efficiently with instances that im-
plicitly belong to a high dimensional feature space.
When used for classification, the decision function
computed by the learning algorithm is equivalent to
a hyperplane in this feature space. Overfitting is
avoided in the SVM formulation by requiring that
positive and negative training instances be maxi-
mally separated by the decision hyperplane.
Gartner et al (2002) adapted SVMs to the MIL
setting using various multi-instance kernels. Two
of these ? the normalized set kernel, and the statis-
tic kernel ? have been experimentally compared to
other methods by Ray and Craven (2005), with com-
petitive results. Alternatively, a simple approach to
MIL is to transform it into a standard supervised
learning problem by labeling all instances from pos-
itive bags as positive. An interesting outcome of the
study conducted by Ray and Craven (2005) was that,
despite the class noise in the resulting positive ex-
amples, such a simple approach often obtains com-
petitive results when compared against other more
sophisticated MIL methods.
We believe that an MIL method based on multi-
instance kernels is not appropriate for training
datasets that contain just a few, very large bags. In
a multi-instance kernel approach, only bags (and
not instances) are considered as training examples,
577
which means that the number of support vectors is
going to be upper bounded by the number of train-
ing bags. Taking the bags from Table 1 as a sam-
ple training set, the decision function is going to be
specified by at most seven parameters: the coeffi-
cients for at most six support vectors, plus an op-
tional bias parameter. A hypothesis space character-
ized by such a small number of parameters is likely
to have insufficient capacity.
Based on these observations, we decided to trans-
form the MIL problem into a standard supervised
problem as described above. The use of this ap-
proach is further motivated by its simplicity and its
observed competitive performance on very diverse
datasets (Ray and Craven, 2005). Let X be the set
of bags used for training, Xp ? X the set of posi-
tive bags, and Xn ? X the set of negative bags. For
any instance x ? X from a bag X ? X , let ?(x)
be the (implicit) feature vector representation of x.
Then the corresponding SVM optimization problem
can be formulated as in Figure 2:
minimize:
J(w, b, ?) = 12?w?2 + CL
(
cp LnL ?p + cn
Lp
L ?n
)
?p =
?
X?Xp
?
x?X
?x
?n =
?
X?Xn
?
x?X
?x
subject to:
w?(x) + b ? +1? ?x, ?x ? X ? Xp
w?(x) + b ? ?1 + ?x, ?x ? X ? Xn
?x ? 0
Figure 2: SVM Optimization Problem.
The capacity control parameter C is normalized
by the total number of instances L = Lp + Ln =
?
X?Xp |X| +
?
X?Xn |X|, so that it remains in-
dependent of the size of the dataset. The additional
non-negative parameter cp (cn = 1?cp) controls the
relative influence that false negative vs. false posi-
tive errors have on the value of the objective func-
tion. Because not all instances from positive bags
are real positive instances, it makes sense to have
false negative errors be penalized less than false pos-
itive errors (i.e. cp < 0.5).
In the dual formulation of the optimization prob-
lem from Figure 2, bag instances appear only inside
dot products of the form K(x1, x2) = ?(x1)?(x2).
The kernel K is instantiated to a subsequence ker-
nel, as described in the next section.
4 Relation Extraction Kernel
The training bags consist of sentences extracted
from online documents, using the methodology de-
scribed in Section 6. Parsing web documents in
order to obtain a syntactic analysis often gives un-
reliable results ? the type of narrative can vary
greatly from one web document to another, and sen-
tences with grammatical errors are frequent. There-
fore, for the initial experiments, we used a modi-
fied version of the subsequence kernel of Bunescu
and Mooney (2006), which does not require syn-
tactic information. This kernel computes the num-
ber of common subsequences of tokens between two
sentences. The subsequences are constrained to be
?anchored? at the two entity names, and there is
a maximum number of tokens that can appear in
a sequence. For example, a subsequence feature
for the sentence S1 in Figure 1 is s? = ??e1? . . .
bought . . . ?e2? . . . in . . . billion . . . deal?, where
?e1? and ?e2? are generic placeholders for the two
entity names. The subsequence kernel induces a
feature space where each dimension corresponds
to a sequence of words. Any such sequence that
matches a subsequence of words in a sentence exam-
ple is down-weighted as a function of the total length
of the gaps between every two consecutive words.
More exactly, let s = w1w2 . . . wk be a sequence of
k words, and s? = w1 g1 w2 g2 . . . wk?1 gk?1 wk a
matching subsequence in a relation example, where
gi stands for any sequence of words between wi and
wi+1. Then the sequence s will be represented in the
relation example as a feature with weight computed
as ?(s) = ?g(s?). The parameter ? controls the mag-
nitude of the gap penalty, where g(s?) = ?i |gi| is
the total gap.
Many relations, like the ones that we explore in
the experimental evaluation, cannot be expressed
without using at least one content word. We there-
fore modified the kernel computation to optionally
ignore subsequence patterns formed exclusively of
578
stop words and punctuation signs. In Section 5.1,
we introduce a new weighting scheme, wherein a
weight is assigned to every token. Correspondingly,
every sequence feature will have an additional mul-
tiplicative weight, computed as the product of the
weights of all the tokens in the sequence. The aim
of this new weighting scheme, as detailed in the next
section, is to eliminate the bias caused by the special
structure of the relation extraction MIL problem.
5 Two Types of Bias
As already hinted at the end of Section 2, there is
one important property that distinguishes the cur-
rent MIL setting for relation extraction from other
MIL problems: the training dataset contains very
few bags, and each bag can be very large. Con-
sequently, an application of the learning model de-
scribed in Sections 3 & 4 is bound to be affected by
the following two types of bias:
 [Type I Bias] By definition, all sentences inside
a bag are constrained to contain the same two ar-
guments. Words that are semantically correlated
with either of the two arguments are likely to oc-
cur in many sentences. For example, consider the
sentences S1 and S2 from the bag associated with
?Google? and ?YouTube? (as shown in Figure 1).
They both contain the words ?search? ? highly cor-
related with ?Google?, and ?video? ? highly corre-
lated with ?YouTube?, and it is likely that a signifi-
cant percentage of sentences in this bag contain one
of the two words (or both). The two entities can be
mentioned in the same sentence for reasons other
than the target relation R, and these noisy training
sentences are likely to contain words that are corre-
lated with the two entities, without any relationship
to R. A learning model where the features are based
on words, or word sequences, is going to give too
much weight to words or combinations of words that
are correlated with either of individual arguments.
This overweighting will adversely affect extraction
performance through an increased number of errors.
A method for eliminating this type of bias is intro-
duced in Section 5.1.
 [Type II Bias] While Type I bias is due to words
that are correlated with the arguments of a relation
instance, the Type II bias is caused by words that
are specific to the relation instance itself. Using
FrameNet terminology (Baker et al, 1998), these
correspond to instantiated frame elements. For ex-
ample, the corporate acquisition frame can be seen
as a subtype of the ?Getting? frame in FrameNet.
The core elements in this frame are the Recipi-
ent (e.g. Google) and the Theme (e.g. YouTube),
which for the acquisition relationship coincide with
the two arguments. They do not contribute any
bias, since they are replaced with the generic tags
?e1? and ?e2? in all sentences from the bag. There
are however other frame elements ? peripheral, or
extra-thematic ? that can be instantiated with the
same value in many sentences. In Figure 1, for in-
stance, sentence S3 contains two non-core frame ele-
ments: the Means element (e.g ?in a stock-for-stock
transaction?) and the Time element (e.g. ?on Oc-
tober 9, 2006?). Words from these elements, like
?stock?, or ?October?, are likely to occur very often
in the Google-YouTube bag, and because the train-
ing dataset contains only a few other bags, subse-
quence patterns containing these words will be given
too much weight in the learned model. This is prob-
lematic, since these words can appear in many other
frames, and thus the learned model is likely to make
errors. Instead, we would like the model to fo-
cus on words that trigger the target relationship (in
FrameNet, these are the lexical units associated with
the target frame).
5.1 A Solution for Type I Bias
In order to account for how strongly the words in a
sequence are correlated with either of the individual
arguments of the relation, we modify the formula for
the sequence weight ?(s) by factoring in a weight
?(w) for each word in the sequence, as illustrated in
Equation 1.
?(s) = ?g(s?) ?
?
w?s
?(w) (1)
Given a predefined set of weights ?(w), it is straight-
forward to update the recursive computation of
the subsequence kernel so that it reflects the new
weighting scheme.
If all the word weights are set to 1, then the new
kernel is equivalent to the old one. What we want,
however, is a set of weights where words that are
correlated with either of the two arguments are given
lower weights. For any word, the decrease in weight
579
should reflect the degree of correlation between that
word and the two arguments. Before showing the
formula used for computing the word weights, we
first introduce some notation:
? Let X ? X be an arbitrary bag, and let X.a1
and X.a2 be the two arguments associated with
the bag.
? Let C(X) be the size of the bag (i.e. the num-
ber of sentences in the bag), and C(X,w) the
number of sentences in the bag X that contain
the word w. Let P (w|X) = C(X,w)/C(X).
? Let P (w|X.a1 ? X.a2) be the probability that
the word w appears in a sentence due only to
the presence of X.a1 or X.a2, assuming X.a1
and X.a2 are independent causes for w.
The word weights are computed as follows:
?(w) = C(X,w)? P (w|X.a1 ?X.a2) ? C(X)C(X,w)
= 1? P (w|X.a1 ?X.a2)P (w|X) (2)
The quantity P (w|X.a1 ? X.a2) ? C(X) represents
the expected number of sentences in which w would
occur, if the only causes were X.a1 or X.a2, inde-
pendent of each other. We want to discard this quan-
tity from the total number of occurrences C(X,w),
so that the effect of correlations with X.a1 or X.a2
is eliminated.
We still need to compute P (w|X.a1 ?X.a2). Be-
cause in the definition of P (w|X.a1 ?X.a2), the ar-
guments X.a1 and X.a2 were considered indepen-
dent causes, P (w|X.a1 ? X.a2) can be computed
with the noisy-or operator (Pearl, 1986):
P (?) = 1?(1?P (w|a1)) ? (1?P (w|a2)) (3)
= P (w|a1)+P (w|a2)?P (w|a1) ? P (w|a2)
The quantity P (w|a) represents the probability that
the word w appears in a sentence due only to the
presence of a, and it could be estimated using counts
on a sufficiently large corpus. For our experimen-
tal evaluation, we used the following approxima-
tion: given an argument a, a set of sentences con-
taining a are extracted from web documents (de-
tails in Section 6). Then P (w|a) is simply approxi-
mated with the ratio of the number of sentences con-
taining w over the total number of sentences, i.e.
P (w|a) = C(w, a)/C(a). Because this may be an
overestimate (w may appear in a sentence contain-
ing a due to causes other than a), and also because
of data sparsity, the quantity ?(w) may sometimes
result in a negative value ? in these cases it is set to
0, which is equivalent to ignoring the word w in all
subsequence patterns.
6 MIL Relation Extraction Datasets
For the purpose of evaluation, we created two
datasets: one for corporate acquisitions, as shown
in Table 2, and one for the person-birthplace rela-
tion, with the example pairs from Table 3. In both
tables, the top part shows the training pairs, while
the bottom part shows the test pairs.
+/? Arg a1 Arg a2 Size
+ Google YouTube 1375
+ Adobe Systems Macromedia 622
+ Viacom DreamWorks 323
+ Novartis Eon Labs 311
? Yahoo Microsoft 163
? Pfizer Teva 247
+ Pfizer Rinat Neuroscience 50 (41)
+ Yahoo Inktomi 433 (115)
? Google Apple 281
? Viacom NBC 231
Table 2: Corporate Acquisition Pairs.
+/? Arg a1 Arg a2 Size
+ Franz Kafka Prague 552
+ Andre Agassi Las Vegas 386
+ Charlie Chaplin London 292
+ George Gershwin New York 260
? Luc Besson New York 74
? Wolfgang A. Mozart Vienna 288
+ Luc Besson Paris 126 (6)
+ Marie Antoinette Vienna 105 (39)
? Charlie Chaplin Hollywood 266
? George Gershwin London 104
Table 3: Person-Birthplace Pairs.
Given a pair of arguments (a1, a2), the corre-
sponding bag of sentences is created as follows:
 A query string ?a1 ? ? ? ? ? ? ? a2? containing
seven wildcard symbols between the two arguments
is submitted to Google. The preferences are set to
search only for pages written in English, with Safe-
search turned on. This type of query will match doc-
uments where an occurrence of a1 is separated from
an occurrence of a2 by at most seven content words.
This is an approximation of our actual information
580
need: ?return all documents containing a1 and a2 in
the same sentence?.
 The returned documents (limited by Google to
the first 1000) are downloaded, and then the text
is extracted using the HTML parser from the Java
Swing package. Whenever possible, the appropriate
HTML tags (e.g. BR, DD, P, etc.) are used as hard
end-of-sentence indicators. The text is further seg-
mented into sentences with the OpenNLP1 package.
 Sentences that do not contain both arguments a1
and a2 are discarded. For every remaining sentence,
we find the occurrences of a1 and a2 that are clos-
est to each other, and create a relation example by
replacing a1 with ?e1? and a2 with ?e2?. All other
occurrences of a1 and a2 are replaced with a null
token ignored by the subsequence kernel.
The number of sentences in every bag is shown in
the last column of Tables 2 & 3. Because Google
also counts pages that are deemed too similar in the
first 1000, some of the bags can be relatively small.
As described in Section 5.1, the word-argument
correlations are modeled through the quantity
P (w|a) = C(w, a)/C(a), estimated as the ratio be-
tween the number of sentences containing w and a,
and the number of sentences containing a. These
counts are computed over a bag of sentences con-
taining a, which is created by querying Google for
the argument a, and then by processing the results
as described above.
7 Experimental Evaluation
Each dataset is split into two sets of bags: one
for training and one for testing. The test dataset
was purposefully made difficult by including neg-
ative bags with arguments that during training were
used in positive bags, and vice-versa. In order to
evaluate the relation extraction performance at the
sentence level, we manually annotated all instances
from the positive test bags. The last column in Ta-
bles 2 & 3 shows, between parentheses, how many
instances from the positive test bags are real pos-
itive instances. The corporate acquisition test set
has a total of 995 instances, out of which 156 are
positive. The person-birthplace test set has a total
of 601 instances, and only 45 of them are positive.
Extrapolating from the test set distribution, the pos-
1http://opennlp.sourceforge.net
itive bags in the person-birthplace dataset are sig-
nificantly sparser in real positive instances than the
positive bags in the corporate acquisition dataset.
The subsequence kernel described in Section 4
was used as a custom kernel for the LibSVM2 Java
package. When run with the default parameters,
the results were extremely poor ? too much weight
was given to the slack term in the objective func-
tion. Minimizing the regularization term is essen-
tial in order to capture subsequence patterns shared
among positive bags. Therefore LibSVM was mod-
ified to solve the optimization problem from Fig-
ure 2, where the capacity parameter C is normal-
ized by the size of the transformed dataset. In this
new formulation, C is set to its default value of 1.0
? changing it to other values did not result in signifi-
cant improvement. The trade-off between false pos-
itive and false negative errors is controlled by the
parameter cp. When set to its default value of 0.5,
false-negative errors and false positive errors have
the same impact on the objective function. As ex-
pected, setting cp to a smaller value (0.1) resulted
in better performance. Tests with even lower values
did not improve the results.
We compare the following four systems:
 SSK?MIL: This corresponds to the MIL formu-
lation from Section 3, with the original subsequence
kernel described in Section 4.
 SSK?T1: This is the SSK?MIL system aug-
mented with word weights, so that the Type I bias
is reduced, as described in Section 5.1.
 BW-MIL: This is a bag-of-words kernel, in
which the relation examples are classified based on
the unordered words contained in the sentence. This
baseline shows the performance of a standard text-
classification approach to the problem using a state-
of-the art algorithm (SVM).
 SSK?SIL: This corresponds to the original sub-
sequence kernel trained with traditional, single in-
stance learning (SIL) supervision. For evaluation,
we train on the manually labeled instances from the
test bags. We use a combination of one positive bag
and one negative bag for training, while the other
two bags are used for testing. The results are aver-
aged over all four possible combinations. Note that
the supervision provided to SSK?SIL requires sig-
2http://www.csie.ntu.edu.tw/?cjlin/libsvm
581
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
isi
on
 (%
)
Recall (%)
SSK-T1
SSK-MIL
BW-MIL
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
isi
on
 (%
)
Recall (%)
SSK-T1
SSK-MIL
BW-MIL
(a) Corporate Acquisitions (b) Person-Birthplace
Figure 3: Precision-Recall graphs on the two datasets.
nificantly more annotation effort, therefore, given a
sufficient amount of training examples, we expect
this system to perform at least as well as its MIL
counterpart.
In Figure 3, precision is plotted against recall by
varying a threshold on the value of the SVM deci-
sion function. To avoid clutter, we show only the
graphs for the first three systems. In Table 4 we
show the area under the precision recall curves of
all four systems. Overall, the learned relation extrac-
tors are able to identify the relationship in novel sen-
tences quite accurately and significantly out-perform
a bag-of-words baseline. The new version of the
subsequence kernel SSK?T1 is significantly more
accurate in the MIL setting than the original sub-
sequence kernel SSK?MIL, and is also competitive
with SSK?SIL, which was trained using a reason-
able amount of manually labeled sentence examples.
Dataset SSK?MIL SSK?T1 BW?MIL SSK?SIL
(a) CA 76.9% 81.1% 45.9% 80.4%
(b) PB 72.5% 78.2% 69.2% 73.4%
Table 4: Area Under Precision-Recall Curve.
8 Future Work
An interesting potential application of our approach
is a web relation-extraction system similar to Google
Sets, in which the user provides only a handful of
pairs of entities known to exhibit or not to exhibit
a particular relation, and the system is used to find
other pairs of entities exhibiting the same relation.
Ideally, the user would only need to provide pos-
itive pairs. Sentences containing one of the rela-
tion arguments could be extracted from the web, and
likely negative sentence examples automatically cre-
ated by pairing this entity with other named enti-
ties mentioned in the sentence. In this scenario, the
training set can contain both false positive and false
negative noise. One useful side effect is that Type
I bias is partially removed ? some bias still remains
due to combinations of at least two words, each cor-
related with a different argument of the relation.
We are also investigating methods for reducing Type
II bias, either by modifying the word weights, or by
integrating an appropriate measure of word distri-
bution across positive bags directly in the objective
function for the MIL problem. Alternatively, im-
plicit negative evidence can be extracted from sen-
tences in positive bags by exploiting the fact that, be-
sides the two relation arguments, a sentence from a
positive bag may contain other entity mentions. Any
pair of entities different from the relation pair is very
likely to be a negative example for that relation. This
is similar to the concept of negative neighborhoods
introduced by Smith and Eisner (2005), and has the
potential of eliminating both Type I and Type II bias.
9 Related Work
One of the earliest IE methods designed to work
with a reduced amount of supervision is that of
Hearst (1992), where a small set of seed patterns
is used in a bootstrapping fashion to mine pairs of
582
hypernym-hyponym nouns. Bootstrapping is actu-
ally orthogonal to our method, which could be used
as the pattern learner in every bootstrapping itera-
tion. A more recent IE system that works by boot-
strapping relation extraction patterns from the web is
KNOWITALL (Etzioni et al, 2005). For a given tar-
get relation, supervision in KNOWITALL is provided
as a rule template containing words that describe the
class of the arguments (e.g. ?company?), and a small
set of seed extraction patterns (e.g. ?has acquired?).
In our approach, the type of supervision is different ?
we ask only for pairs of entities known to exhibit the
target relation or not. Also, KNOWITALL requires
large numbers of search engine queries in order to
collect and validate extraction patterns, therefore ex-
periments can take weeks to complete. Compara-
tively, the approach presented in this paper requires
only a small number of queries: one query per rela-
tion pair, and one query for each relation argument.
Craven and Kumlien (1999) create a noisy train-
ing set for the subcellular-localization relation by
mining Medline for sentences that contain tuples
extracted from relevant medical databases. To our
knowledge, this is the first approach that is using a
?weakly? labeled dataset for relation extraction. The
resulting bags however are very dense in positive ex-
amples, and they are also many and small ? conse-
quently, the two types of bias are not likely to have
significant impact on their system?s performance.
10 Conclusion
We have presented a new approach to relation ex-
traction that leverages the vast amount of informa-
tion available on the web. The new RE system is
trained using only a handful of entity pairs known to
exhibit and not exhibit the target relationship. We
have extended an existing relation extraction ker-
nel to learn in this setting and to resolve problems
caused by the minimal supervision provided. Exper-
imental results demonstrate that the new approach
can reliably extract relations from web documents.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful suggestions. This work was supported
by grant IIS-0325116 from the NSF, and a gift from
Google Inc.
References
Stuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann.
2003. Support vector machines for multiple-instance learn-
ing. In NIPS 15, pages 561?568, Vancouver, BC. MIT Press.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In Proc. of COLING?ACL
?98, pages 86?90, San Francisco, CA. Morgan Kaufmann
Publishers.
Razvan C. Bunescu and Raymond J. Mooney. 2006. Sub-
sequence kernels for relation extraction. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, NIPS 18.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from text
sources. In Proc. of ISMB?99, pages 77?86, Heidelberg,
Germany.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proc. of ACL?04, pages
423?429, Barcelona, Spain, July.
Thomas G. Dietterich, Richard H. Lathrop, and Tomas Lozano-
Perez. 1997. Solving the multiple instance problem with
axis-parallel rectangles. Artificial Intelligence, 89(1-2):31?
71.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria
Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,
and Alexander Yates. 2005. Unsupervised named-entity ex-
traction from the web: an experimental study. Artificial In-
telligence, 165(1):91?134.
T. Gartner, P.A. Flach, A. Kowalczyk, and A.J. Smola. 2002.
Multi-instance kernels. In In Proc. of ICML?02, pages 179?
186, Sydney, Australia, July. Morgan Kaufmann.
M. A. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proc. of ACL?92, Nantes, France.
Judea Pearl. 1986. Fusion, propagation, and structuring in be-
lief networks. Artificial Intelligence, 29(3):241?288.
Soumya Ray and Mark Craven. 2005. Supervised versus mul-
tiple instance learning: An empirical comparison. In Proc.
of ICML?05, pages 697?704, Bonn, Germany.
Bernhard Scho?lkopf and Alexander J. Smola. 2002. Learning
with kernels - support vector machines, regularization, opti-
mization and beyond. MIT Press, Cambridge, MA.
N. A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In Proc. of ACL?05,
pages 354?362, Ann Arbor, Michigan.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. John
Wiley & Sons.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning
Research, 3:1083?1106.
Q. Zhang, S. A. Goldman, W. Yu, and J. Fritts. 2002. Content-
based image retrieval using multiple-instance learning. In
Proc. of ICML?02, pages 682?689.
583
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960?967,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Synchronous Grammars for Semantic Parsing with
Lambda Calculus
Yuk Wah Wong and Raymond J. Mooney
Department of Computer Sciences
The University of Texas at Austin
{ywwong,mooney}@cs.utexas.edu
Abstract
This paper presents the first empirical results
to our knowledge on learning synchronous
grammars that generate logical forms. Using
statistical machine translation techniques, a
semantic parser based on a synchronous
context-free grammar augmented with ?-
operators is learned given a set of training
sentences and their correct logical forms.
The resulting parser is shown to be the best-
performing system so far in a database query
domain.
1 Introduction
Originally developed as a theory of compiling pro-
gramming languages (Aho and Ullman, 1972), syn-
chronous grammars have seen a surge of interest re-
cently in the statistical machine translation (SMT)
community as a way of formalizing syntax-based
translation models between natural languages (NL).
In generating multiple parse trees in a single deriva-
tion, synchronous grammars are ideal for model-
ing syntax-based translation because they describe
not only the hierarchical structures of a sentence
and its translation, but also the exact correspon-
dence between their sub-parts. Among the gram-
mar formalisms successfully put into use in syntax-
based SMT are synchronous context-free gram-
mars (SCFG) (Wu, 1997) and synchronous tree-
substitution grammars (STSG) (Yamada and Knight,
2001). Both formalisms have led to SMT sys-
tems whose performance is state-of-the-art (Chiang,
2005; Galley et al, 2006).
Synchronous grammars have also been used in
other NLP tasks, most notably semantic parsing,
which is the construction of a complete, formal
meaning representation (MR) of an NL sentence. In
our previous work (Wong and Mooney, 2006), se-
mantic parsing is cast as a machine translation task,
where an SCFG is used to model the translation
of an NL into a formal meaning-representation lan-
guage (MRL). Our algorithm, WASP, uses statistical
models developed for syntax-based SMT for lexical
learning and parse disambiguation. The result is a
robust semantic parser that gives good performance
in various domains. More recently, we show that
our SCFG-based parser can be inverted to produce a
state-of-the-art NL generator, where a formal MRL
is translated into an NL (Wong and Mooney, 2007).
Currently, the use of learned synchronous gram-
mars in semantic parsing and NL generation is lim-
ited to simple MRLs that are free of logical vari-
ables. This is because grammar formalisms such as
SCFG do not have a principled mechanism for han-
dling logical variables. This is unfortunate because
most existing work on computational semantics is
based on predicate logic, where logical variables
play an important role (Blackburn and Bos, 2005).
For some domains, this problem can be avoided by
transforming a logical language into a variable-free,
functional language (e.g. the GEOQUERY functional
query language inWong andMooney (2006)). How-
ever, development of such a functional language is
non-trivial, and as we will see, logical languages can
be more appropriate for certain domains.
On the other hand, most existing methods for
mapping NL sentences to logical forms involve sub-
stantial hand-written components that are difficult
to maintain (Joshi and Vijay-Shanker, 2001; Bayer
et al, 2004; Bos, 2005). Zettlemoyer and Collins
(2005) present a statistical method that is consider-
960
ably more robust, but it still relies on hand-written
rules for lexical acquisition, which can create a per-
formance bottleneck.
In this work, we show that methods developed for
SMT can be brought to bear on tasks where logical
forms are involved, such as semantic parsing. In par-
ticular, we extend the WASP semantic parsing algo-
rithm by adding variable-binding ?-operators to the
underlying SCFG. The resulting synchronous gram-
mar generates logical forms using ?-calculus (Mon-
tague, 1970). A semantic parser is learned given a
set of sentences and their correct logical forms us-
ing SMT methods. The new algorithm is called ?-
WASP, and is shown to be the best-performing sys-
tem so far in the GEOQUERY domain.
2 Test Domain
In this work, we mainly consider the GEOQUERY
domain, where a query language based on Prolog is
used to query a database on U.S. geography (Zelle
and Mooney, 1996). The query language consists
of logical forms augmented with meta-predicates
for concepts such as smallest and count. Figure 1
shows two sample logical forms and their English
glosses. Throughout this paper, we use the notation
x1, x2, . . . for logical variables.
Although Prolog logical forms are the main focus
of this paper, our algorithm makes minimal assump-
tions about the target MRL. The only restriction on
the MRL is that it be defined by an unambiguous
context-free grammar (CFG) that divides a logical
form into subformulas (and terms into subterms).
Figure 2(a) shows a sample parse tree of a logical
form, where each CFG production corresponds to a
subformula.
3 The Semantic Parsing Algorithm
Our work is based on the WASP semantic parsing al-
gorithm (Wong andMooney, 2006), which translates
NL sentences into MRs using an SCFG. In WASP,
each SCFG production has the following form:
A ? ??, ?? (1)
where ? is an NL phrase and ? is the MR translation
of ?. Both ? and ? are strings of terminal and non-
terminal symbols. Each non-terminal in ? appears
in ? exactly once. We use indices to show the cor-
respondence between non-terminals in ? and ?. All
derivations start with a pair of co-indexed start sym-
bols, ?S 1 , S 1 ?. Each step of a derivation involves
the rewriting of a pair of co-indexed non-terminals
by the same SCFG production. The yield of a deriva-
tion is a pair of terminal strings, ?e, f?, where e is
an NL sentence and f is the MR translation of e.
For convenience, we call an SCFG production a rule
throughout this paper.
While WASP works well for target MRLs that
are free of logical variables such as CLANG (Wong
and Mooney, 2006), it cannot easily handle various
kinds of logical forms used in computational seman-
tics, such as predicate logic. The problem is that
WASP lacks a principled mechanism for handling
logical variables. In this work, we extend the WASP
algorithm by adding a variable-binding mechanism
based on ?-calculus, which allows for compositional
semantics for logical forms.
This work is based on an extended version of
SCFG, which we call ?-SCFG, where each rule has
the following form:
A ? ??, ?x1 . . . ?xk.?? (2)
where ? is an NL phrase and ? is the MR trans-
lation of ?. Unlike (1), ? is a string of termi-
nals, non-terminals, and logical variables. The
variable-binding operator ? binds occurrences of
the logical variables x1, . . . , xk in ?, which makes
?x1 . . . ?xk.? a ?-function of arity k. When ap-
plied to a list of arguments, (xi1 , . . . , xik), the ?-
function gives ??, where ? is a substitution oper-
ator, {x1/xi1 , . . . , xk/xik}, that replaces all bound
occurrences of xj in ? with xij . If any of the ar-
guments xij appear in ? as a free variable (i.e. not
bound by any ?), then those free variables in ? must
be renamed before function application takes place.
Each non-terminal Aj in ? is followed by a list
of arguments, xj = (xj1 , . . . , xjkj ). During pars-
ing, Aj must be rewritten by a ?-function fj of ar-
ity kj . Like SCFG, a derivation starts with a pair
of co-indexed start symbols and ends when all non-
terminals have been rewritten. To compute the yield
of a derivation, each fj is applied to its correspond-
ing arguments xj to obtain an MR string free of ?-
operators with logical variables properly named.
961
(a) answer(x1,smallest(x2,(state(x1),area(x1,x2))))
What is the smallest state by area?
(b) answer(x1,count(x2,(city(x2),major(x2),loc(x2,x3),next to(x3,x4),state(x3),
equal(x4,stateid(texas)))))
How many major cities are in states bordering Texas?
Figure 1: Sample logical forms in the GEOQUERY domain and their English glosses.
(a)
smallest(x2,(FORM,FORM))
QUERY
answer(x1,FORM)
area(x1,x2)state(x1)
(b)
?x1.smallest(x2,(FORM(x1),FORM(x1, x2)))
QUERY
answer(x1,FORM(x1))
?x1.state(x1) ?x1.?x2.area(x1,x2)
Figure 2: Parse trees of the logical form in Figure 1(a).
As a concrete example, Figure 2(b) shows an
MR parse tree that corresponds to the English
parse, [What is the [smallest [state] [by area]]],
based on the ?-SCFG rules in Figure 3. To
compute the yield of this MR parse tree, we start
from the leaf nodes: apply ?x1.state(x1) to
the argument (x1), and ?x1.?x2.area(x1,x2)
to the arguments (x1, x2). This results in two
MR strings: state(x1) and area(x1,x2).
Substituting these MR strings for the FORM non-
terminals in the parent node gives the ?-function
?x1.smallest(x2,(state(x1),area(x1,x2))).
Applying this ?-function to (x1) gives the MR
string smallest(x2,(state(x1),area(x1,x2))).
Substituting this MR string for the FORM non-
terminal in the grandparent node in turn gives the
logical form in Figure 1(a). This is the yield of the
MR parse tree, since the root node of the parse tree
is reached.
3.1 Lexical Acquisition
Given a set of training sentences paired with their
correct logical forms, {?ei, fi?}, the main learning
task is to find a ?-SCFG, G, that covers the train-
ing data. Like most existing work on syntax-based
SMT (Chiang, 2005; Galley et al, 2006), we con-
structG using rules extracted fromword alignments.
We use the K = 5 most probable word alignments
for the training set given by GIZA++ (Och and Ney,
2003), with variable names ignored to reduce spar-
sity. Rules are then extracted from each word align-
ment as follows.
To ground our discussion, we use the word align-
ment in Figure 4 as an example. To represent
the logical form in Figure 4, we use its linearized
parse?a list of MRL productions that generate the
logical form, in top-down, left-most order (cf. Fig-
ure 2(a)). Since the MRL grammar is unambiguous,
every logical form has a unique linearized parse. We
assume the alignment to be n-to-1, where each word
is linked to at most one MRL production.
Rules are extracted in a bottom-up manner, start-
ing with MRL productions at the leaves of the
MR parse tree, e.g. FORM ? state(x1) in Fig-
ure 2(a). Given an MRL production, A ? ?, a
ruleA ? ??, ?xi1 . . . ?xik .?? is extracted such that:
(1) ? is the NL phrase linked to the MRL produc-
tion; (2) xi1 , . . . , xik are the logical variables that
appear in ? and outside the current leaf node in the
MR parse tree. If xi1 , . . . , xik were not bound by
?, they would become free variables in ?, subject to
renaming during function application (and therefore,
invisible to the rest of the logical form). For exam-
ple, since x1 is an argument of the state predicate
as well as answer and area, x1 must be bound
(cf. the corresponding tree node in Figure 2(b)). The
rule extracted for the state predicate is shown in
Figure 3.
The case for the internal nodes of the MR parse
tree is similar. Given an MRL production, A ? ?,
where ? contains non-terminals A1, . . . , An, a rule
A ? ??, ?xi1 . . . ?xik .??? is extracted such that: (1)
? is the NL phrase linked to the MRL production,
with non-terminals A1, . . . , An showing the posi-
tions of the argument strings; (2) ?? is ? with each
non-terminal Aj replaced with Aj(xj1 , . . . , xjkj ),
where xj1 , . . . , xjkj are the bound variables in the
?-function used to rewrite Aj ; (3) xi1 , . . . , xik are
the logical variables that appear in ?? and outside
the current MR sub-parse. For example, see the rule
962
FORM ? ?state , ?x1.state(x1)?
FORM ? ?by area , ?x1.?x2.area(x1,x2)?
FORM ? ?smallest FORM 1 FORM 2 , ?x1.smallest(x2,(FORM 1 (x1),FORM 2 (x1, x2)))?
QUERY ? ?what is (1) FORM 1 , answer(x1,FORM 1 (x1))?
Figure 3: ?-SCFG rules for parsing the English sentence in Figure 1(a).
smallest
the
is
what
state
by
area
QUERY ? answer(x1,FORM)
FORM ? smallest(x2,(FORM,FORM))
FORM ? state(x1)
FORM ? area(x1,x2)
Figure 4: Word alignment for the sentence pair in Figure 1(a).
extracted for the smallest predicate in Figure 3,
where x2 is an argument of smallest, but it does
not appear outside the formula smallest(...),
so x2 need not be bound by ?. On the other
hand, x1 appears in ??, and it appears outside
smallest(...) (as an argument of answer),
so x1 must be bound.
Rule extraction continues in this manner until the
root of the MR parse tree is reached. Figure 3 shows
all the rules extracted from Figure 4.1
3.2 Probabilistic Semantic Parsing Model
Since the learned ?-SCFG can be ambiguous, a
probabilistic model is needed for parse disambigua-
tion. We use the maximum-entropy model proposed
in Wong and Mooney (2006), which defines a condi-
tional probability distribution over derivations given
an observed NL sentence. The output MR is the
yield of the most probable derivation according to
this model.
Parameter estimation involves maximizing the
conditional log-likelihood of the training set. For
each rule, r, there is a feature that returns the num-
ber of times r is used in a derivation. More features
will be introduced in Section 5.
4 Promoting NL/MRL Isomorphism
We have described the ?-WASP algorithm which
generates logical forms based on ?-calculus. While
reasonably effective, it can be improved in several
ways. In this section, we focus on improving lexical
acquisition.
1For details regarding non-isomorphic NL/MR parse trees,
removal of bad links from alignments, and extraction of word
gaps (e.g. the token (1) in the last rule of Figure 3), see Wong
and Mooney (2006).
To see why the current lexical acquisition algo-
rithm can be problematic, consider the word align-
ment in Figure 5 (for the sentence pair in Fig-
ure 1(b)). No rules can be extracted for the state
predicate, because the shortest NL substring that
covers the word states and the argument string
Texas, i.e. states bordering Texas, contains the word
bordering, which is linked to an MRL production
outside the MR sub-parse rooted at state. Rule
extraction is forbidden in this case because it would
destroy the link between bordering and next to.
In other words, the NL and MR parse trees are not
isomorphic.
This problem can be ameliorated by transforming
the logical form of each training sentence so that
the NL and MR parse trees are maximally isomor-
phic. This is possible because some of the opera-
tors used in the logical forms, notably the conjunc-
tion operator (,), are both associative (a,(b,c)
= (a,b),c = a,b,c) and commutative (a,b =
b,a). Hence, conjuncts can be reordered and re-
grouped without changing the meaning of a conjunc-
tion. For example, rule extraction would be pos-
sible if the positions of the next to and state
conjuncts were switched. We present a method for
regrouping conjuncts to promote isomorphism be-
tween NL and MR parse trees.2 Given a conjunc-
tion, it does the following: (See Figure 6 for the
pseudocode, and Figure 5 for an illustration.)
Step 1. Identify the MRL productions that corre-
spond to the conjuncts and the meta-predicate that
takes the conjunction as an argument (count in
Figure 5), and figure them as vertices in an undi-
2This method also applies to any operators that are associa-
tive and commutative, e.g. disjunction. For concreteness, how-
ever, we use conjunction as an example.
963
QUERY ? answer(x1,FORM)howmany
major
cities
are
in
states
bordering
texas
FORM ? count(x2,(CONJ),x1)
CONJ ? city(x2),CONJ
CONJ ? major(x2),CONJ
CONJ ? loc(x2,x3),CONJ
CONJ ? next to(x3,x4),CONJ
CONJ ? state(x3),FORM
FORM ? equal(x4,stateid(texas))
O
rig
in
al
M
R
pa
rs
e
x2
x3
x4
how many
cities
in
states
bordering
texas
major
QUERY
answer(x1,FORM)
count(x2,(CONJ),x1)
major(x2),CONJ
city(x2),CONJ
loc(x2,x3),CONJ
state(x3),CONJ
next to(x3,x4),FORM
equal(x4,stateid(texas))
QUERY
answer(x1,FORM)
count(x2,(CONJ),x1)
major(x2),CONJ
city(x2),CONJ
loc(x2,x3),CONJ
equal(x4,stateid(texas))
next to(x3,x4),CONJ
state(x3),FORM
(sh
o
w
n
a
bo
ve
a
s
th
ic
k
ed
ge
s)
St
ep
5.
Fi
n
d
M
ST
Step 4. Assign edge weights
Step 6.
Construct MR parse
F
o
rm
g
raph
Step
s1
?3
.
Figure 5: Transforming the logical form in Figure 1(b). The step numbers correspond to those in Figure 6.
Input: A conjunction, c, of n conjuncts; MRL productions, p1, . . . , pn, that correspond to each conjunct; an MRL production,
p0, that corresponds to the meta-predicate taking c as an argument; an NL sentence, e; a word alignment, a.
Let v(p) be the set of logical variables that appear in p. Create an undirected graph, ?, with vertices V = {pi|i = 0, . . . , n}1
and edges E = {(pi, pj)|i < j,v(pi) ? v(pj) 6= ?}.
Let e(p) be the set of words in e to which p is linked according to a. Let span(pi, pj) be the shortest substring of e that2
includes e(pi) ? e(pj). Subtract {(pi, pj)|i 6= 0, span(pi, pj) ? e(p0) 6= ?} from E.
Add edges (p0, pi) to E if pi is not already connected to p0.3
For each edge (pi, pj) in E, set edge weight to the minimum word distance between e(pi) and e(pj).4
Find a minimum spanning tree, T , for ? using Kruskal?s algorithm.5
Using p0 as the root, construct a conjunction c? based on T , and then replace c with c?.6
Figure 6: Algorithm for regrouping conjuncts to promote isomorphism between NL and MR parse trees.
rected graph, ?. An edge (pi, pj) is in ? if and only
if pi and pj contain occurrences of the same logical
variables. Each edge in ? indicates a possible edge
in the transformed MR parse tree. Intuitively, two
concepts are closely related if they involve the same
logical variables, and therefore, should be placed
close together in the MR parse tree. By keeping oc-
currences of a logical variable in close proximity in
the MR parse tree, we also avoid unnecessary vari-
able bindings in the extracted rules.
Step 2. Remove edges from ? whose inclusion in
the MR parse tree would prevent the NL and MR
parse trees from being isomorphic.
Step 3. Add edges to ? to make sure that a spanning
tree for ? exists.
Steps 4?6. Assign edge weights based on word dis-
tance, find a minimum spanning tree, T , for ?, then
regroup the conjuncts based on T . The choice of T
reflects the intuition that words that occur close to-
gether in a sentence tend to be semantically related.
This procedure is repeated for all conjunctions
that appear in a logical form. Rules are then ex-
tracted from the same input alignment used to re-
group conjuncts. Of course, the regrouping of con-
juncts requires a good alignment to begin with, and
that requires a reasonable ordering of conjuncts in
the training data, since the alignment model is sen-
sitive to word order. This suggests an iterative algo-
rithm in which a better grouping of conjuncts leads
to a better alignment model, which guides further re-
grouping until convergence. We did not pursue this,
as it is not needed in our experiments so far.
964
(a) answer(x1,largest(x2,(state(x1),major(x1),river(x1),traverse(x1,x2))))
What is the entity that is a state and also a major river, that traverses something that is the largest?
(b) answer(x1,smallest(x2,(highest(x1,(point(x1),loc(x1,x3),state(x3))),density(x1,x2))))
Among the highest points of all states, which one has the lowest population density?
(c) answer(x1,equal(x1,stateid(alaska)))
Alaska?
(d) answer(x1,largest(x2,(largest(x1,(state(x1),next to(x1,x3),state(x3))),population(x1,x2))))
Among the largest state that borders some other state, which is the one with the largest population?
Figure 7: Typical errors made by the ?-WASP parser, along with their English interpretations, before any
language modeling for the target MRL was done.
5 Modeling the Target MRL
In this section, we propose two methods for model-
ing the target MRL. This is motivated by the fact that
many of the errors made by the ?-WASP parser can
be detected by inspecting the MR translations alone.
Figure 7 shows some typical errors, which can be
classified into two broad categories:
1. Type mismatch errors. For example, a state can-
not possibly be a river (Figure 7(a)). Also it is
awkward to talk about the population density of a
state?s highest point (Figure 7(b)).
2. Errors that do not involve type mismatch. For ex-
ample, a query can be overly trivial (Figure 7(c)),
or involve aggregate functions on a known single-
ton (Figure 7(d)).
The first type of errors can be fixed by type check-
ing. Each m-place predicate is associated with a list
ofm-tuples showing all valid combinations of entity
types that the m arguments can refer to:
point( ): {(POINT)}
density( , ):
{(COUNTRY, NUM), (STATE, NUM), (CITY, NUM)}
These m-tuples of entity types are given as do-
main knowledge. The parser maintains a set of
possible entity types for each logical variable in-
troduced in a partial derivation (except those that
are no longer visible). If there is a logical vari-
able that cannot refer to any types of entities
(i.e. the set of entity types is empty), then the par-
tial derivation is considered invalid. For exam-
ple, based on the tuples shown above, point(x1)
and density(x1, ) cannot be both true, because
{POINT} ? {COUNTRY, STATE, CITY} = ?. The
use of type checking is to exploit the fact that peo-
ple tend not to ask questions that obviously have no
valid answers (Grice, 1975). It is also similar to
Schuler?s (2003) use of model-theoretic interpreta-
tions to guide syntactic parsing.
Errors that do not involve type mismatch are
handled by adding new features to the maximum-
entropy model (Section 3.2). We only consider fea-
tures that are based on the MR translations, and
therefore, these features can be seen as an implicit
language model of the target MRL (Papineni et al,
1997). Of the many features that we have tried,
one feature set stands out as being the most effec-
tive, the two-level rules in Collins and Koo (2005),
which give the number of times a given rule is used
to expand a non-terminal in a given parent rule.
We use only the MRL part of the rules. For ex-
ample, a negative weight for the combination of
QUERY ? answer(x1,FORM(x1)) and FORM
? ?x1.equal(x1, ) would discourage any parse
that yields Figure 7(c). The two-level rules features,
along with the features described in Section 3.2, are
used in the final version of ?-WASP.
6 Experiments
We evaluated the ?-WASP algorithm in the GEO-
QUERY domain. The larger GEOQUERY corpus con-
sists of 880 English questions gathered from various
sources (Wong and Mooney, 2006). The questions
were manually translated into Prolog logical forms.
The average length of a sentence is 7.57 words.
We performed a single run of 10-fold cross
validation, and measured the performance of the
learned parsers using precision (percentage of trans-
lations that were correct), recall (percentage of test
sentences that were correctly translated), and F-
measure (harmonic mean of precision and recall).
A translation is considered correct if it retrieves the
same answer as the correct logical form.
Figure 8 shows the learning curves for the ?-
965
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  100  200  300  400  500  600  700  800  900
Pr
ec
is
io
n 
(%
)
Number of training examples
lambda-WASP
WASP
SCISSOR
Z&C
(a) Precision
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  100  200  300  400  500  600  700  800  900
R
ec
al
l (%
)
Number of training examples
lambda-WASP
WASP
SCISSOR
Z&C
(b) Recall
Figure 8: Learning curves for various parsing algorithms on the larger GEOQUERY corpus.
(%) ?-WASP WASP SCISSOR Z&C
Precision 91.95 87.19 92.08 96.25
Recall 86.59 74.77 72.27 79.29
F-measure 89.19 80.50 80.98 86.95
Table 1: Performance of various parsing algorithms on the larger GEOQUERY corpus.
WASP algorithm compared to: (1) the original
WASP algorithm which uses a functional query lan-
guage (FunQL); (2) SCISSOR (Ge and Mooney,
2005), a fully-supervised, combined syntactic-
semantic parsing algorithm which also uses FunQL;
and (3) Zettlemoyer and Collins (2005) (Z&C), a
CCG-based algorithm which uses Prolog logical
forms. Table 1 summarizes the results at the end
of the learning curves (792 training examples for ?-
WASP, WASP and SCISSOR, 600 for Z&C).
A few observations can be made. First, algorithms
that use Prolog logical forms as the target MRL gen-
erally show better recall than those using FunQL. In
particular, ?-WASP has the best recall by far. One
reason is that it allows lexical items to be combined
in ways not allowed by FunQL or the hand-written
templates in Z&C, e.g. [smallest [state] [by area]]
in Figure 3. Second, Z&C has the best precision, al-
though their results are based on 280 test examples
only, whereas our results are based on 10-fold cross
validation. Third, ?-WASP has the best F-measure.
To see the relative importance of each component
of the ?-WASP algorithm, we performed two abla-
tion studies. First, we compared the performance
of ?-WASP with and without conjunct regrouping
(Section 4). Second, we compared the performance
of ?-WASP with and without language modeling for
the MRL (Section 5). Table 2 shows the results.
It is found that conjunct regrouping improves recall
(p < 0.01 based on the paired t-test), and the use of
two-level rules in the maximum-entropy model im-
proves precision and recall (p < 0.05). Type check-
ing also significantly improves precision and recall.
A major advantage of ?-WASP over SCISSOR and
Z&C is that it does not require any prior knowl-
edge of the NL syntax. Figure 9 shows the perfor-
mance of ?-WASP on the multilingual GEOQUERY
data set. The 250-example data set is a subset of the
larger GEOQUERY corpus. All English questions in
this data set were manually translated into Spanish,
Japanese and Turkish, while the corresponding Pro-
log queries remain unchanged. Figure 9 shows that
?-WASP performed comparably for all NLs. In con-
trast, SCISSOR cannot be used directly on the non-
English data, because syntactic annotations are only
available in English. Z&C cannot be used directly
either, because it requires NL-specific templates for
building CCG grammars.
7 Conclusions
We have presented ?-WASP, a semantic parsing al-
gorithm based on a ?-SCFG that generates logical
forms using ?-calculus. A semantic parser is learned
given a set of training sentences and their correct
logical forms using standard SMT techniques. The
result is a robust semantic parser for predicate logic,
and it is the best-performing system so far in the
GEOQUERY domain.
This work shows that it is possible to use standard
SMT methods in tasks where logical forms are in-
volved. For example, it should be straightforward
to adapt ?-WASP to the NL generation task?all
one needs is a decoder that can handle input logical
forms. Other tasks that can potentially benefit from
966
(%) Precision Recall
?-WASP 91.95 86.59
w/o conj. regrouping 90.73 83.07
(%) Precision Recall
?-WASP 91.95 86.59
w/o two-level rules 88.46 84.32
and w/o type checking 65.45 63.18
Table 2: Performance of ?-WASP with certain components of the algorithm removed.
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
Pr
ec
is
io
n 
(%
)
Number of training examples
English
Spanish
Japanese
Turkish
(a) Precision
 0
 20
 40
 60
 80
 100
 0  50  100  150  200  250
R
ec
al
l (%
)
Number of training examples
English
Spanish
Japanese
Turkish
(b) Recall
Figure 9: Learning curves for ?-WASP on the multilingual GEOQUERY data set.
this include question answering and interlingual MT.
In future work, we plan to further generalize the
synchronous parsing framework to allow different
combinations of grammar formalisms. For exam-
ple, to handle long-distance dependencies that occur
in open-domain text, CCG and TAG would be more
appropriate than CFG. Certain applications may re-
quire different meaning representations, e.g. frame
semantics.
Acknowledgments: We thank Rohit Kate, Raz-
van Bunescu and the anonymous reviewers for their
valuable comments. This work was supported by a
gift from Google Inc.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice Hall, Englewood
Cliffs, NJ.
S. Bayer, J. Burger, W. Greiff, and B. Wellner. 2004.
The MITRE logical form generation system. In Proc. of
Senseval-3, Barcelona, Spain, July.
P. Blackburn and J. Bos. 2005. Representation and Inference
for Natural Language: A First Course in Computational Se-
mantics. CSLI Publications, Stanford, CA.
J. Bos. 2005. Towards wide-coverage semantic interpretation.
In Proc. of IWCS-05, Tilburg, The Netherlands, January.
D. Chiang. 2005. A hierarchical phrase-based model for sta-
tistical machine translation. In Proc. of ACL-05, pages 263?
270, Ann Arbor, MI, June.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguistics,
31(1):25?69.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In Proc. of
COLING/ACL-06, pages 961?968, Sydney, Australia, July.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Proc. of CoNLL-05,
pages 9?16, Ann Arbor, MI, July.
H. P. Grice. 1975. Logic and conversation. In P. Cole and
J. Morgan, eds., Syntax and Semantics 3: Speech Acts, pages
41?58. Academic Press, New York.
A. K. Joshi and K. Vijay-Shanker. 2001. Compositional se-
mantics with lexicalized tree-adjoining grammar (LTAG):
How much underspecification is necessary? In H. Bunt et
al., eds., Computing Meaning, volume 2, pages 147?163.
Kluwer Academic Publishers, Dordrecht, The Netherlands.
R. Montague. 1970. Universal grammar. Theoria, 36:373?398.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997. Feature-
based language understanding. In Proc. of EuroSpeech-97,
pages 1435?1438, Rhodes, Greece.
W. Schuler. 2003. Using model-theoretic semantic interpre-
tation to guide statistical parsing and word recognition in a
spoken language interface. In Proc. of ACL-03, pages 529?
536.
Y. W. Wong and R. J. Mooney. 2006. Learning for seman-
tic parsing with statistical machine translation. In Proc. of
HLT/NAACL-06, pages 439?446, New York City, NY.
Y. W. Wong and R. J. Mooney. 2007. Generation by inverting
a semantic parser that uses statistical machine translation. In
Proc. of NAACL/HLT-07, Rochester, NY, to appear.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proc. of ACL-01, pages 523?530,
Toulouse, France.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse database
queries using inductive logic programming. In Proc. of
AAAI-96, pages 1050?1055, Portland, OR, August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map sen-
tences to logical form: Structured classification with proba-
bilistic categorial grammars. In Proc. of UAI-05, Edinburgh,
Scotland, July.
967
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 611?619,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning a Compositional Semantic Parser
using an Existing Syntactic Parser
Ruifang Ge Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712
{grf,mooney}@cs.utexas.edu
Abstract
We present a new approach to learning a
semantic parser (a system that maps natu-
ral language sentences into logical form).
Unlike previous methods, it exploits an ex-
isting syntactic parser to produce disam-
biguated parse trees that drive the compo-
sitional semantic interpretation. The re-
sulting system produces improved results
on standard corpora on natural language
interfaces for database querying and sim-
ulated robot control.
1 Introduction
Semantic parsing is the task of mapping a natu-
ral language (NL) sentence into a completely for-
mal meaning representation (MR) or logical form.
A meaning representation language (MRL) is a
formal unambiguous language that supports au-
tomated inference, such as first-order predicate
logic. This distinguishes it from related tasks
such as semantic role labeling (SRL) (Carreras
and Marquez, 2004) and other forms of ?shallow?
semantic analysis that do not produce completely
formal representations. A number of systems for
automatically learning semantic parsers have been
proposed (Ge and Mooney, 2005; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2007; Lu et al,
2008). Given a training corpus of NL sentences
annotated with their correct MRs, these systems
induce an interpreter for mapping novel sentences
into the given MRL.
Previous methods for learning semantic parsers
do not utilize an existing syntactic parser that pro-
vides disambiguated parse trees.1 However, ac-
curate syntactic parsers are available for many
1Ge and Mooney (2005) use training examples with
semantically annotated parse trees, and Zettlemoyer and
Collins (2005) learn a probabilistic semantic parsing model
which initially requires a hand-built, ambiguous CCG gram-
mar template.
(a) If our player 2 has the ball,
then position our player 5 in the midfield.
((bowner (player our {2}))
(do (player our {5}) (pos (midfield))))
(b) Which river is the longest?
answer(x1,longest(x1,river(x1)))
Figure 1: Sample NLs and their MRs in the
ROBOCUP and GEOQUERY domains respectively.
languages and could potentially be used to learn
more effective semantic analyzers. This paper
presents an approach to learning semantic parsers
that uses parse trees from an existing syntactic
analyzer to drive the interpretation process. The
learned parser uses standard compositional seman-
tics to construct alternative MRs for a sentence
based on its syntax tree, and then chooses the best
MR based on a trained statistical disambiguation
model. The learning system first employs a word
alignment method from statistical machine trans-
lation (GIZA++ (Och and Ney, 2003)) to acquire
a semantic lexicon that maps words to logical
predicates. Then it induces rules for composing
MRs and estimates the parameters of a maximum-
entropy model for disambiguating semantic inter-
pretations. After describing the details of our ap-
proach, we present experimental results on stan-
dard corpora demonstrating improved results on
learning NL interfaces for database querying and
simulated robot control.
2 Background
In this paper, we consider two domains. The
first is ROBOCUP (www.robocup.org). In the
ROBOCUP Coach Competition, soccer agents
compete on a simulated soccer field and receive
coaching instructions in a formal language called
CLANG (Chen et al, 2003). Figure 1(a) shows a
sample instruction. The second domain is GEO-
QUERY, where a logical query language based on
Prolog is used to query a database on U.S. geog-
raphy (Zelle and Mooney, 1996). The logical lan-
611
CONDITION
(bowner PLAYER )
(player TEAM
our
{UNUM})
2
(a)
P BOWNER
P PLAYER
P OUR P UNUM
(b)
S
NP
PRP$
our
NP
NN
player
CD
2
VP
VB
has
NP
DET
the
NN
ball
(c)
Figure 2: Parses for the condition part of the CLANG in Figure 1(a): (a) The parse of the MR. (b) The
predicate argument structure of (a). (c) The parse of the NL.
PRODUCTION PREDICATE
RULE?(CONDITION DIRECTIVE) P RULE
CONDITION?(bowner PLAYER) P BOWNER
PLAYER?(player TEAM {UNUM}) P PLAYER
TEAM?our P OUR
UNUM?2 P UNUM
DIRECTIVE?(do PLAYER ACTION) P DO
ACTION?(pos REGION) P POS
REGION?(midfield) P MIDFIELD
Table 1: Sample production rules for parsing the
CLANG example in Figure 1(a) and their corre-
sponding predicates.
guage consists of both first-order and higher-order
predicates. Figure 1(b) shows a sample query in
this domain.
We assume that an MRL is defined by an un-
ambiguous context-free grammar (MRLG), so that
MRs can be uniquely parsed, a standard require-
ment for computer languages. In an MRLG, each
production rule introduces a single predicate in the
MRL, where the type of the predicate is given in
the left hand side (LHS), and the number and types
of its arguments are defined by the nonterminals in
the right hand side (RHS). Therefore, the parse of
an MR also gives its predicate-argument structure.
Figure 2(a) shows the parse of the condition
part of the MR in Figure 1(a) using the MRLG
described in (Wong, 2007), and its predicate-
argument structure is in Figure 2(b). Sample
MRLG productions and their predicates for pars-
ing this example are shown in Table 1, where the
predicate P PLAYER takes two arguments (a1 and
a2) of type TEAM and UNUM (uniform number).
3 Semantic Parsing Framework
This section describes our basic framework, which
is based on a fairly standard approach to computa-
tional semantics (Blackburn and Bos, 2005). The
framework is composed of three components: 1)
an existing syntactic parser to produce parse trees
for NL sentences; 2) learned semantic knowledge
(cf. Sec. 5), including a semantic lexicon to assign
possible predicates (meanings) to words, and a set
of semantic composition rules to construct possi-
ble MRs for each internal node in a syntactic parse
given its children?s MRs; and 3) a statistical dis-
ambiguation model (cf. Sec. 6) to choose among
multiple possible semantic constructs as defined
by the semantic knowledge.
The process of generating the semantic parse
for an NL sentence is as follows. First, the syn-
tactic parser produces a parse tree for the NL
sentence. Second, the semantic lexicon assigns
possible predicates to each word in the sentence.
Third, all possible MRs for the sentence are con-
structed compositionally in a recursive, bottom-up
fashion following its syntactic parse using com-
position rules. Lastly, the statistical disambigua-
tion model scores each possible MR and returns
the one with the highest score. Fig. 3(a) shows
one possible semantically-augmented parse tree
(SAPT) (Ge and Mooney, 2005) for the condition
part of the example in Fig. 1(a) given its syntac-
tic parse in Fig. 2(c). A SAPT adds a semantic
label to each non-leaf node in the syntactic parse
tree. The label specifies the MRL predicate for
the node and its remaining (unfilled) arguments.
The compositional process assumes a binary parse
tree suitable for predicate-argument composition;
parses in Penn-treebank style are binarized using
Collins? (1999) method.
Consider the construction of the SAPT in
Fig. 3(a). First, each word is assigned a semantic
label. Most words are assigned an MRL predicate.
For example, the word player is assigned the pred-
icate P PLAYER with its two unbound arguments,
a1 and a2, indicated using ?. Words that do not
introduce a predicate are given the label NULL,
like the and ball.2 Next, a semantic label is as-
2The words the and ball are not truly ?meaningless? since
the predicate P BOWNER (ball owner) is conveyed by the
612
P BOWNER
P PLAYER
P OUR
our
?a1P PLAYER
??a1?a2?P PLAYER
player
P UNUM
2
?a1P BOWNER
?a1P BOWNER
has
NULL
NULL
the
NULL
ball
(a) SAPT
(bowner (player our {2}))
(player our {2})
our
our
?a1 (player a1 {2})
??a1?a2?(player a1 {a2} )
player
2
2
?a1(bowner a1)
?a1(bowner a1)
has
NULL
NULL
the
NULL
ball
(b) Semantic Derivation
Figure 3: Semantic parse for the condition part of the example in Fig. 1(a) using the syntactic parse in
Fig. 2(c): (a) A SAPT with syntactic labels omitted for brevity. (b) The semantic derivation of the MR.
signed to each internal node using learned compo-
sition rules that specify how arguments are filled
when composing two MRs (cf. Sec. 5). The label
?a1P PLAYER indicates that the remaining argu-
ment a2 of the P PLAYER child is filled by the MR
of the other child (labeled P UNUM).
Finally, the SAPT is used to guide the composi-
tion of the sentence?s MR. At each internal node,
an MR for the node is built from the MRs of its
children by filling an argument of a predicate, as
illustrated in the semantic derivation shown in Fig.
3(b). Semantic composition rules (cf. Sec. 5) are
used to specify the argument to be filled. For the
node spanning player 2, the predicate P PLAYER
and its second argument P UNUM are composed to
form the MR: ?a1 (player a1 {2}). Composing
an MR with NULL leaves the MR unchanged. An
MR is said to be complete when it contains no re-
maining ? variables. This process continues up the
phrase has the ball. For simplicity, predicates are intro-
duced by a single word, but statistical disambiguation (cf.
Sec. 6) uses surrounding words to choose a meaning for a
word whose lexicon entry contains multiple possible predi-
cates.
tree until a complete MR for the entire sentence is
constructed at the root.
4 Ensuring Meaning Composition
The basic compositional method in Sec. 3 only
works if the syntactic parse tree strictly follows
the predicate-argument structure of the MR, since
meaning composition at each node is assumed to
combine a predicate with one of its arguments.
However, this assumption is not always satisfied,
for example, in the case of verb gapping and flex-
ible word order. We use constructing the MR for
the directive part of the example in Fig. 1(a) ac-
cording to the syntactic parse in Fig. 4(b) as an
example. Given the appropriate possible predicate
attached to each word in Fig. 5(a), the node span-
ning position our player 5 has children, P POS and
P PLAYER, that are not in a predicate-argument re-
lation in the MR (see Fig. 4(a)).
To ensure meaning composition in this case,
we automatically create macro-predicates that
combine multiple predicates into one, so that
the children?s MRs can be composed as argu-
613
P DO
P PLAYER
P OUR P UNUM
P POS
P MIDFIELD
(a)
VP
ADVP
RB
then
VP
VP
VB
position
NP
our player 5
PP
IN
in
NP
DT
the
NN
midfield
(b)
Figure 4: Parses for the directive part of the CLANG in Fig. 1(a): (a) The predicate-argument structure
of the MR. (b) The parse of the NL (the parse of the phrase our player 5 is omitted for brevity).
ments to a macro-predicate. Fig. 5(b) shows
the macro-predicate P DO POS (DIRECTIVE?(do
PLAYER (pos REGION))) formed by merging the
P DO and P POS in Fig. 4(a). The macro-predicate
has two arguments, one of type PLAYER (a1)
and one of type REGION (a2). Now, P POS and
P PLAYER can be composed as arguments to this
macro-predicate as shown in Fig. 5(c). However,
it requires assuming a P DO predicate that has
not been formally introduced. To indicate this, a
lambda variable, p1, is introduced that ranges over
predicates and is provisionally bound to P DO, as
indicated in Fig. 5(c) using the notation p1:do.
Eventually, this predicate variable must be bound
to a matching predicate introduced from the lexi-
con. In the example, p1:do is eventually bound to
the P DO predicate introduced by the word then to
form a complete MR.
Macro-predicates are introduced as needed dur-
ing training in order to ensure that each MR in
the training set can be composed using the syn-
tactic parse of its corresponding NL given reason-
able assignments of predicates to words. For each
SAPT node that does not combine a predicate with
a legal argument, a macro-predicate is formed by
merging all predicates on the paths from the child
predicates to their lowest common ancestor (LCA)
in the MR parse. Specifically, a child MR be-
comes an argument of the macro-predicate if it
is complete (i.e. contains no ? variables); other-
wise, it also becomes part of the macro-predicate
and its ? variables become additional arguments
of the macro-predicate. For the node spanning po-
sition our player 5 in the example, the LCA of the
children P PLAYER and P POS is their immedi-
ate parent P DO, therefore P DO is included in the
macro-predicate. The complete child P PLAYER
becomes the first argument of the macro-predicate.
The incomplete child P POS is added to the macro-
predicate P DO POS and its ? variable becomes
another argument.
For improved generalization, once a predicate
in a macro-predicate becomes complete, it is re-
moved from the corresponding macro-predicate
label in the SAPT. For the node spanning position
our player 5 in the midfield in Fig. 5(a), P DO POS
becomes P DO once the arguments of pos are
filled.
In the following two sections, we describe the
two subtasks of inducing semantic knowledge and
a disambiguation model for this enhanced compo-
sitional framework. Both subtasks require a train-
ing set of NLs paired with their MRs. Each NL
sentence also requires a syntactic parse generated
using Bikel?s (2004) implementation of Collins
parsing model 2. Note that unlike SCISSOR (Ge
and Mooney, 2005), training our method does not
require gold-standard SAPTs.
5 Learning Semantic Knowledge
Learning semantic knowledge starts from learning
the mapping from words to predicates. We use
an approach based on Wong and Mooney (2006),
which constructs word alignments between NL
sentences and their MRs. Normally, word align-
ment is used in statistical machine translation to
match words in one NL to words in another; here
it is used to align words with predicates based on
a ?parallel corpus? of NL sentences and MRs. We
assume that each word alignment defines a possi-
ble mapping from words to predicates for building
a SAPT and semantic derivation which compose
the correct MR. A semantic lexicon and compo-
sition rules are then extracted directly from the
614
P DO
??a1?a2?P DO
then
?p1P DO POS = ?p1P DO
??p1?a2?P DO POS
?a1P POS
position
P PLAYER
our player 5
P MIDFIELD
NULL
in
P MIDFIELD
NULL
the
P MIDFIELD
midfield
(a) SAPT
P DO
a1:PLAYER P POS
a2:REGION
(b) Macro-Predicate P DO POS
(do (player our {5}) (pos (midfield)))
??a1?a2?(do a1a2)
then
?p1(p1:do (player our {5}) (pos (midfield)))
??p1?a2?(p1:do (player our {5}) (pos a2))
?a1(pos a1)
position
(player our {5})
our player 5
(midfield)
NULL
in
(midfield)
NULL
the
(midfield)
midfield
(c) Semantic Derivation
Figure 5: Semantic parse for the directive part of the example in Fig. 1(a) using the syntactic parse in
Fig. 4(b): (a) A SAPT with syntactic labels omitted for brevity. (b) The predicate-argument structure of
macro-predicate P DO POS (c) The semantic derivation of the MR.
nodes of the resulting semantic derivations.
Generation of word alignments for each train-
ing example proceeds as follows. First, each MR
in the training corpus is parsed using the MRLG.
Next, each resulting parse tree is linearized to pro-
duce a sequence of predicates by using a top-
down, left-to-right traversal of the parse tree. Then
the GIZA++ implementation (Och and Ney, 2003)
of IBM Model 5 is used to generate the five best
word/predicate alignments from the corpus of NL
sentences each paired with the predicate sequence
for its MR.
After predicates are assigned to words using
word alignment, for each alignment of a training
example and its syntactic parse, a SAPT is gener-
ated for composing the correct MR using the pro-
cesses discussed in Sections 3 and 4. Specifically,
a semantic label is assigned to each internal node
of each SAPT, so that the MRs of its children are
composed correctly according to the MR for this
example.
There are two cases that require special han-
dling. First, when a predicate is not aligned to any
word, the predicate must be inferred from context.
For example, in CLANG, our player is frequently
just referred to as player and the our must be in-
ferred. When building a SAPT for such an align-
ment, the assumed predicates and arguments are
simply bound to their values in the MR. Second,
when a predicate is aligned to several words, i.e. it
is represented by a phrase, the alignment is trans-
formed into several alignments where each predi-
cate is aligned to each single word in order to fit
the assumptions of compositional semantics.
Given the SAPTs constructed from the results
of word-alignment, a semantic derivation for each
training sentence is constructed using the methods
described in Sections 3 and 4. Composition rules
615
are then extracted from these derivations.
Formally, composition rules are of the form:
?1.P1 + ?2.P2 ? {?p.Pp, R} (1)
where P1, P2 and Pp are predicates for the left
child, right child, and parent node, respectively.
Each predicate includes a lambda term ? of
the form ??pi1 , . . . , ?pim , ?aj1 , . . . , ?ajn?, an un-
ordered set of all unbound predicate and argument
variables for the predicate. The component R
specifies how some arguments of the parent predi-
cate are filled when composing the MR for the par-
ent node. It is of the form: {ak1=R1, . . . , akl=Rl},
where Ri can be either a child (ci), or a child?s
complete argument (ci, aj) if the child itself is not
complete.
For instance, the rule extracted for the node for
player 2 in Fig. 3(b) is:
??a1?a2?.P PLAYER + P UNUM ? {?a1.P PLAYER, a2=c2},
and for position our player 5 in Fig. 5(c):
?a1.P POS + P PLAYER ? {??p1?a2?.P DO POS, a1=c2},
and for position our player 5 in the midfield:
??p1?a2?.P DO POS + P MIDFIELD
? {?p1.P DO POS, {a1=(c1,a1), a2=c2}}.
The learned semantic knowledge is necessary
for handling ambiguity, such as that involving
word senses and semantic roles. It is also used to
ensure that each MR is a legal string in the MRL.
6 Learning a Disambiguation Model
Usually, multiple possible semantic derivations for
an NL sentence are warranted by the acquired se-
mantic knowledge, thus disambiguation is needed.
To learn a disambiguation model, the learned se-
mantic knowledge (see Section 5) is applied to
each training example to generate all possible se-
mantic derivations for an NL sentence given its
syntactic parse. Here, unique word alignments are
not required, and alternative interpretations com-
pete for the best semantic parse.
We use a maximum-entropy model similar
to that of Zettlemoyer and Collins (2005) and
Wong and Mooney (2006). The model defines a
conditional probability distribution over semantic
derivations (D) given an NL sentence S and its
syntactic parse T :
Pr(D|S, T ; ??) = exp
?
i ?ifi(D)
Z??(S, T )
(2)
where f? (f1, . . . , fn) is a feature vector parame-
terized by ??, and Z??(S, T ) is a normalizing fac-
tor. Three simple types of features are used in
the model. First, are lexical features which count
the number of times a word is assigned a particu-
lar predicate. Second, are bilexical features which
count the number of times a word is assigned a
particular predicate and a particular word precedes
or follows it. Last, are rule features which count
the number of times a particular composition rule
is applied in the derivation.
The training process finds a parameter ??? that
(approximately) maximizes the sum of the condi-
tional log-likelihood of the MRs in the training set.
Since no specific semantic derivation for an MR is
provided in the training data, the conditional log-
likelihood of an MR is calculated as the sum of the
conditional probability of all semantic derivations
that lead to the MR. Formally, given a set of NL-
MR pairs {(S1,M1), (S2,M2), ..., (Sn,Mn)} and
the syntactic parses of the NLs {T1, T2, ..., Tn},
the parameter ??? is calculated as:
??? = argmax
??
n
?
i=1
log Pr(Mi|Si, Ti; ??) (3)
= argmax
??
n
?
i=1
log
?
D?i
Pr(D?i |Si, Ti; ??)
where D?i is a semantic derivation that produces
the correct MR Mi.
L-BFGS (Nocedal, 1980) is used to estimate the
parameters ???. The estimation requires statistics
that depend on all possible semantic derivations
and all correct semantic derivations of an exam-
ple, which are not feasibly enumerated. A vari-
ant of the Inside-Outside algorithm (Miyao and
Tsujii, 2002) is used to efficiently collect the nec-
essary statistics. Following Wong and Mooney
(2006), only candidate predicates and composi-
tion rules that are used in the best semantic deriva-
tions for the training set are retained for testing.
No smoothing is used to regularize the model; We
tried using a Gaussian prior (Chen and Rosenfeld,
1999), but it did not improve the results.
7 Experimental Evaluation
We evaluated our approach on two standard cor-
pora in CLANG and GEOQUERY. For CLANG,
300 instructions were randomly selected from
the log files of the 2003 ROBOCUP Coach
616
Competition and manually translated into En-
glish (Kuhlmann et al, 2004). For GEOQUERY,
880 English questions were gathered from vari-
ous sources and manually translated into Prolog
queries (Tang and Mooney, 2001). The average
sentence lengths for the CLANG and GEOQUERY
corpora are 22.52 and 7.48, respectively.
Our experiments used 10-fold cross validation
and proceeded as follows. First Bikel?s imple-
mentation of Collins parsing model 2 was trained
to generate syntactic parses. Second, a seman-
tic parser was learned from the training set aug-
mented with their syntactic parses. Finally, the
learned semantic parser was used to generate the
MRs for the test sentences using their syntactic
parses. If a test example contains constructs that
did not occur in training, the parser may fail to re-
turn an MR.
Wemeasured the performance of semantic pars-
ing using precision (percentage of returned MRs
that were correct), recall (percentage of test exam-
ples with correct MRs returned), and F-measure
(harmonic mean of precision and recall). For
CLANG, an MR was correct if it exactly matched
the correct MR, up to reordering of arguments
of commutative predicates like and. For GEO-
QUERY, an MR was correct if it retrieved the same
answer as the gold-standard query, thereby reflect-
ing the quality of the final result returned to the
user.
The performance of a syntactic parser trained
only on the Wall Street Journal (WSJ) can de-
grade dramatically in new domains due to cor-
pus variation (Gildea, 2001). Experiments on
CLANG and GEOQUERY showed that the perfor-
mance can be greatly improved by adding a small
number of treebanked examples from the corre-
sponding training set together with the WSJ cor-
pus. Our semantic parser was evaluated using
three kinds of syntactic parses. Listed together
with their PARSEVAL F-measures these are:
gold-standard parses from the treebank (GoldSyn,
100%), a parser trained on WSJ plus a small
number of in-domain training sentences required
to achieve good performance, 20 for CLANG
(Syn20, 88.21%) and 40 for GEOQUERY (Syn40,
91.46%), and a parser trained on no in-domain
data (Syn0, 82.15% for CLANG and 76.44% for
GEOQUERY).
We compared our approach to the following al-
ternatives (where results for the given corpus were
Precision Recall F-measure
GOLDSYN 84.73 74.00 79.00
SYN20 85.37 70.00 76.92
SYN0 87.01 67.00 75.71
WASP 88.85 61.93 72.99
KRISP 85.20 61.85 71.67
SCISSOR 89.50 73.70 80.80
LU 82.50 67.70 74.40
Table 2: Performance on CLANG.
Precision Recall F-measure
GOLDSYN 91.94 88.18 90.02
SYN40 90.21 86.93 88.54
SYN0 81.76 78.98 80.35
WASP 91.95 86.59 89.19
Z&C 91.63 86.07 88.76
SCISSOR 95.50 77.20 85.38
KRISP 93.34 71.70 81.10
LU 89.30 81.50 85.20
Table 3: Performance on GEOQUERY.
available): SCISSOR (Ge and Mooney, 2005), an
integrated syntactic-semantic parser; KRISP (Kate
and Mooney, 2006), an SVM-based parser using
string kernels; WASP (Wong and Mooney, 2006;
Wong and Mooney, 2007), a system based on
synchronous grammars; Z&C (Zettlemoyer and
Collins, 2007)3, a probabilistic parser based on re-
laxed CCG grammars; and LU (Lu et al, 2008),
a generative model with discriminative reranking.
Note that some of these approaches require ad-
ditional human supervision, knowledge, or engi-
neered features that are unavailable to the other
systems; namely, SCISSOR requires gold-standard
SAPTs, Z&C requires hand-built template gram-
mar rules, LU requires a reranking model using
specially designed global features, and our ap-
proach requires an existing syntactic parser. The
F-measures for syntactic parses that generate cor-
rect MRs in CLANG are 85.50% for syn0 and
91.16% for syn20, showing that our method can
produce correct MRs even when given imperfect
syntactic parses. The results of semantic parsers
are shown in Tables 2 and 3.
First, not surprisingly, more accurate syntac-
tic parsers (i.e. ones trained on more in-domain
data) improved our approach. Second, in CLANG,
all of our methods outperform WASP and KRISP,
which also require no additional information dur-
ing training. In GEOQUERY, Syn0 has signifi-
cantly worse results than WASP and our other sys-
tems using better syntactic parses. This is not sur-
prising since Syn0?s F-measure for syntactic pars-
ing is only 76.44% in GEOQUERY due to a lack
3These results used a different experimental setup, train-
ing on 600 examples, and testing on 280 examples.
617
Precision Recall F-measure
GOLDSYN 61.14 35.67 45.05
SYN20 57.76 31.00 40.35
SYN0 53.54 22.67 31.85
WASP 88.00 14.37 24.71
KRISP 68.35 20.00 30.95
SCISSOR 85.00 23.00 36.20
Table 4: Performance on CLANG40.
Precision Recall F-measure
GOLDSYN 95.73 89.60 92.56
SYN20 93.19 87.60 90.31
SYN0 91.81 85.20 88.38
WASP 91.76 75.60 82.90
SCISSOR 98.50 74.40 84.77
KRISP 84.43 71.60 77.49
LU 91.46 72.80 81.07
Table 5: Performance on GEO250 (20 in-domain
sentences are used in SYN20 to train the syntactic
parser).
of interrogative sentences (questions) in the WSJ
corpus. Note the results for SCISSOR, KRISP and
LU on GEOQUERY are based on a different mean-
ing representation language, FUNQL, which has
been shown to produce lower results (Wong and
Mooney, 2007). Third, SCISSOR performs better
than our methods on CLANG, but it requires extra
human supervision that is not available to the other
systems. Lastly, a detailed analysis showed that
our improved performance on CLANG compared
to WASP and KRISP is mainly for long sentences
(> 20 words), while performance on shorter sen-
tences is similar. This is consistent with their
relative performance on GEOQUERY, where sen-
tences are normally short. Longer sentences typ-
ically have more complex syntax, and the tradi-
tional syntactic analysis used by our approach re-
sults in better compositional semantic analysis in
this situation.
We also ran experiments with less training data.
For CLANG, 40 random examples from the train-
ing sets (CLANG40) were used. For GEOQUERY,
an existing 250-example subset (GEO250) (Zelle
and Mooney, 1996) was used. The results are
shown in Tables 4 and 5. Note the performance
of our systems on GEO250 is higher than that
on GEOQUERY since GEOQUERY includes more
complex queries (Tang and Mooney, 2001). First,
all of our systems gave the best F-measures (ex-
cept SYN0 compared to SCISSOR in CLANG40),
and the differences are generally quite substantial.
This shows that our approach significantly im-
proves results when limited training data is avail-
able. Second, in CLANG, reducing the training
data increased the difference between SYN20 and
SYN0. This suggests that the quality of syntactic
parsing becomes more important when less train-
ing data is available. This demonstrates the advan-
tage of utilizing existing syntactic parsers that are
learned from large open domain treebanks instead
of relying just on the training data.
We also evaluated the impact of the word align-
ment component by replacing Giza++ by gold-
standard word alignments manually annotated for
the CLANG corpus. The results consistently
showed that compared to using gold-standard
word alignment, Giza++ produced lower seman-
tic parsing accuracy when given very little training
data, but similar or better results when given suf-
ficient training data (> 160 examples). This sug-
gests that, given sufficient data, Giza++ can pro-
duce effective word alignments, and that imper-
fect word alignments do not seriously impair our
semantic parsers since the disambiguation model
evaluates multiple possible interpretations of am-
biguous words. Using multiple potential align-
ments from Giza++ sometimes performs even bet-
ter than using a single gold-standard word align-
ment because it allows multiple interpretations to
be evaluated by the global disambiguation model.
8 Conclusion and Future work
We have presented a new approach to learning a
semantic parser that utilizes an existing syntactic
parser to drive compositional semantic interpre-
tation. By exploiting an existing syntactic parser
trained on a large treebank, our approach produces
improved results on standard corpora, particularly
when training data is limited or sentences are long.
The approach also exploits methods from statisti-
cal MT (word alignment) and therefore integrates
techniques from statistical syntactic parsing, MT,
and compositional semantics to produce an effec-
tive semantic parser.
Currently, our results comparing performance
on long versus short sentences indicates that our
approach is particularly beneficial for syntactically
complex sentences. Follow up experiments us-
ing a more refined measure of syntactic complex-
ity could help confirm this hypothesis. Reranking
could also potentially improve the results (Ge and
Mooney, 2006; Lu et al, 2008).
Acknowledgments
This research was partially supported by NSF
grant IIS?0712097.
618
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Patrick Blackburn and Johan Bos. 2005. Represen-
tation and Inference for Natural Language: A First
Course in Computational Semantics. CSLI Publica-
tions, Stanford, CA.
Xavier Carreras and Luis Marquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proc. of 8th Conf. on Computational
Natural Language Learning (CoNLL-2004), Boston,
MA.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy model.
Technical Report CMU-CS-99-108, School of Com-
puter Science, Carnegie Mellon University.
Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros
Kapetanakis, Kostas Kostiadis, Johan Kummeneje,
Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-
fens, Yi Wang, and Xiang Yin. 2003. Users
manual: RoboCup soccer server manual for soccer
server version 7.07 and later. Available at http://
sourceforge.net/projects/sserver/.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Ruifang Ge and Raymond J. Mooney. 2005. A statisti-
cal semantic parser that integrates syntax and seman-
tics. In Proc. of 9th Conf. on Computational Natural
Language Learning (CoNLL-2005), pages 9?16.
Ruifang Ge and Raymond J. Mooney. 2006. Dis-
criminative reranking for semantic parsing. In Proc.
of the 21st Intl. Conf. on Computational Linguis-
tics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL-06),
Sydney, Australia, July.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc. of the 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), Pittsburgh, PA, June.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proc. of the 21st Intl. Conf. on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL-06),
pages 913?920, Sydney, Australia, July.
Greg Kuhlmann, Peter Stone, Raymond J. Mooney, and
Jude W. Shavlik. 2004. Guiding a reinforcement
learner with natural language advice: Initial results
in RoboCup soccer. In Proc. of the AAAI-04 Work-
shop on Supervisory Control of Learning and Adap-
tive Systems, San Jose, CA, July.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proc. of the Conf. on Empirical Methods in Natu-
ral Language Processing (EMNLP-08), Honolulu,
Hawaii, October.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc.
of Human Language Technology Conf.(HLT-2002),
San Diego, CA, March.
Jorge Nocedal. 1980. Updating quasi-Newton matri-
ces with limited storage. Mathematics of Computa-
tion, 35(151):773?782, July.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proc. of the
12th European Conf. on Machine Learning, pages
466?477, Freiburg, Germany.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proc. of Human Language
Technology Conf. / N. American Chapter of the
Association for Computational Linguistics Annual
Meeting (HLT-NAACL-2006), pages 439?446.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proc. of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL-07), pages 960?967.
Yuk Wah Wong. 2007. Learning for Semantic Pars-
ing and Natural Language Generation Using Statis-
tical Machine Translation Techniques. Ph.D. the-
sis, Department of Computer Sciences, University of
Texas, Austin, TX, August. Also appears as Artifi-
cial Intelligence Laboratory Technical Report AI07-
343.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of 13th Natl. Conf. on Artifi-
cial Intelligence (AAAI-96), pages 1050?1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of the 21th Annual Conf. on Un-
certainty in Artificial Intelligence (UAI-05).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proc. of the 2007 Joint Conf. on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing (EMNLP-CoNLL-07), pages 678?687, Prague,
Czech Republic, June.
619
Automated Construction of Database Interfaces: Integrating 
Statistical and Relational Learning for Semantic Parsing 
Lappoon R .  Tang and  Raymond J .  Mooney  
Department of Computer Sciences 
University of Texas at Austin 
Austin, TX 78712-1188 
{rupert, mooney}@cs, utexas, edu 
Abst ract  
The development of natural language inter- 
faces (NLI's) for databases has been a chal- 
lenging problem in natural anguage process- 
ing (NLP) since the 1970's. The need for 
NLI's has become more pronounced due to the 
widespread access to complex databases now 
available through the Internet. A challenging 
problem for empirical NLP is the automated 
acquisition of NLI's from training examples. 
We present a method for integrating statisti- 
cal and relational learning techniques for this 
task which exploits the strength of both ap- 
proaches. Experimental results from three dif- 
ferent domains uggest that such an approach 
is more robust than a previous purely logic- 
based approach. 
1 In t roduct ion  
We use the term semantic parsing to refer 
to the process of mapping a natural anguage 
sentence to a structured meaning representa- 
tion. One interesting application of semantic 
parsing is building natural language interfaces 
for online databases. The need for such appli- 
cations is growing since when information is 
delivered through the Internet, most users do 
not know the underlying database access lan- 
guage. An example of such an interface that 
we have developed is shown in Figure 1. 
Traditional (rationalist) approaches to con- 
structing database interfaces require an ex- 
pert to hand-craft an appropriate semantic 
parser (Woods, 1970; Hendrix et al, 1978). 
However, such hand-crafted parsers are time 
consllming to develop and suffer from prob- 
lems with robustness and incompleteness even 
for domain specific applications. Neverthe- 
less, very little research in empirical NLP has 
explored the task of automatically acquiring 
such interfaces from annotated training ex- 
amples. The only exceptions of which we 
are aware axe a statistical approach to map- 
ping airline-information queries into SQL pre- 
sented in (Miller et al, 1996), a probabilistic 
decision-tree method for the same task de- 
scribed in (Kuhn and De Mori, 1995), and 
an approach using relational learning (a.k.a. 
inductive logic programming, ILP) to learn a 
logic-based semantic parser described in (Zelle 
and Mooney, 1996). 
The existing empirical systems for this task 
employ either a purely logical or purely sta- 
tistical approach. The former uses a deter- 
ministic parser, which can suffer from some 
of the same robustness problems as rational- 
ist methods. The latter constructs a prob- 
abilistic grammar, which requires supplying 
a sytactic parse tree as well as a semantic 
representation foreach training sentence, and 
requires hand-crafting a small set of contex- 
tual features on which to condition the pa- 
rameters of the model. Combining relational 
and statistical approaches can overcome the 
need to supply parse-trees and hand-crafted 
features while retaining the robustness of sta- 
tistical parsing. The current work is based 
on the CHILL logic-based parser-acquisition 
framework (Zelle and Mooney, 1996), retain- 
ing access to the complete parse state for mak- 
ing decisions, but building a probabilistic re- 
lational model that allows for statistical pars- 
ing- 
2 Overv iew of  the  Approach  
This section reviews our overall approach 
using an interface developed for a U.S. 
Geography database (Geoquery) as a 
sample application (ZeUe and Mooney, 
1996) which is available on the Web (see 
hl:tp://gvg, cs .  u tezas ,  edu/users/n~./geo .html). 
2.1 Semant ic  Representat ion  
First-order logic is used as a semantic repre- 
sentation language. CHILL has also been ap- 
plied to a restaurant database in which the 
logical form resembles SQL, and is translated 
133 
Damba~ 
QUERY YOU PO~TED: 
all a goo~ ~ z~caL~ ~m ~o ~.t~o'P 
RE~UI.T: 
~ o o a ~ e ~  I,~ p .~. , .~r  ~,~o~o ~ J 
~u~,,o~ ",,~u.,. p~o ~.~. , ,?Bo  ~ ~,.~.o ,~.~o ~.~ I 
a~ooo~z~u~r~ ~ ~rr~r  ~,~o~.~o ~ I 
THE SOL GENERATED: 
~n0~t ~.K~t  ~Fo, LOCAnONS 
C~*t l~rOJ t~ ~ Z3 AgO 
Figure 1: Screenshots ofa Learned Web-based NL Database Interface 
automatically into SQL (see Figure 1). We 
explain the features of the Geoquery repre- 
sentation language through a sample query: 
Input: "W'hat is the largest city in Texas?" 
Quc~'y: a nswer(C,largest(C,(city(C),loc(C,S), 
const (S,stateid (texas))))). 
Objects are represented as logical terms and 
are typed with a semantic category using 
logical functions applied to possibly ambigu- 
ous English constants (e.g. stateid(Mississippi), 
riverid(Mississippi)). Relationships between ob- 
jects are expressed using predicates; for in- 
stance, Ioc(X,Y) states that X is located in Y. 
We also need to handle quantifiers uch 
as 'largest'. We represent these using meta- 
predicates for which at least one argument isa 
conjunction ofliterals. For example, largest(X, 
Goal) states that the object X satisfies Goal 
and is the largest object that does so, using 
the appropriate measure of size for objects of 
its type (e.g. area for states, population for 
cities). Finally, an nn.qpeci~ed object required 
as an argument to a predicate can appear else- 
where in the sentence, requiring the use of the 
predicate const(X,C) to bind the variable X to 
the constant C. Some other database queries 
(or training examples) for the U.S. Geography 
domain are shown below: 
What is the capital of Texas? 
a nswer(C,(ca pital(C,S),const(S,stateid (texas)))). 
What state has the most rivers running through it? 
a nswer(S,most (S,R,(state(S),rlver(R),traverse(R,S)))). 
2.2 Parsing Actions 
Our semantic parser employs a shift-reduce 
architecture that maintains a stack of pre- 
viously built semantic constituents and a 
buffer of remaining words in the input. The 
parsing actions are automatically generated 
from templates given the training data. The 
templates are INTRODUCE, COREF_VABS, 
DROP_CON J, LIFT_CON J, and SttIFT. IN- 
TRODUCE pushes apredicate onto the stack 
based on a word appearing in the input and 
information about its possible meanings in 
the lexicon. COREF_VARS binds two argu- 
ments of two different predicates on the stack. 
DROP_CONJ  (or L IFT_CON J) takes a pred- 
icate on the stack and puts it into one of the 
arguments of a meta-predicate on the stack. 
SH IFT  simply pushes a word from the input 
buffer onto the stack. The parsing actions are 
tried in exactly this order. The parser also 
requires a lexicon to map phrases in the in- 
put into specific predicates, this lexicon can 
also be learned automatically from the train- 
ing data (Thompson and Mooney, 1999). 
Let's go through a simple trace of parsing 
the request "What is the capital of Texas?" 
A lexicon that maps 'capital' to 'capital(_,_)' 
and 'Texas' to 'const(_,stateid(texas))' su.~ces 
134 
here. Interrogatives like "what" may be 
mapped to predicates in the lexicon if neces- 
sary. The parser begins with an initial stack 
and a buffer holding the input sentence. Each 
predicate on the parse stack has an attached 
buffer to hold the context in which it was 
introduced; words from the input sentence 
are shifted onto this buffer during parsing. 
The initial parse state is shown below: 
Parse Stack: \[answer(_,_):O\] 
Input Buffer: \[what,is,the,ca pital,of,texas,?\] 
Since the first three words in the input 
buffer do not map to any predicates, three 
SHIFT actions are performed. The next is an 
INTRODUCE as 'capital' is at the head of 
the input buffer: 
Parse Stack: \[capital(_,_): O, answer(_,_):\[the,is,what\]\] 
Input Buffer: \[capital,of,texas,?\] 
The next action is a COREF_VARS that 
binds the first argument of capital(_,_) with 
the first argument of answer(_,_). 
Parse Stack: \[capital(C,_): O, answer(C,_):\[the,is,what\]\] 
Input Buffer: \[capital,of,texas,?\] 
The next sequence of steps axe two SHIFT's, 
an INTRODUCE,  and then a COR.EF_VARS: 
Parse Stack: \[const(S,stateid(texas)): 0' 
ca pital(C,S):\[of, ca pital\], 
answer(C,_):\[the,is,what~ 
Input Buffer: \[texas,?\] 
The last four steps are two DROP_CONJ's 
followed by two SHIFT's: 
Parse Stack: \[answer(C, (capital(C,S), 
const(S,stateld(texas)))): 
\[?,texas,of, ca pital,the,is,what\]\] 
Input Buffer: I\] 
This is the final state and the logical query is 
extracted from the stack. 
2.3 Learning Control Rules 
The initially constructed parser has no con- 
straints on when to apply actions, and is 
therefore overly general and generates n11rner- 
ous spurious parses. Positive and negative ex- 
amples are collected for each action by parsing 
each tralnlng example and recordlng the parse 
states encountered. Parse states to which an 
action should be applied (i.e. the action leads 
to building the correct semantic representa- 
tion) are labeled positive examples for that 
action. Otherwise, a parse state is labeled a 
negative xample for an action if it is a posi- 
tive example for another action below the cur- 
rent one in the ordered list of actions. Control 
conditions which decide the correct action for 
a given parse state axe learned for each action 
from these positive and negative xamples. 
The initial CHILL system used ILP (Lavrac 
and Dzeroski, 1994) to learn Prolog control 
rules and employed eterministic parsing, us- 
ing the learned rules to decide the appropriate 
parse action for each state. The current ap- 
proach learns a model for estimating the prob- 
ability that each action should be applied to 
a given state, and employs tatistical parsing 
(Manning and Schiitze, 1999) to try to find 
the overall most probable parse, using beam 
search to control the complexity. The advan- 
tage of ILP is that it can perform induction 
over the logical description of the complete 
parse state without he need to pre-engineer a 
fixed set of features (which vary greatly from 
one domain to another) that are relevant o 
making decisions. We maintain this advan- 
tage by using ILP to learn a committee of 
hypotheses, and basing probability estimates 
on a weighted vote of them (Ali and Pazzani, 
1996). We believe that using such a proba- 
bilistic relational model (Getoor and Jensen, 
2000) combines the advantages of frameworks 
based on first-order logic and those based on 
standard statistical techniques. 
3 The  TABULATE ILP  Method 
This section discusses the ILP method used to 
build a committee of logical control hypothe- 
ses for each action. 
3.1 The Basic TABULATE Algorithm 
Most ILP methods use a set-covering method 
to learn one clause (rule) at a time and con- 
struct clauses using either a strictly top-down 
(general to specific) or bottom-up (specific to 
general) search through the space of possi- 
ble rules (Lavrac and Dzeroski, 1994). TAB- 
ULATE, 1 on the other hand, employs both 
bottom-up and top-down methods to con- 
struct potential clauses and searches through 
the hypothesis pace of complete logic pro- 
grams (i.e. sets of clauses called theories). It 
uses beam search to find a set of alternative 
hypotheses guided by a theory evaluation met- 
ric discussed below. The search starts with 
aTABULATB stands for Top-doera And Bottom-Up 
cLAuse construction urith Theory Evaluation. 
135 
Procedure Tabulate 
Input: 
t(X,,. . . ,Xn): the target concept to learn 
~+: the (B examples 
~-: the (9 examples 
Output: 
Q: a queue of learned theories 
Theoryo := {E '?'-I E E ~+} /* the initial theory */ 
T(No) := Theoryo /* theory of node No */ 
C(No) := empty /* the clause being built */ 
Q := \[No\] /* the search queue */ 
Repeat 
CO ? 
Fo_._X each search node Ni E Q D__q 
C(Ni) = empty or C(Ni) = fail Then 
Pairs := sampling of S pairs of clauses from T(N~) 
Find LGG G in Pairs with the greatest cover in ~+ 
Ri := Refine_Clause(t(X1,... ,Xn) +-) U 
Refine_Clause( G ~--) 
Else 
R4 := Reflne_Clause(C(Ni)) 
End I f  
I f  Ri ---- ? Then 
CQ, := {(T(N,), fail)} 
Else 
CQi := {(Coraplete(T(N,), Gj, ~+), neztj) \[ 
for each G~ E ,,~, next~ = empty if Gj 
satisfies the noise criteria; otherwise, G$} 
End I f  
CQ :=  CQ u CQ~ 
End For 
Q := the B best nodes from Q U CQ 
ranked by metric M 
Unt i l  terminatlon-criteria-satisfied 
Return Q 
End Procedure 
Figure 2: The TABULATE algorithm 
the most specific hypothesis (the set of posi- 
tive examples each represented as a separate 
clause). Each iteration of the loop attempts 
to refine each of the hypotheses in the current 
search queue. There are two cases in each it- 
eration: 1) an existing clause in a theory is 
refined or 2) a new clause is begun. Clauses 
are learned using both top-down specialiT.~- 
tion using a method similar to FOIL (Quin- 
lan, 1990) and bottom-up generalization using 
Least General Generalizations (LGG's). Ad- 
vantages of combining both ILP approaches 
were explored in CHILLIN (ZeUe and Mooney, 
1994), an ILP method which motivated the 
design of TABULATE. An outline of the TAB- 
ULATE algorithm is given in Figure 2. 
A noise-handling criterion is used to de- 
cide when an individual clause in a hypoth- 
esis is sufficiently accurate to be permanently 
retained. There are three possible outcomes 
in a refinement: 1) the current clause satisfies 
the noise-handling criterion and is simply re- 
turned (nextj is set to empty), 2) the current 
clause does not satisfy the noise-handling cri- 
teria and all possible refinements are returned 
(neztj is set to the refined clause), and 3) 
the current clause does not satisfy the noise- 
handling criterion but there are no further e- 
finements (neztj is set to fai O. If the refine- 
ment is a new clause, clauses in the current 
theory subs-reed by it are removed. Oth- 
erwise, it is a specialization of an existing 
clause. Positive examples that are not cov- 
ered by the resulting theory, due to special- 
izing the clause, are added back into theory 
as individual clauses. Hence, the theory is al- 
ways maintained complete (i.e. covering all 
positive examples). These final steps are per- 
formed in the Complete procedure. 
The termination criterion checks for two 
conditions. The first is satisfied if the next 
search queue does not improve the sum of 
the metric score over all hypotheses in the 
queue. Second, there is no clause currently 
being built for each theory in the search queue 
and the last finished clause of each theory sat- 
isfies the noise-handling criterion. Finally, a 
committee of hypotheses found by the algo- 
rithm is returned. 
3.2  Compress ion  and  Accuracy  
The goal of the search is to find accurate 
and yet simple hypotheses. We measure accu- 
racy using the m-estimate (Cestnik, 1990), a 
smoothed measure of accuracy on the training 
data which in the case of a two-class problem 
is defined as: 
accuracy(H) s + m.  p+ = (1) 
n ,-I- rrt 
where s is the n-tuber of positive examples 
covered by the hypothesis H,  n is the total 
number of examples covered, p+ is the prior 
probability of the class (9, and m is a smooth- 
ing parameter. 
We measure theory complexity using a met- 
ric slmi\]ar to that introduced in (Muggleton 
and Buntine, 1988). The size of a Clause hav- 
ing a Head and a Body is defined as follows 
(ts="term size" and ar="ar i ty ' ) :  
size(Clause) = 1 + ts(Head) + ts(Body) (2) 
136 
I 1 T is a variable 
ts(T) = 2 r ~,, ?o~t  
2 + ts(argi(T)) 
(3) 
The size of a clause is roughly the n,,mber of 
variables, constants, or predicate symbols it 
contains. The size of a theory is the sum of 
the sizes of its clauses. The metric M(H) used 
as the search heuristic is defined as: 
M(H) = accuracy(H) + C 
log 2 size(H) (4) 
where C is a constant used to control the rel- 
ative weight of accuracy vs. complexity. We 
ass~,me that the most general hypothesis i as 
good as the most specific hypothesis; thus, C 
is determined to be: 
C = EbSt -- EtSb (5) 
&-& 
where Et, Eb are the accuracy estimates of the 
most general and most specific hypotheses re- 
spectively, and St, Sb are their sizes. 
3.3 Noise Handl ing 
A clause needs no further refinement when it 
meets the following criterion (as in RIPPER 
(Cohen, 1995)): 
P -.__.2_ > (6) 
p+n 
where p is the number of positive examples 
covered by the clause, n is the number of neg- 
ative examples covered and -1  </~ _< 1 is a 
parameter. The value of ~ is decreased when- 
ever the sum of the metric over the hypotheses 
in the queue does not improve although some 
of them still have ,nflni~hed or failed clauses. 
4 Statistical Semantic Parsing 
4.1 The  Pars ing  Mode l  
A parser is a relation Parser C_ Sentences x 
Queries where Sentences and Queries are 
the sets of natural language sentences and 
database queries respectively. Given a sen- 
tence I ? Sentences, the set Q(1) = {q ? 
Queries I (l, q) ? Parser} is the set of queries 
that are correct interpretations of I. 
A parse state consists of a stack of lexical- 
ized predicates and a list of words from the 
input sentence. S is the set of states reach- 
able by the parser. Suppose our learned parser 
has n different parsing actions, the ith ac- 
tion a / i s  a function a/(s) : ISi -+ OSi where 
ISi G S is the set of states to which the ac- 
tion is applicable and OSi C_ S is the set of 
states constructed by the action. The function 
ao(l) : Sentences ~ IniS maps each sentence l 
to a corresponding unique initial parse state in 
In/S C_ S. A state is called afinalstate if there 
exists no parsing action applicable to it. The 
partial function a,+l(s) : FS ~ Queries is 
defined as the action that retrieves the query 
from the final state s 6 FS C S if one exists. 
Some final states may not "contain" a query 
(e.g. when the parse stack contain.q predicates 
with unbound ~rariables) and therefore it is a 
partial function. When the parser meets such 
a final state, it reports a failure. 
A path is a finite sequence of parsing ac- 
tions. Given a sentence 1, a good state s is 
one such that there exists a path from it to a 
query q 6 Q(1). Otherwise, it is a bad state. 
The set of parse states can be uniquely divided 
into the set of good states and the set of bad 
states given l and Parser. S + and S-  are the 
sets of good and bad states respectively. 
Given a sentence l, the goal is to construct 
the query ~ such that 
= argmqaX P(q ? Q(l) \ [ l  ~ q) (7) 
where I ~ q means a path exists from l to q. 
Now, we need to estimate P(q ? Q(1) I l =-~ 
q). First, we notice that: 
P(q ? Q(1) \[l =~. q) ---- (8) 
P(s ? FS + I I ~ s and an+l(S) ---- q) 
where FS + = FS N S +. For notational con- 
venience we drop the conditions and denote 
the above probabilities as P(q ? Q(l)) and 
P(s ? FS +) respectively, assuming these con- 
ditions in the following discussion. The equa- 
tion states that the probability that a given 
query is a correct meaning for I is the same as 
the probability that the final state (reached 
by parsing l) is a good state. We need to de- 
termine in general the probability of having a 
good resulting parse state. Given any parse 
state s i at the j th  step of parsing and an ac- 
tion ai such that si+1 = a/(sj), we have: 
PCsi+1 ? (9) 
pCsj+l e o& + I ? x&+)pCs  ? x& +) + 
P(Si+l ? OSi + I sj ? ISi+)P(sj ?~ ISi +) 
where IS~ = ISi N S + and OS~ = OS~ N S +. 
Since no parsing action can produce a good 
137 
parse state from a bad one, the second term 
is zero. Now, we are ready to derive P(q ? 
Q(l)). Suppose q = an+l(Sm), we have: 
P(q 6 Q(l)) (10) 
= P(s~ ? F~)  
. . .  
= P(s,n ? FS  + l sm-1 ?/St,_a)... 
P(s~ ? OS~_, I sj-1 ? I s~_,) . . .  
P(s2 ? Ob~, Is1 ? IS~, )P ( ' I  ? IS~,) 
where ak denotes the index of which action 
is applied at the kth step. We assume that 
= P(sl  ? I~aa) ~ 0 (which may not be true 
in general). Now, we have 
m--I 
P(q 6 Q(l)) = 7 I I  P(sj+I ? O~ l sj ? IS~-3). 
i=l 
(11) 
Next we describe how we estimate the proba- 
bi l i~ of the goodness of each action in a given 
state (P(~(s) ? o$  I s ? I~) ) .  We n~ 
not estimate 7 since its value does not affect 
the outcome of equation (7). 
4.2 Es t imat ing  Probab i l i t i es  for 
Pars ing  Act ions  
The committee of hypotheses learned by TAB- 
ULATE is used to est imate the probability that 
a particular action is a good one to apply to a 
given parse state. Some hypotheses are more 
"important" than others in the sense that they 
carry more weight in the decision. A weight- 
ing parameter is also included to lower the 
probability estimate of actions that appear 
fm'ther down the decision list. For actions ai 
where 1 < i < n - 1: 
P(ai(s) ? o~ Is ? Is7-) = 
.po,(i)-I ~ AkP(~Cs) 60b~ ~ I h~) 
hk~H~ 
(12) 
where s is a given parse state, pos(i) is the 
position of the action ai in the list of ac- 
tions applicable to state s, Ak and 0 < /~ < 
1 are weighting parameters, z Hi is the set 
of hypotheses learned for the action ai, and 
~k A~ = 1. 
To estimate the probability for the last ac- 
tion an, we devise a simple test that checks 
if the maximum of the set A(s) of proba- 
bility estimates for the subset of the actions 
2p is set to  0.95 for all the experiments performed. 
{a l , . . . ,  an-l} applicable to s is less than or 
equal to a threshold a. If A(s) is empty, we 
assume the maxlrn,,rn is zero. More precisely, 
PCa.Cs) ? os~ Is ? xs~) = 
{ ,c..(,)~os~) if maxCACs)) < ~(,~IS~) 
0 otherwise 
(13) 
where a is the threshold, 3 c(an(s) ? Ob~) is 
the count of the number of good states pro- 
duced by the last action, and c(s ? IS~) is the 
count of the number of good states to which 
the last action is applicable. 
Now, let's discuss how P(ai(s) ? OS~  I hk) 
and Ak are estimated. If hk ~ s (i.e. hk covers 
s), we have 
PCai(s) ? o~ I hk) = pc + O " nc 
Pc -t- nc 
(14) 
where Pc and ne are the number of positive 
and negative xamples covered by hk respec- 
tively. Otherwise, if h~ ~= s (i.e. hk does not 
cover s), we have 
PCai(s) ? OS 7" I hk) -- p" + 8 .n , ,  
Pu +nu 
(15) 
where Pu and nu are the n,,rnber of positive 
and negative xamples rejected by hk respec- 
tively. /9 is the probability that a negative 
example is mislabelled and its value can be 
estimated given # (in equation (6)) and the 
total nnrnber of positive and negative xam- 
ples. 
One could use a variety of linear combina- 
tion methods to estimate the weights Ak (e.g. 
Bayesian combination (Buntine, 1990)). How- 
ever, we have taken a simple approach and 
weighted hypotheses based on their relative 
simplicity: 
size(hk) -1 
Ak = ~.lHd size(hi)_1" (16) z-d=l 
4.3 Search ing  for a Parse  
To find the most probably correct parse, the 
parser employs a beam search. At each step, 
the parser finds all of the parsing actions ap- 
plicable to each parse state on the queue and 
calculates the probability of goodness of each 
of them using equations (12) and (13). It then 
SThe threshold is set to 0.5 for all the experiments 
performed. 
138 
computes the probability that the resulting 
state of each possible action is a good state 
using equation (11), sorts the queue of possi- 
ble next states accordingly, and keeps the best 
B options. The parser stops when a complete 
parse is found on the top of the parse queue 
or a failure is reported. 
5 Exper imenta l  Resu l t s  
5.1 The Domains  
Three different domains are used to demon- 
strate the performance of the new approach. 
The first is the U.S. Geography domain. 
The database contains about 800 facts about 
U.S. states like population, area, capital city, 
neighboring states, major rivers, major cities, 
and so on. A hand-crafted parser, GEOBASE 
was previously constructed for this domain as 
a demo product for Turbo Prolog. The second 
application is the restaurant query system il- 
lustrated in Figure 1. The database contains 
information about thousands of restaurants 
in Northern California, including the name of 
the restaurant, its location, its specialty, and a 
guide-book rating. The third domain consists 
of a set of 300 computer-related jobs automat- 
ically extracted from postings to the USENET 
newsgroup aust in . jobs .  The database con- 
talus the following information: the job title, 
the company, the recruiter, the location, the 
salary, the languages and platforms used, and 
required or desired years of experience and de- 
grees. 
5.2 Exper imenta l  Des ign 
The geography corpus contains 560 questions. 
Approximately 100 of these were collected 
from a log of questions ubmitted to the web 
site and the rest were collected in studies in- 
volving students in undergraduate classes at 
our university. We also included results for the 
subset of 250 sentences originally used in the 
experiments reported in (Zelle and Mooney, 
1996). The remaining questions were specif- 
icaUy collected to be more complex than the 
original 250, and generally require one or more 
meta-predicates. The restaurant corpus con- 
taln~ 250 questions automatically generated 
from a hand-built grammar  Constructed to re- 
flect typical queries in this domain. The job 
corpus contains 400 questions automatically 
generated in a similar fashion. The beam 
width for TABULATE was set~ to five for all the 
domains. The deterministic parser used only 
the best hypothesis found. The experiments 
were conducted using 10-fold cross validation. 
For each domain, the average recall (a.k.a. 
accuracy) and precision of the parser on dis- 
joint test data are reported where: 
of correct queries produced 
Recal l  = 
of test sentences 
Prec is ion = # of correct queries produced 
# of complete parses produced" 
A complete parse is one which contains an ex- 
ecutable query (which could be incorrect). A 
query is considered correct if it produces the 
same answer set as the gold-standard query 
supplied with the corpus. 
5.3 Resu l ts  
The results are presented in Table 1 and Fig- 
ure 3. By switching from deterministic to 
probabilistic parsing, the system increased the 
number of correct queries it produced. Re- 
call increases almost monotonically with pars- 
ing beam width in most of the domains. I_m- 
provement is most apparent in the Jobs do- 
maln where probabilistic parsing signiBcantly 
outperformed the deterministic system (80% 
vs 68%). However, using a beam width of 
one (and thus the probabilistic parser picks 
only the best action) results in worse perfor- 
mance than using the original purely logic- 
based determlni~tic parser. This suggests that 
the probability esitmates could be improved 
since overall they are not indicating the sin- 
gle best action as well as a non-probabilistic 
approach. Precision of the system decreased 
with beam width, but not signi~cantly except 
for the larger Geography corpus. Since the 
system conducts a more extensive search for 
a complete parse, it risks increasing the num- 
ber of incorrect as well as correct parses. The 
importance of recall vs. precision depends on 
the relative cost of providing an incorrect an- 
swer versus no answer at all. Individual ap- 
plications may require mphasizing one or the 
other. 
All of the experiments were run on a 
167MHz UltraSparc work station under Sic- 
stus Prolog. Although results on the parsing 
time of the different systems are not formally 
reported here, it was noted that the difference 
between using a beam width of three and the 
original system was less than two seconds in 
all domains but increased to a~r0und twenty 
seconds when using a beam width of twelve. 
However, the current Prolog implementation 
is not highly optimized. 
139 
Parsers \ Corpora Geo250 Geo560 Jobs400 Rest250 
Prob-Parser(12) 
Prob-Parser(8) 
Prob-Parser(5) 
Prob-Parser(3) 
Prob-Parser(1) 
TABULATE 
Original CHILL 
Hand-Built Parser 
R P 
80.40 88.16 
79.60 86.90 
78.40 87.11 
77.60 87.39 
67.60 90.37 
75.60 92.65 
68.50 97.65 
56.00 96.40 
R P 
71.61 78.94 
71.07 79.76 
70.00 79.51 
69.11 79.30 
62.86 82.05 
69.29 89.81 
I~ P 
80.50 86.56 
78.75 86.54 
74.25 86.59 
70.50 87.31 
34.25 85.63 
68.50 87.54 
R P 
99.20 99.60 
99.20 99.60 
99.20 99.60 
99.20 99.60 
99.20 99.60 
99.20 99.60 
Table 1: Results For All Domains: R = % Recall and P = % Precision. Prob-Parser(B) is 
the probabilistic parser using a beam width of B. TABULATE is CHILL using the TABULATE 
induction algorithm with determ;nistic parsing. 
100 ~. 
9O 
80  
70  
60 
50 
40  
30  
0 
Geo250 , 
Geo560 - - -?- - -  
Jobs400-  ~* 
Rest50  -~-  
100 
95 
90 
85 
80 
i i i i / 75 
2 4 6 8 lO 12 
B~n ,~Ze 
Geo250 , 
Geo560 - - -x- - -  
Job?,400 ~ - 
Rest250 - - - c - -  
~ - ~  ......... 
x, ,  ... 
i i i / 
2 4 6 8 10 12 
Seam S?~ 
Figure 3: The recall and precision of the parser using various beam widths in the different 
domains 
While there was an overall improvement in
recall using the new approach, its performance 
varied signiGcantly from dom~;~ to domain. 
As a result, the recall did not always improve 
dramatically by using a larger beam width. 
Domain factors possibly affecting the perfor- 
mance are the quality of the lexicon, the rel- 
ative amount of data available for calculat- 
ing probability estimates, and the problem of 
'~parser incompleteness" with respect o the 
test data (i.e. there is not a path from a sen- 
tence to a correct query which happens when 
'7 = 0). The performance of all systems were 
basically equivalent in the restaurant domain, 
where they were near perfect in both recall 
and precision. This is because this corpus is 
relatively easier given the restricted range of 
possible questions due to the limited informa- 
tion available about each restaurant. The sys- 
tems achieved > 90% in recall and precision 
given only roughly 30% of the training data 
in this domain. Finally, GEOBASE perfomed 
the worst on the original geography queries, 
since it is difficult to hand-crat~ a parser that 
handles a sn~cient variety of questions. 
6 Conc lus ion  
A probabilistic framework for semantic shift- 
reduce parsing was presented. A new ILP 
learning system was also introduced which 
learns multiple hypotheses. These two tech- 
niques were integrated to learn semantic 
parsers for building NLI's to on|ine databases. 
Experimental results were presented that 
demonstrate that such an approach outper- 
forms a purely logical approach in terms of 
the accuracy of the learned parser. 
7 Ack~nowledgements  
This research was supported by a grant from 
the Daimler-Chrysler Research and Technol- 
ogy Center and by the National Science Foun- 
dation under grant n~I-9704943. 
140 
References  
K. Ali and M. Pazzani. 1996. Error reduction 
through learning multiple descriptions. Ma- 
chine Learning Journal, 24:3:100--132. 
W. Buntine. 1990. A theory of learning classifica- 
tion rules. Ph.D. thesis, University of Technol- 
ogy, Sydney, Australia. 
B. Cestnik. 1990. Estimating probabilities: A cru- 
cial task in machine learning. In Proceedings of 
the Ninth European Conference on Artificial In- 
teUigence, pages 147-149, Stockholm, Sweden. 
W. W. Cohen. 1995. Fast effective rule induc- 
tion. In Proceedings of the Twelfth Interna- 
tional Conference on Machine Learning, pages 
115-123. 
L. Getoor and D. Jensen, editors. 2000. Papers 
from the AAA1 Workshop on Learning Statis- 
tical Models from Relational Data, Austin, TX. 
AAAI Press. 
G. G. Hendrix, E. Sacerdoti, D. Sagalowicz, and 
J. Slocum. 1978. Developing a natural language 
interface to complex data. AGM Transactions 
on Database Systems, 3(2):105-147. 
R. Knhn and R. De Mori. 1995. The application of 
semantic classification trees to natural language 
understanding. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 17(5):449-.- 
460. 
N. Lavrac and S. Dzeroski. 1994. Inductive Logic 
Programming: Techniques and Applications. 
Ellis Horwood. 
C. D. Mauning and H. Sch/itze. 1999. Founda- 
tions of Statistical Natural Language Process- 
ing. MIT Press, Cambridge, MA. 
Scott Miller, David StaUard, Robert Bobrow, and 
Richard Schwartz. 1996. A fully statistical ap- 
proach to natural anguage interfaces. In Pro- 
ceedings of the 34th Annual Meeting of the As- 
sociation for Computational Linguistics, pages 
55-61, Santa Cruz, CA. 
S. Muggleton and W. Buntine. 1988. Machine 
invention of first-order predicates by inverting 
resolution. In Proceedings of the Fifth Interna- 
tional Conference on Machine Learning, pages 
339--352, Ann Arbor, MI, June. 
J. tL Q,inlan. 1990. Learning logical definitions 
from relations. Machine Learning, 5(3):239- 
266. 
C. A. Thompson and R. J. Mooney. 1999. Au- 
tomatic construction of semantic lexicons for 
learning natural anguage interfaces. In Pro- 
ceedings of the Sixteenth National Conference 
on Artificial Intelligence, pages 487-493, Or- 
lando, FL, July. 
W. A. Woods. 1970. Transition network gram- 
mars for natural language analysis. Communi- 
cations of the Association for Computing Ma- 
chinery, 13:591-606. 
J. M. Zelle and R. J. Mooney. 1994. Combin- 
ing top-down and bottom-up methods in indue- 
tive logic programming. In Proceedings of the 
Eleventh International Conference on Machine 
Learning, pages 343--351, New Brunswick, NJ, 
July. 
J. M. Zelle and tL J. Mooney. 1996. Learning 
to parse database queries using inductive logic 
programming. In Proceedings of the Thirteenth 
National Conference on Artificial Intelligence, 
pages 1050--1055, Portland, OR, August. 
141 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 9?16, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Statistical Semantic Parser that Integrates Syntax and Semantics
Ruifang Ge Raymond J. Mooney
Department of Computer Sciences
University of Texas, Austin
TX 78712, USA
fgrf,mooneyg@cs.utexas.edu
Abstract
We introduce a learning semantic parser,
SCISSOR, that maps natural-language sen-
tences to a detailed, formal, meaning-
representation language. It first uses
an integrated statistical parser to pro-
duce a semantically augmented parse tree,
in which each non-terminal node has
both a syntactic and a semantic label.
A compositional-semantics procedure is
then used to map the augmented parse
tree into a final meaning representation.
We evaluate the system in two domains,
a natural-language database interface and
an interpreter for coaching instructions in
robotic soccer. We present experimental
results demonstrating that SCISSOR pro-
duces more accurate semantic representa-
tions than several previous approaches.
1 Introduction
Most recent work in learning for semantic parsing
has focused on ?shallow? analysis such as seman-
tic role labeling (Gildea and Jurafsky, 2002). In this
paper, we address the more ambitious task of learn-
ing to map sentences to a complete formal meaning-
representation language (MRL). We consider two
MRL?s that can be directly used to perform useful,
complex tasks. The first is a Prolog-based language
used in a previously-developed corpus of queries to
a database on U.S. geography (Zelle and Mooney,
1996). The second MRL is a coaching language for
robotic soccer developed for the RoboCup Coach
Competition, in which AI researchers compete to
provide effective instructions to a coachable team of
agents in a simulated soccer domain (et al, 2003).
We present an approach based on a statisti-
cal parser that generates a semantically augmented
parse tree (SAPT), in which each internal node in-
cludes both a syntactic and semantic label. We aug-
ment Collins? head-driven model 2 (Collins, 1997)
to incorporate a semantic label on each internal
node. By integrating syntactic and semantic inter-
pretation into a single statistical model and finding
the globally most likely parse, an accurate combined
syntactic/semantic analysis can be obtained. Once a
SAPT is generated, an additional step is required to
translate it into a final formal meaning representa-
tion (MR).
Our approach is implemented in a system called
SCISSOR (Semantic Composition that Integrates
Syntax and Semantics to get Optimal Representa-
tions). Training the system requires sentences an-
notated with both gold-standard SAPT?s and MR?s.
We present experimental results on corpora for both
geography-database querying and Robocup coach-
ing demonstrating that SCISSOR produces more ac-
curate semantic representations than several previ-
ous approaches based on symbolic learning (Tang
and Mooney, 2001; Kate et al, 2005).
2 Target MRL?s
We used two MRLs in our experiments: CLANG and
GEOQUERY. They capture the meaning of linguistic
utterances in their domain in a formal language.
9
2.1 CLANG: the RoboCup Coach Language
RoboCup (www.robocup.org) is an interna-
tional AI research initiative using robotic soccer
as its primary domain. In the Coach Competition,
teams of agents compete on a simulated soccer field
and receive advice from a team coach in a formal
language called CLANG. In CLANG, tactics and
behaviors are expressed in terms of if-then rules.
As described in (et al, 2003), its grammar consists
of 37 non-terminal symbols and 133 productions.
Below is a sample rule with its English gloss:
((bpos (penalty-area our))
(do (player-except our {4})
(pos (half our))))
?If the ball is in our penalty area, all our players
except player 4 should stay in our half.?
2.2 GEOQUERY: a DB Query Language
GEOQUERY is a logical query language for a small
database of U.S. geography containing about 800
facts. This domain was originally chosen to test
corpus-based semantic parsing due to the avail-
ability of a hand-built natural-language interface,
GEOBASE, supplied with Turbo Prolog 2.0 (Borland
International, 1988). The GEOQUERY language
consists of Prolog queries augmented with several
meta-predicates (Zelle and Mooney, 1996). Below
is a sample query with its English gloss:
answer(A,count(B,(city(B),loc(B,C),
const(C,countryid(usa))),A))
?How many cities are there in the US??
3 Semantic Parsing Framework
This section describes our basic framework for se-
mantic parsing, which is based on a fairly stan-
dard approach to compositional semantics (Juraf-
sky and Martin, 2000). First, a statistical parser
is used to construct a SAPT that captures the se-
mantic interpretation of individual words and the
basic predicate-argument structure of the sentence.
Next, a recursive procedure is used to composition-
ally construct an MR for each node in the SAPT
from the semantic label of the node and the MR?s
has2
VP?bowner
player the ball
NN?player CD?unum NP?null
NN?null
VB?bowner
S?bowner
NP?player
DT?null
PRP$?team
our
Figure 1: An SAPT for a simple CLANG sentence.
Function:BUILDMR(N;K)
Input: The root node N of a SAPT;
predicate-argument knowledge, K, for the MRL.
Notation: X
MR
is the MR of node X .
Output: N
MR
C
i
:= the ith child node of N; 1  i  n
C
h
= GETSEMANTICHEAD(N ) // see Section 3
C
h
MR
= BUILDMR(C
h
; K)
for each other child C
i
where i 6= h
C
i
MR
= BUILDMR(C
i
; K)
COMPOSEMR(C
h
MR
, C
i
MR
; K) // see Section 3
N
MR
= C
h
MR
Figure 2: Computing an MR from a SAPT.
of its children. Syntactic structure provides infor-
mation of how the parts should be composed. Am-
biguities arise in both syntactic structure and the se-
mantic interpretation of words and phrases. By in-
tegrating syntax and semantics in a single statistical
parser that produces an SAPT, we can use both se-
mantic information to resolve syntactic ambiguities
and syntactic information to resolve semantic ambi-
guities.
In a SAPT, each internal node in the parse tree
is annotated with a semantic label. Figure 1 shows
the SAPT for a simple sentence in the CLANG do-
main. The semantic labels which are shown after
dashes are concepts in the domain. Some type con-
cepts do not take arguments, like team and unum
(uniform number). Some concepts, which we refer
to as predicates, take an ordered list of arguments,
like player and bowner (ball owner). The predicate-
argument knowledge, K , specifies, for each predi-
cate, the semantic constraints on its arguments. Con-
straints are specified in terms of the concepts that
can fill each argument, such as player(team, unum)
and bowner(player). A special semantic label null
is used for nodes that do not correspond to any con-
cept in the domain.
Figure 2 shows the basic algorithm for build-
ing an MR from an SAPT. Figure 3 illustrates the
10
player the ball
N3?bowner(_)N7?player(our,2)
N2?null
      null      null
N4?player(_,_)    N5?team
our
N6?unum
2
N1?bowner(_)
has
N8?bowner(player(our,2))
Figure 3: MR?s constructed for each SAPT Node.
construction of the MR for the SAPT in Figure 1.
Nodes are numbered in the order in which the con-
struction of their MR?s are completed. The first
step, GETSEMANTICHEAD , determines which of a
node?s children is its semantic head based on hav-
ing a matching semantic label. In the example, node
N3 is determined to be the semantic head of the
sentence, since its semantic label, bowner, matches
N8?s semantic label. Next, the MR of the seman-
tic head is constructed recursively. The semantic
head of N3 is clearly N1. Since N1 is a part-of-
speech (POS) node, its semantic label directly de-
termines its MR, which becomes bowner( ). Once
the MR for the head is constructed, the MR of all
other (non-head) children are computed recursively,
and COMPOSEMR assigns their MR?s to fill the ar-
guments in the head?s MR to construct the com-
plete MR for the node. Argument constraints are
used to determine the appropriate filler for each ar-
gument. Since, N2 has a null label, the MR of N3
also becomes bowner( ). When computing the MR
for N7, N4 is determined to be the head with the
MR: player( , ). COMPOSEMR then assigns N5?s
MR to fill the team argument and N6?s MR to fill
the unum argument to construct N7?s complete MR:
player(our, 2). This MR in turn is composed with
the MR for N3 to yield the final MR for the sen-
tence: bowner(player(our,2)).
For MRL?s, such as CLANG, whose syntax does
not strictly follow a nested set of predicates and ar-
guments, some final minor syntactic adjustment of
the final MR may be needed. In the example, the
final MR is (bowner (player our f2g)). In the fol-
lowing discussion, we ignore the difference between
these two.
There are a few complications left which re-
quire special handling when generating MR?s,
like coordination, anaphora resolution and non-
compositionality exceptions. Due to space limita-
tions, we do not present the straightforward tech-
niques we used to handle them.
4 Corpus Annotation
This section discusses how sentences for training
SCISSOR were manually annotated with SAPT?s.
Sentences were parsed by Collins? head-driven
model 2 (Bikel, 2004) (trained on sections 02-21
of the WSJ Penn Treebank) to generate an initial
syntactic parse tree. The trees were then manually
corrected and each node augmented with a semantic
label.
First, semantic labels for individual words, called
semantic tags, are added to the POS nodes in the
tree. The tag null is used for words that have no cor-
responding concept. Some concepts are conveyed
by phrases, like ?has the ball? for bowner in the pre-
vious example. Only one word is labeled with the
concept; the syntactic head word (Collins, 1997) is
preferred. During parsing, the other words in the
phrase will provide context for determining the se-
mantic label of the head word.
Labels are added to the remaining nodes in a
bottom-up manner. For each node, one of its chil-
dren is chosen as the semantic head, from which it
will inherit its label. The semantic head is chosen
as the child whose semantic label can take the MR?s
of the other children as arguments. This step was
done mostly automatically, but required some man-
ual corrections to account for unusual cases.
In order for COMPOSEMR to be able to construct
the MR for a node, the argument constraints for
its semantic head must identify a unique concept
to fill each argument. However, some predicates
take multiple arguments of the same type, such as
point.num(num,num), which is a kind of point that
represents a field coordinate in CLANG.
In this case, extra nodes are inserted in the tree
with new type concepts that are unique for each ar-
gument. An example is shown in Figure 4 in which
the additional type concepts num1 and num2 are in-
troduced. Again, during parsing, context will be
used to determine the correct type for a given word.
The point label of the root node of Figure 4 is the
concept that includes all kinds of points in CLANG.
Once a predicate has all of its arguments filled, we
11
,0.5 , ?RRB?
?RRB??null  
?LRB? 0.1
CD?num CD?num
?LRB??point.num
PRN?point
CD?num1 CD?num2
Figure 4: Adding new types to disambiguate argu-
ments.
use the most general CLANG label for its concept
(e.g. point instead of point.num). This generality
avoids sparse data problems during training.
5 Integrated Parsing Model
5.1 Collins Head-Driven Model 2
Collins? head-driven model 2 is a generative, lexi-
calized model of statistical parsing. In the following
section, we follow the notation in (Collins, 1997).
Each non-terminal X in the tree is a syntactic label,
which is lexicalized by annotating it with a word,
w, and a POS tag, t
syn
. Thus, we write a non-
terminal as X(x), where X is a syntactic label and
x = hw; t
syn
i. X(x) is then what is generated by
the generative model.
Each production LHS ) RHS in the PCFG is
in the form:
P (h)!L
n
(l
n
):::L
1
(l
1
)H(h)R
1
(r
1
):::R
m
(r
m
)
where H is the head-child of the phrase, which in-
herits the head-word h from its parent P . L
1
:::L
n
and R
1
:::R
m
are left and right modifiers of H .
Sparse data makes the direct estimation of
P(RHSjLHS) infeasible. Therefore, it is decom-
posed into several steps ? first generating the head,
then the right modifiers from the head outward,
then the left modifiers in the same way. Syntactic
subcategorization frames, LC and RC, for the left
and right modifiers respectively, are generated be-
fore the generation of the modifiers. Subcat frames
represent knowledge about subcategorization prefer-
ences. The final probability of a production is com-
posed from the following probabilities:
1. The probability of choosing a head constituent
label H: P
h
(HjP; h).
2. The probabilities of choosing the left and right
subcat frames LC and RC: P
l
(LCjP;H; h)
and P
r
(RCjP;H; h).
has2our player the
PRP$?team NN?player CD?unum
NN?nullDT?null
NP?player(player) VP?bowner(has)
NP?null(ball)
ball
S?bowner(has)
VB?bowner
Figure 5: A lexicalized SAPT.
3. The probabilities of generat-
ing the left and right modifiers:
Q
i=1::m+1
P
r
(R
i
(r
i
)jH;P; h;
i 1
; RC) 
Q
i=1::n+1
P
l
(L
i
(l
i
)jH;P; h;
i 1
; LC).
Where  is the measure of the distance from
the head word to the edge of the constituent,
and L
n+1
(l
n+1
) and R
m+1
(r
m+1
) are STOP .
The model stops generating more modifiers
when STOP is generated.
5.2 Integrating Semantics into the Model
We extend Collins? model to include the genera-
tion of semantic labels in the derivation tree. Un-
less otherwise stated, notation has the same mean-
ing as in Section 5.1. The subscript syn refers to
the syntactic part, and sem refers to the semantic
part. We redefine X and x to include semantics,
each non-terminal X is now a pair of a syntactic la-
bel X
syn
and a semantic label X
sem
. Besides be-
ing annotated with the word, w, and the POS tag,
t
syn
, X is also annotated with the semantic tag,
t
sem
, of the head child. Thus, X(x) now consists of
X = hX
syn
;X
sem
i, and x = hw; t
syn
; t
sem
i. Fig-
ure 5 shows a lexicalized SAPT (but omitting t
syn
and t
sem
).
Similar to the syntactic subcat frames, we also
condition the generation of modifiers on semantic
subcat frames. Semantic subcat frames give se-
mantic subcategorization preferences; for example,
player takes a team and a unum. Thus LC and RC
are now: hLC
syn
; LC
sem
i and hRC
syn
; RC
sem
i.
X(x) is generated as in Section 5.1, but using the
new definitions of X(x), LC and RC . The imple-
mentation of semantic subcat frames is similar to
syntactic subcat frames. They are multisets speci-
fying the semantic labels which the head requires in
its left or right modifiers.
As an example, the probability of generating the
phrase ?our player 2? using NP-[player](player) !
12
PRP$-[team](our) NN-[player](player) CD-[unum](2)
is (omitting only the distance measure):
P
h
(NN-[player]jNP-[player],player)
P
l
(hfg,fteamgijNP-[player],player)
P
r
(hfg,funumgijNP-[player],player)
P
l
(PRP$-[team](our)jNP-[player],player,hfg,fteamgi)
P
r
(CD-[unum](2)jNP-[player],player,hfg,funumgi)
P
l
(STOPjNP-[player],player,hfg,fgi)
P
r
(STOPjNP-[player],player,hfg,fgi)
5.3 Smoothing
Since the left and right modifiers are independently
generated in the same way, we only discuss smooth-
ing for the left side. Each probability estimation in
the above generation steps is called a parameter. To
reduce the risk of sparse data problems, the parame-
ters are decomposed as follows:
P
h
(HjC) = P
h
syn
(H
syn
jC)
P
h
sem
(H
sem
jC;H
syn
)
P
l
(LCjC) = P
l
syn
(LC
syn
jC)
P
l
sem
(LC
sem
jC;LC
syn
)
P
l
(L
i
(l
i
)jC) = P
l
syn
(L
i
syn
(lt
i
syn
; lw
i
)jC)
P
l
sem
(L
i
sem
(lt
i
sem
; lw
i
)jC;L
i
syn
(lt
i
syn
))
For brevity, C is used to represent the context on
which each parameter is conditioned; lw
i
, lt
i
syn
, and
lt
i
sem
are the word, POS tag and semantic tag gener-
ated for the non-terminal L
i
. The word is generated
separately in the syntactic and semantic outputs.
We make the independence assumption that the
syntactic output is only conditioned on syntactic fea-
tures, and semantic output on semantic ones. Note
that the syntactic and semantic parameters are still
integrated in the model to find the globally most
likely parse. The syntactic parameters are the same
as in Section 5.1 and are smoothed as in (Collins,
1997). We?ve also tried different ways of condition-
ing syntactic output on semantic features and vice
versa, but they didn?t help. Our explanation is the
integrated syntactic and semantic parameters have
already captured the benefit of this integrated ap-
proach in our experimental domains.
Since the semantic parameters do not depend on
any syntactic features, we omit the sem subscripts
in the following discussion. As in (Collins, 1997),
the parameter P
l
(L
i
(lt
i
; lw
i
)jP;H;w; t;; LC) is
further smoothed as follows:
P
l1
(L
i
jP;H;w; t;; LC) 
P
l2
(lt
i
jP;H;w; t;; LC;L
i
)
P
l3
(lw
i
jP;H;w; t;; LC;L
i
(lt
i
))
Note this smoothing is different from the syntactic
counterpart. This is due to the difference between
POS tags and semantic tags; namely, semantic tags
are generally more specific.
Table 1 shows the various levels of back-off for
each semantic parameter. The probabilities from
these back-off levels are interpolated using the tech-
niques in (Collins, 1997). All words occurring less
than 3 times in the training data, and words in test
data that were not seen in training, are unknown
words and are replaced with the ?UNKNOWN? to-
ken. Note this threshold is smaller than the one used
in (Collins, 1997) since the corpora used in our ex-
periments are smaller.
5.4 POS Tagging and Semantic Tagging
For unknown words, the POS tags allowed are lim-
ited to those seen with any unknown words during
training. Otherwise they are generated along with
the words using the same approach as in (Collins,
1997). When parsing, semantic tags for each known
word are limited to those seen with that word dur-
ing training data. The semantic tags allowed for an
unknown word are limited to those seen with its as-
sociated POS tags during training.
6 Experimental Evaluation
6.1 Methodology
Two corpora of NL sentences paired with MR?s
were used to evaluate SCISSOR. For CLANG, 300
pieces of coaching advice were randomly selected
from the log files of the 2003 RoboCup Coach Com-
petition. Each formal instruction was translated
into English by one of four annotators (Kate et al,
2005). The average length of an NL sentence in
this corpus is 22.52 words. For GEOQUERY, 250
questions were collected by asking undergraduate
students to generate English queries for the given
database. Queries were then manually translated
13
BACK-OFFLEVEL P
h
(Hj:::) P
LC
(LCj:::) P
L1
(L
i
j:::) P
L2
(lt
i
j:::) P
L3
(lw
i
j:::)
1 P,w,t P,H,w,t P,H,w,t,,LC P,H,w,t,,LC, L
i
P,H,w,t,,LC, L
i
, lt
i
2 P,t P,H,t P,H,t,,LC P,H,t,,LC, L
i
P,H,t,,LC, L
i
, lt
i
3 P P,H P,H,,LC P,H,,LC, L
i
L
i
, lt
i
4 ? ? ? L
i
lt
i
Table 1: Conditioning variables for each back-off level for semantic parameters (sem subscripts omitted).
into logical form (Zelle and Mooney, 1996). The
average length of an NL sentence in this corpus is
6.87 words. The queries in this corpus are more
complex than those in the ATIS database-query cor-
pus used in the speech community (Zue and Glass,
2000) which makes the GEOQUERY problem harder,
as also shown by the results in (Popescu et al, 2004).
The average number of possible semantic tags for
each word which can represent meanings in CLANG
is 1.59 and that in GEOQUERY is 1.46.
SCISSOR was evaluated using standard 10-fold
cross validation. NL test sentences are first parsed
to generate their SAPT?s, then their MR?s were built
from the trees. We measured the number of test sen-
tences that produced complete MR?s, and the num-
ber of these MR?s that were correct. For CLANG,
an MR is correct if it exactly matches the correct
representation, up to reordering of the arguments of
commutative operators like and. For GEOQUERY,
an MR is correct if the resulting query retrieved
the same answer as the correct representation when
submitted to the database. The performance of the
parser was then measured in terms of precision (the
percentage of completed MR?s that were correct)
and recall (the percentage of all sentences whose
MR?s were correctly generated).
We compared SCISSOR?s performance to several
previous systems that learn semantic parsers that can
map sentences into formal MRL?s. CHILL (Zelle and
Mooney, 1996) is a system based on Inductive Logic
Programming (ILP). We compare to the version
of CHILL presented in (Tang and Mooney, 2001),
which uses the improved COCKTAIL ILP system and
produces more accurate parsers than the original ver-
sion presented in (Zelle and Mooney, 1996). SILT is
a system that learns symbolic, pattern-based, trans-
formation rules for mapping NL sentences to formal
languages (Kate et al, 2005). It comes in two ver-
sions, SILT-string, which maps NL strings directly
to an MRL, and SILT-tree, which maps syntactic
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250
Pr
ec
is
io
n 
(%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
GEOBASE
Figure 6: Precision learning curves for GEOQUERY.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  50  100  150  200  250
R
ec
al
l (%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
GEOBASE
Figure 7: Recall learning curves for GEOQUERY.
parse trees (generated by the Collins parser) to an
MRL. In the GEOQUERY domain, we also compare
to the original hand-built parser GEOBASE.
6.2 Results
Figures 6 and 7 show the precision and recall learn-
ing curves for GEOQUERY, and Figures 8 and 9 for
CLANG. Since CHILL is very memory intensive,
it could not be run with larger training sets of the
CLANG corpus.
Overall, SCISSOR gives the best precision and re-
call results in both domains. The only exception
is with recall for GEOQUERY, for which CHILL is
slightly higher. However, SCISSOR has significantly
higher precision (see discussion in Section 7).
14
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300
Pr
ec
is
io
n 
(%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
Figure 8: Precision learning curves for CLANG.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  50  100  150  200  250  300
R
ec
al
l (%
)
Training sentences
SCISSOR
SILT-string
SILT-tree
CHILL
Figure 9: Recall learning curves for CLANG.
Results on a larger GEOQUERY corpus with 880
queries have been reported for PRECISE (Popescu et
al., 2003): 100% precision and 77.5% recall. On
the same corpus, SCISSOR obtains 91.5% precision
and 72.3% recall. However, the figures are not com-
parable. PRECISE can return multiple distinct SQL
queries when it judges a question to be ambigu-
ous and it is considered correct when any of these
SQL queries is correct. Our measure only considers
the top result. Due to space limitations, we do not
present complete learning curves for this corpus.
7 Related Work
We first discuss the systems introduced in Section
6. CHILL uses computationally-complex ILP meth-
ods, which are slow and memory intensive. The
string-based version of SILT uses no syntactic in-
formation while the tree-based version generates a
syntactic parse first and then transforms it into an
MR. In contrast, SCISSOR integrates syntactic and
semantic processing, allowing each to constrain and
inform the other. It uses a successful approach to sta-
tistical parsing that attempts to find the SAPT with
maximum likelihood, which improves robustness
compared to purely rule-based approaches. How-
ever, SCISSOR requires an extra training input, gold-
standard SAPT?s, not required by these other sys-
tems. Further automating the construction of train-
ing SAPT?s from sentences paired with MR?s is a
subject of on-going research.
PRECISE is designed to work only for the spe-
cific task of NL database interfaces. By comparison,
SCISSOR is more general and can work with other
MRL?s as well (e.g. CLANG). Also, PRECISE is not
a learning system and can fail to parse a query it con-
siders ambiguous, even though it may not be consid-
ered ambiguous by a human and could potentially be
resolved by learning regularities in the training data.
In (Lev et al, 2004), a syntax-driven approach
is used to map logic puzzles described in NL to
an MRL. The syntactic structures are paired with
hand-written rules. A statistical parser is used to
generate syntactic parse trees, and then MR?s are
built using compositional semantics. The meaning
of open-category words (with only a few exceptions)
is considered irrelevant to solving the puzzle and
their meanings are not resolved. Further steps would
be needed to generate MR?s in other domains like
CLANG and GEOQUERY. No empirical results are
reported for their approach.
Several machine translation systems also attempt
to generate MR?s for sentences. In (et al, 2002),
an English-Chinese speech translation system for
limited domains is described. They train a statisti-
cal parser on trees with only semantic labels on the
nodes; however, they do not integrate syntactic and
semantic parsing.
History-based models of parsing were first in-
troduced in (Black et al, 1993). Their original
model also included semantic labels on parse-tree
nodes, but they were not used to generate a formal
MR. Also, their parsing model is impoverished com-
pared to the history included in Collins? more recent
model. SCISSOR explores incorporating semantic
labels into Collins? model in order to produce a com-
plete SAPT which is then used to generate a formal
MR.
The systems introduced in (Miller et al, 1996;
Miller et al, 2000) also integrate semantic labels
into parsing; however, their SAPT?s are used to pro-
15
duce a much simpler MR, i.e., a single semantic
frame. A sample frame is AIRTRANSPORTATION
which has three slots ? the arrival time, origin and
destination. Only one frame needs to be extracted
from each sentence, which is an easier task than
our problem in which multiple nested frames (pred-
icates) must be extracted. The syntactic model in
(Miller et al, 2000) is similar to Collins?, but does
not use features like subcat frames and distance mea-
sures. Also, the non-terminal label X is not further
decomposed into separately-generated semantic and
syntactic components. Since it used much more spe-
cific labels (the cross-product of the syntactic and
semantic labels), its parameter estimates are poten-
tially subject to much greater sparse-data problems.
8 Conclusion
SCISSOR learns statistical parsers that integrate syn-
tax and semantics in order to produce a semanti-
cally augmented parse tree that is then used to com-
positionally generate a formal meaning representa-
tion. Experimental results in two domains, a natural-
language database interface and an interpreter for
coaching instructions in robotic soccer, have demon-
strated that SCISSOR generally produces more accu-
rate semantic representations than several previous
approaches. By augmenting a state-of-the-art statis-
tical parsing model to include semantic information,
it is able to integrate syntactic and semantic clues
to produce a robust interpretation that supports the
generation of complete formal meaning representa-
tions.
9 Acknowledgements
We would like to thank Rohit J. Kate , Yuk Wah
Wong and Gregory Kuhlmann for their help in an-
notating the CLANG corpus and providing the eval-
uation tools. This research was supported by De-
fense Advanced Research Projects Agency under
grant HR0011-04-1-0007.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Ezra Black, Frederick Jelineck, John Lafferty, David M. Mager-
man, Robert L. Mercer, and Salim Roukos. 1993. Towards
history-based grammars: Using richer models for probabilis-
tic parsing. In Proc. of ACL-93, pages 31?37, Columbus,
Ohio.
Borland International. 1988. Turbo Prolog 2.0 Reference
Guide. Borland International, Scotts Valley, CA.
Mao Chen et al 2003. Users manual: RoboCup
soccer server manual for soccer server version 7.07
and later. Available at http://sourceforge.net/
projects/sserver/.
Michael J. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of ACL-97, pages 16?23,
Madrid, Spain.
Yuqing Gao et al 2002. Mars: A statistical semantic parsing
and generation-based multilingual automatic translation sys-
tem. Machine Translation, 17:185?212.
Daniel Gildea and Daniel Jurafsky. 2002. Automated labeling
of semantic roles. Computational Linguistics, 28(3):245?
288.
Daniel Jurafsky and James H. Martin. 2000. Speech and Lan-
guage Processing: An Introduction to Natural Language
Processing, Computational Linguistics, and Speech Recog-
nition. Prentice Hall, Upper Saddle River, NJ.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005.
Learning to transform natural to formal languages. To ap-
pear in Proc. of AAAI-05, Pittsburgh, PA.
Iddo Lev, Bill MacCartney, Christopher D. Manning, and Roger
Levy. 2004. Solving logic puzzles: From robust process-
ing to precise semantics. In Proc. of 2nd Workshop on Text
Meaning and Interpretation, ACL-04, Barcelona, Spain.
Scott Miller, David Stallard, Robert Bobrow, and Richard
Schwartz. 1996. A fully statistical approach to natural lan-
guage interfaces. In ACL-96, pages 55?61, Santa Cruz, CA.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.
Weischedel. 2000. A novel use of statistical parsing to ex-
tract information from text. In Proc. of NAACL-00, pages
226?233, Seattle, Washington.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. To-
wards a theory of natural language interfaces to databases. In
Proc. of IUI-2003, pages 149?157, Miami, FL. ACM.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko,
and Alexander Yates. 2004. Modern natural language in-
terfaces to databases: Composing statistical parsing with se-
mantic tractability. In COLING-04, Geneva, Switzerland.
Lappoon R. Tang and Raymond J. Mooney. 2001. Using multi-
ple clause constructors in inductive logic programming for
semantic parsing. In Proc. of ECML-01, pages 466?477,
Freiburg, Germany.
John M. Zelle and Raymond J. Mooney. 1996. Learning to
parse database queries using inductive logic programming.
In Proc. of AAAI-96, pages 1050?1055, Portland, OR.
Victor W. Zue and James R. Glass. 2000. Conversational in-
terfaces: Advances and challenges. In Proc. of the IEEE,
volume 88(8), pages 1166?1180.
16
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 46?53, Detroit, June 2005. c?2005 Association for Computational Linguistics
Using Biomedical Literature Mining to Consolidate the Set of Known
Human Protein?Protein Interactions
Arun Ramani, Edward Marcotte
Institute for Cellular and Molecular Biology
University of Texas at Austin
1 University Station A4800
Austin, TX 78712
arun@icmb.utexas.edu
marcotte@icmb.utexas.edu
Razvan Bunescu, Raymond Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan@cs.utexas.edu
mooney@cs.utexas.edu
Abstract
This paper presents the results of a large-
scale effort to construct a comprehensive
database of known human protein inter-
actions by combining and linking known
interactions from existing databases and
then adding to them by automatically min-
ing additional interactions from 750,000
Medline abstracts. The end result is a
network of 31,609 interactions amongst
7,748 proteins. The text mining sys-
tem first identifies protein names in the
text using a trained Conditional Random
Field (CRF) and then identifies interac-
tions through a filtered co-citation anal-
ysis. We also report two new strategies
for mining interactions, either by finding
explicit statements of interactions in the
text using learned pattern-based rules or
a Support-Vector Machine using a string
kernel. Using information in existing on-
tologies, the automatically extracted data
is shown to be of equivalent accuracy to
manually curated data sets.
1 Introduction
Proteins are often considered in terms of their net-
works of interactions, a view that has spurred con-
siderable effort in mapping large-scale protein in-
teraction networks. Thus far, the most complete
protein networks are measured for yeast and de-
rive from the synthesis of varied large scale experi-
mental interaction data and in-silico interaction pre-
dictions (summarized in (von Mering et al, 2002;
Lee et al, 2004; Jansen et al, 2003)). Unlike the
case of yeast, only minimal progress has been made
with respect to the human proteome. While some
moderate-scale interaction maps have been created,
such as for the purified TNFa/NFKB protein com-
plex (Bouwmeester et al, 2004) and the proteins in-
volved in the human Smad signaling pathway (Col-
land et al, 2004), the bulk of known human pro-
tein interaction data derives from individual, small-
scale experiments reported in Medline. Many of
these interactions have been collected in the Reac-
tome (Joshi-Tope et al, 2005), BIND (Bader et al,
2003), DIP (Xenarios et al, 2002), and HPRD (Peri
et al, 2004) databases, with Reactome contributing
11,000 interactions that have been manually entered
from articles focusing on interactions in core cellular
pathways, and HPRD contributing a set of 12,000
interactions recovered by manual curation of Med-
line articles using teams of readers. Additional inter-
actions have been transferred from other organisms
based on orthology (Lehner and Fraser, 2004).
A comparison of these existing interaction data
sets is enlightening. Although the interactions from
these data sets are in principle derived from the same
source (Medline), the sets are quite disjoint (Fig-
ure 1) implying either that the sets are biased for
different classes of interactions, or that the actual
number of interactions in Medline is quite large.
We suspect both reasons. It is clear that each data
set has a different explicit focus (Reactome towards
core cellular machinery, HPRD towards disease-
linked genes, and DIP and BIND more randomly
46
distributed). Due to these biases, it is likely that
many interactions from Medline are still excluded
from these data sets. The maximal overlap between
interaction data sets is seen for BIND: 25% of these
interactions are also in HPRD or Reactome; only 1%
of Reactome interactions are in HPRD or BIND.
Figure 1: Overlap diagram for known datasets.
Medline now has records from more than 4,800
journals accounting for around 15 million articles.
These citations contain thousands of experimentally
recorded protein interactions, and even a cursory in-
vestigation of Medline reveals human protein inter-
actions not present in the current databases. How-
ever, retrieving these data manually is made diffi-
cult by the large number of articles, all lacking for-
mal structure. Automated extraction of informa-
tion would be preferable, and therefore, mining data
from Medline abstracts is a growing field (Jenssen
et al, 2001; Rzhetsky et al, 2004; Liu and Wong,
2003; Hirschman et al, 2002).
In this paper, we describe a framework for
automatic extraction of protein interactions from
biomedical literature. We focus in particular on the
difficult and important problem of identifying inter-
actions concerning human proteins. We describe a
system for first accurately identifying the names of
human proteins in the documents, then on identify-
ing pairs of interacting human proteins, and demon-
strate that the extracted protein interactions are com-
parable to those extracted manually. In the pro-
cess, we consolidate the existing set of publically-
available human protein interactions into a network
of 31,609 interactions between 7,748 proteins.
2 Assembling existing protein interaction
data
We previously gathered the existing human protein
interaction data sets ((Ramani et al, 2005); sum-
marized in Table 1), representing the current sta-
tus of the publically-available human interactome.
This required unification of the interactions under
a shared naming and annotation convention. For
this purpose, we mapped each interacting protein
to LocusLink (now EntrezGene) identification num-
bers and retained only unique interactions (i.e., for
two proteins A and B, we retain only A?B or B?A,
not both). We have chosen to omit self-interactions,
A?A or B?B, for technical reasons, as their qual-
ity cannot be assessed on the functional benchmark
that we describe in Section 3. In most cases, a small
loss of proteins occurred in the conversion between
the different gene identifiers (e.g., converting from
the NCBI ?gi? codes in BIND to LocusLink iden-
tifiers). In the case of Human Protein Reference
Database (HPRD), this processing resulted in a sig-
nificant reduction in the number of interactions from
12,013 total interactions to 6,054 unique, non-self
interactions, largely due to the fact that HPRD often
records both A-B and B-A interactions, as well as a
large number of self interactions, and indexes genes
by their common names rather than conventional
database entries, often resulting in multiple entries
for different synonyms. An additional 9,283 (or
60,000 at lower confidence) interactions are avail-
able from orthologous transfer of interactions from
large-scale screens in other organisms (orthology-
core and orthology-all) (Lehner and Fraser, 2004).
3 Two benchmark tests of accuracy for
interaction data
To measure the relative accuracy of each protein in-
teraction data set, we established two benchmarks
of interaction accuracy, one based on shared protein
function and the other based on previously known
interactions. First, we constructed a benchmark in
which we tested the extent to which interaction part-
ners in a data set shared annotation, a measure previ-
ously shown to correlate with the accuracy of func-
tional genomics data sets (von Mering et al, 2002;
Lee et al, 2004; Lehner and Fraser, 2004). We
used the functional annotations listed in the KEGG
47
Dataset Version Total Is (Ps) Self (A-A) Is (Ps) Unique (A-B) Is (Ps)
Reactome 08/03/04 12,497 (6,257) 160 (160) 12,336 (807)
BIND 08/03/04 6,212 (5,412) 549 (549) 5,663 (4,762)
HPRD* 04/12/04 12,013 (4,122) 3,028 (3,028) 6,054 (2,747)
Orthology (all) 03/31/04 71,497 (6,257) 373 (373) 71,124 (6,228)
Orthology (core) 03/31/04 11,488 (3,918) 206 (206) 11,282 (3,863)
Table 1: Is = Interactions, Ps = Proteins.
(Kanehisa et al, 2004) and Gene Ontology (Ash-
burner et al, 2000) annotation databases. These
databases provide specific pathway and biological
process annotations for approximately 7,500 human
genes, assigning human genes into 155 KEGG path-
ways (at the lowest level of KEGG) and 1,356 GO
pathways (at level 8 of the GO biological process
annotation). KEGG and GO annotations were com-
bined into a single composite functional annotation
set, which was then split into independent testing
and training sets by randomly assigning annotated
genes into the two categories (3,800 and 3,815 anno-
tated genes respectively). For the second benchmark
based on known physical interactions, we assembled
the human protein interactions from Reactome and
BIND, a set of 11,425 interactions between 1,710
proteins. Each benchmark therefore consists of a
set of binary relations between proteins, either based
on proteins sharing annotation or physically inter-
acting. Generally speaking, we expect more accu-
rate protein interaction data sets to be more enriched
in these protein pairs. More specifically, we expect
true physical interactions to score highly on both
tests, while non-physical or indirect associations,
such as genetic associations, should score highly on
the functional, but not physical interaction, test.
For both benchmarks, the scoring scheme for
measuring interaction set accuracy is in the form of
a log odds ratio of gene pairs either sharing anno-
tations or physically interacting. To evaluate a data
set, we calculate a log likelihood ratio (LLR) as:
LLR = ln
P (DjI)
P (Dj:I)
= ln
P (IjD)P (:I)
P (:IjD)P (I)
(1)
where P (DjI) and P (Dj:I) are the probability
of observing the data D conditioned on the genes
sharing benchmark associations (I) and not sharing
benchmark associations (:I). In its expanded form
(obtained by applying Bayes theorem), P (IjD) and
P (:IjD) are estimated using the frequencies of in-
teractions observed in the given data set D between
annotated genes sharing benchmark associations and
not sharing associations, respectively, while the pri-
ors P (I) and P (:I) are estimated based on the to-
tal frequencies of all benchmark genes sharing the
same associations and not sharing associations, re-
spectively. A score of zero indicates interaction part-
ners in the data set being tested are no more likely
than random to belong to the same pathway or to in-
teract; higher scores indicate a more accurate data
set.
Among the literature-derived interactions (Reac-
tome, BIND, HPRD), a total of 17,098 unique in-
teractions occur in the public data sets. Testing the
existing protein interaction data on the functional
benchmark reveals that Reactome has the highest
accuracy (LLR = 3.8), followed by BIND (LLR =
2.9), HPRD (LLR = 2.1), core orthology-inferred in-
teractions (LLR = 2.1) and the non-core orthology-
inferred interaction (LLR = 1.1). The two most
accurate data sets, Reactome and BIND, form the
basis of the protein interaction?based benchmark.
Testing the remaining data sets on this benchmark
(i.e., for their consistency with these accurate pro-
tein interaction data sets) reveals a similar ranking in
the remaining data. Core orthology-inferred interac-
tions are the most accurate (LLR = 5.0), followed by
HPRD (LLR = 3.7) and non-core orthology inferred
interactions (LLR = 3.7).
4 Framework for Mining Protein?Protein
Interactions
The extraction of interacting proteins from Medline
abstracts proceeds in two separate steps:
1. First, we automatically identify protein names
48
using a CRF system trained on a set of 750
abstracts manually annotated for proteins (see
Section 5 for details).
2. Based on the output of the CRF tagger, we fil-
ter out less confident extractions and then try to
detect which pairs of the remaining extracted
protein names are interaction pairs.
For the second step, we investigate two general
methods:
 Use co-citation analysis to score each pair of
proteins based on the assumption that proteins
co-occurring in a large number of abstracts tend
to be interacting proteins. Out of the resulting
protein pairs we keep only those that co-occur
in abstracts likely to discuss interactions, based
on a Naive Bayes classifier (see Section 6 for
details).
 Given that we already have a set of 230 Med-
line abstracts manually tagged for both proteins
and interactions, we can use it to train an inter-
action extractor. In Section 7 we discuss two
different methods for learning this interaction
extractor.
5 A CRF Tagger for Protein Names
The task of identifying protein names is made diffi-
cult by the fact that unlike other organisms, such as
yeast or E. coli, the human genes have no standard-
ized naming convention, and thus present one of the
hardest sets of gene/protein names to extract. For
example, human proteins may be named with typ-
ical English words, such as ?light?, ?map?, ?com-
plement?, and ?Sonic Hedgehog?. It is therefore
necessary that an information extraction algorithm
be specifically trained to extract gene and protein
names accurately.
We have previously described (Bunescu et al,
2005) effective protein and gene name tagging us-
ing a Maximum Entropy based algorithm. Condi-
tional Random Fields (CRF) (Lafferty et al, 2001)
are new types of probabilistic models that preserve
all the advantages of Maximum Entropy models and
at the same time avoid the label bias problem by al-
lowing a sequence of tagging decisions to compete
against each other in a global probabilistic model.
In both training and testing the CRF protein-name
tagger, the corresponding Medline abstracts were
processed as follows. Text was tokenized using
white-space as delimiters and treating all punctua-
tion marks as separate tokens. The text was seg-
mented into sentences, and part-of-speech tags were
assigned to each token using Brill?s tagger (Brill,
1995). For each token in each sentence, a vector of
binary features was generated using the feature tem-
plates employed by the Maximum Entropy approach
described in (Bunescu et al, 2005). Generally, these
features make use of the words occurring before and
after the current position in the text, their POS tags
and capitalization patterns. Each feature occurring
in the training data is associated with a parameter in
the CRF model. We used the CRF implementation
from (McCallum, 2002). To train the CRF?s parame-
ters, we used 750 Medline abstracts manually anno-
tated for protein names (Bunescu et al, 2005). We
then used the trained system to tag protein and gene
names in the entire set of 753,459 Medline abstracts
citing the word ?human?.
In Figure 2 we compare the performance of the
CRF tagger with that of the Maximum Entropy tag-
ger from (Bunescu et al, 2005), using the same
set of features, by doing 10-fold cross-validation on
Yapex ? a smaller dataset of 200 manually annotated
abstracts (Franzen et al, 2002). Each model assigns
to each extracted protein name a normalized confi-
dence value. The precision?recall curves from Fig-
ure 2 are obtained by varying a threshold on the min-
imum accepted confidence. We also plot the preci-
sion and recall obtained by simply matching textual
phrases against entries from a protein dictionary.
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Pr
ec
is
io
n 
(%
)
Recall (%)
CRF
MaxEnt
Dict
Figure 2: Protein Tagging Performance.
49
The dictionary of human protein names was
assembled from the LocusLink and Swissprot
databases by manually curating the gene names
and synonyms (87,723 synonyms between 18,879
unique gene names) to remove genes that were re-
ferred to as ?hypothetical? or ?probable? and also to
omit entries that referred to more than one protein
identifier.
6 Co-citation Analysis and Bayesian
Classification
In order to establish which interactions occurred
between the proteins identified in the Medline ab-
stracts, we used a 2-step strategy: measure co-
citation of protein names, then enrich these pairs for
physical interactions using a Bayesian filter. First,
we counted the number of abstracts citing a pair of
proteins, and then calculated the probability of co-
citation under a random model based on the hyper-
geometric distribution (Lee et al, 2004; Jenssen et
al., 2001) as:
P (kjN;m; n) =

n
k

N   n
m  k


N
m
 (2)
where N equals the total number of abstracts, n of
which cite the first protein, m cite the second pro-
tein, and k cite both.
Empirically, we find the co-citation probability
has a hyperbolic relationship with the accuracy on
the functional annotation benchmark from Section 3,
with protein pairs co?cited with low random proba-
bility scoring high on the benchmark.
With a threshold on the estimated extraction con-
fidence of 80% (as computed by the CRF model)
in the protein name identification, close to 15,000
interactions are extracted with the co-citation ap-
proach that score comparable or better on the func-
tional benchmark than the manually extracted inter-
actions from HPRD, which serves to establish a min-
imal threshold for our mined interactions.
However, it is clear that proteins are co-cited for
many reasons other than physical interactions. We
therefore tried to enrich specifically for physical in-
teractions by applying a secondary filter. We applied
a Bayesian classifier (Marcotte et al, 2001) to mea-
sure the likelihood of the abstracts citing the pro-
tein pairs to discuss physical protein?protein inter-
actions. The classifier scores each of the co-citing
abstracts according to the usage frequency of dis-
criminating words relevant to physical protein inter-
actions. For a co-cited protein pair, we calculated
the average score of co-citing Medline abstracts and
used this to re-rank the top-scoring 15,000 co-cited
protein pairs.
Interactions extracted by co-citation and filtered
using the Bayesian estimator compare favorably
with the other interaction data sets on the functional
annotation benchmark (Figure 3). Testing the accu-
racy of these extracted protein pairs on the physi-
cal interaction benchmark (Figure 4) reveals that the
co-cited proteins scored high by this classifier are
indeed strongly enriched for physical interactions.
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 0  10000  20000  30000  40000  50000  60000  70000
LL
R
 s
co
re
, f
un
ct
io
na
l b
en
ch
m
ar
k
# of interactions recovered
Co-citation, Bayes filter
BIND
Reactome
HPRD
Orthology (core)
Orthology (core)
Figure 3: Accuracy, functional benchmark
 2
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 0  10000  20000  30000  40000  50000  60000  70000
LL
R
 s
co
re
, p
hy
sic
al
 b
en
ch
m
ar
k
# of interactions recovered
Co-citation, Bayes filter
HPRD
Orthology (core)
Orthology (core)
Figure 4: Accuracy, physical benchmark
Keeping all the interactions that score better than
HPRD, our co-citation / Bayesian classifier analy-
sis yields 6,580 interactions between 3,737 proteins.
By combining these interactions with the 26,280 in-
teractions from the other sources, we obtained a fi-
50
nal set of 31,609 interactions between 7,748 human
proteins.
7 Learning Interaction Extractors
In (Bunescu et al, 2005) we described a dataset of
230 Medline abstracts manually annotated for pro-
teins and their interactions. This can be used as a
training dataset for a method that learns interaction
extractors. Such a method simply classifies a sen-
tence containing two protein names as positive or
negative, where positive means that the sentence as-
serts an interaction between the two proteins. How-
ever a sentence in the training data may contain more
than two proteins and more than one pair of inter-
acting proteins. In order to extract the interacting
pairs, we replicate the sentences having n proteins
(n  2) into Cn
2
sentences such that each one has
exactly two of the proteins tagged, with the rest of
the protein tags omitted. If the tagged proteins in-
teract, then the replicated sentence is added to the
set of positive sentences, otherwise it is added to the
set of negative sentences. During testing, a sentence
having n proteins (n  2) is again replicated into
C
n
2
sentences in a similar way.
7.1 Extraction using Longest Common
Subsequences (ELCS)
Blaschke et al (Blaschke and Valencia, 2001;
Blaschke and Valencia, 2002) manually developed
rules for extracting interacting proteins. Each of
their rules (or frames) is a sequence of words (or
POS tags) and two protein-name tokens. Between
every two adjacent words is a number indicating
the maximum number of intervening words allowed
when matching the rule to a sentence. In (Bunescu
et al, 2005) we described a new method ELCS (Ex-
traction using Longest Common Subsequences) that
automatically learns such rules. ELCS? rule repre-
sentation is similar to that in (Blaschke and Valen-
cia, 2001; Blaschke and Valencia, 2002), except that
it currently does not use POS tags, but allows dis-
junctions of words. Figure 5 shows an example of a
rule learned by ELCS. Words in square brackets sep-
arated by ?j? indicate disjunctive lexical constraints,
i.e. one of the given words must match the sen-
tence at that position. The numbers in parentheses
between adjacent constraints indicate the maximum
number of unconstrained words allowed between the
two (called a word gap). The protein names are de-
noted here with PROT. A sentence matches the rule
if and only if it satisfies the word constraints in the
given order and respects the respective word gaps.
- (7) interaction (0) [between j of] (5) PROT (9) PROT (17) .
Figure 5: Sample extraction rule learned by ELCS.
7.2 Extraction using a Relation Kernel (ERK)
Both Blaschke and ELCS do interaction extraction
based on a limited set of matching rules, where a rule
is simply a sparse (gappy) subsequence of words (or
POS tags) anchored on the two protein-name tokens.
Therefore, the two methods share a common limita-
tion: either through manual selection (Blaschke), or
as a result of the greedy learning procedure (ELCS),
they end up using only a subset of all possible an-
chored sparse subsequences. Ideally, we would want
to use all such anchored sparse subsequences as fea-
tures, with weights reflecting their relative accuracy.
However explicitly creating for each sentence a vec-
tor with a position for each such feature is infeasi-
ble, due to the high dimensionality of the feature
space. Here we can exploit an idea used before in
string kernels (Lodhi et al, 2002): computing the
dot-product between two such vectors amounts to
calculating the number of common anchored sub-
sequences between the two sentences. This can be
done very efficiently by modifying the dynamic pro-
gramming algorithm from (Lodhi et al, 2002) to ac-
count only for anchored subsequences i.e. sparse
subsequences which contain the two protein-name
tokens. Besides restricting the word subsequences
to be anchored on the two protein tokens, we can
further prune down the feature space by utilizing the
following property of natural language statements:
whenever a sentence asserts a relationship between
two entity mentions, it generally does this using one
of the following three patterns:
 [FI] Fore?Inter: words before and between the
two entity mentions are simultaneously used to
express the relationship. Examples: ?interac-
tion of hP
1
i with hP
2
i?, ?activation of hP
1
i by
hP
2
i?.
51
 [I] Inter: only words between the two entity
mentions are essential for asserting the rela-
tionship. Examples: ?hP
1
i interacts with hP
2
i?,
?hP
1
i is activated by hP
2
i?.
 [IA] Inter?After: words between and after the
two entity mentions are simultaneously used
to express the relationship. Examples: hP
1
i ?
hP
2
i complex?, ?hP
1
i and hP
2
i interact?.
Another useful observation is that all these pat-
terns use at most 4 words to express the relationship
(not counting the two entities). Consequently, when
computing the relation kernel, we restrict the count-
ing of common anchored subsequences only to those
having one of the three types described above, with a
maximum word-length of 4. This type of feature se-
lection leads not only to a faster kernel computation,
but also to less overfitting, which results in increased
accuracy (we omit showing here comparative results
supporting this claim, due to lack of space).
We used this kernel in conjunction with Support
Vector Machines (Vapnik, 1998) learning in or-
der to find a decision hyperplane that best separates
the positive examples from negative examples. We
modified the libsvm package for SVM learning by
plugging in the kernel described above.
7.3 Preliminary experimental results
We compare the following three systems on the task
of retrieving protein interactions from the dataset of
230 Medline abstracts (assuming gold standard pro-
teins):
 [Manual]: We report the performance of the
rule-based system of (Blaschke and Valencia,
2001; Blaschke and Valencia, 2002).
 [ELCS]: We report the 10-fold cross-validated
results from (Bunescu et al, 2005) as a
precision-recall graph.
 [ERK]: Based on the same splits as those
used by ELCS, we compute the corresponding
precision-recall graph.
The results, summarized in Figure 6, show that
the relation kernel outperforms both ELCS and the
manually written rules. In future work, we intend
to analyze the complete Medline with ERK and in-
tegrate the extracted interactions into a larger com-
posite set.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
Recall (%)
ERK
Manual
ELCS
Figure 6: PR curves for interaction extractors.
8 Conclusion
Through a combination of automatic text mining and
consolidation of existing databases, we have con-
structed a large database of known human protein
interactions containing 31,609 interactions amongst
7,748 proteins. By mining 753,459 human-related
abstracts from Medline with a combination of a
CRF-based protein tagger, co-citation analysis, and
automatic text classification, we extracted a set of
6,580 interactions between 3,737 proteins. By uti-
lizing information in existing knowledge bases, this
automatically extracted data was found to have an
accuracy comparable to manually developed data
sets. More details on our interaction database have
been published in the biological literature (Ramani
et al, 2005) and it is available on the web at
http://bioinformatics.icmb.utexas.edu/idserve. We
are currently exploring improvements to this
database by more accurately identifying assertions
of interactions in the text using an SVM that exploits
a relational string kernel.
9 Acknowledgements
This work was supported by grants from the N.S.F.
(IIS-0325116, EIA-0219061), N.I.H. (GM06779-
01), Welch (F1515), and a Packard Fellowship
(E.M.M.).
52
References
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler,
J. M. Cherry, A. P. Davis, K. Dolinski, S. S. Dwight, and J. T.
et al Eppig. 2000. Gene ontology: tool for the unification
of biology. the gene ontology consortium. Nature Genetics,
25(1):25?29.
G. D. Bader, D. Betel, and C. W. Hogue. 2003. Bind: the
biomolecular interaction network database. Nucleic Acids
Research, 31(1):248?250.
C. Blaschke and A. Valencia. 2001. Can bibliographic pointers
for known biological data be found automatically? protein
interactions as a case study. Comparative and Functional
Genomics, 2:196?206.
C. Blaschke and A. Valencia. 2002. The frame-based module
of the Suiseki information extraction system. IEEE Intelli-
gent Systems, 17:14?20.
T. Bouwmeester, A. Bauch, H. Ruffner, P. O. Angrand,
G. Bergamini, K. Croughton, C. Cruciat, D. Eberhard,
J. Gagneur, S. Ghidelli, and et al 2004. A physical and
functional map of the human tnf-alpha/nf-kappa b signal
transduction pathway. Nature Cell Biology, 6(2):97?105.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun Kumar Ramani, and
Yuk Wah Wong. 2005. Comparative experiments on learn-
ing information extractors for proteins and their interactions.
Artificial Intelligence in Medicine (special issue on Sum-
marization and Information Extraction from Medical Doc-
uments), 33(2):139?155.
F. Colland, X. Jacq, V. Trouplin, C. Mougin, C. Groizeleau,
A. Hamburger, A. Meil, J. Wojcik, P. Legrain, and J. M.
Gauthier. 2004. Functional proteomics mapping of a human
signaling pathway. Genome Research, 14(7):1324?1332.
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden, and
J. Coster. 2002. Protein names and how to find them. Inter-
national Journal of Medical Informatics, 67(1-3):49?61.
L. Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H. Wu.
2002. Accomplishments and challenges in literature data
mining for biology. Bioinformatics, 18(12):1553?1561.
R. Jansen, H. Yu, D. Greenbaum, Y. Kluger, N. J. Krogan,
S. Chung, A. Emili, M. Snyder, J. F. Greenblatt, and M. Ger-
stein. 2003. A bayesian networks approach for predict-
ing protein-protein interactions from genomic data. Science,
302(5644):449?453.
T. K. Jenssen, A. Laegreid, J. Komorowski, and E. Hovig. 2001.
A literature network of human genes for high-throughput
analysis of gene expression. Nature Genetics, 28(1):21?28.
G. Joshi-Tope, M. Gillespie, I. Vastrik, P. D?Eustachio,
E. Schmidt, B. de Bono, B. Jassal, G. R. Gopinath, G. R.
Wu, L. Matthews, and et al 2005. Reactome: a knowl-
edgebase of biological pathways. Nucleic Acids Research,
33 Database Issue:D428?432.
M. Kanehisa, S. Goto, S. Kawashima, Y. Okuno, and M. Hat-
tori. 2004. The kegg resource for deciphering the genome.
Nucleic Acids Research, 32 Database issue:D277?280.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
18th International Conference on Machine Learning (ICML-
2001), pages 282?289, Williamstown, MA.
I. Lee, S. V. Date, A. T. Adai, and E. M. Marcotte. 2004. A
probabilistic functional network of yeast genes. Science,
306(5701):1555?1558.
B. Lehner and A. G. Fraser. 2004. A first-draft human protein-
interaction map. Genome Biology, 5(9):R63.
H. Liu and L. Wong. 2003. Data mining tools for biological se-
quences. Journal of Bioinformatics and Computational Bi-
ology, 1(1):139?167.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cris-
tianini, and Chris Watkins. 2002. Text classification us-
ing string kernels. Journal of Machine Learning Research,
2:419?444.
E. M. Marcotte, I. Xenarios, and D. Eisenberg. 2001. Min-
ing literature for protein-protein interactions. Bioinformat-
ics, 17(4):359?363.
Andrew Kachites McCallum. 2002. Mallet: A machine learn-
ing for language toolkit. http://mallet.cs.umass.edu.
S. Peri, J. D. Navarro, T. Z. Kristiansen, R. Amanchy, V. Suren-
dranath, B. Muthusamy, T. K. Gandhi, K. N. Chandrika,
N. Deshpande, S. Suresh, and et al 2004. Human protein
reference database as a discovery resource for proteomics.
Nucleic Acids Research, 32 Database issue:D497?501.
A. K. Ramani, R. C. Bunescu, R. J. Mooney, and E. M. Mar-
cotte. 2005. Consolidating the set of know human protein-
protein interactions in preparation for large-scale mapping of
the human interactome. Genome Biology, 6(5):r40.
A. Rzhetsky, I. Iossifov, T. Koike, M. Krauthammer, P. Kra,
M. Morris, H. Yu, P. A. Duboue, W. Weng, W. J. Wilbur,
V. Hatzivassiloglou, and C. Friedman. 2004. Geneways: a
system for extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical Informatics,
37(1):43?53.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. John
Wiley & Sons.
C. von Mering, R. Krause, B. Snel, M. Cornell, S. G. Oliver,
S. Fields, and P. Bork. 2002. Comparative assessment of
large-scale data sets of protein-protein interactions. Nature,
417(6887):399?403.
I. Xenarios, L. Salwinski, X. J. Duan, P. Higney, S. M. Kim, and
D. Eisenberg. 2002. Dip, the database of interacting pro-
teins: a research tool for studying cellular networks of pro-
tein interactions. Nucleic Acids Research, 30(1):303?305.
53
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 49?56,
New York City, June 2006. c?2006 Association for Computational Linguistics
Integrating Co-occurrence Statistics with Information Extraction for
Robust Retrieval of Protein Interactions from Medline
Razvan Bunescu, Raymond Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan@cs.utexas.edu
mooney@cs.utexas.edu
Arun Ramani, Edward Marcotte
Institute for Cellular and Molecular Biology
University of Texas at Austin
1 University Station A4800
Austin, TX 78712
arun@icmb.utexas.edu
marcotte@icmb.utexas.edu
Abstract
The task of mining relations from collec-
tions of documents is usually approached
in two different ways. One type of sys-
tems do relation extraction from individ-
ual sentences, followed by an aggrega-
tion of the results over the entire collec-
tion. Other systems follow an entirely dif-
ferent approach, in which co-occurrence
counts are used to determine whether the
mentioning together of two entities is due
to more than simple chance. We show
that increased extraction performance can
be obtained by combining the two ap-
proaches into an integrated relation ex-
traction model.
1 Introduction
Information Extraction (IE) is a natural language
processing task in which text documents are ana-
lyzed with the aim of finding mentions of relevant
entities and important relationships between them.
In many cases, the subtask of relation extraction re-
duces to deciding whether a sentence asserts a par-
ticular relationship between two entities, which is
still a difficult, unsolved problem. There are how-
ever cases where the decision whether the two enti-
ties are in a relationship is made relative to an en-
tire document, or a collection of documents. In the
biomedical domain, for example, one may be inter-
ested in finding the pairs of human proteins that are
said to be interacting in any of the Medline abstracts,
where the answer is not required to specify which
abstracts are actually describing the interaction. As-
sembling a ranked list of interacting proteins can be
very useful to biologists - based on this list, they can
make more informed decisions with respect to which
genes to focus on in their research.
In this paper, we investigate methods that use
multiple occurrences of the same pair of entities
across a collection of documents in order to boost
the performance of a relation extraction system.
The proposed methods are evaluated on the task
of finding pairs of human proteins whose interac-
tions are reported in Medline abstracts. The major-
ity of known human protein interactions are derived
from individual, small-scale experiments reported in
Medline. Some of these interactions have already
been collected in the Reactome (Joshi-Tope et al,
2005), BIND (Bader et al, 2003), DIP (Xenarios et
al., 2002), and HPRD (Peri et al, 2004) databases.
The amount of human effort involved in creating and
updating these databases is currently no match for
the continuous growth of Medline. It is therefore
very useful to have a method that automatically and
reliably extracts interaction pairs from Medline.
Systems that do relation extraction from a col-
lection of documents can be divided into two ma-
jor categories. In one category are IE systems
that first extract information from individual sen-
tences, and then combine the results into corpus-
level results (Craven, 1999; Skounakis and Craven,
2003). The second category corresponds to ap-
proaches that do not exploit much information from
the context of individual occurrences. Instead,
based on co-occurrence counts, various statistical
49
or information-theoretic tests are used to decide
whether the two entities in a pair appear together
more often than simple chance would predict (Lee
et al, 2004; Ramani et al, 2005). We believe that
a combination of the two approaches can inherit the
advantages of each method and lead to improved re-
lation extraction accuracy.
The following two sections describe the two or-
thogonal approaches to corpus-level relation extrac-
tion. A model that integrates the two approaches is
then introduced in Section 4. This is followed by a
description of the dataset used for evaluation in Sec-
tion 5, and experimental results in Section 6.
2 Sentence-level relation extraction
Most systems that identify relations between enti-
ties mentioned in text documents consider only pair
of entities that are mentioned in the same sentence
(Ray and Craven, 2001; Zhao and Grishman, 2005;
Bunescu and Mooney, 2005). To decide the exis-
tence and the type of a relationship, these systems
generally use lexico-semantic clues inferred from
the sentence context of the two entities. Much re-
search has been focused recently on automatically
identifying biologically relevant entities and their
relationships such as protein-protein interactions or
subcellular localizations. For example, the sentence
?TR6 specifically binds Fas ligand?, states an inter-
action between the two proteins TR6 and Fas ligand.
One of the first systems for extracting interactions
between proteins is described in (Blaschke and Va-
lencia, 2001). There, sentences are matched deter-
ministically against a set of manually developed pat-
terns, where a pattern is a sequence of words or Part-
of-Speech (POS) tags and two protein-name tokens.
Between every two adjacent words is a number in-
dicating the maximum number of words that can be
skipped at that position. An example is: ?interaction
of (3) <P> (3) with (3) <P>?. This approach is
generalized in (Bunescu and Mooney, 2005), where
subsequences of words (or POS tags) from the sen-
tence are used as implicit features. Their weights are
learned by training a customized subsequence ker-
nel on a dataset of Medline abstracts annotated with
proteins and their interactions.
A relation extraction system that works at the
sentence-level and which outputs normalized confi-
dence values for each extracted pair of entities can
also be used for corpus-level relation extraction. A
straightforward way to do this is to apply an aggre-
gation operator over the confidence values inferred
for all occurrences of a given pair of entities. More
exactly, if p
1
and p
2
are two entities that occur in a
total of n sentences s
1
, s
2
, ..., s
n
in the entire corpus
C , then the confidence P (R(p
1
; p
2
)jC) that they are
in a particular relationship R is defined as:
P (R(p
1
; p
2
)jC) =  (fP (R(p
1
; p
2
)js
i
)ji=1:ng)
Table 1 shows only four of the many possible
choices for the aggregation operator  .
max  
max
= max
i
P (R(p
1
; p
2
)js
i
)
noisy-or  
nor
= 1  
Y
i
(1   P (R(p
1
; p
2
)js
i
))
avg  
avg
=
X
i
P (R(p
1
; p
2
)js
i
)
n
and  
and
=
Y
i
P (R(p
1
; p
2
)js
i
)
1=n
Table 1: Aggregation Operators.
Out of the four operators in Table 1, we believe
that the max operator is the most appropriate for ag-
gregating confidence values at the corpus-level. The
question that needs to be answered is whether there
is a sentence somewhere in the corpus that asserts
the relationship R between entities p
1
and p
2
. Us-
ing avg instead would answer a different question -
whether R(p
1
; p
2
) is true in most of the sentences
containing p
1
and p
2
. Also, the and operator would
be most appropriate for finding whether R(p
1
; p
2
)
is true in all corresponding sentences in the corpus.
The value of the noisy-or operator (Pearl, 1986) is
too dependent on the number of occurrences, there-
fore it is less appropriate for a corpus where the oc-
currence counts vary from one entity pair to another
(as confirmed in our experiments from Section 6).
For examples, if the confidence threshold is set at
0:5, and the entity pair (p
1
; p
2
) occurs in 6 sentences
or less, each with confidence 0:1, then R(p
1
; p
2
) is
false, according to the noisy-or operator. However,
if (p
1
; p
2
) occur in more than 6 sentences, with the
same confidence value of 0:1, then the correspond-
ing noisy-or value exceeds 0:5, making R(p
1
; p
2
)
true.
50
3 Co-occurrence statistics
Given two entities with multiple mentions in a large
corpus, another approach to detect whether a re-
lationship holds between them is to use statistics
over their occurrences in textual patterns that are
indicative for that relation. Various measures such
as pointwise mutual information (PMI) , chi-square
(2) or log-likelihood ratio (LLR) (Manning and
Schu?tze, 1999) use the two entities? occurrence
statistics to detect whether their co-occurrence is due
to chance, or to an underlying relationship.
A recent example is the co-citation approach from
(Ramani et al, 2005), which does not try to find spe-
cific assertions of interactions in text, but rather ex-
ploits the idea that if many different abstracts refer-
ence both protein p
1
and protein p
2
, then p
1
and p
2
are likely to interact. Particularly, if the two proteins
are co-cited significantly more often than one would
expect if they were cited independently at random,
then it is likely that they interact. The model used
to compute the probability of random co-citation is
based on the hypergeometric distribution (Lee et al,
2004; Jenssen et al, 2001). Thus, if N is the total
number of abstracts, n of which cite the first protein,
m cite the second protein, and k cite both, then the
probability of co-citation under a random model is:
P (kjN;m; n) =

n
k

N   n
m  k


N
m
 (1)
The approach that we take in this paper is to con-
strain the two proteins to be mentioned in the same
sentence, based on the assumption that if there is
a reason for two protein names to co-occur in the
same sentence, then in most cases that is caused by
their interaction. To compute the ?degree of inter-
action? between two proteins p
1
and p
2
, we use the
information-theoretic measure of pointwise mutual
information (Church and Hanks, 1990; Manning
and Schu?tze, 1999), which is computed based on the
following quantities:
1. N : the total number of protein pairs co-
occurring in the same sentence in the corpus.
2. P (p
1
; p
2
) ' n
12
=N : the probability that p
1
and p
2
co-occur in the same sentence; n
12
= the
number of sentences mentioning both p
1
and
p
2
.
3. P (p
1
; p) ' n
1
=N : the probability that p
1
co-
occurs with any other protein in the same sen-
tence; n
1
= the number of sentences mention-
ing p
1
and p.
4. P (p
2
; p) ' n
2
=N : the probability that p
2
co-
occurs with any other protein in the same sen-
tence; n
2
= the number of sentences mention-
ing p
2
and p.
The PMI is then defined as in Equation 2 below:
PMI(p
1
; p
2
) = log
P (p
1
; p
2
)
P (p
1
; p)  P (p
2
; p)
' logN
n
12
n
1
 n
2
(2)
Given that the PMI will be used only for ranking
pairs of potentially interacting proteins, the constant
factor N and the log operator can be ignored. For
sake of simplicity, we use the simpler formula from
Equation 3.
sPMI(p
1
; p
2
) =
n
12
n
1
 n
2
(3)
4 Integrated model
The sPMI(p
1
; p
2
) formula can be rewritten as:
sPMI(p
1
; p
2
) =
1
n
1
 n
2

n
12
X
i=1
1 (4)
Let s
1
, s
2
, ..., s
n
12
be the sentence contexts corre-
sponding to the n
12
co-occurrences of p
1
and p
2
,
and assume that a sentence-level relation extractor
is available, with the capability of computing nor-
malized confidence values for all extractions. Then
one way of using the extraction confidence is to have
each co-occurrence weighted by its confidence, i.e.
replace the constant 1 with the normalized scores
P (R(p
1
; p
2
)js
i
), as illustrated in Equation 5. This
results in a new formula wPMI (weighted PMI),
which is equal with the product between sPMI and
the average aggregation operator  
avg
.
wPMI(p
1
; p
2
) =
1
n
1
 n
2

n
12
X
i=1
P (R(p
1
; p
2
)js
i
)
=
n
12
n
1
 n
2
  
avg
(5)
51
The operator  
avg
can be replaced with any other ag-
gregation operator from Table 1. As argued in Sec-
tion 2, we consider max to be the most appropriate
operator for our task, therefore the integrated model
is based on the weighted PMI product illustrated in
Equation 6.
wPMI(p
1
; p
2
) =
n
12
n
1
 n
2
  
max
(6)
=
n
12
n
1
 n
2
 max
i
P (R(p
1
; p
2
)js
i
)
If a pair of entities p
1
and p
2
is ranked by wPMI
among the top pairs, this means that it is unlikely
that p
1
and p
2
have co-occurred together in the en-
tire corpus by chance, and at the same time there is
at least one mention where the relation extractor de-
cides with high confidence that R(p
1
; p
2
) = 1.
5 Evaluation Corpus
Contrasting the performance of the integrated model
against the sentence-level extractor or the PMI-
based ranking requires an evaluation dataset that
provides two types of annotations:
1. The complete list of interactions reported in the
corpus (Section 5.1).
2. Annotation of mentions of genes and proteins,
together with their corresponding gene identi-
fiers (Section 5.2).
We do not differentiate between genes and their
protein products, mapping them to the same gene
identifiers. Also, even though proteins may partic-
ipate in different types of interactions, we are con-
cerned only with detecting whether they interact in
the general sense of the word.
5.1 Medline Abstracts and Interactions
In order to compile an evaluation corpus and an as-
sociated comprehensive list of interactions, we ex-
ploited information contained in the HPRD (Peri
et al, 2004) database. Every interaction listed in
HPRD is linked to a set of Medline articles where the
corresponding experiment is reported. More exactly,
each interaction is specified in the database as a tuple
that contains the LocusLink (now EntrezGene) iden-
tifiers of all genes involved and the PubMed identi-
fiers of the corresponding articles (as illustrated in
Table 2).
Interaction (XML) (HPRD)
<interaction>
<gene>2318</gene>
<gene>58529</gene>
<pubmed>10984498 11171996</pubmed>
</interaction>
Participant Genes (XML) (NCBI)
<gene id=?2318?>
<name>FLNC</name>
<description>filamin C, gamma</description>
<synonyms>
<synonym>ABPA</synonym>
<synonym>ABPL</synonym>
<synonym>FLN2</synonym>
<synonym>ABP-280</synonym>
<synonym>ABP280A</synonym>
</synonyms>
<proteins>
<protein>gamma filamin</protein>
<protein>filamin 2</protein>
<protein>gamma-filamin</protein>
<protein>ABP-L, gamma filamin</protein>
<protein>actin-binding protein 280</protein>
<protein>gamma actin-binding protein</protein>
<protein>filamin C, gamma</protein>
</proteins>
</gene>
<gene id=?58529?>
<name>MYOZ1</name>
<description>myozenin 1</description>
<synonyms> ... </synonyms>
<proteins> ... </proteins>
</gene>
Medline Abstract (XML) (NCBI)
<PMID>10984498</PMID>
<AbstractText>
We found that this protein binds to three other Z-disc pro-
teins; therefore, we have named it FATZ, gamma-filamin,
alpha-actinin and telethonin binding protein of the Z-disc.
</AbstractText>
Table 2: Interactions, Genes and Abstracts.
The evaluation corpus (henceforth referred to as
the HPRD corpus) is created by collecting the Med-
line abstracts corresponding to interactions between
human proteins, as specified in HPRD. In total,
5,617 abstracts are included in this corpus, with an
associated list of 7,785 interactions. This list is com-
prehensive - the HPRD database is based on an an-
notation process in which the human annotators re-
port all interactions described in a Medline article.
On the other hand, the fact that only abstracts are
included in the corpus (as opposed to including the
full article) means that the list may contain interac-
tions that are not actually reported in the HPRD cor-
pus. Nevertheless, if the abstracts were annotated
52
with gene mentions and corresponding GIDs, then
a ?quasi-exact? interaction list could be computed
based on the following heuristic:
[H] If two genes with identifiers gid
1
and gid
2
are
mentioned in the same sentence in an abstract with
PubMed identifier pmid, and if gid
1
and gid
2
are
participants in an interaction that is linked to pmid
in HPRD, then consider that the abstract (and con-
sequently the entire HPRD corpus) reports the inter-
action between gid
1
and gid
2
. 
An application of the above heuristic is shown at
the bottom of Table 2. The HPRD record at the
top of the table specifies that the Medline article
with ID 10984498 reports an interaction between the
proteins FATZ (with ID 58529) and gamma-filamin
(with ID 2318). The two protein names are men-
tioned in a sentence in the abstract for 10984498,
therefore, by [H], we consider that the HPRD cor-
pus reports this interaction.
This is very similar to the procedure used in
(Craven, 1999) for creating a ?weakly-labeled?
dataset of subcellular-localization relations. [H] is
a strong heuristic ? it is already known that the full
article reports an interaction between the two genes.
Finding the two genes collocated in the same sen-
tence in the abstract is very likely to be due to the
fact that the abstract discusses their interaction. The
heuristic can be made even more accurate if a pair
of genes is considered as interacting only if they co-
occur in a (predefined) minimum number of sen-
tences in the entire corpus ? with the evaluation
modified accordingly, as described later in Section 6.
5.2 Gene Name Annotation and Normalization
For the annotation of gene names and their normal-
ization, we use a dictionary-based approach similar
to (Cohen, 2005). NCBI1 provides a comprehen-
sive dictionary of human genes, where each gene is
specified by its unique identifier, and qualified with
an official name, a description, synonym names and
one or more protein names, as illustrated in Table 2.
All of these names, including the description, are
considered as potential referential expressions for
the gene entity. Each name string is reduced to a
normal form by: replacing dashes with spaces, intro-
ducing spaces between sequences of letters and se-
1URL: http://www.ncbi.nih.gov
quences of digits, replacing Greek letters with their
Latin counterparts (capitalized), substituting Roman
numerals with Arabic numerals, decapitalizing the
first word if capitalized. All names are further tok-
enized, and checked against a dictionary of close to
100K English nouns. Names that are found in this
dictionary are simply filtered out. We also ignore
all ambiguous names (i.e. names corresponding to
more than one gene identifier). The remaining non-
ambiguous names are added to the final gene dictio-
nary, which is implemented as a trie-like structure in
order to allow a fast lookup of gene IDs based on the
associated normalized sequences of tokens.
Each abstract from the HPRD corpus is tokenized
and segmented in sentences using the OpenNLP2
package. The resulting sentences are then annotated
by traversing them from left to right and finding the
longest token sequences whose normal forms match
entries from the gene dictionary.
6 Experimental Evaluation
The main purpose of the experiments in this section
is to compare the performance of the following four
methods on the task of corpus-level relation extrac-
tion:
1. Sentence-level relation extraction followed by
the application of an aggregation operator that
assembles corpus-level results (SSK.Max).
2. Pointwise Mutual Information (PMI).
3. The integrated model, a product of the two base
models (PMI.SSK.Max).
4. The hypergeometric co-citation method (HG).
7 Experimental Methodology
All abstracts, either from the HPRD corpus, or
from the entire Medline, are annotated using the
dictionary-based approach described in Section 5.2.
The sentence-level extraction is done with the sub-
sequence kernel (SSK) approach from (Bunescu and
Mooney, 2005), which was shown to give good re-
sults on extracting interactions from biomedical ab-
stracts. The subsequence kernel was trained on a
set of 225 Medline abstracts which were manually
2URL: http://opennlp.sourceforge.net
53
annotated with protein names and their interactions.
It is known that PMI gives undue importance to
low frequency events (Dunning, 1993), therefore the
evaluation considers only pairs of genes that occur at
least 5 times in the whole corpus.
When evaluating corpus-level extraction on
HPRD, because the ?quasi-exact? list of interactions
is known, we report the precision-recall (PR) graphs,
where the precision (P) and recall (R) are computed
as follows:
P =
#true interactions extracted
#total interaction extracted
R =
#true interactions extracted
#true interactions
All pairs of proteins are ranked based on each scor-
ing method, and precision recall points are com-
puted by considering the top N pairs, where N
varies from 1 to the total number of pairs.
When evaluating on the entire Medline, we used
the shared protein function benchmark described in
(Ramani et al, 2005). Given the set of interacting
pairs recovered at each recall level, this benchmark
calculates the extent to which interaction partners
in a data set share functional annotation, a measure
previously shown to correlate with the accuracy of
functional genomics data sets (Lee et al, 2004). The
KEGG (Kanehisa et al, 2004) and Gene Ontology
(Ashburner et al, 2000) databases provide specific
pathway and biological process annotations for ap-
proximately 7,500 human genes, assigning human
genes into 155 KEGG pathways (at the lowest level
of KEGG) and 1,356 GO pathways (at level 8 of the
GO biological process annotation).
The scoring scheme for measuring interaction set
accuracy is in the form of a log odds ratio of gene
pairs sharing functional annotations. To evaluate a
data set, a log likelihood ratio (LLR) is calculated as
follows:
LLR = ln
P (DjI)
P (Dj:I)
= ln
P (IjD)P (:I)
P (:IjD)P (I)
(7)
where P (DjI) and P (Dj:I) are the probability
of observing the data D conditioned on the genes
sharing benchmark associations (I) and not sharing
benchmark associations (:I). In its expanded form
(obtained by Bayes theorem), P (IjD) and P (:IjD)
are estimated using the frequencies of interactions
observed in the given data set D between annotated
genes sharing benchmark associations and not shar-
ing associations, respectively, while the priors P (I)
and P (:I) are estimated based on the total frequen-
cies of all benchmark genes sharing the same asso-
ciations and not sharing associations, respectively.
A score of zero indicates interaction partners in the
data set being tested are no more likely than random
to belong to the same pathway or to interact; higher
scores indicate a more accurate data set.
8 Experimental Results
The results for the HPRD corpus-level extraction are
shown in Figure 1. Overall, the integrated model has
a more consistent performance, with a gain in preci-
sion mostly at recall levels past 40%. The SSK.Max
and HG models both exhibit a sudden decrease in
precision at around 5% recall level. While SSK.Max
goes back to a higher precision level, the HG model
begins to recover only late at 70% recall.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
isi
on
Recall
PMI.SSK.Max
PMI
SSK.Max
HG
Figure 1: PR curves for corpus-level extraction.
A surprising result in this experiment is the be-
havior of the HG model, which is significantly out-
performed by PMI, and which does only marginally
better than a simple baseline that considers all pairs
to be interacting.
We also compared the two methods on corpus-
level extraction from the entire Medline, using the
shared protein function benchmark. As before, we
considered only protein pairs occurring in the same
54
sentence, with a minimum frequency count of 5. The
resulting 47,436 protein pairs were ranked accord-
ing to their PMI and HG scores, with pairs that are
most likely to be interacting being placed at the top.
For each ranking, the LLR score was computed for
the top N proteins, where N varied in increments of
1,000.
The comparative results for PMI and HG are
shown in Figure 2, together with the scores for three
human curated databases: HPRD, BIND and Reac-
tome. On the top 18,000 protein pairs, PMI outper-
forms HG substantially, after which both converge
to the same value for all the remaining pairs.
 2
 2.25
 2.5
 2.75
 3
 3.25
 3.5
 3.75
 4
 4.25
 4.5
 4.75
 5
 2500  5000  7500  10000 12500 15000 17500 20000 22500 25000
LL
R
Top N pairs
PMI
HG
HPRD
BIND
Reactome
Figure 2: Functional annotation benchmark.
Figure 3 shows a comparison of the four aggre-
gation operators on the same HPRD corpus, which
confirms that, overall, max is most appropriate for
integrating corpus-level results.
9 Future Work
The piece of related work that is closest to the aim of
this paper is the Bayesian approach from (Skounakis
and Craven, 2003). In their probabilistic model, co-
occurrence statistics are taken into account by using
a prior probability that a pair of proteins are inter-
acting, given the number of co-occurrences in the
corpus. However, they do not use the confidences of
the sentence-level extractions. The GeneWays sys-
tem from (Rzhetsky et al, 2004) takes a different
approach, in which co-occurrence frequencies are
simply used to re-rank the ouput from the relation
extractor.
An interesting direction for future research is to
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
isi
on
Recall
Max
Noisy-Or
Avg
And
Figure 3: PR curves for aggregation operators.
design a model that takes into account both the ex-
traction confidences and the co-occurrence statis-
tics, without losing the probabilistic (or information-
theoretic) interpretation. One could investigate ways
of integrating the two orthogonal approaches to
corpus-level extraction based on other statistical
tests, such as chi-square and log-likelihood ratio.
The sentence-level extractor used in this paper
was trained to recognize relation mentions in iso-
lation. However, the trained model is later used,
through the max aggregation operator, to recognize
whether multiple mentions of the same pair of pro-
teins indicate a relationship between them. This
points to a fundamental mismatch between the train-
ing and testing phases of the model. We expect that
better accuracy can be obtained by designing an ap-
proach that is using information from multiple oc-
currences of the same pair in both training and test-
ing.
10 Conclusion
Extracting relations from a collection of documents
can be approached in two fundamentally different
ways. In one approach, an IE system extracts rela-
tion instances from corpus sentences, and then ag-
gregates the local extractions into corpus-level re-
sults. In the second approach, statistical tests based
on co-occurrence counts are used for deciding if a
given pair of entities are mentioned together more
often than chance would predict. We have described
55
a method to integrate the two approaches, and given
experimental results that confirmed our intuition that
an integrated model would have a better perfor-
mance.
11 Acknowledgements
This work was supported by grants from the N.S.F.
(IIS-0325116, EIA-0219061), N.I.H. (GM06779-
01), Welch (F1515), and a Packard Fellowship
(E.M.M.).
References
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler,
J. M. Cherry, A. P. Davis, K. Dolinski, S. S. Dwight, and J. T.
et al Eppig. 2000. Gene ontology: tool for the unification
of biology. the gene ontology consortium. Nature Genetics,
25(1):25?29.
G. D. Bader, D. Betel, and C. W. Hogue. 2003. Bind: the
biomolecular interaction network database. Nucleic Acids
Research, 31(1):248?250.
C. Blaschke and A. Valencia. 2001. Can bibliographic pointers
for known biological data be found automatically? protein
interactions as a case study. Comparative and Functional
Genomics, 2:196?206.
Razvan C. Bunescu and Raymond J. Mooney. 2005. Subse-
quence kernels for relation extraction. In Proceedings of the
Conference on Neural Information Processing Systems, Van-
couver, BC.
Kenneth W. Church and Patrick W. Hanks. 1990. Word associ-
ation norms, mutual information and lexicography. Compu-
tational Linguistics, 16(1):22?29.
Aaron M. Cohen. 2005. Unsupervised gene/protein named en-
tity normalization using automatically extracted dictionaries.
In Proceedings of the ACL-ISMB Workshop on Linking Bio-
logical Literature, Ontologies and Databases: Minining Bi-
ological Semantics, pages 17?24, Detroit, MI.
Mark Craven. 1999. Learning to extract relations from MED-
LINE. In Papers from the Sixteenth National Conference
on Artificial Intelligence (AAAI-99) Workshop on Machine
Learning for Information Extraction, pages 25?30, July.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61?74.
T. K. Jenssen, A. Laegreid, J. Komorowski, and E. Hovig. 2001.
A literature network of human genes for high-throughput
analysis of gene expression. Nature Genetics, 28(1):21?28.
G. Joshi-Tope, M. Gillespie, I. Vastrik, P. D?Eustachio,
E. Schmidt, B. de Bono, B. Jassal, G. R. Gopinath, G. R.
Wu, L. Matthews, and et al 2005. Reactome: a knowl-
edgebase of biological pathways. Nucleic Acids Research,
33 Database Issue:D428?432.
M. Kanehisa, S. Goto, S. Kawashima, Y. Okuno, and M. Hat-
tori. 2004. The KEGG resource for deciphering the genome.
Nucleic Acids Research, 32 Database issue:D277?280.
I. Lee, S. V. Date, A. T. Adai, and E. M. Marcotte. 2004. A
probabilistic functional network of yeast genes. Science,
306(5701):1555?1558.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press, Cambridge, MA.
Judea Pearl. 1986. Fusion, propagation, and structuring in be-
lief networks. Artificial Intelligence, 29(3):241?288.
S. Peri, J. D. Navarro, T. Z. Kristiansen, R. Amanchy, V. Suren-
dranath, B. Muthusamy, T. K. Gandhi, K. N. Chandrika,
N. Deshpande, S. Suresh, and et al 2004. Human protein
reference database as a discovery resource for proteomics.
Nucleic Acids Research, 32 Database issue:D497?501.
A. K. Ramani, R. C. Bunescu, R. J. Mooney, and E. M. Mar-
cotte. 2005. Consolidating the set of know human protein-
protein interactions in preparation for large-scale mapping of
the human interactome. Genome Biology, 6(5):r40.
Soumya Ray and Mark Craven. 2001. Representing sentence
structure in hidden Markov models for information extrac-
tion. In Proceedings of the Seventeenth International Joint
Conference on Artificial Intelligence (IJCAI-2001), pages
1273?1279, Seattle, WA.
A. Rzhetsky, T. Iossifov, I. Koike, M. Krauthammer, P. Kra,
M. Morris, H. Yu, P.A. Duboue, W. Weng, W.J. Wilbur,
V. Hatzivassiloglou, and C. Friedman. 2004. GeneWays: a
system for extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical Informatics,
37:43?53.
Marios Skounakis and Mark Craven. 2003. Evidence combina-
tion in biomedical natural-language processing. In Proceed-
ings of the 3nd ACM SIGKDD Workshop on Data Mining in
Bioinformatics (BIOKDD 2003), pages 25?32, Washington,
DC.
I. Xenarios, L. Salwinski, X. J. Duan, P. Higney, S. M. Kim, and
D. Eisenberg. 2002. DIP, the database of interacting pro-
teins: a research tool for studying cellular networks of pro-
tein interactions. Nucleic Acids Research, 30(1):303?305.
Shubin Zhao and Ralph Grishman. 2005. Extracting relations
with integrated information using kernel methods. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 419?426, Ann
Arbor, Michigan, June. Association for Computational Lin-
guistics.
56
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 546?554,
Beijing, August 2010
Learning to Predict Readability using Diverse Linguistic Features
Rohit J. Kate1 Xiaoqiang Luo2 Siddharth Patwardhan2 Martin Franz2
Radu Florian2 Raymond J. Mooney1 Salim Roukos2 Chris Welty2
1Department of Computer Science
The University of Texas at Austin
{rjkate,mooney}@cs.utexas.edu
2IBM Watson Research Center
{xiaoluo,spatward,franzm,raduf,roukos,welty}@us.ibm.com
Abstract
In this paper we consider the problem of
building a system to predict readability
of natural-language documents. Our sys-
tem is trained using diverse features based
on syntax and language models which are
generally indicative of readability. The
experimental results on a dataset of docu-
ments from a mix of genres show that the
predictions of the learned system are more
accurate than the predictions of naive hu-
man judges when compared against the
predictions of linguistically-trained expert
human judges. The experiments also com-
pare the performances of different learn-
ing algorithms and different types of fea-
ture sets when used for predicting read-
ability.
1 Introduction
An important aspect of a document is whether it
is easily processed and understood by a human
reader as intended by its writer, this is termed
as the document?s readability. Readability in-
volves many aspects including grammaticality,
conciseness, clarity, and lack of ambiguity. Teach-
ers, journalists, editors, and other professionals
routinely make judgements on the readability of
documents. We explore the task of learning to
automatically judge the readability of natural-
language documents.
In a variety of applications it would be useful to
be able to automate readability judgements. For
example, the results of a web-search can be or-
dered taking into account the readability of the
retrieved documents thus improving user satisfac-
tion. Readability judgements can also be used
for automatically grading essays, selecting in-
structional reading materials, etc. If documents
are generated by machines, such as summariza-
tion or machine translation systems, then they are
prone to be less readable. In such cases, a read-
ability measure can be used to automatically fil-
ter out documents which have poor readability.
Even when the intended consumers of text are
machines, for example, information extraction or
knowledge extraction systems, a readability mea-
sure can be used to filter out documents of poor
readability so that the machine readers will not ex-
tract incorrect information because of ambiguity
or lack of clarity in the documents.
As part of the DARPA Machine Reading Pro-
gram (MRP), an evaluation was designed and con-
ducted for the task of rating documents for read-
ability. In this evaluation, 540 documents were
rated for readability by both experts and novice
human subjects. Systems were evaluated based on
whether they were able to match expert readabil-
ity ratings better than novice raters. Our system
learns to match expert readability ratings by em-
ploying regression over a set of diverse linguistic
features that were deemed potentially relevant to
readability. Our results demonstrate that a rich
combination of features from syntactic parsers,
language models, as well as lexical statistics all
contribute to accurately predicting expert human
readability judgements. We have also considered
the effect of different genres in predicting read-
ability and how the genre-specific language mod-
els can be exploited to improve the readability pre-
dictions.
546
2 Related Work
There is a significant amount of published work
on a related problem: predicting the reading diffi-
culty of documents, typically, as the school grade-
level of the reader from grade 1 to 12. Some early
methods measure simple characteristics of docu-
ments like average sentence length, average num-
ber of syllables per word, etc. and combine them
using a linear formula to predict the grade level of
a document, for example FOG (Gunning, 1952),
SMOG (McLaughlin, 1969) and Flesh-Kincaid
(Kincaid et al, 1975) metrics. These methods
do not take into account the content of the doc-
uments. Some later methods use pre-determined
lists of words to determine the grade level of a
document, for example the Lexile measure (Sten-
ner et al, 1988), the Fry Short Passage measure
(Fry, 1990) and the Revised Dale-Chall formula
(Chall and Dale, 1995). The word lists these
methods use may be thought of as very simple
language models. More recently, language mod-
els have been used for predicting the grade level
of documents. Si and Callan (2001) and Collins-
Thompson and Callan (2004) train unigram lan-
guage models to predict grade levels of docu-
ments. In addition to language models, Heilman
et al (2007) and Schwarm and Ostendorf (2005)
also use some syntactic features to estimate the
grade level of texts.
Pitler and Nenkova (2008) consider a differ-
ent task of predicting text quality for an educated
adult audience. Their system predicts readabil-
ity of texts from Wall Street Journal using lex-
ical, syntactic and discourse features. Kanungo
and Orr (2009) consider the task of predicting
readability of web summary snippets produced by
search engines. Using simple surface level fea-
tures like the number of characters and syllables
per word, capitalization, punctuation, ellipses etc.
they train a regression model to predict readability
values.
Our work differs from this previous research in
several ways. Firstly, the task we have consid-
ered is different, we predict the readability of gen-
eral documents, not their grade level. The doc-
uments in our data are also not from any single
domain, genre or reader group, which makes our
task more general. The data includes human writ-
ten as well as machine generated documents. The
task and the data has been set this way because it
is aimed at filtering out documents of poor quality
for later processing, like for extracting machine-
processable knowledge from them. Extracting
knowledge from openly found text, such as from
the internet, is becoming popular but the quality
of text found ?in the wild?, like found through
searching the internet, vary considerably in qual-
ity and genre. If the text is of poor readability then
it is likely to lead to extraction errors and more
problems downstream. If the readers are going
to be humans instead of machines, then also it is
best to filter out poorly written documents. Hence
identifying readability of general text documents
coming from various sources and genres is an im-
portant task. We are not aware of any other work
which has considered such a task.
Secondly, we note that all of the above ap-
proaches that use language models train a lan-
guage model for each difficulty level using the
training data for that level. However, since the
amount of training data annotated with levels
is limited, they can not train higher-order lan-
guage models, and most just use unigram models.
In contrast, we employ more powerful language
models trained on large quantities of generic text
(which is not from the training data for readabil-
ity) and use various features obtained from these
language models to predict readability. Thirdly,
we use a more sophisticated combination of lin-
guistic features derived from various syntactic
parsers and language models than any previous
work. We also present ablation results for differ-
ent sets of features. Fourthly, given that the doc-
uments in our data are not from a particular genre
but from a mix of genres, we also train genre-
specific language models and show that including
these as features improves readability predictions.
Finally, we also show comparison between var-
ious machine learning algorithms for predicting
readability, none of the previous work compared
learning algorithms.
3 Readability Data
The readability data was collected and re-
leased by LDC. The documents were collected
547
from the following diverse sources or genres:
newswire/newspaper text, weblogs, newsgroup
posts, manual transcripts, machine translation out-
put, closed-caption transcripts and Wikipedia arti-
cles. Documents for newswire, machine transla-
tion and closed captioned genres were collected
automatically by first forming a candidate pool
from a single collection stream and then randomly
selecting documents. Documents for weblogs,
newsgroups and manual transcripts were also col-
lected in the same way but were then reviewed
by humans to make sure they were not simply
spam articles or something objectionable. The
Wikipedia articles were collected manually, by
searching through a data archive or the live web,
using keyword and other search techniques. Note
that the information about genres of the docu-
ments is not available during testing and hence
was not used when training our readability model.
A total of 540 documents were collected in this
way which were uniformly distributed across the
seven genres. Each document was then judged
for its readability by eight expert human judges.
These expert judges are native English speakers
who are language professionals and who have
specialized training in linguistic analysis and an-
notation, including the machine translation post-
editing task. Each document was also judged for
its readability by six to ten naive human judges.
These non-expert (naive) judges are native En-
glish speakers who are not language professionals
(e.g. editors, writers, English teachers, linguistic
annotators, etc.) and have no specialized language
analysis or linguistic annotation training. Both ex-
pert and naive judges provided readability judg-
ments using a customized web interface and gave
a rating on a 5-point scale to indicate how readable
the passage is (where 1 is lowest and 5 is highest
readability) where readability is defined as a sub-
jective judgment of how easily a reader can extract
the information the writer or speaker intended to
convey.
4 Readability Model
We want to answer the question whether a
machine can accurately estimate readability as
judged by a human. Therefore, we built a
machine-learning system that predicts the read-
ability of documents by training on expert hu-
man judgements of readability. The evaluation
was then designed to compare how well machine
and naive human judges predict expert human
judgements. In order to make the machine?s pre-
dicted score comparable to a human judge?s score
(details about our evaluation metrics are in Sec-
tion 6.1), we also restricted the machine scores to
integers. Hence, the task is to predict an integer
score from 1 to 5 that measures the readability of
the document.
This task could be modeled as a multi-class
classification problem treating each integer score
as a separate class, as done in some of the previ-
ous work (Si and Callan, 2001; Collins-Thompson
and Callan, 2004). However, since the classes
are numerical and not unrelated (for example, the
score 2 is in between scores 1 and 3), we de-
cided to model the task as a regression problem
and then round the predicted score to obtain the
closest integer value. Preliminary results verified
that regression performed better than classifica-
tion. Heilman et al (2008) also found that it
is better to treat the readability scores as ordinal
than as nominal. We take the average of the ex-
pert judge scores for each document as its gold-
standard score. Regression was also used by Ka-
nungo and Orr (2009), although their evaluation
did not constrain machine scores to be integers.
We tested several regression algorithms avail-
able in the Weka1 machine learning package, and
in Section 6.2 we report results for several which
performed best. The next section describes the
numerically-valued features that we used as input
for regression.
5 Features for Predicting Readability
Good input features are critical to the success of
any regression algorithm. We used three main cat-
egories of features to predict readability: syntac-
tic features, language-model features, and lexical
features, as described below.
5.1 Features Based on Syntax
Many times, a document is found to be unreadable
due to unusual linguistic constructs or ungram-
1http://www.cs.waikato.ac.nz/ml/weka/
548
matical language that tend to manifest themselves
in the syntactic properties of the text. There-
fore, syntactic features have been previously used
(Bernth, 1997) to gauge the ?clarity? of written
text, with the goal of helping writers improve their
writing skills. Here too, we use several features
based on syntactic analyses. Syntactic analyses
are obtained from the Sundance shallow parser
(Riloff and Phillips, 2004) and from the English
Slot Grammar (ESG) (McCord, 1989).
Sundance features: The Sundance system is a
rule-based system that performs a shallow syntac-
tic analysis of text. We expect that this analysis
over readable text would be ?well-formed?, adher-
ing to grammatical rules of the English language.
Deviations from these rules can be indications of
unreadable text. We attempt to capture such de-
viations from grammatical rules through the fol-
lowing Sundance features computed for each text
document: proportion of sentences with no verb
phrases, average number of clauses per sentence,
average sentence length in tokens, average num-
ber of noun phrases per sentence, average number
of verb phrases per sentence, average number of
prepositional phrases per sentence, average num-
ber of phrases (all types) per sentence and average
number of phrases (all types) per clause.
ESG features: ESG uses slot grammar rules to
perform a deeper linguistic analysis of sentences
than the Sundance system. ESG may consider
several different interpretations of a sentence, be-
fore deciding to choose one over the other inter-
pretations. Sometimes ESG?s grammar rules fail
to produce a single complete interpretation of a
sentence, in which case it generates partial parses.
This typically happens in cases when sentences
are ungrammatical, and possibly, less readable.
Thus, we use the proportion of such incomplete
parses within a document as a readability feature.
In case of extremely short documents, this propor-
tion of incomplete parses can be misleading. To
account for such short documents, we introduce
a variation of the above incomplete parse feature,
by weighting it with a log factor as was done in
(Riloff, 1996; Thelen and Riloff, 2002).
We also experimented with some other syn-
tactic features such as average sentence parse
scores from Stanford parser and an in-house maxi-
mum entropy statistical parer, average constituent
scores etc., however, they slightly degraded the
performance in combination with the rest of the
features and hence we did not include them in
the final set. One possible explanation could be
that averaging diminishes the effect of low scores
caused by ungrammaticality.
5.2 Features Based on Language Models
A probabilistic language model provides a predic-
tion of how likely a given sentence was generated
by the same underlying process that generated a
corpus of training documents. In addition to a
general n-gram language model trained on a large
body of text, we also exploit language models
trained to recognize specific ?genres? of text. If a
document is translated by a machine, or casually
produced by humans for a weblog or newsgroup,
it exhibits a character that is distinct from docu-
ments that go through a dedicated editing process
(e.g., newswire and Wikipedia articles). Below
we describe features based on generic as well as
genre-specific language models.
Normalized document probability: One obvi-
ous proxy for readability is the score assigned to
a document by a generic language model (LM).
Since the language model is trained on well-
written English text, it penalizes documents de-
viating from the statistics collected from the LM
training documents. Due to variable document
lengths, we normalize the document-level LM
score by the number of words and compute the
normalized document probability NP (D) for a
document D as follows:
NP (D) =
(
P (D|M)
) 1
|D| , (1)
where M is a general-purpose language model
trained on clean English text, and |D| is the num-
ber of words in the document D.
Perplexities from genre-specific language mod-
els: The usefulness of LM-based features in
categorizing text (McCallum and Nigam, 1998;
Yang and Liu, 1999) and evaluating readability
(Collins-Thompson and Callan, 2004; Heilman
et al, 2007) has been investigated in previous
work. In our experiments, however, since doc-
uments were acquired through several different
channels, such as machine translation or web logs,
549
we also build models that try to predict the genre
of a document. Since the genre information for
many English documents is readily available, we
trained a series of genre-specific 5-gram LMs us-
ing the modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Stanley and Goodman, 1996). Ta-
ble 1 contains a list of a base LM and genre-
specific LMs.
Given a document D consisting of tokenized
word sequence {wi : i = 1, 2, ? ? ? , |D|}, its per-
plexity L(D|Mj) with respect to a LM Mj is
computed as:
L(D|Mj) = e
(
? 1|D|
P|D|
i=1 logP (wi|hi;Mj)
)
, (2)
where |D| is the number of words in D and hi are
the history words for wi, and P (wi|hi;Mj) is the
probability Mj assigns to wi, when it follows the
history words hi.
Posterior perplexities from genre-specific lan-
guagemodels: While perplexities computed from
genre-specific LMs reflect the absolute probabil-
ity that a document was generated by a specific
model, a model?s relative probability compared to
other models may be a more useful feature. To this
end, we also compute the posterior perplexity de-
fined as follows. Let D be a document, {Mi}Gi=1
be G genre-specific LMs, and L(D|Mi) be the
perplexity of the document D with respect to Mi,
then the posterior perplexity, R(Mi|D), is de-
fined as:
R(Mi|D) =
L(D|Mi)?G
j=1 L(D|Mj)
. (3)
We use the term ?posterior? because if a uni-
form prior is adopted for {Mi}Gi=1,R(Mi|D) can
be interpreted as the posterior probability of the
genre LM Mi given the document D.
5.3 Lexical Features
The final set of features involve various lexical
statistics as described below.
Out-of-vocabulary (OOV) rates: We conjecture
that documents containing typographical errors
(e.g., for closed-caption and web log documents)
may receive low readability ratings. Therefore,
we compute the OOV rates of a document with re-
spect to the various LMs shown in Table 1. Since
modern LMs often have a very large vocabulary,
to get meaningful OOV rates, we truncate the vo-
cabularies to the top (i.e., most frequent) 3000
words. For the purpose of OOV computation, a
document D is treated as a sequence of tokenized
words {wi : i = 1, 2, ? ? ? , |D|}. Its OOV rate
with respect to a (truncated) vocabulary V is then:
OOV (D|V) =
?D
i=1 I(wi /? V)
|D| , (4)
where I(wi /? V) is an indicator function taking
value 1 if wi is not in V , and 0 otherwise.
Ratio of function words: A characteristic of doc-
uments generated by foreign speakers and ma-
chine translation is a failure to produce certain
function words, such as ?the,? or ?of.? So we pre-
define a small set of function words (mainly En-
glish articles and frequent prepositions) and com-
pute the ratio of function words over the total
number words in a document:
RF (D) =
?D
i=1 I(wi ? F)
|D| , (5)
where I(wi ? F) is 1 ifwi is in the set of function
words F , and 0 otherwise.
Ratio of pronouns: Many foreign languages that
are source languages of machine-translated docu-
ments are pronoun-drop languages, such as Ara-
bic, Chinese, and romance languages. We conjec-
ture that the pronoun ratio may be a good indica-
tor whether a document is translated by machine
or produced by humans, and for each document,
we first run a POS tagger, and then compute the
ratio of pronouns over the number of words in the
document:
RP (D) =
?D
i=1 I(POS(wi) ? P)
|D| , (6)
where I(POS(wi) ? F) is 1 if the POS tag of wi
is in the set of pronouns, P , and 0 otherwise.
Fraction of known words: This feature measures
the fraction of words in a document that occur
either in an English dictionary or a gazetteer of
names of people and locations.
6 Experiments
This section describes the evaluation methodol-
ogy and metrics and presents and discusses our
550
Genre Training Size(M tokens) Data Sources
base 5136.8 mostly LDC?s GigaWord set
NW 143.2 newswire subset of base
NG 218.6 newsgroup subset of base
WL 18.5 weblog subset of base
BC 1.6 broadcast conversation subset of base
BN 1.1 broadcast news subset of base
wikipedia 2264.6 Wikipedia text
CC 0.1 closed caption
ZhEn 79.6 output of Chinese to English Machine Translation
ArEn 126.8 output of Arabic to English Machine Translation
Table 1: Genre-specific LMs: the second column contains the number of tokens in LM training data (in million tokens).
experimental results. The results of the official
evaluation task are also reported.
6.1 Evaluation Metric
The evaluation process for the DARPAMRP read-
ability test was designed by the evaluation team
led by SAIC. In order to compare a machine?s
predicted readability score to those assigned by
the expert judges, the Pearson correlation coef-
ficient was computed. The mean of the expert-
judge scores was taken as the gold-standard score
for a document.
To determine whether the machine predicts
scores closer to the expert judges? scores than
what an average naive judge would predict, a
sampling distribution representing the underlying
novice performance was computed. This was ob-
tained by choosing a random naive judge for every
document, calculating the Pearson correlation co-
efficient with the expert gold-standard scores and
then repeating this procedure a sufficient number
of times (5000). The upper critical value was set
at 97.5% confidence, meaning that if the machine
performs better than the upper critical value then
we reject the null hypothesis that machine scores
and naive scores come from the same distribution
and conclude that the machine performs signifi-
cantly better than naive judges in matching the ex-
pert judges.
6.2 Results and Discussion
We evaluated our readability system on the dataset
of 390 documents which was released earlier dur-
ing the training phase of the evaluation task. We
Algorithm Correlation
Bagged Decision Trees 0.8173
Decision Trees 0.7260
Linear Regression 0.7984
SVM Regression 0.7915
Gaussian Process Regression 0.7562
Naive Judges
Upper Critical Value 0.7015
Distribution Mean 0.6517
Baselines
Uniform Random 0.0157
Proportional Random -0.0834
Table 2: Comparing different algorithms on the readability
task using 13-fold cross-validation on the 390 documents us-
ing all the features. Exceeding the upper critical value of the
naive judges? distribution indicates statistically significantly
better predictions than the naive judges.
used stratified 13-fold cross-validation in which
the documents from various genres in each fold
was distributed in roughly the same proportion as
in the overall dataset. We first conducted experi-
ments to test different regression algorithms using
all the available features. Next, we ablated various
feature sets to determine how much each feature
set was contributing to making accurate readabil-
ity judgements. These experiments are described
in the following subsections.
6.2.1 Regression Algorithms
We used several regression algorithms available
in theWeka machine learning package and Table 2
shows the results obtained. The default values
551
Feature Set Correlation
Lexical 0.5760
Syntactic 0.7010
Lexical + Syntactic 0.7274
Language Model based 0.7864
All 0.8173
Table 3: Comparison of different linguistic feature sets.
in Weka were used for all parameters, changing
these values did not show any improvement. We
used decision tree (reduced error pruning (Quin-
lan, 1987)) regression, decision tree regression
with bagging (Breiman, 1996), support vector re-
gression (Smola and Scholkopf, 1998) using poly-
nomial kernel of degree two,2 linear regression
and Gaussian process regression (Rasmussen and
Williams, 2006). The distribution mean and the
upper critical values of the correlation coefficient
distribution for the naive judges are also shown in
the table.
Since they are above the upper critical value, all
algorithms predicted expert readability scores sig-
nificantly more accurately than the naive judges.
Bagged decision trees performed slightly better
than other methods. As shown in the following
section, ablating features affects predictive accu-
racy much more than changing the regression al-
gorithm. Therefore, on this task, the choice of re-
gression algorithm was not very critical once good
readability features are used. We also tested two
simple baseline strategies: predicting a score uni-
formly at random, and predicting a score propor-
tional to its frequency in the training data. As
shown in the last two rows of Table 2, these base-
lines perform very poorly, verifying that predict-
ing readability on this dataset as evaluated by our
evaluation metric is not trivial.
6.2.2 Ablations with Feature Sets
We evaluated the contributions of different fea-
ture sets through ablation experiments. Bagged
decision-tree was used as the regression algorithm
in all of these experiments. First we compared
syntactic, lexical and language-model based fea-
tures as described in Section 5, and Table 3 shows
2Polynomial kernels with other degrees and RBF kernel
performed worse.
the results. The language-model feature set per-
forms the best, but performance improves when it
is combined with the remaining features. The lex-
ical feature set by itself performs the worst, even
below the naive distribution mean (shown in Ta-
ble 2); however, when combined with syntactic
features it performs well.
In our second ablation experiment, we com-
pared the performance of genre-independent and
genre-based features. Since the genre-based fea-
tures exploit knowledge of the genres of text used
in the MRP readability corpus, their utility is
somewhat tailored to this specific corpus. There-
fore, it is useful to evaluate the performance of the
system when genre information is not exploited.
Of the lexical features described in subsection 5.3,
the ratio of function words, ratio of pronoun words
and all of the out-of-vocabulary rates except for
the base language model are genre-based features.
Out of the language model features described in
the Subsection 5.2, all of the perplexities except
for the base language model and all of the poste-
rior perplexities3 are genre-based features. All of
the remaining features are genre-independent. Ta-
ble 4 shows the results comparing these two fea-
ture sets. The genre-based features do well by
themselves but the rest of the features help fur-
ther improve the performance. While the genre-
independent features by themselves do not exceed
the upper critical value of the naive judges? dis-
tribution, they are very close to it and still out-
perform its mean value. These results show that
for a dataset like ours, which is composed of a mix
of genres that themselves are indicative of read-
ability, features that help identify the genre of a
text improve performance significantly.4 For ap-
plications mentioned in the introduction and re-
lated work sections, such as filtering less readable
documents from web-search, many of the input
documents could come from some of the common
genres considered in our dataset.
In our final ablation experiment, we evaluated
3Base model for posterior perplexities is computed using
other genre-based LMs (equation 3) hence it can not be con-
sidered genre-independent.
4We note that none of the genre-based features were
trained on supervised readability data, but were trained on
readily-available large unannotated corpora as shown in Ta-
ble 1.
552
Feature Set Correlation
Genre-independent 0.6978
Genre-based 0.7749
All 0.8173
Table 4: Comparison of genre-independent and genre-
based feature sets.
Feature Set By itself Ablated
from All
Sundance features 0.5417 0.7993
ESG features 0.5841 0.8118
Perplexities 0.7092 0.8081
Posterior perplexities 0.7832 0.7439
Out-of-vocabulary rates 0.3574 0.8125
All 0.8173 -
Table 5: Ablations with some individual feature sets.
the contribution of various individual feature sets.
Table 5 shows that posterior perplexities perform
the strongest on their own, but without them, the
remaining features also do well. When used by
themselves, some feature sets perform below the
naive judges? distribution mean, however, remov-
ing them from the rest of the feature sets de-
grades the performance. This shows that no indi-
vidual feature set is critical for good performance
but each further improves the performance when
added to the rest of the feature sets.
6.3 Official Evaluation Results
An official evaluation was conducted by the eval-
uation team SAIC on behalf of DARPA in which
three teams participated including ours. The eval-
uation task required predicting the readability of
150 test documents using the 390 training docu-
ments. Besides the correlation metric, two addi-
tional metrics were used. One of them computed
for a document the difference between the aver-
age absolute difference of the naive judge scores
from the mean expert score and the absolute dif-
ference of the machine?s score from the mean ex-
pert score. This was then averaged over all the
documents. The other one was ?target hits? which
measured if the predicted score for a document
fell within the width of the lowest and the highest
expert scores for that document, and if so, com-
System Correl. Avg. Diff. Target Hits
Our (A) 0.8127 0.4844 0.4619
System B 0.6904 0.3916 0.4530
System C 0.8501 0.5177 0.4641
Upper CV 0.7423 0.0960 0.3713
Table 6: Results of the systems that participated in the
DARPA?s readability evaluation task. The three metrics used
were correlation, average absolute difference and target hits
measured against the expert readability scores. The upper
critical values are for the score distributions of naive judges.
puted a score inversely proportional to that width.
The final target hits score was then computed by
averaging it across all the documents. The upper
critical values for these metrics were computed in
a way analogous to that for the correlation met-
ric which was described before. Higher score is
better for all the three metrics. Table 6 shows the
results of the evaluation. Our system performed
favorably and always scored better than the up-
per critical value on each of the metrics. Its per-
formance was in between the performance of the
other two systems. The performances of the sys-
tems show that the correlation metric was the most
difficult of the three metrics.
7 Conclusions
Using regression over a diverse combination of
syntactic, lexical and language-model based fea-
tures, we built a system for predicting the read-
ability of natural-language documents. The sys-
tem accurately predicts readability as judged by
linguistically-trained expert human judges and
exceeds the accuracy of naive human judges.
Language-model based features were found to be
most useful for this task, but syntactic and lexical
features were also helpful. We also found that for
a corpus consisting of documents from a diverse
mix of genres, using features that are indicative
of the genre significantly improve the accuracy of
readability predictions. Such a system could be
used to filter out less readable documents for ma-
chine or human processing.
Acknowledgment
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
553
References
Bernth, Arendse. 1997. Easyenglish: A tool for improv-
ing document quality. In Proceedings of the fifth con-
ference on Applied Natural Language Processing, pages
159?165, Washington DC, April.
Breiman, Leo. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
Chall, J.S. and E. Dale. 1995. Readability Revisited: The
New Dale-Chall Readability Formula. Brookline Books,
Cambridge, MA.
Collins-Thompson, Kevyn and James P. Callan. 2004. A
language modeling approach to predicting reading diffi-
culty. In Proc. of HLT-NAACL 2004, pages 193?200.
Fry, E. 1990. A readability formula for short passages. Jour-
nal of Reading, 33(8):594?597.
Gunning, R. 1952. The Technique of Clear Writing.
McGraw-Hill, Cambridge, MA.
Heilman, Michael, Kevyn Collins-Thompson, Jamie Callan,
and Maxine Eskenazi. 2007. Combining lexical and
grammatical features to improve readability measures for
first and second language texts. In Proc. of NAACL-HLT
2007, pages 460?467, Rochester, New York, April.
Heilman, Michael, Kevyn Collins-Thompson, and Maxine
Eskenazi. 2008. An analysis of statistical models and fea-
tures for reading difficulty prediction. In Proceedings of
the Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 71?79, Columbus,
Ohio, June. Association for Computational Linguistics.
Kanungo, Tapas and David Orr. 2009. Predicting the read-
ability of short web summaries. In Proc. of WSDM 2009,
pages 202?211, Barcelona, Spain, February.
Kincaid, J. P., R. P. Fishburne, R. L. Rogers, and B.S.
Chissom. 1975. Derivation of new readability formulas
for navy enlisted personnel. Technical Report Research
Branch Report 8-75, Millington, TN: Naval Air Station.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc. of
ICASSP-95, pages 181?184.
McCallum, Andrew and Kamal Nigam. 1998. A comparison
of event models for naive Bayes text classification. In Pa-
pers from the AAAI-98 Workshop on Text Categorization,
pages 41?48, Madison, WI, July.
McCord, Michael C. 1989. Slot grammar: A system for
simpler construction of practical natural language gram-
mars. In Proceedings of the International Symposium on
Natural Language and Logic, pages 118?145, May.
McLaughlin, G. H. 1969. Smog: Grading: A new readabil-
ity formula. Journal of Reading, 12:639?646.
Pitler, Emily and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proc. of EMNLP 2008, pages 186?195,
Waikiki,Honolulu,Hawaii, October.
Quinlan, J. R. 1987. Simplifying decision trees. Interna-
tional Journal of Man-Machine Studies, 27:221?234.
Rasmussen, Carl and Christopher Williams. 2006. Gaussian
Processes for Machine Leanring. MIT Press, Cambridge,
MA.
Riloff, E. and W. Phillips. 2004. An introduction to the Sun-
dance and Autoslog systems. Technical Report UUCS-
04-015, University of Utah School of Computing.
Riloff, Ellen. 1996. Automatically generating extraction
patterns from untagged text. In Proc. of 13th Natl. Conf.
on Artificial Intelligence (AAAI-96), pages 1044?1049,
Portland, OR.
Schwarm, Sarah E. andMari Ostendorf. 2005. Reading level
assessment using support vector machines and statistical
language models. In Proc. of ACL 2005, pages 523?530,
Ann Arbor, Michigan.
Si, Luo and James P. Callan. 2001. A statistical model for
scientific readability. In Proc. of CIKM 2001, pages 574?
576.
Smola, Alex J. and Bernhard Scholkopf. 1998. A tutorial
on support vector regression. Technical Report NC2-TR-
1998-030, NeuroCOLT2.
Stanley, Chen and Joshua Goodman. 1996. An empirical
study of smoothing techniques for language modeling. In
Proc. of the 34th Annual Meeting of the Association for
Computational Linguistics (ACL-96), pages 310?318.
Stenner, A. J., I. Horabin, D. R. Smith, and M. Smith. 1988.
The Lexile Framework. Durham, NC: MetaMetrics.
Thelen, M. and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proc. of EMNLP 2002, Philadelphia, PA, July.
Yang, Yiming and Xin Liu. 1999. A re-examination of text
cateogrization methods. In Proc. of 22nd Intl. ACM SI-
GIR Conf. on Research and Development in Information
Retrieval, pages 42?48, Berkeley, CA.
554
Coling 2010: Poster Volume, pages 543?551,
Beijing, August 2010
Generative Alignment and Semantic Parsing
for Learning from Ambiguous Supervision
Joohyun Kim
Department of Computer Science
The University of Texas at Austin
scimitar@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
mooney@cs.utexas.edu
Abstract
We present a probabilistic generative
model for learning semantic parsers from
ambiguous supervision. Our approach
learns from natural language sentences
paired with world states consisting of
multiple potential logical meaning repre-
sentations. It disambiguates the mean-
ing of each sentence while simultane-
ously learning a semantic parser that maps
sentences into logical form. Compared
to a previous generative model for se-
mantic alignment, it also supports full
semantic parsing. Experimental results
on the Robocup sportscasting corpora in
both English and Korean indicate that
our approach produces more accurate se-
mantic alignments than existing methods
and also produces competitive semantic
parsers and improved language genera-
tors.
1 Introduction
Most approaches to learning semantic parsers that
map sentences into complete logical forms (Zelle
and Mooney, 1996; Zettlemoyer and Collins,
2005; Kate and Mooney, 2006; Wong and
Mooney, 2007b; Lu et al, 2008) require fully-
supervised corpora that provide full formal logi-
cal representations for each sentence. Such cor-
pora are expensive and difficult to construct. Sev-
eral recent projects on ?grounded? language learn-
ing (Kate and Mooney, 2007; Chen and Mooney,
2008; Chen et al, 2010; Liang et al, 2009) exploit
more easily and naturally available training data
consisting of sentences paired with world states
consisting of multiple potential semantic repre-
sentations. This setting is partially motivated by a
desire to model how children naturally learn lan-
guage in the context of a rich, ambiguous percep-
tual environment.
In particular, Chen and Mooney (2008) in-
troduced the problem of learning to sportscast
by simply observing natural language commen-
tary on simulated Robocup robot soccer games.
The training data consists of natural language
(NL) sentences ambiguously paired with logical
meaning representations (MRs) describing recent
events in the game extracted from the simulator.
Most sentences describe one of the extracted re-
cent events; however, the specific event to which
it refers is unknown. Therefore, the learner has
to figure out the correct matching (alignment) be-
tween NL and MR before inducing a semantic
parser or language generator. Based on an ap-
proach introduced by Kate and Mooney (2007),
Chen and Mooney (2008) repeatedly retrain both
a supervised semantic parser and language gener-
ator using an iterative algorithm analogous to Ex-
pectation Maximization (EM). However, this ap-
proach is somewhat ad hoc and does not exploit
a well-defined probabilistic generative model or
real EM training.
On the other hand, Liang et al (2009) in-
troduced a probabilistic generative model for
learning semantic correspondences in ambigu-
ous training data consisting of sentences paired
with observed world states. Compared to Chen
and Mooney (2008), they demonstrated improved
alignment results on Robocup sportscasting data.
However, their model only produces an NL?MR
alignment and does not learn either an effective
543
semantic parser or language generator. In addi-
tion, they use a combination of a simple Markov
model and a bag-of-words model when generating
natural language for MRs, therefore, they do not
model context-free linguistic syntax.
Motivated by the limitations of these previ-
ous methods, we propose a new generative align-
ment model that includes a full semantic pars-
ing model proposed by Lu et al (2008). Our
approach is capable of disambiguating the map-
ping between language and meanings while also
learning a complete semantic parser for mapping
sentences to logical form. Experimental results
on Robocup sportscasting show that our approach
outperforms all previous results on the NL?MR
matching (alignment) task and also produces com-
petitive performance on semantic parsing and im-
proved language generation.
2 Related Work
The conventional approach to learning seman-
tic parsers (Zelle and Mooney, 1996; Ge and
Mooney, 2005; Kate and Mooney, 2006; Zettle-
moyer and Collins, 2007; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2007b; Lu et
al., 2008) requires detailed supervision unambigu-
ously pairing each sentence with its logical form.
However, developing training corpora for these
methods requires expensive expert human labor.
Chen and Mooney (2008) presented methods
for grounded language learning from ambigu-
ous supervision that address three related tasks:
NL?MR alignment, semantic parsing, and natu-
ral language generation. They solved the prob-
lem of aligning sentences and meanings by iter-
atively retraining an existing supervised seman-
tic parser, WASP (Wong and Mooney, 2007b) or
KRISP (Kate and Mooney, 2006), or an existing
supervised natural-language generator, WASP?1
(Wong and Mooney, 2007a). During each iter-
ation, the currently trained parser (generator) is
used to produce an improved NL?MR alignment
that is used to retrain the parser (generator) in the
next iteration. However, this approach does not
use the power of a probabilistic correspondence
between an NL and MRs during training.
On the other hand, Liang et al (2009) pro-
posed a probabilistic generative approach to pro-
duce a Viterbi alignment between NL and MRs.
They use a hierarchical semi-Markov generative
model that first determines which facts to dis-
cuss and then generates words from the predi-
cates and arguments of the chosen facts. They re-
port improved matching accuracy in the Robocup
sportscasting domain. However, they only ad-
dressed the alignment problem and are unable to
parse new sentences into meaning representations
or generate natural language from logical forms.
In addition, the model uses a weak bag-of-words
assumption when estimating links between NL
segments and MR facts. Although it does use a
simple Markov model to order the generation of
the different fields of an MR record, it does not
utilize the full syntax of the NL or MR or their
relationship.
Chen et al (2010) recently reported results
on utilizing the improved alignment produced by
Liang et al (2009)?s model to initialize their own
iterative retraining method. By combining the ap-
proaches, they produced more accurate NL?MR
alignments and improved semantic parsers.
Motivated by this prior research, our approach
combines the generative alignment model of
Liang et al (2009) with the generative semantic
parsing model of Lu et al (2008) in order to fully
exploit the NL syntax and its relationship to the
MR semantics. Therefore, unlike Liang et al?s
simple Markov + bag-of-words model for gener-
ating language, it uses a tree-based model to gen-
erate grammatical NL from structured MR facts.
3 Background
This section describes existing models and algo-
rithms employed in the current research. Our
model is built on top of the generative semantic
parsing model developed by Lu et al (2008). Af-
ter learning a probabilistic alignment and parsing
model, we also used the WASP and WASP?1 sys-
tems to produce additional parsing and generation
results. In particular, since our current system is
incapable of effectively generating NL sentences
from MR logical forms, in order to demonstrate
how our matching results can aid NL generation,
we use WASP?1 to learn a generator. This follows
the experimental scheme of Chen et al (2010),
which demonstrated that an improved NL?MR
544
S
S : pass (PLAYER, PLAYER)
PLAYER
PLAYER : pink10
pink10
passes the ball to PLAYER
PLAYER : pink11
pink11
Figure 1: Sample hybrid tree from English
sportscasting dataset where (w,m) = (pink10
passes the ball to pink11, pass(pink10, pink11))
matching from Liang et al (2009) results in better
overall parsing and generation. Finally, our over-
all generative model uses the IGSL (Iterative Gen-
eration Strategy Learning) method of Chen and
Mooney (2008) to initially estimate the prior prob-
ability of each event-type generating a natural-
language comment.
3.1 Generative Semantic Parsing
Lu et al (2008) introduced a generative seman-
tic parsing model using a hybrid-tree framework.
A hybrid tree is defined over a pair, (w,m), of a
natural-language sentence and its logical meaning
representation. The tree expresses a correspon-
dence between word segments in the NL and the
grammatical structure of the MR. In a hybrid tree,
MR production rules constitute the internal nodes,
while NL words (or phrases) constitute the leaves.
A sample hybrid tree from the English Robocup
data is given in Figure 1.
A generative model based on hybrid trees is de-
fined as follows: starting from a root semantic
category, the model generates a production of the
MR grammar, and then subsequently generates a
mixed hybrid pattern of NL words and child se-
mantic categories. This process is repeated un-
til all leaves in the hybrid tree are NL words (or
phrases). Each generation step is only dependent
on the parent step, thus, generation is assumed to
be a Markov process.
Lu et al (2008)?s generative parsing model es-
timates the joint probability P (T ,w,m), which
represents the probability of generating a hybrid
tree T with NL w, and MR m. This probability
is computed as the product of the probabilities of
the steps in the generative process. Since there are
multiple ways to construct a hybrid tree given a
pair of NL and MR, the data likelihood of the pair
(w,m) given by the learned model is calculated
by summing P (T ,w,m) over all the possible hy-
brid trees for NL w and MR m.
The model is normally trained in a fully su-
pervised setting using NL?MR pairs. In order to
learn from ambiguous supervision, we extend this
model to include an additional generative process
for selecting the subset of available MRs used to
generate NL sentences.
3.2 WASP and WASP?1
WASP (Word-Alignment-based Semantic Parsing)
is a semantic parsing system that uses syntax-
based statistical machine translation techniques. It
induces a probabilistic synchronous context-free
grammar (PSCFG) for generating corresponding
NL?MR pairs. Since a PSCFG is symmetric
with respect to the two languages it generates,
the same learned model can be used for both se-
mantic parsing (mapping NL to MR) and natural
language generation (mapping MR to NL), Since
there is no prespecified formal grammar for the
NL, the WASP?1 system learns an n-gram lan-
guage model for the NL side and uses it to choose
the most probable NL translation for a given MR
using a noisy-channel model.
3.3 IGSL
Chen and Mooney (2008) introduced the IGSL
method for determining which event types a hu-
man commentator is more likely to describe in
natural language. This is sometimes called strate-
gic generation or content selection, the process of
choosing what to say; as opposed to tactical gen-
eration, which determines how to say it. IGSL
uses a method analogous to EM to train on am-
biguously supervised data and iteratively improve
probability estimates for each event type, speci-
fying how likely each MR predicate is to elicit
a comment. The algorithm alternates between
two processes: calculating the expected proba-
bility of an NL?MR matching based on the cur-
rently learned estimates, and updating the prob-
ability of each event type based on the expected
match counts. IGSL was shown to be quite effec-
tive at predicting which events in a Robocup game
545
English Korean
# of NL comments 2036 1999
# of extracted MR events 10452 10668
# of NLs w/ matching MRs 1868 1913
# of MRs w/ matching NLs 4670 4610
Avg. # of MRs per NL 2.50 2.41
Table 1: Stats for Robocup sportscasting data
a human would comment upon. In our proposed
model, we use IGSL probability scores as initial
priors for our event selection model.
4 Evaluation Dataset
In our experiments, we use the Robocup
sportscasting data produced by Chen et al (2010),
which includes both English and Korean com-
mentaries. The data was collected by having both
English and Korean speakers commentate the fi-
nal games from the RoboCup simulation soccer
league for each year from 2001 through 2004. Ta-
ble 1 presents some statistics on this sportscasting
data. To construct the ambiguous training data,
each NL commentary sentence is paired with MRs
for all extracted simulation events that occurred in
the previous 5 seconds (an average of 2.5 events).
Figure 2 shows a sample trace from the
Robocup English data. Each NL commentary sen-
tence normally has several possible MR matches
that occurred within the 5-second window, in-
dicated by edges between the NL and MR.
Bold edges represent gold standard matches con-
structed solely for evaluation purposes. Note that
not every NL has a gold matching MR. This oc-
curs because the sentence refers to unrecognized
or undetected events or situations or because the
matching MR lies outside the 5-second window.
5 Generative Model
Like Liang et al (2009)?s generative alignment
model, our model is designed to estimate P (w|s),
where w is an NL sentence and s is a world state
containing a set of possible MR logical forms that
can be matched to w. However, our approach
is intended to support both determining the most
likely match between an NL and its MR in its
world state, and semantic parsing, i.e. finding the
Natu
ral L
angu
age
Mea
ning
 Rep
rese
ntati
on
Purp
le9 p
repa
res t
o at
tack
pass
 ( Pu
rple
Play
er9 
, Pu
rple
Play
er6 
)
defe
nse 
( Pin
kPla
yer6
 , Pi
nkP
laye
r6 )
Purp
le9 p
asse
s to 
Purp
le6
Purp
le6's
 pas
s wa
s de
fend
ed b
y Pi
nk6
turn
over
 ( pu
rple
6 , p
ink6
 )
ball
stop
ped
upl
e6s
pass
was
defe
nded
by
ink6
Pink
6 ma
kes a
 sho
rt pa
ss to
 Pin
k3
kick
 ( Pi
nkP
laye
r6 )
pass
 ( Pi
nkP
laye
r6 , 
Pink
Play
er3 
)
Pink
 goa
lie n
ow h
as th
e ba
ll
play
mod
e( f
ree_
kick
_r)
pass
 ( Pi
nkP
laye
r3 , 
Pink
Play
er1 
)
Figure 2: Sample trace from Robocup English
data.
most probable mapping from a given NL sentence
to an MR logical form.
Our generative model consists of two stages:
? Event selection: P (e|s), chooses the event e
in the world state s to be described.
? Natural language generation: P (w|e), mod-
els the probability of generating natural-
language sentence w from the MR specified
by event e.
5.1 Event selection model
The event selection model specifies the probabil-
ity distribution for picking an event that is likely
to be commented upon amongst the multiple MR
logical forms in the world state s. The probabil-
ity of picking an event is assumed to depend only
on its event type as given by the predicate of its
MR. For example, the MR pass(pink10, pink11)
has event type pass and arguments pink10 and
pink11.
Our model is similar to Liang et al (2009)?s
record choice model, but we only model their no-
tion of salience, denoting that some event types
are more likely to be described than others. We do
not model their notion of coherence, which mod-
els the order of event types in the commentary. We
found that for sportscasting the order of described
events depends only on the sequence of events in
the game and does not exhibit any additional de-
tectable pattern due to linguistic preferences.
The probability of picking an event e of type te
is denoted by p(te). If there are multiple events
of type t in a world state s, then an event of type
t is selected uniformly from the set s(t) of events
546
of type t in state s. Therefore, the probability of
picking an event is given by:
P (e|s) = p(te)
1
|s(te)|
(1)
5.2 Natural language generation model
The natural-language generation model defines
the probability distribution of NL sentences given
an MR specified by the previously selected event.
We use Lu et al (2008)?s generative model for this
step, in which:
P (w|e) =
?
?T over (w,m)
P (T ,w|m) (2)
where m is the MR logical form defined by event
e and T is a hybrid tree defined over the NL?MR
pair (w,m).
The probability P (T ,w|m) is calculated using
the generative semantic parsing model of Lu et al
(2008) using the joint probability of the NL?MR
pair (w,m), i.e. the inside probability of gener-
ating (w,m). The likelihood of a sentence w is
then the sum over all possible hybrid trees defined
by the NL?MR pair (w,m). 1
The natural language generation model covers
the roles of both the field choice model and word
choice models of Liang et al (2009). Since our
event selection model only chooses an event based
on its type, the order of its arguments still needs
to be addressed. However, Lu et al?s generative
model includes ordering the MR arguments (as
specified by MR production rules) as well as the
generation of NL words and phrases to express
these arguments. Thus, it is unnecessary to sepa-
rately model argument ordering in our approach.2
1Lu et al (2008) propose 3 models for generative seman-
tic parsing: unigram, bigram, and mixgram (interpolation be-
tween the two). We used the bigram model, where the gen-
eration of a hybrid-tree component (NL word or semantic
category) depends on the previously generated component as
well as the parent MR production. The bigram model always
performed the best on all tasks in our experimental evalua-
tion.
2We also tried using a Markov model to order arguments
like Liang et al (2009), but preliminary experimental results
showed that this additional component actually decreased
performance rather than improving it.
6 Learning and Inference
This composite generative model is trained using
conventional EM methods. The process is similar
to Lu et al (2008)?s, an inside-outside style al-
gorithm using dynamic programming to generate
a hybrid tree from the NL?MR pair (w,m), ex-
cept our model?s estimation process additionally
deals with calculating expected counts under the
posterior P (e|w, s; ?) in the E-step and normaliz-
ing the counts to optimize parameters. The whole
process is quite efficient; training time takes about
30 minutes to run on sportscasts of three games in
either English or Korean.
Unfortunately, we found that EM tended to get
stuck at local maxima with respect to learning the
event-type selection probabilities, p(t). There-
fore, we also tried initializing these parameters
with the corresponding strategic generation values
learned by the IGSL method of Chen and Mooney
(2008). Since IGSL was shown to be quite effec-
tive at predicting which event types were likely to
be described, the use of IGSL priors provides a
good starting point for our event selection model.
Our model is built on top of Lu et al (2008)?s
generative semantic parsing model, which is also
trained in several steps in its best-performing ver-
sion.3 Thus, the overall model is vulnerable to
getting stuck in local optima when running EM
across these multiple steps. We also tried using
random restarts with different initialization of pa-
rameters, but initializing with IGSL priors per-
formed the best in our experimental evaluation.
7 Experimental Evaluation
We evaluated our proposed model on the Robocup
sportscasting data described in Section 4. Our ex-
perimental results cover 3 tasks: NL?MR match-
ing, semantic parsing, and tactical generation.
Following Chen and Mooney (2008), the exper-
iments were conducted using 4-fold (leave one
game out) cross validation. Since the corpus con-
tains data for four separate games, each fold uses
3 games for training and the remaining game for
3The bigram model of Lu et al (2008), which is the one
used in this paper, must be trained using parameters previ-
ously learned for the IBM Model 1 and unigram model in
order to exhibit the best performance. We followed the same
training scheme in our version.
547
testing for semantic parsing and tactical genera-
tion. Matching performance is measured in train-
ing data, since the goal is to disambiguate this
data. All results are averaged across these 4 folds.
We also use the same performance metrics
as Chen and Mooney (2008). The accuracy of
matching and semantic parsing are measured us-
ing F-measure, the harmonic mean of precision
and recall, where precision is the fraction of the
system?s annotations that are correct, and recall
is the fraction of the annotations from the gold-
standard that the system correctly produces. Gen-
eration is evaluated using BLEU score (Papineni
et al, 2002) between generated sentences and ref-
erence NL sentences in the test set. We com-
pare our results to previous results from Chen and
Mooney (2008) and Chen et al (2010) and to
matching results on Robocup data from Liang et
al. (2009).
7.1 NL?MR Matching
The goal of matching is to find the most probable
NL?MR alignment for ambiguous examples con-
sisting of an NL sentence and multiple potential
MR logical forms. In Robocup sportscasting, the
MRs for a given sentence correspond to all game
events that occur within a 5-second window prior
to the NL comment. Not all NL sentences have a
matching MR in this window, but most do. Dur-
ing testing, an NL w is matched to an MR m if
and only if the learned semantic parser produces
m as the most probable parse of w. Thus, our
model does not force every NL to match an MR.
If the most probable semantic parse of a sentence
does not match any of the possible recent events,
it is simply left unmatched. Matching is evaluated
against the gold-standard matches supplied with
the data, which are used for evaluation purposes
only. The gold matching data is never used during
training.
Table 2 shows the detailed results for both
English and Korean data.4 Our best approach
outperforms all previous methods for both En-
glish and Korean by quite large margins. Note
4Since the Korean data was not yet available for use by
either Chen and Mooney (2008) or Liang et al (2009), we
present the results reported by Chen et al (2010) for these
methods.
English Korean
Chen and Mooney (2008) 0.681 0.753
Liang et al (2009) 0.757 0.694
Chen et al (2010) 0.793 0.841
Our model 0.832 0.800
Our model w/ IGSL init 0.885 0.895
Table 2: NL?MR Matching Results (F-measure).
Results are the highest reported in the cited work.
English Korean
Chen and Mooney (2008) 0.702 0.720
Chen et al (2010) 0.803 0.812
Our learned parser 0.742 0.764
Lu et al + our matching 0.810 0.794
WASP + our matching 0.786 0.808
Lu et al + Liang et al 0.790 0.690
WASP + Liang et al 0.803 0.740
Table 3: Semantic Parsing Results (F-measure).
Results are the highest reported in the cited work.
that initializing our EM training with IGSL?s es-
timates improves performance significantly, and
this approach outperforms Chen et al (2010)?s
best method, which also uses IGSL.
In particular, our proposed model outperforms
the generative alignment model of Liang et al
(2009), indicating that the extra linguistic infor-
mation and MR grammatical structure used by Lu
et al (2008)?s generative language model make
our overall model more effective than a simple
Markov + bag-of-words model for language gen-
eration.
7.2 Semantic Parsing
Semantic parsing is evaluated by determining how
accurately NL sentences in the test set are cor-
rectly mapped to their meaning representations.
Results are presented in Table 3.5 6 For our
model, we report results using the parser learned
directly from the ambiguous supervision, as well
5The best result of Chen and Mooney (2008) is for
WASPER-GEN, and that of Chen et al (2010) is for WASPER
with Liang et al?s matching initialization for English and for
WASER-GEN-IGSL-METEOR with Liang et al?s initialization
for Korean.
6Our semantic parsing results are based on our best
matching results with IGSL initialization.
548
as results for training a supervised parser (both
WASP and Lu et al (2009)?s) on the NL?MR
matching produced by our model. We also present
results for training Lu et al?s parser and WASP on
Liang et al?s NL?MR matchings.
Our initial learned semantic parser does not per-
form better than the best results reported by Chen
et al (2010), but it is clearly better than the ini-
tial results of Chen and Mooney (2008). Train-
ing WASP and Lu et al?s supervised parser on
our method?s highly accurate set of disambiguated
NL?MR pairs improved the results. Retraining Lu
et al?s parser gave the best overall results for En-
glish, and retraining WASP gave the second high-
est results for Korean, only failing to beat the very
best results of Chen et al (2010). It is somewhat
surprising that simply retraining on the hardened
set of most probable NL?MR matches gives bet-
ter results than the parser trained using EM, which
actually exploits the uncertainty in the underly-
ing matches. Further investigations of this phe-
nomenon are indicated.
Comparing with the corresponding results for
training WASP and Lu et al?s supervised parser
on the NL?MR matchings produced by Liang et
al.?s alignment method, it is clear that our match-
ings produce more accurate semantic parsers ex-
cept when training WASP on English.
7.3 Tactical Generation
Tactical generation is evaluated based on how
well the learned model generates accurate NL sen-
tences from MR logical forms. Without integrat-
ing a language model for the NL, the existing
generative model is not very effective for tactical
generation. Lu et al (2009) introduced an effec-
tive language generator for the hybrid tree frame-
work using a Tree-CRF model; however, we did
not have access to this system. Therefore, for
tactical generation, we used the publicly avail-
able WASP?1 system (Wong and Mooney, 2007a)
trained on disambiguated NL?MR matches. This
approach also allows direct comparison with the
results of Chen and Mooney (2008) and Chen et
al. (2010), who also used WASP?1 for tactical
generation. Our objective is to show that the more
accurate matchings produced by our generative
model can improve tactical generation.
English Korean
Chen and Mooney (2008) 0.4560 0.5575
Chen et al (2010) 0.4599 0.6796
WASP?1 + Liang et al 0.4580 0.5828
WASP?1 + our matching 0.4727 0.7148
Table 4: Tactical Generation Results (BLEU
score). Results are the highest reported in the cited
work.
The results are shown in Table 4.7 8 Overall,
WASP?1 trained on the NL?MR matching from
our alignment model performs better than all pre-
vious methods. In particular, using the matchings
from our method to train WASP?1 produces bet-
ter tactical generators than using matchings from
Liang et al?s approach.
7.4 Discussion
Overall, our model performs particularly well at
matching NL and MRs under ambiguous supervi-
sion, and the difference is larger for English than
Korean. However, improved matching results do
not necessarily translate into significantly better
semantic parsers. For English, the improvement
in matching is almost 10 percentage points in F-
measure, but the semantic parsing result trained
with this more accurate matching shows only 1
point improvement.
Compared to Liang et al (2009), our more ac-
curate (i.e. higher F-measure) matchings provide
a clear improvement in both semantic parsing and
tactical generation. The only exception is English
parsing using WASP, which seems to be due to
some misleading noise in our alignments. WASP
seems to be affected more than Lu et al?s system
by such extraneous noise. However, in tactical
generation, this extraneous noise does not seem to
lead to worse performance, and our approach al-
ways gives the best results. As discussed by Chen
and Mooney (2008) and Chen et al (2010), tac-
tical generation is somewhat easier than seman-
tic parsing in that semantic parsing needs to learn
7The best result of Chen and Mooney (2008) is for
WASPER-GEN, and that of Chen et al (2010) is for WASPER
with Liang et al?s matching initialization for English and for
WASER-GEN with Liang et al initialization for Korean.
8Our generation results are based on our best matching
results with IGSL initialization.
549
to map a variety of synonymous natural-language
expressions to the same meaning representation,
while tactical generation only needs to learn one
way to produce a correct natural language descrip-
tion of an event. This difference in the nature of
semantic parsing and tactical generation may be
the cause of the different trends in the results.
8 Conclusions and Future Work
We have presented a novel generative model capa-
ble of probabilistically aligning natural-language
sentences to their correct meaning representa-
tions given the ambiguous supervision provided
by a grounded language acquisition scenario. Our
model is also capable of simultaneously learning
to semantically parse NL sentences into their cor-
responding meaning representations. Experimen-
tal results in Robocup sportscasting show that the
NL?MR matchings inferred by our model are sig-
nificantly more accurate than those produced by
all previous methods. Our approach also learns
competitive semantic parsers and improved lan-
guage generators compared to previous methods.
In particular, we showed that our alignments pro-
vide a better foundation for learning accurate se-
mantic parsers and tactical generators compared
to those of Liang et al (2009), whose genera-
tive model is limited by a simple bag-of-words as-
sumption.
In the future, we plan to test our model on
more complicated data with higher degrees of am-
biguity as well as more complex meaning repre-
sentations. One immediate direction is evaluat-
ing our approach on the datasets of weather fore-
casts and NFL football articles used by Liang et al
(2009). However, our current model does not sup-
port matching multiple meaning representations
to the same natural-language sentence, and needs
to be extended to allow multiple MRs to generate
a single NL sentence.
Acknowledgements
We thank Wei Lu and Wee Sun Lee for sharing
their software and giving helpful comments for
the paper. We also thank Percy Liang for sharing
his code and experimental results with us. Ad-
ditionally, we thank David Chen in UTCS ML
group for his comments and advice. Finally, we
thank the anonymous reviewers for their com-
ments. This work was funded by the NSF grant
IIS. 0712907X. The experiments were executed
and run on the Mastodon Cluster, provided by
NSF Grant EIA-0303609.
References
Chen, David L. and Raymond J. Mooney. 2008.
Learning to sportscast: a test of grounded language
acquisition. In ICML ?08: Proceedings of the
25th International Conference on Machine Learn-
ing, pages 128?135, New York, NY, USA. ACM.
Chen, David L., Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual
sportscaster: Using perceptual context to learn lan-
guage. Journal of Artificial Intelligence Research,
37:397?435.
Ge, Ruifang and Raymond J. Mooney. 2005. A sta-
tistical semantic parser that integrates syntax and
semantics. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 9?16, Ann Arbor, MI, July.
Kate, Rohit J. and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics (COLING/ACL-06), pages 913?920, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kate, Rohit J. and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the Twenty-Second Con-
ference on Artificial Intelligence (AAAI-07), pages
895?900, Vancouver, Canada, July.
Liang, Percy, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In ACL-IJCNLP ?09: Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1, pages 91?99, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 783?792, Morristown, NJ, USA. Association
for Computational Linguistics.
550
Lu, Wei, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In EMNLP ?09: Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 400?409, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, PA, July.
Wong, Yuk Wah and Raymond J. Mooney. 2007a.
Generation by inverting a semantic parser that uses
statistical machine translation. In Proceedings of
Human Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
172?179, Rochester, NY.
Wong, Yuk Wah and Raymond J. Mooney. 2007b.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL-07), pages 960?967,
Prague, Czech Republic, June.
Zelle, John M. and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-
96), pages 1050?1055, Portland, OR, August.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of 21st Conference on
Uncertainty in Artificial Intelligence (UAI-2005),
Edinburgh, Scotland, July.
Zettlemoyer, Luke S. and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL-07), pages 678?
687, Prague, Czech Republic, June.
551
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1218?1227, Dublin, Ireland, August 23-29 2014.
Integrating Language and Vision
to Generate Natural Language Descriptions of Videos in the Wild
Jesse Thomason
?
University of Texas at Austin
jesse@cs.utexas.edu
Subhashini Venugopalan
?
University of Texas at Austin
vsub@cs.utexas.edu
Sergio Guadarrama
University of California Berkeley
sguada@eecs.berkeley.edu
Kate Saenko
University of Massachusetts Lowell
saenko@cs.uml.edu
Raymond Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
This paper integrates techniques in natural language processing and computer vision to improve
recognition and description of entities and activities in real-world videos. We propose a strategy
for generating textual descriptions of videos by using a factor graph to combine visual detections
with language statistics. We use state-of-the-art visual recognition systems to obtain confidences
on entities, activities, and scenes present in the video. Our factor graph model combines these
detection confidences with probabilistic knowledge mined from text corpora to estimate the most
likely subject, verb, object, and place. Results on YouTube videos show that our approach im-
proves both the joint detection of these latent, diverse sentence components and the detection of
some individual components when compared to using the vision system alone, as well as over
a previous n-gram language-modeling approach. The joint detection allows us to automatically
generate more accurate, richer sentential descriptions of videos with a wide array of possible
content.
1 Introduction
Integrating language and vision is a topic that is attracting increasing attention in computational lin-
guistics (Berg and Hockenmaier, 2013). Although there is a fair bit of research on generating natural-
language descriptions of images (Feng and Lapata, 2013; Yang et al., 2011; Li et al., 2011; Ordonez et
al., 2011), there is significantly less work on describing videos (Barbu et al., 2012; Guadarrama et al.,
2013; Das et al., 2013; Rohrbach et al., 2013; Senina et al., 2014). In particular, much of the research
on videos utilizes artificially constructed videos with prescribed sets of objects and actions (Barbu et al.,
2012; Yu and Siskind, 2013). Generating natural-language descriptions of videos in the wild, such as
those posted on YouTube, is a very challenging task.
In this paper, we focus on selecting content for generating sentences to describe videos. Due to the
large numbers of video actions and objects and scarcity of training data, we introduce a graphical model
for integrating statistical linguistic knowledge mined from large text corpora with noisy computer vi-
sion detections. This integration allows us to infer which vision detections to trust given prior linguistic
knowledge. Using a large, realistic collection of YouTube videos, we demonstrate that this model effec-
tively exploits linguistic knowledge to improve visual interpretation, producing more accurate descrip-
tions compared to relying solely on visual information. For example, consider the frames of the video
in Figure 1. Instead of generating the inaccurate description ?A person is playing on the keyboard in the
kitchen? using purely visual information, our system generates the more correct ?A person is playing the
piano in the house? by using statistics mined from parsed corpora to improve the interpretation of the
uncertain visual detections, such as the presence of both a computer keyboard and a piano in the video.
2 Background and Related Work
Several recent projects have integrated linguistic and visual information to aid description of images and
videos. The most related work on image description is Baby Talk (Kulkarni et al., 2011), which uses
?
Indicates equal contribution
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1218
Figure 1: Frames which depict a person playing a piano in front of a keyboard from one of the videos
in our dataset. Purely visual information is more confident in the computer keyboard?s presence than the
piano?s, while our model can correctly determine that the person is more likely to be playing the piano
than the computer keyboard.
a Conditional Random Field (CRF) to integrate visual detections with statistical linguistic knowledge
mined from parsed image descriptions and Google queries, and the work of Yang et al. (2011) which
uses corpus statistics to aid the description of objects and scenes. We go beyond the scope of these
previous works by also selecting verbs through the integration of activity recognition from video and
statistics from parsed corpora.
With regard to video description, the work of Barbu et al. (2012) uses a small, hand-coded grammar to
describe a sparse set of prescribed activities. In contrast, we utilize corpus statistics to aid the description
of a wide range of naturally-occurring videos. The most similar work is (Krishnamoorthy et al., 2013;
Guadarrama et al., 2013) which uses an n-gram language model to help determine the best subject-verb-
object for describing a video. Krishnamoorthy et al. (2013) used a limited set of videos containing
a small set of 20 entities, and the work of Guadarrama et al. (2013) showed an advantage of using
linguistic knowledge only for the case of ?zero shot activity recognition,? in which the appropriate verb
for describing the activity was never seen during training. Compared to this prior work, we explore a
much larger set of entities and activities (see Section 3.2) and add scene recognition (see Section 3.3) to
further enrich the descriptions. Our experiments demonstrate that our graphical model produces a more
accurate subject-verb-object-place description than these simpler n-gram language modeling approaches.
Our Contributions:
? We present a new method, a Factor Graph Model (FGM), to perform content selection by integrating
visual and linguistic information to select the best subject-verb-object-place description of a video.
? Our model includes scene (location) information which has not been addressed by previous video
description works (Barbu et al., 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013).
? We demonstrate the scalability of our model by evaluating it on a large dataset of naturally occurring
videos (1297 training, 670 testing), recognizing sentential subjects out of 45 candidate entities,
objects out of 218 candidate objects, verbs out of 218 candidate activities, and places out of 12
candidate scenes.
3 Approach
Our overall approach uses a probabilistic graphical model to integrate the visual detection of entities,
activities, and scenes with language statistics to determine the best subject, verb, object, and place to
describe a given video. A descriptive English sentence is generated from the selected sentential compo-
nents.
3.1 Video Dataset
We use the video dataset collected by Chen and Dolan (2011). The dataset contains 1,967 short YouTube
video clips paired with multiple human-generated natural-language descriptions. The video clips are 10
to 25 seconds in duration and typically consist of a single activity. Portions of this dataset have been
used in previous work on video description (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013;
Guadarrama et al., 2013). We use 1,297 randomly selected videos for training and evaluate predictions
on the remaining 670 test videos.
1219
3.2 Visual Recognition of Subject, Verb, and Object
We utilize the visual recognition techniques employed by Guadarrama et al. (2013) to process the videos
and produce probabilistic detections of grammatical subjects, verbs, and objects. In our data-set there are
45 candidate entities for the grammatical subject (such as animal, baby, cat, chef, and person) and 241
for the grammatical object (such as flute, motorbike, shrimp, person, and tv). There are 218 candidate
activities for the grammatical verb, including climb, cut, play, ride, and walk.
Entity Related Features From each video two frames per second are extracted and passed to pre-
trained visual object classifiers and detectors. As in Guadarrama et al. (2013), we compute represen-
tations based on detected objects using ObjectBank (Li et al., 2010) and the 20 PASCAL (Everingham
et al., 2010) object classes for each frame. We use the PASCAL scores and ObjectBank scores with
max pooling over the set of frames as the entity descriptors for the video clip. Additionally, to be able
to recognize more objects, we use the LLC-10k proposed by Deng et al. (2012) which was trained on
ImageNet 2011 object dataset with 10k categories. LLC-10K uses a bank of linear SVM classifiers over
pooled local vector-quantized features learned from the 7K bottom level synsets of the 10K ImageNet
database. We aggregate the 10K classifier scores obtained for each frame by doing max pooling across
frames.
Activity Related Features We use the activity recognizers described in Guadarrama et al. (2013) to
produce probabilistic verb detections. They extract Dense Trajectories developed by Wang et al. (2011)
and compute HoG (Histogram of Gradients), HoF (Histograms of Optical Flow) and MBH (Motion
Boundary Histogram) features over space time volumes around the trajectories. We used the default
parameters proposed in Wang et al. (2011) (N = 32, n
?
= 2, n
r
= 3) and adopted a standard bag-
of-features representation. We construct a codebook for each descriptor (Trajectory, HoG, HoF, MBH)
separately. For each descriptor we randomly sampled 100K points and clustered them using K-means
into a codebook of 4000 words. Descriptors are assigned to their closest vocabulary word using Eu-
clidean distance. Each video is then represented as a histogram over these clusters.
Multi-channel SVM To allow object and activity features inform one another, we combine all the
features extracted using a multi-channel approach inspired by Zhang et al. (2007) to build three non-linear
SVM (Chang and Lin, 2011) classifiers for the subject, verb, and object, as described in Guadarrama et
al. (2013). Note that we do not employ the hierarchical semantic model of Guadarrama et al. (2013) to
augment our object or activity recognition. In addition, each SVM learns a Platt scaling (Platt, 1999) to
predict the label and a visual confidence value, C(t) ? [0, 1], for each entity or activity t. The output
of the SVMs constitute the visual confidences on subject, verb, and object in all the models described
henceforth.
3.3 Visual Scene Recognition
In addition to the techniques employed by Guadarrama et al. (2013) used to obtain probabilistic de-
tections of grammatical subjects, verbs, and objects, we developed a novel scene detector based on
state-of-the-art computer vision methods.
We examined the description of all the 1,967 videos in the YouTube dataset and extracted scene words
from the dependency parses as described in Section 3.4. With the help of WordNet
1
we grouped the list
of scene words and their synonyms into distinct scene classes. Based on the frequency of mentions and
the coverage of scenes in the dataset, we shortlisted a set of 12 final scenes (mountain, pool, beach, road,
kitchen, field, snow, forest, house, stage, track, and sky).
For the detection itself, we follow Xiao et al. (2010) and select several state-of-the-art features that are
potentially useful for scene recognition. We extract GIST, HOG2x2, SSIM (self-similarity) and Dense
SIFT descriptors. We also extract LBP (Local Binary Patterns), Sparse SIFT Histograms, Line features,
Color Histograms, Texton Histograms, Tiny Images, Geometric Probability Map and Geometric specific
histograms. The code for extracting the features and computing kernels for the features is taken from
1
http://wordnet.princeton.edu
1220
the original papers as described in Xiao et al. (2010). Using the features and kernels, we train one-vs-all
SVMs (Chang and Lin, 2011) to classify images into scene categories. As in Xiao et al. (2010), this gave
us 51 different SVM classifiers with different feature and kernel choices. We use the images from the
UIUC 15 scene dataset (Lazebnik et al., 2006) and the SUN 397 scene dataset (Xiao et al., 2010) for
training the scene classifiers for all scenes except kitchen. The training images for kitchen were obtained
by selecting 100 frames from about 15 training videos, since the classifier trained on images from the
existing scene datasets performed extremely poorly on the videos. We use all the classifiers to detect
scenes for each frame. We then average the scene detection scores over all the classifiers across all the
frames of the video. This gives us visual confidence values, C(t), over all scene categories t for the
video.
3.4 Language Statistics
A key aspect of our approach is the use of language statistics mined from English text corpora to
bias visual interpretation. Like Krishnamoorthy et al. (2013), we use dependency-parsed text from
four large ?out of domain? corpora: English Gigaword, British National Corpus (BNC), ukWac and
WaCkypedia EN. We also use a small, specialized ?in domain? corpus: dependency parsed sentences
from the human-generated, English descriptions for the YouTube training videos mentioned in Sec-
tion 3.1. We extract SVOP (subject, verb, object, place) tuples from the dependency parses. The subject-
verb relationships are identified using nsubj dependencies, the verb-object relationships using dobj and
prep dependencies. Object-place relationships are identified using the prep dependency, checking that
the noun modified by the preposition is one of our recognizable places (or synonyms of the recognizable
scenes as indicated by WordNet). We then extract co-occuring SV, VO, and OP bigram statistics from
the resulting SVOP tuples to inform our factor-graph model, which uses both the out-of-domain (p
o
) and
in-domain (p
i
) bigram probabilities.
3.5 Content Selection Using Factor Graphs
In order to combine visual and linguistic evidence, we use the probabilistic factor-graph model shown
in Figure 2. This model integrates the uncertain visual detections described in Sections 3.2 and 3.3 with
the language statistics described in Section 3.4 to predict the best words for describing the subject (S),
verb (V), object (O), and place (P) for each test video. After instantiating the potential functions for
this model, we perform a maximum a posteriori (MAP) estimation (via the max-product algorithm) to
determine the most probable joint set of values for these latent variables.
Figure 2: The factor graph model used for content selection (right), and sample frames from a video to
be described (left). Visual confidence values are observed (gray potentials) and inform sentence com-
ponents. Language potentials (dashed) connect latent words between sentence components. Samples of
the vision confidence values used as observations for the verb and object are shown for the example test
video.
1221
Observation Potentials. The observations in our model take the form of confidence scores from the
visual detectors described in Sections 3.2 and 3.3. That is, the potential for each sentence component
k ? {S, V,O, P}, ?
k
(t) = C
k
(t) is the detection confidence that the classifier for component k (C
k
)
gives to the word t.
Language Potentials. Language statistics were gathered as described in Section 3.4 and used to deter-
mine the language potentials as follows:
?
k,l
(t, s) := p(l = s|k = t) := ?p
o
(l = s|k = t) + (1? ?)p
i
(l = s|k = t)
Where k and l are two contiguous components in the SVOP sequence and t and s are words that are
possible values for these two components, respectively. We would expect
?
V,O
(ride,motorbike) := p(O=motorbike|V=ride)
to be relatively high, since motorbike is a likely object of the verb ride. The potential between two
sequential components k and l in the SVOP sequence is computed by linearly interpolating the bigram
probability observed in the out-of-domain corpus of general text (p
o
) and the in-domain corpus of video
descriptions (p
i
). The interpolation parameter ? adjusts the importance of these two corpora in deter-
mining the bigram probability. We optimized performance by fixing ? = 0.25 when cross-validating on
the training data. This weighting effectively allows general text corpora to be used to smooth the prob-
ability estimates for video descriptions. We note that meaningful information would likely be captured
by non-contiguous language potentials such as ?
V,P
, but that the resulting factor graphs would contain
cycles, preventing us from performing exact inference tractably.
3.6 Sentence Generation
Finally, we use the SVOP tuple chosen by our model to generate an English sentence using the following
template: ?Determiner (A,The) - Subject - Verb (Present, Present Continuous) - Preposition (optional)
- Determiner (A,The) - Object (optional) - Preposition - Determiner (A,The) - Place (optional)? The
most probable prepositions are identified using preposition-object and preposition-place bigram statistics
mined from the dependency parsed corpora described in Section 3.4. Given an SVOP tuple, our objective
is to generate a rich sentence using the subject, verb, object, and place information. However, it is not
prudent to add the object and place to the description of all videos since some verbs may be intransitive
and the place information may be redundant. In order to achieve the best set of components to include,
we use the above template to first generate a set of candidate sentences based on the SVO triple, SVP
triple and the SVOP quadruple. Then, each sentence type (SVO, SVP, and SVOP) is ranked using the
BerkeleyLM language model (Pauls and Klein, 2011) trained on the GoogleNgram corpus. Finally, we
output the sentence with the highest average 5-gram probability in order to normalize for sentence length.
4 Experimental Results
We compared using the vision system alone to our model, which augments that system with linguistic
knowledge. Specifically, we consider the Highest Vision Confidence (HVC) model, which takes for
each sentence component the word with the highest confidence from the state-of-the-art vision detectors
described in Sections 3.2 and 3.3. We compare the results of this model on the 670 test videos to those
of our Factor Graph Model (FGM), as discussed in Section 3.5.
4.1 N-gram Baseline
Additionally, we compare both models against the existing, baseline n-gram model of Krishnamoorthy
et al. (2013) by extending their best n-gram model to support places. To be specific, we build a quadra-
gram model, similar to the trigram model of Krishnamoorthy et al. (2013). We first extract SVOP tuples
from the dependency parses as described in Section 3.4. We then train a backoff language model with
Kneyser-Ney smoothing (Chen and Goodman, 1996) for estimating the likelihood of the SVOP quadru-
ple. On quadruples that are not seen during training, this quadragram language model backs off to SVO
1222
Most S% V% O% [P]% SVO% SVO[P]%
n-gram 76.57 11.04 11.19 18.30 2.39 1.86
HVC 76.57
+
22.24 11.94 17.24
+
4.33
+
2.92
FGM 76.42
+
21.34 12.39 19.89
+
5.67
+
3.71
Any
n-gram 86.87 19.25 21.94 21.75 5.67 2.65
HVC 86.57
+
38.66 22.09 21.22
+
10.15
+
4.24
FGM 86.27
+
37.16
+
24.63 24.67
+
10.45
+
6.10
Table 1: Average binary accuracy of predicting the most common word (top) and of predicting any given
word (bottom). Bold entries are statistically significantly (p < 0.05) greater than the HVC model, while
+
entries are significantly greater than the n-gram model. No model scored significantly higher than
FGM on any metric. [P] indicates that the score ranges only over the subset of videos for which any
annotator provided a place.
triple and subject-verb, verb-object, object-place bigrams to estimate the probability of the quadruple.
As in the case of the factor graph model, we consider the effect of learning from a domain specific text
corpus. We build quadragram language models for both out-of-domain and in-domain text-corpora de-
scribed in Section 3.4. The probability of a quadragram in the language model is computed by linearly
interpolating the probabilities from the in-domain and out-of-domain corpus. We experiment with dif-
ferent number of top subjects, objects, verbs, and places to estimate the most likely SVOP quadruple
from the quadragram language model. We report the results for the best performing n-gram model that
considers the top 5 subjects, 5 objects, 10 verbs, and 3 places based on the vision confidences and an out-
of-domain corpus weight of 1. This model also incorporates verb expansion as described in the original
work (Krishnamoorthy et al., 2013).
4.2 Content Evaluation
Table 1 shows the accuracy of the models when their prediction for each sentence component is consid-
ered correct only if it is the word most commonly used by human annotators to describe the video, as well
as the accuracy of the models when the prediction is considered correct if used by any of the annotators to
describe the video. We evaluate the accuracy of each component (S,V,O,P) individually, and for complete
SVO and SVOP tuples, where all components must be correct in order for a complete tuple to be judged
correct. Because only about half (56.3%) of test videos were described with a place by some annotator,
accuracies involving places (?[P]?) are averaged only over the subset of videos for which any annotator
provided a place. Significance was determined using a paired t-test which compared the distributions of
the binary correctness of each model?s prediction on each video for the specified component(s).
We also use the WUP metric from Wordnet::Similarity
2
to measure the quality of the predicted words
to account for semantically similar words. For example, where the binary metric would mark ?slice? as
an incorrect substitute for ?cut?, the WUP metric will provide ?partial credit? for such predictions. The
results using WUP similarity metrics for the most common word and any valid word (maximum WUP
similarity is chosen from among valid words) are presented in Table 2. Since WUP provides scores are in
the range [0,1], we view the scores as ?percent relevance,? and we obtain tuple scores for each sentence
by taking the product of the component WUP scores.
5 Discussion
It is clear from the results in Table 1 that both the HVC and the FGM outperform the n-gram language
model approach used in the most-similar previous work (Krishnamoorthy et al., 2013; Guadarrama et
al., 2013). Note that while Krishnamoorthy et al. (2013) showed an improvement with an n-gram model
considering only the top few vision detections, the FGM considers vision confidences over the entire set
2
http://wn-similarity.sourceforge.net/
1223
Most S% V% O% [P]% SVO% SVO[P]%
n-gram 89.00 41.56 44.01 57.62 17.53 10.83
HVC 89.09
+?
48.85 43.99 56.00
+
20.82
+
12.95
FGM 89.01
+
47.05
+
45.29
+
59.64
+
21.54
+
14.50
Any
n-gram 96.60 55.08 65.52 61.98 35.70 22.84
HVC 96.54
+?
65.61 65.32 60.67
+
42.53
+
27.75
FGM 96.32
+
63.49
+
67.52
+
64.68
+
42.43
+
29.34
Table 2: Average WUP score of the predicted word against the most common word (top) and the max-
imum score against any given word (bottom). Bold entries are statistically significantly (p < 0.05)
greater than the HVC model;
+
entries are significantly greater than the n-gram model;
?
entries are
significantly greater than the FGM. [P] indicates that the score ranges only over the subset of videos for
which any annotator provided a place.
of grammatical objects. Additionally, our models are evaluated on a much more diverse set of videos
while Krishnamoorthy et al. (2013) evaluate the n-gram model on 185 videos (a small subset of the
1,967 videos containing the 20 grammatical objects that their system recognized).
The performance differences between the vision system (HVC) and our integrated model (FGM) are
modest but significant in important places. Specifically, the FGM makes improvements to SVO (Table 1,
top) and SVOP (Table 2, top) tuple accuracies. FGM also significantly improves both the O and [P] (Ta-
ble 1, bottom, and Table 2) component accuracies, suggesting that it can help clean up some noise from
the vision systems even at the component level by considering related bigram probabilities. FGM causes
no significant losses under the binary metric, but performs worse than the HVC model on predicting a
verb component semantically similar to the correct verb under the WUP metric (Table 2). This loss on
the verb component is worth the gains in tuple accuracy, since tuple prediction is the more difficult and
most central part of the content selection task. Additionally, experiments by the authors of Guadarrama
et al. (2013) on Amazon Mechanical Turk have shown that humans tend to heavily penalize tuples and
descriptions even if they have most of the components correct.
Table 3 shows frames from some test videos and the sentence components chosen by the models to
describe them. In the top four videos we see the FGM improving raw vision results. For example, it
determines that a person is more likely slicing an onion than an egg. Some specific confidence values
for the HVC can be seen for this video in Figure 2. In the bottom two videos of Table 3 we see the HVC
performing better without linguistic information. For example, the FGM intuits that a person is more
likely to be driving a car than lifting it, and steers the prediction away from the correct verb. This may
be part of a larger phenomenon in which YouTube videos often depict unusual actions, and consequently
general language knowledge can sometimes hurt performance by selecting more common activities.
6 Future Work
Compared to the human gold standard descriptions, there appears to be room for improvement in de-
tecting activities, objects, and scenes with high precision. Visual recognition of entities and activities in
diverse real-world videos is extremely challenging, partially due to lack of training data. As a result our
current model is faced with large amounts of noise in the vision potentials, especially for objects. Going
forward, we believe that improving visual recognition will allow the language statistics to be even more
useful. We are currently exploring deep image feature representations (Donahue et al., 2013) to improve
object and verb recognition, as well as model transfer from large labeled object ontologies (Deng et al.,
2009).
From the generation perspective, there is scope to move beyond the template based sentence gener-
ation. This becomes particularly relevant if we detect multiple grammatical objects such as adjectives
or adverbs. We need to decide whether additional grammatical objects would enrich the sentence de-
1224
FGM improves over HVC
?A person is slicing the onion in the kitchen?
Gold: person, slice, onion, (none)
HVC: person, slice, egg, kitchen
FGM: person, slice, onion, kitchen
?A person is running a race on the road?
Gold: person, run, race, (none)
HVC: person, ride, race, ground
FGM: person, run, race, road
?A person is playing the guitar on the stage?
Gold: person, play, guitar, tree
HVC: person, play, water, kitchen
FGM: person, play, guitar, stage
?A person is playing a guitar in the house?
Gold: person, play, guitar, (none)
HVC: person, pour, chili, kitchen
FGM: person, play, guitar, house
HVC better alone
?A person is lifting a car on the road?
Gold: person, lift, car, ground
HVC: person, lift, car, road
FGM: person, drive, car, road
?A person is pouring the egg in the kitchen?
Gold: person, pour, mushroom, kitchen
HVC: person, pour, egg, kitchen
FGM: person, play, egg, kitchen
Table 3: Example videos and: (Gold) the most common SVOP provided by annotators; (HVC) the
highest vision confidence selections; (FGM) the selections from our factor graph model. The top section
shows videos where the FGM improved over HVC; the bottom shows videos where the HVC did better
alone. For each video, the sentence generated from the components chosen from the more successful
system is shown.
scription and identify when to add them appropriately. With increasing applications for such systems in
automatic video surveillance and video retrieval, generating richer and more diverse sentences for longer
videos is an area for future research. In comparison to previous approaches (Krishnamoorthy et al., 2013;
Yang et al., 2011) the factor graph model can be easily extended to support this. Additional nodes can be
attached suitably to the graph to enable the prediction of adjectives and adverbs to enrich the base SVOP
tuple.
7 Conclusions
This work introduces a new framework to generate simple descriptions of short videos by integrating
visual detection confidences with language statistics obtained from large textual corpora. Experimental
results show that our approach achieves modest improvements over a pure vision system and signifi-
cantly improves over previous methods in predicting the complete subject-verb-object and subject-verb-
object-place tuples. Our work has a broad coverage of objects and verbs and extends previous works by
predicting place information.
1225
There are instances where our model fails to predict the correct verb when compared to the HVC
model. This could partially be because the SVM classifiers that detect activity already leverage entity
information during training, and adding external language does not appear to improve verb prediction
significantly. Further detracting from performance, our model occasionally propagates, rather than cor-
recting, errors from the HVC. For example, when the HVC predicts the correct verb and incorrect object,
such as in ?person ride car? when the video truly depicts a person riding a motorbike, our model selects
the more likely verb pairing ?person drive car?, extending the error from the object to the verb as well.
Despite these drawbacks, our approach predicts complete subject-verb-object-place tuples more
closely related to the most commonly used human descriptions than vision alone (Table 2), and in general
improves both object and place recognition accuracies (Tables 1, 2).
Acknowledgements
This work was funded by NSF grant IIS1016312, DARPA Minds Eye grant W911NF-10-9-0059, and
NSF ONR ATL grant N00014-11-1-0105. Some of our experiments were run on the Mastodon Cluster
(NSF grant EIA-0303609).
References
Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux,
Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark
Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, and Zhiqi Zhang. 2012. Video in sentences out.
In Association for Uncertainty in Artificial Intelligence (UAI).
Tamara Berg and Julia Hockenmaier. 2013. Workshop on vision and language. In Proceedings of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL).
NAACL.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions
on Intelligent Systems and Technology (TIST), 2(3):27.
David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1, pages 190?200. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling.
In Proceedings of the 34th annual meeting on Association for Computational Linguistics (ACL), pages 310?318.
Association for Computational Linguistics.
Pradipto Das, Chenliang Xu, Richard F. Doell, and Jason J. Corso. 2013. A thousand frames in just a few
words: Lingual description of videos through latent topics and sparse object stitching. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
Jia Deng, Kai Li, Minh Do, Hao Su, and Li Fei-Fei. 2009. Construction and analysis of a large scale image
ontology. Vision Sciences Society.
Jia Deng, Jonathan Krause, Alex Berg, and Li Fei-Fei. 2012. Hedging Your Bets: Optimizing Accuracy-
Specificity Trade-offs in Large Scale Visual Recognition. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2013.
Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531.
Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2010. The
pascal visual object classes (voc) challenge. International Journal of Computer Vision (IJCV), 88(2):303?338,
June.
Yansong Feng and Mirella Lapata. 2013. Automatic caption generation for news images. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI), 35(4):797?812.
1226
Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney,
Trevor Darrell, and Kate Saenko. 2013. Youtube2text: Recognizing and describing arbitrary activities using
semantic hierarchies and zero-shot recognition. In IEEE International Conference on Computer Vision (ICCV),
December.
Niveda Krishnamoorthy, Girish Malkarnenkar, Raymond J. Mooney, Kate Saenko, and Sergio Guadarrama. 2013.
Generating natural-language video descriptions using text-mined knowledge. In Proceedings of the AAAI Con-
ference on Artificial Intelligence (AAAI), pages 541?547.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Alexander Berg, Yejin Choi, and Tamara Berg. 2011.
Baby talk: Understanding and generating image descriptions. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). IEEE.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of features: Spatial pyramid matching
for recognizing natural scene categories. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), volume 2, pages 2169?2178. IEEE.
Li-Jia Li, Hao Su, Eric Xing, and Li Fei-Fei. 2010. Object bank: A high-level image representation for scene
classication and semantic feature sparsification. In Advances in Neural Information Processing Systems (NIPS).
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple
image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational
Natural Language Learning (CoNLL), pages 220?228, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Tanvi S. Motwani and Raymond J. Mooney. 2012. Improving video activity recognition using object recognition
and text mining. In Proceedings of the European Conference on Artificial Intelligence (ECAI), pages 600?605.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Information Processing Systems (NIPS), volume 24, pages
1143?1151.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258?267.
Association for Computational Linguistics.
John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood
methods. In Advances In Large Margin Classifiers, pages 61?74. MIT Press.
Marcus Rohrbach, Qiu Wei, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele. 2013. Translating video
content to natural language descriptions. In IEEE International Conference on Computer Vision (ICCV).
Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Sikandar Amin, Mykhaylo Andriluka, Manfred
Pinkal, and Bernt Schiele. 2014. Coherent multi-sentence video description with variable level of detail. arXiv
preprint arXiv:1403.6173.
Heng Wang, Alexander Klaser, Cordelia Schmid, and Cheng-Lin Liu. 2011. Action recognition by dense trajec-
tories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3169?3176. IEEE.
Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. Sun database: Large-
scale scene recognition from abbey to zoo. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3485?3492.
Yezhou Yang, Ching Lik Teo, Hal Daum?e, III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation
of natural images. In Conference on Emperical Methods in Natural Language Processing (EMNLP), pages
444?454.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from video described with sentences. In
Proceedings of the Association for Computational Linguistics (ACL), pages 53?63.
Jianguo Zhang, Marcin Marsza?ek, Svetlana Lazebnik, and Cordelia Schmid. 2007. Local features and kernels
for classification of texture and object categories: A comprehensive study. International Journal of Computer
Vision (IJCV), 73(2):213?238.
1227
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173?1182,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Mixture Model with Sharing for Lexical Semantics
Joseph Reisinger
Department of Computer Science
University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
joeraii@cs.utexas.edu
Raymond Mooney
Department of Computer Science
University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX, 78701
mooney@cs.utexas.edu
Abstract
We introduce tiered clustering, a mixture
model capable of accounting for varying de-
grees of shared (context-independent) fea-
ture structure, and demonstrate its applicabil-
ity to inferring distributed representations of
word meaning. Common tasks in lexical se-
mantics such as word relatedness or selec-
tional preference can benefit from modeling
such structure: Polysemous word usage is of-
ten governed by some common background
metaphoric usage (e.g. the senses of line or
run), and likewise modeling the selectional
preference of verbs relies on identifying com-
monalities shared by their typical arguments.
Tiered clustering can also be viewed as a form
of soft feature selection, where features that do
not contribute meaningfully to the clustering
can be excluded. We demonstrate the applica-
bility of tiered clustering, highlighting partic-
ular cases where modeling shared structure is
beneficial and where it can be detrimental.
1 Introduction
Word meaning can be represented as high-
dimensional vectors inhabiting a common space
whose dimensions capture semantic or syntactic
properties of interest (e.g. Erk and Pado, 2008;
Lowe, 2001). Such vector-space representations of
meaning induce measures of word similarity that can
be tuned to correlate well with judgements made
by humans. Previous work has focused on de-
signing feature representations and semantic spaces
that capture salient properties of word meaning (e.g.
Curran, 2004; Gabrilovich and Markovitch, 2007;
Landauer and Dumais, 1997), often leveraging the
distributional hypothesis, i.e. that similar words ap-
pear in similar contexts (Miller and Charles, 1991;
Pereira et al, 1993).
Since vector-space representations are con-
structed at the lexical level, they conflate multiple
word meanings into the same vector, e.g. collaps-
ing occurrences of bankinstitution and bankriver. Meth-
ods such as Clustering by Committee (Pantel, 2003)
and multi-prototype representations (Reisinger and
Mooney, 2010) address this issue by perform-
ing word-sense disambiguation across word occur-
rences, and then building meaning vectors from
the disambiguated words. Such approaches can
readily capture the structure of homonymous words
with several unrelated meanings (e.g. bat and club),
but are not suitable for representing the common
metaphor structure found in highly polysemous
words such as line or run.
In this paper, we introduce tiered clustering, a
novel probabilistic model of the shared structure
often neglected in clustering problems. Tiered
clustering performs soft feature selection, allocat-
ing features between a Dirichlet Process cluster-
ing model and a background model consisting of
a single component. The background model ac-
counts for features commonly shared by all occur-
rences (i.e. context-independent feature variation),
while the clustering model accounts for variation
in word usage (i.e. context-dependent variation, or
word senses; Table 1).
Using the tiered clustering model, we derive a
multi-prototype representation capable of capturing
varying degrees of sharing between word senses,
and demonstrate its effectiveness in lexical seman-
tic tasks where such sharing is desirable. In partic-
ular we show that tiered clustering outperforms the
multi-prototype approach for (1) selectional prefer-
ence (Resnik, 1997; Pantel et al, 2007), i.e. predict-
1173
ing the typical filler of an argument slot of a verb,
and (2) word-relatedness in the presence of highly
polysemous words. The former case exhibits a high
degree of explicit structure, especially for more se-
lectionally restrictive verbs (e.g. the set of things that
can be eaten or can shoot).
The remainder of the paper is organized as fol-
lows: Section 2 gives relevant background on the
methods compared, Section 3 outlines the multi-
prototype model based on the Dirichlet Process mix-
ture model, Section 4 derives the tiered cluster-
ing model, Section 5 discusses similarity metrics,
Section 6 details the experimental setup and in-
cludes a micro-analysis of feature selection, Section
7 presents results applying tiered clustering to word
relatedness and selectional preference, Section 8 dis-
cusses future work, and Section 9 concludes.
2 Background
Models of the attributional similarity of concepts,
i.e. the degree to which concepts overlap based on
their attributes (Turney, 2006), are commonly imple-
mented using vector-spaces derived from (1) word
collocations (Schu?tze, 1998), directly leveraging the
distributional hypothesis (Miller and Charles, 1991),
(2) syntactic relations (Pado? and Lapata, 2007), (3)
structured corpora (e.g. Gabrilovich and Markovitch
(2007)) or (4) latent semantic spaces (Finkelstein
et al, 2001; Landauer and Dumais, 1997). Such
models can be evaluated based on their correlation
with human-reported lexical similarity judgements
using e.g. the WordSim-353 collection (Finkelstein
et al, 2001). Distributional methods exhibit a high
degree of scalability (Gorman and Curran, 2006) and
have been applied broadly in information retrieval
(Manning et al, 2008), large-scale taxonomy induc-
tion (Snow et al, 2006), and knowledge acquisition
(Van Durme and Pas?ca, 2008).
Reisinger and Mooney (2010) introduced a multi-
prototype approach to vector-space lexical seman-
tics where individual words are represented as col-
lections of ?prototype? vectors. This representation
is capable of accounting for homonymy and poly-
semy, as well as other forms of variation in word
usage, like similar context-dependent methods (Erk
and Pado, 2008). The set of vectors for a word
is determined by unsupervised word sense discov-
ery (Schu?tze, 1998), which clusters the contexts in
which a word appears. Average prototype vectors
LIFE
all, about, life, would, death
my, you, real, your, about
spent, years, rest, lived, last
sentenced, imprisonment, sentence, prison
insurance, peer, Baron, member, company
Guru, Rabbi, Baba, la, teachings
RADIO
station, radio, stations, television
amateur, frequency, waves, system
show, host, personality, American
song, single, released, airplay
operator, contact, communications, message
WIZARD
evil, powerful, magic, wizard
Merlin, King, Arthur, Arthurian
fairy, wicked, scene, tale
Harry, Potter, Voldemort, Dumbledore
STOCK
stock, all, other, company, new
market, crash, markets, price, prices
housing, breeding, fish, water, horses
car, racing, cars, NASCAR, race, engine
card, cards, player, pile, game, paper
rolling, locomotives, line, new, railway
Table 1: Example tiered clustering representation of
words with varying degrees of polysemy. Each boxed
set shows the most common background (shared) fea-
tures, and each prototype captures one thematic usage
of the word. For example, wizard is broken up into a
background cluster describing features common to all us-
ages of the word (e.g., magic and evil) and several genre-
specific usages (e.g. Merlin, fairy tales and Harry Potter).
are then computed separately for each cluster, pro-
ducing a distributed representation for each word.
Distributional methods have also proven to be a
powerful approach to modeling selectional prefer-
ence (Pado? et al, 2007; Pantel et al, 2007), rivaling
methods based on existing semantic resources such
as WordNet (Clark and Weir, 2002; Resnik, 1997)
and FrameNet (Pado?, 2007) and performing nearly
as well as supervised methods (Herdag?delen and Ba-
roni, 2009). Selectional preference has been shown
to be useful for, e.g., resolving ambiguous attach-
ments (Hindle and Rooth, 1991), word sense disam-
biguation (McCarthy and Carroll, 2003) and seman-
tic role labeling (Gildea and Jurafsky, 2002).
3 Multi-Prototype Models
Representing words as mixtures over several pro-
totypes has proven to be a powerful approach to
1174
vector-space lexical semantics (Pantel, 2003; Pantel
et al, 2007; Reisinger and Mooney, 2010). In this
section we briefly introduce a version of the multi-
prototype model based on the Dirichlet Process Mix-
ture Model (DPMM), capable of inferring automat-
ically the number of prototypes necessary for each
word (Rasmussen, 2000). Similarity between two
DPMM word-representations is then computed as a
function of their cluster centroids (?5), instead of the
centroid of all the word?s occurrences.
Multiple prototypes for each word w are gener-
ated by clustering feature vectors vpcq derived from
each occurrence c P Cpwq in a large textual cor-
pus and collecting the resulting cluster centroids
pikpwq, k P r1,Kws. This approach is commonly
employed in unsupervised word sense discovery;
however, we do not assume that clusters correspond
to word senses. Rather, we only rely on clusters to
capture meaningful variation in word usage.
Instead of assuming all words can be repre-
sented by the same number of clusters, we allocate
representational flexibility dynamically using the
DPMM. The DPMM is an infinite capacity model
capable of assigning data to a variable, but finite
number of clusters Kw, with probability of assign-
ment to cluster k proportional to the number of data
points previously assigned to k. A single parameter
? controls the degree of smoothing, producing more
uniform clusterings as ? ? 8. Using this model,
the number of clusters no longer needs to be fixed
a priori, allowing the model to allocate expressivity
dynamically to concepts with richer structure. Such
a model naturally allows the word representation to
allocate additional capacity for highly polysemous
words, with the number of clusters growing loga-
rithmically with the number of occurrences. The
DPMM has been used for rational models of con-
cept organization (Sanborn et al, 2006), but to our
knowledge has not yet been applied directly to lexi-
cal semantics.
4 Tiered Clustering
Tiered clustering allocates features between two
submodels: a (context-dependent) DPMM and a sin-
gle (context-independent) background component.
This model is similar structurally to the feature se-
lective clustering model proposed by Law et al
(2002). However, instead of allocating entire feature
dimensions between model and background compo-
 
 
?
z
?
D w
 
w
?
!
?
background
!
c
 
?
 
?
 
?
clusters
d
Figure 1: Plate diagram for the tiered clustering model
with cluster indicators drawn from the Chinese Restau-
rant Process.
nents, assignment is done at the level of individual
feature occurrences, much like topic assignment in
Latent Dirichlet Allocation (LDA; Griffiths et al,
2007). At a high level, the tiered model can be
viewed as a combination of a multi-prototype model
and a single-prototype back-off model. However,
by leveraging both representations in a joint frame-
work, uninformative features can be removed from
the clustering, resulting in more semantically tight
clusters.
Concretely, each word occurrence wd first selects
a cluster ?d from the DPMM; then each feature wi,d
is generated from either the background model?back
or the selected cluster ?d, determined by the tier
indicator zi,d. The full generative model for tiered
clustering is given by
?d|?  Betap?q d P D,
?d|?, G0  DPp?, G0q d P D,
?back|?back  Dirichletp?backq
zi,d|?d  Bernoullip?dq i P |wd|,
wi,d|?d, zi,d 
$
'
'
&
'
'
%
Multp?backq
pzi,d  1q
Multp?dq
potherwiseq
i P |wd|,
where ? controls the per-data tier distribution
smoothing and ? controls the uniformity of the DP
cluster allocation. The DP is parameterized by a
base measure G0, controlling the per-cluster term
distribution smoothing; which use a Dirichlet with
hyperparameter ?, as is common (Figure 1).
Since the background topic is shared across all oc-
currences, it can account for features with context-
independent variance, such as stop words and other
high-frequency noise, as well as the central tendency
of the collection (Table 1). Furthermore, it is possi-
ble to put an asymmetric prior on ?, yielding more
fine-grained control over the assumed uniformity of
the occurrence of noisy features, unlike in the model
proposed by Law et al (2002).
1175
Although exact posterior inference is intractable
in this model, we derive an efficient collapsed Gibbs
sampler via analogy to LDA (Appendix 1).
5 Measuring Semantic Similarity
Due to its richer representational structure, comput-
ing similarity in the multi-prototype model is less
straightforward than in the single prototype case.
Reisinger and Mooney (2010) found that simply av-
eraging all similarity scores over all pairs of proto-
types (sampled from the cluster distributions) per-
forms reasonably well and is robust to noise. Given
two words w and w1, this AvgSim metric is
AvgSimpw,w1q def
1
KwKw1
Kw?
j1
Kw1
?
k1
dppikpwq, pijpw
1
qq
Kw andKw1 are the number of clusters for w and w1
respectively, and dp, q is a standard distributional
similarity measure (e.g. cosine distance). As cluster
sizes become more uniform, AvgSim tends towards
the single prototype similarity,1 hence the effective-
ness of AvgSim stems from boosting the influence
of small clusters.
Tiered clustering representations offer more pos-
sibilities for computing semantic similarity than
multi-prototype, as the background prototype can be
treated separately from the other prototypes. We
make use of a simple sum of the distance between
the two background components, and the AvgSim
of the two sets of clustering components.
6 Experimental Setup
6.1 Corpus
Word occurrence statistics are collected from a snap-
shot of English Wikipedia taken on Sept. 29th, 2009.
Wikitext markup is removed, as are articles with
fewer than 100 words, leaving 2.8M articles with a
total of 2.05B words. Wikipedia was chosen due to
its semantic breadth.
6.2 Evaluation Methodology
We evaluate the tiered clustering model on two prob-
lems from lexical semantics: word relatedness and
selectional preference. For the word relatedness
1This can be problematic for certain clustering methods
that specify uniform priors over cluster sizes; however the
DPMM naturally exhibits a linear decay in cluster sizes with
the Er# clusters of size M s  ?{M .
Rating distribution
WS-3530.0
0.5
1.0
Evocation Pado
Sense count distribution
WS-3530
3
10
80
Evocation Pado
Figure 2: (top) The distribution of ratings (scaled [0,1])
on WS-353, WN-Evocation and Pado? datasets. (bottom)
The distribution of sense counts for each data set (log-
domain), collected from WordNet 3.0.
evaluation, we compared the predicted similarity of
word pairs from each model to two collections of hu-
man similarity judgements: WordSim-353 (Finkel-
stein et al, 2001) and the Princeton Evocation rela-
tions (WN-Evocation, Ma et al, 2009).
WS-353 contains between 13 and 16 human sim-
ilarity judgements for each of 353 word pairs, rated
on a 1?10 integer scale. WN-Evocation is signif-
icantly larger than WS-353, containing over 100k
similarity comparisons collected from trained hu-
man raters. Comparisons are assigned to only 3-
5 human raters on average and contain a signifi-
cantly higher fraction of zero- and low-similarity
items than WS-353 (Figure 2), reflecting more ac-
curately real-world lexical semantics settings. In our
experiments we discard all comparisons with fewer
than 5 ratings and then sample 10% of the remain-
ing pairs uniformly at random, resulting in a test set
with 1317 comparisons.
For selectional preference, we employ the Pado?
dataset, which contains 211 verb-noun pairs with
human similarity judgements for how plausible the
noun is for each argument of the verb (2 arguments
per verb, corresponding roughly to subject and ob-
ject). Results are averaged across 20 raters; typical
inter-rater agreement is ?  0.7 (Pado? et al, 2007).
In all cases correlation with human judgements
is computed using Spearman?s nonparametric rank
correlation (?) with average human judgements
1176
(Agirre et al, 2009).
6.3 Feature Representation
In the following analyses we confine ourselves to
representing word occurrences using unordered un-
igrams collected from a window of size T10 cen-
tered around the occurrence, represented using tf-idf
weighting. Feature vectors are pruned to a fixed
length f , discarding all but the highest-weight fea-
tures (f is selected via empirical validation, as de-
scribed in the next section). Finally, semantic simi-
larity between word pairs is computed using cosine
distance (`2-normalized dot-product).2
6.4 Feature Pruning
Feature pruning is one of the most significant factors
in obtaining high correlation with human similarity
judgements using vector-space models, and has been
suggested as one way to improve sense disambigua-
tion for polysemous verbs (Xue et al, 2006). In this
section, we calibrate the single prototype and multi-
prototype methods on WS-353, reaching the limit
of human and oracle performance and demonstrat-
ing robust performance gains even with semanti-
cally impoverished features. In particular we obtain
?0.75 correlation on WS-353 using only unigram
collocations and ?0.77 using a fixed-K multi-
prototype representation (Figure 3; Reisinger and
Mooney, 2010). This result rivals average human
performance, obtaining correlation near that of the
supervised oracle approach of Agirre et al (2009).
The optimal pruning cutoff depends on the fea-
ture weighting and number of prototypes as well as
the feature representation. t-test and ?2 features are
most robust to feature noise and perform well even
with no pruning; tf-idf yields the best results but is
most sensitive to the pruning parameter (Figure 3).
As the number of features increases, more pruning
is required to combat feature noise.
Figure 4 breaks down the similarity pairs into four
quantiles for each data set and then shows corre-
lation separately for each quantile. In general the
more polarized data quantiles (1 and 4) have higher
correlation, indicating that fine-grained distinctions
2(Parameter robustness) We observe lower correlations on
average for T25 and T5 and therefore observe T10 to
be near-optimal. Substituting weighted Jaccard similarity for
cosine does not significantly affect the results in this paper.
0
0.4
0.8
0
0.4
0.8
S
p
e
a
r
m
a
n
'
s
 
?
 
0.7
0.0
-0.2
unpruned pruned (best)
Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4
human
Single-p
Multi-p
ESA
Figure 4: Correlation results on WS-353 broken down
over quantiles in the human ratings. Quantile ranges are
shown in Figure 2. In general ratings for highly sim-
ilar (dissimilar) pairs are more predictable (quantiles 1
and 4) than middle similarity pairs (quantiles 2, 3). ESA
shows results for a more semantically rich feature set de-
rived using Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007).
in semantic distance are easier for those sets.3 Fea-
ture pruning improves correlations in quantiles 2?4
while reducing correlation in quantile 1 (lowest sim-
ilarity). This result is to be expected as more fea-
tures are necessary to make fine-grained distinctions
between dissimilar pairs.
7 Results
We evaluate four models: (1) the standard single-
prototype approach, (2) the DPMM multi-prototype
approach outlined in ?3, (3) a simple combina-
tion of the multi-prototype and single-prototype ap-
proaches (MP+SP)4 and (4) the tiered clustering ap-
proach (?4). Each data set is divided into 5 quan-
tiles based on per-pair average sense counts,5 col-
lected from WordNet 3.0 (Fellbaum, 1998); ex-
amples of pairs in the high-polysemy quantile are
shown in Table 2. Unless otherwise specified,
both DPMM multi-prototype and tiered clustering
3The fact that the per-quantile correlation is significantly
lower than the full correlation e.g. in the human case indicates
that fine-grained ordering (within quantile) is more difficult than
coarse-grained (between quantile).
4(MP+SP) Tiered clustering?s ability to model both shared
and idiosyncratic structure can be easily approximated by us-
ing the single prototype model as the shared component and
multi-prototype model as the clustering. However, unlike in the
tiered model, all features are assigned to both components. We
demonstrate that this simplification actually hurts performance.
5Despite many skewed pairs (e.g. line has 36 senses while
insurance has 3), we found that arithmetic average and geomet-
ric average perform the same.
1177
00.4
0.8
0
0.4
0.8
0
0.4
0.8
0
0.4
0.8
K=1 K=10 K=50 tf-idf cosine, K=1,10,50
S
p
e
a
r
m
a
n
'
s
 
?
 
0.0
0.8
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
# of features
all10k5k2k1k5002001002010
tf-idf
ttest
?2
tf
K=50
K=10
K=1
tf-idf
ttest
?2
tf
tf-idf
ttest
?2
tf
// //
// //
Figure 3: Effects of feature pruning and representation on WS-353 correlation broken down across multi-prototype
representation size. In general tf-idf features are the most sensitive to pruning level, yielding the highest correlation for
moderate levels of pruning and significantly lower correlation than other representations without pruning. The optimal
amount of pruning varies with the number of prototypes used, with fewer features being optimal for more clusters.
Bars show 95% confidence intervals.
WordSim-353
stock-live, start-match, line-insurance, game-
round, street-place, company-stock
Evocation
break-fire, clear-pass, take-call, break-tin,
charge-charge, run-heat, social-play
Pado?
see-drop, see-return, hit-stock, raise-bank, see-
face, raise-firm, raise-question
Table 2: Examples of highly polysemous pairs from each
data set using sense counts from WordNet.
use symmetric Dirichlet hyperparameters, ?0.1,
?0.1, and tiered clustering uses?10 for the back-
ground/clustering allocation smoother.
7.1 WordSim-353
Correlation results for WS-353 are shown in Table
3. In general the approaches incorporating multiple
prototypes outperform single prototype (?  0.768
vs. ?  0.734). The tiered clustering model does not
significantly outperform either the multi-prototype
or MP+SP models on the full set, but yields signifi-
cantly higher correlation on the high-polysemy set.
The tiered model generates more clusters than
DPMM multi-prototype (27.2 vs. 14.8), despite us-
ing the same hyperparameter settings: Since words
commonly shared across clusters have been allo-
cated to the background component, the cluster
components have less overlap and hence the model
naturally allocates more clusters.
Examples of the tiered clusterings for several
Method ?  100 ErCs background
Single prototype 73.40.5 1.0 -
high polysemy 76.00.9 1.0 -
Multi-prototype 76.80.4 14.8 -
high polysemy 79.31.3 12.5 -
MP+SP 75.40.5 14.8 -
high polysemy 80.11.0 12.5 -
Tiered 76.90.5 27.2 43.0%
high polysemy 83.11.0 24.2 43.0%
Table 3: Spearman?s correlation on the WS-353 data set.
All refers to the full set of pairs, high polysemy refers to
the top 20% of pairs, ranked by sense count. ErCs is the
average number of clusters employed by each method and
background is the average percentage of features allo-
cated by the tiered model to the background cluster. 95%
confidence intervals are computed via bootstrapping.
words from WS-353 are shown in Table 1 and corre-
sponding clusters from the multi-prototype approach
are shown in Table 4. In general the background
component does indeed capture commonalities be-
tween all the sense clusters (e.g. all wizards use
magic) and hence the tiered clusters are more se-
mantically pure. This effect is most visible in the-
matically polysemous words, e.g. radio and wizard.
7.2 Evocation
Compared to WS-353, the WN-Evocation pair set
is sampled more uniformly from English word pairs
and hence contains a significantly larger fraction of
unrelated words, reflecting the fact that word sim-
1178
LIFE
my, you, real, about, your, would
years, spent, rest, lived, last
sentenced, imprisonment, sentence, prison
years, cycle, life, all, expectancy, other
all, life, way, people, human, social, many
RADIO
station, FM, broadcasting, format, AM
radio, station, stations, amateur,
show, station, host, program, radio
stations, song, single, released, airplay
station, operator, radio, equipment, contact
WIZARD
evil, magic, powerful, named, world
Merlin, King, Arthur, powerful, court
spells, magic, cast, wizard, spell, witch
Harry, Dresden, series, Potter, character
STOCK
market, price, stock, company, value, crash
housing, breeding, all, large, stock, many
car, racing, company, cars, summer, NASCAR
stock, extended, folded, card, barrel, cards
rolling, locomotives, new, character, line
Table 4: Example DPMM multi-prototype representation
of words with varying degrees of polysemy. Compared to
the tiered clustering results in Table 1 the multi-prototype
clusters are significantly less pure for thematically poly-
semous words such as radio and wizard.
ilarity is a sparse relation (Figure 2 top). Further-
more, it contains proportionally more highly polyse-
mous words relative to WS-353 (Figure 2 bottom).
On WN-Evocation, the single prototype and
multi-prototype do not differ significantly in terms
of correlation (?0.198 and ?0.201 respectively;
Table 5), while SP+MP yields significantly lower
correlation (?0.176), and the tiered model yields
significantly higher correlation (?0.224). Restrict-
ing to the top 20% of pairs with highest human
similarity judgements yields similar outcomes, with
single prototype, multi-prototype and SP+MP sta-
tistically indistinguishable (?0.239, ?0.227 and
?0.235), and tiered clustering yielding signifi-
cantly higher correlation (?0.277). Likewise tiered
clustering achieves the most significant gains on the
high polysemy subset.
7.3 Selectional Preference
Tiered clustering is a natural model for verb selec-
tional preference, especially for more selectionally
restrictive verbs: the set of words that appear in a
particular argument slot naturally have some kind of
Method ?  100 ErCs background
Single prototype 19.80.6 1.0 -
high similarity 23.91.1 1.0 -
high polysemy 11.51.2 1.0 -
Multi-prototype 20.10.5 14.8 -
high similarity 22.71.2 14.1 -
high polysemy 13.01.3 13.2 -
MP+SP 17.60.5 14.8 -
high similarity 23.51.2 14.1 -
high polysemy 11.41.0 13.2 -
Tiered 22.40.6 29.7 46.6%
high similarity 27.71.3 29.9 47.2%
high polysemy 15.41.1 27.4 46.6%
Table 5: Spearman?s correlation on the Evocation data
set. The high similarity subset contains the top 20% of
pairs sorted by average rater score.
Method ?  100 ErCs background
Single prototype 25.80.8 1.0 -
high polysemy 17.31.7 1.0 -
Multi-prototype 20.21.0 18.5 -
high polysemy 14.12.4 17.4 -
MP+SP 19.71.0 18.5 -
high polysemy 10.52.5 17.4 -
Tiered 29.41.0 37.9 41.7%
high polysemy 28.52.4 37.4 43.2%
Table 6: Spearman?s correlation on the Pado? data set.
commonality (i.e. they can be eaten or can promise).
The background component of the tiered clustering
model can capture such general argument structure.
We model each verb argument slot in the Pado? set
with a separate tiered clustering model, separating
terms co-occurring with the target verb according to
which slot they fill.
On the Pado? set, the performance of the DPMM
multi-prototype approach breaks down and it yields
significantly lower correlation with human norms
than the single prototype (?0.202 vs. ?0.258;
Table 6), due to its inability to capture the shared
structure among verb arguments. Furthermore com-
bining with the single prototype does not signif-
icantly change its performance (?0.197). Mov-
ing to the tiered model, however, yields significant
improvements in correlation over the other models
(?0.294), primarily improving correlation in the
case of highly polysemous verbs and arguments.
1179
8 Discussion and Future Work
We have demonstrated a novel model for dis-
tributional lexical semantics capable of capturing
both shared (context-independent) and idiosyncratic
(context-dependent) structure in a set of word occur-
rences. The benefits of this tiered model were most
pronounced on a selectional preference task, where
there is significant shared structure imposed by con-
ditioning on the verb. Although our results on the
Pado? are not state of the art,6 we believe this to be
due to the impoverished vector-space design; tiered
clustering can be applied to more expressive vec-
tor spaces, such as those incorporating dependency
parse and FrameNet features.
One potential explanation for the superior perfor-
mance of the tiered model vs. the DPMM multi-
prototype model is simply that it allocates more
clusters to represent each word (Reisinger and
Mooney, 2010). However, we find that decreas-
ing the hyperparameter ? (decreasing vocabulary
smoothing and hence increasing the effective num-
ber of clusters) beyond ?  0.1 actually hurts multi-
prototype performance. The additional clusters do
not provide more semantic content due to significant
background similarity.
Finally, the DPMM multi-prototype and tiered
clustering models allocate clusters based on the vari-
ance of the underlying data set. We observe a neg-
ative correlation (?0.33) between the number of
clusters allocated by the DPMM and the number of
word senses found in WordNet. This result is most
likely due to our use of unigram context window
features, which induce clustering based on thematic
rather than syntactic differences. Investigating this
issue is future work.
(Future Work) The word similarity experiments
can be expanded by breaking pairs down further into
highly homonymous and highly polysemous pairs,
using e.g. WordNet to determine how closely related
the senses are. With this data it would be interest-
ing to validate the hypothesis that the percentage of
features allocated to the background cluster is corre-
lated with the degree of homonymy.
The basic tiered clustering can be extended with
additional background tiers, allocating more expres-
sivity to model background feature variation. This
class of models covers the spectrum between a pure
6E.g., Pado? et al (2007) report ?0.515 on the same data.
topic model (all background tiers) and a pure clus-
tering model and may be reasonable when there is
believed to be more background structure (e.g. when
jointly modeling all verb arguments). Furthermore,
it is straightforward to extend the model to a two-
tier, two-clustering structure capable of additionally
accounting for commonalities between arguments.
Applying more principled feature selection ap-
proaches to vector-space lexical semantics may
yield more significant performance gains. Towards
this end we are currently evaluating two classes of
approaches for setting pruning parameters per-word
instead of globally: (1) subspace clustering, i.e.
unsupervised feature selection (e.g., Parsons et al,
2004) and (2) multiple clustering, i.e. finding fea-
ture partitions that lead to disparate clusterings (e.g.,
Shafto et al, 2006).
9 Conclusions
This paper introduced a simple probabilistic model
of tiered clustering inspired by feature selective
clustering that leverages feature exchangeability to
allocate data features between a clustering model
and shared component. The ability to model back-
ground variation, or shared structure, is shown to be
beneficial for modeling words with high polysemy,
yielding increased correlation with human similarity
judgements modeling word relatedness and selec-
tional preference. Furthermore, the tiered clustering
model is shown to significantly outperform related
models, yielding qualitatively more precise clusters.
Acknowledgments
Thanks to Yinon Bentor and Bryan Silverthorn for
many illuminating discussions. This work was sup-
ported by an NSF Graduate Research Fellowship to
the first author, and a Google Research Award.
A Collapsed Gibbs Sampler
In order to sample efficiently from this model, we
leverage the Chinese Restaurant Process represen-
tation of the DP (cf., Aldous, 1985), introducing a
per-word-occurrence cluster indicator cd. Word oc-
currence features are then drawn from a combination
of a single cluster component indicated by cd and the
background topic.
By exploiting conjugacy, the latent variables ?, ?
and ?d can be integrated out, yielding an efficient
1180
collapsed Gibbs sampler. The likelihood of word
occurrence d is given by
P pwd|z, cd,?q 
?
i
P pwi,d|?cdq
?pzd,i0qP pwi,d|?noiseq
?pzd,i1q.
Hence, this model can be viewed as a two-topic
variant of LDA with the addition of a per-word-
occurrence (i.e. document) cluster indicator.7 The
update rule for the latent tier indicator z is similar
to the update rule for 2-topic LDA, with the back-
ground component as the first topic and the second
topic being determined by the per-word-occurrence
cluster indicator c.
We can efficiently approximate ppz|wq via Gibbs
sampling, which requires the complete conditional
posteriors for all zi,d. These are
P pzi,d  t|z
pi,dq,w, ?, ?q 
n
pwi,dq
t   ?
?
wpn
pwq
t   ?q
npdqt   ?
?
jpn
pdq
j   ?q
.
where z
pi,dq is shorthand for the set ztzi,du, n
pwq
t
is the number of occurrences of wordw in topic t not
counting wi,d and n
pdq
t is the number of features in
occurrence d assigned to topic t, not counting wi,d.
Likewise sampling the cluster indicators condi-
tioned on the data ppcd|w, cd, ?, ?q decomposes
into the DP posterior over cluster assignments
and the cluster-conditional Multinomial-Dirichlet
word-occurrence likelihood ppcd|w, cd, ?, ?q 
ppcd|cd, ?qppwd|wd, c, z, ?q given by
P pcd  kold|cd, ?, ?q9

mpdqk
mpdq

  ?

looooooomooooooon
ppcd|cd,?q
Cp? ??n pdqk  
??n pdq

qq
Cp? ??n pdqk q
loooooooooooooomoooooooooooooon
ppwd|wd,c,z,?q
P pcd  knew|cd, ?, ?q9
?
mpdq

  ?
Cp? ??n pdq

q
Cp?q
where mpdqk is the number of occurrences as-
signed to k not including d, ??n pdqk is the vector of
counts of words from occurrence wd assigned to
7Effectively, the tiered clustering model is a special case of
the nested Chinese Restaurant Process with the tree depth fixed
to two (Blei et al, 2003).
cluster k (i.e. words with zi,d  0) and Cpq is
the normalizing constant for the Dirichlet Cpaq 
?p
?m
j1 ajq
1?m
j1 ?pajq operating over vectors
of counts a.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and Wordnet-based approaches. In Proc.
of NAACL-HLT-09, pages 19?27.
David J. Aldous. 1985. Exchangeability and related
topics. In E?cole d?e?te? de probabilite?s de Saint-
Flour, XIII?1983, volume 1117, pages 1?198.
Springer, Berlin.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2003. Hierarchical topic
models and the nested Chinese restaurant process.
In Proc. NIPS-2003.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
James Richard Curran. 2004. From Distributional
to Semantic Similarity. Ph.D. thesis, University
of Edinburgh. College of Science.
Katrin Erk and Sebastian Pado. 2008. A structured
vector space model for word meaning in context.
In Proceedings of EMNLP 2008.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database and Some of its Ap-
plications. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW 2001.
Evgeniy Gabrilovich and Shaul Markovitch.
2007. Computing semantic relatedness using
Wikipedia-based explicit semantic analysis. In
Proc. of IJCAI-07, pages 1606?1611.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Lin-
guistics, 28(3):245?288.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Proc.
of ACL 2006.
1181
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:2007.
Amac? Herdag?delen and Marco Baroni. 2009. Bag-
pack: A general framework to represent semantic
relations. In Proc. of GEMS 2009.
Donald Hindle and Mats Rooth. 1991. Structural
ambiguity and lexical relations. In Proc. of ACL
1991.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction and repre-
sentation of knowledge. Psychological Review,
104(2):211?240.
Martin H. C. Law, Anil K. Jain, and Ma?rio A. T.
Figueiredo. 2002. Feature selection in mixture-
based clustering. In Proc. of NIPS 2002.
Will Lowe. 2001. Towards a theory of semantic
space. In Proceedings of the 23rd Annual Meeting
of the Cognitive Science Society, pages 576?581.
Xiaojuan Ma, Jordan Boyd-Graber, Sonya S.
Nikolova, and Perry Cook. 2009. Speaking
through pictures: Images vs. icons. In ACM Con-
ference on Computers and Accessibility.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Com-
putational Linguistics, 29(4):639?654.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1?28.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161?199.
Sebastian Pado?, Ulrike Pado?, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plau-
sibility judgements. In Proc. of EMNLP 2007.
Ulrike Pado?. 2007. The Integration of Syntax and Se-
mantic Plausibility in a Wide-Coverage Model of
Sentence Processing. Ph.D. thesis, Saarland Uni-
versity, Saarbru?cken.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski,
and Eduard Hovy. 2007. ISP: Learning inferen-
tial selectional preferences. In In Proceedings of
NAACL 2007.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, Edmonton, Alta., Canada.
Lance Parsons, Ehtesham Haque, and Huan Liu.
2004. Subspace clustering for high dimensional
data: A review. SIGKDD Explor. Newsl., 6(1).
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proc. of ACL 1993.
Carl E. Rasmussen. 2000. The infinite Gaussian
mixture model. In Advances in Neural Informa-
tion Processing Systems. MIT Press.
Joseph Reisinger and Raymond Mooney. 2010.
Multi-prototype vector-space models of word
meaning. In Proc. of NAACL 2010.
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
SIGLEX Workshop on Tagging Text with Lexical
Semantics, pages 52?57. ACL.
Adam N. Sanborn, Thomas L. Griffiths, and
Daniel J. Navarro. 2006. A more rational model
of categorization. In Proceedings of the 28th An-
nual Conference of the Cognitive Science Society.
Hinrich Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?123.
Patrick Shafto, Charles Kemp, Vikash Mansinghka,
Matthew Gordon, and Joshua B. Tenenbaum.
2006. Learning cross-cutting systems of cate-
gories. In Proc. CogSci 2006.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proc. of ACL 2006.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Benjamin Van Durme and Marius Pas?ca. 2008.
Finding cars, goddesses and enzymes:
Parametrizable acquisition of labeled instances
for open-domain information extraction. In Proc.
of AAAI 2008.
Nianwen Xue, Jinying Chen, and Martha Palmer.
2006. Aligning features with sense distinction di-
mensions. In Proc. of COLING/ACL 2006.
1182
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1405?1415,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Cross-Cutting Models of Lexical Semantics
Joseph Reisinger
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Raymond Mooney
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
mooney@cs.utexas.edu
Abstract
Context-dependent word similarity can be
measured over multiple cross-cutting dimen-
sions. For example, lung and breath are sim-
ilar thematically, while authoritative and su-
perficial occur in similar syntactic contexts,
but share little semantic similarity. Both of
these notions of similarity play a role in deter-
mining word meaning, and hence lexical se-
mantic models must take them both into ac-
count. Towards this end, we develop a novel
model, Multi-View Mixture (MVM), that rep-
resents words as multiple overlapping clus-
terings. MVM finds multiple data partitions
based on different subsets of features, sub-
ject to the marginal constraint that feature sub-
sets are distributed according to Latent Dirich-
let Allocation. Intuitively, this constraint fa-
vors feature partitions that have coherent top-
ical semantics. Furthermore, MVM uses soft
feature assignment, hence the contribution of
each data point to each clustering view is vari-
able, isolating the impact of data only to views
where they assign the most features. Through
a series of experiments, we demonstrate the
utility of MVM as an inductive bias for captur-
ing relations between words that are intuitive
to humans, outperforming related models such
as Latent Dirichlet Allocation.
1 Introduction
Humans categorize objects using multiple orthogo-
nal taxonomic systems, where category generaliza-
tion depends critically on what features are relevant
to one particular system. For example, foods can be
organized in terms of their nutritional value (high in
fiber) or situationally (commonly eaten for Thanks-
giving; Shafto et al (2006)). Human knowledge-
bases such as Wikipedia also exhibit such multiple
clustering structure (e.g. people are organized by oc-
cupation or by nationality). The effects of these
overlapping categorization systems manifest them-
selves at the lexical semantic level (Murphy, 2002),
implying that lexicographical word senses and tra-
ditional computational models of word-sense based
on clustering or exemplar activation are too impov-
erished to capture the rich dynamics of word usage.
In this work, we introduce a novel probabilis-
tic clustering method, Multi-View Mixture (MVM),
based on cross-cutting categorization (Shafto et al,
2006) that generalizes traditional vector-space or
distributional models of lexical semantics (Curran,
2004; Pado? and Lapata, 2007; Schu?tze, 1998; Tur-
ney, 2006). Cross-cutting categorization finds multi-
ple feature subsets (categorization systems) that pro-
duce high quality clusterings of the data. For exam-
ple words might be clustered based on their part of
speech, or based on their thematic usage. Context-
dependent variation in word usage can be accounted
for by leveraging multiple latent categorization sys-
tems. In particular, cross-cutting models can be used
to capture both syntagmatic and paradigmatic no-
tions of word relatedness, breaking up word features
into multiple categorization systems and then com-
puting similarity separately for each system.
MVM leverages primitives from Dirichlet-Process
Mixture Models (DPMMs) and Latent Dirichlet Al-
location (LDA). Each clustering (view) in MVM con-
sists of a distribution over features and data and
views are further subdivided into clusters based on a
DPMM. View marginal distributions are determined
by LDA, allowing data features to be distributed over
multiple views, explaining subsets of features.
1405
We evaluate MVM against several other model-
based clustering procedures in a series of human
evaluation tasks, measuring its ability to find mean-
ingful syntagmatic and paradigmatic structure. We
find that MVM finds more semantically and syntac-
tically coherent fine-grained structure, using both
common and rare n-gram contexts.
2 Mixture Modeling and Lexical
Semantics
Distributional, or vector space methods attempt to
model word meaning by embedding words in a com-
mon metric space, whose dimensions are derived
from, e.g., word collocations (Schu?tze, 1998), syn-
tactic relations (Pado? and Lapata, 2007), or latent
semantic spaces (Finkelstein et al, 2001; Landauer
and Dumais, 1997; Turian et al, 2010). The distribu-
tional hypothesis addresses the problem of modeling
word similarity (Curran, 2004; Miller and Charles,
1991; Schu?tze, 1998; Turney, 2006), and can be ex-
tended to selectional preference (Resnik, 1997) and
lexical substitution (McCarthy and Navigli, 2007) as
well. Such methods are highly scalable (Gorman
and Curran, 2006) and have been applied in infor-
mation retrieval (Manning et al, 2008), large-scale
taxonomy induction (Snow et al, 2006), and knowl-
edge acquisition (Van Durme and Pas?ca, 2008).
Vector space models fail to capture the richness
of word meaning since similarity is not a globally
consistent metric. It violates, e.g., the triangle in-
equality: the sum of distances from bat to club and
club to association is less than the distance from bat
to association (Griffiths et al, 2007; Tversky and
Gati, 1982).1 Erk (2007) circumvents this problem
by representing words as multiple exemplars derived
directly from word occurrences and embedded in a
common vector space to capture context-dependent
usage. Likewise Reisinger and Mooney (2010) take
a similar approach using mixture modeling com-
bined with a background variation model to generate
multiple prototype vectors for polysemous words.
Both of these approaches still ultimately embed
all words in a single metric space and hence argue
for globally consistent metrics that capture human
1Similarity also has been shown to violate symmetry (e.g.
people have the intuition that China is more similar to North
Korea than North Korea is to China).
intuitive notions of ?similarity.? Rather than assum-
ing a global metric embedding exists, in this work
we simply leverage the cluster assumption, e.g. that
similar words should appear in the same clusters, in
particular extending it to multiple clusterings. The
cluster assumption is a natural fit for lexical seman-
tics, as partitions can account for metric violations.
The end result is a model capable of representing
multiple, overlapping similarity metrics that result
in disparate valid clusterings leveraging the
Subspace Hypothesis: For any pair of
words, the set of ?active? features govern-
ing their apparent similarity differs. For
example wine and bottle are similar and
wine and vinegar are similar, but it would
not be reasonable to expect that the fea-
tures governing such similarity computa-
tions to overlap much, despite occurring
in similar documents.
MVM can extract multiple competing notions of sim-
ilarity, for example both paradigmatic, or thematic
similarity, and syntagmatic or syntactic similarity, in
addition to more fine grained relations.
3 Multi-View Clustering with MVM
As feature dimensionality increases, the number of
ways the data can exhibit interesting structure goes
up exponentially. Clustering is commonly used to
explain data, but often there are several equally
valid, competing clusterings, keying off of different
subsets of features, especially in high-dimensional
settings such as text mining (Niu et al, 2010). For
example, company websites can be clustered by sec-
tor or by geographic location, with one particular
clustering becoming predominant when a majority
of features correlate with it. In fact, informative fea-
tures in one clustering may be noise in another, e.g.
the occurrence of CEO is not necessarily discrimi-
native when clustering companies by industry sec-
tor, but may be useful in other clusterings. Multi-
ple clustering is one approach to inferring feature
subspaces that lead to high quality data partitions.
Multiple clustering also improves the flexibility of
generative clustering models, as a single model is
no longer required to explain all the variance in the
feature dimensions (Mansinghka et al, 2009).
1406
exceedingly
sincerely
logically
justly
appropriately
unwilling
willing
reluctant
refusing
glad
about
because
and are ___
which was ___
who are ___
and is ___
we are ___
he is ___
toyota
nissan
mercedes
volvo
audi
samsung
panasonic
toshiba
sony
epson
dunlop
yokohama
toyo
uniroyal
michelin
results for ___
the latest ___
to buy ___
brand new ___
selection of ___
___ for sale
Figure 1: Example clusterings from MVM applied to
Google n-gram data. Top contexts (features) for each
view are shown, along with examples of word clusters.
Although these particular examples are interpretable, in
general the relationship captured by the view?s context
subspace is not easily summarized.
MVM is a multinomial-Dirichlet multiple clus-
tering procedure for distributional lexical seman-
tics that fits multiple, overlapping Dirichlet Process
Mixture Models (DPMM) to a set of word data. Fea-
tures are distributed across the set of clusterings
(views) using LDA, and each DPMM is fit using a
subset of the features. This reduces clustering noise
and allows MVM to capture multiple ways in which
the data can be partitioned. Figure 1 shows a sim-
ple example, and Figure 2 shows a larger sample of
feature-view assignments from a 3-view MVM fit to
contexts drawn from the Google n-gram corpus.
We implement MVM using generative model
primitives drawn from Latent Dirichlet Allocation
(LDA) and the Dirichlet Process (DP). |M | disparate
clusterings (views) are inferred jointly from a set of
data D  twd|d P r1 . . . Dsu. Each data vector
wd is associated with a probability distribution over
views ?|M |d . Empirically, ?|M |d is represented as aset of feature-view assignments zd, sampled via the
standard LDA collapsed Gibbs sampler. Hence, each
view maintains a separate distribution over features.
The generative model for feature-view assignment is
given by
?|M |d |?  Dirichletp?q, d P D,
?m|?  Dirichletp?q, m P |M |,
zdn|?d  Discretep?dq, n P |wd|,
wdn|?zdnm  Discretep?zdnmq, n P |wd|,
where ? and ? are hyperparameters smoothing the
per-document topic distributions and per-topic word
distributions respectively.
Conditional on the feature-view assignment tzu,
a clustering is inferred for each view using the Chi-
nese Restaurant Process representation of the DP.
The clustering probability is given by
ppc|z,wq 9 pptcmu, z,wq

M
?
m1
|D|
?
d1
ppwrzmsd |cm, zqppcm|zq.
where ppcm|zq is a prior on the clustering for view
m, i.e. the DPMM, and ppwrzmsd |cm, zq is the like-lihood of the clustering cm given the data point wd
restricted to the features assigned to view m:
wrzmsd
def
 twid|zid  mu.
Thus, we treat them clusterings cm as conditionally
independent given the feature-view assignments.
The feature-view assignments tzu act as a set of
marginal constraints on the multiple clusterings, and
the impact that each data point can have on each
clustering is limited by the number of features as-
signed to it. For example, in a two-view model,
zid  1 might be set for all syntactic features (yield-
ing a syntagmatic clustering) while zid  2 is set for
document features (paradigmatic clustering).
By allowing the clustering model capacity to vary
via the DPMM, MVM can naturally account for the
semantic variance of the view. This provides a novel
mechanism for handling feature noise: noisy fea-
tures can be assigned to a separate view with poten-
tially a small number of clusters. This phenomenon
is apparent in cluster 1, view 1 in the example in
figure 2, where place names and adjectives are clus-
tered together using rare contexts
From a topic modeling perspective, MVM finds
topic refinements within each view, similar to hier-
archical methods such as the nested Chinese Restau-
rant Process (Blei et al, 2003). The main differ-
ence is that the features assigned to the second ?re-
fined topics? level are constrained by the higher
1407
word
context
___ hom
e page
___ open this result in
___ who had
a kind of ___
along the ___
and ___ their
are ___ to
be ___ to
but the ___ of
he is ___
in these ___
is an ___
m
any ___ and
m
ight be ___
of ___ have
of being ___
posts by ___
that ___ are
that was ___
the ___ fam
ily
the ___ m
ust be
the ___ of that
the am
erican ___
the very ___
were not ___
who are ___
___ som
e of
a m
ore ___
also ___ the
and ___ his
and are ___
and is ___
and was ___
as ___ as
be ___ or
been ___ and
could be ___
his ___ of
i was ___
is also ___
near the ___
of a ___ and
of the ___ were
she was ___
so m
any ___
the m
ore ___
to be ___ and
was ___ to
we are ___
were ___ in
which ___ the
which was ___
who is ___
you are ___
do not ___
___ high school
___ said that
___ was born
an ___ and
born in ___
by ___ on
by ___ to
create a ___
degree of ___
dsl ___ dsl
from
 the ___ to
going to ___
hotels in ___
in ___ the
in an ___
like ___ and
located in ___
m
essage to ___
nam
e of ___
posted by ___ at
presence of ___
private m
essage to ___
the ___ does not
the city of ___
to ___ a
town of ___
was the ___ of
welcom
e to ___
city of ___
estate in ___
hotels ___ hotels
of ___ m
ay
real estate in ___
way of ___
written by ___
and an ___
of ___ from
 the
the little ___
___ of hum
an
first ___ of
side of the ___
to an ___
0?0
arbitrary
austin
baltimore
characteristic
comparative
dallas
evolutionary
franklin
fundamental
inadequate
inferior
integral
jackson
kent
likelihood
liverpool
mystical
newcastle
pittsburgh
poetic
proportional
psychological
radical
richmond
singular
0?10
betrayed
conquered
disappointed
divorced
embarked
frustrated
guarded
hated
knocked
murdered
praised
stationed
stole
summoned
wounded
0?77secretly
1?0
arbitrary
betrayed
characteristic
conquered
disappointed
divorced
embarked
evolutionary
examine
franklin
frustrated
fundamental
guarded
hated
inadequate
inferior
integral
jackson
knocked
likelihood
murdered
mystical
poetic
praised
proportional
radical
secretly
singular
stationed
stole
summoned
systematic
wounded
1?34
kent
liverpool
manchester
newcastle
1?94
austin
baltimore
charlotte
dallas
pittsburgh
richmond
2?0
austin
betrayed
charlotte
conquered
disappointed
divorced
embarked
frustrated
guarded
hated
jackson
kent
knocked
murdered
newcastle
praised
richmond
secretly
stationed
stole
summoned
wounded
2?47
arbitrary
characteristic
comparative
evolutionary
fundamental
inadequate
inferior
integral
mystical
poetic
psychological
radical
singular
systematic
V
i
e
w
 
1
C
l
u
s
t
e
r
 
1
C
l
u
s
t
e
r
 
2
V
i
e
w
 
2
C
l
u
s
t
e
r
 
1
V
i
e
w
 
3
C
l
u
s
t
e
r
 
1
C
l
u
s
t
e
r
 
2
Figure 2: Topics with Senses: Shows top 20% of features for each view in a 3-view MVM fit to Google n-gram context
data; different views place different mass on different sets of features. Cluster groupings within each view are shown.
View 1 cluster 2 and View 3 cluster 1 both contain past-tense verbs, but only overlap on a subset of syntactic features.1408
level, similar to hierarchical clustering. Unlike hi-
erarchical clustering, however, the top level top-
ics/views form an admixture, allowing individual
features from a single data point to be assigned to
multiple views.
The most similar model to ours is Cross-cutting
categorization (CCC), which fits multiple DPMMs to
non-overlapping partitions of features (Mansinghka
et al, 2009; Shafto et al, 2006). Unlike MVM,
CCC partitions features among multiple DPMMs,
hence all occurrences of a particular feature will
end up in a single clustering, instead of assigning
them softly using LDA. Such hard feature partition-
ing does not admit an efficient sampling procedure,
and hence Shafto et al (2006) rely on Metropolis-
Hastings steps to perform feature assignment, mak-
ing the model less scalable.
3.1 Word Representation
MVM is trained as a lexical semantic model on
Web-scale n-gram and semantic context data. N-
gram contexts are drawn from a combination of the
Google n-gram and Google books n-gram corpora,
with the head word removed: e.g. for the term ar-
chitect, we collect contexts such as the of the
house, an is a, and the of the universe. Se-
mantic contexts are derived from word occurrence
in Wikipedia documents: each document a word ap-
pears in is added as a potential feature for that word.
This co-occurrence matrix is the transpose of the
standard bag-of-words document representation.
In this paper we focus on two representations:
1. Syntax-only ? Words are represented as bags
of ngram contexts derived slot-filling procedure
described above.
2. Syntax+Documents ? The syntax-only repre-
sentation is augmented with additional docu-
ment contexts drawn from Wikipedia.
Models trained on the syntax-only set are only ca-
pable of capturing syntagmatic similarity relations,
that is, words that tend to appear in similar contexts.
In contrast, the syntax+documents set broadens the
scope of modelable similarity relations, allowing for
paradigmatic similarity (e.g. words that are topically
related, but do not necessarily share common syntac-
tic contexts).
Given such word representation data, MVM gener-
ates a fixed set of M context views corresponding to
dominant eigenvectors in local syntactic or seman-
tic space. Within each view, MVM partitions words
into clusters based on each word?s local representa-
tion in that view; that is, based on the set of con-
text features it allocates to the view. Words have a
non-uniform affinity for each view, and hence may
not be present in every clustering (Figure 2). This
is important as different ways of drawing distinc-
tions between words do not necessarily apply to all
words. In contrast, LDA finds locally consistent col-
lections of contexts but does not further subdivide
words into clusters given that set of contexts. Hence,
it may miss more fine-grained structure, even with
increased model complexity.
4 Experimental Setup
4.1 Corpora
We derive word features from three corpora: (1) the
English Google Web n-gram corpus, containing n-
gram contexts up to 5-gram that occur more than 40
times in a 1T word corpus of Web text, (2) the En-
glish Google Books n-gram corpus2, consisting of
n-gram contexts up to 5-gram that occur more than
40 times in a 500B word corpus of books, and (3) a
snapshot of the English Wikipedia3 taken on Octo-
ber 11, 2010 containing over 3M articles.
MVM is trained on a sample of 20k English words
drawn uniformly at random from the top 200k En-
glish terms appearing in Wikipedia (different parts
of speech were sampled from the Google n-gram
corpus according to their observed frequency). Two
versions of the syntax-only dataset are created from
different subsets of the Google n-gram corpora: (1)
the common subset contains all syntactic contexts
appearing more than 200 times in the combined cor-
pus, and (2) the rare subset, containing only contexts
that appear 50 times or fewer.
4.2 Human Evaluation
Our main goal in this work is to find models that
capture aspects of the syntactic and semantic orga-
nization of word in text that are intuitive to humans.
2http://ngrams.googlelabs.com/datasets
3http://wikipedia.org
1409
Context Intrusion
is characterized top of the country to
symptoms of of understood or less
cases of along the a year
in cases of portion of the per day
real estate in side of the or more
Word Intrusion
metal dues humor
floral premiums ingenuity
nylon pensions advertisers
what did delight
ruby damages astonishment
Document Intrusion
Puerto Rican cuisine Adolf Hitler History of the Han Dynasty
Greek cuisine List of General Hospital characters Romance of the Three Kingdoms
ThinkPad History of France List of dog diseases
Palestinian cuisine Joachim von Ribbentrop Conquest of Wu by Jin
Field ration World War I Mongolia
Table 1: Example questions from the three intrusion tasks, in order of difficulty (left to right, easy to hard; computed
from inter-annotator agreement). Italics show intruder items.
According to the use theory of meaning, lexical se-
mantic knowledge is equivalent to knowing the con-
texts that words appear in, and hence being able to
form reasonable hypotheses about the relatedness of
syntactic contexts.
Vector space models are commonly evaluated by
comparing their similarity predictions to a nom-
inal set of human similarity judgments (Curran,
2004; Pado? and Lapata, 2007; Schu?tze, 1998; Tur-
ney, 2006). In this work, since we are evaluating
models that potentially yield many different simi-
larity scores, we take a different approach, scoring
clusters on their semantic and syntactic coherence
using a set intrusion task (Chang et al, 2009).
In set intrusion, human raters are shown a set of
options from a coherent group and asked to identify
a single intruder drawn from a different group. We
extend intrusion to three different lexical semantic
tasks: (1) context intrusion, where the top contexts
from each cluster are used, (3) document intrusion,
where the top document contexts from each clus-
ter are used, and (2) word intrusion, where the top
words from each cluster are used. For each clus-
ter, the top four contexts/words are selected and ap-
pended with another context/word from a different
cluster.4 The resulting set is then shuffled, and the
human raters are asked to identify the intruder, af-
4Choosing four elements from the cluster uniformly at ran-
dom instead of the top by probability led to lower performance
across all models.
ter being given a short introduction (with common
examples) to the task. Table 1 shows sample ques-
tions of varying degrees of difficulty. As the seman-
tic coherence and distinctness from other clusters in-
creases, this task becomes easier.
Set intrusion is a more robust way to account for
human similarity judgments than asking directly for
a numeric score (e.g., the Miller and Charles (1991)
set) as less calibration is required across raters. Fur-
thermore, the additional cluster context significantly
reduces the variability of responses.
Human raters were recruited from Amazon?s Me-
chanical Turk. A total of 1256 raters completed
30438 evaluations for 5780 unique intrusion tasks
(5 evaluations per task). 2736 potentially fraudulent
evaluations from 11 raters were rejected.5 Table 3
summarizes inter-annotator agreement. Overall we
found ?  0.4 for most tasks; a set of comments
about the task difficulty is given in Table 2, drawn
from an anonymous public message board.
5 Results
We trained DPMM, LDA and MVM models
on the syntax-only and syntax+documents
data across a wide range of settings for M P
t3, 5, 7, 10, 20, 30, 50, 100, 200, 300, 500, 1000u,6
5(Rater Quality) Fraudulent Turkers were identified using
a combination of average answer time, answer entropy, average
agreement with other raters, and adjusted answer accuracy.
6LDA is run on a different range of M settings from MVM
(50-1000 vs 3-100) in order to keep the effective number of
1410
% correct
MVM?100M?0.1?0.01
MVM?50M?0.1?0.01
MVM?30M?0.1?0.01
MVM?20M?0.1?0.01
MVM?10M?0.1?0.005
MVM?10M?0.1?0.01
MVM?5M?0.1?0.005
MVM?5M?0.1?0.01
MVM?3M?0.1?0.01
  
LDA?1000M?0.1?0.01
LDA?1000M?0.1?0.1
LDA?500M?0.1?0.01
LDA?500M?0.1?0.1
LDA?300M?0.1?0.01
LDA?300M?0.1?0.1
LDA?200M?0.1?0.01
LDA?200M?0.1?0.1
LDA?100M?0.1?0.01
LDA?100M?0.1?0.1
LDA?50M?0.1?0.01
LDA?50M?0.1?0.1
 
DPMM?0.1?0.01
DPMM?0.1?0.1
context intrusion
ll
0.0 0.2 0.4 0.6 0.8 1.0
word intrusion
ll ll
l l
ll l
l l ll
0.0 0.2 0.4 0.6 0.8 1.0
(a) Syntax-only, common n-gram contexts.
% correct
MVM?100M?0.1?0.01
MVM?50M?0.1?0.01
MVM?30M?0.1?0.01
MVM?20M?0.1?0.01
MVM?10M?0.1?0.005
MVM?10M?0.1?0.01
MVM?5M?0.1?0.005
MVM?5M?0.1?0.01
MVM?3M?0.1?0.01
  
LDA?1000M?0.1?0.01
LDA?1000M?0.1?0.1
LDA?500M?0.1?0.01
LDA?500M?0.1?0.1
LDA?300M?0.1?0.01
LDA?300M?0.1?0.1
LDA?200M?0.1?0.01
LDA?200M?0.1?0.1
LDA?100M?0.1?0.01
LDA?100M?0.1?0.1
LDA?50M?0.1?0.01
LDA?50M?0.1?0.1
 
DPMM?0.1?0.01
DPMM?0.1?0.1
context intrusion
lll
lll
0.0 0.2 0.4 0.6 0.8 1.0
word intrusion
l
ll l
ll
ll
0.0 0.2 0.4 0.6 0.8 1.0
(b) Syntax-only, rare n-gram contexts.
% correct
MVM?100M?0.1?0.01
MVM?50M?0.1?0.01
MVM?30M?0.1?0.01
MVM?20M?0.1?0.01
MVM?10M?0.1?0.005
MVM?10M?0.1?0.01
MVM?5M?0.1?0.005
MVM?5M?0.1?0.01
MVM?3M?0.1?0.01
  
LDA?1000M?0.1?0.01
LDA?1000M?0.1?0.1
LDA?500M?0.1?0.01
LDA?500M?0.1?0.1
LDA?300M?0.1?0.01
LDA?300M?0.1?0.1
LDA?200M?0.1?0.01
LDA?200M?0.1?0.1
LDA?100M?0.1?0.01
LDA?100M?0.1?0.1
LDA?50M?0.1?0.01
LDA?50M?0.1?0.1
 
DPMM?0.1?0.01
DPMM?0.1?0.1
context intrusion
l l
l
l
0.0 0.2 0.4 0.6 0.8 1.0
document intrusion
l ll
ll
0.0 0.2 0.4 0.6 0.8 1.0
word intrusion
l
l ll
l l l
l
l ll
l
lll l
0.0 0.2 0.4 0.6 0.8 1.0
(c) Syntax+Documents, common n-gram contexts.
Figure 3: Average scores for each model broken down by parameterization and data source. Error bars depict 95%
confidence intervals. X-axis labels show Model-views-?-?. Dots show average rater scores; bar-charts show standard
quantile ranges and median score. 1411
U1 I just tried 30 of the what doesn?t belong ones.
They took about 30 seconds each due to think-
ing time so not worth it for me.
U2 I don?t understand the fill in the blank ones to
be honest. I just kinda pick one,since I don?t
know what?s expected lol
U3 Your not filling in the blank just ignore the
blank and think about how the words they show
relate to each other and choose the one that
relates least. Some have just words and no
blanks.
U4 These seem very subjective to mw. i hope
there isn?t definite correct answers because
some of them make me go [emoticon of head-
scratching]
U5 I looked and have no idea. I guess I?m a word
idiot because I don?t see the relation between
the words in the preview HIT - too scared to try
any of these.
U6 I didn?t dive in but I did more than I should have
they were just too easy. Most of them I could
tell what did not belong, some were pretty iffy
though.
Table 2: Sample of comments about the task taken verba-
tim from a public Mechanical Turk user message board
(TurkerNation). Overall the raters report the task to be
difficult, but engaging.
? P t0.1, 0.01u, and ? P t0.1, 0.05, 0.01u in
order to understand how they perform relatively
on the intrusion tasks and also how sensitive they
are to various parameter settings.7 Models were
run until convergence, defined as no increase in
log-likelihood on the training set for 100 Gibbs
samples. Average runtimes varied from a few hours
to a few days, depending on the number of clusters
or topics. There is little computational overhead
for MVM compared to LDA or DPMM with a similar
number of clusters.
Overall, MVM significantly outperforms both LDA
and DPMM (measured as % of intruders correctly
identified) as the number of clusters increases.
Coarse-grained lexical semantic distinctions are
easy for humans to make, and hence models with
fewer clusters tend to outperform models with more
clusters. Since high granularity predictions are more
clusters (and hence model capacity) roughly comparable.
7We did not compare directly to Cross-cutting categoriza-
tion, as the Metropolis-Hasting steps required that model were
too prohibitively expensive to scale to the Google n-gram data.
model size (clusters)
% 
co
rre
ct 0.0
0.5
1.0
0.0
0.5
1.0
l
l
l l
ll l
ll
ll
l
l ll ll
ll
l
ll
ll ll l
ll
l
lll
ll l
102 102.5 103
context intrusion
w
ord intrusion
(a) Syntax-only, common n-gram contexts.
model size (clusters)
% 
co
rre
ct 0.0
0.5
1.0
0.0
0.5
1.0
l
lll ll
l l l l
l
l ll l
l
l
l
l
l
l l
l
l ll
lll ll l
l l
l
101.8 102 102.2 102.4 102.6 102.8 103 103.2
context intrusion
w
ord intrusion
(b) Syntax-only, rare n-gram contexts.
Figure 4: Scatterplot of model size vs. avg score for MVM
(dashed, purple) and LDA (dotted, orange).
useful for downstream tasks, we focus on the inter-
play between model complexity and performance.
5.1 Syntax-only Model
For common n-gram context features, MVM perfor-
mance is significantly less variable than LDA on both
the word intrusion and context intrusion tasks, and
furthermore significantly outperforms DPMM (Fig-
ure 3(a)). For context intrusion, DPMM, LDA, and
MVM average 57.4%, 49.5% and 64.5% accuracy
respectively; for word intrusion, DPMM, LDA, and
MVM average 66.7%, 66.1% and 73.6% accuracy
respectively (averaged over all parameter settings).
These models vary significantly in the average num-
ber of clusters used: 373.5 for DPMM, 358.3 for LDA
and 639.8 for MVM, i.e. the MVM model is signifi-
1412
Model Syntax Syntax+Documents Overall
DPMM 0.30 0.40 0.33
LDA 0.33 0.39 0.35
MVM 0.44 0.49 0.46
Overall 0.37 0.43 0.39
Table 3: Fleiss? ? scores for various model and data com-
binations. Results from MVM have higher ? scores than
LDA or DPMM; likewise Syntax+Documents data yields
higher agreement, primarily due to the relative ease of the
document intrusion task.
cantly more granular. Figure 4(a) breaks out model
performance by model complexity, demonstrating
that MVM has a significant edge over LDA as model
complexity increases.
For rare n-gram contexts, we obtain similar re-
sults, with MVM scores being less variable across
model parameterizations and complexity (Figure
3(b)). In general, LDA performance degrades faster
as model complexity increases for rare contexts, due
to the increased data sparsity (Figure 4(b)). For
context intrusion, DPMM, LDA, and MVM average
45.9%, 36.1% and 50.9% accuracy respectively;
for word intrusion, DPMM, LDA, and MVM aver-
age 67.4%, 45.6% and 67.9% accuracy; MVM per-
formance does not differ significantly from DPMM,
but both outperform LDA. Average cluster sizes are
more uniform across model types for rare contexts:
384.0 for DPMM, 358.3 for LDA and 391 for MVM.
Human performance on the context intrusion task
is significantly more variable than on the word-
intrusion task, reflecting the additional complexity.
In all models, there is a high correlation between
rater scores and per-cluster likelihood, indicating
that model confidence reflects noise in the data.
5.2 Syntax+Documents Model
With the syntax+documents training set, MVM sig-
nificantly outperforms LDA across a wide range of
model settings. MVM also outperforms DPMM for
word and document intrusion. For context intru-
sion, DPMM, LDA, and MVM average 68.0%, 51.3%
and 66.9% respectively;8 for word intrusion, DPMM,
LDA, and MVM average 56.3%, 64.0% and 74.9%
respectively; for document intrusion, DPMM, LDA,
8High DPMM accuracy is driven by the low number of clus-
ters: 46.5 for DPMM vs. 358.3 for LDA and 725.6 for MVM.
model size (clusters)
% 
co
rre
ct
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
l
ll
l
l
lllll
l
l
l
l l
lll
l l
l
l lll
lll
ll
l
l
ll ll
lll l
l
ll
l
l ll
ll ll
ll
l ll l
l ll
102 102.5 103 103.5
context intrusion
document intrusion
w
ord intrusion
Figure 5: Scatterplot of model size vs. avg score for
MVM (dashed, purple) and LDA (dotted, orange); Syn-
tax+Documents data.
and MVM average 41.5%, 49.7% and 60.6% re-
spectively. Qualitatively, models trained on syn-
tax+document yield a higher degree of paradig-
matic clusters which have intuitive thematic struc-
ture. Performance on document intrusion is sig-
nificantly lower and more variable, reflecting the
higher degree of world knowledge required. As with
the previous data set, performance of MVM mod-
els trained on syntax+documents data degrades less
slowly as the cluster granularity increases (Figure 5).
One interesting question is to what degree MVM
views partition syntax and document features versus
LDA topics. That is, to what degree do the MVM
views capture purely syntagmatic or purely paradig-
matic variation? We measured view entropy for all
three models, treating syntactic features and docu-
ment features as different class labels. MVM with
M  50 views obtained an entropy score of 0.045,
while LDA with M  50 obtained 0.073, and the
best DPMM model 0.082.9 Thus MVM views may in-
deed capture pure syntactic or thematic clusterings.
9The low entropy scores reflect the higher percentage of syn-
tactic contexts overall.
1413
5.3 Discussion
As cluster granularity increases, we find that MVM
accounts for feature noise better than either LDA
or DPMM, yielding more coherent clusters. (Chang
et al, 2009) note that LDA performance degrades
significantly on a related task as the number of top-
ics increases, reflecting the increasing difficulty for
humans in grasping the connection between terms
in the same topic. This suggests that as topics be-
come more ne-grained in models with larger num-
ber of topics, they are less useful for humans. In
this work, we find that although MVM and LDA per-
form similarity on average, MVM clusters are signif-
icantly more interpretable than LDA clusters as the
granularity increases (Figures 4 and 5). We argue
that models capable of making such fine-grained se-
mantic distinctions are more desirable.
The results presented in the previous two sections
hold both for unbiased cluster selection (e.g. where
clusters are drawn uniformly at random from the
model) and when cluster selection is biased based
on model probability (results shown). Biased selec-
tion potentially gives an advantage to MVM, which
generates many more small clusters than either LDA
or DPMM, helping it account for noise.
6 Future Work
Models based on cross-cutting categorization is
a novel approach to lexical semantics and hence
should be evaluated on standard baseline tasks, e.g.
contextual paraphrase or lexical substitution (Mc-
Carthy and Navigli, 2007). Additional areas for fu-
ture work include:
(Latent Relation Modeling) Clusterings formed
from feature partitions in MVM can be viewed as a
form of implicit relation extraction; that is, instead
of relying on explicit surface patterns in text, rela-
tions between words or concepts are identified in-
directly based on common syntactic patterns. For
example, clusterings that divide cities by geography
or clusterings partition adjectives by their polarity.
(Latent Semantic Language Modeling) Genera-
tive models such as MVM can be used to build bet-
ter priors for class-based language modeling (Brown
et al, 1992). The rare n-gram results demonstrate
that MVM is potentially useful for tail contexts; i.e.
inferring tail probabilities from low counts.
(Explicit Feature Selection) In this work we rely on
smoothing to reduce the noise of over-broad extrac-
tion rather than performing feature selection explic-
itly. All of the models in this paper can be combined
with feature selection methods to remove noisy fea-
tures, and it would be particularly interesting to draw
parallels to models of ?clutter? in vision.
(Hierarchical Cross-Categorization) Human con-
cept organization consists of multiple overlapping
local ontologies, similar to the loose ontological
structure of Wikipedia. Furthermore, each ontologi-
cal system has a different set of salient properties. It
would be interesting to extend MVM to model hier-
archy explicitly, and compare against baselines such
as Brown clustering (Brown et al, 1992), the nested
Chinese Restaurant Process (Blei et al, 2003) and
the hierarchical Pachinko Allocation Model (Mimno
et al, 2007).
7 Conclusion
This paper introduced MVM, a novel approach to
modeling lexical semantic organization using mul-
tiple cross-cutting clusterings capable of captur-
ing multiple lexical similarity relations jointly in
the same model. In addition to robustly handling
homonymy and polysemy, MVM naturally captures
both syntagmatic and paradigmatic notions of word
similarity. MVM performs favorably compared to
other generative lexical semantic models on a set of
human evaluations, over a wide range of model set-
tings and textual data sources.
Acknowledgements
We would like to thank the anonymous reviewers for
their extensive comments. This work was supported
by a Google PhD Fellowship to the first author.
References
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2003. Hierarchical topic
models and the nested Chinese restaurant process.
In Proc. NIPS-2003.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18:467?479.
1414
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models.
In NIPS.
James Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edin-
burgh.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proc. of the ACL.
Association for Computer Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW 2001.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Proc.
of ACL 2006.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:2007.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction and repre-
sentation of knowledge. Psychological Review,
104(2):211?240.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
Vikash K. Mansinghka, Eric Jonas, Cap Petschu-
lat, Beau Cronin, Patrick Shafto, and Joshua B.
Tenenbaum. 2009. Cross-categorization: A
method for discovering multiple overlapping clus-
terings. In Proc. of Nonparametric Bayes Work-
shop at NIPS 2009.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical substitu-
tion task. In SemEval ?07: Proceedings of the 4th
International Workshop on Semantic Evaluations.
Association for Computational Linguistics.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Lan-
guage and Cognitive Processes, 6(1):1?28.
David Mimno, Wei Li, and Andrew McCallum.
2007. Mixtures of hierarchical topics with
pachinko allocation. In ICML.
Gregory L. Murphy. 2002. The Big Book of Con-
cepts. The MIT Press.
Donglin Niu, Jennifer G. Dy, and Michael I. Jor-
dan. 2010. Multiple non-redundant spectral
clustering views. In Johannes Fu?rnkranz and
Thorsten Joachims, editors, Proceedings of the
27th International Conference on Machine Learn-
ing (ICML-10), pages 831?838.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics,
33(2):161?199.
Joseph Reisinger and Raymond J. Mooney. 2010.
A mixture model with sharing for lexical seman-
tics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2010).
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In Proceedings of ACL
SIGLEX Workshop on Tagging Text with Lexical
Semantics, pages 52?57. ACL.
Hinrich Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?123.
Patrick Shafto, Charles Kemp, Vikash Mansinghka,
Matthew Gordon, and Joshua B. Tenenbaum.
2006. Learning cross-cutting systems of cate-
gories. In Proc. CogSci 2006.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proc. of ACL 2006.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proc. of
the ACL.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Amos Tversky and Itamar Gati. 1982. Similarity,
separability, and the triangle inequality. Psycho-
logical Review, 89(2):123?154.
Benjamin Van Durme and Marius Pas?ca. 2008.
Finding cars, goddesses and enzymes:
Parametrizable acquisition of labeled instances
for open-domain information extraction. In Proc.
of AAAI 2008.
1415
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 433?444, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised PCFG Induction for Grounded Language Learning
with Highly Ambiguous Supervision
Joohyun Kim Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX 78701, USA
{scimitar,mooney}@cs.utexas.edu
Abstract
?Grounded? language learning employs train-
ing data in the form of sentences paired with
relevant but ambiguous perceptual contexts.
Bo?rschinger et al2011) introduced an ap-
proach to grounded language learning based
on unsupervised PCFG induction. Their ap-
proach works well when each sentence po-
tentially refers to one of a small set of pos-
sible meanings, such as in the sportscasting
task. However, it does not scale to prob-
lems with a large set of potential meanings
for each sentence, such as the navigation in-
struction following task studied by Chen and
Mooney (2011). This paper presents an en-
hancement of the PCFG approach that scales
to such problems with highly-ambiguous su-
pervision. Experimental results on the naviga-
tion task demonstrates the effectiveness of our
approach.
1 Introduction
The ultimate goal of ?grounded? language learning
is to develop computational systems that can acquire
language more like a human child. Given only su-
pervision in the form of sentences paired with rel-
evant but ambiguous perceptual contexts, a system
should learn to interpret and/or generate language
describing situations and events in the world. For
example, systems have learned to commentate sim-
ulated robot soccer games by learning from sample
sportscasts (Chen and Mooney, 2008; Liang et al
2009; Bo?rschinger et al2011), or understand nav-
igation instructions by learning from action traces
produced when following the directions (Chen and
Mooney, 2011; Tellex et al2011).
Bo?rschinger et al2011) recently introduced an
approach to grounded language learning using un-
supervised induction of probabilistic context free
grammars (PCFGs) to learn from ambiguous con-
textual supervision. Their approach first constructs
a large set of production rules from sentences paired
with descriptions of their ambiguous context, and
then trains the parameters of this grammar using
EM. Parsing a novel sentence with this grammar
gives a parse tree which contains the formal mean-
ing representation (MR) for this sentence. This ap-
proach works quite well on the sportscasting task
originally introduced by Chen and Mooney (2008).
In this task, each sentence in a natural-language
commentary describing activity in a simulated robot
soccer game is paired with the small set of actions
observed within the past 5 seconds, one of which
is usually described by the sentence. Even with this
low level of ambiguity in a constrained domain, their
method constructs a PCFG with about 33,000 pro-
ductions. More fundamentally, their approach is re-
stricted to a finite set of potential meaning represen-
tations, and the grammar size grows at least linearly
with the number of possible MRs, which in turn is
inevitably exponential in the number of objects and
actions in the domain.
The navigation task studied by Chen and Mooney
(2011) provides much more ambiguous supervision.
In this task, each instructional sentence is paired
with a formal landmarks plan (represented as a
large graph) that includes a full description of the
observed actions and world-states that result when
433
someone follows this instruction. An instruction
generally refers to a subgraph of this large graph.
Therefore, there are a combinatorial number of pos-
sible meanings to which a given sentence can refer.
Chen and Mooney (2011) circumvent this combi-
natorial problem by never explicitly enumerating the
exponential number of potential meanings for each
sentence. Their system first induces a semantic lex-
icon that maps words and short phrases to formal
representations of actions and objects in the world.
This lexicon is learned by finding words and phrases
whose occurrence highly correlates with specific ob-
served actions and objects in the simulated environ-
ment when executing the corresponding instruction.
This learned lexicon is then used to directly infer
a formal MR for observed instructional sentences
using a greedy covering algorithm. These inferred
MRs are then used to train a supervised semantic
parser capable of mapping novel sentences to their
formal meanings.
We present a novel enhancement of Bo?rschinger
et al PCFG approach that uses Chen and Mooney?s
lexicon learner to avoid a combinatorial explosion in
the number of productions. The learned lexicon is
first used to build a hierarchy of semantic lexemes
(i.e. lexicon entries) called the Lexeme Hierarchy
Graph (LHG) for each ambiguous landmarks plan
in the training data. The intuition behind utilizing
an LHG is that the MR for each lexeme constitutes a
semantic concept that corresponds to some natural-
language (NL) word or phrase. Therefore, the LHG
represents how complex semantic concepts are com-
posed of simpler semantic concepts and ultimately
connected to NL words and phrases. Bo?rschinger
et al approach instead produces NL groundings at
the level of atomic MR constituents, which causes
an explosion in the number of PCFG productions
for complex MR languages. We estimated that
Bo?rschinger et al approach would require more
than 20! (> 1018) productions for our navigation
problem.1 On the other hand, our method, which
uses correspondences from the LHG at the seman-
tic concept level, constructs a more focused PCFG
of tractable size. It then extracts the MR for a novel
1The corpus contains quite a few examples with landmarks
plans containing more than 20 actions. This results in at least
20! permutations representing possible alignments between ac-
tions and NL words.
sentence from the most-probable parse tree for the
resulting PCFG. Our approach can produce a large,
combinatorial number of different MRs for a wide
range of novel sentences by composing relevant MR
components from the resulting parse tree, whereas
Bo?rschinger et al approach is only able to output
MRs that are explicitly included as a nonterminals
in the original learned PCFG.
The remainder of the paper is organized as fol-
lows. Section 2 reviews Bo?rschinger et al PCFG
approach as well as the navigation task and data.
Section 3 describes our enhanced PCFG approach
and Section 4 presents an experimental evaluation
of it. Then, Section 5 discusses the unique aspects
of our approach and Section 6 describes additional
related work. Finally, Section 7 presents future re-
search directions and Section 8 gives our conclu-
sions.
2 Background
2.1 Existing PCFG Approach
Our approach extends that of Bo?rschinger et al
(2011), which in turn was inspired by a series of
previous techniques (Lu et al2008; Liang et al
2009; Kim and Mooney, 2010) following the idea
of constructing correspondences between NL and
MR in a single probabilistic generative framework.
Particularly, their approach automatically constructs
a PCFG that generates NL sentences from MRs,
which indicates how atomic MR constituents are
probabilistically related to NL words. The nonter-
minals in the grammar correspond to complete MRs,
MR constituents, and NL phrases. The nontermi-
nal for a composite MR generates each of its MR
constituents, and each atomic MR, x, generates an
NL phrase, Phrasex. Each Phrasex then gener-
ates a sequence of Wordx?s for describing x, and
each Wordx can generate each possible word in the
natural language. This allows the system to learn
the words and phrases used to describe each atomic
MR by properly weighting these rules. Figure 1
shows one possible derivation tree for a sample NL-
MR pair and the PCFG rules that are constructed for
it. Once a set of productions are assembled, their
probabilities are learned using the Inside-Outside al-
gorithm. Computing the most probable parse for a
novel sentence with the trained PCFG provides its
434
Figure 1: Derivation tree for the NL/MR pair: THE
PINK GOALIE PASSES THE BALL TO PINK11 /
pass(pink1, pink11). Left side shows PCFG rules
that are added for each stage (full MR to atomic
MRs, and atomic MRs to NL words ).
preferred MR interpretation in the topmost nonter-
minal.
Unfortunately, as discussed earlier, this approach
only works for finite MR languages, and the gram-
mar becomes intractably large even for finite but
complex MRs. It effectively assumes that MRs are
fairly small and includes every possible MR con-
stituent as a nonterminal in the PCFG. This is not
tractable for more complex MRs. Therefore, our ex-
tension incorporates a learned lexicon to constrain
the space of productions, thereby making the size
of the PCFG tractable for complex MRs, and even
giving it the ability to handle infinite MR languages.
Moreover, when processing novel sentences, our ap-
proach can produce a large space of novel MRs that
were not anticipated during training, which is not the
case for Bo?rschinger et al approach.
2.2 Navigation Task and Dataset
We employ the task and data introduced by Chen and
Mooney (2011) whose goal is to interpret and follow
NL navigation instructions in a virtual world. Fig-
ure 2 shows a sample execution path in a particular
virtual world. The challenge is learning to perform
this task by simply observing humans following in-
structions. Formally, given training data of the form
{(e1, a1, w1), . . . , (en, an, wn)}, where ei is an NL
instruction, ai is an observed action sequence, and
wi is the current world state (patterns of floors and
walls, positions of any objects, etc.), we want to pro-
duce the correct actions aj for a novel (ej , wj).
Figure 2: Sample virtual world from Chen and
Mooney (2011) of interconnecting hallways with
different floor and wall patterns and objects indi-
cated by letters (e.g. ?H? for hatrack).
Figure 3: Sample instruction with its constructed
landmarks plan, components in bold compose the
correct plan.
In order to learn, their system infers the intended
formal plan pi (the MR for a sentence) which pro-
duced the action sequence ai from the instruction ei.
However, there is a large space of possible plans for
any given action sequence. Chen and Mooney first
construct a formal landmarks plan, ci, for each ai,
which is a graph representing the context of every
action and the world-state encountered during the
execution of the sequence. The correct plan MR,
pi, is assumed to be a subgraph of ci, and this causes
a combinatorial matching problem between ei and
ci in order to learn the correct meaning of ei among
all the possible subgraphs of ci. The landmarks and
correct plans for a sample instruction are shown in
Figure 3, illustrating the complexity of the MRs.
Instead of directly solving the combinatorial cor-
respondence problem, they first learn a semantic lex-
435
Figure 4: An overview of Chen and Mooney
(2011)?s system. Our method replaces the plan re-
finement and semantic parser parts.
icon that maps words and short phrases to small sub-
graphs representing their inferred meanings from the
(ei, ci) pairs. The lexicon is learned by evaluating
pairs of n-grams, wj , and MR graphs, mj , and scor-
ing them based on how much more likely mj is a
subgraph of the context ci when w occurs in the
corresponding instruction ei. This process is simi-
lar to other ?cross-situational? approaches to learn-
ing word meanings (Siskind, 1996; Thompson and
Mooney, 2003). Then, a plan refinement step esti-
mates pi from ci by greedily selecting high-scoring
lexemes of the form (wj ,mj) whose words and
phrases (wj) cover the instruction ei and introduce
components (mj) from the landmarks plan ci. The
refined plans are used to construct supervised train-
ing data (ei, pi) for a supervised semantic-parser
learner. The trained semantic parser can parse a
novel instruction into a formal plan, which is finally
executed for end-to-end evaluation. Figure 4 illus-
trates the overall system.
As this figure indicates, our new PCFG method
replaces the plan refinement and semantic parser
components in their system with a unified model
that both disambiguates the training data and learns
a semantic parser. We use the landmarks plans and
the learned lexicon produced by Chen and Mooney
(2011) as inputs to our system.2
2In our experiments, we used the top 1,000 lexemes learned
by Chen and Mooney (2011).
3 Our PCFG Approach
Like Bo?rschinger et al2011), our approach learns
a semantic parser directly from ambiguous su-
pervision, specifically NL instructions paired with
their complete landmarks plans as context. Our
method incorporates the semantic lexemes as build-
ing blocks to find correspondences between NL
words and semantic concepts represented by the lex-
eme MRs, instead of building connections between
NL words and every possible MR constituent as in
Bo?rschinger et al approach. Particularly, we uti-
lize the hierarchical subgraph relationships between
the MRs in the learned semantic lexicon to produce
a smaller, more focused set of PCFG rules.3 The
intuition behind our approach is analogous to the hi-
erarchical relations between nonterminals in syntac-
tic parsing, where higher-level categories such as S,
VP, or NP are further divided into smaller categories
such as V, N, or Det, thereby forming a hierarchi-
cal structure. Inspired by this idea, we introduce a
directed acyclic graph called the Lexeme Hierarchy
Graph (LHG) which represents the hierarchical rela-
tionships between lexeme MRs. Since complex lex-
eme MRs represent complicated semantic concepts
while simple MRs represent simple concepts, it is
natural to construct a hierarchy amongst them. The
LHGs for all of the training examples are used to
construct production rules for the PCFG, which are
then parametrized using EM. Finally, a novel sen-
tence is semantically parsed by computing its most-
probable parse using the trained PCFG, and then its
MR is extracted from the resulting parse tree.
3.1 Constructing a Lexeme Hierarchy Graph
An LHG represents the hierarchy of lexical mean-
ings relevant to a particular training instance by en-
coding the subgraph relations between the MRs of
relevant lexemes. Algorithm 1 describes how an
LHG is constructed for an ambiguous training pair
of a sentence and its corresponding context, (ei, ci).
First, we obtain all relevant lexemes (wij ,m
i
j) in the
lexicon L, where the MR mij is a subgraph of the
context ci (denoted as mij ? ci). These lexemes are
3The total number of PCFG rules constructed for our navi-
gation training sets is about 18,000, while Bo?rschinger et al
method produces 33,000 rules for the much simpler sportscast-
ing domain.
436
Algorithm 1 LEXEME HIERARCHY GRAPH (LHG)
Input: Training instance (ei, ci), Lexicon L
Output: Lexeme hierarchy graph for (ei, ci)
Find relevant lexemes (wi1,m
i
1), . . . , (w
i
n,m
i
n)
s.t. mij ? ci
Create a starting node T ; MR(T )? ci
for all mij in the descending order of size do
Create a node T ij ; MR(T
i
j )? m
i
j
PLACELEXEME(T ij ,T )
end for
procedure PLACELEXEME(T ?,T )
for all children Tj of T do
if MR(T ?) ? MR(Tj) then
PLACELEXEME(T ?,Tj)
end if
end for
if T ? was not placed under any child Tj then
Add T ? as child of T
end if
end procedure
sorted in descending order based on the number of
nodes in their MRs mij . Then, after setting the con-
text ci as the MR of the root node (MR(T ) ? ci),
lexemes are inserted, in order, into the graph to cre-
ate a hierarchy of MRs, where each child?s MR is a
subgraph of the MR of each of its parents. Figure 5
illustrates a sample construction of an LHG for the
following landmarks plan (ci):
Turn(RIGHT),
Verify(side:HATRACK, front:SOFA),
Travel(steps:3),
Verify(at:EASEL)
The initial LHG may contain nodes with too many
children. This is a problem, because when we sub-
sequently extract PCFG rules, we need to add a pro-
duction for every k-permutation of the children of
each node (see Section 3.2). To reduce the branch-
ing factor in the LHG, we introduce pseudo-lexeme
nodes by repeatedly combining the two most similar
children of each node. Pseudocode for the process is
shown in Algorithm 2. The MR for a pseudo-lexeme
is the minimal graph, m?, that is a supergraph of both
of the lexeme MRs that it combines. The pair of
(a) All relevant lexemes are obtained for the training exam-
ple and ordered by the number of nodes in their MR.
(b) Lexeme MR [1] is added as a child of the top node. MR
[2] is a subgraph of [1], so it is added as its child.
(c) MR [3] is not a subgraph of [1] or [2], so it is added as a
child of the root. MR [4] is added under [3], and MR [5] is
recursively filtered down and added under [2].
Figure 5: Sample LHG construction.
437
Algorithm 2 ADDING PSEUDO LEXEMES TO LHG
Input: LHG with root T
Output: LHG with pseudo lexemes added
procedure RECONSTRUCTLHG(T )
repeat
((Ti, Tj),m?) ? pick the most similar
pair (Ti, Tj) of children of T and the minimal ex-
tension m? s.t. MR(Ti) ? m?, MR(Tj) ? m?,
m? ? MR(T )
Add child T ? of T ; MR(T ?)? m?
Move Ti and Tj to be children of T ?
until There are no more pairs to combine
for all non-leaf children Tk of T do
RECONSTRUCTLHG(Tk)
end for
end procedure
most similar children, (mi,mj), is determined by
measuring the fraction of the nodes in mi and mj
that overlap with their minimum extension m? and
is calculated as follows:
Sim(mi,mj ,m
?) =
|mi|+ |mj |
2 |m?|
where |m| is the number of nodes in the MR m.
Adding pseudo-lexemes also has another advan-
tage. They can be considered to be higher-level
semantic concepts composed of two or more sub-
concepts. These higher-level concepts will likely
occur in other training examples as well, which al-
lows for more flexible interpretations. For example,
assuming the rule A ? BCD is constructed from
an LHG, we will introduce a pseudo lexeme E and
build two rules A? BE and E ? CD. It is likely
that E also occurs in another rule constructed from
other training examples such as E ? FG. This
increases the model?s expressive power by support-
ing additional derivations such as A?? BFG, pro-
viding more flexibility when parsing novel NL sen-
tences.
3.2 Composing PCFG Rules
The next step composes PCFG rules from the LHGs
and is summarized in Figure 6. We basically fol-
low the scheme of Bo?rschinger et al2011), but
instead of generating NL words from each atomic
MR, words are generated from each lexeme MR,
Figure 6: Summary of the rule generation process.
NLs refer to the set of NL words in the corpus. Lex-
eme rules come from the schemata of Bo?rschinger
et al2011), and allow every lexeme MR to gener-
ate one or more NL words. Note that pseudo-lexeme
nodes do not produce NL words.
and smaller lexeme MRs are generated from more
complex ones as given by the LHGs. A nonterminal
Sm is generated for the MR, m, of each LHG node.
Then, for every LHG node, T , with MR, m, we add
rules of the form Sm ? Smi ...Smj , where the RHS
is some k-permutation of the nonterminals for the
MRs of the children of node T . Bo?rschinger et al
assume that every atomic MR generates at least one
NL word. However, since we do not know which
subgraph of the overall context (i.e. ci, the MR of the
root node) conveys the intended plan and is therefore
expressed in the NL instruction, we must allow each
ordered subset of the children of a node (i.e. each
k-permutation) to be a possible generation.
The rest of the process more closely follows
Bo?rschinger et al. Every MR, m, of a lexeme
node4 generates a rule Sm ? Phrasem, and ev-
ery Phrasem generates a sequence of NL words, in-
cluding one or more ?content words? (Wordm) for
expressing m and zero or more ?extraneous? words
(Word?). While Bo?rschinger et alave Wordm
generate all possible NL words (each of which are
4We exclude pseudo-lexeme nodes in this process, because
they should only generate words through generating lexemes.
438
subsequently weighted by EM training), in our ap-
proach, each Wordm only produces the NL phrase
associated with m in the lexicon, or individual words
that appear in this phrase. The words not covered
by Wordm also can be generated by Word? which
has rules for every word. Phm and PhXm ensure
that Phrasem produces at least one Wordm, where
PhXm indicates that one or more Wordm?s have
already been generated, and Phm indicates that no
Wordm has yet been generated.
3.3 Parsing Novel NL Sentences
To learn the parameters of the resulting PCFG, we
use the Inside-Outside algorithm.5 Then, the stan-
dard probabilistic CKY algorithm is used to produce
the most probable parse for novel NL sentences (Ju-
rafsky and Martin, 2000).
Bo?rschinger et al2011) simply read the MR, m,
for a sentence off the top Sm nonterminal of the
most probable parse tree. However, in our approach,
the correct MR is constructed by properly compos-
ing the appropriate subset of lexeme MRs from the
most-probable parse tree. This allows the system to
produce a wide variety of novel MRs for novel sen-
tences, as long as the correct MR is a subgraph of the
complete context (ci) for at least one of the training
sentences.
First, the parse tree is pruned to remove all sub-
trees starting with Phrasex nodes. This leaves a
tree consisting of the Root and a set of Sm nodes.
The pruned subtrees only concern generating NL
words and phrases from the selected MRs. The re-
maining tree shows which MR constituents were se-
lected from the available context, from which the
sentence is then generated. Each leaf in the pruned
tree represents an MR constituent that was used to
generate a phrase in the sentence. These are the con-
stituents we want to assemble and compose into a
final MR for the sentence.
Algorithm 3 describes the procedure for extract-
ing the final MR from the pruned parse tree. Fig-
ure 7 graphically depicts a sample trace of this algo-
rithm. The algorithm recursively traverses the parse
tree. When a leaf-node is reached, it marks all of the
nodes in its MR. After traversing all of its children,
5We used the implementation available at http://web.
science.mq.edu.au/?mjohnson/Software.htm
which was also used by Bo?rschinger et al2011).
Algorithm 3 CONSTRUCT PARSED MR RESULT
Input: Parse tree T for input NL, e, with all
Phrasex subtrees removed.
Output: Semantic parse MR, m, for e
procedure OBTAINPARSEDOUTPUT(T )
if T is a leaf then
return MR(T ) with all its nodes marked
end if
for all children Ti of T do
mi ? OBTAINPARSEDOUTPUT(Ti)
Mark the nodes in MR(T ) corresponding
to the marked nodes in mi
end for
if T is not the root then
return MR(T )
end if
return MR(T ) with unmarked nodes removed
end procedure
a node in the MR for the current parse-tree node is
marked iff its corresponding node in any of the chil-
dren?s MRs were marked. The final output is the MR
constructed by removing all of the unmarked nodes
from the MR for the root node.
4 Experimental Evaluation
For evaluation, we used the same data and method-
ology as Chen and Mooney (2011). Please see their
paper for more details.
4.1 Data
We used the English instructions and follower data
collected by MacMahon et al2006).6 This data
contains 706 route instructions for three virtual
worlds. The instructions were produced by six in-
structors for 126 unique starting and ending loca-
tion pairs spread evenly across the three worlds, and
there were 1 to 15 human followers for each instruc-
tion who executed an average of 10.4 actions per in-
struction. Each instruction is a paragraph consist-
ing of an average of 5.0 sentences, each contain-
ing an average of 7.8 words. Chen and Mooney
constructed the additional single-sentence corpus by
matching each sentence with the majority of human
6Available at http://www.cs.utexas.edu/users/
ml/clamp/navigation/
439
(a) Pruned parse tree showing only MRs for Sm
nodes
(b) Leaf nodes have all their elements marked
(c) Upper level nodes are marked according to leaf-
node markings
(d) Removing all unmarked elements for the root
node leads to the final MR output
Figure 7: Sample construction of MR output from pruned parse tree.
followers? actions. We use this single-sentence ver-
sion for training, but use both the single-sentence
and the original paragraph version for testing. Each
sentence was manually annotated with a ?gold stan-
dard? execution plan, which is used for evaluation
but not for training.
4.2 Methodology and Results
Experiments were conducted using ?leave one envi-
ronment out? cross-validation, training on two envi-
ronments and testing on the third, averaging over all
three test environments. We perform direct compar-
ison to the best results of Chen and Mooney (2011)
(referred to as CM). A Wilcoxon signed-rank test
is performed for statistical significance, and ??? de-
notes significant differences (p < .01) in the tables.
Semantic Parsing Results
We first evaluated how well our system learns to
map novel NL sentences for new test environments
into their correct MRs. Partial semantic-parsing ac-
curacy (Chen and Mooney, 2011) is calculated by
Precision Recall F1
Our system 87.58 ?65.41 ?74.81
CM ?90.22 55.10 68.37
Table 1: Test accuracy for semantic parsing.
??? denotes difference is statistically significant.
comparing the system?s MR output to the hand-
annotated gold standard. Accuracy is measured in
terms of precision, recall, and F1 for individual MR
constituents (thereby awarding partial credit for ap-
proximately correct MRs).
Table 1 demonstrates that our method outper-
forms CM by 6 points in F1. Our PCFG-based ap-
proach is able to probabilistically disambiguate the
training data as well as simultaneously learn a sta-
tistical semantic parser within a single framework.
This results in better overall performance compared
to CM, since they lose potentially useful informa-
tion, particularly during the refinement stage, due to
the separate disjoint components of the system.
440
Single-sentence Paragraph
Our system ?57.22% ?20.17%
CM 54.40% 16.18%
Table 2: Successful plan execution rates for novel
test data. ??? means statistical significance.
Navigation Plan Execution Results
Next, we test the end-to-end system by execut-
ing the parsed navigation plans for test instructions
in novel environments to see if they reach the ex-
act desired destinations in the environment. Table
2 shows the successful end-to-end navigation-task
completion rate for both single-sentences and com-
plete paragraph instructions.
Again, our system outperforms CM?s best results
since more accurate semantic parsing produces more
successful plans. However, the difference in per-
formance is smaller than that observed for semantic
parsing. This is because the redundancy in the hu-
man generated instructions allows an incorrect se-
mantic parse to be successful, as long as the errors
do not affect its ability to guide the system to the
correct destination.
5 Discussion
Our approach improves on Bo?rschinger et al
(2011)?s method in the following ways:
? The building blocks for associating NL and MR
are semantic lexemes instead of atomic MR con-
stituents. This prevents the number of constructed
PCFG rules from becoming intractably large as hap-
pens with Bo?rschinger et al approach. As previ-
ously mentioned, lexeme MRs are intuitively anal-
ogous to syntactic categories in that complex lex-
eme MRs represent complicated semantic concepts
whereas higher-level syntactic categories such as S,
VP, or NP represent complex syntactic structures.
? Our approach has the ability to produce previ-
ously unseen MRs, whereas Bo?rschinger et alan
only generate an MR if it is explicitly included in
the PCFG rules constructed from the training data.
Even though our MR parse is restricted to be a sub-
graph of some training context, ci, our model allows
for exponentially many combinations.
In addition, our approach can produce a wider
range of MR outputs than Chen and Mooney
(2011)?s even though we use their semantic lexi-
con as input. Their system deterministically builds a
supervised training set by greedily selecting high-
scoring lexemes, thus implicitly including only
high-scoring lexemes during training. On the other
hand, our probabilistic approach also considers rela-
tively low-scoring but useful lexemes, thereby utiliz-
ing more semantic concepts in the lexicon. In partic-
ular, this explains why our approach obtains higher
recall in the evaluation of semantic parsing.
Even though we have demonstrated our approach
on the specific task of following navigation in-
structions, it is straightforward to apply it to other
language-grounding tasks where NL sentences po-
tentially refer to some subset of states, events, or ac-
tions in the world, as long as this overall context can
be represented as a semantic graph or logical form.
Since the semantic lexicon is an input to our system,
other approaches to lexicon learning are also easily
incorporated.
6 Related Work
Most work on learning semantic parsers that map
natural-language sentences to formal representa-
tions of their meaning have relied upon totally su-
pervised training data consisting of NL/MR pairs
(Zelle and Mooney, 1996; Zettlemoyer and Collins,
2005; Kate and Mooney, 2006; Wong and Mooney,
2007; Zettlemoyer and Collins, 2007; Lu et al
2008; Zettlemoyer and Collins, 2009). Several re-
cent approaches have investigated grounded learn-
ing from ambiguous supervision extracted from per-
ceptual context. A number of approaches (Kate and
Mooney, 2007; Chen and Mooney, 2008; Chen et al
2010; Kim and Mooney, 2010; Bo?rschinger et al
2011) assume training data consisting of a set of sen-
tences each associated with a small set of MRs, one
of which is usually the correct meaning of the sen-
tence. Many of these approaches (Kate and Mooney,
2007; Chen and Mooney, 2008; Chen et al2010)
disambiguate the data and match NL sentences to
their correct MR by iteratively retraining a super-
vised semantic parser. Kim and Mooney (2010)
proposed a generative semantic parsing model that
first chooses which MRs to describe and then gen-
erates a hybrid tree structure (Lu et al2008) con-
taining both the MR and NL sentence. They train
441
this model on ambiguous data using EM. As pre-
viously discussed, Bo?rschinger et al2011) use a
PCFG generative model and also train it on ambigu-
ous data using EM. Liang et al2009) assume each
sentence maps to one or more semantic records (i.e.
MRs) and trains a hierarchical semi-Markov genera-
tive model using EM, and then finds a Viterbi align-
ment between NL words and records and their con-
stituents. Several recent projects (Branavan et al
2009; Vogel and Jurafsky, 2010) use NL instructions
to guide reinforcement learning from independent
exploration with delayed rewards. These systems do
not even need the ambiguous supervision obtained
from observing humans follow instructions; how-
ever, they do not learn semantic parsers that map
sentences to complex, structural representations of
their meaning.
Interpreting and executing NL navigation instruc-
tions is our primary task, and several other recent
projects have studied related problems. Shimizu and
Haas (2009) present a system that parses natural lan-
guage instructions into actions. However, they limit
the number of possible actions to only 15 and treat
the problem as a sequence labeling problem that is
solved using a CRF with supervised training. Ma-
tuszek et al2010) developed a system that learns to
map NL instructions to executable commands for a
robot navigating in an environment constructed by a
laser range finder. However, their approach has limi-
tations of ignoring any objects or other landmarks in
the environment to which the instructions can refer.
There are several recent projects (Vogel and Juraf-
sky, 2010; Kollar et al2010; Tellex et al2011)
which learn to follow instructions in more linguisti-
cally complex environments. However, they assume
predefined spatial words, direct matching between
NL words and the names of objects and other land-
marks in the MR, and/or an existing syntactic parser.
By contrast, our work does not assume any prior lin-
guistic knowledge, syntactic, lexical, or semantic,
and must learn the mapping between NL words and
phrases and the MR terms describing landmarks.
7 Future Work
In the future, we would like to develop a better lex-
icon learner since our PCFG approach critically re-
lies on the quality of the learned lexicon. Particu-
larly, we would like to investigate how syntactic in-
formation (such as part-of-speech tags induced us-
ing unsupervised learning) could be used to improve
semantic-lexicon learning. For example, some of the
current lexicon entries violate the general constraint
that nouns usually refer to objects and verbs to ac-
tions. Ideally, the lexicon learner would be able to
induce and then utilize this sort of relationship be-
tween syntax and semantics.
In addition, we want to investigate the use of dis-
criminative reranking (Collins, 2000), which has
proven effective in various other NLP tasks. We
would expect the final MR output to improve if a
discriminative model, which uses additional global
features, is used to rerank the top-k parses produced
by our generative PCFG model.
8 Conclusions
We have presented a novel method for learning a
semantic parser given only highly ambiguous su-
pervision. Our model enhances Bo?rschinger et
al. (2011)?s approach to reducing the problem of
grounded learning of semantic parsers to PCFG in-
duction. We use a learned semantic lexicon to aid
the construction of a smaller and more focused set
of PCFG productions. This allows the approach
to scale to complex MR languages that define a
large (potentially infinite) space of representations
for capturing the meaning of sentences. By contrast,
the previous PCFG approach requires a finite MR
language and its grammar grows intractably large
for even moderately complex MR languages. In ad-
dition, our algorithm for composing MRs from the
final parse tree provides the flexibility to produce a
wide range of novel MRs that were not seen during
training. Evaluations on a previous corpus of nav-
igational instructions for virtual environments has
demonstrated the effectiveness of our method com-
pared to a recent competing system.
Acknowledgments
We thank the anonymous reviewers and David Chen
for useful comments that helped improve this paper.
This work was funded by NSF grants IIS-0712907
and IIS-1016312. Experiments were performed on
the Mastodon Cluster, provided by NSF grant EIA-
0303609.
442
References
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-11), pages 1416?1425, Stroudsburg, PA,
USA. Association for Computational Linguistics.
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing
(ACL-IJCNLP), Singapore.
David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language ac-
quisition. In Proceedings of 25th International Con-
ference on Machine Learning (ICML-2008), Helsinki,
Finland, July.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-11),
San Francisco, CA, USA, August.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML-2000), pages 175?182, Stanford, CA, June.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall, Upper Saddle
River, NJ.
R. J. Kate and R. J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proceedings
of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL-
06), pages 913?920, Sydney, Australia, July.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambiguous supervision.
In Proceedings of the Twenty-Second Conference on
Artificial Intelligence (AAAI-07), pages 895?900, Van-
couver, Canada, July.
Joohyun Kim and Raymond. J. Mooney. 2010. Genera-
tive alignment and semantic parsing for learning from
ambiguous supervision. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING-10), pages 543?551. Association for Com-
putational Linguistics.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Toward understanding natural language
directions. In Proceedings of Human Robot Interac-
tion Conference (HRI-2010).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In Joint
Conference of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), Singapore.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A generative model for parsing natural
language to meaning representations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP-08), pages 783?792,
Morristown, NJ, USA. Association for Computational
Linguistics.
M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006.
Walk the talk: Connecting language, knowledge, and
action in route instructions. In Proceedings of the
Twenty-First National Conference on Artificial Intel-
ligence (AAAI-06), Boston, MA, July.
Cynthia Matuszek, Dieter Fox, and Karl Koscher. 2010.
Following directions using statistical machine transla-
tion. In Proceedings of the 5th ACM/IEEE interna-
tional conference on Human-robot interaction (HRI-
10), pages 251?258, New York, NY, USA. ACM.
Nobuyuki Shimizu and Andrew Haas. 2009. Learning to
follow navigational route instructions. In Proceedings
of the Twenty First International Joint Conference on
Artificial Intelligence (IJCAI-2009).
Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1):39?91, October.
Stefanie Tellex, Thomas Kolla, Steven Dickerson,
Matthew R. Walter, Ashis G. Banerjee, Seth Teller, and
Nicholas Roy. 2011. Understanding natural language
commands for robotic navigation and mobile manipu-
lation. In Proceedings of the National Conference on
Artificial Intelligence (AAAI-11), August.
Cynthia A. Thompson and Raymond J. Mooney. 2003.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence Re-
search, 18:1?44.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL-10).
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
443
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 960?967, Prague, Czech Repub-
lic, June.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In Proceedings of the Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1050?1055, Portland, OR, August.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of 21st Conference on Uncertainty in Ar-
tificial Intelligence (UAI-2005), Edinburgh, Scotland,
July.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL-07), pages 678?687, Prague, Czech
Republic, June.
Luke .S. Zettlemoyer and Micheal Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP (ACL-IJCNLP-09), pages
976?984. Association for Computational Linguistics.
444
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1851?1857,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Detecting Promotional Content in Wikipedia
Shruti Bhosale Heath Vinicombe Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
{shruti,vini,mooney}@cs.utexas.edu
Abstract
This paper presents an approach for detecting
promotional content in Wikipedia. By incor-
porating stylometric features, including fea-
tures based on n-gram and PCFG language
models, we demonstrate improved accuracy
at identifying promotional articles, compared
to using only lexical information and meta-
features.
1 Introduction
Wikipedia is a free, collaboratively edited encyclo-
pedia. Since normally anyone can create and edit
pages, some articles are written in a promotional
tone, violating Wikipedia?s policy requiring a neu-
tral viewpoint. Currently, such articles are identified
manually and tagged with an appropriate Cleanup
message1 by Wikipedia editors. Given the scale and
rate of growth of Wikipedia, it is infeasible to man-
ually identify all such articles. Hence, we present
an approach to automatically detect promotional ar-
ticles.
Related work in quality flaw detection in
Wikipedia (Anderka et al, 2012) has relied on
meta-features based on edit history, Wikipedia links,
structural features and counts of words, sentences
and paragraphs. However, we hypothesize that there
are subtle differences in the linguistic style that dis-
tinguish promotional tone, which we attempt to cap-
ture using stylometric features, particularly deeper
syntactic features. We model the style of promo-
tional and normal articles using language models
1http://en.wikipedia.org/wiki/Wikipedia:
Template_messages/Cleanup
based on both n-grams and Probabilistic Context
Free Grammars (PCFGs). We show that using such
stylometric features improves over using only shal-
low lexical and meta-features.
2 Related Work
Anderka et al (2012) developed a general model for
detecting ten of Wikipedia?s most frequent quality
flaws. One of these flaw types, ?Advert?2, refers to
articles written like advertisements. Their classifiers
were trained using a set of lexical, structural, net-
work and edit-history related features of Wikipedia
articles. However, they used no features capturing
syntactic structure, at a level deeper than Part-Of-
Speech (POS) tags.
A related area is that of vandalism detection in
Wikipedia. Several systems have been developed
to detect vandalizing edits in Wikipedia. These fall
into two major categories: those analyzing author in-
formation and edit metadata (Wilkinson and Huber-
man, 2007; Stein and Hess, 2007); and those using
NLP techniques such as n-gram language models
and PCFGs (Wang and McKeown, 2010; Harpalani
et al, 2011). We combine relevant features from
both these categories to train a classifier that distin-
guishes promotional content from normal Wikipedia
articles.
3 Dataset Collection
We extracted a set of about 13,000 articles from
English Wikipedia?s category, ?Category:All arti-
2?Advert? is the flaw-type of majority of the articles in the
Category ?Articles with a promotional tone?.
1851
Content Features
Number of characters
Number of words
Number of sentences
Average Word Length
Average, Minimum, Maximum Sentence Lengths,
Ratio of Maximum to minimum sentence lengths
Ratio of long sentences (>48 words) to Short Sen-
tences (<33 words)
Percentage of Sentences in the passive voice
Relative Frequencies of POS tags for pronouns, con-
junctions, prepositions, auxiliary verbs, modal verbs,
adjectives and adverbs
Percentage of sentences beginning with a pronoun,
article, conjunction, preposition, adjective, adverb
Percentage of special phrases3 such as peacock
terms (?legendary?, ?acclaimed?, ?world-class?),
weasel terms (?many scholars state?, ?it is be-
lieved/regarded?, ?many are of the opinion?, ?most
feel?, ?experts declare?, ?it is often reported?) , edi-
torializing terms (?without a doubt?, ?of course?, ?es-
sentially?)
Percentage of easy words, difficult words (Dale-
Chall List), long words and stop words
Overall Sentiment Score based on SentiWordNet4
Table 1: Content Features of a Wikipedia Article
cles with a promotional tone? as a set of positive
examples. We extracted a set of 26,000 untagged
articles to form a noisy set of negative examples,
which may contain some promotional articles that
have not yet been tagged by Wikipedia editors. To
counter this noise, we repeated the experiment us-
ing Wikipedia?s Featured Articles and Good Articles
(approx. 11,000) as a set of clean negative exam-
ples. We used 70% of the articles in each category
to train language models for each of the three cate-
gories (promotional articles, featured/good articles,
untagged articles), and used the remaining 30% to
evaluate classifier performance using 10-fold cross-
validation.
4 Features
4.1 Content and Meta Features of an Article
We used the content and meta features proposed by
Anderka et al (2012) as given in Tables 1-4. We also
3http://en.wikipedia.org/wiki/Wikipedia:
Manual_of_Style/Words_to_watch
4This feature is not included in Anderka et al (2012)
Structural Features
Number of Sections
Number of Images
Number of Categories
Number of Wikipedia Templates used
Number of References, Number of References per
sentence and Number of references per section
Table 2: Structural Features of a Wikipedia Article
Wikipedia Network Features
Number of Internal Wikilinks (to other Wikipedia
pages)
Number of External Links (to other websites)
Number of Backlinks (i.e. Number of wikilinks from
other Wikipedia articles to an article)
Number of Language Links (i.e. Number of links to
the same article in other languages)
Table 3: Network Features of a Wikipedia Article
added a new feature, ?Overall Sentiment Score? for
an article. This feature is the average of the senti-
ment scores assigned by SentiWordnet (Baccianella
et al, 2010) to all positive and negative sentiment
bearing words in an article. In total, this results in
58 basic document features.
4.2 N-Gram Language Models
Language models are commonly used to measure
stylistic differences in language usage between au-
thors. For this work, we employed them to model
the difference in style of neutral vs. promotional
Wikipedia articles. We trained trigram word lan-
guage models and trigram character language mod-
els5 with Witten-Bell smoothing to produce proba-
bilistic models of both classes.
4.3 PCFG Language Models
Probabilistic Context Free Grammars (PCFG) cap-
ture the syntactic structure of language by mod-
eling sentence generation using probabilistic CFG
productions. We hypothesize that sentences in pro-
motional articles and those in neutral articles tend
to have different kinds of syntactic structures and
therefore, we explored the utility of PCFG models
for detecting this difference. Since we do not have
ground-truth parse trees for sentences in our dataset,
5Modeling longer character sequences did not help.
1852
Features based on PCFG models and n-gram Language models
Difference in the probabilities assigned to an article by the positive and the negative class character trigram
language models (LM char trigram)
Difference in the probabilities assigned to an article by the positive and the negative class word trigram language
models (LM word trigram)
Difference in the mean values of the probabilities assigned to sentences of an article by the positive and negative
class PCFG models (PCFG mean)
Difference in the maximum values of the probabilities assigned to sentences of an article by the positive and
negative class PCFG models (PCFG max)
Difference in the minimum values of the probabilities assigned to sentences of an article by the positive and
negative class PCFG models (PCFG min)
Difference in the standard deviation values of the probabilities of sentences of an article by the positive and
negative class PCFG models (PCFG std deviation)
Table 5: Features of a Wikipedia Article based on PCFG models and n-gram Language models
Edit History Features
Age of the article
Days since last revision of the article
Number of edits to the article
Number of unique editors
Number of edits made by registered users and by
anonymous IP addresses
Number of edits per editor
Percentage of edits by top 5% of the top contributors
to the article
Table 4: Edit-History Features of a Wikipedia Article
we followed the method of (Raghavan et al, 2010;
Harpalani et al, 2011), which uses the output of
the Stanford parser to train PCFG models for stylis-
tic analysis. We used the PCFG implementation of
Klein and Manning (2003) to learn a PCFG model
for each category.
4.4 Classification
The n-gram and PCFG language models were used
to create a set of additional document features. We
used the probability assigned by the language mod-
els to each sentence in a test document to calculate
document-wide statistics such as the mean, maxi-
mum, and minimum probability and standard devia-
tion in probabilities of the set of sentences in an arti-
cle. The language-modeling features used are shown
in Table 5.
Since we have a wide variety of features, we
experimented with various ensemble learning tech-
niques and found that LogitBoost performed best
empirically. We used the Weka implementation of
LogitBoost (Friedman et al, 2000) to train a classi-
fier using various combinations of features. We used
Decision Stumps as a base classifier and ran boost-
ing for 500 iterations.
5 Experimental Evaluation
5.1 Methodology
We used 10-fold cross-validation to test the perfor-
mance of our classifier using various combinations
of features. We ran the classifier on the portion
(30%) of the dataset not used for language model-
ing.6 We measured overall classification accuracy
as well as precision, recall, F-measure, and area un-
der the ROC curve for all experiments. We tested
performance in two settings (Anderka et al, 2012):
? Pessimistic Setting: The negative class consists
of articles from the Untagged set. Since some
of these could be manually undetected promo-
tional articles, the accuracy measured in this
setting is probably an under-estimate.
? Optimistic Setting: The negative class consists
of articles from the Featured/Good set. These
articles are at one end of the quality spectrum,
making it relatively easier to distinguish them
from promotional articles.
The true performance of the classifier is likely some-
where between that achieved in these two settings.
6We maintain an equal number of positive and negative test
cases in both the settings.
1853
Features
Pessimistic Setting Optimistic Setting
P R F1 AUC P R F1 AUC
Bag-of-words Baseline 0.823 0.820 0.821 0.893 0.931 0.931 0.931 0.979
PCFG 0.881 0.870 0.865 0.903 0.910 0.910 0.910 0.961
Character trigrams 0.889 0.887 0.888 0.952 0.858 0.843 0.841 0.877
Word trigrams 0.863 0.863 0.863 0.931 0.887 0.883 0.882 0.931
Character trigrams + Word trigrams 0.89 0.888 0.889 0.952 0.908 0.907 0.907 0.962
PCFG+Char. trigrams+Word trigrams 0.914 0.915 0.914 0.974 0.950 0.950 0.950 0.983
58 Content and Meta Features 0.866 0.867 0.867 0.938 0.986 0.986 0.986 0.996
All Features 0.940 0.940 0.940 0.986 0.989 0.989 0.989 0.997
Table 6: Performance (Precision(P), Recall(R), F1 score, AUC) of the classifier in the two settings
5.2 Results for Pessimistic Setting
From Table 6, we see that all features perform
better than the bag-of-words baseline. We also
see that character trigrams, one of the simplest
features, gives the best individual performance.
However, deeper syntactic features using PCFGs
also performs quite well. Combining all of the
language-modeling features (PCFG + character tri-
grams + Word trigrams) further improves perfor-
mance. Compared to the 58 content and meta fea-
tures utilized by Anderka et al, (2012) described
in Section 4.1, the PCFG and character trigram fea-
tures give much better performance, both individu-
ally and when combined. It is interesting to note
that adding Anderka et al?s features to the language-
modeling ones gives a fairly small improvement in
performance. This validates our hypothesis that pro-
motional articles tend to have a distinct linguistic
style that is captured well using language models.
5.3 Results for Optimistic Setting
In the Optimistic Setting, as shown in Table 6,
the content and meta features give the best perfor-
mance, which improves only slightly when com-
bined with language-modeling features. The bag-of-
words baseline performs better than all the language
modeling features. This performance could be be-
cause there is a much clearer distinction between
promotional articles and featured/good articles that
can be captured by simple features alone. For exam-
ple, featured/good articles are generally longer than
usual Wikipedia articles and have more references.
5.4 Top Ranked Features and their
Performance
To analyze the performance of different features, we
determined the top ranked features using Informa-
tion Gain. In the Pessimistic Setting, the top six
features are all language-modeling features (charac-
ter trigram model feature works best), followed by
basic meta-features such as character count, word
count, category count and sentence count. The new
feature we introduced, ?Overall Sentiment Score? is
the 18th most informative feature in the pessimistic
setting, indicating that the cumulative sentiment of a
bag of words is not as discriminative as we would in-
tuitively assume. Using the 10 top-ranked features,
we get an F1 of 0.93, which is only slightly worse
than that achieved using all features (F1 = 0.94).
In the Optimistic Setting, the top-ranked features
are the number of references and the number of
references per section. This is consistent with the
observation that featured/good articles have very
long and comprehensive lists of references, since
Wikipedia?s fundamental policy is to maintain ver-
ifiability by citing relevant sources. Features based
on the n-gram and PCFG models also appear in the
list of ten best features. Using only the top 10 fea-
tures, gives an F1 of 0.988, which is almost as good
as using all features (F1 = 0.989).
5.5 Optimistic and Pessimistic Settings
In the optimistic setting, there is a clear distinc-
tion between the positive (promotional) and negative
(featured/good) classes. But there are only subtle
differences between the positive and negative (un-
tagged articles) classes in the pessimistic setting.
1854
Best Features in Pessimistic Setting Best Features in Optimistic Setting
LM char trigram Number of References
LM word trigram Number of References per Section
PCFG min LM word trigram
PCFG max Number of Words
PCFG mean PCFG mean
PCFG std deviation Number of Sentences
Number of Characters LM char trigram
Number of Words Number of Words
Number of Categories Number of Characters
Number of Sentences Number of Backlinks
Table 7: Top 10 Features (listed in order) in both Settings ranked using Information Gain
These two classes are superficially similar, in terms
of length, reference count, section count etc. Stylo-
metric features based on the trained language mod-
els are successful at detecting the subtle linguistic
differences in the two types of articles. This is use-
ful because the pessimistic setting is closer to the
real-world setting of articles in Wikipedia.
5.6 Error Analysis
Since the pessimistic setting is close to the real set-
ting of Wikipedia articles, it is useful to do an error
analysis of the classifier?s performance in this set-
ting. There is an approximately equal proportion of
false positives and false negatives.
A significant number of false positives seem to
be cases of manually undetected promotional arti-
cles. This demonstrates the practical utility of our
classifier. But there are also many false positives
that seem to be truly unbiased. These articles ap-
pear to have been poorly written, without following
Wikipedia?s editing policies. Examples include use
of very long lists of nouns, use of ambiguous terms
like ?many believe? and excessive use of superla-
tives. Other common characteristics of most of the
false positives are presence of a considerable num-
ber of complex sentences with multiple subordinate
clauses. These stylistic cues seem to be misleading
the classifier.
A common thread underlying most of the false
negatives is the fact that they are written in a nar-
rative style or they have excessive details in terms of
the content. Examples include narrating a detailed
story of a fictional character in an unbiased manner
or writing a minutely detailed account of the history
of an organization. Another source of false negatives
comes from biographical Wikipedia pages which are
written in a resume style, listing all their qualifi-
cations and achievements. These cues could help
one manually detect that the article, though not pro-
motional in style, is probably written with the view
of promoting the entity. As possible future work,
we could incorporate features derived from language
models for narrative style trained using an appropri-
ate external corpus of narrative text. This might en-
able the classifier to detect some cases of unbiased
promotional articles.
6 Conclusion
Our experiments and analysis show that stylomet-
ric features based on n-gram language models and
deeper syntactic PCFG models work very well for
detecting promotional articles in Wikipedia. Af-
ter analyzing the errors that are made during clas-
sification, we realize that though promotional con-
tent is non-neutral in majority of the cases, there do
exist promotional articles that are neutral in style.
Adding additional features based on language mod-
els of narrative style could lead to a better model of
Wikipedia?s promotional content.
7 Acknowledgements
This research was supported in part by the DARPA
DEFT program under AFRL grant FA8750-13-2-
0026 and by MURI ARO grant W911NF-08-1-
0242. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author and do not necessarily reflect the
view of DARPA, AFRL, ARO, or the US govern-
ment.
1855
References
Maik Anderka, Benno Stein, and Nedim Lipka. 2012.
Predicting quality flaws in user-generated content: the
case of Wikipedia. In Proceedings of the 35th Inter-
national ACM SIGIR Conference on Research and de-
velopment in Information Retrieval, SIGIR ?12, pages
981?990, New York, NY, USA. ACM.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th Conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Joachim Diederich, Jo?rg Kindermann, Edda Leopold, and
Gerhard Paass. 2003. Authorship attribution with
support vector machines. Applied Intelligence, 19(1-
2):109?123.
Hugo J Escalante, Thamar Solorio, and M Montes-y
Go?mez. 2011. Local histograms of character n-
grams for authorship attribution. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
volume 1, pages 288?298.
Rudolf Flesch. 1948. A new readability yardstick. The
Journal of Applied Psychology, 32(3):221.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
2000. Additive logistic regression: a statistical view
of boosting (with discussion and a rejoinder by the au-
thors). The Annals of Statistics, 28(2):337?407.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analy-
sis features. In Proceedings of the 20th International
Conference on Computational Linguistics, page 611.
Association for Computational Linguistics.
Manoj Harpalani, Michael Hart, Sandesh Singh, Rob
Johnson, and Yejin Choi. 2011. Language of van-
dalism: Improving Wikipedia vandalism detection via
stylometric analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Papers, volume 2, pages 83?88.
Daniel Hasan Dalip, Marcos Andre? Gonc?alves, Marco
Cristo, and Pa?vel Calado. 2009. Automatic qual-
ity assessment of content created collaboratively by
web communities: a case study of Wikipedia. In Pro-
ceedings of the 9th ACM/IEEE-CS Joint Conference
on Digital libraries, JCDL ?09, pages 295?304, New
York, NY, USA. ACM.
Michael Heilman, Kevyn Collins-Thompson, and Max-
ine Eskenazi. 2008. An analysis of statistical models
and features for reading difficulty prediction. In Pro-
ceedings of the Third Workshop on Innovative Use of
NLP for Building Educational Applications, pages 71?
79. Association for Computational Linguistics.
Vlado Kes?elj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for au-
thorship attribution. In Proceedings of the Conference
Pacific Association for Computational Linguistics, PA-
CLING, volume 3, pages 255?264.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
annual meeting of the Association for Computational
Linguistics, pages 544?554. Association for Computa-
tional Linguistics.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
ACL 2010 Conference Short Papers, ACLShort ?10,
pages 38?42, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Congzhou He Ramyaa and Khaled Rasheed. 2004. Us-
ing machine learning techniques for stylometry. In
Proceedings of International Conference on Machine
Learning.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
british national corpus sampler. Language and Com-
puters, 36(1):295?306.
Klaus Stein and Claudia Hess. 2007. Does it matter who
contributes: a study on featured articles in the German
Wikipedia. In Proceedings of the Eighteenth Confer-
ence on Hypertext and Hypermedia, pages 171?174.
ACM.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In Proceedings of the
2000 Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora: held in conjunction with the 38th Annual Meet-
ing of the Association for Computational Linguistics-
Volume 13, pages 63?70. Association for Computa-
tional Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 173?180. Association for Computational Lin-
guistics.
1856
William Yang Wang and Kathleen R. McKeown. 2010.
?Got you!?: Automatic vandalism detection in
Wikipedia with web-based shallow syntactic-semantic
modeling. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
?10, pages 1146?1154, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Dennis M Wilkinson and Bernardo A Huberman. 2007.
Assessing the value of coooperation in Wikipedia.
arXiv preprint cs/0702140.
1857
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, page 602,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Learning Language from Perceptual Context
Raymond Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
Machine learning has become the dominant approach to building natural-language processing sys-
tems. However, current approaches generally require a great deal of laboriously constructed human-
annotated training data. Ideally, a computer would be able to acquire language like a child by being
exposed to linguistic input in the context of a relevant but ambiguous perceptual environment. As
a step in this direction, we have developed systems that learn to sportscast simulated robot soccer
games and to follow navigation instructions in virtual environments by simply observing sample hu-
man linguistic behavior in context. This work builds on our earlier work on supervised learning of
semantic parsers that map natural language into a formal meaning representation. In order to apply
such methods to learning from observation, we have developed methods that estimate the meaning of
sentences given just their ambiguous perceptual context.
602
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220?229,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Statistical Script Learning with Multi-Argument Events
Karl Pichotta
Department of Computer Science
The University of Texas at Austin
pichotta@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
mooney@cs.utexas.edu
Abstract
Scripts represent knowledge of stereotyp-
ical event sequences that can aid text un-
derstanding. Initial statistical methods
have been developed to learn probabilis-
tic scripts from raw text corpora; how-
ever, they utilize a very impoverished rep-
resentation of events, consisting of a verb
and one dependent argument. We present
a script learning approach that employs
events with multiple arguments. Unlike
previous work, we model the interactions
between multiple entities in a script. Ex-
periments on a large corpus using the task
of inferring held-out events (the ?narrative
cloze evaluation?) demonstrate that mod-
eling multi-argument events improves pre-
dictive accuracy.
1 Introduction
Scripts encode knowledge of stereotypical events,
including information about their typical ordered
sequences of sub-events and corresponding argu-
ments (Schank and Abelson, 1977). The clas-
sic example is the ?restaurant script,? which en-
codes knowledge about what normally happens
when dining out. Such knowledge can be used
to improve text understanding by supporting in-
ference of missing actions and events, as well as
resolution of lexical and syntactic ambiguities and
anaphora (Rahman and Ng, 2012). For example,
given the text ?John went to Olive Garden and or-
dered lasagna. He left a big tip and left,? an infer-
ence that scripts would ideally allow us to make is
?John ate lasagna.?
There is a small body of recent research on auto-
matically learning probabilistic models of scripts
from large corpora of raw text (Manshadi et al.,
2008; Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Jans et al., 2012). However,
this work uses a very impoverished representation
of events that only includes a verb and a single de-
pendent entity. We propose a more complex multi-
argument event representation for use in statistical
script models, capable of directly capturing inter-
actions between multiple entities. We present a
method for learning such a model, and provide ex-
perimental evidence that modeling entity interac-
tions allows for better prediction of events in docu-
ments, compared to previous single-entity ?chain?
models. We also compare to a competitive base-
line not used in previous work, and introduce a
novel evaluation metric.
2 Background
The idea of representing stereotypical event se-
quences for textual inference originates in the
seminal work of Schank and Abelson (1977).
Early scripts were manually engineered for spe-
cific domains; however, Mooney and DeJong
(1985) present an early knowledge-based method
for learning scripts from a single document. These
early scripts (and methods for learning them) were
non-statistical and fairly brittle.
Chambers and Jurafsky (2008) introduced a
method for learning statistical scripts that, using a
much simpler event representation that allows for
efficient learning and inference. Jans et al. (2012)
use the same simple event representation, but in-
troduce a new model that more accurately predicts
test data. These methods only model the actions of
a single participant, called the protagonist. Cham-
bers and Jurafsky (2009) extended their approach
to the multi-participant case, modeling the events
in which all of the entities in a document are in-
volved; however, their method cannot represent in-
teractions between multiple entities.
Balasubramanian et al. (2012; 2013) describe
the Rel-gram system, a Markov model similar to
that of Jans et al. (2012), but with tuples instead
of (verb, dependency) pairs. Our approach is sim-
220
ilar, but instead of modeling a distribution over co-
occurring verbs and nominal arguments, we model
interactions between entities directly by incorpo-
rating coreference information into the model.
Previous statistical script learning systems pro-
ceed broadly as follows. For a document D:
1. Run a dependency parser on D, to match up
verbs with their argument NPs.
2. Run a coreference resolver onD to determine
which NPs likely refer to the same entity.
3. Construct a sequence of event objects, using
syntactic and coreference information.
One can then build a statistical model of the event
sequences produced by Step 3. Such a model may
be evaluated using the narrative cloze evaluation,
described in Section 4.1, in which we hold out an
event from a sequence and attempt to infer it.
The major difference between the current work
and previous work is that the event sequences pro-
duced in Step 3 are of a different sort from those
in other models. Our events are more structured,
as described in Section 3.1, and we produce one
event sequence per document, instead of one event
sequence per entity. This requires a different sta-
tistical model, as described in Section 3.2.
3 Script Models
In Section 3.1, we describe the multi-argument
events we use as the basis of our script models.
Section 3.2 describes a script model using these
events, and Section 3.3 describes the baseline sys-
tems to which we compare.
3.1 Multi-Argument Events
Statistical scripts are models of stereotypical se-
quences of events. In Chambers and Juraf-
sky (2008; 2009) and Jans et al. (2012), events
are (verb, dependency) pairs, forming ?chains,?
grouped according to the entity involved. For ex-
ample, the text
(1) Mary emailed Jim and he responded to her
immediately.
yields two chains. First, there is a chain for Mary:
(email, subject)
(respond, object)
indicating that Mary was the subject of an email-
ing event and the object of a responding event.
Second, there is a chain for Jim:
(email, object)
(respond, subject)
indicating that Jim was the object of an emailing
event and the subject of a responding event. Thus,
one document produces many chains, each cor-
responding to an entity. Note that a single verb
may produce multiple pair events, each present
in a chain corresponding to one of the verb?s ar-
guments. Note also that there is no connection
between the different events produced by a verb:
there is nothing connecting (email, subject) in
Mary?s chain with (email, object) in Jim?s chain.
We propose a richer event representation, in
which a document is represented as a single se-
quence of event tuples, the arguments of which are
entities. Each entity may be mentioned in many
events, and, unlike previous work, each event may
involve multiple entities. For example, sentence
(1) will produce a single two-event sequence, the
first event representing Mary emailing Jim, and the
second representing Jim responding to Mary.
Formally, an entity is represented by a con-
stant, and noun phrases are mapped to entities,
where two noun phrases are mapped to the same
constant if and only if they corefer. A multi-
argument event is a relational atom v(e
s
, e
o
, e
p
),
where v is a verb lemma, and e
s
, e
o
, and e
p
are
possibly-null entities. The first entity, e
s
, stands
in a subject relation to the verb v; the second, e
o
,
is the direct object of v; the third e
p
stands in
a prepositional relation to v. One of these enti-
ties is null (written as ???) if and only if no noun
phrase stands in the appropriate relation to v. For
example, Mary hopped would be represented as
hop(mary, ?, ?), while Mary gave the book to John
would be give(mary, book, john). In this formula-
tion, Example (1) produces the sequence
email(m, j, ?)
respond(j,m, ?)
where m and j are entity constants representing
all mentions of Mary and Jim, respectively. Note
that this formulation is capable of capturing inter-
actions between entities: we directly encode the
fact that after one person emails another, the lat-
ter responds to the former. In contrast, pair events
can capture only that after an entity emails, they
are responded to (or after they are emailed, they
respond). Multi-argument events capture more of
the basic event structure of text, and are therefore
well-suited as a representation for scripts.
221
3.2 Multi-argument Statistical Scripts
We now describe our script model. Section 3.2.1
describes our method of estimating a joint prob-
ability distribution over pairs of events, modeling
event co-occurrence, and Section 3.2.2 shows how
this co-occurrence probability can be used to infer
new events from a set of known events.
3.2.1 Estimating Joint Probabilities
Suppose we have a sequence of multi-argument
events, each of which is a verb with entities as ar-
guments. We are interested in predicting which
event is most likely to have happened at some
point in the sequence. Our model will require
a conditional probability P (a|a
?
), the probability
of seeing event a after event a
?
, given we have
observed a
?
. However, as described below, di-
rectly estimating this probability is more compli-
cated than in previous work because events now
have additional structure.
By definition, we have
P (a
2
|a
1
) =
P (a
1
, a
2
)
P (a
1
)
where P (a
1
, a
2
) is the probability of seeing a
1
and a
2
, in order. The most straightforward way
to estimate P (a
1
, a
2
) is, if possible, by counting
the number of times we observe a
1
and a
2
co-
occurring and normalizing the function to sum to
1 over all pairs (a
1
, a
2
). For Chambers and Ju-
rafsky (2008; 2009) and Jans et al. (2012), such a
Maximum Likelihood Estimate is straightforward
to arrive at: events are (verb, dependency) pairs,
and two events co-occur when they are in the same
event chain, relating to the same entity (Jans et al.
(2012) further require a
1
and a
2
to be near each
other). One need simply traverse a training corpus
and count the number of times each pair (a
1
, a
2
)
co-occurs. The Rel-grams of Balasubramanian et
al. (2012; 2013) admit a similar strategy: to arrive
at a joint distribution of pairwise co-occurrence,
one can simply count co-occurrence of ground re-
lations in a corpus and normalize.
However, given two multi-argument events of
the form v(e
s
, e
o
, e
p
), this strategy will not suffice.
For example, if during training we observe the two
co-occurring events
(2) ask(mary, bob, question)
answer(bob, ?, ?)
we would like this to lend evidence to the
co-occurrence of events ask(x, y, z) and
Algorithm 1 Learning with entity substitution
1: for a
1
, a
2
? evs do
2: N(a
1
, a
2
)? 0
3: end for
4: for D ? documents do
5: for a
1
, a
2
? coocurEvs(D) do
6: for ? ? subs(a
1
, a
2
) do
7: N(?(a
1
), ?(a
2
)) += 1
8: end for
9: end for
10: end for
answer(y, ?, ?) for all distinct entities x, y,
and z. If we were to simply keep the entities as
they are and calculate raw co-occurrence counts,
we would get evidence only for x = mary,
y = bob, and z = question.
One approach to this problem would be to de-
ploy one of many previously described Statistical
Relational Learning methods, for example Logi-
cal Hidden Markov Models (Kersting et al., 2006)
or Relational Markov Models (Anderson et al.,
2002). These methods can learn various statisti-
cal relationships between relational logical atoms
with variables, of the sort considered here. How-
ever, we investigate a simpler option.
The most important relationship between the
entities in two multi-argument events concerns
their overlapping entities. For example, to de-
scribe the relationship between the three entities
in (2), it is most important to note that the object
of the first event is identical with the subject of the
second (namely, both are bob). The identity of the
non-overlapping entities mary and question is not
important for capturing the relationship between
the two events.
We note that two multi-argument events
v(e
s
, e
o
, e
p
) and v
?
(e
?
s
, e
?
o
, e
?
p
), share at most three
entities. We thus introduce four variables x, y, z,
and O. The three variables x, y, and z repre-
sent arbitrary distinct entities, and the fourth, O,
stands for ?Other,? for entities not shared between
the two events. We can rewrite the entities in our
two multi-argument events using these variables,
with the constraint that two identical (i.e. corefer-
ent) entities must be mapped to the same variable
in {x, y, z}, and no two distinct entities may map
to the same variable in {x, y, z}. This formulation
simplifies calculations while still capturing pair-
wise entity relationships between events.
Algorithm 1 gives the pseudocode for the learn-
222
ing method. This populates a co-occurrence
matrix N , where entry N(a
1
, a
2
) gives the co-
occurrence count of events a
1
and a
2
. The vari-
able evs in line 1 is the set of all events in our
model, which are of the form v(e
s
, e
o
, e
p
), with v
a verb lemma and e
s
, e
o
, e
p
? {x, y, z, O}. The
variable documents in line 4 is the collection
of documents in our training corpus. The func-
tion cooccurEvs in line 5 takes a document D
and returns all ordered pairs of co-occurring events
in D, where, following the 2-skip bigram model
of Jans et al. (2012), and similar to Balasubrama-
nian et al. (2012; 2013), two events a
1
and a
2
are
said to co-occur if they occur in order, in the same
document, with at most two intervening events be-
tween them.
1
The function subs in line 6 takes
two events and returns all variable substitutions ?
mapping from entities mentioned in the events a
1
and a
2
to the set {x, y, z, O}, such that two coref-
erent entities map to the same element of {x, y, z}.
A substitution ? applied to an event v(e
s
, e
o
, e
p
),
as in line 7, is defined as v(?(e
s
), ?(e
o
), ?(e
p
)),
with the null entity mapped to itself.
Once we have calculatedN(a
1
, a
2
) using Algo-
rithm 1, we may define P (a
1
, a
2
) for two events
a
1
and a
2
, giving an estimate for the probability
of observing a
2
occurring after a
1
, as
P (a
1
, a
2
) =
N(a
1
, a
2
)
?
a
?
1
,a
?
2
N(a
?
1
, a
?
2
)
. (3)
We may then define the conditional probability of
seeing a
2
after a
1
, given an observation of a
1
:
P (a
2
|a
1
) =
P (a
1
, a
2
)
?
a
?
P (a
1
, a
?
)
=
N(a
1
, a
2
)
?
a
?
N(a
1
, a
?
)
. (4)
3.2.2 Inferring Events
Suppose we have a sequence of multi-argument
events extracted from a document. A natural task
for a statistical script model is to infer what other
events likely occurred, given the events explic-
itly stated in a document. Chambers and Jurafsky
(2008; 2009) treat the events involving an entity
as an unordered set, inferring the most likely ad-
ditional event, with no relative ordering between
the inferred event and known events. We adopt
the model of Jans et al. (2012), which was demon-
strated to give better empirical performance. This
1
Other notions of co-occurrence could easily be substi-
tuted here.
model takes an ordered sequence of events and
a position in that sequence, and guesses events
that likely occurred at that position. In that work,
events are (verb, dependency) pairs, and an event
sequence consists of all such pairs involving a par-
ticular entity. We use this model in the multi-
argument event setting, in which a document pro-
duces a single sequence of multi-argument events.
LetA be an ordered list of events, and let p be an
integer between 1 and |A|, the length ofA. For i =
1, . . . , |A|, define a
i
to be the ith element of A.
We follow Jans et al. (2012) by scoring a candidate
event a according to its probability of following all
of the events before position p, and preceding all
events after position p. That is, we rank candidate
events a by maximizing S(a), defined as
S(a) =
p?1
?
i=1
logP (a|a
i
) +
|A|
?
i=p
logP (a
i
|a) (5)
with conditional probabilities P (a|a
?
) calculated
using (4). Each event in a
i
? A independently
contributes to a candidate a?s score; the ordering
between a and a
i
is taken into account, but the or-
dering between the different events a
i
? A does
not directly affect a?s score.
3.3 Baseline Systems
We describe the baseline systems against which
we compare the performance of the multi-
argument script system described in section 3.2.
These systems infer new events (either multi-
argument or pair events) given the events con-
tained in a document.
Performance of these systems is measured using
the narrative cloze task, in which we hold out a sin-
gle event (either a multi-argument or pair event),
and rate a system by its ability to infer this event,
given the other events in a document. The narra-
tive cloze task is described in detail in Section 4.1.
3.3.1 Random Model
The simplest baseline we compare to is the ran-
dom baseline, which outputs randomly selected
events observed during training. This model can
guess either multi-argument or pair events.
3.3.2 Unigram Model
The unigram system guesses events ordered by
prior probability, as calculated from the train-
ing set. If scripts are viewed as n-gram models
223
over events, this baseline corresponds to a bag-of-
words unigram model. In this model, events are
assumed to occur independently, drawn from a sin-
gle distribution. This model can be used to guess
either multi-argument or pair events.
3.3.3 Single Protagonist Model
We refer to the system of Jans et al. (2012) as the
single protagonist system. This model takes a
single sequence of (verb, dependency) pair events,
all relating to a single entity. It then produces
a list of pair events, giving the model?s top pre-
dictions for additional events involving the entity.
This model maximizes the objective given in (5),
with the sequence A (and the candidate guesses a)
comprised of pair events.
3.3.4 Multiple Protagonist Model
The multiple protagonist system infers multi-
argument events. While this method is not de-
scribed in previous work, it is the most direct way
of guessing a full multi-argument event using a
single protagonist script model.
The multiple protagonist system uses a single-
protagonist model, which models pair events, to
predict multi-argument events, given a sequence
of known multi-argument events. Suppose we
have a non-empty set E of entities mentioned in
the known events. We describe the most direct
method of using a single-protagonist system to in-
fer additional multi-argument events involving E.
A multi-argument event a = v(e
s
, e
o
, e
p
) repre-
sents three pairs: (v, e
s
), (v, e
o
), and (v, e
p
). The
multiple protagonist model scores an event a ac-
cording to the score the single protagonist model
assigns to these three pairs individually.
For entity e ? E in some multi-argument event
in a document, we first extract the sequence of
(verb, dependency) pairs corresponding to e from
all known multi-argument events. For a pair d,
we calculate the score S
e
(d), the score the sin-
gle protagonist system assigns the pair d, given the
known pairs corresponding to e. If e has no known
pairs corresponding to it (in the cloze evaluation
described below, this will happen if e occurs only
in the held-out event), we fall back to calculating
S
e
(d) with a unigram model, as described in Sec-
tion 3.3.2, over (verb, dependency) pair events.
We then rank a multi-argument event a =
v(e
s
, e
o
, e
p
), with e
s
, e
o
, e
p
? E, with the follow-
ing objective function:
M(a) =S
e
s
((v, subj)) + S
e
o
((v, obj))+
S
e
p
((v, prep)) (6)
where, for null entity e, we define S
e
(d) = 0 for
all d. In the cloze evaluation, E will be the entities
in the held-out event. Each entity in a contributes
independently to the score M(a), based on the
known (verb, dependency) pairs involving that en-
tity. This model scores a multi-argument event a
by combining one independent single-protagonist
model for every entity in a.
This model is similar to the multi-participant
narrative schemas described in Chambers and Ju-
rafsky (2009), but whereas they infer bare verbs,
we infer an entire multi-argument event.
4 Evaluation
4.1 Evaluation Task
We follow previous work in using the narrative
cloze task to evaluate statistical scripts (Chambers
and Jurafsky, 2008; Chambers and Jurafsky, 2009;
Jans et al., 2012). The task is as follows: given
a sequence of events a
1
, . . . , a
n
from a document,
hold out some event a
p
and attempt to predict that
event, given the other events in the sequence. As
we cannot automatically evaluate the prediction of
truly unmentioned events in a document, this eval-
uation acts as a straightforward proxy.
In the aforementioned work, the cloze task is
to guess a pair event, given the other events in
which the held-out pair?s entity occurs. In Section
4.2.2, we evaluate directly on this task of guess-
ing pair events. However, in Section 4.2.1, we
evaluate on the task of guessing a multi-argument
event, given all other events in a document and the
entities mentioned in the held-out event. This is,
we argue, the most natural way to adapt the cloze
evaluation to the multi-argument event setting: in-
stead of guessing a held-out pair event based on
the other events involving its lone entity, we will
guess a held-out multi-argument event based on
the other events involving any of its entities.
A document may contain arbitrarily many enti-
ties. The script model described in Section 3.2.1,
however, only models events involving entities
from a closed class of four variables {x, y, z, O}.
We therefore rewrite entities in a document?s se-
quences of events to the variables {x, y, z, O} in
a way that maintains all pairwise relationships be-
tween the held-out event and others. That is, if the
224
held-out event shares an entity with another event,
this remains true after rewriting.
We perform entity rewriting relative to a single
held-out event, proceeding as follows:
? Any entity in the held-out event that is men-
tioned at least once in another event gets
rewritten consistently to one of x, y, or z,
such that distinct entities never get rewritten
to the same variable.
? Any entity mentioned only in the held-out
event is rewritten as O.
? All entities not present in the held-out event
are rewritten as O.
This simplification removes structure from the
original sequence, but retains the important pair-
wise entity relationships between the held-out
event and the other events.
4.2 Experimental Evaluation
For each document, we use the Stanford depen-
dency parser (De Marneffe et al., 2006) to get syn-
tactic information about the document; we then
use the Stanford coreference resolution engine
(Raghunathan et al., 2010) to get (noisy) equiva-
lence classes of coreferent noun phrases in a doc-
ument.
2
We train on approximately 1.1M arti-
cles from years 1994-2006 of the NYT portion
of the Gigaword Corpus, Third Edition (Graff et
al., 2007), holding out a random subset of the arti-
cles from 1999 for development and test sets. Our
test set consists of 10,000 randomly selected held-
out events, and our development set is 500 disjoint
randomly selected held-out events. To remove du-
plicate documents, we hash the first 500 characters
of each article and remove any articles with hash
collisions. We use add-one smoothing on all joint
probabilities. To reduce the size of our model, we
remove all events that occur fewer than 50 times.
3
We evaluate performance using the following
two metrics:
1. Recall at 10: Following Jans et al. (2012),
we measure performance by outputting the
top 10 guesses for each held-out event and
calculating the percentage of such lists con-
2
We use version 1.3.4 of the Stanford CoreNLP system.
3
A manual inspection reveals that the majority of these
removed events come from noisy text or parse errors.
taining the correct answer.
4
This value will
be between 0 and 1, with 1 indicating perfect
system performance.
2. Accuracy: A multi-argument event
v(e
s
, e
o
, e
p
) has four components; a pair
event has two components. For a held-out
event, we may judge the accuracy of a
system?s top guess by giving one point for
getting each of its components correct and
dividing by the number of possible points.
We average this value over the test set,
yielding a value between 0 and 1, with 1
indicating perfect system performance. This
is a novel evaluation metric for the script
learning task.
These metrics target a system?s most confident
predicted events: we argue that a script system is
best evaluated by its top inferences.
In Section 4.2.1, we evaluate on the task of in-
ferring multi-argument events. In Section 4.2.2,
we evaluate on the task of guessing pair events.
4.2.1 System Comparison on Multi-argument
Events
We first compare system performance on inferring
multi-argument events, evaluated on the narrative
cloze task as described in Section 4.1, using the
corpora and metrics described in Section 4.2. We
compare against three baselines: the uninformed
random baseline from Section 3.3.1, the unigram
system from 3.3.2, and the multiple protagonist
system from Section 3.3.4.
The joint system guesses the held-out event,
given the other events in the document that involve
the entities in that held-out tuple. The system or-
ders candidate events a by their scores S(a), as
given in Equation (5). This is the primary sys-
tem described in this paper, modeling full multi-
argument events directly.
Table 1 gives the recall at 10 (?R@10?) and ac-
curacy scores for the different systems. The uni-
gram system is quite competitive, achieving per-
formance comparable to the multiple protagonist
system on accuracy, and superior performance on
recall at 10.
Evaluating by the recall at 10 metric, the joint
system provides a 2.9% absolute (13.2% relative)
improvement over the unigram system, and a 3.6%
4
Jans et al. (2012) instead use recall at 50, but we observe,
as they also report, that the comparative differences between
systems using recall at k for various values of k is similar.
225
Method R@10 Accuracy
Random 0.001 0.334
Unigram 0.216 0.507
Multiple Protagonist 0.209 0.504
Joint 0.245 0.549
Table 1: Results for multi-argument events.
absolute (17.2% relative) improvement over the
multiple protagonist system. These differences
are statistically significant (p < 0.01) by McNe-
mar?s test. By accuracy, the joint system provides
a 4.2% absolute (8.3% relative) improvement over
the unigram model, and a 4.5% absolute (8.9%
relative) improvement over the multiple protago-
nist model. Accuracy differences are significant
(p < 0.01) by a Wilcoxon signed-rank test.
These results provide evidence that directly
modeling full multi-argument events, as opposed
to modeling chains of (verb, dependency) pairs for
single entities, allows us to better infer held-out
verbs with all participating entities.
4.2.2 System Comparison on Pair Events
In Section 4.2.1, we adapted a baseline pair-event
system to the task of guessing multi-argument
events. We may also do the converse, adapting our
multi-argument event system to the task of guess-
ing the simpler pair events. That is, we infer a full
multi-argument event and extract from it a (sub-
ject,verb) pair relating to a particular entity. This
allows us to compare directly to previously pub-
lished methods.
The random, unigram, and single protagonist
systems are pair-event systems described in Sec-
tions 3.3.1, 3.3.2, and 3.3.3, respectively. The
joint pair system takes the multi-argument events
guessed by the joint system of Section 4.2.1 and
converts them to pair events by discarding any in-
formation not related to the target entity; that is, if
the held-out pair event relates to an entity e, then
every occurrence of e as an argument of a guessed
multi-argument event will be converted into a sin-
gle pair event, scored identically to its original
multi-argument event. Ties are broken arbitrarily.
Table 2 gives the comparative results for these
four systems. The test set is constructed by ex-
tracting one pair event from each of the 10,000
multi-argument events in the test set used in Sec-
tion 4.2.1, such that the extracted pair event relates
to an entity with at least one additional known pair
Method R@10 Accuracy
Random 0.001 0.495
Unigram 0.297 0.552
Single Protagonist 0.282 0.553
Joint Pair 0.336 0.561
Table 2: Results for pair events.
event. Evaluating by recall at 10, the joint sys-
tem provides a 3.9% absolute (13.1% relative) im-
provement over the unigram baseline, and a 5.4%
absolute (19.1% relative) improvement over the
single protagonist system. These differences are
significant (p < 0.01) by McNemar?s test. By
accuracy, the joint system provides a 0.9% abso-
lute (1.6% relative) improvement over the unigram
model, and a 0.8% absolute (1.4% relative) im-
provement over the single protagonist model. Ac-
curacy differences are significant (p < 0.01) by a
Wilcoxon signed-rank test.
These results indicate that modeling multi-
argument event sequences allows better inference
of simpler pair events. These performance im-
provements may be due to the fact that the joint
model conditions on information not representable
in the single protagonist model (namely, all of the
events in which a multi-argument event?s entities
are involved).
5 Related Work
The procedural encoding of common situations
for automated reasoning dates back decades. The
frames of Minsky (1974), schemas of Rumelhart
(1975), and scripts of Schank and Abelson (1977)
are early examples. These models use quite com-
plex representations for events, with many differ-
ent relations between events. They are not statis-
tical, and use separate models for different scenar-
ios (e.g. the ?restaurant script? is different from
the ?bank script?). Generally, they require humans
to encode procedural information by hand; see,
however, Mooney and DeJong (1985) for an early
method for learning scripts automatically from a
document. Miikkulainen (1990; 1993) gives a hi-
erarchical Neural Network system which stores
sequences of events from text in episodic memory,
capable of simple question answering.
Regneri et al. (2010) and Li et al. (2012)
give methods for using crowdsourcing to cre-
ate situation-specific scripts. These methods
226
help alleviate the bottleneck of the knowledge-
engineering required for traditionally conceived
script systems. These systems are precision-
oriented: they create small, highly accurate scripts
for very limited scenarios. The current work,
in contrast, focuses on building high-recall mod-
els of general event sequences. There are also a
number of systems addressing the related problem
of modeling domain-specific human-human dia-
log for building dialog systems (Bangalore et al.,
2006; Chotimongkol, 2008; Boyer et al., 2009).
There have been a number of recent approaches
to learning statistical scripts. Chambers and Ju-
rafsky (2008) and Jans et al. (2012) give methods
for learning models of (verb, dependency) pairs,
as described above. Manshadi et al. (2008) give
an n-gram model for sequences of verbs and their
patients. McIntyre and Lapata (2009; 2010) use
script objects learned from corpora of fairy tales
to automatically generate stories. Chambers and
Jurafsky (2009) extend their previous model to
incorporate multiple entities, but do not directly
model the different arguments of an event. Bam-
man et al. (2013) learn latent character personas
from film summaries, associating character types
with stereotypical actions; they focus on identify-
ing persona types, rather than event inference.
Manshadi et al. (2008) and Balasubramanian et
al. (2012; 2013) give approaches similar to the
current work for modeling sequences of events as
n-grams. These methods differ from the current
work in that they do not model entities directly, in-
stead modeling co-occurrence of particular nouns
standing as arguments to particular verbs. Lewis
and Steedman (2013) build clusters of relations
similar to these events, finding such clusters help-
ful to question answering and textual inference.
There has also been recent interest in the related
problem of automatically learning event frames
(Bejan, 2008; Chambers and Jurafsky, 2011; Che-
ung et al., 2013; Chambers, 2013). These ap-
proaches focus on identifying frames for infor-
mation extraction tasks, as opposed to inferring
events directly. Balasubramanian et al. (2013) give
an event frame identification method, developed in
parallel with the current work, using sequences of
tuples similar to our multi-argument events, noting
coherence issues with pair events. Their formu-
lation differs from ours primarily in that they do
not incorporate coreference information into their
event co-occurrence distribution, and evaluate us-
ing human judgments of frame coherence rather
than a narrative cloze test.
6 Future Work
We have evaluated only one type of multi-
argument event inference, in which a script infers
an event given a set of entities and the events in-
volving those entities. We claim that this is the
most natural adaptation of the cloze evaluation to
the multi-argument event setting. However, other
types of inferences would be useful as well for
question-answering. Additional script inferences,
and their applications to question answering, are
worth investigating more fully.
The evaluation methodology used here has two
serious benefits: it is totally automatic, and it does
not require labeled data. The cloze evaluation is
intuitively reasonable: a good script system should
be able to predict stated events as having taken
place. Basic pragmatic reasoning, however, tells
us that the most obvious inferable events are not
typically stated in text. This evaluation thus fails
to capture some of the most important common-
sense inferences. Further investigation into evalu-
ation methodologies for script systems is needed.
7 Conclusion
We described multi-argument events for statisti-
cal scripts, which can directly encode the pair-
wise entity relationships between events in a doc-
ument. We described a script model that can han-
dle the important aspects of the additional com-
plexity introduced by these events, and a baseline
model that can infer multi-argument events using
single-protagonist chains instead of directly mod-
eling full relations. We introduced the novel uni-
gram baseline model for comparison, as well as
the novel accuracy metric, and provided empir-
ical evidence that modeling full multi-argument
events provides more predictive power than mod-
eling event chains individually.
Acknowledgments
Thanks to Katrin Erk, Amelia Harrison, and the
DEFT group at UT Austin for helpful discussions.
Thanks also to the anonymous reviewers for their
helpful comments. This research was supported in
part by the DARPA DEFT program under AFRL
grant FA8750-13-2-0026. Some of our experi-
ments were run on the Mastodon Cluster, sup-
ported by NSF Grant EIA-0303609.
227
References
Corin R Anderson, Pedro Domingos, and Daniel S
Weld. 2002. Relational Markov models and their
application to adaptive web navigation. In Proceed-
ings of the Eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining
(KDD-2002), pages 143?152.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2012. Rel-grams: a
probabilistic model of relations in text. In Proceed-
ings of the Joint Workshop on Automatic Knowledge
Base Construction and Web-scale Knowledge
Extraction at NAACL-HLT 2012 (AKBC-WEKEX
2012), pages 101?105.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP-2013).
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2013. Learning latent personas of film char-
acters. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-13), pages 352?361.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2006. Learning the structure of task-
driven human?human dialogs. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING/ACL-
06), pages 201?208.
Cosmin Adrian Bejan. 2008. Unsupervised discov-
ery of event scenarios from texts. In Prodeedings of
the 21st International Florida Artificial Intelligence
Research Society Conference (FLAIRS-2008), pages
124?129.
Kristy Elizabeth Boyer, Robert Phillips, Eun Young
Ha, Michael D. Wallis, Mladen A. Vouk, and
James C. Lester. 2009. Modeling dialogue structure
with adjacency pair analysis and Hidden Markov
Models. In Proceedings of Human Language Tech-
nologies: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Paper (NAACL-
HLT-09 Short), pages 49?52.
Nathanael Chambers and Daniel Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 789?797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP), pages 602?610.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT-
11), pages 976?986.
Nathanael Chambers. 2013. Event schema induc-
tion with a probabilistic entity-driven model. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2013).
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-13).
Ananlada Chotimongkol. 2008. Learning the struc-
ture of task-oriented conversations from the corpus
of in-domain dialogs. Ph.D. thesis, Carnegie Mellon
University.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources & Evaluation
(LREC-2006), volume 6, pages 449?454.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Linguistic Data Consortium.
Bram Jans, Steven Bethard, Ivan Vuli?c, and
Marie Francine Moens. 2012. Skip n-grams
and ranking functions for predicting script events.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-12), pages 336?344.
Kristian Kersting, Luc De Raedt, and Tapani Raiko.
2006. Logical Hidden Markov Models. Journal of
Artificial Intelligence Research, 25:425?456.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179?192.
Boyang Li, Stephen Lee-Urban, Darren Scott Appling,
and Mark O Riedl. 2012. Crowdsourcing narrative
intelligence. Advances in Cognitive Systems, 2:25?
42.
Mehdi Manshadi, Reid Swanson, and Andrew S Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Prodeed-
ings of the 21st International Florida Artificial In-
telligence Research Society Conference (FLAIRS-
2008), pages 159?164.
228
Neil McIntyre and Mirella Lapata. 2009. Learn-
ing to tell tales: A data-driven approach to story
generation. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP), pages 217?225.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-10),
pages 1562?1572.
Risto Miikkulainen. 1990. DISCERN: A Distributed
Artificial Neural Network Model of Script Process-
ing and Memory. Ph.D. thesis, University of Cali-
fornia.
Risto Miikkulainen. 1993. Subsymbolic Natural Lan-
guage Processing: An Integrated Model of Scripts,
Lexicon, and Memory. MIT Press, Cambridge, MA.
Marvin Minsky. 1974. A framework for representing
knowledge. Technical report, MIT-AI Laboratory.
Raymond J. Mooney and Gerald F. DeJong. 1985.
Learning schemata for natural language processing.
In Proceedings of the Ninth International Joint Con-
ference on Artificial Intelligence (IJCAI-85), pages
681?687, Los Angeles, CA, August.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2010), pages
492?501.
Altaf Rahman and Vincent Ng. 2012. Resolving
complex cases of definite pronouns: the Winograd
schema challenge. In Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL-12), pages 777?789.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-10), Uppsala, Sweden, July.
David Rumelhart. 1975. Notes on a schema for sto-
ries. Representation and Understanding: Studies in
Cognitive Science.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: An Inquiry into
Human Knowledge Structures. Lawrence Erlbaum
and Associates, Hillsdale, NJ.
229
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 109?117,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Multi-Prototype Vector-Space Models of Word Meaning
Joseph Reisinger
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233
joeraii@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233
mooney@cs.utexas.edu
Abstract
Current vector-space models of lexical seman-
tics create a single ?prototype? vector to rep-
resent the meaning of a word. However, due
to lexical ambiguity, encoding word mean-
ing with a single vector is problematic. This
paper presents a method that uses cluster-
ing to produce multiple ?sense-specific? vec-
tors for each word. This approach provides
a context-dependent vector representation of
word meaning that naturally accommodates
homonymy and polysemy. Experimental com-
parisons to human judgements of semantic
similarity for both isolated words as well as
words in sentential contexts demonstrate the
superiority of this approach over both proto-
type and exemplar based vector-space models.
1 Introduction
Automatically judging the degree of semantic sim-
ilarity between words is an important task useful
in text classification (Baker and McCallum, 1998),
information retrieval (Sanderson, 1994), textual en-
tailment, and other language processing tasks. The
standard empirical approach to this task exploits the
distributional hypothesis, i.e. that similar words ap-
pear in similar contexts (Curran and Moens, 2002;
Lin and Pantel, 2002; Pereira et al, 1993). Tra-
ditionally, word types are represented by a sin-
gle vector of contextual features derived from co-
occurrence information, and semantic similarity is
computed using some measure of vector distance
(Lee, 1999; Lowe, 2001).
However, due to homonymy and polysemy, cap-
turing the semantics of a word with a single vector is
problematic. For example, the word club is similar
to both bat and association, which are not at all simi-
lar to each other. Word meaning violates the triangle
inequality when viewed at the level of word types,
posing a problem for vector-space models (Tver-
sky and Gati, 1982). A single ?prototype? vector
is simply incapable of capturing phenomena such as
homonymy and polysemy. Also, most vector-space
models are context independent, while the meaning
of a word clearly depends on context. The word club
in ?The caveman picked up the club? is similar to bat
in ?John hit the robber with a bat,? but not in ?The
bat flew out of the cave.?
We present a new resource-lean vector-space
model that represents a word?s meaning by a set of
distinct ?sense specific? vectors. The similarity of
two isolated words A and B is defined as the mini-
mum distance between one of A?s vectors and one of
B?s vectors. In addition, a context-dependent mean-
ing for a word is determined by choosing one of the
vectors in its set based on minimizing the distance
to the vector representing the current context. Con-
sequently, the model supports judging the similarity
of both words in isolation and words in context.
The set of vectors for a word is determined by un-
supervised word sense discovery (WSD) (Schu?tze,
1998), which clusters the contexts in which a word
appears. In previous work, vector-space lexical sim-
ilarity and word sense discovery have been treated
as two separate tasks. This paper shows how they
can be combined to create an improved vector-space
model of lexical semantics. First, a word?s contexts
are clustered to produce groups of similar context
vectors. An average ?prototype? vector is then com-
puted separately for each cluster, producing a set of
vectors for each word. Finally, as described above,
these cluster vectors can be used to determine the se-
109
mantic similarity of both isolated words and words
in context. The approach is completely modular, and
can integrate any clustering method with any tradi-
tional vector-space model.
We present experimental comparisons to human
judgements of semantic similarity for both isolated
words and words in sentential context. The results
demonstrate the superiority of a clustered approach
over both traditional prototype and exemplar-based
vector-space models. For example, given the iso-
lated target word singer our method produces the
most similar word vocalist, while using a single pro-
totype gives musician. Given the word cell in the
context: ?The book was published while Piasecki
was still in prison, and a copy was delivered to his
cell.? the standard approach produces protein while
our method yields incarcerated.
The remainder of the paper is organized as fol-
lows: Section 2 gives relevant background on pro-
totype and exemplar methods for lexical semantics,
Section 3 presents our multi-prototype method, Sec-
tion 4 presents our experimental evaluations, Section
5 discusses future work, and Section 6 concludes.
2 Background
Psychological concept models can be roughly di-
vided into two classes:
1. Prototype models represented concepts by an
abstract prototypical instance, similar to a clus-
ter centroid in parametric density estimation.
2. Exemplar models represent concepts by a con-
crete set of observed instances, similar to non-
parametric approaches to density estimation in
statistics (Ashby and Alfonso-Reese, 1995).
Tversky and Gati (1982) famously showed that con-
ceptual similarity violates the triangle inequality,
lending evidence for exemplar-based models in psy-
chology. Exemplar models have been previously
used for lexical semantics problems such as selec-
tional preference (Erk, 2007) and thematic fit (Van-
dekerckhove et al, 2009). Individual exemplars can
be quite noisy and the model can incur high com-
putational overhead at prediction time since naively
computing the similarity between two words using
each occurrence in a textual corpus as an exemplar
requires O(n2) comparisons. Instead, the standard
... chose Zbigniew Brzezinski 
for the position of ...
... thus the symbol s position 
on his clothing was ...
... writes call options against 
the stock position ...
... offered a position with ...
... a position he would hold 
until his retirement in ...
... endanger their position as 
a cultural group...
... on the chart of the vessel s 
current position ...
... not in a position to help...
(cluster#2) 
post
appointme
nt, role, job
(cluster#4) 
lineman, 
tackle, role, 
scorer
(cluster#1) 
location
importance 
bombing
(collect contexts) (cluster)
(cluster#3) 
intensity, 
winds, 
hour, gust
(similarity)
single
prototype
Figure 1: Overview of the multi-prototype approach
to near-synonym discovery for a single target word
independent of context. Occurrences are clustered
and cluster centroids are used as prototype vectors.
Note the ?hurricane? sense of position (cluster 3) is
not typically considered appropriate in WSD.
approach is to compute a single prototype vector for
each word from its occurrences.
This paper presents a multi-prototype vector space
model for lexical semantics with a single parame-
ter K (the number of clusters) that generalizes both
prototype (K = 1) and exemplar (K = N , the total
number of instances) methods. Such models have
been widely studied in the Psychology literature
(Griffiths et al, 2007; Love et al, 2004; Rosseel,
2002). By employing multiple prototypes per word,
vector space models can account for homonymy,
polysemy and thematic variation in word usage.
Furthermore, such approaches require only O(K2)
comparisons for computing similarity, yielding po-
tential computational savings over the exemplar ap-
proach when K  N , while reaping many of the
same benefits.
Previous work on lexical semantic relatedness has
focused on two approaches: (1) mining monolin-
gual or bilingual dictionaries or other pre-existing
resources to construct networks of related words
(Agirre and Edmond, 2006; Ramage et al, 2009),
and (2) using the distributional hypothesis to au-
tomatically infer a vector-space prototype of word
meaning from large corpora (Agirre et al, 2009;
Curran, 2004; Harris, 1954). The former approach
tends to have greater precision, but depends on hand-
110
crafted dictionaries and cannot, in general, model
sense frequency (Budanitsky and Hirst, 2006). The
latter approach is fundamentally more scalable as it
does not rely on specific resources and can model
corpus-specific sense distributions. However, the
distributional approach can suffer from poor preci-
sion, as thematically similar words (e.g., singer and
actor) and antonyms often occur in similar contexts
(Lin et al, 2003).
Unsupervised word-sense discovery has been
studied by number of researchers (Agirre and Ed-
mond, 2006; Schu?tze, 1998). Most work has also
focused on corpus-based distributional approaches,
varying the vector-space representation, e.g. by in-
corporating syntactic and co-occurrence information
from the words surrounding the target term (Pereira
et al, 1993; Pantel and Lin, 2002).
3 Multi-Prototype Vector-Space Models
Our approach is similar to standard vector-space
models of word meaning, with the addition of a per-
word-type clustering step: Occurrences for a spe-
cific word type are collected from the corpus and
clustered using any appropriate method (?3.1). Sim-
ilarity between two word types is then computed as
a function of their cluster centroids (?3.2), instead of
the centroid of all the word?s occurrences. Figure 1
gives an overview of this process.
3.1 Clustering Occurrences
Multiple prototypes for each word w are generated
by clustering feature vectors v(c) derived from each
occurrence c ? C(w) in a large textual corpus and
collecting the resulting cluster centroids pik(w), k ?
[1,K]. This approach is commonly employed in un-
supervised word sense discovery; however, we do
not assume that clusters correspond to traditional
word senses. Rather, we only rely on clusters to cap-
ture meaningful variation in word usage.
Our experiments employ a mixture of von Mises-
Fisher distributions (movMF) clustering method
with first-order unigram contexts (Banerjee et al,
2005). Feature vectors v(c) are composed of indi-
vidual features I(c, f), taken as all unigrams occur-
ring f ? F in a 10-word window around w.
Like spherical k-means (Dhillon and Modha,
2001), movMF models semantic relatedness using
cosine similarity, a standard measure of textual sim-
ilarity. However, movMF introduces an additional
per-cluster concentration parameter controlling its
semantic breadth, allowing it to more accurately
model non-uniformities in the distribution of cluster
sizes. Based on preliminary experiments comparing
various clustering methods, we found movMF gave
the best results.
3.2 Measuring Semantic Similarity
The similarity between two words in a multi-
prototype model can be computed straightforwardly,
requiring only simple modifications to standard dis-
tributional similarity methods such as those pre-
sented by Curran (2004). Given words w and w?, we
define two noncontextual clustered similarity met-
rics to measure similarity of isolated words:
AvgSim(w,w?)
def
=
1
K2
K?
j=1
K?
k=1
d(pik(w), pij(w
?))
MaxSim(w,w?)
def
= max
1?j?K,1?k?K
d(pik(w), pij(w
?))
where d(?, ?) is a standard distributional similarity
measure. In AvgSim, word similarity is computed
as the average similarity of all pairs of prototype
vectors; In MaxSim the similarity is the maximum
over all pairwise prototype similarities. All results
reported in this paper use cosine similarity, 1
Cos(w,w?) =
?
f?F I(w, f) ? I(w
?, f)
??
f?F I(w, f)
2
??
f?F I(w
?, f)2
We compare across two different feature functions
tf-idf weighting and ?2 weighting, chosen due to
their ubiquity in the literature (Agirre et al, 2009;
Curran, 2004).
In AvgSim, all prototype pairs contribute equally
to the similarity computation, thus two words are
judged as similar if many of their senses are simi-
lar. MaxSim, on the other hand, only requires a sin-
gle pair of prototypes to be close for the words to be
judged similar. Thus, MaxSim models the similarity
of words that share only a single sense (e.g. bat and
club) at the cost of lower robustness to noisy clusters
that might be introduced when K is large.
When contextual information is available,
AvgSim and MaxSim can be modified to produce
1The main results also hold for weighted Jaccard similarity.
111
more precise similarity computations:
AvgSimC(w,w?)
def
=
1
K2
K?
j=1
K?
k=1
dc,w,kdc?,w?,jd(pik(w), pij(w
?))
MaxSimC(w,w?)
def
= d(p?i(w), p?i(w?))
where dc,w,k
def
= d(v(c), pik(w)) is the likelihood of
context c belonging to cluster pik(w), and p?i(w)
def
=
piargmax1?k?K dc,w,k(w), the maximum likelihood
cluster for w in context c. Thus, AvgSimC corre-
sponds to soft cluster assignment, weighting each
similarity term in AvgSim by the likelihood of the
word contexts appearing in their respective clus-
ters. MaxSimC corresponds to hard assignment,
using only the most probable cluster assignment.
Note that AvgSim and MaxSim can be thought of as
special cases of AvgSimC and MaxSimC with uni-
form weight to each cluster; hence AvgSimC and
MaxSimC can be used to compare words in context
to isolated words as well.
4 Experimental Evaluation
4.1 Corpora
We employed two corpora to train our models:
1. A snapshot of English Wikipedia taken on Sept.
29th, 2009. Wikitext markup is removed, as
are articles with fewer than 100 words, leaving
2.8M articles with a total of 2.05B words.
2. The third edition English Gigaword corpus,
with articles containing fewer than 100 words
removed, leaving 6.6M articles and 3.9B words
(Graff, 2003).
Wikipedia covers a wider range of sense distribu-
tions, whereas Gigaword contains only newswire
text and tends to employ fewer senses of most am-
biguous words. Our method outperforms baseline
methods even on Gigaword, indicating its advan-
tages even when the corpus covers few senses.
4.2 Judging Semantic Similarity
To evaluate the quality of various models, we first
compared their lexical similarity measurements to
human similarity judgements from the WordSim-
353 data set (Finkelstein et al, 2001). This test
corpus contains multiple human judgements on 353
word pairs, covering both monosemous and poly-
semous words, each rated on a 1?10 integer scale.
Spearman?s rank correlation (?) with average human
judgements (Agirre et al, 2009) was used to mea-
sure the quality of various models.
Figure 2 plots Spearman?s ? on WordSim-353
against the number of clusters (K) for Wikipedia
and Gigaword corpora, using pruned tf-idf and ?2
features.2 In general pruned tf-idf features yield
higher correlation than ?2 features. Using AvgSim,
the multi-prototype approach (K > 1) yields higher
correlation than the single-prototype approach (K =
1) across all corpora and feature types, achieving
state-of-the-art results with pruned tf-idf features.
This result is statistically significant in all cases for
tf-idf and for K ? [2, 10] on Wikipedia and K > 4
on Gigaword for ?2 features.3 MaxSim yields simi-
lar performance when K < 10 but performance de-
grades as K increases.
It is possible to circumvent the model-selection
problem (choosing the best value of K) by simply
combining the prototypes from clusterings of dif-
ferent sizes. This approach represents words using
both semantically broad and semantically tight pro-
totypes, similar to hierarchical clustering. Table 1
and Figure 2 (squares) show the result of such a com-
bined approach, where the prototypes for clusterings
of size 2-5, 10, 20, 50, and 100 are unioned to form a
single large prototype set. In general, this approach
works about as well as picking the optimal value of
K, even outperforming the single best cluster size
for Wikipedia.
Finally, we also compared our method to a pure
exemplar approach, averaging similarity across all
occurrence pairs.4 Table 1 summarizes the results.
The exemplar approach yields significantly higher
correlation than the single prototype approach in all
cases except Gigaword with tf-idf features (p <
0.05). Furthermore, it performs significantly worse
2(Feature pruning) We find that results using tf-idf features
are extremely sensitive to feature pruning while ?2 features are
more robust. In all experiments we prune tf-idf features by their
overall weight, taking the top 5000. This setting was found to
optimize the performance of the single-prototype approach.
3Significance is calculated using the large-sample approxi-
mation of the Spearman rank test; (p < 0.05).
4Averaging across all pairs was found to yield higher corre-
lation than averaging over the most similar pairs.
112
Spearman?s ? prototype exemplar multi-prototype (AvgSim) combined
K = 5 K = 20 K = 50
Wikipedia tf-idf 0.53?0.02 0.60?0.06 0.69?0.02 0.76?0.01 0.76?0.01 0.77?0.01
Wikipedia ?2 0.54?0.03 0.65?0.07 0.58?0.02 0.56?0.02 0.52?0.03 0.59?0.04
Gigaword tf-idf 0.49?0.02 0.48?0.10 0.64?0.02 0.61?0.02 0.61?0.02 0.62?0.02
Gigaword ?2 0.25?0.03 0.41?0.14 0.32?0.03 0.35?0.03 0.33?0.03 0.34?0.03
Table 1: Spearman correlation on the WordSim-353 dataset broken down by corpus and feature type.
Figure 2: WordSim-353 rank correlation vs. num-
ber of clusters (log scale) for both the Wikipedia
(left) and Gigaword (right) corpora. Horizontal bars
show the performance of single-prototype. Squares
indicate performance when combining across clus-
terings. Error bars depict 95% confidence intervals
using the Spearman test. Squares indicate perfor-
mance when combining across clusterings.
than combined multi-prototype for tf-idf features,
and does not differ significantly for ?2 features.
Overall this result indicates that multi-prototype per-
forms at least as well as exemplar in the worst case,
and significantly outperforms when using the best
feature representation / corpus pair.
4.3 Predicting Near-Synonyms
We next evaluated the multi-prototype approach on
its ability to determine the most closely related
words for a given target word (using the Wikipedia
corpus with tf-idf features). The top k most simi-
lar words were computed for each prototype of each
target word. Using a forced-choice setup, human
subjects were asked to evaluate the quality of these
near synonyms relative to those produced by a sin-
homonymous
carrier, crane, cell, company, issue, interest, match,
media, nature, party, practice, plant, racket, recess,
reservation, rock, space, value
polysemous
cause, chance, journal, market, network, policy,
power, production, series, trading, train
Table 2: Words used in predicting near synonyms.
gle prototype. Participants on Amazon?s Mechani-
cal Turk5 (Snow et al, 2008) were asked to choose
between two possible alternatives (one from a proto-
type model and one from a multi-prototype model)
as being most similar to a given target word. The
target words were presented either in isolation or in
a sentential context randomly selected from the cor-
pus. Table 2 lists the ambiguous words used for this
task. They are grouped into homonyms (words with
very distinct senses) and polysemes (words with re-
lated senses). All words were chosen such that their
usages occur within the same part of speech.
In the non-contextual task, 79 unique raters com-
pleted 7,620 comparisons of which 72 were dis-
carded due to poor performance on a known test set.6
In the contextual task, 127 raters completed 9,930
comparisons of which 87 were discarded.
For the non-contextual case, Figure 3 left plots
the fraction of raters preferring the multi-prototype
prediction (using AvgSim) over that of a single pro-
totype as the number of clusters is varied. When
asked to choose between the single best word for
5http://mturk.com
6(Rater reliability) The reliability of Mechanical Turk
raters is quite variable, so we computed an accuracy score for
each rater by including a control question with a known cor-
rect answer in each HIT. Control questions were generated by
selecting a random word from WordNet 3.0 and including as
possible choices a word in the same synset (correct answer) and
a word in a synset with a high path distance (incorrect answer).
Raters who got less than 50% of these control questions correct,
or spent too little time on the HIT were discarded.
113
Non-contextual Near-Synonym Prediction Contextual Near-Synonym Prediction
Figure 3: (left) Near-synonym evaluation for isolated words showing fraction of raters preferring multi-
prototype results vs. number of clusters. Colored squares indicate performance when combining across
clusterings. 95% confidence intervals computed using the Wald test. (right) Near-synonym evaluation for
words in a sentential context chosen either from the minority sense or the majority sense.
each method (top word), the multi-prototype pre-
diction is chosen significantly more frequently (i.e.
the result is above 0.5) when the number of clus-
ters is small, but the two methods perform sim-
ilarly for larger numbers of clusters (Wald test,
? = 0.05.) Clustering more accurately identi-
fies homonyms? clearly distinct senses and produces
prototypes that better capture the different uses of
these words. As a result, compared to using a sin-
gle prototype, our approach produces better near-
synonyms for homonyms compared to polysemes.
However, given the right number of clusters, it also
produces better results for polysemous words.
The near-synonym prediction task highlights one
of the weaknesses of the multi-prototype approach:
as the number of clusters increases, the number of
occurrences assigned to each cluster decreases, in-
creasing noise and resulting in some poor prototypes
that mainly cover outliers. The word similarity task
is somewhat robust to this phenomenon, but syn-
onym prediction is more affected since only the top
predicted choice is used. When raters are forced
to chose between the top three predictions for each
method (presented as top set in Figure 3 left), the ef-
fect of this noise is reduced and the multi-prototype
approach remains dominant even for a large num-
ber of clusters. This indicates that although more
clusters can capture finer-grained sense distinctions,
they also can introduce noise.
When presented with words in context (Figure
3 right),7 raters found no significant difference in
the two methods for words used in their majority
sense.8 However, when a minority sense is pre-
7Results for the multi-prototype method are generated using
AvgSimC (soft assignment) as this was found to significantly
outperform MaxSimC.
8Sense frequency determined using Google; senses labeled
manually by trained human evaluators.
114
sented (e.g. the ?prison? sense of cell), raters pre-
fer the choice predicted by the multi-prototype ap-
proach. This result is to be expected since the sin-
gle prototype mainly reflects the majority sense, pre-
venting it from predicting appropriate synonyms for
a minority sense. Also, once again, the perfor-
mance of the multi-prototype approach is better for
homonyms than polysemes.
4.4 Predicting Variation in Human Ratings
Variance in pairwise prototype distances can help
explain the variance in human similarity judgements
for a given word pair. We evaluate this hypothe-
sis empirically on WordSim-353 by computing the
Spearman correlation between the variance of the
per-cluster similarity computations, V[D], D def=
{d(pik(w), pij(w?)) : 1 ? k, j ? K}, and the vari-
ance of the human annotations for that pair. Cor-
relations for each dataset are shown in Figure 4 left.
In general, we find a statistically significant negative
correlation between these values using ?2 features,
indicating that as the entropy of the pairwise cluster
similarities increases (i.e., prototypes become more
similar, and similarities become uniform), rater dis-
agreement increases. This result is intuitive: if the
occurrences of a particular word cannot be easily
separated into coherent clusters (perhaps indicating
high polysemy instead of homonymy), then human
judgement will be naturally more difficult.
Rater variance depends more directly on the ac-
tual word similarity: word pairs at the extreme
ranges of similarity have significantly lower vari-
ance as raters are more certain. By removing word
pairs with similarity judgements in the middle two
quartile ranges (4.4 to 7.5) we find significantly
higher variance correlation (Figure 4 right). This
result indicates that multi-prototype similarity vari-
ance accounts for a secondary effect separate from
the primary effect that variance is naturally lower for
ratings in extreme ranges.
Although the entropy of the prototypes correlates
with the variance of the human ratings, we find that
the individual senses captured by each prototype do
not correspond to human intuition for a given word,
e.g. the ?hurricane? sense of position in Figure 1.
This notion is evaluated empirically by computing
the correlation between the predicted similarity us-
Figure 4: Plots of variance correlation; lower num-
bers indicate higher negative correlation, i.e. that
prototype entropy predicts rater disagreement.
ing the contextual multi-prototype method and hu-
man similarity judgements for different usages of
the same word. The Usage Similarity (USim) data
set collected in Erk et al (2009) provides such simi-
larity scores from human raters. However, we find
no evidence for correlation between USim scores
and their corresponding prototype similarity scores
(? = 0.04), indicating that prototype vectors may
not correspond well to human senses.
5 Discussion and Future Work
Table 3 compares the inferred synonyms for several
target words, generally demonstrating the ability of
the multi-prototype model to improve the precision
of inferred near-synonyms (e.g. in the case of singer
or need) as well as its ability to include synonyms
from less frequent senses (e.g., the experiment sense
of research or the verify sense of prove). However,
there are a number of ways it could be improved:
Feature representations: Multiple prototypes im-
prove Spearman correlation on WordSim-353 com-
pared to previous methods using the same under-
lying representation (Agirre et al, 2009). How-
ever we have not yet evaluated its performance
when using more powerful feature representations
such those based on Latent or Explicit Semantic
Analysis (Deerwester et al, 1990; Gabrilovich and
Markovitch, 2007). Due to its modularity, the multi-
prototype approach can easily incorporate such ad-
vances in order to further improve its effectiveness.
115
Inferred Thesaurus
bass
single guitar, drums, rhythm, piano, acoustic
multi basses, contrabass, rhythm, guitar, drums
claim
single argue, say, believe, assert, contend
multi assert, contend, allege, argue, insist
hold
single carry, take, receive, reach, maintain
multi carry, maintain, receive, accept, reach
maintain
single ensure, establish, achieve, improve, promote
multi preserve, ensure, establish, retain, restore
prove
single demonstrate, reveal, ensure, confirm, say
multi demonstrate, verify, confirm, reveal, admit
research
single studies, work, study, training, development
multi studies, experiments, study, investigations,
training
singer
single musician, actress, actor, guitarist, composer
multi vocalist, guitarist, musician, singer-
songwriter, singers
Table 3: Examples of the top 5 inferred near-
synonyms using the single- and multi-prototype ap-
proaches (with results merged). In general such
clustering improves the precision and coverage of
the inferred near-synonyms.
Nonparametric clustering: The success of the
combined approach indicates that the optimal num-
ber of clusters may vary per word. A more prin-
cipled approach to selecting the number of proto-
types per word is to employ a clustering model with
infinite capacity, e.g. the Dirichlet Process Mixture
Model (Rasmussen, 2000). Such a model would al-
low naturally more polysemous words to adopt more
flexible representations.
Cluster similarity metrics: Besides AvgSim and
MaxSim, there are many similarity metrics over
mixture models, e.g. KL-divergence, which may
correlate better with human similarity judgements.
Comparing to traditional senses: Compared to
WordNet, our best-performing clusterings are sig-
nificantly more fine-grained. Furthermore, they of-
ten do not correspond to agreed upon semantic dis-
tinctions (e.g., the ?hurricane? sense of position in
Fig. 1). We posit that the finer-grained senses actu-
ally capture useful aspects of word meaning, leading
to better correlation with WordSim-353. However, it
would be good to compare prototypes learned from
supervised sense inventories to prototypes produced
by automatic clustering.
Joint model: The current method independently
clusters the contexts of each word, so the senses dis-
covered forw cannot influence the senses discovered
for w? 6= w. Sharing statistical strength across simi-
lar words could yield better results for rarer words.
6 Conclusions
We presented a resource-light model for vector-
space word meaning that represents words as col-
lections of prototype vectors, naturally accounting
for lexical ambiguity. The multi-prototype approach
uses word sense discovery to partition a word?s con-
texts and construct ?sense specific? prototypes for
each cluster. Doing so significantly increases the ac-
curacy of lexical-similarity computation as demon-
strated by improved correlation with human similar-
ity judgements and generation of better near syn-
onyms according to human evaluators. Further-
more, we show that, although performance is sen-
sitive to the number of prototypes, combining pro-
totypes across a large range of clusterings performs
nearly as well as the ex-post best clustering. Finally,
variance in the prototype similarities is found to cor-
relate with inter-annotator disagreement, suggesting
psychological plausibility.
Acknowledgements
We would like to thank Katrin Erk for helpful dis-
cussions and making the USim data set available.
This work was supported by an NSF Graduate Re-
search Fellowship and a Google Research Award.
Experiments were run on the Mastodon Cluster, pro-
vided by NSF Grant EIA-0303609.
References
Eneko Agirre and Phillip Edmond. 2006. Word Sense
Disambiguation: Algorithms and Applications (Text,
Speech and Language Technology). Springer-Verlag
New York, Inc., Secaucus, NJ, USA.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proc. of NAACL-
HLT-09, pages 19?27.
116
F. Gregory Ashby and Leola A. Alfonso-Reese. 1995.
Categorization as probability density estimation. J.
Math. Psychol., 39(2):216?233.
L. Douglas Baker and Andrew K. McCallum. 1998. Dis-
tributional clustering of words for text classification.
In Proceedings of 21st International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 96?103.
Arindam Banerjee, Inderjit Dhillon, Joydeep Ghosh, and
Suvrit Sra. 2005. Clustering on the unit hypersphere
using von Mises-Fisher distributions. Journal of Ma-
chine Learning Research, 6:1345?1382.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
James R. Curran and Marc Moens. 2002. Improvements
in automatic thesaurus extraction. In Proceedings of
the ACL-02 workshop on Unsupervised lexical acqui-
sition, pages 59?66.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
College of Science.
Scott C. Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41:391?407.
Inderjit S. Dhillon and Dharmendra S. Modha. 2001.
Concept decompositions for large sparse text data us-
ing clustering. Machine Learning, 42:143?175.
Katrin Erk, Diana McCarthy, Nicholas Gaylord Investi-
gations on Word Senses, and Word Usages. 2009. In-
vestigations on word senses and word usages. In Proc.
of ACL-09.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics. Association for Computer Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the concept
revisited. In Proc. of WWW-01, pages 406?414, New
York, NY, USA. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proc. of IJCAI-07, pages
1606?1611.
David Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadephia.
Tom L. Griffiths, Kevin. R. Canini, Adam N. Sanborn,
and Daniel. J. Navarro. 2007. Unifying rational mod-
els of categorization via the hierarchical Dirichlet pro-
cess. In Proc. of CogSci-07.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Lillian Lee. 1999. Measures of distributional similarity.
In 37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 25?32.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of COLING-02, pages 1?7.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the Interational Joint
Conference on Artificial Intelligence, pages 1492?
1493. Morgan Kaufmann.
Bradley C. Love, Douglas L. Medin, and Todd M.
Gureckis. 2004. SUSTAIN: A network model of cat-
egory learning. Psych. Review, 111(2):309?332.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the 23rd Annual Meeting of the Cog-
nitive Science Society, pages 576?581.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proc. of SIGKDD-02, pages 613?
619, New York, NY, USA. ACM.
Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-93), pages
183?190, Columbus, Ohio.
Daniel Ramage, Anna N. Rafferty, and Christopher D.
Manning. 2009. Random walks for text seman-
tic similarity. In Proc. of the 2009 Workshop on
Graph-based Methods for Natural Language Process-
ing (TextGraphs-4), pages 23?31.
Carl E. Rasmussen. 2000. The infinite Gaussian mixture
model. In Advances in Neural Information Processing
Systems, pages 554?560. MIT Press.
Yves Rosseel. 2002. Mixture models of categorization.
J. Math. Psychol., 46(2):178?210.
Mark Sanderson. 1994. Word sense disambiguation and
information retrieval. In Proc. of SIGIR-94, pages
142?151.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast?but is it good? Eval-
uating non-expert annotations for natural language
tasks. In Proc. of EMNLP-08.
Amos Tversky and Itamar Gati. 1982. Similarity, sepa-
rability, and the triangle inequality. Psychological Re-
view, 89(2):123?154.
Bram Vandekerckhove, Dominiek Sandra, and Walter
Daelemans. 2009. A robust and extensible exemplar-
based model of thematic fit. In Proc. of EACL 2009,
pages 826?834. Association for Computational Lin-
guistics.
117
Proceedings of the ACL 2010 Conference Short Papers, pages 38?42,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Authorship Attribution Using Probabilistic Context-Free Grammars
Sindhu Raghavan Adriana Kovashka Raymond Mooney
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
{sindhu,adriana,mooney}@cs.utexas.edu
Abstract
In this paper, we present a novel approach
for authorship attribution, the task of iden-
tifying the author of a document, using
probabilistic context-free grammars. Our
approach involves building a probabilistic
context-free grammar for each author and
using this grammar as a language model
for classification. We evaluate the perfor-
mance of our method on a wide range of
datasets to demonstrate its efficacy.
1 Introduction
Natural language processing allows us to build
language models, and these models can be used
to distinguish between languages. In the con-
text of written text, such as newspaper articles or
short stories, the author?s style could be consid-
ered a distinct ?language.? Authorship attribution,
also referred to as authorship identification or pre-
diction, studies strategies for discriminating be-
tween the styles of different authors. These strate-
gies have numerous applications, including set-
tling disputes regarding the authorship of old and
historically important documents (Mosteller and
Wallace, 1984), automatic plagiarism detection,
determination of document authenticity in court
(Juola and Sofko, 2004), cyber crime investiga-
tion (Zheng et al, 2009), and forensics (Luyckx
and Daelemans, 2008).
The general approach to authorship attribution
is to extract a number of style markers from the
text and use these style markers as features to train
a classifier (Burrows, 1987; Binongo and Smith,
1999; Diederich et al, 2000; Holmes and Forsyth,
1995; Joachims, 1998; Mosteller and Wallace,
1984). These style markers could include the
frequencies of certain characters, function words,
phrases or sentences. Peng et al (2003) build a
character-level n-gram model for each author. Sta-
matatos et al (1999) and Luyckx and Daelemans
(2008) use a combination of word-level statistics
and part-of-speech counts or n-grams. Baayen et
al. (1996) demonstrate that the use of syntactic
features from parse trees can improve the accu-
racy of authorship attribution. While there have
been several approaches proposed for authorship
attribution, it is not clear if the performance of one
is better than the other. Further, it is difficult to
compare the performance of these algorithms be-
cause they were primarily evaluated on different
datasets. For more information on the current state
of the art for authorship attribution, we refer the
reader to a detailed survey by Stamatatos (2009).
We further investigate the use of syntactic infor-
mation by building complete models of each au-
thor?s syntax to distinguish between authors. Our
approach involves building a probabilistic context-
free grammar (PCFG) for each author and using
this grammar as a language model for classifica-
tion. Experiments on a variety of corpora includ-
ing poetry and newspaper articles on a number of
topics demonstrate that our PCFG approach per-
forms fairly well, but it only outperforms a bi-
gram language model on a couple of datasets (e.g.
poetry). However, combining our approach with
other methods results in an ensemble that performs
the best on most datasets.
2 Authorship Attribution using PCFG
We now describe our approach to authorship at-
tribution. Given a training set of documents from
different authors, we build a PCFG for each author
based on the documents they have written. Given
a test document, we parse it using each author?s
grammar and assign it to the author whose PCFG
produced the highest likelihood for the document.
In order to build a PCFG, a standard statistical
parser takes a corpus of parse trees of sentences
as training input. Since we do not have access to
authors? documents annotated with parse trees,
we use a statistical parser trained on a generic
38
corpus like the Wall Street Journal (WSJ) or
Brown corpus from the Penn Treebank (http:
//www.cis.upenn.edu/?treebank/)
to automatically annotate (i.e. treebank) the
training documents for each author. In our
experiments, we used the Stanford Parser (Klein
and Manning, 2003b; Klein and Manning,
2003a) and the OpenNLP sentence segmenter
(http://opennlp.sourceforge.net/).
Our approach is summarized below:
Input ? A training set of documents labeled
with author names and a test set of documents with
unknown authors.
1. Train a statistical parser on a generic corpus
like the WSJ or Brown corpus.
2. Treebank each training document using the
parser trained in Step 1.
3. Train a PCFG Gi for each author Ai using the
treebanked documents for that author.
4. For each test document, compute its likeli-
hood for each grammar Gi by multiplying the
probability of the top PCFG parse for each
sentence.
5. For each test document, find the author Ai
whose grammar Gi results in the highest like-
lihood score.
Output ? A label (author name) for each docu-
ment in the test set.
3 Experimental Comparison
This section describes experiments evaluating our
approach on several real-world datasets.
3.1 Data
We collected a variety of documents with known
authors including news articles on a wide range of
topics and literary works like poetry. We down-
loaded all texts from the Internet and manually re-
moved extraneous information as well as titles, au-
thor names, and chapter headings. We collected
several news articles from the New York Times
online journal (http://global.nytimes.
com/) on topics related to business, travel, and
football. We also collected news articles on
cricket from the ESPN cricinfo website (http:
//www.cricinfo.com). In addition, we col-
lected poems from the Project Gutenberg web-
site (http://www.gutenberg.org/wiki/
Main_Page). We attempted to collect sets of
documents on a shared topic written by multiple
authors. This was done to ensure that the datasets
truly tested authorship attribution as opposed to
topic identification. However, since it is very dif-
ficult to find authors that write literary works on
the same topic, the Poetry dataset exhibits higher
topic variability than our news datasets. We had
5 different datasets in total ? Football, Business,
Travel, Cricket, and Poetry. The number of au-
thors in our datasets ranged from 3 to 6.
For each dataset, we split the documents into
training and test sets. Previous studies (Stamatatos
et al, 1999) have observed that having unequal
number of words per author in the training set
leads to poor performance for the authors with
fewer words. Therefore, we ensured that, in the
training set, the total number of words per author
was roughly the same. We would like to note that
we could have also selected the training set such
that the total number of sentences per author was
roughly the same. However, since we would like
to compare the performance of the PCFG-based
approach with a bag-of-words baseline, we de-
cided to normalize the training set based on the
number of words, rather than sentences. For test-
ing, we used 15 documents per author for datasets
with news articles and 5 or 10 documents per au-
thor for the Poetry dataset. More details about the
datasets can be found in Table 1.
Dataset # authors # words/auth # docs/auth # sent/auth
Football 3 14374.67 17.3 786.3
Business 6 11215.5 14.16 543.6
Travel 4 23765.75 28 1086
Cricket 4 23357.25 24.5 1189.5
Poetry 6 7261.83 24.16 329
Table 1: Statistics for the training datasets used in
our experiments. The numbers in columns 3, 4 and
5 are averages.
3.2 Methodology
We evaluated our approach to authorship predic-
tion on the five datasets described above. For news
articles, we used the first 10 sections of the WSJ
corpus, which consists of annotated news articles
on finance, to build the initial statistical parser in
39
Step 1. For Poetry, we used 7 sections of the
Brown corpus which consists of annotated docu-
ments from different areas of literature.
In the basic approach, we trained a PCFG model
for each author based solely on the documents
written by that author. However, since the num-
ber of documents per author is relatively low, this
leads to very sparse training data. Therefore, we
also augmented the training data by adding one,
two or three sections of the WSJ or Brown corpus
to each training set, and up-sampling (replicating)
the data from the original author. We refer to this
model as ?PCFG-I?, where I stands for interpo-
lation since this effectively exploits linear interpo-
lation with the base corpus to smooth parameters.
Based on our preliminary experiments, we repli-
cated the original data three or four times.
We compared the performance of our approach
to bag-of-words classification and n-gram lan-
guage models. When using bag-of-words, one
generally removes commonly occurring ?stop
words.? However, for the task of authorship pre-
diction, we hypothesized that the frequency of
specific stop words could provide useful infor-
mation about the author?s writing style. Prelim-
inary experiments verified that eliminating stop
words degraded performance; therefore, we did
not remove them. We used the Maximum Entropy
(MaxEnt) and Naive Bayes classifiers in the MAL-
LET software package (McCallum, 2002) as ini-
tial baselines. We surmised that a discriminative
classifier like MaxEnt might perform better than
a generative classifier like Naive Bayes. How-
ever, when sufficient training data is not available,
generative models are known to perform better
than discriminative models (Ng and Jordan, 2001).
Hence, we chose to compare our method to both
Naive Bayes and MaxEnt.
We also compared the performance of the
PCFG approach against n-gram language models.
Specifically, we tried unigram, bigram and trigram
models. We used the same background corpus
mixing method used for the PCFG-I model to ef-
fectively smooth the n-gram models. Since a gen-
erative model like Naive Bayes that uses n-gram
frequencies is equivalent to an n-gram language
model, we also used the Naive Bayes classifier in
MALLET to implement the n-gram models. Note
that a Naive-Bayes bag-of-words model is equiva-
lent to a unigram language model.
While the PCFG model captures the author?s
writing style at the syntactic level, it may not accu-
rately capture lexical information. Since both syn-
tactic and lexical information is presumably useful
in capturing the author?s overall writing style, we
also developed an ensemble using a PCFG model,
the bag-of-words MaxEnt classifier, and an n-
gram language model. We linearly combined the
confidence scores assigned by each model to each
author, and used the combined score for the final
classification. We refer to this model as ?PCFG-
E?, where E stands for ensemble. We also de-
veloped another ensemble based on MaxEnt and
n-gram language models to demonstrate the con-
tribution of the PCFG model to the overall per-
formance of PCFG-E. For each dataset, we report
accuracy, the fraction of the test documents whose
authors were correctly identified.
3.3 Results and Discussion
Table 2 shows the accuracy of authorship predic-
tion on different datasets. For the n-gram mod-
els, we only report the results for the bigram
model with smoothing (Bigram-I) as it was the
best performing model for most datasets (except
for Cricket and Poetry). For the Cricket dataset,
the trigram-I model was the best performing n-
gram model with an accuracy of 98.34%. Gener-
ally, a higher order n-gram model (n = 3 or higher)
performs poorly as it requires a fair amount of
smoothing due to the exponential increase in all
possible n-gram combinations. Hence, the supe-
rior performance of the trigram-I model on the
Cricket dataset was a surprising result. For the
Poetry dataset, the unigram-I model performed
best among the smoothed n-gram models at 81.8%
accuracy. This is unsurprising because as men-
tioned above, topic information is strongest in
the Poetry dataset, and it is captured well in the
unigram model. For bag-of-words methods, we
find that the generatively trained Naive Bayes
model (unigram language model) performs bet-
ter than or equal to the discriminatively trained
MaxEnt model on most datasets (except for Busi-
ness). This result is not suprising since our
datasets are limited in size, and generative models
tend to perform better than discriminative meth-
ods when there is very little training data available.
Amongst the different baseline models (MaxEnt,
Naive Bayes, Bigram-I), we find Bigram-I to be
the best performing model (except for Cricket and
Poetry). For both Cricket and Poetry, Naive Bayes
40
Dataset MaxEnt Naive Bayes Bigram-I PCFG PCFG-I PCFG-E MaxEnt+Bigram-I
Football 84.45 86.67 86.67 93.34 80 91.11 86.67
Business 83.34 77.78 90.00 77.78 85.56 91.11 92.22
Travel 83.34 83.34 91.67 81.67 86.67 91.67 90.00
Cricket 91.67 95.00 91.67 86.67 91.67 95.00 93.34
Poetry 56.36 78.18 70.90 78.18 83.63 87.27 76.36
Table 2: Accuracy in % for authorship prediction on different datasets. Bigram-I refers to the bigram
language model with smoothing. PCFG-E refers to the ensemble based on MaxEnt, Bigram-I , and
PCFG-I . MaxEnt+Bigram-I refers to the ensemble based on MaxEnt and Bigram-I .
is the best performing baseline model. While dis-
cussing the performance of the PCFG model and
its variants, we consider the best performing base-
line model.
We observe that the basic PCFG model and the
PCFG-I model do not usually outperform the best
baseline method (except for Football and Poetry,
as discussed below). For Football, the basic PCFG
model outperforms the best baseline, while for
Poetry, the PCFG-I model outperforms the best
baseline. Further, the performance of the basic
PCFG model is inferior to that of PCFG-I for most
datasets, likely due to the insufficient training data
used in the basic model. Ideally one would use
more training documents, but in many domains
it is impossible to obtain a large corpus of doc-
uments written by a single author. For example,
as Luyckx and Daelemans (2008) argue, in foren-
sics one would like to identify the authorship of
documents based on a limited number of docu-
ments written by the author. Hence, we investi-
gated smoothing techniques to improve the perfor-
mance of the basic PCFG model. We found that
the interpolation approach resulted in a substan-
tial improvement in the performance of the PCFG
model for all but the Football dataset (discussed
below). However, for some datasets, even this
improvement was not sufficient to outperform the
best baseline.
The results for PCFG and PCFG-I demon-
strate that syntactic information alone is gener-
ally a bit less accurate than using n-grams. In or-
der to utilize both syntactic and lexical informa-
tion, we developed PCFG-E as described above.
We combined the best n-gram model (Bigram-I)
and PCFG model (PCFG-I) with MaxEnt to build
PCFG-E. For the Travel dataset, we find that the
performance of the PCFG-E model is equal to that
of the best constituent model (Bigram-I). For the
remaining datasets, the performance of PCFG-E
is better than the best constituent model. Further-
more, for the Football, Cricket and Poetry datasets
this improvement is quite substantial. We now
find that the performance of some variant of PCFG
is always better than or equal to that of the best
baseline. While the basic PCFG model outper-
forms the baseline for the Football dataset, PCFG-
E outperforms the best baseline for the Poetry
and Business datasets. For the Cricket and Travel
datasets, the performance of the PCFG-E model
equals that of the best baseline. In order to as-
sess the statistical significance of any performance
difference between the best PCFG model and the
best baseline, we performed the McNemar?s test,
a non-parametric test for binomial variables (Ros-
ner, 2005). We found that the difference in the
performance of the two methods was not statisti-
cally significant at .05 significance level for any of
the datasets, probably due to the small number of
test samples.
The performance of PCFG and PCFG-I is par-
ticularly impressive on the Football and Poetry
datasets. For the Football dataset, the basic PCFG
model is the best performing PCFG model and it
performs much better than other methods. It is sur-
prising that smoothing using PCFG-I actually re-
sults in a drop in performance on this dataset. We
hypothesize that the authors in the Football dataset
may have very different syntactic writing styles
that are effectively captured by the basic PCFG
model. Smoothing the data apparently weakens
this signal, hence causing a drop in performance.
For Poetry, PCFG-I achieves much higher accu-
racy than the baselines. This is impressive given
the much looser syntactic structure of poetry com-
pared to news articles, and it indicates the value of
syntactic information for distinguishing between
literary authors.
Finally, we consider the specific contribution of
the PCFG-I model towards the performance of
41
the PCFG-E ensemble. Based on comparing the
results for PCFG-E and MaxEnt+Bigram-I , we
find that there is a drop in performance for most
datasets when removing PCFG-I from the ensem-
ble. This drop is quite substantial for the Football
and Poetry datasets. This indicates that PCFG-I
is contributing substantially to the performance of
PCFG-E. Thus, it further illustrates the impor-
tance of broader syntactic information for the task
of authorship attribution.
4 Future Work and Conclusions
In this paper, we have presented our ongoing work
on authorship attribution, describing a novel ap-
proach that uses probabilistic context-free gram-
mars. We have demonstrated that both syntac-
tic and lexical information are useful in effec-
tively capturing authors? overall writing style. To
this end, we have developed an ensemble ap-
proach that performs better than the baseline mod-
els on several datasets. An interesting extension
of our current approach is to consider discrimina-
tive training of PCFGs for each author. Finally,
we would like to compare the performance of our
method to other state-of-the-art approaches to au-
thorship prediction.
Acknowledgments
Experiments were run on the Mastodon Cluster,
provided by NSF Grant EIA-0303609.
References
H. Baayen, H. van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: using syntactic annota-
tion to enhance authorship attribution. Literary and
Linguistic Computing, 11(3):121?132, September.
Binongo and Smith. 1999. A Study of Oscar Wilde?s
Writings. Journal of Applied Statistics, 26:781.
J Burrows. 1987. Word-patterns and Story-shapes:
The Statistical Analysis of Narrative Style.
Joachim Diederich, Jo?rg Kindermann, Edda Leopold,
and Gerhard Paass. 2000. Authorship Attribu-
tion with Support Vector Machines. Applied Intel-
ligence, 19:2003.
D. I. Holmes and R. S. Forsyth. 1995. The Federal-
ist Revisited: New Directions in Authorship Attri-
bution. Literary and Linguistic Computing, 10:111?
127.
Thorsten Joachims. 1998. Text categorization with
Support Vector Machines: Learning with many rel-
evant features. In Proceedings of the 10th European
Conference on Machine Learning (ECML), pages
137?142, Berlin, Heidelberg. Springer-Verlag.
Patrick Juola and John Sofko. 2004. Proving and
Improving Authorship Attribution Technologies. In
Proceedings of Canadian Symposium for Text Anal-
ysis (CaSTA).
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 423?430, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Dan Klein and Christopher D. Manning. 2003b. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), pages 3?10.
MIT Press.
Kim Luyckx and Walter Daelemans. 2008. Author-
ship Attribution and Verification with Many Authors
and Limited Data. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING), pages 513?520, August.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
the Federalist Papers. Springer-Verlag.
Andrew Y. Ng and Michael I. Jordan. 2001. On Dis-
criminative vs. Generative classifiers: A compari-
son of logistic regression and naive Bayes. In Ad-
vances in Neural Information Processing Systems 14
(NIPS), pages 841?848.
Fuchun Peng, Dale Schuurmans, Viado Keselj, and
Shaojun Wang. 2003. Language Independent
Authorship Attribution using Character Level Lan-
guage Models. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL).
Bernard Rosner. 2005. Fundamentals of Biostatistics.
Duxbury Press.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 1999.
Automatic Authorship Attribution. In Proceedings
of the 9th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 158?164, Morristown, NJ, USA. Association
for Computational Linguistics.
E. Stamatatos. 2009. A Survey of Modern Author-
ship Attribution Methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Rong Zheng, Yi Qin, Zan Huang, and Hsinchun
Chen. 2009. Authorship Analysis in Cybercrime
Investigation. Lecture Notes in Computer Science,
2665/2009:959.
42
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349?358,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning to ?Read Between the Lines? using Bayesian Logic Programs
Sindhu Raghavan Raymond J. Mooney Hyeonseo Ku
Department of Computer Science
The University of Texas at Austin
1616 Guadalupe, Suite 2.408
Austin, TX 78701, USA
{sindhu,mooney,yorq}@cs.utexas.edu
Abstract
Most information extraction (IE) systems
identify facts that are explicitly stated in text.
However, in natural language, some facts are
implicit, and identifying them requires ?read-
ing between the lines?. Human readers nat-
urally use common sense knowledge to in-
fer such implicit information from the explic-
itly stated facts. We propose an approach
that uses Bayesian Logic Programs (BLPs),
a statistical relational model combining first-
order logic and Bayesian networks, to infer
additional implicit information from extracted
facts. It involves learning uncertain common-
sense knowledge (in the form of probabilis-
tic first-order rules) from natural language text
by mining a large corpus of automatically ex-
tracted facts. These rules are then used to de-
rive additional facts from extracted informa-
tion using BLP inference. Experimental eval-
uation on a benchmark data set for machine
reading demonstrates the efficacy of our ap-
proach.
1 Introduction
The task of information extraction (IE) involves au-
tomatic extraction of typed entities and relations
from unstructured text. IE systems (Cowie and
Lehnert, 1996; Sarawagi, 2008) are trained to extract
facts that are stated explicitly in text. However, some
facts are implicit, and human readers naturally ?read
between the lines? and infer them from the stated
facts using commonsense knowledge. Answering
many queries can require inferring such implicitly
stated facts. Consider the text ?Barack Obama is the
president of the United States of America.? Given
the query ?Barack Obama is a citizen of what coun-
try??, standard IE systems cannot identify the an-
swer since citizenship is not explicitly stated in the
text. However, a human reader possesses the com-
monsense knowledge that the president of a country
is almost always a citizen of that country, and easily
infers the correct answer.
The standard approach to inferring implicit infor-
mation involves using commonsense knowledge in
the form of logical rules to deduce additional in-
formation from the extracted facts. Since manually
developing such a knowledge base is difficult and
arduous, an effective alternative is to automatically
learn such rules by mining a substantial database of
facts that an IE system has already automatically
extracted from a large corpus of text (Nahm and
Mooney, 2000). Most existing rule learners assume
that the training data is largely accurate and com-
plete. However, the facts extracted by an IE sys-
tem are always quite noisy and incomplete. Conse-
quently, a purely logical approach to learning and in-
ference is unlikely to be effective. Consequently, we
propose using statistical relational learning (SRL)
(Getoor and Taskar, 2007), specifically, Bayesian
Logic Programs (BLPs) (Kersting and De Raedt,
2007), to learn probabilistic rules in first-order logic
from a large corpus of extracted facts and then use
the resulting BLP to make effective probabilistic in-
ferences when interpreting new documents.
We have implemented this approach by using an
off-the-shelf IE system and developing novel adap-
tations of existing learning methods to efficiently
construct fast and effective BLPs for ?reading be-
349
tween the lines.? We present an experimental evalu-
ation of our resulting system on a realistic test cor-
pus from DARPA?s Machine Reading project, and
demonstrate improved performance compared to a
purely logical approach based on Inductive Logic
Programming (ILP) (Lavrac? and Dz?eroski, 1994),
and an alternative SRL approach based on Markov
Logic Networks (MLNs) (Domingos and Lowd,
2009).
To the best of our knowledge, this is the first paper
that employs BLPs for inferring implicit information
from natural language text. We demonstrate that it
is possible to learn the structure and the parameters
of BLPs automatically using only noisy extractions
from natural language text, which we then use to in-
fer additional facts from text.
The rest of the paper is organized as follows. Sec-
tion 2 discusses related work and highlights key dif-
ferences between our approach and existing work.
Section 3 provides a brief background on BLPs.
Section 4 describes our BLP-based approach to
learning to infer implicit facts. Section 5 describes
our experimental methodology and discusses the re-
sults of our evaluation. Finally, Section 6 discusses
potential future work and Section 7 presents our fi-
nal conclusions.
2 Related Work
Several previous projects (Nahm and Mooney, 2000;
Carlson et al, 2010; Schoenmackers et al, 2010;
Doppa et al, 2010; Sorower et al, 2011) have mined
inference rules from data automatically extracted
from text by an IE system. Similar to our approach,
these systems use the learned rules to infer addi-
tional information from facts directly extracted from
a document. Nahm and Mooney (2000) learn propo-
sitional rules using C4.5 (Quinlan, 1993) from data
extracted from computer-related job-postings, and
therefore cannot learn multi-relational rules with
quantified variables. Other systems (Carlson et al,
2010; Schoenmackers et al, 2010; Doppa et al,
2010; Sorower et al, 2011) learn first-order rules
(i.e. Horn clauses in first-order logic).
Carlson et al (2010) modify an ILP system simi-
lar to FOIL (Quinlan, 1990) to learn rules with prob-
abilistic conclusions. They use purely logical de-
duction (forward-chaining) to infer additional facts.
Unlike BLPs, this approach does not use a well-
founded probabilistic graphical model to compute
coherent probabilities for inferred facts. Further,
Carlson et al (2010) used a human judge to man-
ually evaluate the quality of the learned rules before
using them to infer additional facts. Our approach,
on the other hand, is completely automated and
learns fully parameterized rules in a well-defined
probabilistic logic.
Schoenmackers et al (2010) develop a system
called SHERLOCK that uses statistical relevance to
learn first-order rules. Unlike our system and others
(Carlson et al, 2010; Doppa et al, 2010; Sorower et
al., 2011) that use a pre-defined ontology, they auto-
matically identify a set of entity types and relations
using ?open IE.? They use HOLMES (Schoenmack-
ers et al, 2008), an inference engine based on MLNs
(Domingos and Lowd, 2009) (an SRL approach that
combines first-order logic and Markov networks)
to infer additional facts. However, MLNs include
all possible type-consistent groundings of the rules
in the corresponding Markov net, which, for larger
datasets, can result in an intractably large graphical
model. To overcome this problem, HOLMES uses
a specialized model construction process to control
the grounding process. Unlike MLNs, BLPs natu-
rally employ a more ?focused? approach to ground-
ing by including only those literals that are directly
relevant to the query.
Doppa et al (2010) use FARMER (Nijssen and
Kok, 2003), an existing ILP system, to learn first-
order rules. They propose several approaches to
score the rules, which are used to infer additional
facts using purely logical deduction. Sorower et al
(2011) propose a probabilistic approach to modeling
implicit information as missing facts and use MLNs
to infer these missing facts. They learn first-order
rules for the MLN by performing exhaustive search.
As mentioned earlier, inference using both these ap-
proaches, logical deduction and MLNs, have certain
limitations, which BLPs help overcome.
DIRT (Lin and Pantel, 2001) and RESOLVER
(Yates and Etzioni, 2007) learn inference rules, also
called entailment rules that capture synonymous re-
lations and entities from text. Berant et al (Berant
et al, 2011) propose an approach that uses transitiv-
ity constraints for learning entailment rules for typed
predicates. Unlike the systems described above,
350
these systems do not learn complex first-order rules
that capture common sense knowledge. Further,
most of these systems do not use extractions from
an IE system to learn entailment rules, thereby mak-
ing them less related to our approach.
3 Bayesian Logic Programs
Bayesian logic programs (BLPs) (Kersting and De
Raedt, 2007; Kersting and Raedt, 2008) can be con-
sidered as templates for constructing directed graph-
ical models (Bayes nets). Formally, a BLP con-
sists of a set of Bayesian clauses, definite clauses
of the form a|a1, a2, a3, .....an, where n ? 0 and
a, a1, a2, a3,......,an are Bayesian predicates (de-
fined below), and where a is called the head of
the clause (head(c)) and (a1, a2, a3,....,an) is the
body (body(c)). When n = 0, a Bayesian clause
is a fact. Each Bayesian clause c is assumed to
be universally quantified and range restricted, i.e
variables{head} ? variables{body}, and has an
associated conditional probability table CPT(c) =
P(head(c)|body(c)). A Bayesian predicate is a pred-
icate with a finite domain, and each ground atom for
a Bayesian predicate represents a random variable.
Associated with each Bayesian predicate is a com-
bining rule such as noisy-or or noisy-and that maps
a finite set of CPTs into a single CPT.
Given a knowledge base as a BLP, standard logi-
cal inference (SLD resolution) is used to automat-
ically construct a Bayes net for a given problem.
More specifically, given a set of facts and a query,
all possible Horn-clause proofs of the query are con-
structed and used to build a Bayes net for answering
the query. The probability of a joint assignment of
truth values to the final set of ground propositions is
defined as follows:
P(X) =
?
i P (Xi|Pa(Xi)),
where X = X1, X2, ..., Xn represents the set of
random variables in the network and Pa(Xi) rep-
resents the parents of Xi. Once a ground network is
constructed, standard probabilistic inference meth-
ods can be used to answer various types of queries
as reviewed by Koller and Friedman (2009). The
parameters in the BLP model can be learned using
the methods described by Kersting and De Raedt
(2008).
4 Learning BLPs to Infer Implicit Facts
4.1 Learning Rules from Extracted Data
The first step involves learning commonsense
knowledge in the form of first-order Horn rules from
text. We first extract facts that are explicitly stated
in the text using SIRE (Florian et al, 2004), an IE
system developed by IBM. We then learn first-order
rules from these extracted facts using LIME (Mc-
creath and Sharma, 1998), an ILP system designed
for noisy training data.
We first identify a set of target relations we want
to infer. Typically, an ILP system takes a set of
positive and negative instances for a target relation,
along with a background knowledge base (in our
case, other facts extracted from the same document)
from which the positive instances are potentially in-
ferable. In our task, we only have direct access to
positive instances of target relations, i.e the relevant
facts extracted from the text. So we artificially gen-
erate negative instances using the closed world as-
sumption, which states that any instance of a rela-
tion that is not extracted can be considered a nega-
tive instance. While there are exceptions to this as-
sumption, it typically generates a useful (if noisy)
set of negative instances. For each relation, we gen-
erate all possible type-consistent instances using all
constants in the domain. All instances that are not
extracted facts (i.e. positive instances) are labeled
as negative. The total number of such closed-world
negatives can be intractably large, so we randomly
sample a fixed-size subset. The ratio of 1:20 for
positive to negative instances worked well in our ap-
proach.
Since LIME can learn rules using only positive in-
stances, or both positive and negative instances, we
learn rules using both settings. We include all unique
rules learned from both settings in the final set, since
the goal of this step is to learn a large set of po-
tentially useful rules whose relative strengths will
be determined in the next step of parameter learn-
ing. Other approaches could also be used to learn
candidate rules. We initially tried using the popular
ALEPH ILP system (Srinivasan, 2001), but it did not
produce useful rules, probably due to the high level
of noise in our training data.
351
4.2 Learning BLP Parameters
The parameters of a BLP include the CPT entries as-
sociated with the Bayesian clauses and the parame-
ters of combining rules associated with the Bayesian
predicates. For simplicity, we use a deterministic
logical-and model to encode the CPT entries associ-
ated with Bayesian clauses, and use noisy-or to com-
bine evidence coming from multiple ground rules
that have the same head (Pearl, 1988). The noisy-
or model requires just a single parameter for each
rule, which can be learned from training data.
We learn the noisy-or parameters using the EM
algorithm adapted for BLPs by Kersting and De
Raedt (2008). In our task, the supervised training
data consists of facts that are extracted from the
natural language text. However, we usually do not
have evidence for inferred facts as well as noisy-or
nodes. As a result, there are a number of variables in
the ground networks which are always hidden, and
hence EM is appropriate for learning the requisite
parameters from the partially observed training data.
4.3 Inference of Additional Facts using BLPs
Inference in the BLP framework involves backward
chaining (Russell and Norvig, 2003) from a spec-
ified query (SLD resolution) to obtain all possi-
ble deductive proofs for the query. In our context,
each target relation becomes a query on which we
backchain. We then construct a ground Bayesian
network using the resulting deductive proofs for
all target relations and learned parameters using
the standard approach described in Section 3. Fi-
nally, we perform standard probabilistic inference
to estimate the marginal probability of each inferred
fact. Our system uses Sample Search (Gogate and
Dechter, 2007), an approximate sampling algorithm
developed for Bayesian networks with determinis-
tic constraints (0 values in CPTs). We tried several
exact and approximate inference algorithms on our
data, and this was the method that was both tractable
and produced the best results.
5 Experimental Evaluation
5.1 Data
For evaluation, we used DARPA?s machine-reading
intelligence-community (IC) data set, which con-
sists of news articles on terrorist events around the
world. There are 10, 000 documents each contain-
ing an average of 89.5 facts extracted by SIRE (Flo-
rian et al, 2004). SIRE assigns each extracted fact
a confidence score and we used only those with a
score of 0.5 or higher for learning and inference. An
average of 86.8 extractions per document meet this
threshold.
DARPA also provides an ontology describing the
entities and relations in the IC domain. It con-
sists of 57 entity types and 79 relations. The
entity types include Agent, PhysicalThing, Event,
TimeLocation, Gender, and Group, each with sev-
eral subtypes. The type hierarchy is a DAG rather
than a tree, and several types have multiple super-
classes. For instance, a GeopoliticalEntity can be
a HumanAgent as well as a Location. This can
cause some problems for systems that rely on a
strict typing system, such as MLNs which rely on
types to limit the space of ground literals that are
considered. Some sample relations are attended-
School, approximateNumberOfMembers, mediatin-
gAgent, employs, hasMember, hasMemberHuman-
Agent, and hasBirthPlace.
5.2 Methodology
We evaluated our approach using 10-fold cross vali-
dation. We learned first-order rules for the 13 tar-
get relations shown in Table 3 from the facts ex-
tracted from the training documents (Section 4.1).
These relations were selected because the extrac-
tor?s recall for them was low. Since LIME does not
scale well to large data sets, we could train it on
at most about 2, 500 documents. Consequently, we
split the 9, 000 training documents into four disjoint
subsets and learned first-order rules from each sub-
set. The final knowledge base included all unique
rules learned from any subset. LIME learned sev-
eral rules that had only entity types in their bodies.
Such rules make many incorrect inferences; hence
we eliminated them. We also eliminated rules vio-
lating type constraints. We learned an average of 48
rules per fold. Table 1 shows some sample learned
rules.
We then learned parameters as described in Sec-
tion 4.2. We initially set al noisy-or parameters to
0.9 based on the intuition that if exactly one rule for
a consequent was satisfied, it could be inferred with
a probability of 0.9.
352
governmentOrganization(A) ? employs(A,B)? hasMember(A,B)
If a government organization A employs person B, then B is a member of A
eventLocation(A,B) ? bombing(A)? thingPhysicallyDamaged(A,B)
If a bombing event A took place in location B, then B is physically damaged
isLedBy(A,B)? hasMemberPerson(A,B)
If a group A is led by person B, then B is a member of A
nationState(B) ? eventLocationGPE(A,B)? eventLocation(A,B)
If an event A occurs in a geopolitical entity B, then the event location for that event is B
mediatingAgent(A,B) ? humanAgentKillingAPerson(A)? killingHumanAgent(A,B)
If A is an event in which a human agent is killing a person and the mediating agent of A is an agent B, then B is
the human agent that is killing in event A
Table 1: A sample set of rules learned using LIME
For each test document, we performed BLP in-
ference as described in Section 4.3. We ranked all
inferences by their marginal probability, and evalu-
ated the results by either choosing the top n infer-
ences or accepting inferences whose marginal prob-
ability was equal to or exceeded a specified thresh-
old. We evaluated two BLPs with different param-
eter settings: BLP-Learned-Weights used noisy-or
parameters learned using EM, BLP-Manual-Weights
used fixed noisy-or weights of 0.9.
5.3 Evaluation Metrics
The lack of ground truth annotation for inferred facts
prevents an automated evaluation, so we resorted
to a manual evaluation. We randomly sampled 40
documents (4 from each test fold), judged the ac-
curacy of the inferences for those documents, and
computed precision, the fraction of inferences that
were deemed correct. For probabilistic methods like
BLPs and MLNs that provide certainties for their
inferences, we also computed precision at top n,
which measures the precision of the n inferences
with the highest marginal probability across the 40
test documents. Measuring recall for making infer-
ences is very difficult since it would require labeling
a reasonable-sized corpus of documents with all of
the correct inferences for a given set of target rela-
tions, which would be extremely time consuming.
Our evaluation is similar to that used in previous re-
lated work (Carlson et al, 2010; Schoenmackers et
al., 2010).
SIRE frequently makes incorrect extractions, and
therefore inferences made from these extractions are
also inaccurate. To account for the mistakes made
by the extractor, we report two different precision
scores. The ?unadjusted? (UA) score, does not cor-
rect for errors made by the extractor. The ?adjusted?
(AD) score does not count mistakes due to extraction
errors. That is, if an inference is incorrect because
it was based on incorrect extracted facts, we remove
it from the set of inferences and calculate precision
for the remaining inferences.
5.4 Baselines
Since none of the existing approaches have been
evaluated on the IC data, we cannot directly compare
our performance to theirs. Therefore, we compared
to the following methods:
? Logical Deduction: This method forward
chains on the extracted facts using the first-
order rules learned by LIME to infer additional
facts. This approach is unable to provide any
confidence or probability for its conclusions.
? Markov Logic Networks (MLNs): We use the
rules learned by LIME to define the structure
of an MLN. In the first setting, which we call
MLN-Learned-Weights, we learn the MLN?s
parameters using the generative weight learn-
ing algorithm (Domingos and Lowd, 2009),
which we modified to process training exam-
ples in an online manner. In online generative
learning, gradients are calculated and weights
are estimated after processing each example
and the learned weights are used as the start-
ing weights for the next example. The pseudo-
likelihood of one round is obtained by multi-
plying the pseudo-likelihood of all examples.
353
UA AD
Precision 29.73 (443/1490) 35.24 (443/1257)
Table 2: Precision for logical deduction. ?UA? and ?AD?
refer to the unadjusted and adjusted scores respectively
In our approach, the initial weights of clauses
are set to 10. The average number of itera-
tions needed to acquire the optimal weights is
131. In the second setting, which we call MLN-
Manual-Weights, we assign a weight of 10 to
all rules and maximum likelihood prior to all
predicates. MLN-Manual-Weights is similar to
BLP-Manual-Weights in that all rules are given
the same weight. We then use the learned rules
and parameters to probabilistically infer addi-
tional facts using the MC-SAT algorithm im-
plemented in Alchemy,1 an open-source MLN
package.
6 Results and Discussion
6.1 Comparison to Baselines
Table 2 gives the unadjusted (UA) and adjusted
(AD) precision for logical deduction. Out of 1, 490
inferences for the 40 evaluation documents, 443
were judged correct, giving an unadjusted preci-
sion of 29.73%. Out of these 1, 490 inferences, 233
were determined to be incorrect due to extraction er-
rors, improving the adjusted precision to a modest
35.24%.
MLNs made about 127, 000 inferences for the 40
evaluation documents. Since it is not feasible to
manually evaluate all the inferences made by the
MLN, we calculated precision using only the top
1000 inferences. Figure 1 shows both unadjusted
and adjusted precision at top-n for various values
of n for different BLP and MLN models. For both
BLPs and MLNs, simple manual weights result in
superior performance than the learned weights. De-
spite the fairly large size of the overall training sets
(9,000 documents), the amount of data for each
target relation is apparently still not sufficient to
learn particularly accurate weights for both BLPs
and MLNs. However, for BLPs, learned weights
do show a substantial improvement initially (i.e.
1http://alchemy.cs.washington.edu/
top 25?50 inferences), with an average of 1 infer-
ence per document at 91% adjusted precision as
opposed to an average of 5 inferences per docu-
ment at 85% adjusted precision for BLP-Manual-
Weights. For MLNs, learned weights show a small
improvement initially only with respect to adjusted
precision. Between BLPs and MLNs, BLPs per-
form substantially better than MLNs at most points
in the curve. However, MLN-Manual-Weights im-
prove marginally over BLP-Learned-Weights at later
points (top 600 and above) on the curve, where the
precision is generally very low. Here, the superior
performance of BLPs over MLNs could be possibly
due to the focused grounding used in the BLP frame-
work.
For BLPs, as n increases towards including all of
the logically sanctioned inferences, as expected, the
precision converges to the results for logical deduc-
tion. However, as n decreases, both adjusted and
unadjusted precision increase fairly steadily. This
demonstrates that probabilistic BLP inference pro-
vides a clear improvement over logical deduction,
allowing the system to accurately select the best in-
ferences that are most likely to be correct. Unlike the
two BLP models, MLN-Manual-Weights has more
or less the same performance at most points on the
curve, and it is slightly better than that of purely-
logical deduction. MLN-Learned-Weights is worse
than purely-logical deduction at most points on the
curve.
6.2 Results for Individual Target Relations
Table 3 shows the adjusted precision for each
relation for instances inferred using logical de-
duction, BLP-Manual-Weights and BLP-Learned-
Weights with a confidence threshold of 0.95. The
probabilities estimated for inferences by MLNs are
not directly comparable to those estimated by BLPs.
As a result, we do not include results for MLNs
here. For this evaluation, using a confidence thresh-
old based cutoff is more appropriate than using top-
n inferences made by the BLP models since the esti-
mated probabilities can be directly compared across
target relations.
For logical deduction, precision is high for a few
relations like employs, hasMember, and hasMem-
berHumanAgent, indicating that the rules learned
for these relations are more accurate than the ones
354
0 100 200 300 400 500 600 700 800 900 10000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Top?n inferences
Una
djust
ed Pr
ecisio
n
 
 BLP?Learned?WeightsBLP?Manual?WeightsMLN?Learned?WeightsMLN?Manual?Weights
0 100 200 300 400 500 600 700 800 900 10000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Top?n inferences
Adjus
ted P
recisi
on
 
 BLP?Learned?WeightsBLP?Manual?WeightsMLN?Learned?WeightsMLN?Manual?Weights
Figure 1: Unadjusted and adjusted precision at top-n for different BLP and MLN models for various values of n
learned for the remaining relations. Unlike rela-
tions like hasMember that are easily inferred from
relations like employs and isLedBy, certain relations
like hasBirthPlace are not easily inferable using the
information in the ontology. As a result, it might
not be possible to learn accurate rules for such tar-
get relations. Other reasons include the lack of a
sufficiently large number of target-relation instances
during training and lack of strictly defined types in
the IC ontology.
Both BLP-Manual-Weights and BLP-Learned-
Weights also have high precision for several re-
lations (eventLocation, hasMemberHumanAgent,
thingPhysicallyDamaged). However, the actual
number of inferences can be fairly low. For in-
stance, 103 instances of hasMemberHumanAgent
are inferred by logical deduction (i.e. 0 confidence
threshold), but only 2 of them are inferred by BLP-
Learned-Weights at 0.95 confidence threshold, in-
dicating that the parameters learned for the corre-
sponding rules are not very high. For several rela-
tions like hasMember, hasMemberPerson, and em-
ploys, no instances were inferred by BLP-Learned-
Weights at 0.95 confidence threshold. Lack of suffi-
cient training instances (extracted facts) is possibly
the reason for learning low weights for such rules.
On the other hand, BLP-Manual-Weights has in-
ferred 26 instances of hasMemberHumanAgent, out
which all are correct. These results therefore demon-
strate the need for sufficient training examples to
learn accurate parameters.
6.3 Discussion
We now discuss the potential reasons for BLP?s su-
perior performance compared to other approaches.
Probabilistic reasoning used in BLPs allows for a
principled way of determining the most confident
inferences, thereby allowing for improved precision
over purely logical deduction. The primary dif-
ference between BLPs and MLNs lies in the ap-
proaches used to construct the ground network. In
BLPs, only propositions that can be logically de-
duced from the extracted evidence are included in
the ground network. On the other hand, MLNs in-
clude all possible type-consistent groundings of all
rules in the network, introducing many ground liter-
als which cannot be logically deduced from the ev-
idence. This generally results in several incorrect
inferences, thereby yielding poor performance.
Even though learned weights in BLPs do not re-
sult in a superior performance, learned weights in
MLNs are substantially worse. Lack of sufficient
training data is one of the reasons for learning less
accurate weights by the MLN weight learner. How-
ever, a more important issue is due to the use of the
closed world assumption during learning, which we
believe is adversely impacting the weights learned.
As mentioned earlier, for the task considered in the
paper, if a fact is not explicitly stated in text, and
hence not extracted by the extractor, it does not nec-
essarily imply that it is not true. Since existing
weight learning approaches for MLNs do not deal
with missing data and open world assumption, de-
veloping such approaches is a topic for future work.
Apart from developing novel approaches for
355
Relation Logical Deduction BLP-Manual-Weights-.95 BLP-Learned-Weights-.95 No. training instances
employs 69.44 (25/36) 92.85 (13/14) nil (0/0) 18440
eventLocation 18.75 (18/96) 100.00 (1/1) 100 (1/1) 6902
hasMember 95.95 (95/99) 97.26 (71/73) nil (0/0) 1462
hasMemberPerson 43.75 (42/96) 100.00 (14/14) nil (0/0) 705
isLedBy 12.30 (8/65) nil (0/0) nil (0/0) 8402
mediatingAgent 19.73 (15/76) nil (0/0) nil (0/0) 92998
thingPhysicallyDamaged 25.72 (62/241) 90.32 (28/31) 90.32 (28/31) 24662
hasMemberHumanAgent 95.14 (98/103) 100.00 (26/26) 100.00 (2/2) 3619
killingHumanAgent 15.35 (43/280) 33.33 (2/6) 66.67 (2/3) 3341
hasBirthPlace 0 (0/88) nil (0/0) nil (0/0) 89
thingPhysicallyDestroyed nil (0/0) nil (0/0) nil (0/0) 800
hasCitizenship 48.05 (37/77) 58.33 (35/60) nil (0/0) 222
attendedSchool nil (0/0) nil (0/0) nil (0/0) 2
Table 3: Adjusted precision for individual relations (highest values are in bold)
weight learning, additional engineering could poten-
tially improve the performance of MLNs on the IC
data set. Due to MLN?s grounding process, sev-
eral spurious facts like employs(a,a) were inferred.
These inferences can be prevented by including ad-
ditional clauses in the MLN that impose integrity
constraints that prevent such nonsensical proposi-
tions. Further, techniques proposed by Sorower et
al. (2011) can be incorporated to explicitly han-
dle missing information in text. Lack of strict typ-
ing on the arguments of relations in the IC ontol-
ogy has also resulted in inferior performance of the
MLNs. To overcome this, relations that do not have
strictly defined types could be specialized. Finally,
we could use the deductive proofs constructed by
BLPs to constrain the ground Markov network, sim-
ilar to the model-construction approach adopted by
Singla and Mooney (2011).
However, in contrast to MLNs, BLPs that use
first-order rules that are learned by an off-the-shelf
ILP system and given simple intuitive hand-coded
weights, are able to provide fairly high-precision in-
ferences that augment the output of an IE system and
allow it to effectively ?read between the lines.?
7 Future Work
A primary goal for future research is developing an
on-line structure learner for BLPs that can directly
learn probabilistic first-order rules from uncertain
training data. This will address important limita-
tions of LIME, which cannot accept uncertainty in
the extractions used for training, is not specifically
optimized for learning rules for BLPs, and does not
scale well to large datasets. Given the relatively poor
performance of BLP parameters learned using EM,
tests on larger training corpora of extracted facts and
the development of improved parameter-learning al-
gorithms are clearly indicated. We also plan to per-
form a larger-scale evaluation by employing crowd-
sourcing to evaluate inferred facts for a bigger cor-
pus of test documents. As described above, a num-
ber of methods could be used to improve the per-
formance of MLNs on this task. Finally, it would
be useful to evaluate our methods on several other
diverse domains.
8 Conclusions
We have introduced a novel approach using
Bayesian Logic Programs to learn to infer implicit
information from facts extracted from natural lan-
guage text. We have demonstrated that it can learn
effective rules from a large database of noisy extrac-
tions. Our experimental evaluation on the IC data
set demonstrates the advantage of BLPs over logical
deduction and an approach based on MLNs.
Acknowledgements
We thank the SIRE team from IBM for providing SIRE
extractions on the IC data set. This research was funded
by MURI ARO grant W911NF-08-1-0242 and Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program. Experiments were run on the
Mastodon Cluster, provided by NSF grant EIA-0303609.
356
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACl-HLT 2011), pages 610?619.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010. Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Conference on Artificial Intelligence
(AAAI), pages 1306?1313. AAAI Press.
Jim Cowie and Wendy Lehnert. 1996. Information ex-
traction. CACM, 39(1):80?91.
P. Domingos and D. Lowd. 2009. Markov Logic: An
Interface Layer for Artificial Intelligence. Morgan &
Claypool, San Rafael, CA.
Janardhan Rao Doppa, Mohammad NasrEsfahani, Mo-
hammad S. Sorower, Thomas G. Dietterich, Xiaoli
Fern, and Prasad Tadepalli. 2010. Towards learn-
ing rules from natural texts. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing (FAM-LbR 2010), pages 70?77, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A statisti-
cal model for multilingual entity detection and track-
ing. In Proceedings of Human Language Technolo-
gies: The Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL-HLT 2004), pages 1?8.
L. Getoor and B. Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Vibhav Gogate and Rina Dechter. 2007. Samplesearch:
A scheme that searches for consistent samples. In Pro-
ceedings of Eleventh International Conference on Ar-
tificial Intelligence and Statistics (AISTATS-07).
K. Kersting and L. De Raedt. 2007. Bayesian Logic
Programming: Theory and tool. In L. Getoor and
B. Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press, Cambridge, MA.
Kristian Kersting and Luc De Raedt. 2008. Basic princi-
ples of learning Bayesian Logic Programs. Springer-
Verlag, Berlin, Heidelberg.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
Nada Lavrac? and Saso Dz?eroski. 1994. Inductive Logic
Programming: Techniques and Applications. Ellis
Horwood.
Deaking Lin and Patrick Pantel. 2001. Discovery of
inference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Eric Mccreath and Arun Sharma. 1998. Lime: A system
for learning relations. In Ninth International Work-
shop on Algorithmic Learning Theory, pages 336?374.
Springer-Verlag.
Un Yong Nahm and Raymond J. Mooney. 2000. A mu-
tually beneficial integration of data mining and infor-
mation extraction. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence (AAAI
2000), pages 627?632, Austin, TX, July.
Siegfried Nijssen and Joost N. Kok. 2003. Efficient fre-
quent query discovery in FARMER. In Proceedings
of the Seventh Conference in Principles and Practices
of Knowledge Discovery in Database (PKDD 2003),
pages 350?362. Springer.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo,CA.
J. Ross Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239?266.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo,CA.
Stuart Russell and Peter Norvig. 2003. Artificial Intel-
ligence: A Modern Approach. Prentice Hall, Upper
Saddle River, NJ, 2 edition.
S. Sarawagi. 2008. Information extraction. Foundations
and Trends in Databases, 1(3):261?377.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2008),
pages 79?88, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order Horn
clauses from web text. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2010), pages 1088?1098, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Parag Singla and Raymond Mooney. 2011. Abductive
Markov Logic for plan recognition. In Twenty-fifth
National Conference on Artificial Intelligence.
Mohammad S. Sorower, Thomas G. Dietterich, Janard-
han Rao Doppa, Orr Walker, Prasad Tadepalli, and Xi-
aoli Fern. 2011. Inverting Grice?s maxims to learn
rules from natural language extractions. In Proceed-
ings of Advances in Neural Information Processing
Systems 24.
A. Srinivasan, 2001. The Aleph manual.
http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/.
Alexander Yates and Oren Etzioni. 2007. Unsupervised
resolution of objects and relations on the web. In Pro-
357
ceedings of Human Language Technologies: The An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL-
HLT 2007).
358
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 218?227,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adapting Discriminative Reranking to Grounded Language Learning
Joohyun Kim
Department of Computer Science
The University of Texas at Austin
Austin, TX 78701, USA
scimitar@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
Austin, TX 78701, USA
mooney@cs.utexas.edu
Abstract
We adapt discriminative reranking to im-
prove the performance of grounded lan-
guage acquisition, specifically the task of
learning to follow navigation instructions
from observation. Unlike conventional
reranking used in syntactic and semantic
parsing, gold-standard reference trees are
not naturally available in a grounded set-
ting. Instead, we show how the weak su-
pervision of response feedback (e.g. suc-
cessful task completion) can be used as
an alternative, experimentally demonstrat-
ing that its performance is comparable to
training on gold-standard parse trees.
1 Introduction
Grounded language acquisition involves learn-
ing to comprehend and/or generate language by
simply observing its use in a naturally occur-
ring context in which the meaning of a sentence
is grounded in perception and/or action (Roy,
2002; Yu and Ballard, 2004; Gold and Scassel-
lati, 2007; Chen et al, 2010). Bo?rschinger et
al. (2011) introduced an approach that reduces
grounded language learning to unsupervised prob-
abilistic context-free grammar (PCFG) induction
and demonstrated its effectiveness on the task of
sportscasting simulated robot soccer games. Sub-
sequently, Kim and Mooney (2012) extended their
approach to make it tractable for the more complex
problem of learning to follow natural-language
navigation instructions from observations of hu-
mans following such instructions in a virtual envi-
ronment (Chen and Mooney, 2011). The observed
sequence of actions provides very weak, ambigu-
ous supervision for learning instructional language
since there are many possible ways to describe the
same execution path. Although their approach im-
proved accuracy on the navigation task compared
to the original work of Chen and Mooney (2011),
it was still far from human performance.
Since their system employs a generative model,
discriminative reranking (Collins, 2000) could po-
tentially improve its performance. By training a
discriminative classifier that uses global features
of complete parses to identify correct interpreta-
tions, a reranker can significantly improve the ac-
curacy of a generative model. Reranking has been
successfully employed to improve syntactic pars-
ing (Collins, 2002b), semantic parsing (Lu et al,
2008; Ge and Mooney, 2006), semantic role la-
beling (Toutanova et al, 2005), and named entity
recognition (Collins, 2002c). Standard reranking
requires gold-standard interpretations (e.g. parse
trees) to train the discriminative classifier. How-
ever, grounded language learning does not provide
gold-standard interpretations for the training ex-
amples. Only the ambiguous perceptual context
of the utterance is provided as supervision. For
the navigation task, this supervision consists of
the observed sequence of actions taken by a hu-
man when following an instruction. Therefore, it
is impossible to directly apply conventional dis-
criminative reranking to such problems. We show
how to adapt reranking to work with such weak
supervision. Instead of using gold-standard an-
notations to determine the correct interpretations,
we simply prefer interpretations of navigation in-
structions that, when executed in the world, actu-
ally reach the intended destination. Additionally,
we extensively revise the features typically used in
parse reranking to work with the PCFG approach
to grounded language learning.
The rest of the paper is organized as fol-
lows: Section 2 reviews the navigation task and
the PCFG approach to grounded language learn-
ing. Section 3 presents our modified approach to
reranking and Section 4 describes the novel fea-
tures used to evaluate parses. Section 5 experi-
mentally evaluates the approach comparing to sev-
218
(a) Sample virtual world of hallways with varying tiles,
wallpapers, and landmark objects indicated by letters
(e.g. ?H? for hat-rack) and illustrating a sample path
taken by a human follower.
(b) A sample natural language instruction and its formal land-
marks plan for the path illustrated above. The subset corre-
sponding to the correct formal plan is shown in bold.
Figure 1: Sample virtual world and instruction.
eral baselines. Finally, Section 6 describes related
work, Section 7 discusses future work, and Sec-
tion 8 concludes.
2 Background
2.1 Navigation Task
We address the navigation learning task intro-
duced by Chen and Mooney (2011). The goal is
to interpret natural-language (NL) instructions in a
virtual environment, thereby allowing a simulated
robot to navigate to a specified location. Figure 1a
shows a sample path executed by a human follow-
ing the instruction in Figure 1b. Given no prior lin-
guistic knowledge, the task is to learn to interpret
such instructions by simply observing humans fol-
low sample directions. Formally speaking, given
training examples of the form (ei, ai, wi), where
ei is an NL instruction, ai is an executed action
sequence for the instruction, and wi is the initial
world state, we want to learn to produce an appro-
priate action sequence aj given a novel (ej , wj).
More specifically, one must learn a seman-
tic parser that produces a plan pj using a for-
mal meaning representation (MR) language intro-
duced by Chen and Mooney (2011). This plan is
then executed by a simulated robot in a virtual en-
vironment. The MARCO system, introduced by
MacMahon et al (2006), executes the formal plan,
flexibly adapting to situations encountered dur-
ing execution and producing the action sequence
aj . During learning, Chen and Mooney construct
a landmarks plan ci for each training example,
which includes the complete context observed in
the world-state resulting from each observed ac-
tion. The correct plan, pi, (which is latent and
must be inferred) is assumed to be composed from
a subset of the components in the corresponding
landmarks plan. The landmarks and correct plans
for a sample instruction are shown in Figure 1b.
2.2 PCFG Induction for Grounded Language
Learning
The baseline generative model we use for rerank-
ing employs the unsupervised PCFG induction ap-
proach introduced by Kim and Mooney (2012).
This model is, in turn, based on the earlier model
of Bo?rschinger et al (2011), which transforms
the grounded language learning into unsupervised
PCFG induction. The general approach uses
grammar-formulation rules which construct CFG
productions that form a grammar that effectively
maps NL sentences to formal meaning represen-
tations (MRs) encoded in its nonterminals. After
using Expectation-Maximization (EM) to estimate
the parameters for these productions using the am-
biguous supervision provided by the grounded-
learning setting, it produces a PCFG whose most
probable parse for a sentence encodes its correct
semantic interpretation. Unfortunately, the initial
approach of Bo?rschinger et al (2011) produces ex-
plosively large grammars when applied to more
complex problems, such as our navigation task.
Therefore, Kim and Mooney enhanced their ap-
proach to use a previously learned semantic lexi-
con to reduce the induced grammar to a tractable
size. They also altered the processes for construct-
ing productions and mapping parse trees to MRs in
order to make the construction of semantic inter-
pretations more compositional and allow the ef-
ficient construction of more complex representa-
219
Figure 2: Simplified parse for the sentence ?Turn
left and find the sofa then turn around the corner?
for Kim and Mooney?s model. Nonterminals show
the MR graph, where additional nonterminals for
generating NL words are omitted.
tions.
The resulting PCFG can be used to produce
a set of most-probable interpretations of instruc-
tional sentences for the navigation task. Our pro-
posed reranking model is used to discriminatively
reorder the top parses produced by this generative
model. A simplified version of a sample parse tree
for Kim and Mooney?s model is shown in Figure 2.
3 Modified Reranking Algorithm
In reranking, a baseline generative model is first
trained and generates a set of candidate outputs
for each training example. Next, a second con-
ditional model is trained which uses global fea-
tures to rescore the candidates. Reranking using
an averaged perceptron (Collins, 2002a) has been
successfully applied to a variety of NLP tasks.
Therefore, we modify it to rerank the parse trees
generated by Kim and Mooney (2012)?s model.
The approach requires three subcomponents: 1)
a GEN function that returns the list of top n can-
didate parse trees for each NL sentence produced
by the generative model, 2) a feature function ?
that maps a NL sentence, e, and a parse tree, y,
into a real-valued feature vector ?(e, y) ? Rd, and
3) a reference parse tree that is compared to the
highest-scoring parse tree during training.
However, grounded language learning tasks,
such as our navigation task, do not provide ref-
erence parse trees for training examples. Instead,
our modified model replaces the gold-standard ref-
erence parse with the ?pseudo-gold? parse tree
Algorithm 1 AVERAGED PERCEPTRON TRAIN-
ING WITH RESPONSE-BASED UPDATE
Input: A set of training examples (ei, y?i ),
where ei is a NL sentence and y?i =
arg maxy?GEN(ei) EXEC(y)Output: The parameter vector W? , averaged
over all iterations 1...T
1: procedure PERCEPTRON
2: Initialize W? = 0
3: for t = 1...T, i = 1...n do
4: yi = arg maxy?GEN(ei) ?(ei, y) ? W?
5: if yi 6= y?i then
6: W? = W? + ?(ei, y?i )? ?(ei, yi)
7: end if
8: end for
9: end procedure
whose derived MR plan is most successful at get-
ting to the desired goal location. Thus, the third
component in our reranking model becomes an
evaluation function EXEC that maps a parse tree
y into a real number representing the success rate
(w.r.t. successfully reaching the intended destina-
tion) of the derived MR plan m composed from
y.
Additionally, we improve the perceptron train-
ing algorithm by using multiple reference parses
to update the weight vector W? . Although
we determine the pseudo-gold reference tree to
be the candidate parse y? such that y? =
arg maxy?GEN(e) EXEC(y), it may not actually be
the correct parse for the sentence. Other parses
may contain useful information for learning, and
therefore we devise a way to update weights us-
ing all candidate parses whose successful execu-
tion rate is greater than the parse preferred by the
currently learned model.
3.1 Response-Based Weight Updates
To circumvent the need for gold-standard refer-
ence parses, we select a pseudo-gold parse from
the candidates produced by the GEN function. In a
similar vein, when reranking semantic parses, Ge
and Mooney (2006) chose as a reference parse the
one which was most similar to the gold-standard
semantic annotation. However, in the navigation
task, the ultimate goal is to generate a plan that,
when actually executed in the virtual environment,
leads to the desired destination. Therefore, the
pseudo-gold reference is chosen as the candidate
parse that produces the MR plan with the great-
220
est execution success. This requires an external
module that evaluates the execution accuracy of
the candidate parses. For the navigation task, we
use the MARCO (MacMahon et al, 2006) ex-
ecution module, which is also used to evaluate
how well the overall system learns to follow direc-
tions (Chen and Mooney, 2011). Since MARCO
is nondeterministic when executing underspecified
plans, we execute each candidate plan 10 times,
and its execution rate is the percentage of trials
in which it reaches the correct destination. When
there are multiple candidate parses tied for the
highest execution rate, the one assigned the largest
probability by the baseline model is selected. Our
modified averaged perceptron procedure with such
a response-based update is shown in Algorithm 1.
One additional issue must be addressed when
computing the output of the GEN function. The fi-
nal plan MRs are produced from parse trees using
compositional semantics (see Kim and Mooney
(2012) for details). Consequently, the n-best parse
trees for the baseline model do not necessarily pro-
duce the n-best distinct plans, since many parses
can produce the same plan. Therefore, we adapt
the GEN function to produce the n best distinct
plans rather than the n best parses. This may
require examining many more than the n best
parses, because many parses have insignificant
differences that do not affect the final plan. The
score assigned to a plan is the probability of the
most probable parse that generates that plan. In
order to efficiently compute the n best plans, we
modify the exact n-best parsing algorithm devel-
oped by Huang and Chiang (2005). The modified
algorithm ensures that each plan in the computed
n best list produces a new distinct plan.
3.2 Weight Updates Using Multiple Parses
Typically, when used for reranking, the averaged
perceptron updates its weights using the feature-
vector difference between the current best pre-
dicted candidate and the gold-standard reference
(line 6 in Algorithm 1). In our initial modified
version, we replaced the gold-standard reference
parse with the pseudo-gold reference, which has
the highest execution rate amongst all candidate
parses. However, this ignores all other candidate
parses during perceptron training. However, it is
not ideal to regard other candidate parses as ?use-
less.? There may be multiple candidate parses with
the same maximum execution rate, and even can-
didates with lower execution rates could represent
the correct plan for the instruction given the weak,
indirect supervision provided by the observed se-
quence of human actions.
Therefore, we also consider a further mod-
ification of the averaged perceptron algorithm
which updates its weights using multiple candi-
date parses. Instead of only updating the weights
with the single difference between the predicted
and pseudo-gold parses, the weight vector W? is
updated with the sum of feature-vector differences
between the current predicted candidate and all
other candidates that have a higher execution rate.
Formally, in this version, we replace lines 5?6 of
Algorithm 1 with:
1: for all y ? GEN(ei) where y 6= yi and
EXEC(y) > EXEC(yi) do
2: W? = W? + (EXEC(y)? EXEC(yi))
?(?(ei, y)? ?(ei, yi))
3: end for
where EXEC(y) is the execution rate of the MR
plan m derived from parse tree y.
In the experiments below, we demonstrate that,
by exploiting multiple reference parses, this new
update rule increases the execution accuracy of
the final system. Intuitively, this approach gathers
additional information from all candidate parses
with higher execution accuracy when learning the
discriminative reranker. In addition, as shown in
line 2 of the algorithm above, it uses the differ-
ence in execution rates between a candidate and
the currently preferred parse to weight the update
to the parameters for that candidate. This allows
more effective plans to have a larger impact on the
learned model in each iteration.
4 Reranking Features
This section describes the features ? extracted
from parses produced by the generative model and
used to rerank the candidates.
4.1 Base Features
The base features adapt those used in previous
reranking methods, specifically those of Collins
(2002a), Lu et al (2008), and Ge and Mooney
(2006), which are directly extracted from parse
trees. In addition, we also include the log prob-
ability of the parse tree as an additional feature.
Figure 3 shows a sample full parse tree from our
baseline model, which is used when explaining the
221
L1: Turn(LEFT), Verify(front : SOFA, back : EASEL),
Travel(steps : 2), Verify(at : SOFA), Turn(RIGHT)
L6: Turn()
PhraseL6
WordL6
corner
PhXL6
Word?
the
PhXL6
WordL6
around
PhXL6
WordL6
turn
PhXL6
Word?
then
L3: Travel(steps : 2),
Verify(at : SOFA), Turn(RIGHT)
L5: Travel(), Verify(at : SOFA)
PhraseL5
WordL5
sofa
PhXL5
Word?
the
PhXL5
WordL5
find
L2: Turn(LEFT),
Verify(front : SOFA)
L4: Turn(LEFT)
PhraseL4
Word?
and
PhL4
WordL4
left
PhXL4
WordL4
Turn
Figure 3: Sample full parse tree for the sentence ?Turn left and find the soft then turn around the corner?
used to explain reranking features. Nonterminals representing MR plan components are shown, which
are labeled L1 to L6 for ease of reference. Additional nonterminals such as Phrase, Ph, PhX , and
Word are subsidiary ones for generating NL words from MR nonterminals. They are also shown in
order to represent the entire process of how parse trees are constructed (for details, refer to Kim and
Mooney (2012)).
reranking features below, each illustrated by an ex-
ample.
a) PCFG Rule. Indicates whether a particular
PCFG rule is used in the parse tree: f(L1 ?
L2L3) = 1.
b) Grandparent PCFG Rule. Indicates whether
a particular PCFG rule as well as the non-
terminal above it is used in the parse tree:
f(L3 ? L5L6|L1) = 1.
c) Long-range Unigram. Indicates whether a
nonterminal has a given NL word below it
in the parse tree: f(L2 ; left) = 1 and
f(L4 ; turn) = 1.
d) Two-level Long-range Unigram. Indicates
whether a nonterminal has a child nontermi-
nal which eventually generates a NL word in
the parse tree: f(L4 ; left|L2) = 1
e) Unigram. Indicates whether a nonterminal
produces a given child nonterminal or terminal
NL word in the parse tree: f(L1 ? L2) = 1
and f(L1 ? L3) = 1.
f) Grandparent Unigram. Indicates whether
a nonterminal has a given child nontermi-
nal/terminal below it, as well as a given parent
nonterminal: f(L2 ? L4|L1) = 1
g) Bigram. Indicates whether a given bigram of
nonterminal/terminals occurs for given a par-
ent nonterminal: f(L1 ? L2 : L3) = 1.
h) Grandparent Bigram. Same as Bigram, but
also includes the nonterminal above the parent
nonterminal: f(L3 ? L5 : L6|L1) = 1.
i) Log-probability of Parse Tree. Certainty as-
signed by the base generative model.
4.2 Predicate-Only Features
The base features above generally include non-
terminal symbols used in the parse tree. In the
grounded PCFG model, nonterminals are named
after components of the semantic representations
(MRs), which are complex and numerous. There
are ' 2,500 nonterminals in the grammar con-
structed for the navigation data, most of which
are very specific and rare. This results in a very
large, sparse feature space which can easily lead
222
the reranking model to over-fit the training data
and prevent it from generalizing properly.
Therefore, we also tried constructing more gen-
eral features that are less sparse. First, we con-
struct generalized versions of the base features
in which nonterminal symbols use only predicate
names and omit their arguments. In the navigation
task, action arguments frequently contain redun-
dant, rarely used information. In particular, the
interleaving verification steps frequently include
many details that are never actually mentioned in
the NL instructions. For instance, a nonterminal
for the MR
Turn(LEFT),
Verify(at:SOFA,front:EASEL),
Travel(steps:3)
is transformed into the predicate-only form
Turn(), Verify(), Travel()
, and then used to construct more general versions
of the base features described in the previous sec-
tion. Second, another version of the base features
are constructed in which nonterminal symbols in-
clude action arguments but omit all interleaving
verification steps. This is a somewhat more con-
servative simplification of the nonterminal sym-
bols. Although verification steps sometimes help
interpret the actions and their surrounding context,
they frequently cause the nonterminal symbols to
become unnecessarily complex and specific.
4.3 Descended Action Features
Finally, another feature group which we utilize
captures whether a particular atomic action in a
nonterminal ?descends? into one of its child non-
terminals or not. An atomic action consists of a
predicate and its arguments, e.g. Turn(LEFT),
Travel(steps:2), or Verify(at:SOFA).
When an atomic action descends into lower non-
terminals in a parse tree, it indicates that it is men-
tioned in the NL instruction and is therefore im-
portant. Below are several feature types related to
descended actions that are used in our reranking
model:
a) Descended Action. Indicates whether a given
atomic action in a nonterminal descends to the
next level. In Figure 3, f(Turn(LEFT)) = 1
since it descends into L2 and L4.
b) Descended Action Unigram. Same as De-
scended Action, but also includes the current
nonterminal: f(Turn(LEFT)|L1) = 1.
c) Grandparent Descended Action Unigram.
Same as Descended Action Unigram,
but additionally includes the parent
nonterminal as well as the current one:
f(Turn(LEFT)|L2, L1) = 1.
d) Long-range Descended Action Unigram. Indi-
cates whether a given atomic action in a non-
terminal descends to a child nonterminal and
this child generates a given NL word below it:
f(Turn(LEFT) ; left) = 1
5 Experimental Evaluation
5.1 Data and Methodology
The navigation data was collected by MacMahon
et al (2006), and includes English instructions
and human follower data.1 The data contains 706
route instructions for three virtual worlds. The in-
structions were produced by six instructors for 126
unique starting and ending location pairs over the
three maps. Each instruction is annotated with 1
to 15 human follower traces with an average of
10.4 actions per instruction. Each instruction con-
tains an average of 5.0 sentences each with an av-
erage of 7.8 words. Chen and Mooney (2011)
constructed a version of the data in which each
sentence is annotated with the actions taken by
the majority of followers when responding to this
sentence. This single-sentence version is used for
training. Manually annotated ?gold standard? for-
mal plans for each sentence are used for evaluation
purposes only.
We followed the same experimental methodol-
ogy as Kim and Mooney (2012) and Chen and
Mooney (2011). We performed ?leave one en-
vironment out? cross-validation, i.e. 3 trials of
training on two environments and testing on the
third. The baseline model is first trained on data
for two environments and then used to generate
the n = 50 best plans for both training and test-
ing instructions. As mentioned in Section 3.1, we
need to generate many more top parse trees to get
50 distinct formal MR plans. We limit the num-
ber of best parse trees to 1,000,000, and even with
this high limit, some training examples were left
with less than 50 distinct plans.2 Each candidate
1Data is available at http://www.cs.utexas.
edu/users/ml/clamp/navigation/
29.6% of the examples (310 out of total 3237) produced
less than 50 distinct MR plans in the evaluation. This was
mostly due to exceeding the parse-tree limit and partly be-
cause the baseline model failed to parse some NL sentences.
223
n 1 2 5 10 25 50
Parse Accuracy F1 74.81 79.08 82.78 85.32 87.52 88.62
Plan Execution Single-sentence 57.22 63.86 70.93 76.41 83.59 87.02Paragraph 20.17 28.08 35.34 40.64 48.69 53.66
Table 1: Oracle parse and execution accuracy for single sentence and complete paragraph instructions
for the n best parses.
plan is then executed using MARCO and its rate
of successfully reaching the goal is recorded. Our
reranking model is then trained on the training
data using the n-best candidate parses. We only
retain reranking features that appear (i.e. have a
value of 1) at least twice in the training data.
Finally, we measure both parse and execution
accuracy on the test data. Parse accuracy evalu-
ates how well a system maps novel NL sentences
for new environments into correct MR plans (Chen
and Mooney, 2011). It is calculated by compar-
ing the system?s MR output to the gold-standard
MR. Accuracy is measured using F1, the harmonic
mean of precision and recall for individual MR
constituents, thereby giving partial credit to ap-
proximately correct MRs. We then execute the re-
sulting MR plans in the test environment to see
whether they successfully reach the desired des-
tinations. Execution is evaluated both for sin-
gle sentence and complete paragraph instructions.
Successful execution rates are calculated by aver-
aging 10 nondeterministic MARCO executions.
5.2 Reranking Results
Oracle results
As typical in reranking experiments, we first
present results for an ?oracle? that always returns
the best result amongst the top-n candidates pro-
duced by the baseline system, thereby providing
an upper bound on the improvements possible
with reranking. Table 1 shows oracle accuracy for
both semantic parsing and plan execution for sin-
gle sentence and complete paragraph instructions
for various values of n. For oracle parse accuracy,
for each sentence, we pick the parse that gives
the highest F1 score. For oracle single-sentence
execution accuracy, we pick the parse that gives
the highest execution success rate. These single-
sentence plans are then concatenated to produce a
complete plan for each paragraph instruction in or-
der to measure overall execution accuracy. Since
making an error in any of the sentences in an in-
struction can easily lead to the wrong final destina-
tion, paragraph-level accuracies are always much
lower than sentence-level ones. In order to bal-
ance oracle accuracy and the computational ef-
fort required to produce n distinct plans, we chose
n = 50 for the final experiments since oracle per-
formance begins to asymptote at this point.
Response-based vs. gold-standard reference
weight updates
Table 2 presents reranking results for our proposed
response-based weight update (Single) for the
averaged perceptron (cf. Section 3.1) compared
to the typical weight update method using gold-
standard parses (Gold). Since the gold-standard
annotation gives the correct MR rather than a parse
tree for each sentence, Gold selects as a single
reference parse the candidate in the top 50 whose
resulting MR is most similar to the gold-standard
MR as determined by its parse accuracy. Ge and
Mooney (2006) employ a similar approach when
reranking semantic parses.
The results show that our response-based ap-
proach (Single) has better execution accuracy
than both the baseline and the standard approach
using gold-standard parses (Gold). However,
Gold does perform best on parse accuracy since
it explicitly focuses on maximizing the accuracy
of the resulting MR. In contrast, by focusing dis-
criminative training on optimizing performance
of the ultimate end task, our response-based ap-
proach actually outperforms the traditional ap-
proach on the final task. In addition, it only uti-
lizes feedback that is naturally available for the
task, rather than requiring an expert to laboriously
annotate each sentence with a gold-standard MR.
Even though Gold captures more elements of the
gold-standard MRs, it may miss some critical MR
components that are crucial to the final naviga-
tion task. The overall result is very promising be-
cause it demonstrates how reranking can be ap-
plied to grounded language learning tasks where
gold-standard parses are not readily available.
224
Parse Acc Plan Execution
F1 Single Para
Baseline 74.81 57.22 20.17
Gold 78.26 52.57 19.33
Single 73.32 59.65 22.62
Multi 73.43 62.81 26.57
Table 2: Reranking results comparing our
response-based methods using single (Single)
or multiple (Multi) pseudo-gold parses to the
standard approach using a single gold-standard
parse (Gold). Baseline refers to Kim and
Mooney (2012)?s system. Reranking results use
all features described in Section 4. ?Single? means
the single-sentence version and ?Para? means the
full paragraph version of the corpus.
Weight update with single vs. multiple
reference parses
Table 2 also shows performance when using mul-
tiple reference parse trees to update weights (cf.
Section 3.2). Using multiple parses (Multi)
clearly performs better for all evaluation met-
rics, particularly execution. As explained in Sec-
tion 3.2, the single-best pseudo-gold parse pro-
vides weak, ambiguous feedback since it only pro-
vides a rough estimate of the response feedback
from the execution module. Using a variety of
preferable parses to update weights provides a
greater amount and variety of weak feedback and
therefore leads to a more accurate model.3
Comparison of different feature groups
Table 3 compares reranking results using the dif-
ferent feature groups described in Section 4. Com-
pared to the baseline model (Kim and Mooney,
2012), each of the feature groups Base (base
features), Pred (predicate-only and verification-
removed features), and Desc (descended action
features) helps improve the performance of plan
execution for both single sentence and complete
paragraph navigation instructions. Among them,
Desc is the most effective group of features.
Combinations of the feature groups helps fur-
3We also tried extending Gold to use multiple reference
parses in the same manner, but this actually degraded its per-
formance for all metrics. This indicates that, unlike Multi,
parses other than the best one do not have useful information
in terms of optimizing normal parse accuracy. Instead, ad-
ditional parses seem to add noise to the training process in
this case. Therefore, updating with multiple parses does not
appear to be useful in standard reranking.
Features Parse Acc Plan ExecutionF1 Single Para
Baseline 74.81 57.22 20.17
Base 71.50 60.09 23.20
Pred 71.61 60.87 24.13
Desc 73.90 61.33 25.00
Base+Pred 69.52 61.49 26.24
Base+Desc 73.66 61.72 25.58
Pred+Desc 72.56 62.36 26.04
All 73.43 62.81 26.57
Table 3: Reranking results comparing different
sets of features. Base refers to base features (cf.
Section 4.1), Pred refers to predicate-only fea-
tures and also includes features based on remov-
ing interleaving verification steps (cf. Section 4.2),
Desc refers to descended action features (cf. Sec-
tion 4.3). All refers to all the features including
Base, Pred, and Desc. All results use weight
update with multiple reference parses (cf. Sec-
tion 3.2).
ther improve the plan execution performance, and
reranking using all of the feature groups (All)
performs the best, as expected. However, since
our model is optimizing plan execution during
training, the results for parse accuracy are always
worse than the baseline model.
6 Related Work
Discriminative reranking is a common machine
learning technique to improve the output of gen-
erative models. It has been shown to be effective
for various natural language processing tasks in-
cluding syntactic parsing (Collins, 2000; Collins,
2002b; Collins and Koo, 2005; Charniak and
Johnson, 2005; Huang, 2008), semantic parsing
(Lu et al, 2008; Ge and Mooney, 2006), part-
of-speech tagging (Collins, 2002a), semantic role
labeling (Toutanova et al, 2005), named entity
recognition (Collins, 2002c). machine translation
(Shen et al, 2004; Fraser and Marcu, 2006) and
surface realization in generation (White and Ra-
jkumar, 2009; Konstas and Lapata, 2012). How-
ever, to our knowledge, there has been no pre-
vious attempt to apply discriminative reranking
to grounded language acquisition, where gold-
standard reference parses are not typically avail-
able for training reranking models.
Our use of response-based training is similar
225
to work on learning semantic parsers from execu-
tion output such as the answers to database queries
(Clarke et al, 2010; Liang et al, 2011). Although
the demands of grounded language tasks, such as
following navigation instructions, are different, it
would be interesting to try adapting these alterna-
tive approaches to such problems.
7 Future Work
In the future, we would like to explore the con-
struction of better, more-general reranking fea-
tures that are less prone to over-fitting. Since
typical reranking features rely on the combina-
tion and/or modification of nonterminals appear-
ing in parse trees, for the large PCFG?s produced
for grounded language learning, such features are
very sparse and rare. Although the current features
provide a significant increase in performance, or-
acle results imply that an even larger benefit may
be achievable.
In addition, employing other reranking method-
ologies, such as kernel methods (Collins, 2002b),
and forest reranking exploiting a packed forest of
exponentially many parse trees (Huang, 2008), is
another area of future work. We also would like
to apply our approach to other reranking algo-
rithms such as SVMs (Joachims, 2002) and Max-
Ent methods (Charniak and Johnson, 2005).
8 Conclusions
In this paper, we have shown how to adapt dis-
criminative reranking to grounded language learn-
ing. Since typical grounded language learning
problems, such as navigation instruction follow-
ing, do not provide the gold-standard reference
parses required by standard reranking models, we
have devised a novel method for using the weaker
supervision provided by response feedback (e.g.
the execution of inferred navigation plans) when
training a perceptron-based reranker. This ap-
proach was shown to be very effective compared
to the traditional method of using gold-standard
parses. In addition, since this response-based su-
pervision is weak and ambiguous, we have also
presented a method for using multiple reference
parses to perform perceptron weight updates and
shown a clear further improvement in end-task
performance with this approach.
Acknowledgments
We thank anonymous reviewers for their helpful
comments to improve this paper. This work was
funded by the NSF grant IIS-0712907 and IIS-
1016312. Experiments were performed on the
Mastodon Cluster, provided by NSF Grant EIA-
0303609.
References
Benjamin Bo?rschinger, Bevan K. Jones, and Mark
Johnson. 2011. Reducing grounded learning tasks
to grammatical inference. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 1416?1425,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), pages 173?180, Ann Arbor, MI, June.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011), San Francisco, CA, USA, August.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. Journal
of Artificial Intelligence Research, 37:397?435.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world?s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL-2010), pages 18?27, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning (ICML-2000), pages 175?182, Stanford,
CA, June.
Michael Collins. 2002a. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proceed-
ings of the 2002 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-02),
Philadelphia, PA, July.
Michael Collins. 2002b. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of
226
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 263?270,
Philadelphia, PA, July.
Michael Collins. 2002c. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2002), pages 489?496, Philadelphia,
PA.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (ACL-06), pages 769?776, Stroudsburg, PA,
USA. Association for Computational Linguistics.
R. Ge and R. J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING/ACL-
06), Sydney, Australia, July.
Kevin Gold and Brian Scassellati. 2007. A robot that
uses existing vocabulary to infer non-visual word
meanings from observation. In Proceedings of the
22nd national conference on Artificial intelligence -
Volume 1, AAAI?07, pages 883?888. AAAI Press.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the Eighth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-
2002), Edmonton, Canada.
Joohyun Kim and Raymond J. Mooney. 2012. Un-
supervised PCFG induction for grounded language
learning with highly ambiguous supervision. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing and Natural Lan-
guage Learning, EMNLP-CoNLL ?12.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 369?378, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL, Portland, Oregon, June.
Association for Computational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08), Honolulu, HI, October.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: connecting language,
knowledge, and action in route instructions. In pro-
ceedings of the 21st national conference on Artifi-
cial intelligence - Volume 2, AAAI?06, pages 1475?
1482. AAAI Press.
Deb Roy. 2002. Learning visually grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3):353?385.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
177?184, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL-05), pages 589?596, Ann Arbor, MI,
June.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 -
Volume 1, EMNLP ?09, pages 410?419, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Chen Yu and Dana H. Ballard. 2004. On the integra-
tion of grounding language and learning objects. In
Proceedings of the Nineteenth National Conference
on Artificial Intelligence (AAAI-04), pages 488?493.
227
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1210?1219,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Probabilistic Soft Logic for Semantic Textual Similarity
Islam Beltagy
?
Katrin Erk
?
Raymond Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?
{beltagy,mooney}@cs.utexas.edu
?
katrin.erk@mail.utexas.edu
Abstract
Probabilistic Soft Logic (PSL) is a re-
cently developed framework for proba-
bilistic logic. We use PSL to combine
logical and distributional representations
of natural-language meaning, where distri-
butional information is represented in the
form of weighted inference rules. We ap-
ply this framework to the task of Seman-
tic Textual Similarity (STS) (i.e. judg-
ing the semantic similarity of natural-
language sentences), and show that PSL
gives improved results compared to a pre-
vious approach based on Markov Logic
Networks (MLNs) and a purely distribu-
tional approach.
1 Introduction
When will people say that two sentences are sim-
ilar? This question is at the heart of the Semantic
Textual Similarity task (STS)(Agirre et al, 2012).
Certainly, if two sentences contain many of the
same words, or many similar words, that is a good
indication of sentence similarity. But that can be
misleading. A better characterization would be to
say that if two sentences use the same or similar
words in the same or similar relations, then those
two sentences will be judged similar.
1
Interest-
ingly, this characterization echoes the principle of
compositionality, which states that the meaning of
a phrase is uniquely determined by the meaning of
its parts and the rules that connect those parts.
Beltagy et al (2013) proposed a hybrid ap-
proach to sentence similarity: They use a very
1
Mitchell and Lapata (2008) give an amusing example of
two sentences that consist of all the same words, but are very
different in their meaning: (a) It was not the sales manager
who hit the bottle that day, but the office worker with the
serious drinking problem. (b) That day the office manager,
who was drinking, hit the problem sales worker with a bottle,
but it was not serious.
deep representation of sentence meaning, ex-
pressed in first-order logic, to capture sentence
structure, but combine it with distributional sim-
ilarity ratings at the word and phrase level. Sen-
tence similarity is then modelled as mutual entail-
ment in a probabilistic logic. This approach is in-
teresting in that it uses a very deep and precise
representation of meaning, which can then be re-
laxed in a controlled fashion using distributional
similarity. But the approach faces large hurdles
in practice, stemming from efficiency issues with
the Markov Logic Networks (MLN) (Richardson
and Domingos, 2006) that they use for performing
probabilistic logical inference.
In this paper, we use the same combined logic-
based and distributional framework as Beltagy et
al., (2013) but replace Markov Logic Networks
with Probabilistic Soft Logic (PSL) (Kimmig et
al., 2012; Bach et al, 2013). PSL is a proba-
bilistic logic framework designed to have efficient
inference. Inference in MLNs is theoretically in-
tractable in the general case, and existing approxi-
mate inference algorithms are computationally ex-
pensive and sometimes inaccurate. Consequently,
the MLN approach of Beltagy et al (2013) was
unable to scale to long sentences and was only
tested on the relatively short sentences in the Mi-
crosoft video description corpus used for STS
(Agirre et al, 2012). On the other hand, inference
in PSL reduces to a linear programming problem,
which is theoretically and practically much more
efficient. Empirical results on a range of prob-
lems have confirmed that inference in PSL is much
more efficient than in MLNs, and frequently more
accurate (Kimmig et al, 2012; Bach et al, 2013).
We show how to use PSL for STS, and describe
changes to the PSL framework that make it more
effective for STS. For evaluation, we test on three
STS datasets, and compare our PSL system with
the MLN approach of Beltagy et al, (2013) and
with distributional-only baselines. Experimental
1210
results demonstrate that, overall, PSL models hu-
man similarity judgements more accurately than
these alternative approaches, and is significantly
faster than MLNs.
The rest of the paper is organized as follows:
section 2 presents relevant background material,
section 3 explains how we adapted PSL for the
STS task, section 4 presents the evaluation, and
sections 5 and 6 discuss future work and conclu-
sions, respectively.
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, their binary
nature prevents them from capturing the ?graded?
aspects of meaning in language. Also, it is difficult
to construct formal ontologies of properties and re-
lations that have broad coverage, and semantically
parsing sentences into logical expressions utilizing
such an ontology is very difficult. Consequently,
current semantic parsers are mostly restricted to
quite limited domains, such as querying a specific
database (Kwiatkowski et al, 2013; Berant et al,
2013). In contrast, our system is not limited to any
formal ontology and can use a wide-coverage tool
for semantic analysis, as discussed below.
2.2 Distributional Semantics
Distributional models (Turney and Pantel, 2010),
on the other hand, use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Landauer and Du-
mais, 1997; Mitchell and Lapata, 2010). They are
relatively easier to build than logical representa-
tions, automatically acquire knowledge from ?big
data,? and capture the ?graded? nature of linguis-
tic meaning, but do not adequately capture logical
structure (Grefenstette, 2013).
Distributional models are motivated by the ob-
servation that semantically similar words occur in
similar contexts, so words can be represented as
vectors in high dimensional spaces generated from
the contexts in which they occur (Landauer and
Dumais, 1997; Lund and Burgess, 1996). Such
models have also been extended to compute vec-
tor representations for larger phrases, e.g. by
adding the vectors for the individual words (Lan-
dauer and Dumais, 1997) or by a component-wise
product of word vectors (Mitchell and Lapata,
2008; Mitchell and Lapata, 2010), or more com-
plex methods that compute phrase vectors from
word vectors and tensors (Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). We use
vector addition (Landauer and Dumais, 1997), and
component-wise product (Mitchell and Lapata,
2008) as baselines for STS. Vector addition was
previously found to be the best performing sim-
ple distributional method for STS (Beltagy et al,
2013).
2.3 Markov Logic Networks
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints
and thereby allowing situations in which not all
clauses are satisfied. MLNs define a probability
distribution over possible worlds, where a world?s
probability increases exponentially with the to-
tal weight of the logical clauses that it satisfies.
A variety of inference methods for MLNs have
been developed, however, developing a scalable,
general-purpose, accurate inference method for
complex MLNs is an open problem. Beltagy et
al. (2013) use MLNs to represent the meaning of
natural language sentences and judge textual en-
tailment and semantic similarity, but they were un-
able to scale the approach beyond short sentences
due to the complexity of MLN inference.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is a recently pro-
posed alternative framework for probabilistic logic
(Kimmig et al, 2012; Bach et al, 2013). It uses
logical representations to compactly define large
graphical models with continuous variables, and
includes methods for performing efficient proba-
bilistic inference for the resulting models. A key
distinguishing feature of PSL is that ground atoms
have soft, continuous truth values in the interval
[0, 1] rather than binary truth values as used in
MLNs and most other probabilistic logics. Given
a set of weighted logical formulas, PSL builds a
graphical model defining a probability distribution
over the continuous space of values of the random
variables in the model.
1211
A PSL model is defined using a set of weighted
if-then rules in first-order logic, as in the following
example:
?x, y, z. friend(x, y) ? votesFor(y, z)?
votesFor(x, z) | 0.3 (1)
?x, y, z. spouse(x, y) ? votesFor(y, z)?
votesFor(x, z) | 0.8 (2)
In our notation, we use lower case letters like
x, y, z to represent variables and upper case let-
ters for constants. The first rule states that a per-
son is likely to vote for the same person as his/her
friend. The second rule encodes the same regular-
ity for a person?s spouse. The weights encode the
knowledge that a spouse?s influence is greater than
a friend?s in this regard.
In addition, PSL includes similarity functions.
Similarity functions take two strings or two sets as
input and return a truth value in the interval [0, 1]
denoting the similarity of the inputs. For example,
in our application, we generate inference rules that
incorporate the similarity of two predicates. This
can be represented in PSL as:
?x. similarity(?predicate1?, ?predicate2?) ?
predicate1(x)? predicate2(x)
As mentioned above, each ground atom, a,
has a soft truth value in the interval [0, 1],
which is denoted by I(a). To compute soft truth
values for logical formulas, Lukasiewicz?s re-
laxation of conjunctions(?), disjunctions(?) and
negations(?) are used:
I(l
1
? l
1
) = max{0, I(l
1
) + I(l
2
)? 1}
I(l
1
? l
1
) = min{I(l
1
) + I(l
2
), 1}
I(?l
1
) = 1? I(l
1
)
Then, a given rule r ? r
body
? r
head
, is said to be
satisfied (i.e. I(r) = 1) iff I(r
body
) ? I(r
head
).
Otherwise, PSL defines a distance to satisfaction
d(r) which captures how far a rule r is from being
satisfied: d(r) = max{0, I(r
body
) ? I(r
head
)}.
For example, assume we have the set of evidence:
I(spouse(B,A)) = 1, I(votesFor(A,P )) =
0.9, I(votesFor(B,P )) = 0.3, and that r
is the resulting ground instance of rule (2).
Then I(spouse(B,A) ? votesFor(A,P )) =
max{0, 1 + 0.9 ? 1} = 0.9, and d(r) =
max{0, 0.9? 0.3} = 0.6.
Using distance to satisfaction, PSL defines a
probability distribution over all possible interpre-
tations I of all ground atoms. The pdf is defined
as follows:
p(I) =
1
Z
exp [?
?
r?R
?
r
(d(r))
p
]; (3)
Z =
?
I
exp [?
?
r?R
?
r
(d(r))
p
]
where Z is the normalization constant, ?
r
is the
weight of rule r, R is the set of all rules, and p ?
{1, 2} provides two different loss functions. For
our application, we always use p = 1
PSL is primarily designed to support MPE in-
ference (Most Probable Explanation). MPE infer-
ence is the task of finding the overall interpretation
with the maximum probability given a set of evi-
dence. Intuitively, the interpretation with the high-
est probability is the interpretation with the lowest
distance to satisfaction. In other words, it is the
interpretation that tries to satisfy all rules as much
as possible. Formally, from equation 3, the most
probable interpretation, is the one that minimizes
?
r?R
?
r
(d(r))
p
. In case of p = 1, and given
that all d(r) are linear equations, then minimizing
the sum requires solving a linear program, which,
compared to inference in other probabilistic logics
such as MLNs, can be done relatively efficiently
using well-established techniques. In case p = 2,
MPE inference can be shown to be a second-order
cone program (SOCP) (Kimmig et al, 2012).
2.5 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 0 to 5, and was recently introduced
as a SemEval task (Agirre et al, 2012). Gold
standard scores are averaged over multiple hu-
man annotations and systems are evaluated using
the Pearson correlation between a system?s out-
put and gold standard scores. The best perform-
ing system in 2012?s competition was by B?ar et
al. (2012), a complex ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics. Two of the
datasets we use for evaluation are from the 2012
competition. We did not utilize the new datasets
added in the 2013 competition since they did not
contain naturally-occurring, full sentences, which
is the focus of our work.
1212
2.6 Combining logical and distributional
methods using probabilistic logic
There are a few recent attempts to combine log-
ical and distributional representations in order to
obtain the advantages of both. Lewis and Steed-
man (2013) use distributional information to deter-
mine word senses, but still produce a strictly log-
ical semantic representation that does not address
the ?graded? nature of linguistic meaning that is
important to measuring semantic similarity.
Garrette et al (2011) introduced a framework
for combining logic and distributional models us-
ing probabilistic logic. Distributional similarity
between pairs of words is converted into weighted
inference rules that are added to the logical repre-
sentation, and Markov Logic Networks are used to
perform probabilistic logical inference.
Beltagy et al (2013) extended this framework
by generating distributional inference rules from
phrase similarity and tailoring the system to the
STS task. STS is treated as computing the prob-
ability of two textual entailments T |= H and
H |= T , where T and H are the two sentences
whose similarity is being judged. These two en-
tailment probabilities are averaged to produce a
measure of similarity. The MLN constructed to
determine the probability of a given entailment
includes the logical forms for both T and H as
well as soft inference rules that are constructed
from distributional information. Given a similar-
ity score for all pairs of sentences in the dataset,
a regressor is trained on the training set to map
the system?s output to the gold standard scores.
The trained regressor is applied to the scores in
the test set before calculating Pearson correlation.
The regression algorithm used is Additive Regres-
sion (Friedman, 2002).
To determine an entailment probability, first,
the two sentences are mapped to logical repre-
sentations using Boxer (Bos, 2008), a tool for
wide-coverage semantic analysis that maps a CCG
(Combinatory Categorial Grammar) parse into a
lexically-based logical form. Boxer uses C&C for
CCG parsing (Clark and Curran, 2004).
Distributional semantic knowledge is then en-
coded as weighted inference rules in the MLN.
A rule?s weight (w) is a function of the cosine
similarity (sim) between its antecedent and con-
sequent. Rules are generated on the fly for each
T and H . Let t and h be the lists of all words
and phrases in T and H respectively. For all
pairs (a, b), where a ? t, b ? h, it generates
an inference rule: a ? b | w, where w =
f(sim(
??
a ,
??
b )). Both a and b can be words or
phrases. Phrases are defined in terms of Boxer?s
output. A phrase is more than one unary atom
sharing the same variable like ?a little kid? which
in logic is little(K) ? kid(K). A phrase also can
be two unary atoms connected by a relation like
?a man is driving? which in logic is man(M) ?
agent(D,M) ? drive(D). The similarity func-
tion sim takes two vectors as input. Phrasal vec-
tors are constructed using Vector Addition (Lan-
dauer and Dumais, 1997). The set of generated
inference rules can be regarded as the knowledge
base KB.
Beltagy et al (2013) found that the logical con-
junction in H is very restrictive for the STS task,
so they relaxed the conjunction by using an aver-
age evidence combiner (Natarajan et al, 2010).
The average combiner results in computationally
complex inference and only works for short sen-
tences. In case inference breaks or times-out, they
back off to a simpler combiner that leads to much
faster inference but loses most of the structure of
the sentence and is therefore less accurate.
Given T , KB and H from the previous
steps, MLN inference is then used to compute
p(H|T,KB), which is then used as a measure of
the degree to which T entails H .
3 PSL for STS
For several reasons, we believe PSL is a more ap-
propriate probabilistic logic for STS than MLNs.
First, it is explicitly designed to support efficient
inference, therefore it scales better to longer sen-
tences with more complex logical forms. Sec-
ond, it was also specifically designed for com-
puting similarity between complex structured ob-
jects rather than determining probabilistic logical
entailment. In fact, the initial version of PSL
(Broecheler et al, 2010) was called Probabilis-
tic Similarity Logic, based on its use of similar-
ity functions. This initial version was shown to be
very effective for measuring the similarity of noisy
database records and performing record linkage
(i.e. identifying database entries referring to the
same entity, such as bibliographic citations refer-
ring to the same paper). Therefore, we have devel-
oped an approach that follows that of Beltagy et
al. (2013), but replaces Markov Logic with PSL.
This section explains how we formulate the STS
1213
task as a PSL program. PSL does not work very
well ?out of the box? for STS, mainly because
Lukasiewicz?s equation for the conjunction is very
restrictive. Therefore, we use a different interpre-
tation for conjunction that uses averaging, which
requires corresponding changes to the optimiza-
tion problem and the grounding technique.
3.1 Representation
Given the logical forms for a pair of sentences,
a text T and a hypothesis H , and given a set of
weighted rules derived from the distributional se-
mantics (as explained in section 2.6) composing
the knowledge base KB, we build a PSL model
that supports determining the truth value of H in
the most probable interpretation (i.e. MPE) given
T and KB.
Consider the pair of sentences is ?A man is driv-
ing?, and ?A guy is walking?. Parsing into logical
form gives:
T : ?x, y. man(x) ? agent(y, x) ? drive(y)
H : ?x, y. guy(x) ? agent(y, x) ? walk(y)
The PSL program is constructed as follows:
T : The text is represented in the evidence set. For
the example, after Skolemizing the existential
quantifiers, this contains the ground atoms:
{man(A), agent(B,A), drive(B)}
KB: The knowledge base is a set of lexical and
phrasal rules generated from distributional
semantics, along with a similarity score for
each rule (section 2.6). For the exam-
ple, we generate the rules: ?x. man(x) ?
vs sim(?man?, ?guy?)? guy(x) ,
?x.drive(x)?vs sim(?drive?, ?walk?)?
walk(x)
where vs sim is a similarity function that
calculates the distributional similarity score
between the two lexical predicates. All rules
are assigned the same weight because all
rules are equally important.
H: The hypothesis is represented as H ?
result(), and then PSL is queried for the
truth value of the atom result(). For
our example, the rule is: ?x, y. guy(x) ?
agent(y, x) ? walk(y)? result().
Priors: A low prior is given to all predicates. This
encourages the truth values of ground atoms
to be zero, unless there is evidence to the con-
trary.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where T = S
1
, H = S
2
and
another where T = S
2
, H = S
1
, and output the
two scores. To produce a final similarity score, we
train a regressor to learn the mapping between the
two PSL scores and the overall similarity score.
As in Beltagy et al, (2013) we use Additive Re-
gression (Friedman, 2002).
3.2 Changing Conjunction
As mentioned above, Lukasiewicz?s formula for
conjunction is very restrictive and does not work
well for STS. For example, for T: ?A man is driv-
ing? and H: ?A man is driving a car?, if we use the
standard PSL formula for conjunction, the output
value is zero because there is no evidence for a car
and max(0, X + 0 ? 1) = 0 for any truth value
0 ? X ? 1. However, humans find these sen-
tences to be quite similar.
Therefore, we introduce a new averaging inter-
pretation of conjunction that we use for the hy-
pothesis H . The truth value for a conjunction
is defined as I(p
1
? .... ? p
n
) =
1
n
?
n
i=1
I(p
i
).
This averaging function is linear, and the result is
a valid truth value in the interval [0, 1], therefore
this change is easily incorporated into PSL with-
out changing the complexity of inference which
remains a linear-programming problem.
It would perhaps be even better to use a
weighted average, where weights for different
components are learned from a supervised train-
ing set. This is an important direction for future
work.
3.3 Grounding Process
Grounding is the process of instantiating the vari-
ables in the quantified rules with concrete con-
stants in order to construct the nodes and links in
the final graphical model. In principle, ground-
ing requires instantiating each rule in all possible
ways, substituting every possible constant for each
variable in the rule. However, this is a combinato-
rial process that can easily result in an explosion in
the size of the final network. Therefore, PSL em-
ploys a ?lazy? approach to grounding that avoids
the construction of irrelevant groundings. If there
is no evidence for one of the antecedents in a par-
ticular grounding of a rule, then the normal PSL
formula for conjunction guarantees that the rule is
1214
Algorithm 1 Heuristic Grounding
Input: r
body
= a
1
? ....?a
n
: antecedent of a rule
with average interpretation of conjunction
Input: V : set of variables used in r
body
Input: Ant(v
i
): subset of antecedents a
j
con-
taining variable v
i
Input: Const(v
i
): list of possible constants of
variable v
i
Input: Gnd(a
i
): set of ground atoms of a
i
.
Input: GndConst(a, g, v): takes an atom a,
grounding g for a, and variable v, and returns
the constant that substitutes v in g
Input: gnd limit: limit on the number of
groundings
1: for all v
i
? V do
2: for all C ? Const(v
i
) do
3: score(C) =
?
a?Ant(v
i
)
(max I(g))
for g ? Gnd(a) ?GndConst(a, g, v
i
) = C
4: end for
5: sort Const(v
i
) on scores, descending
6: end for
7: return For all v
i
? V , take the Cartesian-
product of the sortedConst(v
i
) and return the
top gnd limit results
trivially satisfied (I(r) = 1) since the truth value
of the antecedent is zero. Therefore, its distance to
satisfaction is also zero, and it can be omitted from
the ground network without impacting the result of
MPE inference.
However, this technique does not work once
we switch to using averaging to interpret conjunc-
tions. For example, given the rule ?x. p(x) ?
q(x) ? t() and only one piece of evidence p(C)
there are no relevant groundings because there is
no evidence for q(C), and therefore, for normal
PSL, I(p(C) ? q(C)) = 0 which does not affect
I(t()). However, when using averaging with the
same evidence, we need to generate the grounding
p(C)?q(C) because I(p(C)?q(C)) = 0.5 which
does affect I(t()).
One way to solve this problem is to eliminate
lazy grounding and generate all possible ground-
ings. However, this produces an intractably large
network. Therefore, we developed a heuristic ap-
proximate grounding technique that generates a
subset of the most impactful groundings.
Pseudocode for this heuristic approach is shown
in algorithm 1. Its goal is to find constants that
participate in ground propositions with high truth
value and preferentially use them to construct a
limited number of groundings of each rule.
The algorithm takes the antecedents of a rule
employing averaging conjunction as input. It also
takes the grounding limit which is a threshold on
the number of groundings to be returned. The al-
gorithm uses several subroutines, they are:
? Ant(v
i
): given a variable v
i
, it returns the set
of rule antecedent atoms containing v
i
. E.g,
for the rule: a(x) ? b(y) ? c(x), Ant(x) re-
turns the set of atoms {a(x), c(x)}.
? Const(v
i
): given a variable v
i
, it returns the
list of possible constants that can be used to
instantiate the variable v
i
.
? Gnd(a
i
): given an atom a
i
, it returns the set
of all possible ground atoms generated for a
i
.
? GndConst(a, g, v): given an atom a and
grounding g for a, and a variable v, it finds
the constant that substitutes for v in g. E.g,
assume there is an atom a = a
i
(v
1
, v
2
), and
the ground atom g = a
i
(A,B) is one of its
groundings. GndConst(a, g, v
2
) would re-
turn the constant B since it is the substitution
for the variable v
2
in g.
Lines 1-6 loop over all variables in the rule. For
each variable, lines 2-5 construct a list of constants
for that variable and sort it based on a heuristic
score. In line 3, each constant is assigned a score
that indicates the importance of this constant in
terms of its impact on the truth value of the overall
grounding. A constant?s score is the sum, over all
antedents that contain the variable in question, of
the maximum truth value of any grounding of that
antecedent that contains that constant.
Pushing constants with high scores to the top
of each variable?s list will tend to make the over-
all truth value of the top groundings high. Line
7 computes a subset of the Cartesian product of
the sorted lists of constants, selecting constants in
ranked order and limiting the number of results to
the grounding limit.
One point that needs to be clarified about this
approach is how it relies on the truth values of
ground atoms when the goal of inference is to ac-
tually find these values. PSL?s inference is ac-
tually an iterative process where in each itera-
tion a grounding phase is followed by an opti-
mization phase (solving the linear program). This
loop repeats until convergence, i.e. until the truth
1215
values stop changing. The truth values used in
each grounding phase come from the previous op-
timization phase. The first grounding phase as-
sumes only the propositions in the evidence pro-
vided have non-zero truth values.
4 Evaluation
This section evaluates the performance of PSL on
the STS task.
4.1 Datasets
We evaluate our system on three STS datasets.
? msr-vid: Microsoft Video Paraphrase Cor-
pus from STS 2012. The dataset consists
of 1,500 pairs of short video descriptions
collected using crowdsourcing (Chen and
Dolan, 2011) and subsequently annotated for
the STS task (Agirre et al, 2012). Half of
the dataset is for training, and the second half
is for testing.
? msr-par: Microsoft Paraphrase Corpus from
STS 2012 task. The dataset is 5,801
pairs of sentences collected from news
sources (Dolan et al, 2004). Then, for STS
2012, 1,500 pairs were selected and anno-
tated with similarity scores. Half of the
dataset is for training, and the second half is
for testing.
? SICK: Sentences Involving Compositional
Knowledge is a dataset collected for SemEval
2014. Only the training set is available at this
point, which consists of 5,000 pairs of sen-
tences. Pairs are annotated for RTE and STS,
but we only use the STS data. Training and
testing was done using 10-fold cross valida-
tion.
4.2 Systems Compared
We compare our PSL system with several others.
In all cases, we use the distributional word vec-
tors employed by Beltagy et al (2013) based on
context windows from Gigaword.
? vec-add: Vector Addition (Landauer and
Dumais, 1997). We compute a vector rep-
resentation for each sentence by adding the
distributional vectors of all of its words and
measure similarity using cosine. This is a
simple yet powerful baseline that uses only
distributional information.
? vec-mul: Component-wise Vector Multipli-
cation (Mitchell and Lapata, 2008). The
same as vec-add except uses component-
wise multiplication to combine word vectors.
? MLN: The system of Beltagy et al (2013),
which uses Markov logic instead of PSL for
probabilistic inference. MLN inference is
very slow in some cases, so we use a 10
minute timeout. When MLN times out, it
backs off to a simpler sentence representation
as explained in section 2.6.
? PSL: Our proposed PSL system for combin-
ing logical and distributional information.
? PSL-no-DIR: Our PSL system without dis-
tributional inference rules(empty knowledge
base). This system uses PSL to compute sim-
ilarity of logical forms but does not use dis-
tributional information on lexical or phrasal
similarity. It tests the impact of the proba-
bilistic logic only
? PSL+vec-add: PSL ensembled with vec-
add. Ensembling the MLN approach with a
purely distributional approach was found to
improve results (Beltagy et al, 2013), so we
also tried this with PSL. The methods are en-
sembled by using both entailment scores of
both systems as input features to the regres-
sion step that learns to map entailment scores
to STS similarity ratings. This way, the train-
ing data is used to learn how to weight the
contribution of the different components.
? PSL+MLN: PSL ensembled with MLN in
the same manner.
4.3 Experiments
Systems are evaluated on two metrics, Pearson
correlation and average CPU time per pair of sen-
tences.
? Pearson correlation: The Pearson correlation
between the system?s similarity scores and
the human gold-standards.
? CPU time: This metric only applies to MLN
and PSL. The CPU time taken by the infer-
ence step is recorded and averaged over all
pairs in each of the test datasets. In many
cases, MLN inference is very slow, so we
timeout after 10 minutes and report the num-
ber of timed-out pairs on each dataset.
1216
msr-vid msr-par SICK
vec-add 0.78 0.24 0.65
vec-mul 0.76 0.12 0.62
MLN 0.63 0.16 0.47
PSL-no-DIR 0.74 0.46 0.68
PSL 0.79 0.53 0.70
PSL+vec-add 0.83 0.49 0.71
PSL+MLN 0.79 0.51 0.70
Best Score (B?ar
et al, 2012)
0.87 0.68 n/a
Table 1: STS Pearson Correlations
PSL MLN
time time timeouts/total
msr-vid 8s 1m 31s 132/1500
msr-par 30s 11m 49s 1457/1500
SICK 10s 4m 24s 1791/5000
Table 2: Average CPU time per STS pair, and
number of timed-out pairs in MLN with a 10
minute time limit. PSL?s grounding limit is set to
10,000 groundings.
We also evaluated the effect of changing the
grounding limit on both Pearson correlation and
CPU time for the msr-par dataset. Most of the
sentences in msr-par are long, which results is
large number of groundings, and limiting the num-
ber of groundings has a visible effect on the over-
all performance. In the other two datasets, the
sentences are fairly short, and the full number of
groundings is not large; therefore, changing the
grounding limit does not significantly affect the re-
sults.
4.4 Results and Discussion
Table 1 shows the results for Pearson correlation.
PSL out-performs the purely distributional base-
lines (vec-add and vec-mul) because PSL is able
to combine the information available to vec-add
and vec-mul in a better way that takes sentence
structure into account. PSL also outperforms
the unaided probabilistic-logic baseline that does
not use distributional information (PSL-no-DIR).
PSL-no-DIR works fairly well because there is
significant overlap in the exact words and struc-
ture of the paired sentences in the test data, and
PSL combines the evidence from these similari-
ties effectively. In addition, PSL always does sig-
nificantly better than MLN, because of the large
Figure 1: Effect of PSL?s grounding limit on the
correlation score for the msr-par dataset
number of timeouts, and because the conjunction-
averaging in PSL is combining evidence bet-
ter than MLN?s average-combiner, whose perfor-
mance is sensitive to various parameters. These
results further support the claim that using prob-
abilistic logic to integrate logical and distribu-
tional information is a promising approach to
natural-language semantics. More specifically,
they strongly indicate that PSL is a more effective
probabilistic logic for judging semantic similarity
than MLNs.
Like for MLNs (Beltagy et al, 2013), en-
sembling PSL with vector addition improved the
scores a bit, except for msr-par where vec-add?s
performance is particularly low. However, this en-
semble still does not beat the state of the art (B?ar et
al., 2012) which is a large ensemble of many dif-
ferent systems. It would be informative to add our
system to their ensemble to see if it could improve
it even further.
Table 2 shows the CPU time for PSL and MLN.
The results clearly demonstrate that PSL is an or-
der of magnitude faster than MLN.
Figures 1 and 2 show the effect of changing the
grounding limit on Pearson correlation and CPU
time. As expected, as the grounding limit is in-
creased, accuracy improves but CPU time also
increases. However, note that the difference in
scores between the smallest and largest grounding
limit tested is not large, suggesting that the heuris-
tic approach to limiting grounding is quite effec-
tive.
5 Future Work
As mentioned in Section 3.2, it would be good
to use a weighted average to compute the truth
1217
Figure 2: Effect of PSL?s grounding limit on CPU
time for the msr-par dataset
values for conjunctions, weighting some predi-
cates more than others rather than treating them
all equally. Appropriate weights for different com-
ponents could be learned from training data. For
example, such an approach could learn that the
type of an object determined by a noun should be
weighted more than a property specified by an ad-
jective. As a result, ?black dog? would be appro-
priately judged more similar to ?white dog? than
to ?black cat.?
One of the advantages of using a probabilis-
tic logic is that additional sources of knowledge
can easily be incorporated by adding additional
soft inference rules. To complement the soft in-
ference rules capturing distributional lexical and
phrasal similarities, PSL rules could be added that
encode explicit paraphrase rules, such as those
mined from monolingual text (Berant et al, 2011)
or multi-lingual parallel text (Ganitkevitch et al,
2013).
This paper has focused on STS; however, as
shown by Beltagy et al (2013), probabilistic logic
is also an effective approach to recognizing tex-
tual entailment (RTE). By using the appropriate
functions to combine truth values for various log-
ical connectives, PSL could also be adapted for
RTE. Although we have shown that PSL outper-
forms MLNs on STS, we hypothesize that MLNs
may still be a better approach for RTE. However, it
would be good to experimentally confirm this in-
tuition. In any case, the high computational com-
plexity of MLN inference could mean that PSL is
still a more practical choice for RTE.
6 Conclusion
This paper has presented an approach that uses
Probabilistic Soft Logic (PSL) to determine Se-
mantic Textual Similarity (STS). The approach
uses PSL to effectively combine logical seman-
tic representations of sentences with soft infer-
ence rules for lexical and phrasal similarities com-
puted from distributional information. The ap-
proach builds upon a previous method that uses
Markov Logic (MLNs) for STS, but replaces the
probabilistic logic with PSL in order to improve
the efficiency and accuracy of probabilistic infer-
ence. The PSL approach was experimentally eval-
uated on three STS datasets and was shown to out-
perform purely distributional baselines as well as
the MLN approach. The PSL approach was also
shown to be much more scalable and efficient than
using MLNs
Acknowledgments
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Stephen H. Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss Markov random fields:
Convex inference for structured prediction. In Pro-
ceedings of Uncertainty in Artificial Intelligence
(UAI-13).
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of Semantic
Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
1218
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Matthias Broecheler, Lilyana Mihalkova, and Lise
Getoor. 2010. Probabilistic Similarity Logic. In
Proceedings of Uncertainty in Artificial Intelligence
(UAI-20).
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the International Conference on
Computational Linguistics (COLING-04).
Jerome H Friedman. 2002. Stochastic gradient boost-
ing. Journal of Computational Statistics & Data
Analysis (CSDA-02).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Dan Garrette, Katrin Erk, and Raymond Mooney.
2011. Integrating logical representations with prob-
abilistic information using Markov logic. In Pro-
ceedings of International Conference on Computa-
tional Semantics (IWCS-11).
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics
(TACL-13).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik.
2010. Exploiting causal independence in Markov
logic networks: Combining undirected and directed
models. In Proceedings of European Conference in
Machine Learning (ECML-10).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
1219
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 11?21, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Montague Meets Markov: Deep Semantics with Probabilistic Logical Form
Islam Beltagy?, Cuong Chau?, Gemma Boleda?, Dan Garrette?, Katrin Erk?,
Raymond Mooney?
?Department of Computer Science
?Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?{beltagy,ckcuong,dhg,mooney}@cs.utexas.edu
?gemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu
Abstract
We combine logical and distributional rep-
resentations of natural language meaning by
transforming distributional similarity judg-
ments into weighted inference rules using
Markov Logic Networks (MLNs). We show
that this framework supports both judg-
ing sentence similarity and recognizing tex-
tual entailment by appropriately adapting the
MLN implementation of logical connectives.
We also show that distributional phrase simi-
larity, used as textual inference rules created
on the fly, improves its performance.
1 Introduction
Tasks in natural language semantics are very diverse
and pose different requirements on the underlying
formalism for representing meaning. Some tasks
require a detailed representation of the structure of
complex sentences. Some tasks require the ability to
recognize near-paraphrases or degrees of similarity
between sentences. Some tasks require logical infer-
ence, either exact or approximate. Often it is neces-
sary to handle ambiguity and vagueness in meaning.
Finally, we frequently want to be able to learn rele-
vant knowledge automatically from corpus data.
There is no single representation for natural lan-
guage meaning at this time that fulfills all require-
ments. But there are representations that meet some
of the criteria. Logic-based representations (Mon-
tague, 1970; Kamp and Reyle, 1993) provide an
expressive and flexible formalism to express even
complex propositions, and they come with standard-
ized inference mechanisms. Distributional mod-
hamster(gerbil(
sim( #                ?hamster, #         ?gerbil) = w
8x hamster(x) ! gerbil(x)  | f(w)
Figure 1: Turning distributional similarity into a
weighted inference rule
els (Turney and Pantel, 2010) use contextual sim-
ilarity to predict semantic similarity of words and
phrases (Landauer and Dumais, 1997; Mitchell and
Lapata, 2010), and to model polysemy (Schu?tze,
1998; Erk and Pado?, 2008; Thater et al, 2010).
This suggests that distributional models and logic-
based representations of natural language meaning
are complementary in their strengths (Grefenstette
and Sadrzadeh, 2011; Garrette et al, 2011), which
encourages developing new techniques to combine
them.
Garrette et al (2011; 2013) propose a framework
for combining logic and distributional models in
which logical form is the primary meaning repre-
sentation. Distributional similarity between pairs of
words is converted into weighted inference rules that
are added to the logical form, as illustrated in Fig-
ure 1. Finally, Markov Logic Networks (Richardson
and Domingos, 2006) (MLNs) are used to perform
weighted inference on the resulting knowledge base.
However, they only employed single-word distribu-
tional similarity rules, and only evaluated on a small
11
set of short, hand-crafted test sentences.
In this paper, we extend Garrette et al?s approach
and adapt it to handle two existing semantic tasks:
recognizing textual entailment (RTE) and seman-
tic textual similarity (STS). We show how this sin-
gle semantic framework using probabilistic logical
form in Markov logic can be adapted to support both
of these important tasks. This is possible because
MLNs constitute a flexible programming language
based on probabilistic logic (Domingos and Lowd,
2009) that can be easily adapted to support multiple
types of linguistically useful inference.
At the word and short phrase level, our approach
model entailment through ?distributional? similarity
(Figure 1). If X and Y occur in similar contexts, we
assume that they describe similar entities and thus
there is some degree of entailment between them. At
the sentence level, however, we hold that a stricter,
logic-based view of entailment is beneficial, and we
even model sentence similarity (in STS) as entail-
ment.
There are two main innovations in the formalism
that make it possible for us to work with naturally
occurring corpus data. First, we use more expres-
sive distributional inference rules based on the sim-
ilarity of phrases rather than just individual words.
In comparison to existing methods for creating tex-
tual inference rules (Lin and Pantel, 2001b; Szpek-
tor and Dagan, 2008), these rules are computed on
the fly as needed, rather than pre-compiled. Second,
we use more flexible probabilistic combinations of
evidence in order to compute degrees of sentence
similarity for STS and to help compensate for parser
errors. We replace deterministic conjunction by an
average combiner, which encodes causal indepen-
dence (Natarajan et al, 2010).
We show that our framework is able to han-
dle both sentence similarity (STS) and textual en-
tailment (RTE) by making some simple adapta-
tions to the MLN when switching between tasks.
The framework achieves reasonable results on both
tasks. On STS, we obtain a correlation of r = 0.66
with full logic, r = 0.73 in a system with weak-
ened variable binding, and r = 0.85 in an ensemble
model. On RTE-1 we obtain an accuracy of 0.57.
We show that the distributional inference rules ben-
efit both tasks and that more flexible probabilistic
combinations of evidence are crucial for STS. Al-
though other approaches could be adapted to handle
both RTE and STS, we do not know of any other
methods that have been explicitly tested on both
problems.
2 Related work
Distributional semantics Distributional models
define the semantic relatedness of words as the
similarity of vectors representing the contexts in
which they occur (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Recently, such mod-
els have also been used to represent the meaning
of larger phrases. The simplest models compute
a phrase vector by adding the vectors for the indi-
vidual words (Landauer and Dumais, 1997) or by a
component-wise product of word vectors (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010).
Other approaches, in the emerging area of distribu-
tional compositional semantics, use more complex
methods that compute phrase vectors from word
vectors and tensors (Baroni and Zamparelli, 2010;
Grefenstette and Sadrzadeh, 2011).
Wide-coverage logic-based semantics Boxer
(Bos, 2008) is a software package for wide-coverage
semantic analysis that produces logical forms using
Discourse Representation Structures (Kamp and
Reyle, 1993). It builds on the C&C CCG parser
(Clark and Curran, 2004).
Markov Logic In order to combine logical and
probabilistic information, we draw on existing work
in Statistical Relational AI (Getoor and Taskar,
2007). Specifically, we utilize Markov Logic Net-
works (MLNs) (Domingos and Lowd, 2009), which
employ weighted formulas in first-order logic to
compactly encode complex undirected probabilistic
graphical models. MLNs are well suited for our ap-
proach since they provide an elegant framework for
assigning weights to first-order logical rules, com-
bining a diverse set of inference rules and perform-
ing sound probabilistic inference.
An MLN consists of a set of weighted first-order
clauses. It provides a way of softening first-order
logic by allowing situations in which not all clauses
are satisfied. More specifically, they provide a
well-founded probability distribution across possi-
ble worlds by specifying that the probability of a
12
world increases exponentially with the total weight
of the logical clauses that it satisfies. While methods
exist for learning MLN weights directly from train-
ing data, since the appropriate training data is lack-
ing, our approach uses weights computed using dis-
tributional semantics. We use the open-source soft-
ware package Alchemy (Kok et al, 2005) for MLN
inference, which allows computing the probability
of a query literal given a set of weighted clauses as
background knowledge and evidence.
Tasks: RTE and STS Recognizing Textual En-
tailment (RTE) is the task of determining whether
one natural language text, the premise, implies an-
other, the hypothesis. Consider (1) below.
(1) p: Oracle had fought to keep the forms from
being released
h: Oracle released a confidential document
Here, h is not entailed. RTE directly tests whether
a system can construct semantic representations that
allow it to draw correct inferences. Of existing RTE
approaches, the closest to ours is by Bos and Mark-
ert (2005), who employ a purely logical approach
that uses Boxer to convert both the premise and hy-
pothesis into first-order logic and then checks for
entailment using a theorem prover. By contrast, our
approach uses Markov logic with probabilistic infer-
ence.
Semantic Textual Similarity (STS) is the task of
judging the similarity of two sentences on a scale
from 0 to 5 (Agirre et al, 2012). Gold standard
scores are averaged over multiple human annota-
tions. The best performer in 2012?s competition was
by Ba?r et al (2012), an ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics.
Weighted inference, and combined structural-
distributional representations One approach to
weighted inference in NLP is that of Hobbs et al
(1993), who proposed viewing natural language in-
terpretation as abductive inference. In this frame-
work, problems like reference resolution and syntac-
tic ambiguity resolution become inferences to best
explanations that are associated with costs. How-
ever, this leaves open the question of how costs are
assigned. Raina et al (2005) use this framework for
RTE, deriving inference costs from WordNet simi-
larity and properties of the syntactic parse.
Garrette et al (2011; 2013) proposed an approach
to RTE that uses MLNs to combine traditional log-
ical representations with distributional information
in order to support probabilistic textual inference.
This approach can be viewed as a bridge between
Bos and Markert (2005)?s purely logical approach,
which relies purely on hard logical rules and the-
orem proving, and distributional approaches, which
support graded similarity between concepts but have
no notion of logical operators or entailment.
There are also other methods that combine dis-
tributional and structured representations. Stern et
al. (2011) conceptualize textual entailment as tree
rewriting of syntactic graphs, where some rewrit-
ing rules are distributional inference rules. Socher
et al (2011) recognize paraphrases using a ?tree of
vectors,? a phrase structure tree in which each con-
stituent is associated with a vector, and overall sen-
tence similarity is computed by a classifier that inte-
grates all pairwise similarities. (This is in contrast to
approaches like Baroni and Zamparelli (2010) and
Grefenstette and Sadrzadeh (2011), who do not of-
fer a proposal for using vectors at multiple levels in
a syntactic tree simultaneously.)
3 MLN system
Our system extends that of Garrette et al (2011;
2013) to support larger-scale evaluation on standard
benchmarks for both RTE and STS. We conceptual-
ize both tasks as probabilistic entailment in Markov
logic, where STS is judged as the average degree of
mutual entailment, i.e. we compute the probability
of both S1 |= S2 and S2 |= S1 and average the re-
sults. Below are some sentence pairs that we use as
examples in the discussion below:
(2) S1: A man is slicing a cucumber.
S2: A man is slicing a zucchini.
(3) S1: A boy is riding a bicycle.
S2: A little boy is riding a bike.
(4) S1: A man is driving.
S2: A man is driving a car.
13
System overview. To compute the probability of
an entailment S1 |= S2, the system first constructs
logical forms for each sentence using Boxer and
then translates them into MLN clauses. In example
(2) above, the logical form for S1:
?x0, e1, x2
(
man(x0) ? slice(e1) ?Agent(e1, x0)?
cucumber(x2) ? Patient(e1, x2)
)
is used as evidence, and the logical form for S2 is
turned into the following formula (by default, vari-
ables are assumed to be universally quantified):
man(x) ? slice(e) ?Agent(e, x)?
zucchini(y) ? Patient(e, y)? result()
where result() is the query for which we have
Alchemy compute the probability.
However, S2 is not strictly entailed by S1 because
of the mismatch between ?cucumber? and ?zuc-
chini?, so with just the strict logical-form transla-
tions of S1 and S2, the probability of result() will
be zero. This is where we introduce distributional
similarity, in this case the similarity of ?cucumber?
and ?zucchini?, cos( #                  ?cucumber, #               ?zucchini). We cre-
ate inference rules from such similarities as a form
of background knowledge. We then treat similarity
as degree of entailment, a move that has a long tradi-
tion (e.g., (Lin and Pantel, 2001b; Raina et al, 2005;
Szpektor and Dagan, 2008)). In general, given two
words a and b, we transform their cosine similarity
into an inference-rule weight wt(a, b) using:
wt(a, b) = log( cos(
#?a , #?b )
1? cos( #?a , #?b )
)? prior (5)
Where prior is a negative weight used to initialize
all predicates, so that by default facts are assumed
to have very low probability. In our experiments,
we use prior = ?3. In the case of sentence pair
(2), we generate the inference rule:
cucumber(x)? zucchini(x) | wt(cuc., zuc.)
Such inference rules are generated for all pairs of
words (w1, w2) where w1 ? S1 and w2 ? S2.1
1We omit inference rules for words (a, b) where cos(a, b) <
? for a threshold ? set to maximize performance on the training
data. Low-similarity pairs usually indicate dissimilar words.
This removes a sizeable number of rules for STS, while for RTE
the tuned threshold was near zero.
The distributional model we use contains all lem-
mas occurring at least 50 times in the Gigaword cor-
pus (Graff et al, 2007) except a list of stop words.
The dimensions are the 2,000 most frequent of these
words, and cell values are weighted with point-wise
mutual information. 2
Phrase-based inference rules. Garrette et al only
considered distributional inference rules for pairs of
individual words. We extend their approach to dis-
tributional inference rules for pairs of phrases in or-
der to handle cases like (3). To properly estimate
the similarity between S1 and S2 in (3), we not only
need an inference rule linking ?bike? to ?bicycle?,
but also a rule estimating how similar ?boy? is to
?little boy?. To do so, we make use of existing ap-
proaches that compute distributional representations
for phrases. In particular, we compute the vector for
a phrase from the vectors of the words in that phrase,
using either vector addition (Landauer and Dumais,
1997) or component-wise multiplication (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010). The
inference-rule weight, wt(p1, p2), for two phrases
p1 and p2 is then determined using Eq. (5) in the
same way as for words.
Existing approaches that derive phrasal inference
rules from distributional similarity (Lin and Pantel,
2001a; Szpektor and Dagan, 2008; Berant et al,
2011) precompile large lists of inference rules. By
comparison, distributional phrase similarity can be
seen as a generator of inference rules ?on the fly?,
as it is possible to compute distributional phrase
vectors for arbitrary phrases on demand as they are
needed for particular examples.
Inference rules are generated for all pairs of con-
stituents (c1, c2) where c1 ? S1 and c2 ? S2, a
constituent is a single word or a phrase. The log-
ical form provides a handy way to extract phrases,
as they are generally mapped to one of two logical
constructs. Either we have multiple single-variable
predicates operating on the same variable. For ex-
ample the phrase ?a little boy? has the logical form
boy(x) ? little(x). Or we have two unary predi-
cates connected with a relation. For example, ?pizza
slice? and ?slice of pizza? are both mapped to the
2It is customary to transform raw counts in a way that cap-
tures association between target words and dimensions, for ex-
ample through point-wise mutual information (Lowe, 2001).
14
logical form, slice(x0) ? of(x0, x1) ? pizza(x1).
We consider all binary predicates as relations.
Average Combiner to determine similarity in the
presence of missing phrases. The logical forms
for the sentences in (4): are
S1: ?x0, e1
(
man(x0)?agent(x0, e1)?drive(e1)
)
S2: ?x0, e1, x2
(
man(x0) ? agent(x0, e1) ?
drive(e1) ? patient(e1, x2) ? car(x2)
)
If we try to prove S1 |= S2, the probability of
the result() will be zero: There is no evidence for
a car, and the hypothesis predicates are conjoined
using a deterministic AND. For RTE, this makes
sense: If one of the hypothesis predicates is False,
the probability of entailment should be zero. For the
STS task, this should in principle be the same, at
least if the omitted facts are vital, but it seems that
annotators rated the data points in this task more for
overall similarity than for degrees of entailment. So
in STS, we want the similarity to be a function of
the number of elements in the hypothesis that are
inferable. Therefore, we need to replace the deter-
ministic AND with a different way of combining
evidence. We chose to use the average evidence
combiner for MLNs introduced by Natarajan et al
(2010). To use the average combiner, the full logi-
cal form is divided into smaller clauses (which we
call mini-clauses), then the combiner averages their
probabilities. In case the formula is a list of con-
juncted predicates, a mini-clause is a conjunction
of a single-variable predicate with a relation predi-
cate(as in the example below). In case the logical
form contains a negated sub-formula, the negated
sub-formula is also a mini-clause. The hypothesis
above after dividing clauses for the average com-
biner looks like this:
man(x0) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? patient(e1, x2)? result(x0, e1, x2) | w
car(x2) ? patient(e1, x2)? result(x0, e1, x2) | w
where result is again the query predicate. Here,
result has all of the variables in the clause as argu-
ments in order to maintain the binding of variables
across all of the mini-clauses. The weights w are the
following function of n, the number of mini-clauses
(4 in the above example):
w = 1n ? (log(
p
1? p)? prior) (6)
where p is a value close to 1 that is set to maximize
performance on the training data, and prior is the
same negative weight as before. Setting w this way
produces a probability of p for the result() in cases
that satisfy the antecedents of all mini-clauses. For
the example above, the antecedents of the first two
mini-clauses are satisfied, while the antecedents of
the last two are not since the premise provides no
evidence for an object of the verb drive. The simi-
larity is then computed to be the maximum probabil-
ity of any grounding of the result predicate, which
in this case is around p2 .
3
An interesting variation of the average combiner
is to omit variable bindings between the mini-
clauses. In this case, the hypothesis clauses look
like this for our example:
man(x) ? agent(x, e)? result() | w
drive(e) ? agent(x, e)? result() | w
drive(e) ? patient(e, x)? result() | w
car(x) ? patient(e, x)? result() | w
This implementation loses a lot of information,
for example it does not differentiate between ?A
man is walking and a woman is driving? and ?A
man is driving and a woman is walking?. In fact,
logical form without variable binding degrades to a
representation similar to a set of independent syn-
tactic dependencies, 4 while the average combiner
with variable binding retains all of the information
in the original logical form. Still, omitting variable
binding turns out to be useful for the STS task.
It is also worth commenting on the efficiency of
the inference algorithm when run on the three dif-
ferent approaches to combining evidence. The aver-
age combiner without variable binding is the fastest
and has the least memory requirements because all
cliques in the ground network are of limited size
(just 3 or 4 nodes). Deterministic AND is much
slower than the average combiner without variable
binding, because the maximum clique size depends
on the sentence. The average combiner with vari-
able binding is the most memory intensive since the
3One could also give mini-clauses different weights depend-
ing on their importance, but we have not experimented with this
so far.
4However, it is not completely the same since we do not
divide up formulas under negation into mini-clauses.
15
number of arguments of the result() predicate can
become large (there is an argument for each individ-
ual and event in the sentence). Consequently, the
inference algorithm needs to consider a combinato-
rial number of possible groundings of the result()
predicate, making inference very slow.
Adaptation of the logical form. As discussed by
Garrette et al (2011), Boxer?s output is mapped to
logical form and augmented with additional infor-
mation to handle a variety of semantic phenomena.
However, we do not use their additional rules for
handling implicatives and factives, as we wanted to
test the system without background knowledge be-
yond that supplied by the vector space.
Unfortunately, current MLN inference algorithms
are not able to efficiently handle complex formu-
las with nested quantifiers. For that reason, we re-
placed universal quantifiers in Boxer?s output with
existentials since they caused serious problems for
Alchemy. Although this is a radical change to the
semantics of the logical form, due to the nature of
the STS and RTE data, it only effects about 5% of
the sentences, and we found that most of the uni-
versal quantifiers in these cases were actually due
to parsing errors. We are currently exploring more
effective ways of dealing with this issue.
4 Task 1: Recognizing Textual Entailment
4.1 Dataset
In order to compare directly to the logic-based sys-
tem of Bos and Markert (2005), we focus on the
RTE-1 dataset (Dagan et al, 2005), which includes
567 Text-Hypothesis (T-H) pairs in the development
set and 800 pairs in the test set. The data covers a
wide range of issues in entailment, including lexical,
syntactic, logical, world knowledge, and combina-
tions of these at different levels of difficulty. In both
development and test sets, 50% of sentence pairs are
true entailments and 50% are not.
4.2 Method
We run our system for different configurations of in-
ference rules and evidence combiners. For distri-
butional inference rules (DIR), three different lev-
els are tested: without inference rules (no DIR),
inference rules for individual words (word DIR),
and inference rules for words and phrases (phrase
DIR). Phrase vectors were built using vector addi-
tion, as point-wise multiplication performed slightly
worse. To combine evidence for the result() query,
three different options are available: without av-
erage combiner which is just using Deterministic
AND (Deterministic AND), average combiner with
variable binding (AvgComb) and average combiner
without variable binding (AvgComb w/o VarBind).
Different combinations of configurations are tested
according to its suitability for the task; RTE and
STS.
We also tested several ?distributional only? sys-
tems. The first such system builds a vector represen-
tation for each sentence by adding its word vectors,
then computes the cosine similarity between the sen-
tence vectors for S1 and S2 (VS-Add). The second
uses point-wise multiplication instead of vector ad-
dition (VS-Mul). The third scales pairwise words
similarities to the sentence level using weighted av-
erage where weights are inverse document frequen-
cies idf as suggested by Mihalcea et al (2006) (VS-
Pairwise).
For the RTE task, systems were evaluated using
both accuracy and confidence-weighted score (cws)
as used by Bos and Markert (2005) and the RTE-
1 challenge (Dagan et al, 2005). In order to map
a probability of entailment to a strict prediction of
True or False, we determined a threshold that op-
timizes performance on the development set. The
cws score rewards a system?s ability to assign higher
confidence scores to correct predictions than incor-
rect ones. For cws, a system?s predictions are sorted
in decreasing order of confidence and the score is
computed as:
cws = 1n
n?
i=1
#correct-up-to-rank-i
i
where n is the number of the items in the test set,
and i ranges over the sorted items. In our systems,
we defined the confidence value for a T-H pair as
the distance between the computed probability for
the result() predicate and the threshold.
4.3 Results
The results are shown in Table 1. They show
that the distributional only baselines perform very
poorly. In particular, they perform worse than strict
16
Method acc cws
Chance 0.50 0.50
Bos & Markert, strict 0.52 0.55
Best system in RTE-1 challenge
(Bayer et al, 2005)
0.59 0.62
VS-Add 0.49 0.53
VS-Mul 0.51 0.52
VS-Pairwise 0.50 0.50
AvgComb w/o VarBind + phrase
DIR
0.52 0.53
Deterministic AND + phrase DIR 0.57 0.57
Table 1: Results on the RTE-1 Test Set.
entailment from Bos and Markert (2005), a system
that uses only logic. This illustrates the important
role of logic-based representations for the entail-
ment task. Due to intractable memory demands of
Alchemy inference, our current system with deter-
ministic AND fails to execute on 118 of the 800 test
pairs, so, by default, the system classifies these cases
as False (non-entailing) with very low confidence.
Comparing the two configurations of our system,
using deterministic AND vs. the average combiner
without variable binding (last two lines in Table 1),
we see that for RTE, it is essential to retain the full
logical form.
Our system with deterministic AND obtains both
an accuracy and cws of 0.57. The best result in
the RTE-1 challenge by Bayer et al (2005) ob-
tained an accuracy of 0.59 and cws of 0.62. 5 In
terms of both accuracy and cws, our system outper-
forms both ?distributional only? systems and strict
logical entailment, showing again that integrating
both logical form and distributional inference rules
using MLNs is beneficial. Interestingly, the strict
entailment system of Bos and Markert incorporated
generic knowledge, lexical knowledge (from Word-
Net) and geographical knowledge that we do not
utilize. This demonstrates the advantage of us-
ing a model that operationalizes entailment between
words and phrases as distributional similarity.
5On other RTE datasets there are higher previous results.
Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the com-
bined RTE-2 and RTE-3 dataset.
5 Task 2: Semantic Textual Similarity
5.1 Dataset
The dataset we use in our experiments is the MSR
Video Paraphrase Corpus (MSR-Vid) subset of the
STS 2012 task, consisting of 1,500 sentence pairs.
The corpus itself was built by asking annotators
from Amazon Mechanical Turk to describe very
short video fragments (Chen and Dolan, 2011). The
organizers of the STS 2012 task (Agirre et al, 2012)
sampled video descriptions and asked Turkers to as-
sign similarity scores (ranging from 0 to 5) to pairs
of sentences, without access to the video. The gold
standard score is the average of the Turkers? annota-
tions. In addition to the MSR Video Paraphrase Cor-
pus subset, the STS 2012 task involved data from
machine translation and sense descriptions. We do
not use these because they do not consist of full
grammatical sentences, which the parser does not
handle well. In addition, the STS 2012 data included
sentences from the MSR Paraphrase Corpus, which
we also do not currently use because some sentences
are long and create intractable MLN inference prob-
lems. This issue is discussed further in section 6.
Following STS standards, our evaluation compares
a system?s similarity judgments to the gold standard
scores using Pearson?s correlation coefficient r.
5.2 Method
Our system can be tested for different configuration
of inference rules and evidence combiners which
are explained in section 4.2. The tested systems on
the STS task are listed in table 2. Out experiments
showed that using average combiner (AvgComb) is
very memory intensive and MLN inference for 28 of
the 1,500 pairs either ran out of memory or did not
finish in reasonable time. In such cases, we back off
to AvgComb w/o VarBind.
We compare to several baselines; our MLN
system without distributional inference rules
(AvgComb + no DIR), and distributional-only
systems (VS-Add, VS-Mul, VS-Pairwise). These
are the natural baselines for our system, since they
use only one of the two types of information that
we combine (i.e. logical form and distributional
representations).
Finally, we built an ensemble that combines the
output of multiple systems using regression trained
17
Method r
AvgComb + no DIR 0.58
AvgComb + word DIR 0.60
AvgComb + phrase DIR 0.66
AvgComb w/o VarBind + no DIR 0.58
AvgComb w/o VarBind + word DIR 0.60
AvgComb w/o VarBind + phrase DIR 0.73
VS-Add 0.78
VS-Mul 0.58
VS-Pairwise 0.77
Ensemble (VS-Add + VS-Mul + VS-
Pairwise)
0.83
Ensemble ([AvgComb + phrase DIR] +
VS-Add + VS-Mul + VS-Pairwise)
0.85
Best MSR-Vid score in STS 2012 (Ba?r
et al, 2012)
0.87
Table 2: Results on the STS video dataset.
on the training data. We then compare the perfor-
mance of an ensemble with and without our sys-
tem. This is the same technique used by Ba?r et al
(2012) except we used additive regression (Fried-
man, 2002) instead of linear regression since it gave
better results.
5.3 Results
Table 2 summarizes the results of our experiments.
They show that adding distributional information
improves results, as expected, and also that adding
phrase rules gives further improvement: Using only
word distributional inference rules improves results
from 0.58 to 0.6, and adding phrase inference rules
further improves them to 0.66. As for variable bind-
ing, note that although it provides more precise in-
formation, the STS scores actually improve when it
is dropped, from 0.66 to 0.73. We offer two expla-
nations for this result: First, this information is very
sensitive to parsing errors, and the C&C parser, on
which Boxer is based, produces many errors on this
dataset, even for simple sentences. When the C&C
CCG parse is wrong, the resulting logical form is
wrong, and the resulting similarity score is greatly
affected. Dropping variable binding makes the sys-
tems more robust to parsing errors. Second, in con-
trast to RTE, the STS dataset does not really test the
important role of syntax and logical form in deter-
mining meaning. This also explains why the ?dis-
tributional only? baselines are actually doing better
than the MLN systems.
Although the MLN system on its own does not
perform better than the distributional compositional
models, it does provide complementary information,
as shown by the fact that ensembling it with the rest
of the models improves performance (0.85 with the
MLN system, compared to 0.83 without it). The per-
formance of this ensemble is close to the current best
result for this dataset (0.87).
6 Future Work
The approach presented in this paper constitutes a
step towards achieving the challenging goal of effec-
tively combining logical representations with dis-
tributional information automatically acquired from
text. In this section, we discuss some of limita-
tions of the current work and directions for future
research.
As noted before, parse errors are currently a sig-
nificant problem. We use Boxer to obtain a logi-
cal representation for a sentence, which in turn re-
lies on the C&C parser. Unfortunately, C&C mis-
parses many sentences, which leads to inaccurate
logical forms. To reduce the impact of misparsing,
we plan to use a version of C&C that can produce
the top-n parses together with parse re-ranking (Ng
and Curran, 2012). As an alternative to re-ranking,
one could obtain logical forms for each of the top-
n parses, and create an MLN that integrates all of
them (together with their certainty) as an underspec-
ified meaning representation that could then be used
to directly support inferences such as STS and RTE.
We also plan to exploit a greater variety of dis-
tributional inference rules. First, we intend to in-
corporate logical form translations of existing dis-
tributional inference rule collections (e.g., (Berant
et al, 2011; Chan et al, 2011)). Another issue
is obtaining improved rule weights based on dis-
tributional phrase vectors. We plan to experiment
with more sophisticated approaches to computing
phrase vectors such as those recently presented by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011). Furthermore, we are currently
deriving symmetric similarity ratings between word
pairs or phrase pairs, when really what we need is di-
18
rectional similarity. We plan to incorporate directed
similarity measures such as those of Kotlerman et al
(2010) and Clarke (2012).
A primary problem for our approach is the limita-
tions of existing MLN inference algorithms, which
do not effectively scale to large and complex MLNs.
We plan to explore ?coarser? logical representa-
tions such as Minimal Recursion Semantics (MRS)
(Copestake et al, 2005). Another potential approach
to this problem is to trade expressivity for efficiency.
Domingos and Webb (2012) introduced a tractable
subset of Markov Logic (TML) for which a future
software release is planned. Formulating the infer-
ence problem in TML could potentially allow us to
run our system on longer and more complex sen-
tences.
7 Conclusion
In this paper we have used an approach that com-
bines logic-based and distributional representations
for natural language meaning. It uses logic as
the primary representation, transforms distributional
similarity judgments to weighted inference rules,
and uses Markov Logic Networks to perform in-
ferences over the weighted clauses. This approach
views textual entailment and sentence similarity as
degrees of ?logical? entailment, while at the same
time using distributional similarity as an indicator
of entailment at the word and short phrase level. We
have evaluated the framework on two different tasks,
RTE and STS, finding that it is able to handle both
tasks given that we adapt the way evidence is com-
bined in the MLN. Even though other entailment
models could be applied to STS, given that similar-
ity can obviously be operationalized as a degree of
mutual entailment, this has not been done before to
our best knowledge. Our framework achieves rea-
sonable results on both tasks. On RTE-1 we obtain
an accuracy of 0.57. On STS, we obtain a correla-
tion of r = 0.66 with full logic, r = 0.73 in a system
with weakened variable binding, and r = 0.85 in an
ensemble model. We find that distributional word
and phrase similarity, used as textual inference rules
on the fly, leads to sizeable improvements on both
tasks. We also find that using more flexible proba-
bilistic combinations of evidence is crucial for STS.
Acknowledgements
This research was supported in part by the NSF CA-
REER grant IIS 0845925, by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026, by
MURI ARO grant W911NF-08-1-0242 and by an
NDSEG grant. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the author and do not necessarily reflect
the view of DARPA, AFRL, ARO, DoD or the US
government.
Some of our experiments were run on the
Mastodon Cluster supported by NSF Grant EIA-
0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
SemEval.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In SemEval-2012.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association for
Computational Linguistics.
Samuel Bayer, John Burger, Lisa Ferro, John Hender-
son, and Alexander Yeh. 2005. MITREs Submissions
to the EU Pascal RTE Challenge. In In Proceedings
of the PASCAL Challenges Workshop on Recognising
Textual Entailment, pages 41?44.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceedings
of EMNLP 2005, pages 628?635, Vancouver, B.C.,
Canada.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277?286. College Publications.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin
Van Durme. 2011. Reranking bilingually extracted
19
paraphrases using monolingual distributional similar-
ity. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33?42, Edinburgh, UK.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 190?200,
Portland, Oregon, USA, June.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of ACL 2004, pages 104?111, Barcelona, Spain.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1).
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
3(2-3):281?332.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 1?8.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Synthesis Lectures on Artificial Intelligence and Ma-
chine Learning. Morgan & Claypool Publishers.
Pedro Domingos and W Austin Webb. 2012. A tractable
first-order probabilistic logic. In Proceedings of the
Twenty-Sixth National Conference on Artificial Intel-
ligence.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008, pages 897?906, Honolulu,
HI.
Jerome H Friedman. 2002. Stochastic gradient boosting.
Computational Statistics & Data Analysis, 38(4):367?
378.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2011.
Integrating logical representations with probabilistic
information using markov logic. In Proceedings of
IWCS, Oxford, UK.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2013.
A formal approach to linking logical form and vector-
space lexical semantics. In Harry Bunt, Johan Bos,
and Stephen Pulman, editors, Computing Meaning,
Vol. 4.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC2007T07.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Andrew Hickl. 2008. Using Discourse Commitments
to Recognize Textual Entailment. In Proceedings of
COLING 2008, pages 337?344.
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and DRT. Kluwer,
Dordrecht.
Stanley Kok, Parag Singla, Matthew Richardson, and Pe-
dro Domingos. 2005. The Alchemy system for sta-
tistical relational AI. Technical report, Department
of Computer Science and Engineering, University
of Washington. http://www.cs.washington.
edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04):359?389.
Thomas Landauer and Susan Dumais. 1997. A solution
to Platos problem: the latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
Dekang Lin and Patrick Pantel. 2001a. DIRT - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Dekang Lin and Patrick Pantel. 2001b. Discovery of
inference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the Cognitive Science Society, pages
576?581.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244.
20
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1970. Universal grammar. Theoria,
36:373?398. Reprinted in Thomason (1974), pp 7-27.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik. 2010.
Exploiting causal independence in markov logic net-
works: Combining undirected and directed models.
In Proceedings of European Conference in Machine
Learning (ECML), Barcelona, Spain.
Dominick Ng and James R Curran. 2012. Dependency
hashing for n-best ccg parsing. In In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1).
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng,
and Christopher Manning. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase
detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett,
F.C.N. Pereira, and K.Q. Weinberger, editors, Pro-
ceedings of NIPS.
Asher Stern, Amnon Lotan, Shachar Mirkin, Eyal
Shnarch, Lili Kotlerman, Jonathan Berant, and Ido Da-
gan. 2011. Knowledge and tree-edits in learnable en-
tailment proofs. In TAC, Gathersburg, MD.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, pages 948?957, Uppsala, Sweden.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy. Selected Papers of Richard Montague. Yale Uni-
versity Press, New Haven.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
21
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796?801,
Dublin, Ireland, August 23-24, 2014.
UTexas: Natural Language Semantics using Distributional Semantics and
Probabilistic Logic
Islam Beltagy
?
, Stephen Roller
?
, Gemma Boleda
?
, Katrin Erk
?
, Raymond J. Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
{beltagy, roller, mooney}@cs.utexas.edu
gemma.boleda@upf.edu, katrin.erk@mail.utexas.edu
Abstract
We represent natural language semantics
by combining logical and distributional in-
formation in probabilistic logic. We use
Markov Logic Networks (MLN) for the
RTE task, and Probabilistic Soft Logic
(PSL) for the STS task. The system is
evaluated on the SICK dataset. Our best
system achieves 73% accuracy on the RTE
task, and a Pearson?s correlation of 0.71 on
the STS task.
1 Introduction
Textual Entailment systems based on logical infer-
ence excel in correct reasoning, but are often brit-
tle due to their inability to handle soft logical in-
ferences. Systems based on distributional seman-
tics excel in lexical and soft reasoning, but are un-
able to handle phenomena like negation and quan-
tifiers. We present a system which takes the best
of both approaches by combining distributional se-
mantics with probabilistic logical inference.
Our system builds on our prior work (Belt-
agy et al., 2013; Beltagy et al., 2014a; Beltagy
and Mooney, 2014; Beltagy et al., 2014b). We
use Boxer (Bos, 2008), a wide-coverage semantic
analysis tool to map natural sentences to logical
form. Then, distributional information is encoded
in the form of inference rules. We generate lexical
and phrasal rules, and experiment with symmetric
and asymmetric similarity measures. Finally, we
use probabilistic logic frameworks to perform in-
ference, Markov Logic Networks (MLN) for RTE,
and Probabilistic Soft Logic (PSL) for STS.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle ?graded? aspects of meaning in language
because they are binary by nature.
2.2 Distributional Semantics
Distributional models use statistics of word co-
occurrences to predict semantic similarity of
words and phrases (Turney and Pantel, 2010;
Mitchell and Lapata, 2010), based on the obser-
vation that semantically similar words occur in
similar contexts. Words are represented as vec-
tors in high dimensional spaces generated from
their contexts. Also, it is possible to compute vec-
tor representations for larger phrases composition-
ally from their parts (Mitchell and Lapata, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010). Distributional similarity is usually a
mixture of semantic relations, but particular asym-
metric similarity measures can, to a certain ex-
tent, predict hypernymy and lexical entailment
distributionally (Kotlerman et al., 2010; Lenci and
Benotto, 2012; Roller et al., 2014). Distribu-
tional models capture the graded nature of mean-
ing, but do not adequately capture logical struc-
ture (Grefenstette, 2013).
2.3 Markov Logic Network
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
796
MLNs define a probability distribution over pos-
sible worlds, where the probability of a world in-
creases exponentially with the total weight of the
logical clauses that it satisfies. A variety of in-
ference methods for MLNs have been developed,
however, computational overhead is still an issue.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms (i.e., atoms without vari-
ables) have soft, continuous truth values on the
interval [0, 1] rather than binary truth values as
used in MLNs and most other probabilistic logics.
Given a set of weighted inference rules, and with
the help of Lukasiewicz?s relaxation of the logical
operators, PSL builds a graphical model defining a
probability distribution over the continuous space
of values of the random variables in the model
(Kimmig et al., 2012). Then, PSL?s MPE infer-
ence (Most Probable Explanation) finds the overall
interpretation with the maximum probability given
a set of evidence. This optimization problem is a
second-order cone program (SOCP) (Kimmig et
al., 2012) and can be solved in polynomial time.
2.5 Recognizing Textual Entailment
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or is not related
(Neutral) to another, the hypothesis.
2.6 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system?s output and
gold standard scores.
3 Approach
3.1 Logical Representation
The first component in the system is Boxer (Bos,
2008), which maps the input sentences into logical
form, in which the predicates are words in the sen-
tence. For example, the sentence ?A man is driving
a car? in logical form is:
?x, y, z. man(x) ? agent(y, x) ? drive(y) ?
patient(y, z) ? car(z)
3.2 Distributional Representation
Next, distributional information is encoded in
the form of weighted inference rules connecting
words and phrases of the input sentences T and H .
For example, for sentences T : ?A man is driving
a car?, and H: ?A guy is driving a vehicle?, we
would like to generate rules like ?x. man(x) ?
guy(x) |w
1
, ?x.car(x)? vehicle(x) |w
2
, where
w
1
and w
2
are weights indicating the similarity of
the antecedent and consequent of each rule.
Inferences rules are generated as in Beltagy et
al. (2013). Given two input sentences T and H ,
for all pairs (a, b), where a and b are words or
phrases of T and H respectively, generate an infer-
ence rule: a ? b | w, where the rule weight w is
a function of sim(
??
a ,
??
b ), and sim is a similarity
measure of the distributional vectors
??
a ,
??
b . We
experimented with the symmetric similarity mea-
sure cosine, and asym, the supervised, asymmet-
ric similarity measure of Roller et al. (2014).
The asym measure uses the vector difference
(
??
a ?
??
b ) as features in a logistic regression clas-
sifier for distinguishing between four different
word relations: hypernymy, cohyponymy, meron-
omy, and no relation. The model is trained us-
ing the noun-noun subset of the BLESS data set
(Baroni and Lenci, 2011). The final similarity
weight is given by the model?s estimated probabil-
ity that the word relationship is either hypernymy
or meronomy: asym(
??
a ,
??
b ) = P (hyper(a, b))+
P (mero(a, b)).
Distributional representations for words are de-
rived by counting co-occurrences in the ukWaC,
WaCkypedia, BNC and Gigaword corpora. We
use the 2000 most frequent content words as ba-
sis dimensions, and count co-occurrences within
a two word context window. The vector space is
weighted using Positive Pointwise Mutual Infor-
mation.
Phrases are defined in terms of Boxer?s output
to be more than one unary atom sharing the same
variable like ?a little kid? (little(k) ? kid(k)),
or two unary atoms connected by a relation like
?a man is driving? (man(m) ? agent(d,m) ?
drive(d)). We compute vector representations of
797
phrases using vector addition across the compo-
nent predicates. We also tried computing phrase
vectors using component-wise vector multiplica-
tion (Mitchell and Lapata, 2010), but found it per-
formed marginally worse than addition.
3.3 Probabilistic Logical Inference
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E,RB).
3.4 Task 1: RTE using MLNs
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE classification prob-
lem for the relation between T and H can be
split into two inference tasks. The first is test-
ing if T entails H , Pr(H|T,RB). The second
is testing if the negation of the text ?T entails H ,
Pr(H|?T,RB). In case Pr(H|T,RB) is high,
while Pr(H|?T,RB) is low, this indicates En-
tails. In case it is the other way around, this in-
dicates Contradicts. If both values are close, this
means T does not affect the probability of H and
indicative of Neutral. We train an SVM classifier
with LibSVM?s default parameters to map the two
probabilities to the final decision.
The MLN implementation we use is
Alchemy (Kok et al., 2005). Queries in Alchemy
can only be ground atoms. However, in our
case the query is a complex formula (H). We
extended Alchemy to calculate probabilities of
queries (Beltagy and Mooney, 2014). Probability
of a formula Q given an MLN K equals the ratio
between the partition function Z of the ground
network of K with and without Q added as a hard
rule (Gogate and Domingos, 2011)
P (Q | K) =
Z(K ? {(Q,?)})
Z(K)
(1)
We estimate Z of the ground networks using Sam-
pleSearch (Gogate and Dechter, 2011), an ad-
vanced importance sampling algorithm that is suit-
able for ground networks generated by MLNs.
A general problem with MLN inference is
its computational overhead, especially for the
complex logical formulae generated by our ap-
proach. To make inference faster, we reduce the
size of the ground network through an automatic
type-checking technique proposed in Beltagy and
Mooney (2014). For example, consider the ev-
idence ground atom man(M) denoting that the
constant M is of type man. Then, consider an-
other predicate like car(x). In case there are no in-
ference rule connecting man(x) and car(x), then
we know that M which we know is a man cannot
be a car, so we remove the ground atom car(M)
from the ground network. This technique reduces
the size of the ground network dramatically and
makes inference tractable.
Another problem with MLN inference is that
quantifiers sometimes behave in an undesir-
able way, due to the Domain Closure Assump-
tion (Richardson and Domingos, 2006) that MLNs
make. For example, consider the text-hypothesis
pair: ?There is a black bird? and ?All birds are
black?, which in logic are T : bird(B)?black(B)
and H : ?x. bird(x) ? black(x). Because of
the Domain Closure Assumption, MLNs conclude
that T entails H because H is true for all constants
in the domain (in this example, the single constant
B). We solve this problem by introducing extra
constants and evidence in the domain. In the ex-
ample above, we introduce evidence of a new bird
bird(D), which prevents the hypothesis from be-
ing true. The full details of the technique of deal-
ing with the domain closure is beyond the scope of
this paper.
3.5 Task 2: STS using PSL
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach for computing similarity between struc-
tured objects. We showed in Beltagy et al. (2014a)
how to perform the STS task using PSL. PSL
does not work ?out of the box? for STS, be-
cause Lukasiewicz?s equation for the conjunction
is very restrictive. We address this by replacing
Lukasiewicz?s equation for conjunction with an
averaging equation, then change the optimization
problem and grounding technique accordingly.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where E = S
1
, Q = S
2
and an-
other where E = S
2
, Q = S
1
, and output the two
scores. The final similarity score is produced from
798
an Additive Regression model with WEKA?s de-
fault parameters trained to map the two PSL scores
to the overall similarity score (Friedman, 1999;
Hall et al., 2009).
3.6 Task 3: RTE and STS using Vector
Spaces and Keyword Counts
As a baseline, we also attempt both the RTE and
STS tasks using only vector representations and
unigram counts. This baseline model uses a super-
vised regressor with features based on vector sim-
ilarity and keyword counts. The same input fea-
tures are used for performing RTE and STS, but a
SVM classifier and Additive Regression model is
trained separately for each task. This baseline is
meant to establish whether the task truly requires
the sophisticated logical inference of MLNs and
PSL, or if merely checking for logical keywords
and textual similarity is sufficient.
The first two features are simply the cosine and
asym similarities between the text and hypothesis,
using vector addition of the unigrams to compute
a single vector for the entire sentence.
We also compute vectors for both the text and
hypothesis using vector addition of the mutually
exclusive unigrams (MEUs). The MEUs are de-
fined as the unigrams of the premise and hypoth-
esis with common unigrams removed. For exam-
ple, if the premise is ?A dog chased a cat? and the
hypothesis is ?A dog watched a mouse?, the MEUs
are ?chased cat? and ?watched mouse.? We com-
pute vector addition of the MEUs, and compute
similarity using both the cosine and asym mea-
sures. These form two features for the regressor.
The last feature of the model is a keyword
count. We count how many times 13 different
keywords appear in either the text or the hypoth-
esis. These keywords include negation (no, not,
nobody, etc.) and quantifiers (a, the, some, etc.)
The counts of each keyword form the last 13 fea-
tures as input to the regressor. In total, there are
17 features used in this baseline system.
4 Evaluation
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014 (Marelli et al.,
2014a; Marelli et al., 2014b). The dataset is
10,000 pairs of sentences, 5000 training and 5000
for testing. Sentences are annotated for both tasks.
SICK-RTE SICK-STS
Baseline 70.0 71.1
MLN/PSL + Cosine 72.8 68.6
MLN/PSL + Asym 73.2 68.9
Ensemble 73.2 71.5
Table 1: Test RTE accuracy and STS Correlation.
4.1 Systems Compared
We compare multiple configurations of our proba-
bilistic logic system.
? Baseline: Vector- and keyword-only baseline
described in Section 3.6;
? MLN/PSL + Cosine: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using cosine as a similarity measure;
? MLN/PSL + Asym: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using asym as a similarity measure;
? Ensemble: An ensemble method which uses
all of the features in the above methods as in-
puts for the RTE and STS classifiers.
4.2 Results and Discussion
Table 1 shows our results on the held-out test set
for SemEval 2014 Task 1.
On the RTE task, we see that both the MLN +
Cosine and MLN + Asym models outperformed
the Baseline, indicating that textual entailment re-
quires real inference to handle negation and quan-
tifiers. The MLN + Asym and Ensemble sys-
tems perform identically on RTE, further suggest-
ing that the logical inference subsumes keyword
detection.
The MLN + Asym system outperforms the
MLN + Cosine system, emphasizing the impor-
tance of asymmetric measures for predicting lex-
ical entailment. Intuitively, this makes perfect
sense: dog entails animal, but not vice versa.
In an error analysis performed on a development
set, we found our RTE system was extremely con-
servative: we rarely confused the Entails and Con-
tradicts classes, indicating we correctly predict the
direction of entailment, but frequently misclassify
examples as Neutral. An examination of these ex-
amples showed the errors were mostly due to miss-
ing or weakly-weighted distributional rules.
On STS, our vector space baseline outperforms
both PSL-based systems, but the ensemble outper-
forms any of its components. This is a testament to
799
the power of distributional models in their ability
to predict word and sentence similarity. Surpris-
ingly, we see that the PSL + Asym system slightly
outperforms the PSL + Cosine system. This may
indicate that even in STS, some notion of asymme-
try plays a role, or that annotators may have been
biased by simultaneously annotating both tasks.
As with RTE, the major bottleneck of our system
appears to be the knowledge base, which is built
solely using distributional inference rules.
Results also show that our system?s perfor-
mance is close to the baseline system. One of
the reasons behind that could be that sentences are
not exploiting the full power of logical represen-
tations. On RTE for example, most of the con-
tradicting pairs are two similar sentences with one
of them being negated. This way, the existence
of any negation cue in one of the two sentences is
a strong signal for contradiction, which what the
baseline system does without deeply representing
the semantics of the negation.
5 Conclusion & Future Work
We showed how to combine logical and distribu-
tional semantics using probabilistic logic, and how
to perform the RTE and STS tasks using it. The
system is tested on the SICK dataset.
The distributional side can be extended in many
directions. We would like to use longer phrases,
more sophisticated compositionality techniques,
and contextualized vectors of word meaning. We
also believe inference rules could be dramatically
improved by integrating from paraphrases collec-
tions like PPDB (Ganitkevitch et al., 2013).
Finally, MLN inference could be made more ef-
ficient by exploiting the similarities between the
two ground networks (the one with Q and the one
without). PLS inference could be enhanced by us-
ing a learned, weighted average of rules, rather
than the simple mean.
Acknowledgements
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Some experiments were run on the Mastodon
Cluster supported by NSF Grant EIA-0303609.
The authors acknowledge the Texas Advanced
Computing Center (TACC)
1
for providing grid re-
sources that have contributed to these results. We
thank the anonymous reviewers and the UTexas
1
http://www.tacc.utexas.edu
Natural Language and Learning group for their
helpful comments and suggestions.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1?10, Edinburgh, UK, July. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy and Raymond J. Mooney. 2014. Ef-
ficient Markov logic inference for natural language
semantics. In Proceedings of AAAI 2014 Workshop
on Statistical Relational AI (StarAI-14).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014a. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014b. Semantic parsing using distributional se-
mantics and probabilistic logic. In Proceedings
of ACL 2014 Workshop on Semantic Parsing (SP-
2014).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
J.H. Friedman. 1999. Stochastic gradient boosting.
Technical report, Stanford University.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Vibhav Gogate and Rina Dechter. 2011. Sample-
search: Importance sampling in presence of deter-
minism. Artificial Intelligence, 175(2):694?729.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In 27th Conference on Un-
certainty in Artificial Intelligence (UAI-11).
800
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Stanley Kok, Parag Singla, Matthew Richardson, and
Pedro Domingos. 2005. The Alchemy system
for statistical relational AI. Technical report, De-
partment of Computer Science and Engineering,
University of Washington. http://www.cs.
washington.edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation
of compositional distributional semantic models.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Hrafn Loftsson, Bente
Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, Ice-
land, may. European Language Resources Associa-
tion (ELRA).
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(3):1388?1429.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of the Twenty
Fifth International Conference on Computational
Linguistics (COLING-14), Dublin, Ireland.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
801
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203?212,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joint Entity and Relation Extraction using Card-Pyramid Parsing
Rohit J. Kate and Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
1 University Station C0500
Austin, TX 78712-0233, USA
{rjkate,mooney}@cs.utexas.edu
Abstract
Both entity and relation extraction can
benefit from being performed jointly, al-
lowing each task to correct the errors of
the other. We present a new method for
joint entity and relation extraction using
a graph we call a ?card-pyramid.? This
graph compactly encodes all possible en-
tities and relations in a sentence, reducing
the task of their joint extraction to jointly
labeling its nodes. We give an efficient la-
beling algorithm that is analogous to pars-
ing using dynamic programming. Exper-
imental results show improved results for
our joint extraction method compared to a
pipelined approach.
1 Introduction
Information extraction (IE) is the task of extract-
ing structured information from text. The two
most common sub-tasks of IE are extracting enti-
ties (like Person, Location and Organization) and
extracting relations between them (like Work For
which relates a Person and an Organization, Org-
Based In which relates an Organization and a Lo-
cation etc.). Figure 1 shows a sample sentence an-
notated with entities and relations. The applica-
tion domain and requirements of the downstream
tasks usually dictate the type of entities and rela-
tions that an IE system needs to extract.
Most work in IE has concentrated on entity ex-
traction alone (Tjong Kim Sang, 2002; Sang and
Meulder, 2003) or on relation extraction assum-
ing entities are either given or previously extracted
(Bunescu et al, 2005; Zhang et al, 2006; Giuliano
et al, 2007; Qian et al, 2008). However, these
tasks are very closely inter-related. While iden-
tifying correct entities is essential for identifying
relations between them, identifying correct rela-
tions can in turn improve identification of entities.
For example, if the relation Work For is identified
with high confidence by a relation extractor, then
it can enforce identifying its arguments as Person
and Organization, about which the entity extractor
might not have been confident.
A brute force algorithm for finding the most
probable joint extraction will soon become in-
tractable as the number of entities in a sentence
grows. If there are n entities in a sentence, then
there are O(n2) possible relations between them
and if each relation can take l labels then there are
O(ln2) total possibilities, which is intractable even
for small l and n. Hence, an efficient inference
mechanism is needed for joint entity and relation
extraction.
The only work we are aware of for jointly ex-
tracting entities and relations is by Roth & Yih
(2004; 2007). Their method first identifies the pos-
sible entities and relations in a sentence using sep-
arate classifiers which are applied independently
and then computes a most probable consistent
global set of entities and relations using linear pro-
gramming. In this paper, we present a different ap-
proach to joint extraction using a ?card-pyramid?
graph. The labeled nodes in this graph compactly
encode the possible entities and relations in a sen-
tence. The task of joint extraction then reduces
to finding the most probable joint assignment to
the nodes in the card-pyramid. We give an ef-
ficient dynamic-programming algorithm for this
task which resembles CYK parsing for context-
free grammars (Jurafsky and Martin, 2008). The
algorithm does a beam search and gives an approx-
imate solution for a finite beam size. A natural
advantage of this approach is that extraction from
a part of the sentence is influenced by extraction
from its subparts and vice-versa, thus leading to a
joint extraction. During extraction from a part of
the sentence it also allows use of features based on
the extraction from its sub-parts, thus leading to a
more integrated extraction. We use Roth & Yih?s
203
John lives in Los Angeles , California and works there for an American company called ABC Inc .
Person Location Location
Live_In
Located_In
Work_For
OrgBased_In
OrgBased_In
Other Organization
   0      1     2     3      4        5        6          7       8        9   10  11        12            13        14      15   16 17  
Live_In
Figure 1: A sentence shown with entities and relations.
Figure 2: A pyramid built out of playing-cards.
(2004; 2007) dataset in our experiments and show
that card-pyramid parsing improves accuracy over
both their approach and a pipelined extractor.
2 Card-Pyramid Parsing for Joint
Extraction
In this section, we first introduce the card-pyramid
structure and describe how it represents entities
and their relations in a sentence. We then describe
an efficient algorithm for doing joint extraction us-
ing this structure.
2.1 Card-Pyramid Structure
We define a binary directed graph we call a card-
pyramid because it looks like a pyramid built out
of playing-cards as shown in Figure 2. A card-
pyramid is a ?tree-like? graph with one root, in-
ternal nodes, and leaves, such that if there are
n leaves, then there are exactly n levels with a
decreasing number of nodes from bottom to top,
leaves are at the lowest level (0) and the root is
at the highest level (n ? 1) (see Figure 3 for an
example). In addition, every non-leaf node at po-
Live_In
Live_In NR OrgBased_In
NR OrgBased_In
Work_For
Location Location Other Organization
Located_In NRNRLevel 1
Level 2
Level 3
Level 4
Person
0
2
10
0 1
0 1 3
(0?0) (3?4) (6?6) (12?12) (15?16)
0 1 2 3 4
2
Level 0
Figure 3: The card-pyramid for the sentence shown in Fig-
ure 1. Levels are shown by the horizontal lines under which
the positions of its nodes are indicated.
sition i in level l is the parent of exactly two nodes
at positions i and i + 1 at level l ? 1. Note that
a card-pyramid is not a tree because many of its
nodes have two parents. A useful property of a
card-pyramid is that a non-leaf node at position i
in level l is always the lowest common ancestor of
the leaves at positions i and l + i.
We now describe how entities and relations in a
sentence are easily represented in a card-pyramid.
We assume that in addition to the given entity
types, there is an extra type, Other, indicating that
the entity is of none of the given types. Similarly,
there is an extra relation type, NR, indicating that
its two entity arguments are not related.
Figure 3 shows the card-pyramid corresponding
to the annotated sentence shown in figure 1. To ob-
tain it, first, all entities present in the sentence are
made leaves of the card-pyramid in the same order
as they appear in the sentence. The label of a leaf
is the type of the corresponding entity. The leaf
also stores the range of the indices of its entity?s
words in the sentence. Note that although there is
no overlap between entities in the given example
(nor in the dataset we used for our experiments),
204
overlapping entities do not pose a problem. Over-
lapping entities can still be ordered and supplied as
the leaves of the card-pyramid. Next, the relation
between every two entities (leaves) is encoded as
the label of their lowest common ancestor. If two
entities are not related, then the label of their low-
est common ancestor is NR. This way, every non-
leaf node relates exactly two entities: the left-most
and right-most leaves beneath it.
2.2 Card-Pyramid Parsing
The task of jointly extracting entities and rela-
tions from a sentence reduces to jointly label-
ing the nodes of a card-pyramid which has all
the candidate entities (i.e. entity boundaries) of
the sentence as its leaves. We call this process
card-pyramid parsing. We assume that all candi-
date entities are given up-front. If needed, candi-
date entities can be either obtained automatically
(Punyakanok and Roth, 2001) or generated using
a simple heuristic, like including all noun-phrase
chunks as candidate entities. Or, in the worst case,
each substring of words in the sentence can be
given as a candidate entity. Liberally including
candidate entities is possible since they can sim-
ply be given the label Other if they are none of the
given types.
In this section we describe our card-pyramid
parsing algorithm whose pseudo-code is shown
in Figure 4. While the process is analogous to
context-free grammar (CFG) parsing, particularly
CYK bottom-up parsing, there are several major
differences. Firstly, in card-pyramid parsing the
structure is already known and the only task is
labeling the nodes, whereas in CFG parsing the
structure is not known in advance. This fact sim-
plifies some aspects of card-pyramid parsing. Sec-
ondly, in CFG parsing the subtrees under a node
do not overlap which simplifies parsing. How-
ever, in card-pyramid parsing there is significant
overlap between the two sub-card-pyramids under
a given node and this overlap needs to be consis-
tently labeled. This could have potentially com-
plicated parsing, but there turns out to be a simple
constant-time method for checking consistency of
the overlap. Thirdly, while CFG parsing parses the
words in a sentence, here we are parsing candi-
date entities. Finally, as described below, in card-
pyramid parsing, a production at a non-leaf node
relates the left-most and right-most leaves beneath
it, while in CFG parsing a production at a non-leaf
node relates its immediate children which could be
other non-leaf nodes.
Parsing requires specifying a grammar for the
card-pyramid. The productions in this grammar
are of two types. For leaf nodes, the produc-
tions are of the form entityLabel ? ce where ce,
which stands for candidate entity, is the only ter-
minal symbol in the grammar. We call these pro-
ductions entity productions. For non-leaf nodes,
the productions are of the form relationLabel ?
entityLabel1 entityLabel2. We call these produc-
tions relation productions. Note that their right-
hand-side (RHS) non-terminals are entity labels
and not other relation labels. From a training
set of labeled sentences, the corresponding card-
pyramids can be constructed using the procedure
described in the previous section. From these
card-pyramids, the entity productions are obtained
by simply reading off the labels of the leaves. A
relation productions is obtained from each non-
leaf node by making the node?s label the produc-
tion?s left-hand-side (LHS) non-terminal and mak-
ing the labels of its left-most and right-most leaves
the production?s RHS non-terminals. For the ex-
ample shown in Figure 3, some of the productions
are Work For? Person Organization, NR? Per-
son Other, OrgBased In? Loc Org, Person? ce,
Location ? ce etc. Note that there could be two
separate productions like Work For ? Person Or-
ganization and Work For ? Organization Person
based on the order in which the entities are found
in a sentence. For the relations which take argu-
ments of the same type, like Kill(Person,Person),
two productions are used with different LHS non-
terminals (Kill and Kill reverse) to distinguish be-
tween the argument order of the entities.
The parsing algorithm needs a classifier for ev-
ery entity production which gives the probabil-
ity of a candidate entity being of the type given
in the production?s LHS. In the pseudo-code,
this classifier is given by the function: entity-
classifier(production, sentence, range). The func-
tion range(r) represents the boundaries or the
range of the word indices for the rth candidate
entity. Similarly, we assume that a classifier is
given for every relation production which gives
the probability that its two RHS entities are re-
lated by its LHS relation. In the pseudo-code it is
the function: relation-classifier(production, sen-
tence, range1, range2, sub-card-pyramid1, sub-
card-pyramid2), where range1 and range2 are the
205
ranges of the word indices of the two entities
and sub-card-pyramid1 and sub-card-pyramid2
are the sub-card-pyramids rooted at its two chil-
dren. Thus, along with the two entities and the
words in the sentence, information from these sub-
card-pyramids is also used in deciding the relation
at a node. In the next section, we further spec-
ify these entity and relation classifiers and explain
how they are trained. We note that this use of
multiple classifiers to determine the most probable
parse is similar to the method used in the KRISP
semantic parser (Kate and Mooney, 2006).
Given the candidate entities in a sentence, the
grammar, and the entity and relation classifiers,
the card-pyramid parsing algorithm tries to find
the most probable joint-labeling of all of its nodes,
and thus jointly extracts entities and their rela-
tions. The parsing algorithm does a beam search
and maintains a beam at each node of the card-
pyramid. A node is represented by l[i][j] in the
pseudo-code which stands for the node in the jth
position in the ith level. Note that at level i, the
nodes range from l[i][0] to l[i][n? i? 1], where n
is the number of leaves. The beam at each node is
a queue of items we call beam elements. At leaf
nodes, a beam element simply stores a possible
entity label with its corresponding probability. At
non-leaf nodes, a beam element contains a possi-
ble joint assignment of labels to all the nodes in
the sub-card-pyramid rooted at that node with its
probability. This is efficiently maintained through
indices to the beam elements of its children nodes.
The parsing proceeds as follows. First, the en-
tity classifiers are used to fill the beams at the leaf
nodes. The add(beam, beam-element) function
adds the beam element to the beam while main-
taining its maximum beam-width size and sorted
order based on the probabilities. Next, the beams
of the non-leaf nodes are filled in a bottom-up
manner. At any node, the beams of its children
nodes are considered and every combination of
their beam elements are tried. To be considered
further, the two possible sub-card-pyramids en-
coded by the two beam elements must have a con-
sistent overlap. This is easily enforced by check-
ing that its left child?s right child?s beam element
is same as its right child?s left child?s beam ele-
ment. If this condition is satisfied, then those re-
lation productions are considered which have the
left-most leaf of the left child and right-most leaf
of the right child as its RHS non-terminals.1 For
every such production in the grammar, 2 the prob-
ability of the relation is determined using the re-
lation classifier. This probability is then multi-
plied by the probabilities of the children sub-card-
pyramids. But, because of the overlap between the
two children, a probability mass gets multiplied
twice. Hence the probability of the overlap sub-
card-pyramid is then suitably divided. Finally, the
estimated most-probable labeling is obtained from
the top beam element of the root node.
We note that this algorithm may not find the
optimal solution but only an approximate solu-
tion owing to a limited beam size, this is unlike
probabilistic CFG parsing algorithms in which the
optimal solution is found. A limitless beam size
will find the optimal solution but will reduce the
algorithm to a computationally intractable brute
force search. The parsing algorithm with a fi-
nite beam size keeps the search computationally
tractable while allowing a joint labelling.
3 Classifiers for Entity and Relation
Extraction
The card-pyramid parsing described in the previ-
ous section requires classifiers for each of the en-
tity and relation productions. In this section, we
describe the classifiers we used in our experiments
and how they were trained.
We use a support vector machine (SVM) (Cris-
tianini and Shawe-Taylor, 2000) classifier for each
of the entity productions in the grammar. An entity
classifier gets as input a sentence and a candidate
entity indicated by the range of the indices of its
words. It outputs the probability that the candi-
date entity is of the respective entity type. Prob-
abilities for the SVM outputs are computed using
the method by Platt (1999). We use all possible
word subsequences of the candidate entity words
as implicit features using a word-subsequence ker-
nel (Lodhi et al, 2002). In addition, we use
the following standard entity extraction features:
the part-of-speech (POS) tag sequence of the can-
didate entity words, two words before and after
the candidate entity and their POS tags, whether
any or all candidate entity words are capitalized,
1These are stored in the beam elements.
2Note that this step enforces the consistency constraint of
Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that
a relation can only be between the entities of specific types.
The grammar in our approach inherently enforces this con-
straint.
206
function Card-Pyramid-Parsing(Sentence,Grammar,entity-classifiers,relation-classifiers)
n = number of candidate entities in S
// Let range(r) represent the range of the indices of the words for the rth candidate entity.
// Let l[i][j] represent the jth node at ith level in the card-pyramid.
// For leaves
// A beam element at a leaf node is (label,probability).
for j = 0 to n // for every leaf
for each entityLabel ? candidate entity ? Grammar
prob = entity-classifier(entityLabel ? candidate entity, S, range(j))
add(l[0][j].beam, (entityLabel,prob))
// For non-leaf nodes
// A beam element at a non-leaf node is (label,probability,leftIndex,rightIndex,leftMostLeaf,rightMostLeaf)
// where leftIndex and rightIndex are the indices in the beams of the left and right children respectively.
for i = 1 to n // for every level above the leaves
for j = 0 to n ? i ? 1 // for every position at a level
// for each combination of beam elements of the two children
for each f ? l[i ? 1][j].beam and g ? l[i ? 1][j + 1].beam
// the overlapped part must be same (overlap happens for i > 1)
if (i == 1||f.rightIndex == g.leftIndex)
for each relationLabel ? f.leftMostLeaf g.rightMostLeaf ? Grammar
// probability of that relation between the left-most and right-most leaf under the node
prob = relation-classifier(relationLabel ? f.leftMostLeaf g.rightMostLeaf , S, range(i), range(i + j), f , g);
prob *= f.probability ? g.probability // multiply probabilities of the children sub-card-pyramids
// divide by the common probability that got multiplied twice
if (i > 1) prob /= l[i ? 2][j + 1].beam[f.rightIndex].probability
add(l[i][j].beam, (relationLabel, prob, index of f , index of g, f.leftMostLeaf , g.rightMostLeaf )
return the labels starting from the first beam element of the root i.e. l[n][0].beam[0]
Figure 4: Card-Pyramid Parsing Algorithm.
whether any or all words are found in a list of en-
tity names, whether any word has sufffix ?ment?
or ?ing?, and finally the alphanumeric pattern of
characters (Collins, 2002) of the last candidate
entity word obtained by replacing each charac-
ter by its character type (lowercase, uppercase or
numeric) and collapsing any consecutive repeti-
tion (for example, the alphanumeric pattern for
CoNLL2010 will be AaA0). The full kernel is
computed by adding the word-subsequence kernel
and the dot-product of all these features, exploit-
ing the convolution property of kernels.
We also use an SVM classifier for each of the
relation productions in the grammar which out-
puts the probability that the relation holds between
the two entities. A relation classifier is applied
at an internal node of a card-pyramid. It takes
the input in two parts. The first part is the sen-
tence and the range of the word indices of its two
entities l and r which are the left-most and the
right-most leaves under it respectively. The sec-
ond part consists of the sub-card-pyramids rooted
at the node?s two children which represent a pos-
sible entity and relation labeling for all the nodes
underneath. In general, any information from the
sub-card-pyramids could be used in the classifier.
We use the following information: pairs of rela-
tions that exist between l and b and between b and
r for every entity (leaf) b that exists between the
two entities l and r. For example, in figure 3,
the relation classifier at the root node which re-
lates Person(0-0) and Organization (15-16) will
take three pairs of relations as the information
from the two sub-card-pyramids of its children:
?Live In?OrgBased In? (with Location(3-4) as
the in-between entity), ?Live In?OrgBased In?
(with Location(6-6) as the in-between entity) and
?NR?NR? (with Other(12-12) as the in-between
entity). This information tells how the two enti-
ties are related to the entities present in between
them. This can affect the relation between the two
entities, for example, if the sentence mentions that
a person lives at a location and also mentions that
an organization is based at that location then that
person is likely to work at that organization. Note
that this information can not be incorporated in a
pipelined approach in which each relation is de-
termined independently. It is also not possible to
incorporate this in the linear programming method
presented in (Roth and Yih, 2004; Roth and Yih,
2007) because that method computes the probabil-
ities of all the relations independently before find-
ing the optimal solution through linear program-
ming. It would also not help to add hard con-
straints to their linear program relating the rela-
tions because they need not always hold.
We add the kernels for each part of the input to
compute the final kernel for the SVM classifiers.
The kernel for the second part of the input is com-
puted by simply counting the number of common
207
pairs of relations between two examples thus im-
plicitly considering every pair of relation (as de-
scribed in the last paragraph) as a feature. For the
first part of the input, we use word-subsequence
kernels which have shown to be effective for re-
lation extraction (Bunescu and Mooney, 2005b).
We compute the kernel as the sum of the word-
subsequence kernels between: the words between
the two entities (between pattern), k (a parame-
ter) words before the first entity (before pattern),
k words after the second entity (after pattern) and
the words from the beginning of the first entity to
the end of the second entity (between-and-entity
pattern). The before, between and after patterns
have been found useful in previous work (Bunescu
and Mooney, 2005b; Giuliano et al, 2007). Some-
times the words of the entities can indicate the re-
lations they are in, hence we also use the between-
and-entity pattern. When a relation classifier is
used at a node, the labels of the leaves beneath it
are already known, so we replace candidate entity
words that are in the between and between-and-
entity3 patterns by their entity labels. This pro-
vides useful information to the relation classifier
and also makes these patterns less sparse for train-
ing.
Given training data of sentences annotated with
entities and relations, the positive and negative ex-
amples for training the entity and relation clas-
sifiers are collected in the following way. First,
the corresponding card-pyramids are obtained for
each of the training sentences as described in sec-
tion 2.1. For every entity production in a card-
pyramid, a positive example is collected for its
corresponding classifier as the sentence and the
range of the entity?s word indices. Similarly, for
every relation production in a card-pyramid, a pos-
itive example is collected for its corresponding
classifier as the sentence, the ranges of the two
entities? word indices and the sub-card-pyramids
rooted at its two children. The positive examples
of a production become the negative examples for
all those productions which have the same right-
hand-sides but different left-hand-sides. We found
that for NR productions, training separate classi-
fiers is harmful because it has the unwanted side-
effect of preferring one label assignment of enti-
ties over another due to the fact that these pro-
ductions gave different probabilities for the ?not-
related? relation between the entities. To avoid
3Except for the two entities at the ends
this, we found that it suffices if all these classi-
fiers for NR productions always return 0.5 as the
probability. This ensures that a real relation will
be preferred over NR if and only if its probability
is greater than 0.5, otherwise nothing will change.
4 Experiments
We conducted experiments to compare our card-
pyramid parsing approach for joint entity and re-
lation extraction to a pipelined approach.
4.1 Methodology
We used the dataset4 created by Roth & Yih (2004;
2007) that was also used by Giuliano et el. (2007).
The sentences in this data were taken from the
TREC corpus and annotated with entities and re-
lations. As in the previous work with this dataset,
in order to observe the interaction between enti-
ties and relations, our experiments used only the
1437 sentences that include at least one relation.
The boundaries of the entities are already supplied
by this dataset. There are three types of entities:
Person (1685), Location (1968) and Organization
(978), in addition there is a fourth type Other
(705), which indicates that the candidate entity is
none of the three types. There are five types of re-
lations: Located In (406) indicates that one Loca-
tion is located inside another Location, Work For
(394) indicates that a Person works for an Orga-
nization, OrgBased In (451) indicates that an Or-
ganization is based in a Location, Live In (521)
indicates that a Person lives at a Location and Kill
(268) indicates that a Person killed another Per-
son. There are 17007 pairs of entities that are not
related by any of the five relations and hence have
the NR relation between them which thus signifi-
cantly outnumbers other relations.
Our implementation uses the LIBSVM5 soft-
ware for SVM classifiers. We kept the noise
penalty parameter of SVM very high (100) as-
suming there is little noise in our data. For the
word-subsequence kernel, we set 5 as the max-
imum length of a subsequence and 0.25 as the
penalty parameter for subsequence gaps (Lodhi et
al., 2002). We used k = 5 words for before and
after patterns for the relation classifiers. These pa-
rameter values were determined through pilot ex-
periments on a subset of the data. We used a beam
4Available at: http://l2r.cs.uiuc.edu/
?cogcomp/Data/ER/conll04.corp
5http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
208
Entity Person Location Organization
Approach Rec Pre F Rec Pre F Rec Pre F
Pipeline 93.6 92.0 92.8 94.0 90.3 92.1 87.9 90.6 89.2
Card-pyramid 94.2 92.1 93.2 94.2 90.8 92.4? 88.7 90.5 89.5
RY07 Pipeline 89.1 88.7 88.6 88.1 89.8 88.9 71.4 89.3 78.7
RY07 Joint 89.5 89.1 89.0 88.7 89.7 89.1 72.0 89.5 79.2
Relation Located In Work For OrgBased In Live In Kill
Approach Rec Pre F Rec Pre F Rec Pre F Rec Pre F Rec Pre F
Pipeline 57.0 71.5 62.3 66.0 74.1 69.7 60.2 70.6 64.6 56.6 68.1 61.7 61.2 91.1 73.1
Card-pyramid 56.7 67.5 58.3 68.3 73.5 70.7 64.1 66.2 64.7 60.1 66.4 62.9? 64.1 91.6 75.2
RY07 Pipeline 56.4 52.5 50.7 44.4 60.8 51.2 42.1 77.8 54.3 50.0 58.9 53.5 81.5 73.0 76.5
RY07 Joint 55.7 53.9 51.3 42.3 72.0 53.1 41.6 79.8 54.3 49.0 59.1 53.0 81.5 77.5 79.0?
Table 1: Results of five-fold cross-validation for entity and relation extraction using pipelined and joint extraction. Boldface
indicates statistical significance (p < 0.1 using paired t-test) when compared to the corresponding value in the other row
grouped with it. Symbol ? indicates statistical significance with p < 0.05. Only statistical significance for F-measures are
indicated. RY07 stands for the ?E ? R? model in (Roth and Yih, 2007).
size of 5 in our card-pyramid parsing algorithm at
which the performance plateaus.
We note that by using a beam size of 1 and by
not using the second part of input for relation clas-
sifiers as described in section 3 (i.e. by ignoring
the relations at the lower levels), the card-parsing
algorithm reduces to the traditional pipelined ap-
proach because then only the best entity label for
each candidate entity is considered for relation ex-
traction. Hence, in our experiments we simply use
this setting as our pipelined approach.
We performed a 5-fold cross-validation to com-
pare with the previous work with this dataset by
Roth & Yih (2007), however, our folds are not
same as their folds which were not available. We
also note that our entity and relation classifiers are
different from theirs. They experimented with sev-
eral models to see the effect of joint inference on
them, we compare with the results they obtained
with their most sophisticated model which they
denote by ?E ? R?. For every entity type and
relation type, we measured Precision (percentage
of output labels correct), Recall (percentage of
gold-standard labels correctly identified) and F-
measure (the harmonic mean of Precision and Re-
call).
4.2 Results and Discussion
Table 1 shows the results of entity and relation ex-
traction. The statistical significance is shown only
for F-measures. We first note that except for the
Kill relation, all the results of our pipelined ap-
proach are far better than the pipelined approach
of (Roth and Yih, 2007), for both entities and rela-
tions. This shows that the entity and relation clas-
sifiers we used are better that the ones they used.
These strong baselines also set a higher ceiling for
our joint extraction method to improve upon.
The entity extraction results show that on all
the entities the card-pyramid parsing approach for
joint extraction obtains a better performance than
the pipelined approach. This shows that entity
extraction benefits when it is jointly done with
relation extraction. Joint extraction using card-
pyramid parsing also gave improvement in perfor-
mance on all the relations except the Located In
relation.6
The results thus show that entity and relation ex-
traction correct some of each other?s errors when
jointly performed. Roth & Yih (2004; 2007) re-
port that 5% to 25% of the relation predictions
of their pipeline models were incoherent, meaning
that the types of the entities related by the relations
are not of the required types. Their joint inference
method corrects these mistakes, hence a part of the
improvement their joint model obtains over their
pipeline model is due to the fact that their pipeline
model can output incoherent relations. Since the
types of the entities a relation?s arguments should
6Through error analysis we found that the drop in the
performance for this relation was mainly because of an un-
usual sentence in the data which had twenty Location entities
in it separated by commas. After incorrectly extracting Lo-
cated In relation between the Location entities at the lower
levels, these erroneous extractions would be taken into ac-
count at higher levels in the card-pyramid, leading to extract-
ing many more incorrect instances of this relation while do-
ing joint extraction. Since this is the only such sentence in the
data, when it is present in the test set during cross-validation,
the joint method never gets a chance to learn not to make
these mistakes. The drop occurs in only that one fold and
hence the overall drop is not found as statistically significant
despite being relatively large.
209
take are known, we believe that filtering out the
incoherent relation predictions of their pipeline
model can improve its precision without hurting
the recall. On the other hand our pipelined ap-
proach never outputs incoherent relations because
the grammar of relation productions enforce that
the relations are always between entities of the re-
quired types. Thus the improvement obtained by
our joint extraction method over our pipelined ap-
proach is always non-trivial.
5 Related Work
To our knowledge, Roth & Yih (2004; 2007) have
done the only other work on joint entity and re-
lation extraction. Their method employs inde-
pendent entity and relation classifiers whose out-
puts are used to compute a most probable consis-
tent global set of entities and relations using lin-
ear programming. One key advantage of our card-
pyramid method over their method is that the clas-
sifiers can take the output of other classifiers under
its node as input features during parsing. This is
not possible in their approach because all classi-
fier outputs are determined before they are passed
to the linear program solver. Thus our approach
is more integrated and allows greater interaction
between dependent extraction decisions.
Miller et al (2000) adapt a probabilistic
context-free parser for information extraction by
augmenting syntactic labels with entity and rela-
tion labels. They thus do a joint syntactic parsing
and information extraction using a fixed template.
However, as designed, such a CFG approach can-
not handle the cases when an entity is involved
in multiple relations and when the relations criss-
cross each other in the sentence, as in Figure 1.
These cases occur frequently in the dataset we
used in our experiments and many other relation-
extraction tasks.
Giuliano et al (2007) thoroughly evaluate the
effect of entity extraction on relation extraction us-
ing the dataset used in our experiments. However,
they employ a pipeline architecture and did not in-
vestigate joint relation and entity extraction. Carl-
son et al (2009) present a method to simultane-
ously do semi-supervised training of entity and re-
lation classifiers. However, their coupling method
is meant to take advantage of the available unsu-
pervised data and does not do joint inference.
Riedel et al (2009) present an approach for ex-
tracting bio-molecular events and their arguments
using Markov Logic. Such an approach could
also be adapted for jointly extracting entities and
their relations, however, this would restrict entity
and relation extraction to the same machine learn-
ing method that is used with Markov Logic. For
example, one would not be able to use kernel-
based SVM for relation extraction, which has been
very successful at this task, because Markov Logic
does not support kernel-based machine learning.
In contrast, our joint approach is independent of
the individual machine learning methods for en-
tity and relation extraction, and hence allows use
of the best machine learning methods available for
each of them.
6 Future Work
There are several possible directions for extend-
ing the current approach. The card-pyramid struc-
ture could be used to perform other language-
processing tasks jointly with entity and rela-
tion extraction. For example, co-reference res-
olution between two entities within a sentence
can be easily incorporated in card-pyramid pars-
ing by introducing a production like coref ?
Person Person, indicating that the two person
entities are the same.
In this work, and in most previous work, re-
lations are always considered between two enti-
ties. However, there could be relations between
more than two entities. In that case, it should
be possible to binarize those relations and then
use card-pyramid parsing. If the relations are be-
tween relations instead of between entities, then
card-pyramid parsing can handle it by considering
the labels of the immediate children as RHS non-
terminals instead of the labels of the left-most and
the right-most leaves beneath it. Thus, it would
be interesting to apply card-pyramid parsing to ex-
tract higher-order relations (such as causal or tem-
poral relations).
Given the regular graph structure of the card-
pyramid, it would be interesting to investigate
whether it can be modeled using a probabilistic
graphical model (Koller and Friedman, 2009). In
that case, instead of using multiple probabilis-
tic classifiers, one could employ a single jointly-
trained probabilistic model, which is theoretically
more appealing and might give better results.
Finally, we note that a better relation classifier
could be used in the current approach which makes
more use of linguistic information. For example,
210
by using dependency-based kernels (Bunescu and
Mooney, 2005a; Kate, 2008) or syntactic kernels
(Qian et al, 2008; Moschitti, 2009) or by includ-
ing the word categories and their POS tags in the
subsequences. Also, it will be interesting to see if
a kernel that computes the similarity between sub-
card-pyramids could be developed and used for re-
lation classification.
7 Conclusions
We introduced a card-pyramid graph structure and
presented a new method for jointly extracting enti-
ties and their relations from a sentence using it. A
card-pyramid compactly encodes the entities and
relations in a sentence thus reducing the joint ex-
traction task to jointly labeling its nodes. We pre-
sented an efficient parsing algorithm for jointly
labeling a card-pyramid using dynamic program-
ming and beam search. The experiments demon-
strated the benefit of our joint extraction method
over a pipelined approach.
Acknowledgments
This research was funded by Air Force Contract
FA8750-09-C-0172 under the DARPA Machine
Reading Program.
References
Razvan C. Bunescu and Raymond J. Mooney. 2005a. A
shortest path dependency kernel for relation extraction. In
Proc. of the Human Language Technology Conf. and Conf.
on Empirical Methods in Natural Language Processing
(HLT/EMNLP-05), pages 724?731, Vancouver, BC, Oc-
tober.
Razvan C. Bunescu and Raymond J. Mooney. 2005b. Sub-
sequence kernels for relation extraction. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in Neural In-
formation Processing Systems 18, Vancouver, BC.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Mar-
cotte, Raymond J. Mooney, Arun Kumar Ramani, and
Yuk Wah Wong. 2005. Comparative experiments on
learning information extractors for proteins and their inter-
actions. Artificial Intelligence in Medicine (special issue
on Summarization and Information Extraction from Med-
ical Documents), 33(2):139?155.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka,
and Tom M. Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In SemiSupLearn
?09: Proceedings of the NAACL HLT 2009 Workshop on
Semi-Supervised Learning for Natural Language Process-
ing, pages 1?9, Boulder, Colorado.
Michael Collins. 2002. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In Proc. of
the 40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2002), pages 489?496, Philadel-
phia, PA.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2007. Relation extraction and the influence of automatic
named-entity recognition. ACM Trans. Speech Lang. Pro-
cess., 5(1):1?26.
D. Jurafsky and J. H. Martin. 2008. Speech and Language
Processing: An Introduction to Natural Lan guage Pro-
cessing, Computational Linguistics, and Speech Recogni-
tion. Prentice Hall, Upper Saddle River, NJ.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proc. of the 21st
Intl. Conf. on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics
(COLING/ACL-06), pages 913?920, Sydney, Australia,
July.
Rohit J. Kate. 2008. A dependency-based word subsequence
kernel. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP 2008),
pages 400?409, Waikiki,Honolulu,Hawaii, October.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. The MIT
Press, Cambridge, MA.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine Learning Re-
search, 2:419?444.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.
Weischedel. 2000. A novel use of statistical parsing to
extract information from text. In Proc. of the Meeting of
the N. American Association for Computational Linguis-
tics, pages 226?233, Seattle, Washington.
Alessandro Moschitti. 2009. Syntactic and semantic ker-
nels for short text pair categorization. In Proceedings of
the 12th Conference of the European Chapter of the ACL
(EACL 2009), pages 576?584, Athens,Greece, March.
John C. Platt. 1999. Probabilistic outputs for support vec-
tor machines and comparisons to regularized likelihood
methods. In Alexander J. Smola, Peter Bartlett, Bern-
hard Scho?lkopf, and Dale Schuurmans, editors, Advances
in Large Margin Classifiers, pages 185?208. MIT Press.
Vasin Punyakanok and Dan Roth. 2001. The use of classi-
fiers in sequential inference. In Advances in Neural Infor-
mation Processing Systems 13.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu,
and Peide Qian. 2008. Exploiting constituent depen-
dencies for tree kernel-based semantic relation extraction.
In Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 697?704,
Manchester, UK, August.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and
Jun?ichi Tsujii. 2009. A Markov logic approach to
bio-molecular event extraction. In Proceedings of the
BioNLP 2009 Workshop Companion Volume for Shared
Task, pages 41?49, Boulder, Colorado, June. Association
for Computational Linguistics.
211
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. of 8th Conf. on Computational Natural Language
Learning (CoNLL-2004), pages 1?8, Boston, MA.
D. Roth and W. Yih. 2007. Global inference for entity and
relation identification via a linear programming formula-
tion. In L. Getoor and B. Taskar, editors, Introduction to
Statistical Relational Learning, pages 553?580. The MIT
Press, Cambridge, MA.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
7th Conf. on Computational Natural Language Learning
(CoNLL-2003), Edmonton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL-2002, pages 155?
158. Taipei, Taiwan.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006.
A composite kernel to extract relations between entities
with both flat and structured features. In Proc. of the 21st
Intl. Conf. on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics
(COLING/ACL-06), Sydney, Australia, July.
212
Implementing Weighted Abduction in Markov Logic
James Blythe
USC ISI
blythe@isi.edu
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Pedro Domingos
University of Washington
pedrod@cs.washington.edu
Rohit J. Kate
University of Wisconsin-Milwaukee
katerj@uwm.edu
Raymond J. Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
Abduction is a method for finding the best explanation for observations. Arguably
the most advanced approach to abduction, especially for natural language processing, is
weighted abduction, which uses logical formulas with costs to guide inference. But it
has no clear probabilistic semantics. In this paper we propose an approach that imple-
ments weighted abduction in Markov logic, which uses weighted first-order formulas to
represent probabilistic knowledge, pointing toward a sound probabilistic semantics for
weighted abduction. Application to a series of challenge problems shows the power and
coverage of our approach.
1 Introduction
Abduction is inference to the best explanation.1 Typically, one uses it to find the best hypothesis ex-
plaining a set of observations, e.g., in diagnosis and plan recognition. In natural language processing the
content of an utterance can be viewed as a set of observations, and the best explanation then constitutes
the interpretation of the utterance. Hobbs et al [7] described a variety of abduction called ?weighted
abduction? for interpreting natural language discourse. The key idea was that the best interpretation of
a text is the best explanation or proof of the logical form of the text, allowing for assumptions. What
counted as ?best? was defined in terms of a cost function which favored proofs with the fewest number of
assumptions and the most salient and plausible axioms, and in which the pervasive redundancy implicit
in natural language discourse was exploited. It was argued in that paper that such interpretation problems
as coreference and syntactic ambiguity resolution, determining the specific meanings of vague predicates
and lexical ambiguity resolution, metonymy resolution, metaphor interpretation, and the recognition of
discourse structure could be seen to ?fall out? of the best abductive proof.
Specifically, weighted abduction has the following features:
1. In a goal expression consisting of an existentially quantified conjunction of positive literals, each
literal is given a cost that represents the utility of proving that literal as opposed to assuming it.
That is, a low cost on a literal will make it more likely for it to be assumed, whereas a high cost
will result in a greater effort to find a proof.
1We are indebted to Jesse Davis, Parag Singla and Marc Sumner for discussions about this work. This research was
supported in part by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force
Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172, in part by the Office of Naval Research under contract
no. N00014-09-1-1029, and in part by the Army Research Office under grant W911NF-08-1-0242. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of
the DARPA, AFRL, ONR, ARO, or the US government.
55
2. Costs are passed back across the implication in Horn clauses according to weights on the conjuncts
in the antecedents. Specifically, if a consequent costs $c and the weight on a conjunct in the
antecedent is v, then the cost on that conjunct will be $vc. Note that if the weights add up to less
than one, backchaining on the rule will be favored, as the cost of the antecedent will be less than
the cost of the consequent. If the weights add up to more than one, backchaining will be disfavored
unless a proof can be found for one or more of the conjuncts in the antecedent, thereby providing
partial evidence for the consequent.
3. Two literals can be factored or unified, where the result is given the minimum cost of the two,
providing no contradiction would result. This is a frequent mechanism for coreference resolution.
In practice, only a shallow or heuristic check for contradiction is done.
4. The lowest-cost proof is the best interpretation, or the best abductive proof of the goal expression.
However, there are two significant problems with weighted abduction as it was originally presented.
First, it required a large knowledge base of commonsense knowledge. This was not available when
weighted abduction was first described, but since that time there have been substantial efforts to build up
knowledge bases for various purposes, and at least two of these have been used with promising results
in an abductive setting?Extended WordNet [6] for question-answering and FrameNet [11] for textual
inference.
The second problem with weighted abduction was that the weights and costs did not have a prob-
abilistic semantics. This, for example, hampers automatic learning of weights from data or existing
resources. That is the issue we address in the present paper.
In the last decade and a half, a number of formalisms for adding uncertain reasoning to predicate logic
have been developed that are well-founded in probability theory. Among the most widely investigated
is Markov logic [14, 4]. In this paper we show how weighted abduction can be implemented in Markov
logic. This demonstrates that Markov logic networks can be used as a powerful mechanism for interpret-
ing natural language discourse, and at the same time provides weighted abduction with something like a
probabilistic semantics.
In Section 2 we briefly describe Markov logic and Markov logic networks. Section 3 then describes
how weighted abduction can be implemented in Markov logic. In Section 4 we describe experiments in
which fourteen published examples of the use of weighted abduction in natural language understanding
are implemented in Markov logic networks, with good results. Section 5 on current and future directions
briefly describes an ongoing experiment in which we are attempting to scale up to apply this procedure
to the textual inference problem with a knowledge base derived from FrameNet with tens of thousands
of axioms.
2 Markov Logic Networks and Related Work
Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order
logic and probabilistic graphical models. A traditional first-order knowledge base can be seen as a set of
hard constraints on the set of possible worlds: if a world violates even one formula, its probability is zero.
In order to soften these constraints, Markov logic attaches a weight to each first-order logic formula in
the knowledge base. Such a set of weighted first-order logic formulae is called a Markov logic network
(MLN). A formula?s weight reflects how strong a constraint it imposes on the set of possible worlds: the
higher the weight, the lower the probability of a world that violates it; however, that probability need not
be zero. An MLN with all infinite weights reduces to a traditional first-order knowledge base with only
hard constraints.
56
Formally, an MLN L is a set of formula?weight pairs (Fi, wi). Given a set of constants, it defines
a joint probability distribution over a set of boolean variables X = (X1, X2...) corresponding to the
possible groundings (using the given constants) of the literals present in the first-order formulae:
P (X = x) = 1Z exp(
?
iwini(x))
where ni(x) is the number of true groundings of Fi in x and Z is a normalization term obtained by
summing P (X = x) over all values of X .
Semantically, an MLN can be viewed as a set of templates for constructing Markov networks [12],
the undirected counterparts of Bayesian networks. An MLN and a set of constants produce a Markov
network in which each ground literal is a node and every pair of ground literals that appear together in
some grounding of some formula are connected by an edge. Different sets of constants produce different
Markov networks; however, there are certain regularities in their structure and parameters. For example,
all groundings of the same formula have the same weight.
Probabilistic inference for an MLN (such as finding the most probable truth assignment for a given
set of ground literals, or finding the probability that a particular formula holds) can be performed by
first producing the ground Markov network and then using well known inference techniques for Markov
networks, like Gibbs sampling. Given a knowledge base as a set of first-order logic formulae, and a
database of training examples each consisting of a set of true ground literals, it is also possible to learn
appropriate weights for the MLN formulae which maximize the probability of the training data. An open-
source software package for MLNs, called Alchemy 2, is also available with many built-in algorithms
for performing inference and learning.
Much of the early work on abduction was done in a purely logical framework (e.g., [13, 3, 9, 10].
Typically the choice between alternative explanations is made on the basis of parsimony; the shortest
proofs with the fewest assumptions are favored. However, a significant limitation of these purely logical
approaches is that they are unable to reason under uncertainty or estimate the likelihood of alternative
explanations. A probabilistic form of abduction is needed in order to account for uncertainty in the
background knowledge and to handle noisy and incomplete observations.
In Bayesian networks [12] background knowledge with its uncertainties is encoded in a directed
graph. Then, given a set of observations, probabilistic inference over the graph structure is done to
compute the posterior probability of alternative explanations. However, Bayesian networks are based on
propositional logic and cannot handle structured representations, hence preventing their use in situations,
characteristic of natural language processing, that involve an unbounded number of entities with a variety
of relations between them.
In recent years there have been a number of proposals attempting to combine the probabilistic nature
of Bayesian networks with structured first-order representations. It is impossible here to review this liter-
ature here. A a good review of much of it can be found in [5], and in [14] there are detailed comparisonss
of various models to MLNs.
Charniak and Shimony [2] define a variant of weighted abduction, called ?cost-based abduction? in
which weights are attached to terms rather than to rules or to antecedents in rules. Thus, the term Pi
has the same cost whatever rule it is used in. The cost of an assignment to the variables in the domain
is the sum of the costs of the variables that are true in the assignment. Charniak and Shimony provide
a probabilistic semantics for their approach by showing how to construct a Bayesian network from a
domain such that a most probable explanation solution to the Bayes net corresponds to a lowest-cost
solution to the abduction problem. However, in natural language applications the utility of proving a
proposition can vary by context; weighted abduction accomodates this, whereas cost-based abduction
does not.2http://alchemy.cs.washington.edu
57
3 Weighted Abduction and MLNs
Kate and Mooney [8] show how logical abduction can be implemented in Markov logic networks. They
use forward inference in MLNs to perform abduction by adding clauses with reverse implications. Uni-
versally quantified variables from the left hand side of rules are converted to existentially quantified
variables in the reversed clause. For example, suppose we have the following rule saying that mosquito
bites transmit malaria:
mosquito(x) ? infected(x,Malaria) ? bite(x, y) ? infected(y,Malaria)
This would be translated into the soft rule
[w] infected(y,Malaria) ? ?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
Where there is more than one possible explanation, they include a closure axiom saying that one of the
explanations must hold. Since blood transfusions also cause malaria, they have the hard rule
infected(y,Malaria) ?
?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
??x[infected(x,Malaria) ? transfuse(Blood, x, y)].
Kate and Mooney also add a soft mutual exclusivity clause that states that no more than one of the
possible explanations is true.
In translating between weighted abduction and Markov logic, we need similarly to specify the axioms
in Markov logic that correspond to a Horn clause axiom in weighted abduction. In addition, we need to
describe the relation between the numbers in weighted abduction and the weights on the Markov logic
axioms. Hobbs et al [7] give only broad, informal guidelines about how the numbers correspond to
probabilities. In this development, we elaborate on how the numbers can be defined more precisely
within these guidelines in a way that links with the weights in Markov logic, thereby pointing to a
probabilistic semantics for the weighted abduction numbers.
There are two sorts of numbers in weighted abduction?the weights on conjuncts in the antecedents
of Horn clause axioms, and the costs on conjuncts in goal expressions, which are existentially quantified
conjunctions of positive literals. We deal first with the weights, then with the costs.
The space of events over which probabilities are taken is the set of proof graphs constituting the best
interpretations of a set of texts in a corpus. Thus, by the probability of p(x) given q(x), we mean the
probability that p(x) will occur in a proof graph in which q(x) occurs.
The translation from weighted abduction axioms to Markov logic axioms can be broken into two
steps. First we consider the ?or? node case, determining the relative costs of axioms that have the same
consequent. Then we look at the ?and? node case, determining how the weights should be distributed
across the conjuncts in the antecedent of a Horn clause, given the total weight for the antecedent.
Weights on Antecedents in Axioms. First consider a set of Horn clause axioms all with the same
consequent, where we collapse the antecedent into a single literal, and for simplicity allow x to stand for
all the universally quantified variables in the antecedent, and assume the consequent to have only those
variables. That is, we convert all axioms of the form
p1(x) ? . . . ? q(x)
into axioms of the form
Ai(x) ? q(x), where p1(x) ? . . . ? Ai(x)
To convert this into Markov logic, we first introduce the hard constraint
Ai(x) ? q(x).
In addition, given a goal of proving q(x), in weighted abduction we will want to backchain on at least
(and usually at most) one of these axioms or we will want simply to assume q(x). Thus, we can introduce
another hard constraint with the disjunction of these antecedents as well as a literal AssumeQ(x) that
means q(x) is assumed rather than proved.
58
q(x) ? A1(x) ? A2(x) ? . . . ? An(x) ? AssumeQ(x).
Then we need to introduce soft constraints to indicate that each of these disjuncts is a possible explana-
tion, or proof, of q(x), with an associated probability, or weight.
[wi] q(x) ? Ai(x), . . .
[w0] q(x) ? AssumeQ(x)
The probability that AssumeQ(x) is true is the conditional probability P0 that none of the antecedents
is true given that q(x) is true.
P0 = P (?[A1(x) ? A2(x) ? . . . ? An(x)] | q(x))
In weighted abduction, when the antecedent weight is greater than one, we prefer assuming the conse-
quent to assuming the antecedent. When the antecedent weight is less than one we prefer to assume the
antecedent. If the probability that an antecedent Ai(x) is the explanation of q(x) is greater than P0, it
should be given a weighted abduction weight vi less than 1, making it more likely to be chosen.3 Cor-
respondingly, if it is less than P0, it should be given a weight vi greater than 1, making it less likely
to be chosen. In general, the weighted abduction weights should be in reverse order of the conditional
probabilities Pi that Ai(x) is the explanation of q(x).
Pi = P (Ai(x) | q(x))
If we assign the weights vi in weighted abduction to be
vi = logPilogP0
then this is consistent with informal guidelines in [7] on the meaning of these weights. We use the logs
of the probabilities rather than the probabilities themselves to moderate the effect of one axiom being
very much more probable than any of the others.
Kate and Mooney [8], in their translation of logical abduction into Markov logic, also include soft
constraints stipulating that the different possible explanations Ai(x) are normally mutually exclusive.
We do not do that here, but we get a kind of soft mutual exclusivity constraint by virtue of the axioms
below that levy a cost for any literal that is taken to be true. In general, more parimonious explanations
will be favored.
Nevertheless, in most cases a single explanation will suffice. When this is true, the probability of
Ai(x) holding when q(x) holds is e
wi
Z . Then a reasonable approximation for the relation between the
weighted abduction weights vi and the Markov logic weights wi is
wi = ?vilogP0
Weights on Conjuncts in Antecedents. Next consider how cost is spread across the conjuncts in the
antecedent of a Horn clause in weighted abduction. Here we use u?s to represent the weighted abduction
weights on the conjuncts.
p1(x)u1 ? p2(x)u2 ? ... ? A(x)
The u?s should somehow represent the semantic contribution of each conjunct to the conclusion. That is,
given that the conjunct is true, what is the probability that it is part of an explanation of the consequent?
Conjuncts with a higher such probability should be given higher weights u; they play a more significant
role in explaining A(x).
Let Pi be the conditional probability of the consequent given the ith conjunct in the antecedent.
Pi = P (A(x)|pi(x))
and let Z be a normalization factor.
Z = ?ni=1 Pi
3We use vi for these weighted abduction weights and wi for Markov logic weights.
59
Let v be the weight of the entire antecedent as determined above.
Then it is consistent with the guidelines in [7] to define the weights on the conjuncts as follows:
ui = vPiZ
The weights ui will sum to v and each will correspond to the semantic contribution of its conjunct to the
consequent.
In Markov logic, weights apply only to axioms as a whole, not parts of axioms. Thus, the single
axiom above must be decomposed into one axiom for each conjunct and the dependencies must be
written as
[wi] pi(x) ? A(x), . . .
The relation between the weighted abduction weights ui and the Markov logic weights wi can be
approximated by
ui = ve
?wi
Z
Costs on Goals. The other numbers in weighted abduction are the costs associated with the conjuncts
in the goal expression. In weighted abduction these costs function as utilities. Some parts of the goal
expression are more important to interpret correctly than others; we should try harder to prove these
parts, rather than simply assuming them. In language it is important to recognize the referential anchor
of an utterance in shared knowledge. Thus, those parts of a sentence most likely to provide this anchor
have the highest utility. If we simply assume them, we lose their connection with what is already known.
Those parts of a sentence most likely to be new information will have a lower cost, because we usually
would not be able to prove them in any case.
Consider the two sentences
The smart man is tall.
The tall man is smart.
The logical form for each of them will be
(?x)[smart(x) ? tall(x) ?man(x)]
In weighted abduction, an interpretation of the sentence is a proof of the logical form, allowing assump-
tions. In the first sentence we want to prove smart(x) to anchor the sentence referentially. Then tall(x)
is new information; it will have to be assumed. We will want to have a high cost on smart(x) to force
the proof procedure to find this referential anchor. The cost on tall(x) will be low, to allow it to be
assumed without expending too much effort in trying to locate that fact in shared knowledge.
In the second sentence, the case is the reverse.
Let?s focus on the first sentence and assume we know that educated people are smart and big people
are tall, and furthermore that John is educated and Bill is big.
educated(x)1.2 ? smart(x)
big(x)1.2 ? tall(x)
educated(J), big(B)
In weighted abduction, the best interpretation will be that the smart man is John, because he is educated,
and we pay the cost for assuming he is tall. The interpretation we want to avoid is one that says x is Bill;
he is tall because he is big, and we pay the cost of assuming he is smart. Weighted abduction with its
differential costs on conjuncts in the goal expression favors the first and disfavors the second.
In weighted abduction, only assumptions cost; literals that are proved cost nothing. When the above
axioms are translated into Markov logic, it would be natural to capture the differential costs by attaching a
negative weight to smart(x) to model the cost associated with assuming it. However, this weight would
apply to any assignment in which smart(J) is true, regardless of whether it was assumed, derived from
60
an assumed fact, or derived from a known fact. A potential solution might be to attach the negative weight
to AssumeSmart(x). But the first axiom above allows us to bypass the negative weight on smart(x).
We can hypothesize that x is Bill, pay a low cost on AssumeEducated(B), derive smart(B), and get
the wrong assignment. Thus it is not enough to attach a negative weight to high-cost conjuncts in the
goal expression. This negative weight would have to be passed back through the whole knowledge base,
making the complexity of setting the weights at problem time in the MLN knowledge base equal to the
complexity of running the inference problem.
An alternative solution, which avoids this problem when the forward inferences are exact, is to use
a set of predicates that express knowing a fact without any assumptions. In the current example, we
would add Ksmart(x) for knowing that an entity is smart. The facts asserted in the data base are now
Keducated(J) and Kbig(B). For each hard axiom involving non-K predicates, we have a correspond-
ing axiom that expresses the relation between the K-predicates, and we have a soft axiom allowing us to
cross the border between the K predicates and their non-K counterparts.
Keducated(x) ? Ksmart(x)., . . .
[w] Ksmart(x) ? smart(x), . . .
Here the positive weight w attached is chosen to counteract the negative weight we would attach to
smart(x) to reflect the high cost of assuming it.
This removes the weight associated with assuming smart(x) regardless of the inference path that
leads to knowing smart(x) (KSmart(x))). Further, this translation takes linear time in the size of
the goal expression to compute, since we do not need to know the equivalent weighted abduction cost
assigned to the possible antecedents of smart(x).
If the initial facts do not include KEducated(B) and instead educated(B) must be assumed, then
the negative weight associated with smart(B) is still present. In this solution, there is no danger that
the inference process can by-pass the cost of assuming smart(B), since it is attached to the required
predicate and can only be removed by inferring KSmart(B).
Finally, there is a tendency in Markov logic networks for assignments of high probability for proposi-
tions for which there is no evidence one way or the other. To suppress this, we associate a small negative
weight with every predicate. In practice, it has turned out that a weight of ?1 effectively suppresses this
behavior.
4 Experimental Results
We have tested our approach on a set of fourteen challenge problems from [7] and subsequent papers,
designed to exercise the principal features of weighted abduction and show its utility for solving natural
language interpretation problems. The knowledge bases used for each of these problems are sparse,
consisting of only the axioms required for solving the problems plus a few distractors.
An example of a relatively simple problem is #5 in the table below, resolving ?he? in the text
I saw my doctor last week. He told me to get more exercise.
where we are given a knowledge base that says a doctor is a person and a male person is a ?he?. Solving
the problem requires assuming the doctor is male.
(?x)[doctor(x)1.2 ? person(x)]
(?x)[male(x).6 ? person(x).6 ? he(x)]
The logical form fragment to prove is (?x)he(x), where we know doctor(D).
A problem of intermediate difficulty (#7) is resolving the three lexical ambiguities in the sentence
The plane taxied to the terminal.
61
where we are given a knowledge base saying that airplanes and wood smoothers are planes, planes
moving on the ground and people taking taxis are both described as ?taxiing?, and computer terminals
and airport terminals are both terminals.
An example of a difficult problem is #12, finding the coherence relation, thereby resolving the pro-
noun ?they?, between the sentences
The police prohibited the women from demonstrating. They feared violence.
The axioms specify relations between fearing, not wanting, and prohibiting, as well as the defeasible
transitivity of causality and the fact that a causal relation between sentences makes the discourse coher-
ent.
The weights in the axioms were mostly distributed evenly across the conjuncts in the antecedents and
summed to 1.2.
For each of these problems, we compare the performance of the method described here with a man-
ually constructed gold standard and also with a method based on Kate and Mooney?s (KM) approach.
In this method, weights were assigned to the reversed clauses based on the negative log of the sum of
weights in the original clause. This approach does not capture different weights for different antecedents
of the same rule, and so has less fidelity to weighted abduction than our approach. In each case, we used
Alchemy?s probabilistic inference to determine the most probable explanation (MPE) [12].
In some of the problems the system should make more than one assumption, so there are 22 assump-
tions in total over all 14 problems in the gold standard. Using our method, 18 of the assumptions were
found, while 15 were found using the KM method. Table 1 shows the number of correct assumptions
found and the running time for the two approaches for each problem. Our method in particular provides
good coverage, with a recall of over 80% of the assumptions made in the gold standard. It has a shorter
running time overall, approximately 5.3 seconds versus 8.7 seconds for the reversal method. This is
largely due to one problem in the test set, problem #9, where the running time for the KM method is
relatively high because the technique finds a less sparse network, leading to larger cliques. There were
two problems in the test set that neither approach could solve. One of these contains predicates that have
a large number of arguments, leading to large clique sizes.
5 Current and Future Directions
In other work [11] we are experimenting with using weighted abduction with a knowledge base with tens
of thousands of axioms derived from FrameNet for solving problems in recognizing textual entailment
(RTE2) from the Pascal dataset [1]. For a direct comparison between standard weighted abduction and
the Markov logic approach described here, we are also experimenting with using the latter on the same
task with the same knowledge base.
For each text-hypothesis pair, the sentences are parsed and a logical form is produced. The output for
the first sentence forms the specific knowledge the system has while the output for the second sentence
is used as the target to be explained. If the cost of the best explanation is below a threshold we take the
target sentence to be true given the initial information.
It is a major challenge to scale our approach to handle all the problems from the RTE2 development
and test sets. We are not yet able to address the most complex of these using inference in Markov logic
networks. However, we have devised a number of pre-processing steps to reduce the complexity of the
resultant network, which significantly increase the number of problems that are tractable.
The FrameNet knowledge base contains a large number of axioms with general coverage. For any
individual entailment problem, most of them are irrelevant and can be removed after a simple graphical
analysis. We are able to remove more irrelevant axioms and predicates with an iterative approach that in
62
Our Method KM Method Gold
Problem score seconds score seconds standard
1 3 300 3 16 3
2 1 250 1 265 1
3 1 234 1 266 1
4 2 234 2 203 2
5 1 218 1 218 1
6 1 218 0 265 1
7 3 300 3 218 3
8 1 200 1 250 1
9 2 421 0 5000 2
10 1 2500 1 1500 3
11 0 0 1
12 0 0 1
13 1 250 1 250 1
14 1 219 1 219 1
Total 18 5344 15 8670 22
Table 1: Performance on each problem in our test set, comparing two encodings of weighted abduction
into Markov logic networks and a gold standard.
each iteration both drops axioms that are shown to be irrelevant and simplifies remaining axioms in such
a way as not to change the probability of entailment.
We also simplify predications by removing unnecessary arguments. The most natural way to convert
FrameNet frames to axioms is to treat a frame as a predicate whose arguments are the frame elements for
all of its roles. After converting to Markov logic, this results in rules having large numbers of existentially
quantified variables in the consequent. This can lead to a combinatorial explosion in the number of
possible ground rules. Many of the variables in the frame predicate are for general use and can be pruned
in the particular entailment. Our approach essentially creates abstractions of the original predicates that
preserve all the information that is relevant to the current problem but greatly reduces the number of
ground instances to consider.
Before implementing these pre-processing steps, only two or three problems could be run to com-
pletion on a Macbook Pro with 8 gigabytes of RAM. After making them, 28 of the initial 100 problems
could be run to completion.
Work on this effort continues.
6 Summary
Weighted abduction is a logical reasoning framework that has been successfully applied to solve a num-
ber of interesting and important problems in computational natural-language semantics ranging from
word sense disambiguation to coreference resolution. However, its method for representing and combin-
ing assumption costs to determine the most preferred explanation is ad hoc and without a firm theoretical
foundation. Markov Logic is a recently developed formalism for combining first-order logic with prob-
abilistic graphical models that has a well-defined formal semantics in terms of specifying a probability
distribution over possible worlds. This paper has presented a method for mapping weighted abduction
63
to Markov logic, thereby providing a sound probabilistic semantics for the approach and also allowing
it to exploit the growing toolbox of inference and learning algorithms available for Markov logic. Com-
plementarily, it has also demonstrated how Markov logic can thereby be applied to help solve important
problems in computational semantics.
References
[1] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpek-
tor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment, Venice, Italy, 2006.
[2] Eugene Charniak and Solomon E. Shimony. Cost-based abduction and map explanation. Artificial Artificial
Intelligence Journal, 66(2):345?374, 1994.
[3] P. T. Cox and T. Pietrzykowski. Causes for events: Their computation and applications. In J. Siekmann,
editor, 8th International Conference on Automated Deduction (CADE-8), Berlin, 1986. Springer-Verlag.
[4] P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Morgan & Claypool,
San Rafael, CA, 2009.
[5] L. Getoor and B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, Cambridge,
MA, 2007.
[6] S. Harabagiu and D.I. Moldovan. Lcc?s question answering system. In 11th Text Retrieval Conference,
TREC-11, Gaithersburg, MD., 2002.
[7] Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and Paul A. Martin. Interpretation as abduction. Artifi-
cial Intelligence, 63(1-2):69?142, 1993.
[8] Rohit Kate and Ray Mooney. Probabilistic abduction using markov logic networks. In IJCAI 09 Workshop
on Plan, Activity and Intent Recognition, 2009.
[9] Hector J. Levesque. A knowledge-level account of abduction. In Eleventh International Joint Conference on
Artificial Intelligence, volume 2, pages 1061?1067, Detroit, Michigan, 1989.
[10] Hwee Tou Ng and Raymond J. Mooney. The role of coherence in constructing and evaluating abductive
explanations. In P. O?Rorke, editor, Working Notes, AAAI Spring Symposium on Automated Abduction,
Stanford, California, March 1990.
[11] E. Ovchinnikova, N. Montazeri, T. Alexandrov, J. Hobbs, M. McCord, and R. Mulkar-Mehta. Abductive
reasoning with a large knowledge base for discourse processing. In Proceedings of the 9th International
Conference on Computational Semantics, Oxford, United Kingdom, 2011.
[12] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann,
San Francisco, CA, 1988.
[13] Harry E. Pople. On the mechanization of abductive logic. In Third International Joint Conference on Artificial
Intelligence, pages 147?152, Stanford, California, August 1973.
[14] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62:107?136, 2006.
64
Integrating Logical Representations with
Probabilistic Information using Markov Logic
Dan Garrette
University of Texas at Austin
dhg@cs.utexas.edu
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Raymond Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
First-order logic provides a powerful and flexible mechanism for representing natural language
semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic
knowledge, for example regarding word meaning. This paper describes the first steps of an approach
to recasting first-order semantics into the probabilistic models that are part of Statistical Relational
AI. Specifically, we show how Discourse Representation Structures can be combined with distribu-
tional models for word meaning inside a Markov Logic Network and used to successfully perform
inferences that take advantage of logical concepts such as factivity as well as probabilistic informa-
tion on word meaning in context.
1 Introduction
Logic-based representations of natural language meaning have a long history. Representing the meaning
of language in a first-order logical form is appealing because it provides a powerful and flexible way to
express even complex propositions. However, systems built solely using first-order logical forms tend
to be very brittle as they have no way of integrating uncertain knowledge. They, therefore, tend to have
high precision at the cost of low recall (Bos and Markert, 2005).
Recent advances in computational linguistics have yielded robust methods that use weighted or prob-
abilistic models. For example, distributional models of word meaning have been used successfully to
judge paraphrase appropriateness. This has been done by representing the word meaning in context as
a point in a high-dimensional semantics space (Erk and Pado?, 2008; Thater et al, 2010; Erk and Pado?,
2010). However, these models typically handle only individual phenomena instead of providing a mean-
ing representation for complete sentences. It is a long-standing open question how best to integrate the
weighted or probabilistic information coming from such modules with logic-based representations in a
way that allows for reasoning over both. See, for example, Hobbs et al (1993).
The goal of this work is to combine logic-based meaning representations with probabilities in a
single unified framework. This will allow us to obtain the best of both situations: we will have the
full expressivity of first-order logic and be able to reason with probabilities. We believe that this will
allow for a more complete and robust approach to natural language understanding. In order to perform
logical inference with probabilities, we draw from the large and active body of work related to Statistical
Relational AI (Getoor and Taskar, 2007). Specifically, we make use of Markov Logic Networks (MLNs)
(Richardson and Domingos, 2006) which employ weighted graphical models to represent first-order
logical formulas. MLNs are appropriate for our approach because they provide an elegant method of
assigning weights to first-order logical rules, combining a diverse set of inference rules, and performing
inference in a probabilistic way.
While this is a large and complex task, this paper proposes a series of first steps toward our goal.
In this paper, we focus on three natural language phenomena and their interaction: implicativity and
factivity, word meaning, and coreference. Our framework parses natural language into a logical form,
adds rule weights computed by external NLP modules, and performs inferences using an MLN. Our
end-to-end approach integrates multiple existing tools. We use Boxer (Bos et al, 2004) to parse natural
105
language into a logical form. We use Alchemy (Kok et al, 2005) for MLN inference. Finally, we use the
exemplar-based distributional model of Erk and Pado? (2010) to produce rule weights.
2 Background
Logic-based semantics. Boxer (Bos et al, 2004) is a software package for wide-coverage semantic anal-
ysis that provides semantic representations in the form of Discourse Representation Structures (Kamp
and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Bos and Markert (2005)
describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise
and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem
prover to check for logical entailment.
Distributional models for lexical meaning. Distributional models describe the meaning of a word
through the context in which it appears (Landauer and Dumais, 1997; Lund and Burgess, 1996), where
contexts can be documents, other words, or snippets of syntactic structure. Distributional models are able
to predict semantic similarity between words based on distributional similarity and they can be learned
in an unsupervised fashion. Recently distributional models have been used to predict the applicability
of paraphrases in context (Mitchell and Lapata, 2008; Erk and Pado?, 2008; Thater et al, 2010; Erk and
Pado?, 2010). For example, in ?The wine left a stain?, ?result in? is a better paraphrase for ?leave? than is
?entrust?, while the opposite is true in ?He left the children with the nurse?. Usually, the distributional
representation for a word mixes all its usages (senses). For the paraphrase appropriateness task, these
representations are then reweighted, extended, or filtered to focus on contextually appropriate usages.
Markov Logic. An MLN consists of a set of weighted first-order clauses. It provides a way of softening
first-order logic by making situations in which not all clauses are satisfied less likely but not impossible
(Richardson and Domingos, 2006). More formally, letX be the set of all propositions describing a world
(i.e. the set of all ground atoms), F be the set of all clauses in the MLN, wi be the weight associated
with clause fi ? F , Gfi be the set of all possible groundings of clause fi, and Z be the normalization
constant. Then the probability of a particular truth assignment x to the variables in X is defined as:
P (X = x) =
1
Z exp
?
?
?
fi?F
wi
?
g?Gfi
g(x)
?
? =
1
Z exp
?
?
?
fi?F
wini(x)
?
? (1)
where g(x) is 1 if g is satisfied and 0 otherwise, and ni(x) =
?
g?Gfi
g(x) is the number of groundings
of fi that are satisfied given the current truth assignment to the variables in X . This means that the
probability of a truth assignment rises exponentially with the number of groundings that are satisfied.
Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)).
However, this paper marks the first attempt at representing deep logical semantics in an MLN.
While it is possible learn rule weights in anMLN directly from training data, our approach at this time
focuses on incorporating weights computed by external knowledge sources. Weights for word meaning
rules are computed from the distributional model of lexical meaning and then injected into the MLN.
Rules governing implicativity and coreference are given infinite weight (hard constraints).
3 Evaluation and phenomena
Textual entailment offers a good framework for testing whether a system performs correct analyses and
thus draws the right inferences from a given text. For example, to test whether a system correctly handles
implicative verbs, one can use the premise p along with the hypothesis h in (1) below. If the system
analyses the two sentences correctly, it should infer that h holds. While the most prominent forum using
textual entailment is the Recognizing Textual Entailment (RTE) challenge (Dagan et al, 2005), the RTE
datasets do not test the phenomena in which we are interested. For example, in order to evaluate our
system?s ability to determine word meaning in context, the RTE pair would have to specifically test word
106
sense confusion by having a word?s context in the hypothesis be different from the context of the premise.
However, this simply does not occur in the RTE corpora. In order to properly test our phenomena, we
construct hand-tailored premises and hypotheses based on real-world texts.
In this paper, we focus on three natural language phenomena and their interaction: implicativity and
factivity, word meaning, and coreference. The first phenomenon, implicativity and factivity, is concerned
with analyzing the truth conditions of nested propositions. For example, in the premise of the entailment
pair shown in example (1), ?arrange that? falls under the scope of ?forget to? and ?fail? is under the scope
of ?arrange that?. Correctly recognizing nested propositions is necessary for preventing false inferences
such as the one in example (2).
(1) p: Ed did not forget to arrange that Dave fail1
h: Dave failed
(2) p: The mayor hoped to build a new stadium2
h*: The mayor built a new stadium
For the second phenomenon, word meaning, we address paraphrasing and hypernymy. For example,
in (3) ?covering? is a good paraphrase for ?sweeping? while ?brushing? is not.
(3) p: A stadium craze is sweeping the country
h1: A stadium craze is covering the country
h2*: A stadium craze is brushing the country
The third phenomenon is coreference, as illustrated in (4). For this example, to correctly judge the
hypothesis as entailed, it is necessary to recognize that ?he? corefers with ?Christopher? and ?the new
ballpark? corefers with ?a replacement for Candlestick Park?.
(4) p: George Christopher has been a critic of the plan to build a replacement for Candlestick Park.
As a result, he won?t endorse the new ballpark.
h: Christopher won?t endorse a replacement for Candlestick Park.
Some natural language phenomena are most naturally treated as categorial, while others are more
naturally treated using weights or probabilities. In this paper, we treat implicativity and coreference as
categorial phenomena, while using a probabilistic approach to word meaning.
4 Transforming natural language text to logical form
In transforming natural language text to logical form, we build on the software package Boxer (Bos et al,
2004). We chose to use Boxer for two main reasons. First, Boxer is a wide-coverage system that can deal
with arbitrary text. Second, the DRSs that Boxer produces are close to the standard first-order logical
forms that are required for use by the MLN software package Alchemy. Our system transforms Boxer
output into a format that Alchemy can read and augments it with additional information.
To demonstrate our transformation procedure, consider again the premise of example (1). When
given to Boxer, the sentence produces the output given in Figure 1a. We then transform this output to the
format given in Figure 1b.
Flat structure. In Boxer output, nested propositional statements are represented as nested sub-DRS
structures. For example, in the premise of (1), the verbs ?forget to? and ?arrange that? both introduce
nested propositions, as is shown in Figure 1a where DRS x3 (the ?arranging that?) is the theme of ?forget
to? and DRS x5 (the ?failing?) is the theme of ?arrange that?.
In order to write logical rules about the truth conditions of nested propositions, the structure has to
be flattened. However, it is clearly not sufficient to just conjoin all propositions at the top level. Such an
approach, applied to example (2), would yield (hope(x1) ? theme(x1, x2) ? build(x2) ? . . .), leading
to the wrong inference that the stadium was built. Instead, we add a new argument to each predicate that
1Examples (1) and (16) and Figure 2 are based on examples by MacCartney and Manning (2009)
2Examples (2), (3), (4), and (18) are modified versions of sentences from document wsj 0126 from the Penn Treebank
107
x0 x1
named(x0,ed,per)
named(x1,dave,per)
?
x2 x3
forget(x2)
event(x2)
agent(x2,x0)
theme(x2,x3)
x3:
x4 x5
arrange(x4)
event(x4)
agent(x4,x0)
theme(x4,x5)
x5:
x6
fail(x6)
event(x6)
agent(x6,x1)
(a) Output from Boxer
transforms to????????
named(l0, ne per ed d s0 w0, z0)
named(l0, ne per dave d s0 w7, z1)
not(l0, l1)
pred(l1, v forget d s0 w3, e2)
event(l1, e2)
rel(l1, agent, e2, z0)
rel(l1, theme, e2, l2)
prop(l1, l2)
pred(l2, v arrange d s0 w5, e4)
event(l2, e4)
rel(l2, agent, e4, z0)
rel(l2, theme, e4, l3)
prop(l2, l3)
pred(l3, v fail d s0 w8, e6)
event(l3, e6)
rel(l3, agent, e6, z1)
(b) Canonical form
Figure 1: Converting the premise of (1) from Boxer output to MLN input
names the DRS in which the predicate originally occurred. Assigning the label l1 to the DRS containing
the predicate forget, we add l1 as the first argument to the atom pred(l1, v forget d s0 w3, e2).3 Having
flattened the structure, we need to re-introduce the information about relations between DRSs. For this
we use predicates not, imp, and or whose arguments are DRS labels. For example, not(l0, l1) states that
l1 is inside l0 and negated. Additionally, an atom prop(l0, l1) indicates that DRS l0 has a subordinate
DRS labeled l1.
One important consequence of our flat structure is that the truth conditions of our representation no
longer coincide with the truth conditions of the underlying DRS being represented. For example, we do
not directly express the fact that the ?forgetting? is actually negated, since the negation is only expressed
as a relation between DRS labels. To access the information encoded in relations between DRS labels, we
add predicates that capture the truth conditions of the underlying DRS. We use the predicates true(label)
and false(label) that state whether the DRS referenced by label is true or false. We also add rules that
govern how the predicates for logical operators interact with these truth values. For example, the rules in
(5) state that if a DRS is true, then any negated subordinate must be false and vice versa.
? p n.[not(p, n) ? (true(p) ? false(n)) ? (false(p) ? true(n))] (5)
Injecting additional information into the logical form. We want to augment Boxer output with addi-
tional information, for example gold coreference annotation for sentences that we subsequently analyze
with Boxer. In order to do so, we need to be able to tie predicates in the Boxer output back to words in
the original sentence. Fortunately, the optional ?Prolog? output format from Boxer provides the sentence
and word indices from the original sentence. When parsing the Boxer output, we extract these indices
and concatenate them to the word lemma to specific the exact occurrence of the lemma that is under
discussion. For example, the atom pred(l1, v forget d s0 w3, e2) indicates that event e2 refers to the
lemma ?forget? that appears in the 0th sentence of discourse d at word index 3.
Atomic formulas. We represent the words from the sentence as arguments instead of predicates in order
to simplify the set of inference rules we need to specify. Because our flattened structure requires that
the inference mechanism be reimplemented as a set of logical rules, it is desirable for us to be able to
write general rules that govern the interaction of atoms. With the representation we have chosen, we
can quantify over all predicates or all relations. For example, the rule in (6) states that a predicate is
accessible if it is found in an out-scoping DRS.
3The extension to the word, such as d s0 w3 for ?forget?, is an index providing the location of the original word that
triggered this atom; this is addressed in more detail shortly.
108
signature example
managed to +/- he managed to escape  he escaped
he did not manage to escape  he did not escape
refused to -/o he refused to fight  he did not fight
he did not refuse to fight 2 {he fought, he did not fight}
Figure 2: Implication Signatures
? l1 l2.[outscopes(l1, l2) ? ? p x.[pred(l1, p, x) ? pred(l2, p, x)]] (6)
We use three different predicate symbols to distinguish three types of atomic concepts: predicates,
named entities, and relations. Predicates and named entities represent words that appear in the text.
For example, named(l0, ne per ed d s0 w0, z0) indicates that variable z0 is a person named ?Ed? while
pred(l1, v forget d s0 w3, e2) says that e2 is a ?forgetting to? event. Relations capture the relationships
between words. For example, rel(l1, agent, e2, z0) indicates that z0, ?Ed?, is the ?agent? of the ?forgetting
to? event e2.
5 Handling the phenomena
Implicatives and factives
Nairn et al (2006) presented an approach to the treatment of inferences involving implicatives and fac-
tives. Their approach identifies an ?implication signature? for every implicative or factive verb that
determines the truth conditions for the verb?s nested proposition, whether in a positive or negative en-
vironment. Implication signatures take the form ?x/y? where x represents the implicativity in the the
positive environment and y represents the implicativity in the negative environment. Both x and y have
three possible values: ?+? for positive entailment, meaning the nested proposition is entailed, ?-? for
negative entailment, meaning the negation of the proposition is entailed, and ?o? for ?null? entailment,
meaning that neither the proposition nor its negation is entailed. Figure 2 gives concrete examples.
We use these implication signatures to automatically generate rules that license specific entailments
in the MLN. Since ?forget to? has implication signature ?-/+?, we generate the two rules in (7).
(7) (a) ? l1 l2 e.[(pred(l1, ?forget?, e) ? true(l1) ? rel(l1, ?theme?, e, l2) ? prop(l1, l2)) ? false(l2)]]4
(b) ? l1 l2 e.[(pred(l1, ?forget?, e) ? false(l1) ? rel(l1, ?theme?, e, l2) ? prop(l1, l2)) ? true(l2)]
To understand these rules, consider (7a). The rule says that if the atom for the verb ?forget to? appears
in a DRS that has been determined to be true, then the DRS representing any ?theme? proposition of that
verb should be considered false. Likewise, (7b) says that if the occurrence of ?forget to? appears in a
DRS determined to be false, then the theme DRS should be considered true.
Note that when the implication signature indicates a ?null? entailment, no rule is generated for that
case. This prevents the MLN from licensing entailments related directly to the nested proposition, but
still allows for entailments that include the factive verb. So he wanted to fly entails neither he flew nor he
did not fly, but it does still license he wanted to fly.
Ambiguity in word meaning
In order for our system to be able to make correct natural language inference, it must be able to handle
paraphrasing and deal with hypernymy. For example, in order to license the entailment pair in (8), the
system must recognize that ?owns? is a valid paraphrase for ?has?, and that ?car? is a hypernym of
?convertible?.
(8) p: Ed has a convertible
h: Ed owns a car
4Occurrence-indexing on the predicate ?forget? has been left out for brevity.
109
In this section we discuss our probabilistic approach to paraphrasing. In the next section we discuss
how this approach is extended to cover hypernymy. A central problem to solve in the context of para-
phrases is that they are context-dependent. Consider again example (3) and its two hypotheses. The first
hypothesis replaces the word ?sweeping? with a paraphrase that is valid in the given context, while the
second uses an incorrect paraphrase.
To incorporate paraphrasing information into our system, we first generate rules stating all paraphrase
relationships that may possibly apply to a given predicate/hypothesis pair, using WordNet (Miller, 2009)
as a resource. Then we associate those rules with weights to signal contextual adequacy. For any two
occurrence-indexed words w1, w2 occurring anywhere in the premise or hypothesis, we check whether
they co-occur in a WordNet synset. If w1, w2 have a common synset, we generate rules of the form
? l x.[pred(l, w1, x) ? pred(l, w2, x)] to connect them. For named entities, we perform a similar
routine: for each pair of matching named entities found in the premise and hypothesis, we generate a
rule ? l x.[named(l, w1, x) ? named(l, w2, x)].
We then use the distributional model of Erk and Pado? (2010) to compute paraphrase appropriateness.
In the case of (3) this means measuring the cosine similarity between the vectors for ?sweep? and ?cover?
(and between ?sweep? and ?brush?) in the sentential context of the premise. MLN formula weights are
expected to be log-odds (i.e., log(P/(1?P )) for some probability P ), so we rank all possible paraphrases
of a given word w by their cosine similarity to w and then give them probabilities that decrease by
rank according to a Zipfian distribution. So, the kth closest paraphrase by cosine similarity will have
probability Pk given by (9):
Pk ? 1/k (9)
The generated rules are given in (10) with the actual weights calculated for example (3). Note that
the valid paraphrase ?cover? is given a higher weight than the incorrect paraphrase ?brush?, which allows
the MLN inference procedure to judge h1 as a more likely entailment than h2.5 This same result would
not be achieved if we did not take context into consideration because, without context, ?brush? is a more
likely paraphrase of ?sweep? than ?cover?.
(10) (a) -2.602 ? l x.[pred(l, ?v sweep p s0 w4?, x) ? pred(l, ?v cover h s0 w4?, x)]
(b) -3.842 ? l x.[pred(l, ?v sweep p s0 w4?, x) ? pred(l, ?v brush h s0 w4?, x)]
Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to
specify a probability threshold indicating entailment. An appropriate threshold between ?entailment?
and ?non-entailment? will be one that separates the probability of an inference with the valid rule from
the probability of an inference with the invalid rule. While we plan to automatically induce a threshold
in the future, our current implementation uses a value set manually.
Hypernymy
Like paraphrasehood, hypernymy is context-dependent: In ?A bat flew out of the cave?, ?animal? is
an appropriate hypernym for ?bat?, but ?artifact? is not. So we again use distributional similarity to
determine contextual appropriateness. However, we do not directly compute cosine similarities between
a word and its potential hypernym. We can hardly assume ?baseball bat? and ?artifact? to occur in similar
distributional contexts. So instead of checking for similarity of ?bat? and ?artifact? in a given context, we
check ?bat? and ?club?. That is, we pick a synonym or close hypernym of the word in question (?bat?)
that is also a WordNet hyponym of the hypernym to check (?artifact?).
A second problem to take into account is the interaction of hypernymy and polarity. While (8) is a
valid pair, (11) is not, because ?have a convertible? is under negation. So, we create weighted rules of
the form hypernym(w, h), along with inference rules to guide their interaction with polarity. We create
5Because weights are calculated according to the equation log(P/(1 ? P )), any paraphrase that has a probability of less
than 0.5 will have a negative weight. Since most paraphrases will have probabilities less than 0.5, most will yield negative
rule weights. However, the inferences are still handled properly in the MLN because the inference is dependent on the relative
weights.
110
these rules for all pairs of words w, h in premise and hypothesis such that h is a hypernym of w, again
using WordNet to determine potential hypernymy.
(11) p: Ed does not have a convertible
h: Ed does not own a car
Our inference rules governing the interaction of hypernymy and polarity are given in (12). The rule
in (12a) states that in a positive environment, the hyponym entails the hypernym while the rule in (12b)
states that in a negative environment, the opposite is true: the hypernym entails the hyponym.
(12) (a) ? l p1 p2 x.[(hypernym(p1, p2) ? true(l) ? pred(l, p1, x)) ? pred(l, p2, x)]]
(b) ? l p1 p2 x.[(hypernym(p1, p2) ? false(l) ? pred(l, p2, x)) ? pred(l, p1, x)]]
Making use of coreference information
As a test case for incorporating additional resources into Boxer?s logical form, we used the coreference
data in OntoNotes (Hovy et al, 2006). However, the same mechanism would allow us to transfer in-
formation into Boxer output from arbitrary additional NLP tools such as automatic coreference analysis
tools or semantic role labelers. Our system uses coreference information into two distinct ways.
The first way we make use of coreference data is to copy atoms describing a particular variable
to those variables that corefer. Consider again example (4) which has a two-sentence premise. This
inference requires recognizing that the ?he? in the second sentence of the premise refers to ?George
Christopher? from the first sentence. Boxer alone is unable to make this connection, but if we receive
this information as input, either from gold-labeled data or a third-party coreference tool, we are able to
incorporate it. Since Boxer is able to identify the index of the word that generated a particular predicate,
we can tie each predicate to any related coreference chains. Then, for each atom on the chain, we can
inject copies of all of the coreferring atoms, replacing the variables to match. For example, the word
?he? generates an atom pred(l0, male, z5)6 and ?Christopher? generates atom named(l0, christopher, x0).
So, we can create a new atom by taking the atom for ?christopher? and replacing the label and variable
with that of the atom for ?he?, generating named(l0, christopher, x5).
As a more complex example, the coreference information will inform us that ?the new ballpark?
corefers with ?a replacement for Candlestick Park?. However, our system is currently unable to handle
this coreference correctly at this time because, unlike the previous example, the expression ?a replace-
ment for Candlestick Park? results in a complex three-atom conjunct with two separate variables: pred(l2,
replacement, x6), rel(l2, for, x6, x7), and named(l2, candlestick park, x7). Now, unifying with the atom
for ?a ballpark?, pred(l0, ballpark, x3), is not as simple as replacing the variable because there are two
variables to choose from. Note that it would not be correct to replace both variables since this would
result in a unification of ?ballpark? with ?candlestick park? which is wrong. Instead we must determine
that x6 should be the one to unify with x3 while x7 is replaced with a fresh variable. The way that we can
accomplish this is to look at the dependency parse of the sentence that is produced by the C&C parser is
a precursor to running Boxer. By looking up both ?replacement? and ?Candlestick Park? in the parse, we
can determine that ?replacement? is the head of the phrase, and thus is the atom whose variable should
be unified. So, we would create new atoms, pred(l0, replacement, x3), rel(l0, for, x3, z0), and named(l0,
candlestick park, z0), where z0 is a fresh variable.
The second way that we make use of coreference information is to extend the sentential contexts
used for calculating the appropriateness of paraphrases in the distributional model. In the simplest case,
the sentential context of a word would simply be the other words in the sentence. However, consider the
context of the word ?lost? in the second sentence of (13).
(13) p1: In [the final game of the season]1, [the team]2 held on to their lead until overtime
p2: But despite that, [they]2 eventually lost [it all]1
6Atoms simplified for brevity
111
Here we would like to disambiguate ?lost?, but its immediate context, words like ?despite? and
?eventually?, gives no indication as to its correct sense. Our procedure extends the context of the sentence
by incorporating all of the words from all of the phrases that corefer with a word in the immediate
context. Since coreference chains 1 and 2 have words in p2, the context of ?lost? ends up including
?final?, ?game?, ?season?, and ?team? which give a strong indication of the sense of ?lost?. Note that
using coreference data is stronger than simply expanding the window because coreferences can cover
arbitrarily long distances.
6 Evaluation
As a preliminary evaluation of our system, we constructed a set of demonstrative examples to test our
ability to handle the previously discussed phenomena and their interactions and ran each example with
both a theorem prover and Alchemy. Note that when running an example in the theorem prover, weights
are not possible, so any rule that would be weighted in an MLN is simply treated as a ?hard clause?
following Bos and Markert (2005).
Checking the logical form. We constructed a list of 72 simple examples that exhaustively cover cases
of implicativity (positive, negative, null entailments in both positive and negative environments), hyper-
nymy, quantification, and the interaction between implicativity and hypernymy. The purpose of these
simple tests is to ensure that our flattened logical form and truth condition rules correctly maintain the
semantics of the underlying DRSs. Examples are given in (14).
(14) (a) The mayor did not manage to build a stadium 2 The mayor built a stadium
(b) Fido is a dog and every dog walks  A dog walks
Examples in previous sections. Examples (1), (2), (3), (8), and (11) all come out as expected. Each
of these examples demonstrates one of the phenomena in isolation. However, example (4) returns ?not
entailed?, the incorrect answer. As discussed previously, this failure is a result of our system?s inabil-
ity to correctly incorporate the complex coreferring expression ?a replacement for Candlestick Park?.
However, the system is able to correctly incorporate the coreference of ?he? in the second sentence to
?Christopher? in the first.
Implicativity and word sense. For example (15), ?fail to? is a negatively entailing implicative in a
positive environment. So, p correctly entails hgood in both the theorem prover and Alchemy. However,
the theorem prover incorrectly licenses the entailment of hbad while Alchemy does not. The probabilistic
approach performs better in this situation because the categorial approach does not distinguish between
a good paraphrase and a bad one. This example also demonstrates the advantage of using a context-
sensitive distributional model to calculate the probabilities of paraphrases because ?reward? is an a priori
better paraphrase than ?observe? according to WordNet since it appears in a higher ranked synset.
(15) p: The U.S. is watching closely as South Korea fails to honor U.S. patents7
hgood: South Korea does not observe U.S. patents
hbad: South Korea does not reward U.S. patents
Implicativity and hypernymy. MacCartney and Manning (2009) extended the work by Nairn et al
(2006) in order to correctly treat inference involving monotonicity and exclusion. Our approaches to
implicatives and factivity and hyper/hyponymy combine naturally to address these issues because of the
structure of our logical representations and rules. For example, no additional work is required to license
the entailments in (16).
(16) (a) John refused to dance  John didn?t tango
(b) John did not forget to tango  John danced
7Example (15) is adapted from Penn Treebank document wsj 0020 while (17) is adapted from document wsj 2358
112
Example (17) demonstrates how our system combines categorial implicativity with a probabilistic
approach to hypernymy. The verb ?anticipate that? is positively entailing in the negative environment.
The verb ?moderate? can mean ?chair? as in ?chair a discussion? or ?curb? as in ?curb spending?. Since
?restrain? is a hypernym of ?curb?, it receives a weight based on the applicability of the word ?curb? in
the context. Similarly, ?talk? receives a weight based on its hyponym ?chair?. Since our model predicts
?curb? to be a more probable paraphrase of ?moderate? in this context than ?chair? (even though the
priors according to WordNet are reversed), the system is able to infer hgood while rejecting hbad.
(17) p: He did not anticipate that inflation would moderate this year
hgood: Inflation restrained this year
hbad: Inflation talked this year
Word sense, coreference, and hypernymy. Example (18) demonstrates the interaction between para-
phrase, hypernymy, and coreference incorporated into a single entailment. The relevant coreference
chains are marked explicitly in the example. The correct inference relies on recognizing that ?he? in the
hypothesis refers to ?Joe Robbie? and ?it? to ?coliseum?, which is a hyponym of ?stadium?. Further,
our model recognizes that ?sizable? is a better paraphrase for ?healthy? than ?intelligent? even though
WordNet has the reverse order.
(18) p: [Joe Robbie]53 couldn?t persuade the mayor , so [he]53 built [[his]53 own coliseum]54.
[He]53 has used [it]54 to turn a healthy profit.8
hgood: Joe Robbie used a stadium to turn a sizable profit
hbad?1: Joe Robbie used a stadium to turn an intelligent profit
hbad?2: The mayor used a stadium to turn a healthy profit
7 Future work
The next step is to execute a full-scale evaluation of our approach using more varied phenomena and
naturally occurring sentences. However, the memory requirements of Alchemy are a limitation that
prevents us from currently executing larger and more complex examples. The problem arises because
Alchemy considers every possible grounding of every atom, even when a more focused subset of atoms
and inference rules would suffice. There is on-going work to modify Alchemy so that only the required
groundings are incorporated into the network, reducing the size of the model and thus making it possible
to handle more complex inferences. We will be able to begin using this new version of Alchemy very
soon and our task will provide an excellent test case for the modification.
Since Alchemy outputs a probability of entailment, it is necessary to fix a threshold that separates
entailment from nonentailment. We plan to use machine learning techniques to compute an appropriate
threshold automatically from a calibration dataset such as a corpus of valid and invalid paraphrases.
8 Conclusion
In this paper, we have introduced a system that implements a first step towards integrating logical seman-
tic representations with probabilistic weights using methods from Statistical Relational AI, particularly
Markov Logic. We have focused on three phenomena and their interaction: implicatives, coreference,
and word meaning. Taking implicatives and coreference as categorial and word meaning as probabilis-
tic, we have used a distributional model to generate paraphrase appropriateness ratings, which we then
transformed into weights on first order formulas. The resulting MLN approach is able to correctly solve
a number of difficult textual entailment problems that require handling complex combinations of these
important semantic phenomena.
8Only relevent coreferences have been marked
113
References
Bos, J., S. Clark, M. Steedman, J. R. Curran, and J. Hockenmaier (2004). Wide-coverage semantic
representations from a CCG parser. In Proceedings of COLING 2004, Geneva, Switzerland, pp. 1240?
1246.
Bos, J. and K. Markert (2005). Recognising textual entailment with logical inference. In Proceedings of
EMNLP 2005, pp. 628?635.
Clark, S. and J. R. Curran (2004). Parsing the WSJ using CCG and log-linear models. In Proceedings of
ACL 2004, Barcelona, Spain, pp. 104?111.
Dagan, I., O. Glickman, and B. Magnini (2005). The pascal recognising textual entailment challenge. In
In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.
Erk, K. and S. Pado? (2008). A structured vector space model for word meaning in context. In Proceedings
of EMNLP 2008, Honolulu, HI, pp. 897?906.
Erk, K. and S. Pado? (2010). Exemplar-based models for word meaning in context. In Proceedings of
ACL 2010, Uppsala, Sweden, pp. 92?97.
Getoor, L. and B. Taskar (Eds.) (2007). Introduction to Statistical Relational Learning. Cambridge, MA:
MIT Press.
Hobbs, J. R., M. Stickel, D. Appelt, and P. Martin (1993). Interpretation as abduction. Artificial Intelli-
gence 63(1?2), 69?142.
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). Ontonotes: The 90% solution.
In Proceedings of HLT/NAACL 2006, pp. 57?60.
Kamp, H. and U. Reyle (1993). From Discourse to Logic; An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and DRT. Dordrecht: Kluwer.
Kok, S., P. Singla, M. Richardson, and P. Domingos (2005). The Alchemy system for statistical relational
AI. Technical report, Department of Computer Science and Engineering, University of Washington.
http://www.cs.washington.edu/ai/alchemy.
Landauer, T. and S. Dumais (1997). A solution to Platos problem: the latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological Review 104(2), 211?240.
Lund, K. and C. Burgess (1996). Producing high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments, and Computers 28, 203?208.
MacCartney, B. and C. D. Manning (2009). An extended model of natural logic. In Proceedings of the
Eighth International Conference on Computational Semantics (IWCS-8), pp. 140?156.
Miller, G. A. (2009). Wordnet - about us. http://wordnet.princeton.edu.
Mitchell, J. and M. Lapata (2008). Vector-based models of semantic composition. In Proceedings of
ACL, pp. 236?244.
Nairn, R., C. Condoravdi, and L. Karttunen (2006). Computing relative polarity for textual inference. In
Proceedings of Inference in Computational Semantics (ICoS-5), Buxton, UK.
Poon, H. and P. Domingos (2009). Unsupervised semantic parsing. In Proceedings of EMNLP 2009, pp.
1?10.
Richardson, M. and P. Domingos (2006). Markov logic networks. Machine Learning 62, 107?136.
Thater, S., H. Fu?rstenau, and M. Pinkal (2010). Contextualizing semantic representations using syntac-
tically enriched vector models. In Proceedings of ACL 2010, Uppsala, Sweden, pp. 948?957.
114
Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ?13), pages 10?19,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Generating Natural-Language Video Descriptions
Using Text-Mined Knowledge
Niveda Krishnamoorthy ?
UT Austin
niveda@cs.utexas.edu
Girish Malkarnenkar ?
UT Austin
girish@cs.utexas.edu
Raymond Mooney
UT Austin
mooney@cs.utexas.edu
Kate Saenko
UMass Lowell
saenko@cs.uml.edu
Sergio Guadarrama
UC Berkeley
sguada@eecs.berkeley.edu
Abstract
We present a holistic data-driven technique
that generates natural-language descriptions
for videos. We combine the output of state-of-
the-art object and activity detectors with ?real-
world? knowledge to select the most proba-
ble subject-verb-object triplet for describing a
video. We show that this knowledge, automat-
ically mined from web-scale text corpora, en-
hances the triplet selection algorithm by pro-
viding it contextual information and leads to
a four-fold increase in activity identification.
Unlike previous methods, our approach can
annotate arbitrary videos without requiring the
expensive collection and annotation of a sim-
ilar training video corpus. We evaluate our
technique against a baseline that does not use
text-mined knowledge and show that humans
prefer our descriptions 61% of the time.
1 Introduction
Combining natural-language processing (NLP) with
computer vision to to generate English descriptions
of visual data is an important area of active research
(Farhadi et al, 2010; Motwani and Mooney, 2012;
Yang et al, 2011). We present a novel approach to
generating a simple sentence for describing a short
video that:
1. Identifies the most likely subject, verb and
object (SVO) using a combination of visual
object and activity detectors and text-mined
knowledge to judge the likelihood of SVO
triplets. From a natural-language generation
?Indicates equal contribution
(NLG) perspective, this is the content planning
stage.
2. Given the selected SVO triplet, it uses a simple
template-based approach to generate candidate
sentences which are then ranked using a statis-
tical language model trained on web-scale data
to obtain the best overall description. This is
the surface realization stage.
Figure 1 shows sample system output. Our ap-
proach can be viewed as a holistic data-driven three-
step process where we first detect objects and ac-
tivities using state-of-the-art visual recognition al-
gorithms. Next, we combine these often noisy de-
tections with an estimate of real-world likelihood,
which we obtain by mining SVO triplets from large-
scale web corpora. Finally, these triplets are used to
generate candidate sentences which are then ranked
for plausibility and grammaticality. The resulting
natural-language descriptions can be usefully em-
ployed in applications such as semantic video search
and summarization, and providing video interpreta-
tions for the visually impaired.
Using vision models alone to predict the best sub-
ject and object for a given activity is problematic,
especially while dealing with challenging real-world
YouTube videos as shown in Figures 4 and 5, as
it requires a large annotated video corpus of simi-
lar SVO triplets (Packer et al, 2012). We are in-
terested in annotating arbitrary short videos using
off-the-shelf visual detectors, without the engineer-
ing effort required to build domain-specific activity
models. Our main contribution is incorporating the
pragmatics of various entities? likelihood of being
10
Figure 1: Content planning and surface realization
the subject/object of a given activity, learned from
web-scale text corpora. For example, animate ob-
jects like people and dogs are more likely to be sub-
jects compared to inanimate objects like balls or TV
monitors. Likewise, certain objects are more likely
to function as subjects/objects of certain activities,
e.g., ?riding a horse? vs. ?riding a house.?
Selecting the best verb may also require recog-
nizing activities for which no explicit training data
has been provided. For example, consider a video
with a man walking his dog. The object detectors
might identify the man and dog; however the action
detectors may only have the more general activity,
?move,? in their training data. In such cases, real-
world pragmatics is very helpful in suggesting that
?walk? is best used to describe a man ?moving? with
his dog. We refer to this process as verb expansion.
After describing the details of our approach, we
present experiments evaluating it on a real-world
corpus of YouTube videos. Using a variety of meth-
ods for judging the output of the system, we demon-
strate that it frequently generates useful descriptions
of videos and outperforms a purely vision-based ap-
proach that does not utilize text-mined knowledge.
2 Background and Related Work
Although there has been a lot of interesting work
done in natural language generation (Bangalore and
Rambow, 2000; Langkilde and Knight, 1998), we
use a simple template for generating our sentences
as we found it to work well for our task.
Most prior work on natural-language descrip-
tion of visual data has focused on static images
(Felzenszwalb et al, 2008; Kulkarni et al, 2011;
Kuznetsova et al, 2012; Laptev et al, 2008; Li et al,
2011; Yao et al, 2010). The small amount of exist-
ing work on videos (Ding et al, 2012; Khan and Go-
toh, 2012; Kojima et al, 2002; Lee et al, 2008; Yao
and Fei-Fei, 2010) uses hand-crafted templates or
rule-based systems, works in constrained domains,
and does not exploit text mining. Barbu et al (2012)
produce sentential descriptions for short video clips
by using an interesting dynamic programming ap-
proach combined with Hidden Markov Models for
obtaining verb labels for each video. However, they
do not use any text mining to improve the quality of
their visual detections.
Our work differs in that we make extensive use of
text-mined knowledge to select the best SVO triple
and generate coherent sentences. We also evaluate
our approach on a generic, large and diverse set of
challenging YouTube videos that cover a wide range
of activities. Motwani and Mooney (2012) explore
how object detection and text mining can aid activity
recognition in videos; however, they do not deter-
mine a complete SVO triple for describing a video
nor generate a full sentential description.
With respect to static image description, Li et al
(2011) generate sentences given visual detections of
objects, visual attributes and spatial relationships;
however, they do not consider actions. Farhadi et al
(2010) propose a system that maps images and the
corresponding textual descriptions to a ?meaning?
space which consists of an object, action and scene
triplet. However, they assume a single object per
image and do not use text-mining to determine the
likelihood of objects matching different verbs. Yang
et al (2011) is the most similar to our approach in
that it uses text-mined knowledge to generate sen-
tential descriptions of static images after performing
object and scene detection. However, they do not
perform activity recognition nor use text-mining to
select the best verb.
3 Approach
Our overall approach is illustrated in Figure 2 and
consists of visual object and activity recognition fol-
lowed by content-planning to generate the best SVO
triple and surface realization to generate the final
sentence.
11
Figure 2: Summary of our approach
3.1 Dataset
We used the English portion of the YouTube data
collected by Chen et al (2010), consisting of short
videos each with multiple natural-language descrip-
tions. This data was previously used by Motwani
and Mooney (2012), and like them, we ensured that
the test data only contained videos in which we can
potentially detect objects. We used the object de-
tector by Felzenszwalb et al (2008) as it achieves
the state-of-the-art performance on the PASCAL Vi-
sual Object Classes (VOC) Challenge. As such, we
selected test videos whose subjects and objects be-
long to the 20 VOC object classes - aeroplane, car,
horse, sheep, bicycle, cat, sofa, bird, chair, motor-
bike, train, boat, cow, person, tv monitor, bottle,
dining table, bus, dog, potted plant. During this
filtering, we also allow synonyms of these object
names by including all words with a Lesk similar-
ity (as implemented by Pedersen et al (2004)) of at
least 0.5.1 Using this approach, we chose 235 po-
tential test videos; the remaining 1,735 videos were
reserved for training.
All the published activity recognition methods
that work on datasets such as KTH (Schuldt et al,
2004), Drinking and Smoking (Laptev and Perez,
2007) and UCF50 (Reddy and Shah, 2012) have
a very limited recognition vocabulary of activity
classes. Since we did not have explicit activity la-
1Empirically, this method worked better than using WordNet
synsets.
Figure 3: Activity clusters discovered by HAC
bels for our YouTube videos, we followed Motwani
and Mooney (2012)?s approach to automatically dis-
cover activity clusters. We first parsed the train-
ing descriptions using Stanford?s dependency parser
(De Marneffe et al, 2006) to obtain the set of verbs
describing each video. We then clustered these verbs
using Hierarchical Agglomerative Clustering (HAC)
using the res metric from WordNet::Similarity by
Pedersen et al (2004) to measure the distance be-
tween verbs. By manually cutting the resulting hi-
erarchy at a desired level (ensuring that each clus-
ter has at least 9 videos), we discovered the 58 ac-
tivity clusters shown in Figure 3. We then filtered
the training and test sets to ensure that all verbs be-
longed to these 58 activity clusters. The final data
contains 185 test and 1,596 training videos.
3.2 Object Detection
We used Felzenszwalb et al (2008)?s
discriminatively-trained deformable parts mod-
els to detect the most likely objects in each video.
Since these object detectors were designed for
static images, each video was split into frames at
one-second intervals. For each frame, we ran the
object detectors and selected the maximum score
assigned to each object in any of the frames. We
converted the detection scores, f(x), to estimated
probabilities p(x) using a sigmoid p(x) = 1
1+e?f(x)
.
3.3 Activity Recognition
In order to get an initial probability distribution for
activities detected in the videos, we used the motion
descriptors developed by Laptev et al (2008). Their
approach extracts spatio-temporal interest points
(STIPs) from which it computes HoG (Histograms
12
Corpora Size of text
British National Corpus (BNC) 1.5GB
WaCkypedia EN 2.6GB
ukWaC 5.5GB
Gigaword 26GB
GoogleNgrams 1012 words
Table 1: Corpora used to Mine SVO Triplets
of Oriented Gradients) and HoF (Histograms of Op-
tical Flow) features over a 3-dimensional space-time
volume. These descriptors are then randomly sam-
pled and clustered to obtain a ?bag of visual words,?
and each video is then represented as a histogram
over these clusters. We experimented with different
classifiers such as LIBSVM (Chang and Lin, 2011)
to train a final activity detector using these features.
Since we achieved the best classification accuracy
(still only 8.65%) using an SVM with the intersec-
tion kernel, we used this approach to obtain a prob-
ability distribution over the 58 activity clusters for
each test video. We later experimented with Dense
Trajectories (Wang et al, 2011) for activity recogni-
tion but there was only a minor improvement.
3.4 Text Mining
We improve these initial probability distributions
over objects and activities by incorporating the like-
lihood of different activities occuring with particular
subjects and objects using two different approaches.
In the first approach, using the Stanford dependency
parser, we parsed 4 different text corpora covering a
wide variety of text: English Gigaword, British Na-
tional Corpus (BNC), ukWac and WaCkypedia EN.
In order to obtain useful estimates, it is essential to
collect text that approximates all of the written lan-
guage in scale and distribution. The sizes of these
corpora (after preprocessing) are shown in Table 1.
Using the dependency parses for these corpora,
we mined SVO triplets. Specifically, we looked for
subject-verb relationships using nsubj dependencies
and verb-object relationships using dobj and prep
dependencies. The prep dependency ensures that
we account for intransitive verbs with prepositional
objects. Synonyms of subjects and objects and con-
jugations of verbs were reduced to their base forms
(20 object classes, 58 activity clusters) while form-
ing triplets. If a subject, verb or object not belonging
to these base forms is encountered, it is ignored dur-
ing triplet construction.
These triplets are then used to train a backoff lan-
guage model with Kneser-Ney smoothing (Chen and
Goodman, 1999) for estimating the likelihood of an
SVO triple. In this model, if we have not seen train-
ing data for a particular SVO trigram, we ?back-off?
to the Subject-Verb and Verb-Object bigrams to co-
herently estimate its probability. This results in a
sophisticated statistical model for estimating triplet
probabilities using the syntactic context in which
the words have previously occurred. This allows us
to effectively determine the real-world plausibility
of any SVO using knowledge automatically mined
from raw text. We call this the ?SVO Language
Model? approach (SVO LM).
In a second approach to estimating SVO prob-
abilities, we used BerkeleyLM (Pauls and Klein,
2011) to train an n-gram language model on the
GoogleNgram corpus (Lin et al, 2012). This sim-
ple model does not consider synonyms, verb con-
jugations, or SVO dependencies but only looks at
word sequences. Given an SVO triplet as an in-
put sequence, it estimates its probability based on
n-grams. We refer to this as the ?Language Model?
approach (LM).
3.5 Verb Expansion
As mentioned earlier, the top activity detections are
expanded with their most similar verbs in order to
generate a larger set of potential words for describ-
ing the action. We used the WUP metric from Word-
Net::Similarity to expand each activity cluster to in-
clude all verbs with a similarity of at least 0.5. For
example, we expand the verb ?move? with go 1.0,
walk 0.8, pass 0.8, follow 0.8, fly 0.8, fall 0.8, come
0.8, ride 0.8, run 0.67, chase 0.67, approach 0.67,
where the number is the WUP similarity.
3.6 Content Planning
To combine the vision detection and NLP scores and
determine the best overall SVO, we use simple lin-
ear interpolation as shown in Equation 1. When
computing the overall vision score, we make a con-
ditional independence assumption and multiply the
probabilities of the subject, activity and object. To
account for expanded verbs, we additionally mul-
tiply by the WUP similarity between the original
13
(Vorig) and expanded (Vsim) verbs. The NLP score
is obtained from either the ?SVO Language Model?
or the ?Language Model? approach, as previously
described.
score = w1 ? vis score + w2 ? nlp score (1)
(2)vis score = P (S|vid) ? P (Vorig|vid)
? Sim(Vsim, Vorig) ? P (O|vid)
After determining the top n=5 object detections
and top k=10 verb detections for each video, we
generate all possible SVO triplets from these nouns
and verbs, including all potential verb expansions.
Each resulting SVO is then scored using Equation 1,
and the best is selected. We compare this approach
to a ?pure vision? baseline where the subject is the
highest scored object detection (which empirically
is more likely to be the subject than the object), the
object is the second highest scored object detection,
and the verb is the activity cluster with the highest
detection probability.
3.7 Surface Realization
Finally, the subject, verb and object from the top-
scoring SVO are used to produce a set of candi-
date sentences, which are then ranked using a lan-
guage model. The text corpora in Table 1 are mined
again to get the top three prepositions for every verb-
object pair. We use a template-based approach in
which each sentence is of the form:
?Determiner (A,The) - Subject - Verb (Present,
Present Continuous) - Preposition (optional) - De-
terminer (A,The) - Object.?
Using this template, a set of candidate sentences are
generated and ranked using the BerkeleyLM lan-
guage model trained on the GoogleNgram corpus.
The top sentence is then used to describe the video.
This surface realization technique is used for both
the vision baseline triplet and our proposed triplet.
In addition to the one presented here, we tried al-
ternative ?pure vision? baselines, but they are not
included since they performed worse. We tried a
non-parametric approach similar to Ordonez et al
(2011), which computes global similarity of the
query to a large captioned dataset and returns the
nearest neighbor?s description. To compute the sim-
ilarity we used an RBF-Chi2 kernel over bag-of-
words STIP features. However, as noted by Ordonez
et al (2011), who used 1 million Flickr images, our
dataset is likely not large enough to produce good
matches. In an attempt to combine information from
both object and activity recognition, we also tried
combining object detections from 20 PASCAL ob-
ject detectors (Felzenszwalb et al, 2008) and from
Object Bank (Li et al, 2010) using a multi-channel
approach as proposed in (Zhang et al, 2007), with a
RBF-Chi2 kernel for the STIP features and a RBF-
Correlation Distance kernel for object detections.
4 Experimental Results
4.1 Content Planning
We first evaluated the ability of the system to iden-
tify the best SVO content. From the ? 50 human
descriptions available for each video, we identified
the SVO for each description and then determined
the ground-truth SVO for each of the 185 test videos
using majority vote. These verbs were then mapped
back to their 58 activity clusters. For the results pre-
sented in Tables 2 and 3, we assigned the vision
score a weight of 0 (w1 = 0) and the NLP score
a weight of 1 (w2 = 1) since these weights gave us
the best performance for thresholds of 5 and 10 for
the objects and activity detections respectively. Note
that while the vision score is given a weight of zero,
the vision detections still play a vital role in the de-
termination of the final triplet since our model only
considers the objects and activities with the highest
vision detection scores.
To evaluate the accuracy of SVO identification,
we used two metrics. The first is a binary metric that
requires exactly matching the gold-standard subject,
verb and object. We also evaluate the overall triplet
accuracy. Note that the verb accuracy in the vision
baseline is not word-based and is measured on the
58 activity classes. Its results are shown in Table 2,
where VE and NVE stand for ?verb expansion?
and ?no verb expansion? respectively. However,
the binary evaluation can be unduly harsh. If we
incorrectly choose ?bicycle? instead of a ?motor-
bike? as the object, it should be considered better
than choosing ?dog.? Similarly, predicting ?chop?
instead of ?slice? is better than choosing ?go?.
14
Method Subject% Verb% Object% All%
Vision Baseline 71.35 8.65 29.19 1.62
LM(VE) 71.35 8.11 10.81 0.00
SVO LM(NVE) 85.95 16.22 24.32 11.35
SVO LM(VE) 85.95 36.76 33.51 23.78
Table 2: SVO Triplet accuracy: Binary metric
Method Subject% Verb% Object% All%
Vision Baseline 87.76 40.20 61.18 63.05
LM(VE) 85.77 53.32 61.54 66.88
SVO LM(NVE) 94.90 63.54 69.39 75.94
SVO LM(VE) 94.90 66.36 72.74 78.00
Table 3: SVO Triplet accuracy: WUP metric
In order to account for such similarities, we also
measure the WUP similarity between the predicted
and correct items. For the examples above, the rel-
evant scores are: wup(motorbike,bicycle)=0.7826,
wup(motorbike,dog)=0.1, wup(slice,chop)=0.8,
wup(slice,go)=0.2857. The results for the WUP
metric are shown in Table 3.
4.2 Surface Realization
Figures 4 and 5 show examples of good and bad sen-
tences generated by our method compared to the vi-
sion baseline.
4.2.1 Automatic Metrics
To automatically compare the sentences gener-
ated for the test videos to ground-truth human de-
scriptions, we employed the BLEU and METEOR
metrics used to evaluate machine-translation output.
METEOR was designed to fix some of the prob-
lems with the more popular BLEU metric. They
both measure the number of matching n-grams (for
various values of n) between the automatic and hu-
man generated sentences. METEOR takes stem-
ming and synonymy into consideration. We used
the SVO Language Model (with verb expansion) ap-
proach since it gave us the best results for triplets.
The results are given in Table 4.
4.2.2 Human Evaluation using Mechanical
Turk
Given the limitations of metrics like BLEU and
METEOR, we also asked human judges to evalu-
ate the quality of the sentences generated by our ap-
Figure 4: Examples where we outperform the baseline
Figure 5: Examples where we underperform the baseline
proach compared to those generated by the baseline
system. For each of the 185 test videos, we asked 9
unique workers (with >95% HIT approval rate and
who had worked on more than 1000 HITs) on Ama-
zon Mechanical Turk to pick which sentence better
described the video. We also gave them a ?none
of the above two sentences? option in case neither
of the sentences were relevant to the video. Qual-
ity was controlled by also including in each HIT a
gold-standard example generated from the human
descriptions, and discarding judgements of workers
who incorrectly answered this gold-standard item.
Overall, when they expressed a preference, hu-
mans picked our descriptions to that of the baseline
Method BLEU score METEOR score
Vision Baseline 0.37?0.05 0.25?0.08
SVO LM(VE) 0.45?0.05 0.36?0.27
Table 4: Automatic evaluation of sentence quality
15
61.04% of the time. Out of the 84 videos where the
majority of judges had a clear preference, they chose
our descriptions 65.48% of the time.
5 Discussion
Overall, the results consistently show the advantage
of utilizing text-mined knowledge to improve the se-
lection of an SVO that best describes a video. Below
we discuss various specific aspects of the results.
Vision Baseline: For the vision baseline, the sub-
ject accuracy is quite high compared to the object
and activity accuracies. This is likely because the
person detector has higher recall and confidence
than the other object detectors. Since most test
videos have a person as the subject, this works in
favor of the vision baseline, as typically the top ob-
ject detection is ?person?. Activity (verb) accuracy
is quite low (8.65% binary accuracy). This is be-
cause there are 58 activity clusters, some with very
little training data. Object accuracy is not as high
as subject accuracy because the true object, while
usually present in the top object detections, is not
always the second-highest object detection. By al-
lowing ?partial credit?, the WUP metric increases
the verb and object accuracies to 40.2% and 61.18%,
respectively.
Language Model(VE): The Language Model ap-
proach performs even worse than the vision baseline
especially for object identification. This is because
we consider the language model score directly for
the SVO triplet without any verb conjugations and
presence of determiners between the verb and ob-
ject. For example, while the GoogleNgram corpus
is likely to contain many instances of a sentence like
?A person is walking with a dog?, it will probably
not contain many instances of ?person walk dog?,
resulting in lower scores.
SVO Language Model(NVE): The SVO Lan-
guage Model (without verb expansion) improves
verb accuracy from 8.65% to 16.22%. For the WUP
metric, we see an improvement in accuracy in all
cases. This indicates that we are getting semanti-
cally closer to the right object compared to the ob-
ject predicted by the vision baseline.
SVO Language Model(VE): When used with
verb expansion, the SVO Language Model approach
results in a dramatic improvement in verb accu-
racy, causing it to jump to 36.76%. The WUP
score increase for verbs between SVO Language
Model(VE) and SVO Language Model(NVE) is mi-
nor, probably because even without verb expansion,
semantically similar verbs are selected but not the
one used in most human descriptions. So, the jump
in verb accuracy for the binary metric is much more
than the one for WUP.
Importance of verb expansion: Verb expansion
clearly improves activity accuracy. This idea could
be extended to a scenario where the test set contains
many activities for which we do not have any ex-
plicit training data. As such, we cannot train activ-
ity classifiers for these ?missing? classes. However,
we can train a ?coarse? activity classifier using the
training data that is available, get the top predictions
from this coarse classifier and then refine them by
using verb expansion. Thus, we can even detect and
describe activities that were unseen at training time
by using text-mined knowledge to determine the de-
scription of an activity that best fits the detected ob-
jects.
Effect of different training corpora: As men-
tioned earlier, we used a variety of textual cor-
pora. Since they cover newswire articles, web pages,
Wikipedia pages and neutral content, we compared
their individual effect on the accuracy of triplet se-
lection. The results of this ablation study are shown
in Tables 5 and 6 for the binary and WUP met-
ric respectively. We also show results for training
the SVO model on the descriptions of the training
videos. The WaCkypedia EN corpus gives us the
best overall results, probably because it covers a
wide variety of topics, unlike Gigaword which is re-
stricted to the news domain. Also, using our SVO
Language Model approach on the triplets from the
descriptions of the training videos is not sufficient.
This is because of the relatively small size and nar-
row domain of the training descriptions in compari-
son to the other textual corpora.
Effect of changing the weight of the NLP score
We experimented with different weights for the Vi-
sion and NLP scores (in Equation 1). These results
can be seen in Figure 6 for the binary-metric evalu-
ation. The WUP-metric evaluation graph is qualita-
tively similar. A general trend seems to be that the
subject and activity accuracies increase with increas-
ing weights of the NLP score. There is a significant
16
Method Subject% Verb% Object% All%
Vision Baseline 71.35 8.65 29.19 1.62
Train Desc. 85.95 16.22 16.22 8.65
Gigaword 85.95 32.43 20.00 14.05
BNC 85.95 17.30 29.73 14.59
ukWaC 85.95 34.05 32.97 22.16
WaCkypedia EN 85.95 35.14 40.00 28.11
All 85.95 36.76 33.51 23.78
Table 5: Effect of training corpus on SVO binary accu-
racy
Method Subject% Verb% Object% All%
Vision Baseline 87.76 40.20 61.18 63.05
Train Desc. 94.95 45.12 61.43 67.17
Gigaword 94.90 63.99 65.71 74.87
BNC 94.88 51.48 73.93 73.43
ukWaC 94.86 60.59 72.83 76.09
WaCkypedia EN 94.90 62.52 76.48 77.97
All 94.90 66.36 72.74 78.00
Table 6: Effect of training corpus on SVO WUP accuracy
improvement in verb accuracy as the NLP weight is
increased towards 1. However, for objects we notice
a slight increase in accuracy until the weight for the
NLP component is 0.9 after which there is a slight
dip. We hypothesize that this dip is caused by the
loss of vision-based information about the objects
which provide some guidance for the NLP system.
BLEU and METEOR results: From the results
in Table 4, it is clear that the sentences generated
by our approach outperform those generated by the
vision baseline, using both the BLEU and METEOR
evaluation metrics.
MTurk results: The Mechanical Turk results
show that human judges generally prefer our sys-
tem?s sentences to those of the vision baseline. As
previously seen, our method improves verbs far
more than it improves subjects or objects. We hy-
pothesize that the reason we do not achieve a simi-
larly large jump in performance in the MTurk evalu-
ation is because people seem to be more influenced
by the object than the verb when both options are
partially irrelevant. For example, in a video of a per-
son riding his bike onto the top of a car, our pro-
posed sentence was ?A person is a riding a motor-
bike? while the vision sentence was ?A person plays
Figure 6: Effect of increasing NLP weights (Binary met-
ric)
a car?, and most workers selected the vision sen-
tence.
Drawback of Using YouTube Videos: YouTube
videos often depict unusual and ?interesting? events,
and these might not agree with the statistics on typ-
ical SVOs mined from text corpora. For instance,
the last video in Figure 5 shows a person dragging a
cat on the floor. Since sentences describing people
moving or dragging cats around are not common in
text corpora, our system actually down-weights the
correct interpretation.
6 Conclusion
This paper has introduced a holistic data-driven
approach for generating natural-language descrip-
tions of short videos by identifying the best subject-
verb-object triplet for describing realistic YouTube
videos. By exploiting knowledge mined from large
corpora to determine the likelihood of various SVO
combinations, we improve the ability to select the
best triplet for describing a video and generate de-
scriptive sentences that are prefered by both au-
tomatic and human evaluation. From our experi-
ments, we see that linguistic knowledge significantly
improves activity detection, especially when train-
ing and test distributions are very different, one of
the advantages of our approach. Generating more
complex sentences with adjectives, adverbs, and
multiple objects and multi-sentential descriptions of
longer videos with multiple activities are areas for
future research.
17
7 Acknowledgements
This work was funded by NSF grant IIS1016312
and DARPA Minds Eye grant W911NF-10-2-0059.
Some of our experiments were run on the Mastodon
Cluster (NSF Grant EIA-0303609).
References
Bangalore, S. and Rambow, O. (2000), Exploiting
a probabilistic hierarchical model for generation,
in ?Proceedings of the 18th conference on Com-
putational linguistics-Volume 1?, Association for
Computational Linguistics, pp. 42?48.
Barbu, A., Bridge, A., Burchill, Z., Coroian, D.,
Dickinson, S., Fidler, S., Michaux, A., Mussman,
S., Narayanaswamy, S., Salvi, D. et al (2012),
Video in sentences out, in ?Proceedings of the
28th Conference on Uncertainty in Artificial In-
telligence (UAI)?, pp. 102?12.
Chang, C. and Lin, C. (2011), ?LIBSVM: a li-
brary for support vector machines?, ACM Trans-
actions on Intelligent Systems and Technology
(TIST) 2(3), 27.
Chen, D., Dolan, W., Raghavan, S., Huynh, T.,
Mooney, R., Blythe, J., Hobbs, J., Domingos, P.,
Kate, R., Garrette, D. et al (2010), ?Collecting
highly parallel data for paraphrase evaluation?,
Journal of Artificial Intelligence Research (JAIR)
37, 397?435.
Chen, S. and Goodman, J. (1999), ?An empirical
study of smoothing techniques for language mod-
eling?, Computer Speech & Language 13(4), 359?
393.
De Marneffe, M., MacCartney, B. and Manning, C.
(2006), Generating typed dependency parses from
phrase structure parses, in ?Proceedings of the In-
ternational Conference on Language Resources
and Evaluation (LREC)?, Vol. 6, pp. 449?454.
Ding, D., Metze, F., Rawat, S., Schulam, P., Burger,
S., Younessian, E., Bao, L., Christel, M. and
Hauptmann, A. (2012), Beyond audio and video
retrieval: towards multimedia summarization, in
?Proceedings of the 2nd ACM International Con-
ference on Multimedia Retrieval?.
Farhadi, A., Hejrati, M., Sadeghi, M., Young, P.,
Rashtchian, C., Hockenmaier, J. and Forsyth, D.
(2010), ?Every picture tells a story: Generating
sentences from images?, Computer Vision?
European Conference on Computer Vision
(ECCV) pp. 15?29.
Felzenszwalb, P., McAllester, D. and Ramanan,
D. (2008), A discriminatively trained, multi-
scale, deformable part model, in ?IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR)?, pp. 1?8.
Khan, M. and Gotoh, Y. (2012), ?Describing video
contents in natural language?, European Chapter
of the Association for Computational Linguistics
(EACL) .
Kojima, A., Tamura, T. and Fukunaga, K. (2002),
?Natural language description of human activities
from video images based on concept hierarchy of
actions?, International Journal of Computer Vi-
sion (IJCV) 50(2), 171?184.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi,
Y., Berg, A. and Berg, T. (2011), Baby talk:
Understanding and generating simple image de-
scriptions, in ?IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR)?, pp. 1601?
1608.
Kuznetsova, P., Ordonez, V., Berg, A. C., Berg,
T. L. and Choi, Y. (2012), Collective genera-
tion of natural image descriptions, in ?Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-
Volume 1?, Association for Computational Lin-
guistics, pp. 359?368.
Langkilde, I. and Knight, K. (1998), Generation that
exploits corpus-based statistical knowledge, in
?Proceedings of the 17th international conference
on Computational linguistics-Volume 1?, Associ-
ation for Computational Linguistics, pp. 704?710.
Laptev, I., Marszalek, M., Schmid, C. and Rozen-
feld, B. (2008), Learning realistic human actions
from movies, in ?IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)?, pp. 1?8.
Laptev, I. and Perez, P. (2007), Retrieving actions in
movies, in ?Proceedings of the 11th IEEE Interna-
tional Conference on Computer Vision (ICCV)?,
pp. 1?8.
Lee, M., Hakeem, A., Haering, N. and Zhu, S.
18
(2008), Save: A framework for semantic anno-
tation of visual events, in ?IEEE Computer Vision
and Pattern Recognition Workshops (CVPR-W)?,
pp. 1?8.
Li, L., Su, H., Xing, E. and Fei-Fei, L. (2010), ?Ob-
ject bank: A high-level image representation for
scene classification and semantic feature sparsifi-
cation?, Advances in Neural Information Process-
ing Systems (NIPS) 24.
Li, S., Kulkarni, G., Berg, T., Berg, A. and Choi,
Y. (2011), Composing simple image descriptions
using web-scale n-grams, in ?Proceedings of the
Fifteenth Conference on Computational Natural
Language Learning (CoNLL)?, Association for
Computational Linguistics (ACL), pp. 220?228.
Lin, Y., Michel, J., Aiden, E., Orwant, J., Brockman,
W. and Petrov, S. (2012), Syntactic annotations
for the google books ngram corpus, in ?Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL)?.
Motwani, T. and Mooney, R. (2012), Improving
video activity recognition using object recogni-
tion and text mining, in ?European Conference on
Artificial Intelligence (ECAI)?.
Ordonez, V., Kulkarni, G. and Berg, T. (2011),
Im2text: Describing images using 1 million
captioned photographs, in ?Proceedings of Ad-
vances in Neural Information Processing Systems
(NIPS)?.
Packer, B., Saenko, K. and Koller, D. (2012), A
combined pose, object, and feature model for ac-
tion understanding, in ?IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR)?,
pp. 1378?1385.
Pauls, A. and Klein, D. (2011), Faster and smaller
n-gram language models, in ?Proceedings of the
49th annual meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies?, Vol. 1, pp. 258?267.
Pedersen, T., Patwardhan, S. and Michelizzi, J.
(2004), Wordnet:: Similarity: measuring the re-
latedness of concepts, in ?Demonstration Papers
at Human Language Technologies-NAACL?, As-
sociation for Computational Linguistics, pp. 38?
41.
Reddy, K. and Shah, M. (2012), ?Recognizing 50
human action categories of web videos?, Machine
Vision and Applications pp. 1?11.
Schuldt, C., Laptev, I. and Caputo, B. (2004), Rec-
ognizing human actions: A local SVM approach,
in ?Proceedings of the 17th International Con-
ference on Pattern Recognition (ICPR)?, Vol. 3,
pp. 32?36.
Wang, H., Klaser, A., Schmid, C. and Liu, C.-L.
(2011), Action recognition by dense trajectories,
in ?IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)?, pp. 3169?3176.
Yang, Y., Teo, C. L., Daume?, III, H. and Aloimonos,
Y. (2011), Corpus-guided sentence generation of
natural images, in ?Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP)?, Association for Computa-
tional Linguistics, pp. 444?454.
Yao, B. and Fei-Fei, L. (2010), Modeling mutual
context of object and human pose in human-
object interaction activities, in ?IEEE Confer-
ence on Computer Vision and Pattern Recognition
(CVPR)?.
Yao, B., Yang, X., Lin, L., Lee, M. and Zhu, S.
(2010), ?I2t: Image parsing to text description?,
Proceedings of the IEEE 98(8), 1485?1508.
Zhang, J., Marsza?ek, M., Lazebnik, S. and Schmid,
C. (2007), ?Local features and kernels for classi-
fication of texture and object categories: A com-
prehensive study?, International Journal of Com-
puter Vision (IJCV) 73(2), 213?238.
19
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7?11,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Semantic Parsing using Distributional Semantics and Probabilistic Logic
Islam Beltagy
?
Katrin Erk
?
Raymond Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?
{beltagy,mooney}@cs.utexas.edu
?
katrin.erk@mail.utexas.edu
Abstract
We propose a new approach to semantic
parsing that is not constrained by a fixed
formal ontology and purely logical infer-
ence. Instead, we use distributional se-
mantics to generate only the relevant part
of an on-the-fly ontology. Sentences and
the on-the-fly ontology are represented in
probabilistic logic. For inference, we
use probabilistic logic frameworks like
Markov Logic Networks (MLN) and Prob-
abilistic Soft Logic (PSL). This seman-
tic parsing approach is evaluated on two
tasks, Textual Entitlement (RTE) and Tex-
tual Similarity (STS), both accomplished
using inference in probabilistic logic. Ex-
periments show the potential of the ap-
proach.
1 Introduction
Semantic Parsing is probably best defined as the
task of representing the meaning of a natural lan-
guage sentence in some formal knowledge repre-
sentation language that supports automated infer-
ence. A semantic parser is best defined as having
three parts, a formal language, an ontology, and an
inference mechanism. Both the formal language
(e.g. first-order logic) and the ontology define the
formal knowledge representation. The formal lan-
guage uses predicate symbols from the ontology,
and the ontology provides them with meanings by
defining the relations between them.
1
. A formal
expression by itself without an ontology is insuf-
ficient for semantic interpretation; we call it un-
interpreted logical form. An uninterpreted logical
form is not enough as a knowledge representation
1
For conciseness, here we use the term ?ontology? to refer
to a set of predicates as well as a knowledge base (KB) of
axioms that defines a complex set of relationships between
them
because the predicate symbols do not have mean-
ing in themselves, they get this meaning from the
ontology. Inference is what takes a problem repre-
sented in the formal knowledge representation and
the ontology and performs the target task (e.g. tex-
tual entailment, question answering, etc.).
Prior work in standard semantic parsing uses a
pre-defined set of predicates in a fixed ontology.
However, it is difficult to construct formal ontolo-
gies of properties and relations that have broad
coverage, and very difficult to do semantic parsing
based on such an ontology. Consequently, current
semantic parsers are mostly restricted to fairly lim-
ited domains, such as querying a specific database
(Kwiatkowski et al., 2013; Berant et al., 2013).
We propose a semantic parser that is not re-
stricted to a predefined ontology. Instead, we
use distributional semantics to generate the needed
part of an on-the-fly ontology. Distributional se-
mantics is a statistical technique that represents
the meaning of words and phrases as distributions
over context words (Turney and Pantel, 2010; Lan-
dauer and Dumais, 1997). Distributional infor-
mation can be used to predict semantic relations
like synonymy and hyponymy between words and
phrases of interest (Lenci and Benotto, 2012;
Kotlerman et al., 2010). The collection of pre-
dicted semantic relations is the ?on-the-fly ontol-
ogy? our semantic parser uses. A distributional
semantics is relatively easy to build from a large
corpus of raw text, and provides the wide cover-
age that formal ontologies lack.
The formal language we would like to use in the
semantic parser is first-order logic. However, dis-
tributional information is graded in nature, so the
on-the-fly ontology and its predicted semantic re-
lations are also graded. This means, that standard
first-order logic is insufficient because it is binary
by nature. Probabilistic logic solves this problem
because it accepts weighted first order logic for-
mulas. For example, in probabilistic logic, the
7
synonymy relation between ?man? and ?guy? is
represented by: ?x. man(x) ? guy(x) | w
1
and
the hyponymy relation between ?car? and ?vehi-
cle? is: ?x. car(x) ? vehicle(x) | w
2
where w
1
and w
1
are some certainty measure estimated from
the distributional semantics.
For inference, we use probabilistic logic
frameworks like Markov Logic Networks
(MLN) (Richardson and Domingos, 2006) and
Probabilistic Soft Logic (PSL) (Kimmig et al.,
2012). They are Statistical Relational Learning
(SRL) techniques (Getoor and Taskar, 2007) that
combine logical and statistical knowledge in one
uniform framework, and provide a mechanism for
coherent probabilistic inference. We implemented
this semantic parser (Beltagy et al., 2013; Beltagy
et al., 2014) and used it to perform two tasks
that require deep semantic analysis, Recognizing
Textual Entailment (RTE), and Semantic Textual
Similarity (STS).
The rest of the paper is organized as follows:
section 2 presents background material, section
3 explains the three components of the semantic
parser, section 4 shows how this semantic parser
can be used for RTE and STS tasks, section 5
presents the evaluation and 6 concludes.
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle ?graded? aspects of meaning in language
because they are binary by nature. Also, the logi-
cal predicates and relations do not have semantics
by themselves without an accompanying ontology,
which we want to replace in our semantic parser
with distributional semantics.
To map a sentence to logical form, we use Boxer
(Bos, 2008), a tool for wide-coverage semantic
analysis that produces uninterpreted logical forms
using Discourse Representation Structures (Kamp
and Reyle, 1993). It builds on the C&C CCG
parser (Clark and Curran, 2004).
2.2 Distributional Semantics
Distributional models use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Turney and Pantel,
2010; Mitchell and Lapata, 2010), based on the
observation that semantically similar words occur
in similar contexts (Landauer and Dumais, 1997;
Lund and Burgess, 1996). So words can be rep-
resented as vectors in high dimensional spaces
generated from the contexts in which they occur.
Distributional models capture the graded nature
of meaning, but do not adequately capture log-
ical structure (Grefenstette, 2013). It is possi-
ble to compute vector representations for larger
phrases compositionally from their parts (Lan-
dauer and Dumais, 1997; Mitchell and Lapata,
2008; Mitchell and Lapata, 2010; Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011). Distributional similarity is usually a mix-
ture of semantic relations, but particular asymmet-
ric similarity measures can, to a certain extent,
predict hypernymy and lexical entailment distri-
butionally (Lenci and Benotto, 2012; Kotlerman
et al., 2010).
2.3 Markov Logic Network
Markov Logic Network (MLN) (Richardson and
Domingos, 2006) is a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
MLNs define a probability distribution over possi-
ble worlds, where a world?s probability increases
exponentially with the total weight of the logical
clauses that it satisfies. A variety of inference
methods for MLNs have been developed, however,
their computational complexity is a fundamental
issue.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms have soft, continuous
truth values in the interval [0, 1] rather than bi-
nary truth values as used in MLNs and most other
probabilistic logics. Given a set of weighted in-
ference rules, and with the help of Lukasiewicz?s
relaxation of the logical operators, PSL builds a
graphical model defining a probability distribution
8
over the continuous space of values of the random
variables in the model. Then, PSL?s MPE infer-
ence (Most Probable Explanation) finds the over-
all interpretation with the maximum probability
given a set of evidence. It turns out that this op-
timization problem is second-order cone program
(SOCP) (Kimmig et al., 2012) and can be solved
efficiently in polynomial time.
2.5 Recognizing Textual Entailment
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or not related
(Neutral) to another, the hypothesis.
2.6 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system?s output and
gold standard scores.
3 Approach
A semantic parser is three components, a formal
language, an ontology, and an inference mecha-
nism. This section explains the details of these
components in our semantic parser. It also points
out the future work related to each part of the sys-
tem.
3.1 Formal Language: first-order logic
Natural sentences are mapped to logical form us-
ing Boxer (Bos, 2008), which maps the input
sentences into a lexically-based logical form, in
which the predicates are words in the sentence.
For example, the sentence ?A man is driving a car?
in logical form is:
?x, y, z. man(x) ? agent(y, x) ? drive(y) ?
patient(y, z) ? car(z)
We call Boxer?s output alone an uninterpreted
logical form because predicates do not have mean-
ing by themselves. They still need to be connected
with an ontology.
Future work: While Boxer has wide coverage,
additional linguistic phenomena like generalized
quantifiers need to be handled.
3.2 Ontology: on-the-fly ontology
Distributional information is used to generate the
needed part of an on-the-fly ontology for the given
input sentences. It is encoded in the form of
weighted inference rules describing the seman-
tic relations connecting words and phrases in the
input sentences. For example, for sentences ?A
man is driving a car?, and ?A guy is driving a
vehicle?, we would like to generate rules like
?x.man(x)? guy(x) |w
1
indicating that ?man?
and ?guy? are synonyms with some certainty w
1
,
and ?x. car(x)? vehicle(x) | w
2
indicating that
?car? is a hyponym of ?vehicle? with some cer-
tainty w
2
. Other semantic relations can also be
easily encoded as inference rules like antonyms
?x. tall(x)? ?short(x) |w, contextonymy rela-
tion ?x. hospital(x) ? ?y. doctor(y) | w. For
now, we generate inference rules only as syn-
onyms (Beltagy et al., 2013), but we are experi-
menting with more types of semantic relations.
In (Beltagy et al., 2013), we generate infer-
ence rules between all pairs of words and phrases.
Given two input sentences T and H , for all pairs
(a, b), where a and b are words or phrases of T
and H respectively, generate an inference rule:
a ? b | w, where the rule?s weight w =
sim(
??
a ,
??
b ), and sim is the cosine of the angle
between vectors
??
a and
??
b . Note that this simi-
larity measure cannot yet distinguish relations like
synonymy and hypernymy. Phrases are defined in
terms of Boxer?s output to be more than one unary
atom sharing the same variable like ?a little kid?
which in logic is little(k) ? kid(k), or two unary
atoms connected by a relation like ?a man is driv-
ing? which in logic is man(m) ? agent(d,m) ?
drive(d). We used vector addition (Mitchell and
Lapata, 2010) to calculate vectors for phrases.
Future Work: This can be extended in many
directions. We are currently experimenting with
asymmetric similarity functions to distinguish se-
mantic relations. We would also like to use longer
phrases and other compositionality techniques as
in (Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011). Also more inference rules can
be added from paraphrases collections like PPDB
(Ganitkevitch et al., 2013).
3.3 Inference: probabilistic logical inference
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
9
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E,RB).
Probabilistic logic frameworks define a proba-
bility distribution over all possible worlds. The
number of constants in a world depends on the
number of the discourse entities in the Boxer out-
put, plus additional constants introduced to han-
dle quantification. Mostly, all constants are com-
bined with all literals, except for rudimentary type
checking.
4 Tasks
This section explains how we perform the RTE
and STS tasks using our semantic parser.
4.1 Task 1: RTE using MLNs
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE?s classification prob-
lem for the relation between T and H , and given
the rule base RB generated as in 3.2, can be
split into two inference tasks. The first is find-
ing if T entails H , Pr(H|T,RB). The second
is finding if the negation of the text ?T entails H ,
Pr(H|?T,RB). In case Pr(H|T,RB) is high,
while Pr(H|?T,RB) is low, this indicates En-
tails. In case it is the other way around, this indi-
cates Contradicts. If both values are close to each
other, this means T does not affect probability of
H and that is an indication of Neutral. We train a
classifier to map the two values to the final classi-
fication decision.
Future Work: One general problem with
MLNs is its computational overhead especially
for the type of inference problems we have. The
other problem is that MLNs, as with most other
probabilistic logics, make the Domain Closure
Assumption (Richardson and Domingos, 2006)
which means that quantifiers sometimes behave in
an undesired way.
4.2 Task 2: STS using PSL
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach to compute similarity between struc-
tured objects. PSL does not work ?out of the
box? for STS, because Lukasiewicz?s equation for
the conjunction is very restrictive. We addressed
this problem (Beltagy et al., 2014) by replacing
SICK-RTE SICK-STS
dist 0.60 0.65
logic 0.71 0.68
logic+dist 0.73 0.70
Table 1: RTE accuracy and STS Correlation
Lukasiewicz?s equation for the conjunction with
an averaging equation, then change the optimiza-
tion problem and the grounding technique accord-
ingly.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where E = S
1
, Q = S
2
and
another where E = S
2
, Q = S
1
, and output the
two scores. The final similarity score is produced
from a regressor trained to map the two PSL scores
to the overall similarity score.
Future Work: Use a weighted average where
different weights are learned for different parts of
the sentence.
5 Evaluation
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014. The initial data
release for the competition consists of 5,000 pairs
of sentences which are annotated for both RTE and
STS. For this evaluation, we performed 10-fold
cross validation on this initial data.
Table 1 shows results comparing our full
approach (logic+dist) to two baselines, a
distributional-only baseline (dist) that uses vector
addition, and a probabilistic logic-only baseline
(logic) which is our semantic parser without distri-
butional inference rules. The integrated approach
(logic+dist) out-performs both baselines.
6 Conclusion
We presented an approach to semantic parsing that
has a wide-coverage for words and relations, and
does not require a fixed formal ontology. An
on-the-fly ontology of semantic relations between
predicates is derived from distributional informa-
tion and encoded in the form of soft inference rules
in probabilistic logic. We evaluated this approach
on two task, RTE and STS, using two probabilistic
logics, MLNs and PSL respectively. The semantic
parser can be extended in different direction, es-
pecially in predicting more complex semantic re-
lations, and enhancing the inference mechanisms.
10
Acknowledgments
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
L. Getoor and B. Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
11
