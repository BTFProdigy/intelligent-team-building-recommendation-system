Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 361?368,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Scaling Distributional Similarity to Large Corpora
James Gorman and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{jgorman2,james}@it.usyd.edu.au
Abstract
Accurately representing synonymy using
distributional similarity requires large vol-
umes of data to reliably represent infre-
quent words. However, the na??ve nearest-
neighbour approach to comparing context
vectors extracted from large corpora scales
poorly (O(n2) in the vocabulary size).
In this paper, we compare several existing
approaches to approximating the nearest-
neighbour search for distributional simi-
larity. We investigate the trade-off be-
tween efficiency and accuracy, and find
that SASH (Houle and Sakuma, 2005) pro-
vides the best balance.
1 Introduction
It is a general property of Machine Learning that
increasing the volume of training data increases
the accuracy of results. This is no more evident
than in Natural Language Processing (NLP), where
massive quantities of text are required to model
rare language events. Despite the rapid increase in
computational power available for NLP systems,
the volume of raw data available still outweighs
our ability to process it. Unsupervised learning,
which does not require the expensive and time-
consuming human annotation of data, offers an
opportunity to use this wealth of data. Curran
and Moens (2002) show that synonymy extraction
for lexical semantic resources using distributional
similarity produces continuing gains in accuracy
as the volume of input data increases.
Extracting synonymy relations using distribu-
tional similarity is based on the distributional hy-
pothesis that similar words appear in similar con-
texts. Terms are described by collating informa-
tion about their occurrence in a corpus into vec-
tors. These context vectors are then compared for
similarity. Existing approaches differ primarily in
their definition of ?context?, e.g. the surrounding
words or the entire document, and their choice of
distance metric for calculating similarity between
the context vectors representing each term.
Manual creation of lexical semantic resources
is open to the problems of bias, inconsistency and
limited coverage. It is difficult to account for the
needs of the many domains in which NLP tech-
niques are now being applied and for the rapid
change in language use. The assisted or auto-
matic creation and maintenance of these resources
would be of great advantage.
Finding synonyms using distributional similar-
ity requires a nearest-neighbour search over the
context vectors of each term. This is computation-
ally intensive, scaling to O(n2m) for the number
of terms n and the size of their context vectors m.
Increasing the volume of input data will increase
the size of both n and m, decreasing the efficiency
of a na??ve nearest-neighbour approach.
Many approaches to reduce this complexity
have been suggested. In this paper we evaluate
state-of-the-art techniques proposed to solve this
problem. We find that the Spatial Approximation
Sample Hierarchy (Houle and Sakuma, 2005) pro-
vides the best accuracy/efficiency trade-off.
2 Distributional Similarity
Measuring distributional similarity first requires
the extraction of context information for each of
the vocabulary terms from raw text. These terms
are then compared for similarity using a nearest-
neighbour search or clustering based on distance
calculations between the statistical descriptions of
their contexts.
361
2.1 Extraction
A context relation is defined as a tuple (w, r, w?)
where w is a term, which occurs in some grammat-
ical relation r with another word w? in some sen-
tence. We refer to the tuple (r, w?) as an attribute
of w. For example, (dog, direct-obj, walk) indicates
that dog was the direct object of walk in a sentence.
In our experiments context extraction begins
with a Maximum Entropy POS tagger and chun-
ker. The SEXTANT relation extractor (Grefen-
stette, 1994) produces context relations that are
then lemmatised. The relations for each term are
collected together and counted, producing a vector
of attributes and their frequencies in the corpus.
2.2 Measures and Weights
Both nearest-neighbour and cluster analysis meth-
ods require a distance measure to calculate the
similarity between context vectors. Curran (2004)
decomposes this into measure and weight func-
tions. The measure calculates the similarity
between two weighted context vectors and the
weight calculates the informativeness of each con-
text relation from the raw frequencies.
For these experiments we use the Jaccard (1)
measure and the TTest (2) weight functions, found
by Curran (2004) to have the best performance.
?
(r,w?) min(w(wm, r, w?), w(wn, r, w?))
?
(r,w?) max(w(wm, r, w?), w(wn, r, w?))
(1)
p(w, r, w?)? p(?, r, w?)p(w, ?, ?)
?
p(?, r, w?)p(w, ?, ?)
(2)
2.3 Nearest-neighbour Search
The simplest algorithm for finding synonyms is
a k-nearest-neighbour (k-NN) search, which in-
volves pair-wise vector comparison of the target
term with every term in the vocabulary. Given an
n term vocabulary and up to m attributes for each
term, the asymptotic time complexity of nearest-
neighbour search is O(n2m). This is very expen-
sive, with even a moderate vocabulary making the
use of huge datasets infeasible. Our largest exper-
iments used a vocabulary of over 184,000 words.
3 Dimensionality Reduction
Using a cut-off to remove low frequency terms
can significantly reduce the value of n. Unfortu-
nately, reducing m by eliminating low frequency
contexts has a significant impact on the quality of
the results. There are many techniques to reduce
dimensionality while avoiding this problem. The
simplest methods use feature selection techniques,
such as information gain, to remove the attributes
that are less informative. Other techniques smooth
the data while reducing dimensionality.
Latent Semantic Analysis (LSA, Landauer and
Dumais, 1997) is a smoothing and dimensional-
ity reduction technique based on the intuition that
the true dimensionality of data is latent in the sur-
face dimensionality. Landauer and Dumais admit
that, from a pragmatic perspective, the same effect
as LSA can be generated by using large volumes
of data with very long attribute vectors. Experi-
ments with LSA typically use attribute vectors of a
dimensionality of around 1000. Our experiments
have a dimensionality of 500,000 to 1,500,000.
Decompositions on data this size are computation-
ally difficult. Dimensionality reduction is often
used before using LSA to improve its scalability.
3.1 Heuristics
Another technique is to use an initial heuristic
comparison to reduce the number of full O(m)
vector comparisons that are performed. If the
heuristic comparison is sufficiently fast and a suffi-
cient number of full comparisons are avoided, the
cost of an additional check will be easily absorbed
by the savings made.
Curran and Moens (2002) introduces a vector of
canonical attributes (of bounded length k  m),
selected from the full vector, to represent the term.
These attributes are the most strongly weighted
verb attributes, chosen because they constrain the
semantics of the term more and partake in fewer
idiomatic collocations. If a pair of terms share at
least one canonical attribute then a full similarity
comparison is performed, otherwise the terms are
not compared. They show an 89% reduction in
search time, with only a 3.9% loss in accuracy.
There is a significant improvement in the com-
putational complexity. If a maximum of p posi-
tive results are returned, our complexity becomes
O(n2k + npm). When p  n, the system will
be faster as many fewer full comparisons will be
made, but at the cost of accuracy as more possibly
near results will be discarded out of hand.
4 Randomised Techniques
Conventional dimensionality reduction techniques
can be computationally expensive: a more scal-
362
able solution is required to handle the volumes of
data we propose to use. Randomised techniques
provide a possible solution to this.
We present two techniques that have been used
recently for distributional similarity: Random In-
dexing (Kanerva et al, 2000) and Locality Sensi-
tive Hashing (LSH, Broder, 1997).
4.1 Random Indexing
Random Indexing (RI) is a hashing technique
based on Sparse Distributed Memory (Kanerva,
1993). Karlgren and Sahlgren (2001) showed RI
produces results similar to LSA using the Test of
English as a Foreign Language (TOEFL) evalua-
tion. Sahlgren and Karlgren (2005) showed the
technique to be successful in generating bilingual
lexicons from parallel corpora.
In RI, we first allocate a d length index vec-
tor to each unique attribute. The vectors con-
sist of a large number of 0s and small number
(?) number of randomly distributed ?1s. Context
vectors, identifying terms, are generated by sum-
ming the index vectors of the attributes for each
non-unique context in which a term appears. The
context vector for a term t appearing in contexts
c1 = [1, 0, 0,?1] and c2 = [0, 1, 0,?1] would be
[1, 1, 0,?2]. The distance between these context
vectors is then measured using the cosine measure:
cos(?(u, v)) = ~u ? ~v|~u| |~v| (3)
This technique allows for incremental sampling,
where the index vector for an attribute is only gen-
erated when the attribute is encountered. Con-
struction complexity is O(nmd) and search com-
plexity is O(n2d).
4.2 Locality Sensitive Hashing
LSH is a probabilistic technique that allows the
approximation of a similarity function. Broder
(1997) proposed an approximation of the Jaccard
similarity function using min-wise independent
functions. Charikar (2002) proposed an approx-
imation of the cosine measure using random hy-
perplanes Ravichandran et al (2005) used this co-
sine variant and showed it to produce over 70%
accuracy in extracting synonyms when compared
against Pantel and Lin (2002).
Given we have n terms in an m? dimensional
space, we create d  m? unit random vectors also
of m? dimensions, labelled {~r1, ~r2, ..., ~rd}. Each
vector is created by sampling a Gaussian function
m? times, with a mean of 0 and a variance of 1.
For each term w we construct its bit signature
using the function
h~r(~w) =
{
1 : ~r. ~w ? 0
0 : ~r. ~w < 0
where ~r is a spherically symmetric random vector
of length d. The signature, w?, is the d length bit
vector:
w? = {h~r1(~w), h ~r2(~w), . . . , h ~rd(~w)}
The cost to build all n signatures is O(nm?d).
For terms u and v, Goemans and Williamson
(1995) approximate the angular similarity by
p(h~r(~u) = h~r(~v)) = 1?
?(~u, ~u)
pi (4)
where ?(~u, ~u) is the angle between ~u and ~u. The
angular similarity gives the cosine by
cos(?(~u, ~u)) =
cos((1 ? p(h~r(~u) = h~r(~v)))pi)
(5)
The probability can be derived from the Hamming
distance:
p(hr(u) = hr(v)) = 1 ?
H(u?, v?)
d (6)
By combining equations 5 and 6 we get the fol-
lowing approximation of the cosine distance:
cos(?(~u, ~u)) = cos
((H(u?, v?)
d
)
pi
)
(7)
That is, the cosine of two context vectors is ap-
proximated by the cosine of the Hamming distance
between their two signatures normalised by the
size of the signatures. Search is performed using
Equation 7 and scales to O(n2d).
5 Data Structures
The methods presented above fail to address the
n2 component of the search complexity. Many
data structures have been proposed that can be
used to address this problem in similarity search-
ing. We present three data structures: the vantage
point tree (VPT, Yianilos, 1993), which indexes
points in a metric space, Point Location in Equal
363
Balls (PLEB, Indyk and Motwani, 1998), a proba-
bilistic structure that uses the bit signatures gener-
ated by LSH, and the Spatial Approximation Sam-
ple Hierarchy (SASH, Houle and Sakuma, 2005),
which approximates a k-NN search.
Another option inspired by IR is attribute index-
ing (INDEX). In this technique, in addition to each
term having a reference to its attributes, each at-
tribute has a reference to the terms referencing it.
Each term is then only compared with the terms
with which it shares attributes. We will give a the-
oretically comparison against other techniques.
5.1 Vantage Point Tree
Metric space data structures provide a solution to
near-neighbour searches in very high dimensions.
These rely solely on the existence of a compari-
son function that satisfies the conditions of metri-
cality: non-negativity, equality, symmetry and the
triangle inequality.
VPT is typical of these structures and has been
used successfully in many applications. The VPT
is a binary tree designed for range searches. These
are searches limited to some distance from the tar-
get term but can be modified for k-NN search.
VPT is constructed recursively. Beginning with
a set of U terms, we take any term to be our van-
tage point p. This becomes our root. We now find
the median distance mp of all other terms to p:
mp = median{dist(p, u)|u ? U}. Those terms
u such that dist(p, u) ? mp are inserted into the
left sub-tree, and the remainder into the right sub-
tree. Each sub-tree is then constructed as a new
VPT, choosing a new vantage point from within its
terms, until all terms are exhausted.
Searching a VPT is also recursive. Given a term
q and radius r, we begin by measuring the distance
to the root term p. If dist(q, p) ? r we enter p into
our list of near terms. If dist(q, p) ? r ? mp we
enter the left sub-tree and if dist(q, p) + r > mp
we enter the right sub-tree. Both sub-trees may be
entered. The process is repeated for each entered
subtree, taking the vantage point of the sub-tree to
be the new root term.
To perform a k-NN search we use a back-
tracking decreasing radius search (Burkhard and
Keller, 1973). The search begins with r = ?,
and terms are added to a list of the closest k terms.
When the kth closest term is found, the radius is
set to the distance between this term and the tar-
get. Each time a new, closer element is added to
the list, the radius is updated to the distance from
the target to the new kth closest term.
Construction complexity is O(n log n). Search
complexity is claimed to be O(log n) for small ra-
dius searches. This does not hold for our decreas-
ing radius search, whose worst case complexity is
O(n).
5.2 Point Location in Equal Balls
PLEB is a randomised structure that uses the bit
signatures generated by LSH. It was used by
Ravichandran et al (2005) to improve the effi-
ciency of distributional similarity calculations.
Having generated our d length bit signatures for
each of our n terms, we take these signatures and
randomly permute the bits. Each vector has the
same permutation applied. This is equivalent to a
column reordering in a matrix where the rows are
the terms and the columns the bits. After applying
the permutation, the list of terms is sorted lexico-
graphically based on the bit signatures. The list is
scanned sequentially, and each term is compared
to its B nearest neighbours in the list. The choice
of B will effect the accuracy/efficiency trade-off,
and need not be related to the choice of k. This is
performed q times, using a different random per-
mutation function each time. After each iteration,
the current closest k terms are stored.
For a fixed d, the complexity for the permuta-
tion step is O(qn), the sorting O(qn log n) and the
search O(qBn).
5.3 Spatial Approximation Sample Hierarchy
SASH approximates a k-NN search by precomput-
ing some near neighbours for each node (terms in
our case). This produces multiple paths between
terms, allowing SASH to shape itself to the data
set (Houle, 2003). The following description is
adapted from Houle and Sakuma (2005).
The SASH is a directed, edge-weighted graph
with the following properties (see Figure 1):
? Each term corresponds to a unique node.
? The nodes are arranged into a hierarchy of
levels, with the bottom level containing n2
nodes and the top containing a single root
node. Each level, except the top, will contain
half as many nodes as the level below.
? Edges between nodes are linked to consecu-
tive levels. Each node will have at most p
parent nodes in the level above, and c child
nodes in the level below.
364
AB C D
E F G H
I J
K L
1
2
3
4
5
Figure 1: A SASH, where p = 2, c = 3 and k = 2
? Every node must have at least one parent so
that all nodes are reachable from the root.
Construction begins with the nodes being ran-
domly distributed between the levels. SASH is
then constructed iteratively by each node finding
its closest p parents in the level above. The par-
ent will keep the closest c of these children, form-
ing edges in the graph, and reject the rest. Any
nodes without parents after being rejected are then
assigned as children of the nearest node in the pre-
vious level with fewer than c children.
Searching is performed by finding the k nearest
nodes at each level, which are added to a set of
near nodes. To limit the search, only those nodes
whose parents were found to be nearest at the pre-
vious level are searched. The k closest nodes from
the set of near nodes are then returned. The search
complexity is O(ck log n).
In Figure 1, the filled nodes demonstrate a
search for the near-neighbours of some node q, us-
ing k = 2. Our search begins with the root node
A. As we are using k = 2, we must find the two
nearest children of A using our similarity measure.
In this case, C and D are closer than B. We now
find the closest two children of C and D. E is not
checked as it is only a child of B. All other nodes
are checked, including F and G, which are shared
as children by B and C . From this level we chose
G and H . The final levels are considered similarly.
At this point we now have the list of near nodes
A, C , D, G, H , I , J , K and L. From this we
chose the two nodes nearest q, H and I marked in
black, which are then returned.
k can be varied at each level to force a larger
number of elements to be tested at the base of the
SASH using, for instance, the equation:
ki = max{ k1?
h?i
log n , 12pc } (8)
This changes our search complexity to:
k1+
1
log n
k
1
log n?1
+ pc
2
2 log n (9)
We use this geometric function in our experiments.
Gorman and Curran (2005a; 2005b) found the
performance of SASH for distributional similarity
could be improved by replacing the initial random
ordering with a frequency based ordering. In ac-
cordance with Zipf?s law, the majority of terms
have low frequencies. Comparisons made with
these low frequency terms are unreliable (Curran
and Moens, 2002). Creating SASH with high fre-
quency terms near the root produces more reliable
initial paths, but comparisons against these terms
are more expensive.
The best accuracy/efficiency trade-off was
found when using more reliable initial paths rather
than the most reliable. This is done by folding the
data around some mean number of relations. For
each term, if its number of relations mi is greater
than some chosen number of relations M, it is
given a new ranking based on the score M2mi . Oth-
erwise its ranking based on its number of relations.
This has the effect of pushing very high and very
low frequency terms away from the root.
6 Evaluation Measures
The simplest method for evaluation is the direct
comparison of extracted synonyms with a manu-
ally created gold standard (Grefenstette, 1994). To
reduce the problem of limited coverage, our evalu-
ation combines three electronic thesauri: the Mac-
quarie, Roget?s and Moby thesauri.
We follow Curran (2004) and use two perfor-
mance measures: direct matches (DIRECT) and
inverse rank (INVR). DIRECT is the percentage
of returned synonyms found in the gold standard.
INVR is the sum of the inverse rank of each match-
ing synonym, e.g. matches at ranks 3, 5 and 28
365
CORPUS CUT-OFF TERMS AVERAGE
RELATIONS
PER TERM
BNC 0 246,067 43
5 88,926 116
100 14,862 617
LARGE 0 541,722 97
5 184,494 281
100 35,618 1,400
Table 1: Extracted Context Information
give an inverse rank score of 13 + 15 + 128 . With
at most 100 matching synonyms, the maximum
INVR is 5.187. This more fine grained as it in-
corporates the both the number of matches and
their ranking. The same 300 single word nouns
were used for evaluation as used by Curran (2004)
for his large scale evaluation. These were chosen
randomly from WordNet such that they covered
a range over the following properties: frequency,
number of senses, specificity and concreteness.
For each of these terms, the closest 100 terms and
their similarity scores were extracted.
7 Experiments
We use two corpora in our experiments: the
smaller is the non-speech portion of the British
National Corpus (BNC), 90 million words covering
a wide range of domains and formats; the larger
consists of the BNC, the Reuters Corpus Volume 1
and most of the English news holdings of the LDC
in 2003, representing over 2 billion words of text
(LARGE, Curran, 2004).
The semantic similarity system implemented by
Curran (2004) provides our baseline. This per-
forms a brute-force k-NN search (NAIVE). We
present results for the canonical attribute heuristic
(HEURISTIC), RI, LSH, PLEB, VPT and SASH.
We take the optimal canonical attribute vector
length of 30 for HEURISTIC from Curran (2004).
For SASH we take optimal values of p = 4 and c =
16 and use the folded ordering taking M = 1000
from Gorman and Curran (2005b).
For RI, LSH and PLEB we found optimal values
experimentally using the BNC. For LSH we chose
d = 3, 000 (LSH3,000) and 10, 000 (LSH10,000),
showing the effect of changing the dimensionality.
The frequency statistics were weighted using mu-
tual information, as in Ravichandran et al (2005):
log( p(w, r, w
?)
p(w, ?, ?)p(?, r, w?)) (10)
PLEB used the values q = 500 and B = 100.
CUT-OFF
5 100
NAIVE 1.72 1.71
HEURISTIC 1.65 1.66
RI 0.80 0.93
LSH10,000 1.26 1.31
SASH 1.73 1.71
Table 2: INVR vs frequency cut-off
The initial experiments on RI produced quite
poor results. The intuition was that this was
caused by the lack of smoothing in the algo-
rithm. Experiments were performed using the
weights given in Curran (2004). Of these, mu-
tual information (10), evaluated with an extra
log2(f(w, r, w?) + 1) factor and limited to posi-
tive values, produced the best results (RIMI). The
values d = 1000 and ? = 5 were found to produce
the best results.
All experiments were performed on 3.2GHz
Xeon P4 machines with 4GB of RAM.
8 Results
As the accuracy of comparisons between terms in-
creases with frequency (Curran, 2004), applying a
frequency cut-off will both reduce the size of the
vocabulary (n) and increase the average accuracy
of comparisons. Table 1 shows the reduction in
vocabulary and increase in average context rela-
tions per term as cut-off increases. For LARGE,
the initial 541,722 word vocabulary is reduced by
66% when a cut-off of 5 is applied and by 86%
when the cut-off is increased to 100. The average
number of relations increases from 97 to 1400.
The work by Curran (2004) largely uses a fre-
quency cut-off of 5. When this cut-off was used
with the randomised techniques RI and LSH, it pro-
duced quite poor results. When the cut-off was
increased to 100, as used by Ravichandran et al
(2005), the results improved significantly. Table 2
shows the INVR scores for our various techniques
using the BNC with cut-offs of 5 and 100.
Table 3 shows the results of a full thesaurus ex-
traction using the BNC and LARGE corpora using
a cut-off of 100. The average DIRECT score and
INVR are from the 300 test words. The total exe-
cution time is extrapolated from the average search
time of these test words and includes the setup
time. For LARGE, extraction using NAIVE takes
444 hours: over 18 days. If the 184,494 word vo-
cabulary were used, it would take over 7000 hours,
or nearly 300 days. This gives some indication of
366
BNC LARGE
DIRECT INVR Time DIRECT INVR Time
NAIVE 5.23 1.71 38.0hr 5.70 1.93 444.3hr
HEURISTIC 4.94 1.66 2.0hr 5.51 1.93 30.2hr
RI 2.97 0.93 0.4hr 2.42 0.85 1.9hr
RIMI 3.49 1.41 0.4hr 4.58 1.75 1.9hr
LSH3,000 2.00 0.76 0.7hr 2.92 1.07 3.6hr
LSH10,000 3.68 1.31 2.3hr 3.77 1.40 8.4hr
PLEB3,000 2.00 0.76 1.2hr 2.85 1.07 4.1hr
PLEB10,000 3.66 1.30 3.9hr 3.63 1.37 11.8hr
VPT 5.23 1.71 15.9hr 5.70 1.93 336.1hr
SASH 5.17 1.71 2.0hr 5.29 1.89 23.7hr
Table 3: Full thesaurus extraction
the scale of the problem.
The only technique to become less accurate
when the corpus size is increased is RI; it is likely
that RI is sensitive to high frequency, low informa-
tion contexts that are more prevalent in LARGE.
Weighting reduces this effect, improving accuracy.
The importance of the choice of d can be seen in
the results for LSH. While much slower, LSH10,000
is also much more accurate than LSH3,000, while
still being much faster than NAIVE. Introducing
the PLEB data structure does not improve the ef-
ficiency while incurring a small cost on accuracy.
We are not using large enough datasets to show the
improved time complexity using PLEB.
VPT is only slightly faster slightly faster than
NAIVE. This is not surprising in light of the origi-
nal design of the data structure: decreasing radius
search does not guarantee search efficiency.
A significant influence in the speed of the ran-
domised techniques, RI and LSH, is the fixed di-
mensionality. The randomised techniques use a
fixed length vector which is not influenced by the
size of m. The drawback of this is that the size of
the vector needs to be tuned to the dataset.
It would seem at first glance that HEURIS-
TIC and SASH provide very similar results, with
HEURISTIC slightly slower, but more accurate.
This misses the difference in time complexity be-
tween the methods: HEURISTIC is n2 and SASH
n log n. The improvement in execution time over
NAIVE decreases as corpus size increases and this
would be expected to continue. Further tuning of
SASH parameters may improve its accuracy.
RIMI produces similar result using LARGE to
SASH using BNC. This does not include the cost
of extracting context relations from the raw text, so
the true comparison is much worse. SASH allows
the free use of weight and measure functions, but
RI is constrained by having to transform any con-
text space into a RI space. This is important when
LARGE
CUT-OFF 0 5 100
NAIVE 541,721 184,493 35,617
SASH 10,599 8,796 6,231
INDEX 5,844 13,187 32,663
Table 4: Average number of comparisons per term
considering that different tasks may require differ-
ent weights and measures (Weeds and Weir, 2005).
RI also suffers n2 complexity, where as SASH is
n log n. Taking these into account, and that the im-
provements are barely significant, SASH is a better
choice.
The results for LSH are disappointing. It per-
forms consistently worse than the other methods
except VPT. This could be improved by using
larger bit vectors, but there is a limit to the size of
these as they represent a significant memory over-
head, particularly as the vocabulary increases.
Table 4 presents the theoretical analysis of at-
tribute indexing. The average number of com-
parisons made for various cut-offs of LARGE are
shown. NAIVE and INDEX are the actual values
for those techniques. The values for SASH are
worst case, where the maximum number of terms
are compared at each level. The actual number
of comparisons made will be much less. The ef-
ficiency of INDEX is sensitive to the density of
attributes and increasing the cut-off increases the
density. This is seen in the dramatic drop in per-
formance as the cut-off increases. This problem of
density will increase as volume of raw input data
increases, further reducing its effectiveness. SASH
is only dependent on the number of terms, not the
density.
Where the need for computationally efficiency
out-weighs the need for accuracy, RIMI provides
better results. SASH is the most balanced of the
techniques tested and provides the most scalable,
high quality results.
367
9 Conclusion
We have evaluated several state-of-the-art tech-
niques for improving the efficiency of distribu-
tional similarity measurements. We found that,
in terms of raw efficiency, Random Indexing (RI)
was significantly faster than any other technique,
but at the cost of accuracy. Even after our mod-
ifications to the RI algorithm to significantly im-
prove its accuracy, SASH still provides a better ac-
curacy/efficiency trade-off. This is more evident
when considering the time to extract context in-
formation from the raw text. SASH, unlike RI, also
allows us to choose both the weight and the mea-
sure used. LSH and PLEB could not match either
the efficiency of RI or the accuracy of SASH.
We intend to use this knowledge to process even
larger corpora to produce more accurate results.
Having set out to improve the efficiency of dis-
tributional similarity searches while limiting any
loss in accuracy, we are producing full nearest-
neighbour searches 18 times faster, with only a 2%
loss in accuracy.
Acknowledgements
We would like to thank our reviewers for their
helpful feedback and corrections. This work has
been supported by the Australian Research Coun-
cil under Discovery Project DP0453131.
References
Andrei Broder. 1997. On the resemblance and containment
of documents. In Proceedings of the Compression and
Complexity of Sequences, pages 21?29, Salerno, Italy.
Walter A. Burkhard and Robert M. Keller. 1973. Some ap-
proaches to best-match file searching. Communications of
the ACM, 16(4):230?236, April.
Moses S. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of the 34th
Annual ACM Symposium on Theory of Computing, pages
380?388, Montreal, Quebec, Canada, 19?21 May.
James Curran and Marc Moens. 2002. Improvements in au-
tomatic thesaurus extraction. In Proceedings of the Work-
shop of the ACL Special Interest Group on the Lexicon,
pages 59?66, Philadelphia, PA, USA, 12 July.
James Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum cut
and satisfiability problems using semidefinite program-
ming. Journal of Association for Computing Machinery,
42(6):1115?1145, November.
James Gorman and James Curran. 2005a. Approximate
searching for distributional similarity. In ACL-SIGLEX
2005 Workshop on Deep Lexical Acquisition, Ann Arbor,
MI, USA, 30 June.
James Gorman and James Curran. 2005b. Augmenting ap-
proximate similarity searching with lexical information.
In Australasian Language Technology Workshop, Sydney,
Australia, 9?11 November.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston.
Michael E. Houle and Jun Sakuma. 2005. Fast approximate
similarity search in extremely high-dimensional data sets.
In Proceedings of the 21st International Conference on
Data Engineering, pages 619?630, Tokyo, Japan.
Michael E. Houle. 2003. Navigating massive data sets via
local clustering. In Proceedings of the 9th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 547?552, Washington, DC, USA.
Piotr Indyk and Rajeev Motwani. 1998. Approximate near-
est neighbors: towards removing the curse of dimension-
ality. In Proceedings of the 30th annual ACM Symposium
on Theory of Computing, pages 604?613, New York, NY,
USA, 24?26 May. ACM Press.
Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000.
Random indexing of text samples for latent semantic anal-
ysis. In Proceedings of the 22nd Annual Conference of the
Cognitive Science Society, page 1036, Mahwah, NJ, USA.
Pentti Kanerva. 1993. Sparse distributed memory and re-
lated models. In M.H. Hassoun, editor, Associative Neu-
ral Memories: Theory and Implementation, pages 50?76.
Oxford University Press, New York, NY, USA.
Jussi Karlgren and Magnus Sahlgren. 2001. From words to
understanding. In Y. Uesaka, P. Kanerva, and H Asoh, ed-
itors, Foundations of Real-World Intelligence, pages 294?
308. CSLI Publications, Stanford, CA, USA.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240, April.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD-02,
pages 613?619, 23?26 July.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and NLP: Using locality
sensitive hash functions for high speed noun clustering.
In Proceedings of the 43rd Annual Meeting of the ACL,
pages 622?629, Ann Arbor, USA.
Mangus Sahlgren and Jussi Karlgren. 2005. Automatic bilin-
gual lexicon acquisition using random indexing of parallel
corpora. Journal of Natural Language Engineering, Spe-
cial Issue on Parallel Texts, 11(3), June.
Julie Weeds and David Weir. 2005. Co-occurance retrieval:
A flexible framework for lexical distributional similarity.
Computational Linguistics, 31(4):439?475, December.
Peter N. Yianilos. 1993. Data structures and algorithms for
nearest neighbor search in general metric spaces. In Pro-
ceedings of the fourth annual ACM-SIAM Symposium on
Discrete algorithms, pages 311?321, Philadelphia.
368
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 97?104,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Approximate Searching for Distributional Similarity
James Gorman and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{jgorman2,james}@it.usyd.edu.au
Abstract
Distributional similarity requires large
volumes of data to accurately represent
infrequent words. However, the nearest-
neighbour approach to finding synonyms
suffers from poor scalability. The Spa-
tial Approximation Sample Hierarchy
(SASH), proposed by Houle (2003b), is
a data structure for approximate nearest-
neighbour queries that balances the effi-
ciency/approximation trade-off. We have
intergrated this into an existing distribu-
tional similarity system, tripling efficiency
with a minor accuracy penalty.
1 Introduction
With the development of WordNet (Fellbaum, 1998)
and large electronic thesauri, information from lex-
ical semantic resources is regularly used to solve
NLP problems. These problems include collocation
discovery (Pearce, 2001), smoothing and estimation
(Brown et al, 1992; Clark and Weir, 2001) and ques-
tion answering (Pasca and Harabagiu, 2001).
Unfortunately, these resources are expensive and
time-consuming to create manually, and tend to suf-
fer from problems of bias, inconsistency, and limited
coverage. In addition, lexicographers cannot keep
up with constantly evolving language use and can-
not afford to build new resources for the many sub-
domains that NLP techniques are being applied to.
There is a clear need for methods to extract lexical
semantic resources automatically or tools that assist
in their manual creation and maintenance.
Much of the existing work on automatically ex-
tracting resources is based on the distributional hy-
pothesis that similar words appear in similar con-
texts. Existing approaches differ primarily in their
definition of ?context?, e.g. the surrounding words
or the entire document, and their choice of distance
metric for calculating similarity between the vector
of contexts representing each term. Finding syn-
onyms using distributional similarity involves per-
forming a nearest-neighbour search over the context
vectors for each term. This is very computation-
ally intensive and scales according to the vocabulary
size and the number of contexts for each term. Cur-
ran and Moens (2002b) have demonstrated that dra-
matically increasing the quantity of text used to ex-
tract contexts significantly improves synonym qual-
ity. Unfortunately, this also increases the vocabulary
size and the number of contexts for each term, mak-
ing the use of huge datasets infeasible.
There have been many data structures and ap-
proximation algorithms proposed to reduce the com-
putational complexity of nearest-neighbour search
(Cha?vez et al, 2001). Many of these approaches re-
duce the search space by using clustering techniques
to generate an index of near-neighbours. We use the
Spacial Approximation Sample Hierarchy (SASH)
data structure developed by Houle (2003b) as it al-
lows more control over the efficiency-approximation
trade-off than other approximation methods.
This paper describes integrating the SASH into
an existing distributional similarity system (Cur-
ran, 2004). We show that replacing the nearest-
neighbour search improves efficiency by a factor of
three with only a minor accuracy penalty.
97
2 Distributional Similarity
Distributional similarity systems can be separated
into two components. The first component extracts
the contexts from raw text and compiles them into a
statistical description of the contexts each term ap-
pears in. The second component performs nearest-
neighbour search or clustering to determine which
terms are similar, based on distance calculations be-
tween their context vectors. The approach used in
this paper follows Curran (2004).
2.1 Extraction Method
A context relation is defined as a tuple (w, r,w?)
where w is a term, which occurs in some grammati-
cal relation r with another word w? in some sentence.
We refer to the tuple (r,w?) as an attribute of w. For
example, (dog, diect-obj, walk) indicates that dog
was the direct object of walk in a sentence.
Context extraction begins with a Maximum En-
tropy POS tagger and chunker (Ratnaparkhi, 1996).
The Grefenstette (1994) relation extractor produces
context relations that are then lemmatised using the
Minnen et al (2000) morphological analyser. The
relations for each term are collected together and
counted, producing a context vector of attributes and
their frequencies in the corpus.
2.2 Measures and Weights
Both nearest-neighbour and cluster analysis meth-
ods require a distance measure that calculates the
similarity between context vectors. Curran (2004)
decomposes this measure into measure and weight
functions. The measure function calculates the sim-
ilarity between two weighted context vectors and the
weight function calculates a weight from the raw fre-
quency information for each context relation.
The SASH requires a distance measure that pre-
serves metric space (see Section 4.1). For these ex-
periments we use the JACCARD (1) measure and the
TTEST (2) weight, as Curran and Moens (2002a)
found them to have the best performance in their
comparison of many distance measures.
?
(r,w?) min(wgt(wm, ?r, ?w?),wgt(wn, ?r, ?w?))
?
(r,w?) max(wgt(wm, ?r, ?w?),wgt(wn, ?r, ?w?))
(1)
p(w, r,w?) ? p(?, r,w?)p(w, ?, ?)
?
p(?, r,w?)p(w, ?, ?)
(2)
3 Nearest-neighbour search
The simplest algorithm for finding synonyms is
nearest-neighbour search, which involves pairwise
vector comparison of the target term with every term
in the vocabulary. Given an n term vocabulary and
up to m attributes for each term, the asymptotic time
complexity of nearest-neighbour search is O(n2m).
This is very expensive with even a moderate vocab-
ulary and small attribute vectors making the use of
huge datasets infeasible.
3.1 Heuristic
Using cutoff to remove low frequency terms can sig-
nificantly reduce the value of n. In these experi-
ments, we used a cutoff of 5. However, a solution
is still needed to reduce the factor m. Unfortunately,
reducing m by eliminating low frequency contexts
has a significant impact on the quality of the results.
Curran and Moens (2002a) propose an initial
heuristic comparison to reduce the number of
full O(m) vector comparisons. They introduce a
bounded vector (length k) of canonical attributes,
selected from the full vector, to represent the
term. The selected attributes are the most strongly
weighted verb attributes: Curran and Moens chose
these relations as they generally constrain the se-
mantics of the term more and partake in fewer id-
iomatic collocations.
If a pair of terms share at least one canonical
attribute then a full similarity comparison is per-
formed, otherwise the terms are not considered sim-
ilar. If a maximum of p positive results are returned,
our complexity becomes O(n2k+npm), which, since
k is constant, is O(n2 + npm).
4 The SASH
The SASH approximates a nearest-neighbour search
by pre-computing some of the near-neighbours of
each node (terms in our case). It is arranged as a
multi-leveled pyramid, where each node is linked
to its (approximate) near-neighbours on the levels
above and below. This produces multiple paths be-
tween nodes, allowing the SASH to shape itself to
the data set (Houle, 2003a). This graph is searched
by finding the near-neighbours of the target node
at each level. The following description is adapted
from Houle (2003b).
98
AB C D
E F G H
I J
K L
1
2
3
4
5
Figure 1: A SASH, where p = 2, c = 3 and k = 2
4.1 Metric Spaces
The SASH organises nodes that can be measured in
metric space. Although it is not necessary for the
SASH to work, only in this space can performance
be guaranteed. Our meaures produce a metric-like
space for the terms derived from large datasets.
A domain D is a metric space if there exists a
function dist : D ? D ? R?0 such that:
1. dist(p, q) ? 0 ? p, q ? D (non-negativity)
2. dist(p, q) = 0 iff p = q ? p, q ? D (equality)
3. dist(p, q) = dist(q, p) ? p, q ? D (symmetry)
4. dist(p, q) + dist(q, r) ? dist(p, r)
? p, q, r ? D (triangle inequality)
We invert the similarity measure to produce a dis-
tance, resulting in condition 2 not being satisfied
since dist(p, p) = x, x > 0. For most measures x
is constant, so dist(p, q) > dist(p, p) if p , q and p
and q do not occur in exactly the same contexts. For
some measures, e.g. DICE, dist(p, p) > dist(p, q),
that is, p is closer to q than it is to itself. These do
not preserve metric space in any way, so cannot be
used with the SASH.
Cha?vez et al (2001) divides condition 2 into:
5. dist(p, p) = 0 ? p ? D (reflexivity)
6. dist(p, q) > 0 iff p , q ? p, q ? D
(strict positiveness)
If strict positiveness is not satisfied the space is
called pseudometric. In theory, our measures do not
satisfy this condition, however in practice most large
datasets will satisfy this condition.
4.2 Structure
The SASH is a directed, edge-weighted graph with
the following properties:
? Each term corresponds to a unique node.
? The nodes are arranged into a hierarchy of lev-
els, with the bottom level containing n2 nodes
and the top containing a single root node. Each
level, except the top, will contain half as many
nodes as the level below. These are numbered
from 1 (top) to h.
? Edges between nodes are linked from consecu-
tive levels. Each node will have at most p par-
ent nodes in the level above, and c child nodes
in the level below.
? Every node must have at least one parent so that
all nodes are reachable from the root.
Figure 1 shows a SASH which will be used below.
4.3 Construction
The SASH is constructed iteratively by finding the
nearest parents in the level above. The nodes are
first randomly distributed to reduce any clustering
effects. They are then split into the levels described
above, with level h having n2 nodes, level 2 at most c
nodes and level 1 having a single root node.
The root node has all nodes at level 2 as children
and each node at level 2 has the root as its sole par-
ent. Then for each node in each level i from 3 to
h, we find the set of p nearest parent nodes in level
(i ? 1). The node then asks that parent if it can be
a child. As only the closest c nodes can be children
of a node, it may be the case that a requested parent
rejects a child.
99
DIST c LOAD TIME
RANDOM 16 21.0hr
RANDOM 64 15.6hr
RANDOM 128 21.1hr
FOLD1500 16 50.2hr
FOLD1500 64 33.4hr
FOLD1500 128 25.7hr
SORT 16 75.5hr
SORT 64 23.8hr
SORT 128 33.8hr
Table 1: Load time distributions and values of c
If a child is left without any parents it is said to be
orphaned. Any orphaned nodes must now find the
closest node in the above level that has fewer than
c children. Once all nodes have at least one parent,
we move to the next level. This proceeds iteratively
through the levels.
4.4 Search
Searching the SASH is also performed iteratively. To
find the k nearest neighbours of a node q, we first
find the k nearest neighbours at each level. At level 1
we take the single root node to be nearest. Then, for
each level after, we find the k nearest unique children
of the nodes found in the level above. When the
last level has been searched, we return the closest k
nodes from all the sets of near neighbours returned.
In Figure 1, the filled nodes demonstrate a search
for the near-neighbours of some node q, using k = 2.
Our search begins with the root node A. As we are
using k = 2, we must find the two nearest children of
A using our similarity measure. In this case, C and
D are closer than B. We now find the closest two
children of C and D. E is not checked as it is only
a child of B. All other nodes are checked, including
F and G, which are shared as children by B and C.
From this level we chose G and H. We then consider
the fourth and fifth levels similarly.
At this point we now have the list of near nodes
A, C, D, G, H, I, J, K and L. From this we chose
the two nodes closest to q: H and I marked in black.
These are returned as the near-neighbours of q.
k can also be varied at each level to force a larger
number of elements to be tested at the base of the
SASH using, for instance, the equation:
ki = max{ k1?
h?i
log2 n ,
1
2
pc } (3)
We use this geometric function in our experiments.
4.5 Complexity
When measuring the time complexity, we consider
the number of distance measurements as these dom-
inate the computation. If we do not consider the
problem of assigning parents to orphans, for n
nodes, p parents per child, at most c children per
parent and a search returning k elements, the loose
upper bounds are:
SASH construction
pcn log2 n (4)
Approx. k-NN query (uniform)
ck log2 n (5)
Approx. k-NN query (geometric)
k1+
1
log2 n
k
1
log2 n
?1
+
pc2
2
log2 n (6)
Since the average number of children per node is
approximately 2p, practical complexities can be de-
rived using c = 2p.
In Houle?s experiments, typically less than 5% of
computation time was spent assigning parents to or-
phans, even for relatively small c. In some of our
experiments we found that low values of c produced
significantly worse load times that for higher values,
but this was highly dependant on the distribution of
nodes. Table 1 shows this with respect to several
distributions and values of c.
5 Evaluation
The simplest method of evaluation is direct com-
parison of the extracted synonyms with a manually-
created gold standard (Grefenstette, 1994). How-
ever, on small corpora, rare direct matches provide
limited information for evaluation, and thesaurus
coverage is a problem. Our evaluation uses a com-
bination of three electronic thesauri: the Macquarie
(Bernard, 1990), Roget?s (Roget, 1911) and Moby
(Ward, 1996) thesauri.
100
With this gold standard in place, it is possible
to use precision and recall measures to evaluate the
quality of the extracted thesaurus. To help overcome
the problems of direct comparisons we use several
measures of system performance: direct matches
(DIRECT), inverse rank (INVR), and precision of the
top n synonyms (P(n)), for n = 1, 5 and 10.
INVR is the sum of the inverse rank of each
matching synonym, e.g. matching synonyms at
ranks 3, 5 and 28 give an inverse rank score of
1
3 +
1
5 +
1
28 , and with at most 100 synonyms, the max-
imum INVR score is 5.187. Precision of the top n is
the percentage of matching synonyms in the top n
extracted synonyms.
The same 70 single-word nouns were used for the
evaluation as in Curran and Moens (2002a). These
were chosen randomly from WordNet such that they
covered a range over the following properties:
frequency Penn Treebank and BNC frequencies;
number of senses WordNet and Macquarie senses;
specificity depth in the WordNet hierarchy;
concreteness distribution across WordNet subtrees.
For each of these terms, the closest 100 terms and
their similarity score were extracted.
6 Experiments
The contexts were extracted from the non-speech
portion of the British National Corpus (Burnard,
1995). All experiments used the JACCARD measure
function, the TTEST weight function and a cutoff
frequency of 5. The SASH was constructed using the
geometric equation for ki described in Section 4.4.
When the heuristic was applied, the TTESTLOG
weight function was used with a canonical set size
of 100 and a maximum frequency cutoff of 10,000.
The values 4?16, 8?32, 16?64, and 32?128 were
chosen for p and c. This gives a range of branch-
ing factors to test the balance between sparseness,
where there is potential for erroneous fragmentation
of large clusters, and bushiness, where more tests
must be made to find near children. The c = 4p re-
lationship is derived from the simple hashing rule
of thumb that says that a hash table should have
roughly twice the size required to store all its ele-
ments (Houle, 2003b).
DIST FREQUENCY # RELATIONS
Mean Median Mean Median
RANDOM 342 18 126 13
FOLD500 915 865.5 500 500
FOLD1000 2155 1970.5 1001 1001.5
FOLD1500 3656 3444 1506 1510.5
SORT 44753 37937.5 8290 7583.5
Table 2: Top 3 SASH level averages with c = 128
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0  1000  2000  3000  4000  5000  6000  7000
In
vR
Avg Search Time (ms)
random
fold1500
sort
Figure 2: INVR against average search time
Our initial experiments showed that the random
distribution of nodes (RANDOM) in SASH construc-
tion caused the nearest-neighbour approximation to
be very inaccurate for distributional similarity. Al-
though the speed was improved by two orders of
magnitude when c = 16, it achieved only 13% of the
INVR of the na??ve implementation. The best RAN-
DOM result was less than three times faster then the
na??ve solution and only 60% INVR.
In accordance with Zipf?s law the majority of
terms have very low frequencies. Similarity mea-
surements made against these low frequency terms
are less reliable, as accuracy increases with the num-
ber of relations and their frequencies (Curran and
Moens, 2002b). This led to the idea that ordering
the nodes by frequency before generating the SASH
would improve accuracy.
The SASH was then generated with the highest
frequency terms were near the root so that the initial
search paths would be more accurate. This has the
unfortunate side-effect of slowing search by up to
four times because comparisons with high frequency
terms take longer than with low frequency terms as
they have a larger number of relations.
101
DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME
NAIVE 2.83 49% 41% 32% 1.43 12217ms
RANDOM 16 0.17 9% 6% 3% 0.18 13% 120ms
RANDOM 64 1.09 30% 21% 15% 0.72 50% 1388ms
RANDOM 128 1.53 31% 24% 20% 0.86 60% 4488ms
SORT 16 1.51 33% 25% 20% 0.90 63% 490ms
SORT 64 2.55 47% 38% 31% 1.34 94% 2197ms
SORT 128 2.81 49% 41% 33% 1.43 100% 6960ms
Table 3: Evaluation of different random and fully sorted distributions
This led to updating our original frequency order-
ing idea by recognising that we did not need the most
accurately comparable terms at the top of the SASH,
only more accurately comparable terms than those
randomly selected.
As a first attempt, we constructed SASHs with fre-
quency orderings that were folded about a chosen
number of relations M. For each term, if its num-
ber of relations mi was greater than M, it was given
a new ranking based on the score M2
mi
. In this way,
very high and very low frequency terms were pushed
away from the root. The folding points this was
tested for were 500, 1000 and 1500. There are many
other node organising schemes we are yet to explore.
The frequency distributions over the top three lev-
els for each ordering scheme are shown in Table 2.
Zipf?s law results in a large difference between the
mean and median frequency values in the RANDOM
results: most of the nodes have low frequency, but
some high frequency results push the mean up. The
four-fold reduction in efficiency for SORT (see Ta-
ble 3) is a result of the mean number of relations
being over 65 times that of RANDOM.
Experiments covering the full set of permutations
of these parameters were run, with and without the
heuristic applied. In the cases where the heuristic
rejected pairs of terms, the SASH treated the rejected
pairs as being as infinitely far apart. In addition, the
brute force solutions were generated with (NAIVE
HEURISTIC) and without (NAIVE) the heuristic.
We have assumed that all weights and measures
introduce similar distribution properties into the
SASH, so that the best weight and measure when per-
forming a brute-force search will also produce the
best results when combined with the SASH. Future
experiments will explore SASH behaviour with other
similarity measures.
7 Results
Table 3 presents the results for the initial experi-
ments. SORT was consistently more accurate than
RANDOM, and when c = 128, performed as well as
NAIVE for all evaluation measures except for direct
matches. Both SASH solutions outperformed NAIVE
in efficiency.
The trade-off between efficiency and approxima-
tion accuracy is evident in these results. The most
efficient result is 100 times faster than NAIVE, but
only 13% accurate on INVR, with 6% of direct
matches. The most accurate result is 100% accu-
rate on INVR, with 99% of direct matches, but is
less than twice as fast.
Table 4 shows the trade-off for folded distribu-
tions. The least accurate FOLD500 result is 30%
accurate but 50 times faster than NAIVE, while the
most accurate is 87% but less than two times faster.
The least accurate FOLD1500 result is 43% accurate
but 71 times faster than NAIVE, while the most ac-
curate is 101% and two and half times faster. These
results show the impact of moving high frequency
terms away from the root.
Figure 2 plots the trade-off using search time and
INVR at c = 16, 32, 64 and 128. For c = 16 every
SASH has very poor accuracy. By c = 64 their ac-
curacy has improved dramatically, but their search
time also increased somewhat. At c = 128, there
is only a small improvement in accuracy, coinciding
with a large increase in search time. The best trade-
off between efficiency and approximation accuracy
occurs at the knee of the curve where c = 64.
When c = 128 both SORT and FOLD1500 perform
as well as, or slightly outperform NAIVE on some
evaluation measures. These evaluation measures in-
volve the rank of correct synonyms, so if the SASH
102
DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME
FOLD500 16 0.53 23% 11% 8% 0.43 30% 243ms
FOLD500 64 1.69 49% 29% 23% 1.09 76% 2880ms
FOLD500 128 2.29 50% 35% 27% 1.25 87% 6848ms
FOLD1000 16 0.61 29% 14% 9% 0.51 35% 228ms
FOLD1000 64 2.07 49% 36% 26% 1.21 84% 3192ms
FOLD1000 128 2.57 50% 39% 31% 1.40 98% 4330ms
FOLD1500 16 0.90 30% 17% 13% 0.62 43% 171ms
FOLD1500 64 2.36 57% 39% 30% 1.36 95% 3193ms
FOLD1500 128 2.67 53% 42% 32% 1.44 101% 4739ms
Table 4: Evaluation of folded distributions
approximation was to fail to find some incorrectly
proposed synonyms ranked above some other cor-
rect synonyms, those correct synonyms would have
their ranking pushed up. In this way, the approxima-
tion can potentially outperform the original nearest-
neighbour algorithm.
From Tables 3 and 4 we also see that as the value
of c increases, so does the accuracy across all of
the experiments. This is because as c increases the
number of paths between nodes increases and we
have a solution closer to a true nearest-neighbour
search, that is, there are more ways of finding the
true nearest-neighbour nodes.
Table 5 presents the results of combining the
canonical attributes heuristic (see Section 3.1) with
the SASH approximation. This NAIVE HEURISTIC is
14 times faster than NAIVE and 97% accurate, with
96% of direct matches. The combination has com-
parable accuracy and is much more efficient than the
best of the SASH solutions. The best heuristic SASH
results used the SORT ordering with c = 16, which
was 37 times faster than NAIVE and 2.5 times faster
than NAIVE HEURISTIC. Its performance was statis-
tically indistinguishable from NAIVE HEURISTIC.
Using the heuristic changes the impact of the
number of children c on the SASH performance char-
acteristics. It seems that beyond c = 16 the only
significant effect is to reduce the efficiency (often to
slower than NAIVE HEURISTIC).
The heuristic interacts in an interesting way with
the ordering of the nodes in the SASH. This is most
obvious with the RANDOM results. The RANDOM
heuristic INVR results are eight times better than the
full RANDOM results. Similar, though less dramatic,
results are seen with other orderings. It appears that
using the heuristic changes the clustering of nearest-
neighbours within the SASH so that better matching
paths are chosen and more noisy matches are elimi-
nated entirely by the heuristic.
It may seem that there are no major advantages
to using the SASH with the already efficient heuris-
tic matching method. However, our experiments
have used small canonical attribute vectors (maxi-
mum length 100). Increasing the canonical vector
size allows us to increase the accuracy of heuristic
solutions at the cost of efficiency. Using a SASH so-
lution would offset some of this efficiency penalty.
This has the potential for a solution that is more than
an order of magnitude faster than NAIVE and is al-
most as accurate.
8 Conclusion
We have integrated a nearest-neighbour approxima-
tion data structure, the Spacial Approximation Sam-
ple Hierarchy (SASH), with a state-of-the-art distri-
butional similarity system. In the process we have
extended the original SASH construction algorithms
(Houle, 2003b) to deal with the non-uniform distri-
bution of words within semantic space.
We intend to test other similarity measures and
node ordering strategies, including a more linguistic
analysis using WordNet, and further explore the in-
teraction between the canonical vector heuristic and
the SASH. The larger 300 word evaluation set used
by Curran (2004) will be used, and combined with a
more detailed analyis. Finally, we plan to optimise
our SASH implementation so that it is comparable
with the highly optimised nearest-neighbour code.
103
DIST c DIRECT P(1) P(5) P(10) INVR SEARCH TIME
NAIVE HEURISTIC 2.72 49% 40% 32% 1.40 827ms
RANDOM 16 2.61 50% 40% 31% 1.39 99% 388ms
RANDOM 64 2.72 49% 40% 32% 1.40 100% 1254ms
RANDOM 128 2.71 49% 40% 32% 1.40 100% 1231ms
FOLD1500 16 2.53 49% 40% 31% 1.36 97% 363ms
FOLD1500 64 2.72 49% 40% 32% 1.40 100% 900ms
FOLD1500 128 2.72 49% 40% 32% 1.40 100% 974ms
SORT 16 2.78 49% 40% 32% 1.41 100% 323ms
SORT 64 2.73 49% 40% 32% 1.40 100% 1238ms
SORT 128 2.73 49% 40% 32% 1.40 100% 1049ms
Table 5: Evaluation of different distributions using the approximation
The result is distributional similarity calculated
three times faster than existing systems with only a
minor accuracy penalty.
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful feedback and corrections. This
work has been supported by the Australian Research
Council under Discovery Project DP0453131.
References
John R. L. Bernard, editor. 1990. The Macquarie Encyclopedic
Thesaurus. The Macquarie Library, Sydney, Australia.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class-based n-
gram models of natural language. Computational Linguis-
tics, 18(4):467?479, December.
Lou Burnard, editor. 1995. Users Reference Guide British Na-
tional Corpus Version 1.0. Oxford University Computing
Services.
Edgar Cha?vez, Gonzalo Navarro, Ricardo Baeza-Yates, and
Jose? L. Marroqu??n. 2001. Searching in metric spaces. ACM
Computing Surveys, 33(3):273?321, September.
Stephen Clark and David Weir. 2001. Class-based probability
estimation using a semantic hierarchy. In Proceedings of the
Second Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 95?102, Pitts-
burgh, PA USA, 2?7 June.
James R. Curran and Marc Moens. 2002a. Improvements
in automatic thesaurus extraction. In Proceedings of the
Workshop of the ACL Special Interest Group on the Lexicon
(SIGLEX), pages 59?66, Philadelphia, USA, 12 July.
James R. Curran and Marc Moens. 2002b. Scaling context
space. In Proceedings of the 40th annual meeting of the
Association for Computational Linguistics, pages 231?238,
Philadelphia, USA, 7?12 July.
James R. Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
Christiane Fellbaum, editor. 1998. WordNet: an electronic lex-
ical database. The MIT Press, Cambridge, MA USA.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston,
USA.
Michael E. Houle. 2003a. Navigating massive data sets via lo-
cal clustering. In Proceedings of the ninth ACM SIGKDD
international conference on Knowledge discovery and data
mining, pages 547?552, Washington, DC, USA, 24?27 Au-
gust.
Michael E. Houle. 2003b. SASH: a saptial approximation
sample hierarchy for similarity search. Technical Report
RT0517, IBM Reasearch, Tokyo Research Laboratory, Yam-
ato Kanagawa, Japan, March.
Guido Minnen, John Carroll, and Darren Pearce. 2000. Ro-
bust applied morphological generation. In Proceedings of
the First International Natural Language Generation Con-
ference, pages 201?208, Mitzpe Ramon, Israel, 12?16 June.
Marius Pasca and Sanda Harabagiu. 2001. The informative
role of wordnet in open-domain question answering. In Pro-
ceedings of the Workshop on WordNet and Other Lexical
Resources: Applications, Extensions and Customizations,
pages 138?143, Pittsburgh, PA USA, 2?7 June.
Darren Pearce. 2001. Synonymy in collocation extraction. In
Proceedings of the Workshop on WordNet and Other Lex-
ical Resources: Applications, Extensions and Customiza-
tions, pages 41?46, Pittsburgh, PA USA, 2?7 June.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages 133?
142, 17?18 May.
Peter Roget. 1911. Thesaurus of English words and phrases.
Longmans, Green and Co., London, UK.
Grady Ward. 1996. Moby Thesaurus. Moby Project.
104
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 457?464,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Random Indexing using Statistical Weight Functions
James Gorman and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{jgorman2,james}@it.usyd.edu.au
Abstract
Random Indexing is a vector space tech-
nique that provides an efficient and scal-
able approximation to distributional simi-
larity problems. We present experiments
showing Random Indexing to be poor at
handling large volumes of data and evalu-
ate the use of weighting functions for im-
proving the performance of Random In-
dexing. We find that Random Index is ro-
bust for small data sets, but performance
degrades because of the influence high fre-
quency attributes in large data sets. The
use of appropriate weight functions im-
proves this significantly.
1 Introduction
Synonymy relations between words have been
used to inform many Natural Language Processing
(NLP) tasks. While these relations can be extracted
from manually created resources such as thesauri
(e.g. Roget?s Thesaurus) and lexical databases
(e.g. WordNet, Fellbaum, 1998), it is often ben-
eficial to extract these relationships from a corpus
representative of the task.
Manually created resources are expensive and
time-consuming to create, and tend to suffer from
problems of bias, inconsistency, and limited cover-
age. These problems may result in an inappropri-
ate vocabulary, where some terms are not present
or an unbalanced set of synonyms. In a medical
context it is more likely that administration will re-
fer to the giving of medicine than to paper work,
whereas in a business context the converse is more
likely.
The most common method for automatically
creating these resources uses distributional simi-
larity and is based on the distributional hypoth-
esis that similar words appear in similar con-
texts. Terms are described by collating informa-
tion about their occurrence in a corpus into vec-
tors. These context vectors are then compared for
similarity. Existing approaches differ primarily in
their definition of context, e.g. the surrounding
words or the entire document, and their choice of
distance metric for calculating similarity between
the context vectors representing each term.
In this paper, we analyse the use of Random In-
dexing (Kanerva et al, 2000) for semantic similar-
ity measurement. Random Indexing is an approxi-
mation technique proposed as an alternative to La-
tent Semantic Analysis (LSA, Landauer and Du-
mais, 1997). Random Indexing is more scalable
and allows for the incremental learning of context
information.
Curran and Moens (2002) found that dramati-
cally increasing the volume of raw input data for
distributional similarity tasks increases the accu-
racy of synonyms extracted. Random Indexing
performs poorly on these volumes of data. Noting
that in many NLP tasks, including distributional
similarity, statistical weighting is used to improve
performance, we modify the Random Indexing al-
gorithm to allow for weighted contexts.
We test the performance of the original and our
modified system using existing evaluation metrics.
We further evaluate against bilingual lexicon ex-
traction using distributional similarity (Sahlgren
and Karlgren, 2005). The paper concludes with
a more detailed analysis of Random Indexing in
terms of both task and corpus composition. We
find that Random Index is robust for small cor-
pora, but larger corpora require that the contexts
be weighted to maintain accuracy.
457
2 Random Indexing
Random Indexing is an approximating technique
proposed by Kanerva et al (2000) as an alternative
to Singular Value Decomposition (SVD) for Latent
Semantic Analysis (LSA, Landauer and Dumais,
1997). In LSA, it is assumed that there is some
underlying dimensionality in the data, so that the
attributes of two or more terms that have similar
meanings can be folded onto a single axis.
Sahlgren (2005) criticise LSA for being both
computationally inefficient and requiring the for-
mation of a full co-occurrence matrix and its de-
composition before any similarity measurements
can be made. Random Indexing avoids both these
by creating a short index vector for each unique
context, and producing the context vector for each
term by summing index vectors for each context
as it is read, allowing an incremental building of
the context space.
Hecht-Nielsen (1994) observed that there are
many more nearly orthogonal directions in high-
dimensional space than there are truly orthogo-
nal directions. The random index vectors are
nearly-orthogonal, resulting in an approximate
description of the context space. The approx-
imation comes from the Johnson-Lindenstrauss
lemma (Johnson and Lindenstrauss, 1984), which
states that if we project points in a vector space
into a randomly selected subspace of sufficiently
high dimensionality, the distances between the
points are approximately preserved. Random Pro-
jection (Papadimitriou et al, 1998) and Random
Mapping (Kaski, 1998) are similar techniques that
use this lemma. Achlioptas (2001) showed that
most zero-mean distributions with unit variance,
including very simple ones like that used in Ran-
dom Indexing, produce a mapping that satisfies
the lemma. The following description of Ran-
dom Indexing is taken from Sahlgren (2005) and
Sahlgren and Karlgren (2005).
We allocate a d length index vector to each
unique context as is it found. These vectors con-
sist of a large number of 0s and a small number
(?) of ?1s. Each element is allocated one of these
values with the following probability:
?
?
?
?
?
+1 with probability ?/2d
0 with probability d??d
?1 with probability ?/2d
Context vectors are generated on-the-fly. As the
corpus is scanned, for each term encountered, its
contexts are extracted. For each new context, an
index vector is produced for it as above. The con-
text vector is the sum of the index vectors of all
the contexts in which the term appears.
The context vector for a term t appearing in one
each in the contexts c1 = [1, 0, 0,?1] and c2 =
[0, 1, 0,?1] would be [1, 1, 0,?2]. If the context
c1 encountered again, no new index vector would
be generated and the existing index vector for c1
would be added to the existing context vector to
produce a new context vector for t of [2, 1, 0,?3].
The distance between these context vectors can
then be measured using any vector space distance
measure. Sahlgren and Karlgren (2005) use the
cosine measure:
cos(?(u, v)) = ~u ? ~v|~u| |~v| =
?d
i=1 ~ui~vi
?
?d
i=1 ~u2i
?
?d
i=1 ~v2i
Random Indexing allows for incremental sam-
pling. This means that the entire data set need not
be sampled before similarity between terms can be
measured. It also means that additional context
information can be added at any time without in-
validating the information already produced. This
is not feasible with most other word-space mod-
els. The approach used by Grefenstette (1994) and
Curran (2004) requires the re-computation of all
non-linear weights if new data is added, although
some of these weights can be approximated when
adding new data incrementally. Similarly, new
data can be folded into a reduced LSA space, but
there is no guarantee that the original smoothing
will apply correctly to the new data (Sahlgren,
2005).
3 Weights
Our initial experiments using Random Indexing
to extract synonymy relations produced worse re-
sults than those using full vector measures, such as
JACCARD (Curran, 2004), when the full vector is
weighted. We experiment using weight functions
with Random Indexing.
Only a linear weighting scheme can be applied
while maintaining incremental sampling. While
incremental sampling is part of the rationale be-
hind its development, it is not required for Ran-
dom Indexing to work as a dimensionality reduc-
tion technique.
To this end, we revise Random Indexing to en-
able us to use weight functions. For each unique
458
IDENTITY 1.0 FREQ f(w, r, w?)
RELFREQ f(w,r,w
?)
f(w,?,?) TF-IDF
f(w,r,w?)
n(?,r,w?)
TF-IDF? log2(f(w,r,w
?)+1)
log2(1+
N(r,w?)
n(?,r,w?) )
MI log( p(w,r,w
?)
p(w,?,?)p(?,r,w?) )
TTEST p(w,r,w
?)?p(?,r,w?)p(w,?,?)?
p(?,r,w?)p(w,?,?)
GREF94 log2(f(w,r,w
?)+1)
log2(n(?,r,w?)+1)
LIN98A log( f(w,r,w
?)f(?,r?)
f(?,r,w?)f(w,r,?) ) LIN98B ? log(
n(?,r,w?)
Nw )
CHI2 cf. Manning and Schu?tze (1999) LR cf. Manning and Schu?tze (1999)
DICE 2p(w,r,w
?)
p(w,?,?)+p(?,r,w?)
Table 1: Weight Functions Evaluated
context attribute, a d length index vector will be
generated. The context vector of a term w is then
created by the weighted sum of each of its at-
tributes. The results of the original Random In-
dexing algorithm are reproduced using frequency
weighting (FREQ).
Weights are generated using the frequency dis-
tribution of each term and its contexts. This in-
creases the overhead, as we must store the context
attributes for each term. Rather than the context
vector being generated by adding each individual
context, it is generated by adding each the index
vector for each unique context multiplied by its
weight.
The time to calculate the weight of all attributes
of all terms is negligible. The original technique
scales to O(dnm) in construction, for n terms and
m unique attributes. Our new technique scales to
O(d(a + nm)) for a non-zero context attributes
per term, which since a  m is also O(dnm).
Following the notation of Curran (2004), a con-
text relation is defined as a tuple (w, r, w?) where
w is a term, which occurs in some grammatical re-
lation r with another word w? in some sentence.
We refer to the tuple (r, w?) as an attribute of w.
For example, (dog, direct-obj, walk) indicates that
dog was the direct object of walk in a sentence.
An asterisk indicates the set of all existing val-
ues of that component in the tuple.
(w, ?, ?) ? {(r, w?)|?(w, r, w?)}
The frequency of a tuple, that is the number of
times a word appears in a context is f(w, r, w?).
f(w, ?, ?) is the instance or token frequency of the
contexts in which w appears. n(w, ?, ?) is the type
frequency. This is the number of attributes of w.
f(w, ?, ?) ?
?
(r,w?)?(w,?,?) f(w, r, w?)
p(w, ?, ?) ? f(w,?,?)f(?,?,?)
n(w, ?, ?) ? |(w, ?, ?)|
Nw ? |{w|n(w, ?, ?) > 0}|
Most experiments limited weights to the positive
range; those evaluated with an unrestricted range
are marked with a ? suffix. Some weights were
also evaluated with an extra log2(f(w, r, w?) +
1) factor to promote the influence of higher fre-
quency attributes, indicated by a LOG suffix. Al-
ternative functions are marked with a dagger.
The context vector of each term w is thus:
w? =
?
(r,w?)?(w,?,?)
~(r, w?) wgt(w, r, w?)
where ~(r, w?) is the index vector of the context
(r, w?). The weights functions we evaluate are
those from Curran (2004) and are given in Table 1.
4 Semantic Similarity
The first use of Random Indexing was to measure
semantic similarity using distributional similarity.
Kanerva et al (2000) used Random Indexing to
find the best synonym match in Test of English
as a Foreign Language (TOEFL). TOEFL was used
by Landauer and Dumais (1997), who reported an
accuracy 36% using un-normalised vectors, which
was improved to 64% using LSA. Kanerva et al
(2000) produced an accuracy of 48?51% using the
same type of document based contexts and Ran-
dom Indexing, which improved to 62?70% using
narrow context windows. Karlgren and Sahlgren
(2001) improved this to 72% using lemmatisation
and POS tagging.
459
4.1 Distributional Similarity
Measuring distributional similarity first requires
the extraction of context information for each of
the vocabulary terms from raw text. The contexts
for each term are collected together and counted,
producing a vector of context attributes and their
frequencies in the corpus. These terms are then
compared for similarity using a nearest-neighbour
search based on distance calculations between the
statistical descriptions of their contexts.
The simplest algorithm for finding synonyms is
a k-nearest-neighbour search, which involves pair-
wise vector comparison of the context vector of
the target term with the context vector of every
other term in the vocabulary.
We use two types of context extraction to pro-
duce both high and low quality context descrip-
tions. The high quality contexts were extracted
from grammatical relations extracted using the
SEXTANT relation extractor (Grefenstette, 1994)
and are lemmatised. This is the same data used in
Curran (2004).
The low quality contexts were extracted taking
a window of one word to the left and right of the
target term. The context is marked as to whether
it preceded or followed the term. Curran (2004)
found this extraction technique to provided rea-
sonable results on the non-speech portion of the
BNC when the data was lemmatised. We do not
lemmatise, which produces noisier data.
4.2 Bilingual Lexicon Acquisition
A variation on the extraction of synonymy rela-
tions, is the extraction of bilingual lexicons. This
is the task of finding for a word in one language
words of a similar meaning in a second language.
The results of this can be used to aid manual con-
struction of resources or directly aid translation.
This task was first approached as a distribu-
tional similarity-like problem by Brown et al
(1988). Their approach uses aligned corpora in
two or more languages: the source language, from
which we are translating, and the target language,
to which we are translating. For a each aligned
segment, they measure co-occurrence scores be-
tween each word in the source segment and each
word in the target segment. These co-occurrence
scores are used to measure the similarity between
source and target language terms
Sahlgren and Karlgren?s approach models the
problem as a distributional similarity problem us-
Source Context Target
Language Language
aaabbc I xxyzzz
bcc II wxy
aab III xzz
Table 2: Paragraph Aligned Corpora
ing the paragraph as context. In Table 2, the source
language is limited to the words a, b and c and the
target language to the words x, y and z. Three para-
graphs in each of these languages are presented as
pairs of translations labelled as a context: aaabbc
is translated as xxyzzz and labelled context I. The
frequency weighted context vector for a is {I:3,
III:2} and for x is {I:2, II:1, III:1}.
A translation candidate for a term in the source
language is found by measuring the similarity be-
tween its context vector and the context vectors of
each of the terms in the target language. The most
similar target language term is the most likely
translation candidate.
Sahlgren and Karlgren (2005) use Random In-
dexing to produce the context vectors for the
source and target languages. We re-implement
their system and apply weighting functions in an
attempt to achieve improved results.
5 Experiments
For the experiments extracting synonymy rela-
tions, high quality contexts were extracted from
the non-speech portion of the British National
Corpus (BNC) as described above. This represents
90% of the BNC, or 90 million words.
Comparisons between low frequency terms are
less accurate than between high frequency terms
as there is less evidence describing them (Cur-
ran and Moens, 2002). This is compounded in
randomised vector techniques because the ran-
domised nature of the representation means that
a low frequency term may have a similar context
vector to a high frequency term while not sharing
many contexts. A frequency cut-off of 100 was
found to balance this inaccuracy with the reduc-
tion in vocabulary size. This reduces the original
246,046 word vocabulary to 14,862 words. Exper-
iments showed d = 1000 and ? = 10 to provide a
balance between speed and accuracy.
Low quality contexts were extracted from por-
tions of the entire of the BNC. These formed cor-
pora of 100,000, 500,000, 1 million, 5 million, 10
460
million, 50 million and 100 million words, cho-
sen from random documents. This allowed us test
the effect of both corpus size and context qual-
ity. This produced vocabularies of between 10,380
and 522,163 words in size. Because of the size
of the smallest corpora meant that a high cutoff
would remove to many terms for a fair test, a cut-
off of 5 was applied. The values d = 1000 and
? = 6 were used.
For our experiments in bilingual lexicon acqui-
sition we follow Sahlgren and Karlgren (2005).
We use the Spanish-Swedish and the English-
German portions of the Europarl corpora (Koehn,
2005).1 These consist of 37,379 aligned para-
graphs in Spanish?Swedish and 45,556 in English-
German. The text was lemmatised using Con-
nexor Machinese (Tapanainen and Ja?vinen, 1997)2
producing vocabularies of 42,671 terms of Span-
ish, 100,891 terms of Swedish, 40,181 terms of
English and 70,384 terms of German. We use
d = 600 and ? = 6 and apply a frequency cut-
off of 100.
6 Evaluation Measures
The simplest method for evaluation is the direct
comparison of extracted synonyms with a man-
ually created gold standard (Grefenstette, 1994).
To reduce the problem of limited coverage, our
evaluation of the extraction of synonyms combines
three electronic thesauri: the Macquarie, Roget?s
and Moby thesauri.
We follow Curran (2004) and use two perfor-
mance measures: direct matches (DIRECT) and
inverse rank (INVR). DIRECT is the number of
returned synonyms found in the gold standard.
INVR is the sum of the inverse rank of each match-
ing synonym, e.g. matches at ranks 3, 5 and 28
give an inverse rank score of 13 + 15 + 128 . With
at most 100 matching synonyms, the maximum
INVR is 5.187. This more fine grained as it incor-
porates the both the number of matches and their
ranking.
The same 300 single word nouns were used for
evaluation as used by Curran (2004) for his large
scale evaluation. These were chosen randomly
from WordNet such that they covered a range over
the following properties: frequency, number of
senses, specificity and concreteness. On average
each evaluation term had 301 gold-standard syn-
1http://www.statmt.org/europarl/
2http://www.connexor.com/
Weight DIRECT INVR
FREQ 2.87 0.94
IDENTITY 3.18 0.95
RELFREQ 2.87 0.94
TF-IDF 0.30 0.07
TF-IDF? 3.92 1.39
MI 1.52 0.54
MILOG 3.38 1.39
MI? 1.87 0.65
MILOG? 3.49 1.41
TTEST 1.06 0.52
TTESTLOG 1.53 0.62
TTEST? 1.06 0.52
TTESTLOG? 1.52 0.61
GREF94 2.82 0.86
LIN98A 1.52 0.50
LIN98B 2.95 0.84
CHI2 0.46 0.25
DICE 3.32 1.11
DICELOG 2.56 0.81
LR 1.96 0.58
Table 3: Evaluation of synonym extraction
onyms. For each of these terms, the closest 100
terms and their similarity scores were extracted.
For the evaluation of bilingual lexicon acqui-
sition we use two online lexical resources used
by Sahlgren and Karlgren (2005) as gold stan-
dards: Lexin?s online Swedish-Spanish lexicon3
and TU Chemnitz? online English-German dic-
tionary.4 Each of the elements in a compound
or multi-word expression is treated as a poten-
tial translation. The German abblendlicht (low beam
light) is treated as a translation candidate for low,
beam and light separately.
Low coverage is more of problem than in our
thesaurus task as we have not used combined re-
sources. There are an average of 19 translations
for each of the 3,403 Spanish terms and 197 trans-
lations for each of the 4,468 English terms. The
English-German translation count is skewed by
the presence of connectives in multi-word expres-
sions, such as of and on, producing mistranslations.
Sahlgren and Karlgren (2005) provide good com-
mentary on the evaluation of this task.
Spanish and English are used as the source lan-
guages. The 200 closest terms in the target lan-
guage are found for all terms in both the source
vocabulary and the gold-standards.
We measure the DIRECT score and INVR as
above. In addition we measure the precision of the
closest translation candidate, as used in Sahlgren
and Karlgren (2005).
3http://lexin.nada.kth.se/sve-spa.shtml
4http://dict.tu-chemnitz.de/
461
Weight English-German Spanish-Swedish
DIRECT Precision INVR DIRECT Precision INVR
FREQ 6.1 58% 0.97 0.8 47% 0.53
IDENTITY 6.0 58% 0.91 0.8 47% 0.53
RELFREQ 6.1 58% 0.97 0.8 47% 0.53
TF-IDF 4.9 53% 0.84 0.8 43% 0.50
TF-IDF? 6.3 58% 0.94 0.8 47% 0.53
MI 2.3 58% 0.76 0.8 48% 0.56
MILOG 2.1 58% 0.76 0.8 49% 0.56
MI? 4.6 57% 0.86 0.8 46% 0.53
MILOG? 4.6 57% 0.87 0.8 47% 0.54
TTEST 2.1 57% 0.75 0.8 48% 0.56
TTESTLOG 1.9 56% 0.72 0.8 46% 0.54
TTEST? 4.3 57% 0.85 0.8 45% 0.53
TTESTLOG? 4.0 56% 0.80 0.8 46% 0.53
GREF94 6.1 58% 0.95 0.8 48% 0.54
LIN98A 4.0 59% 0.82 0.8 48% 0.56
LIN98B 5.9 58% 0.91 0.8 48% 0.54
CHI2 3.1 50% 0.71 0.7 41% 0.48
DICE 5.7 58% 0.95 0.8 47% 0.53
DICELOG 4.7 57% 0.90 0.8 46% 0.52
LR 4.5 57% 0.86 0.8 47% 0.54
Table 4: Evaluation of bilingual lexicon extraction
7 Results
Table 3 shows the results for the experiments ex-
tracting synonymy. The basic Random Indexing
algorithm (FREQ) produces a DIRECT score of
2.87, and an INVR of 0.94. It is interesting that
the only other linear weight, IDENTITY, produces
more accurate results. This shows high frequency,
low information contexts reduce the accuracy of
Random Indexing. IDENTITY removes this effect
by ignoring frequency, but does not address the
information aspect. A more accurate weight will
consider the information provided by a context in
its weighting.
There was a large variance in the effective-
ness of the other weights and most proved to be
detrimental to Random Indexing. TF-IDF was the
worst, reducing the DIRECT score to 0.30 and the
INVR to 0.07. TF-IDF?, which is a log-weighted
alternative to TF-IDF, produced very good results.
With the exception of DICELOG, adding an
additional log factor improved performance (TF-
IDF?, MILOG and TTESTLOG). Unrestricted
ranges improved the MI family, but made no dif-
ference to TTEST. Grefenstette?s variation on
TF-IDF (GREF94) does not perform as well as
TF-IDF?, and Lin?s variations on MI? (LIN98A,
LIN98B) do not perform as well as MILOG?.
MILOG? had a higher INVR than TF-IDF?, but
a lower DIRECT score, indicating that it forces
more correct results to the top of the results list,
but also forces some correct results further down
so that they no longer appear in the top 100.
Weight BNC LARGE
DIRECT INVR DIRECT INVR
FREQ 8.9 0.93 7.2 0.85
TF-IDF? 11.8 1.39 12.5 1.50
MILOG? 10.5 1.41 13.8 1.75
Table 5: Evaluation of Random Indexing using a
very large corpus
The effect of high frequency contexts is in-
creased further as we increase the size of the cor-
pus. Table 5 presents results using the 2 billion
word corpus used by Curran (2004). This consists
of the non-speech portion of the BNC, the Reuter?s
Corpus Volume 1 and most of the English news
holdings of the LDC in 2003. Contexts were ex-
tracted as presented in Section 4. A frequency cut-
off of 100 was applied and the values d = 1000
and ? = 5 for FREQ and ? = 10 for the improved
weights were used.
We see that the very large corpus has reduced
the accuracy of frequency weighted Random In-
dexing. In contrast, our two top performers have
both substantially increased in accuracy, present-
ing a 75?100% improvment in performance over
FREQ. MILOG? is more accurate than TF-IDF?
for both measures of accuracy now, indicating it is
a better weight function for very large data sets.
7.1 Bilingual Lexicon Acquisition
When the same function were applied to the bilin-
gual lexicon acquisition task we see substantially
different results: neither the improvement nor the
extremely poor results are found (Table 4).
462
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 0  20  40  60  80  100
IN
VR
Corpus Size (millions of words)
FREQ
IDENTITY
TF-IDF?
MILOG
JACCARD
Figure 1: Random Indexing using window-based
context
In the English-German corpora we replicate
Sahlgren and Karlgren?s (2005) results, with a pre-
cision of 58%. This has a DIRECT score of 6.1 and
an INVR of 0.97. The only weight to make an im-
provement is TF-IDF?, which has a DIRECT score
of 6.3, but a lower INVR and all weights perform
worse in at least one measure.
Our results for the Spanish-Swedish corpora
show similar results. Our accuracy is down from
that in Sahlgren and Karlgren (2005). This is ex-
plained by our application of the frequency cut-off
to both the source and target languages. There are
more weights with higher accuracies, and fewer
with significantly lower accuracies.
7.2 Smaller Corpora
The absence of a substantial improvement in bilin-
gual lexicon acquisition requires further investiga-
tion. Three main factors differ between our mono-
lingual and bilingual experiments: that we are
smoothing a homogeneous data set in our mono-
lingual experiments and a heterogeneous data set
in our bilingual experiments; we are using local
grammatical contexts in our monolingual experi-
ments and paragraph contexts in our bilingual ex-
periments; and, the volume of raw data used in our
monolingual experiments is many times that used
in our bilingual experiments.
Figure 1 presents results for corpora extracted
from the BNC using the window-based context.
Results are shown for the original Random Index-
ing (FREQ) and using IDENTITY, MILOG? and
TF-IDF?, as well as for the full vector measure-
ment using JACCARD measure and the TTEST?
weight (Curran, 2004). Of the Random Index-
ing results FREQ produces the lowest overall re-
sults. It performs better than MILOG? for very
small corpora, but produces near constant results
for greater corpus sizes. Curran and Moens (2002)
found that increasing the volume of input data in-
creased the accuracy of results generated using a
full vector space model. Without weighting, Ran-
dom Indexing fails this, but after weighting is ap-
plied Curran and Moens? results are confirmed.
The quality of context extracted influences how
weights perform individually, but Random In-
dexing using weights still outperforms not using
weights. The relative performance of MILOG?
has been reduced when compared with TF-IDF?,
but is still greater then FREQ.
Gorman and Curran (2006) showed Random In-
dexing to be much faster than full vector space
techniques, but with a 46?56% reduction in accu-
racy compared to using JACCARD and TTEST? .
Using the MI? weight kept the improvement in
speed but with only a 10?18% reduction in accu-
racy. When JACCARD and TTEST? are used with
our low quality contexts they perform consistently
worse that Random Indexing. This indicates Ran-
dom Indexing is stable in the presence of noisy
data. It would be interesting to further compare
these results to those produced by LSA.
The results we have presented have shown that
applying weights to Random Indexing can im-
prove its performance for thesaurus extraction
tasks. This improvement is dependent on the vol-
ume of raw data used to generate the context in-
formation. It is less dependent on the quality of
contexts extracted.
What we have not shown is whether this extends
to the extraction of bilingual lexicons. The bilin-
gual corpora have 12-16 million words per lan-
guage, and for this sized corpora we already see
substantial improvement with corpora as small as
5 million words (Figure 1). It may be that ex-
tracting paragraph-level contexts is not well suited
to weighting, or that the heterogeneous nature of
the aligned corpora reduces the meaningfulness of
weighting. There is also the question as to whether
it can be applied to all languages. There is a lack of
freely available large-scale multi-lingual resources
that makes this difficult to examine.
8 Conclusion
We have applied weighting functions to the vec-
tor space approximation Random Indexing. For
large data sets we found a significant improvement
463
when weights were applied. For smaller data sets
we found that Random Indexing was sufficiently
robust that weighting had at most a minor effect.
Our weighting schemes removed the possibil-
ity of incremental learning of the term space. An
interesting direction would be the development of
algorithms that allowed the incremental applica-
tion of weights, perhaps by re-weighting vectors
when a new context is learned.
Other areas left open for investigation are the in-
teraction between Random Indexing, weights and
the type of context extracted, the use of large-
scale bilingual corpora, the acquisition of lexi-
cons for non-Indo-European languages and across
language family boundaries, and the difference in
effect term and paragraph/document contexts for
thesaurus extraction.
We have demonstrated that the accuracy of Ran-
dom Indexing can be improved by applying weight
functions, increasing accuracy by up to 50% on the
BNC and 100% on a 2 billion word corpus.
Acknowledgements
We would like to thank Magnus Sahlgren for gen-
erously supplying his training and evaluation data
and our reviewers for their helpful feedback and
corrections. This work has been supported by
the Australian Research Council under Discovery
Project DP0453131.
References
Dimitris Achlioptas. 2001. Database-friendly random pro-
jections. In Symposium on Principles of Database Sys-
tems, pages 274?281, Santa Barbara, CA, USA, 21?23
May.
Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, Robert L. Mer-
cer, and Paul S. Roossin. 1988. A statistical approach
to language translation. In Proceedings of the 12th Con-
ference on Computational linguistics, pages 71?76, Bu-
dapest, Hungry, 22?27 August.
James R. Curran and Marc Moens. 2002. Scaling con-
text space. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, pages
231?238, Philadelphia, PA, USA, 7?12 July.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Christiane Fellbaum, editor. 1998. WordNet: an electronic
lexical database. The MIT Press, Cambridge, MA, USA.
James Gorman and James R. Curran. 2006. Scaling distri-
butional similarity to large corpora. In Proceedings of the
44th Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia, 17?21 July. To appear.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston.
Robert Hecht-Nielsen. 1994. Context vectors: general pur-
pose approximate meaning representations self-organized
from raw data. Computational Intelligence: Imitating
Life, pages 43?56.
William B. Johnson and Joram Lindenstrauss. 1984. Exten-
sions to Lipshitz mapping into Hilbert space. Contempo-
rary mathematics, 26:189?206.
Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000.
Random indexing of text samples for latent semantic anal-
ysis. In Proceedings of the 22nd Annual Conference of the
Cognitive Science Society, page 1036, Philadelphia, PA,
USA, 13?15 August.
Jussi Karlgren and Magnus Sahlgren. 2001. From words to
understanding. In Y. Uesaka, P. Kanerva, and H Asoh, ed-
itors, Foundations of Real-World Intelligence, pages 294?
308. CSLI Publications, Stanford, CA, USA.
Samuel Kaski. 1998. Dimensionality reduction by random
mapping: Fast similarity computation for clustering. In
Proceedings of the International Joint Conference on Neu-
ral Networks, pages 413?418. Piscataway, NJ, USA, 31
July?4 August.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit X, Phuket, Thai-
land, 12?16 September.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240, April.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press, Cambridge, MA, USA.
Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Ragha-
van, and Santosh Vempala. 1998. Latent semantic index-
ing: A probabilistic analysis. In Proceedings of the 17th
ACM Symposium on the Principle of Database Systems,
pages 159?168, Seattle, WA, USA, 2?4 June.
Magnus Sahlgren and Jussi Karlgren. 2005. Automatic bilin-
gual lexicon acquisition using random indexing of parallel
corpora. Journal of Natural Language Engineering, Spe-
cial Issue on Parallel Texts, 11(3), June.
Magnus Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Termi-
nology and Knowledge Engineering, Copenhagen, Den-
mark, 16 August.
Pasi Tapanainen and Timo Ja?vinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Conference
on Applied Natural Language Processing, pages 64?71,
31 March?3 April.
464
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 73?80,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
The Topology of Synonymy and Homonymy Networks
James Gorman and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{jgorman2,james}@it.usyd.edu.au
Abstract
Semantic networks have been used suc-
cessfully to explain access to the men-
tal lexicon. Topological analyses of these
networks have focused on acquisition and
generation. We extend this work to look
at models that distinguish semantic rela-
tions. We find the scale-free properties
of association networks are not found in
synonymy-homonymy networks, and that
this is consistent with studies of childhood
acquisition of these relationships. We fur-
ther find that distributional models of lan-
guage acquisition display similar topolog-
ical properties to these networks.
1 Introduction
Semantic networks have played an important role in
the modelling of the organisation of lexical knowl-
edge. In these networks, words are connected by
graph edges based on their semantic relations. In re-
cent years, researchers have found that many seman-
tic networks are small-world, scale-free networks,
having a high degree of structure and a short distance
between nodes (Steyvers and Tenenbaum, 2005).
Early models were taxonomic and explained some
aspects of human reasoning (Collins and Quillian,
1969) (and are still used in artificial reasoning sys-
tems), but were replaced by models that focused on
general graph structures (e.g. Collins and Loftus,
1975). These better modelled many observed phe-
nomena but explained only the searching of seman-
tic space, not its generation or properties that exist
at a whole-network level.
Topological analyses, looking at the statistical
regularities of whole semantic networks, can be
used to model phenomena not easily explained from
the smaller scale data found in human experiments.
These networks are typically formed from corpora,
expert compiled lexical resources, or human word-
association data.
Existing work has focused language acquisition
(Steyvers and Tenenbaum, 2005) and generation
(Cancho and Sole?, 2001). These models use the gen-
eral notion of semantic association which subsumes
all specific semantic relations, e.g. synonymy.
There is evidence that there are distinct cogni-
tive processes for different semantic relations (e.g.
Casenhiser, 2005). We perform a graph analysis
of synonymy, nearness of meaning, and homonymy,
shared lexicalisation.
We find that synonymy and homonymy produce
graphs that are topologically distinct from those pro-
duced using association. They still produce small-
world networks with short path lengths but lack
scale-free properties. Adding edges of different se-
mantic relations, in particular hyponymy, produces
graphs more similar to the association networks. We
argue our analyses consistent with other semantic
network models where nodes of a common type
share edges of different types (e.g. Collins and Lof-
tus, 1975).
We further analyse the distributional model of lan-
guage acquisition. We find that it does not well
explain whole-language acquisition, but provides a
model for synonym and homonym acquisition.
73
2 Graph Theory
Our overview of graph theory follows Watts (1999).
A graph consists of a set of n vertices (nodes) and
a set of edges, or arcs, which join pairs of ver-
tices. Edges are undirected and arcs are directed.
Edges and arcs can be weighted or unweighted,
with weights indicating the relative strength or im-
portance of the edges. We will only consider un-
weighted, undirected networks. Although there is
evidence that semantic relations are both directed
(Tversky, 1977) and weighted (Collins and Loftus,
1975), we do not have access to this information in
a consistent and meaningful format for all our re-
sources.
Two vertices connected by an edge are called
neighbours. The degree k of a vertex is the count
of it neighbours. From this we measure the average
degree ?k? for the graph and the degree distribution
P(k) for all values of k. The degree distribution is
the probability of a vertex having a degree k.
The neighbourhood ?v of a vertex v is the set of all
neighbours of v not including v. The neighbourhood
?S of a subgraph S is the set of all neighbours of S ,
not including the members of S .
The distance between any two vertices is the
shortest path length, or the minimum number of
edges that must be traversed, to reach the first from
the second. The characteristic path length L is the
average distance between vertices.1 The diameter
D of a graph is the maximum shortest path length
between any two vertices. At most D steps are re-
quired to reach any vertex from any other vertex but,
on average, only L are required.
For very large graphs, calculating the values for L
and D is computationally difficult. We instead sam-
ple n?  n nodes and find the mean values of L and
D across the samples. The diameter produced will
always be less than or equal to the true diameter. We
found n? = 100 to be most efficient.
It is not a requirement that every vertex be reach-
able from every other vertex and in these cases both
L and D will be infinite. In these cases we analyse
the largest connected subgraph.
1Here we follow Steyvers and Tenenbaum (2005) as it is
more commonly used in the cognitive science literature. Watts
(1999) defines the characteristic path length as the median of
the means of shortest path lengths for each vertex.
2.1 Small-world Networks
Traditional network models assume that networks
are either completely random or completely regu-
lar. Many natural networks are somewhere between
these two extremes. These small-world networks a
have the high degree of clustering of a regular lattice
and the short average path length of a random net-
work (Watts and Strogatz, 1998). The clustering is
indicative of organisation, and the short paths make
for easier navigation.
The clustering coefficient Cv is used to measure
the degree of clustering around a vertex v:
Cv =
|E(?v)|
(kv
2
)
where |E(?v)| is the number of edges in the neigh-
bourhood ?v and
(kv
2
)
is the total number of possible
edges in ?v. The clustering coefficient C of a graph
is the average over the coefficients of all the vertices.
2.2 The Scale of Networks
Amaral et al (2000) describe three classes of small
world networks based on their degree distributions:
Scale-free networks are characterised by their
degree distribution decaying as a power law, having
a small number of vertices with many links (hubs)
and many vertices with few links. Networks in this
class include the internet (Faloutsos et al, 1999)
and semantic networks (Steyvers and Tenenbaum,
2005).
Broad-scale networks are characterised by their
degree distribution decaying as a power law fol-
lowed by a sharp cut-off. This class includes col-
laborative networks (Watts and Strogatz, 1998).
Single-scale networks are characterised by fast
decaying degree distribution, such exponential or
Gaussian, in which hubs are scarce or nonexistent.
This class includes power grids (Watts and Strogatz,
1998) and airport traffic (Amaral et al, 2000).
Amaral et al (2000) model these differences us-
ing a constrained preferential attachment model,
where new nodes prefer to attach to highly con-
nected nodes. Scale-free networks result when there
are no constraints. Broad-scale networks are pro-
duced when ageing and cost-to-add-link constraints
are added, making it more difficult to produce very
high degree hubs. Single-scale networks occur when
74
these constraints are strengthened. This is one of
several models for scale-free network generation,
and different models will result in different internal
structures and properties (Keller, 2005).
3 Semantics Networks
Semantic networks represent the structure of hu-
man knowledge through the connections of words.
Collins and Quillian (1969) proposed a taxonomic
representation of knowledge, where words are con-
nected by hyponym relations, like in the WordNet
noun hierarchy (Fellbaum, 1998). While this struc-
ture predicted human reaction times for verifying
facts it allows only a limited portion of knowledge
to be expressed. Later models represented knowl-
edge as semi-structured networks, and focused on
explaining performance in memory retrieval tasks.
One such model is spreading-activation, in which
the degree to which a concept is able to be recalled is
related to its similarity both to other concepts in gen-
eral and to some particular prime or primes (Collins
and Loftus, 1975). In this way, if one is asked to
name a red vehicle, fire truck is more likely re-
sponse than car: while both are strongly associated
with vehicle, fire truck is more strongly associated
with red than is car.
More recently, graph theoretic approaches have
examined the topologies of various semantic net-
works. Cancho and Sole? (2001) examine graphs of
English modelled from the British National Corpus.
Since co-occurrence is non-trivial ? words in a sen-
tence must share some semantic content for the sen-
tence to be coherent ? edges were formed between
adjacent words, with punctuation skipped. Two
graphs were formed: one from all co-occurrences
and the other from only those co-occurrences with
a frequency greater than chance. Both models pro-
duced scale-free networks. They find this model
compelling for word choice during speech, not-
ing function words are the most highly connected.
These give structure without conveying significant
meaning, so can be omitted without rendering a
sentence incoherent, but when unavailable render
speech non-fluent. This is consistent with work by
Albert et al (2000) showing that scale-free networks
are tolerant to random deletion but sensitive to tar-
geted removal of highly connected vertices.
Sigman and Cecchi (2002) investigate the struc-
ture of WordNet to study the effects of nounal pol-
ysemy on graph navigation. Beginning with synsets
and the hyponym tree, they find adding polysemy
both reduces the characteristic path length and in-
creases the clustering coefficient, producing a small-
world network. They propose, citing word priming
experiments as evidence, that these changes in struc-
ture give polysemy a role in metaphoric thinking and
generalisation by increasing the navigability of se-
mantic networks.
Steyvers and Tenenbaum (2005) examine the
growth of semantic networks using graphs formed
from several resources: the free association index
collected by Nelson et al (1998), Wordnet and
the 1911 Roget?s thesaurus. All these produced
scale-free networks, and, using an age of acquisi-
tion and frequency weighted preferential attache-
ment model, show that this corresponds to age-of-
acquisition norms for a small set of words. This is
compared to networks produced by Latent Semantic
Analysis (LSA, Landauer and Dumais, 1997), and
conclude that LSA is an inadequate model for lan-
guage growth as it does not produce the same scale-
free networks as their association models.
3.1 Synonymy and Homonymy
While there have been many studies using human
subjects on the acquisition of particular semantic re-
lations, there have been no topological studies differ-
entiating these from the general notion of semantic
association. This is interesting as psycholinguistic
studies have shown that semantic relationships are
distinguishable (e.g. Casenhiser, 2005). Here we
consider synonymy and homonymy.
There are very few cases of true synonymy, where
two words are substitutable in all contexts. Near-
synonymy, where two words share some close com-
mon meaning, is more common. Sets of synonyms
can be grouped together into synsets, representing a
common idea.
Homonymy occurs when a word has multiple
meanings. Formally, homonymy is occurs when
words do not share an etymological root (in lin-
guistics) or when the distinction between meanings
is coarse (in cognitive science). When the words
share a root or meanings are close, the relationship
is called polysemy. This distinction is significant
75
in language acquisition, but as yet little research
has been performed on the learning of polysemes
(Casenhiser, 2005). It is also significant for Natural
Language Processing. The effect of disambiguating
homonyms is markedly different from polysemes in
Information Retrieval (Stokoe, 2005).
We do not have access to these distinctions, as
they are not available in most resources, nor are
there techniques to automatically acquire these dis-
tinctions (Kilgarriff and Yallop, 2000). For simplic-
ity, will conflate the categories under homonymy.
There have been several studies into synonymy
and homonymy acquisition in children, and these
have shown that it lags behind vocabulary growth
(Doherty and Perner, 1998; Garnham et al, 2000).
A child will associate both rabbit and bunny with
the same concept, but before the age of four, most
children have difficulty in choosing the word bunny
if they have already been presented with the word
rabbit. Similarly, a young child asked to point to two
pictures that have the same name but mean different
things will have difficulty, despite knowing each of
the things independently.
Despite this improvement with age, there are
tendencies for language to avoid synonyms and
homonyms as a more general principle of economy
(Casenhiser, 2005). This is balanced by the utility of
ambiguous relations for mental navigation (Sigman
and Cecchi, 2002) which goes some way to explain-
ing why they still play such a large role in language.
4 The Topology of Synonymy and
Homonymy Relations
For each of our resources we form a graph based on
the relations between lexical items. This differs to
the earlier work of Sigman and Cecchi (2002), who
use synsets as vertices, and Steyvers and Tenenbaum
(2005) who use both lexical items and synsets..
This is motivated largely by our automatic ac-
quisition techniques, and also by human studies, in
which we can only directly access relationships be-
tween words. This also allows us to directly com-
pare resources where we have information about
synsets to those without. We distinguish parts of
speech as disambiguation across them is relatively
easy psychologically (Casenhiser, 2005) and com-
putationally (e.g. Ratnaparkhi, 1996).
4.1 Lexical Semantic Resources
A typical resource for providing this information
are manually constructed lexical semantic resources.
We will consider three: Roget?s, WordNet and Moby
Roget?s thesaurus is a common language the-
saurus providing a hierarchy of synsets. Synsets
with the same general or overlapping meaning and
part of speech are collected into paragraphs. The
parts of speech covered are nouns, verbs, adjectives,
adverbs, prepositions, phrases, pronouns, interjec-
tions, conjunctions, and interrogatives. Paragraphs
with similar meaning are collated by part of speech
into labeled categories. Categories are then collected
into classes using a three-tiered hierarchy, with the
most general concept at the top. Where a word has
several senses, it will appear in several synsets. Sev-
eral editions of Roget?s have been released repre-
senting the change in language since the first edi-
tion in 1852. The last freely available edition is the
1911, which uses outdated vocabulary, but the global
topology has not changed with more recent editions
(Old, 2003). As our analysis is not concerned with
the specifics of the vocabulary, this is the edition we
will use. It consists of a vocabulary of 29,460 nouns,
15,173 verbs, 13,052 adjectives and 3,005 adverbs.
WordNet (Fellbaum, 1998) is an electronic lex-
ical database. Like Roget?s, it main unit of or-
ganisation is the synset, and a word with several
senses will appear in several synsets. These are di-
vided into four parts of speech: nouns, verbs, ad-
jectives and adverbs. Synsets are connected by se-
mantic relationships, e.g antonymy, hyponymy and
meronym. WordNet 2.1 provides a vocabulary of
117,097 nouns, 11,488 verbs, 22,141 adjectives and
4,601 adverbs.
The Moby thesaurus provides synonymy lists
for over 30,000 words, with a total vocabulary of
322,263 words. These lists are not distinguished by
part of speech. A separate file is supplied containing
part of speech mappings for words in the vocabu-
lary. We extracted separate synonym lists for nouns,
verbs, adjectives and adverbs using this list com-
bined with WordNet part of speech information.2
This produces a vocabulary of 42,821 nouns, 11,957
verbs, 16,825 adjectives and 3,572 adverbs.
Table 1 presents the statistics for the largest con-
2http://aspell.sourceforge.net/wl/
76
Roget?s WordNet Moby
Noun Verb Adj Adv Noun Verb Adj Adv Noun Verb Adj Adv
n 15,517 8,060 6,327 626 11,746 6,506 4,786 62 42,819 11,934 16,784 3501
?k? 8.97 8.46 7.40 7.17 4.58 6.34 5.16 4.97 34.65 51.98 39.26 16.07
L 6.5 6.0 6.4 10.5 9.8 6.0 9.5 5.6 3.7 3.1 3.4 3.7
D 21.4 17 17 31 27 15.3 26.4 14 9.6 9.8 9.3 9.8
C 0.74 0.68 0.69 0.77 0.63 0.62 0.66 0.57 0.60 0.49 0.57 0.55
Lr 4.7 4.5 4.6 3.5 6.3 5.0 5.9 3.3 3.4 2.8 2.9 3.2
Dr 8.5 8.4 9.0 7 13.3 10.1 11.8 8 5.5 5 5 6
Cr 0.00051 0.0011 0.0012 0.0090 0.00036 0.00099 0.00094 0.028 0.00081 0.0043 0.0023 0.0047
Table 1: Topological statistics for nouns, verbs, adjectives and adverbs for our three gold standard resources
 1e-04
 0.001
 0.01
 0.1
 1
 1  10  100  1000  10000
P(
k)
k
Roget?s
WordNet
Moby
Random
Figure 1: Degree distributions for nouns
nected subgraph for the four parts of speech con-
sidered, along with statistics for random graphs of
equivalent size and average degree (subscript r). In
all cases the clustering coefficient is significantly
higher than that for the random graph. While the
characteristic path length and diameter are larger
than for the random graphs, they are still short in
comparison to an equivalent latice. This, combined
with the high clustering coefficient, indicates that
they are producing small-world networks. The di-
ameter is larger still than for the random graphs. To-
gether these indicate a more lattice like structure,
which is consistent with the intuition that dissimi-
lar words are unlikely to share similar words. This
is independent of part of speech.
Figure 1 shows the degree distributions for nouns,
and for a random graph plotted on log-log axes.
Other parts of speech produce equivalent graphs.
These clearly show that we have not produced scale-
free networks as we are not seeing straight line
power law distributions. Instead we are seeing what
is closer to single- or broad-scale distributions.
The differences in the graphs is explained by the
WordNet Roget?s
Hyp Synset Para Cat
n 11,746 118,264 15,517 27,989 29,431
?k? 4.58 6.61 8.97 26.84 140.36
L 9.8 6.3 6.5 4.3 2.9
D 27 16.4 21.4 12.6 7
C 0.63 0.74 0.74 0.85 0.86
Table 2: Effect of adding hyponym relations
granularity of the synonymy relations presented, as
indicated by the characteristic path length. WordNet
has fine grained synsets and the smallest characteris-
tic path length, while Moby has coarse grained syn-
onyms and the largest characteristic path length.
4.2 Synonymy-Like Relations
Having seen that synonymy and homonymy alone
do not produce scale-free networks, we investigate
the synonymy-like relations of hyponymy and topic
relatedness. Hyponymy is the IS-A class subsump-
tion relationship and occurs between noun synsets in
WordNet. Topic relatedness occurs in the grouping
of synsets in Roget?s in paragraphs and categories.
Table 2 compares adding hyponym edges to the
graph of WordNet nouns and increasing the gran-
ularity of Roget?s synsets using edges between all
words in a paragraph or category. Adding hyponymy
relations increases the connectivity of the graph sig-
nificantly and there are no longer any disconnected
subgraphs. At the same time the diameter is nearly
halved and characteristic path length reduce one
third, but average degree only increases by one third.
To achieving the same reduction in path length and
diameter by the granularity of Roget?s requires the
average degree to increase by nearly three times.
Figure 2 shows the degree distributions when hy-
ponyms are added to WordNet nouns and the granu-
larity of Roget?s is increased. Roget?s category level
graph is omitted for clarity. We can see that the orig-
77
 1e-05
 1e-04
 0.001
 0.01
 0.1
 1
 1  10  100  1000  10000
P(
k)
k
Roget?s
Paragraph
WordNet
Hyponym
Figure 2: Degree distributions adding hyponym re-
lations to nouns
inally broad-scale structure of the Roget?s distribu-
tion is tending to have a more gaussian distribution.
The addition of hyponyms produces a power law dis-
tribution for k > 10 of P(k) ? k?1.7.
Additional constraints on attachment reduce the
ability of networks to be scale-free (Amaral et
al., 2000). The difference between synonymy-
homonymy networks and association networks can
be explained by this. Steyvers and Tenenbaum
(2005) propose a plausible attachment model for
their association networks which has no additional
constraint function. If we use the tendency for lan-
guages to avoid lexical ambiguity from synonymy
and homonymy as a constraint to the production of
edges we will produce broad-scale networks rather
than scale-free networks.
As hyponymy is primarily semantic and does not
produce lexical ambiguity, adding hyponym edges
weakens the constraint on ambiguity, producing a
scale-free network. Generalising synonymy to in-
clude topicality weakens the constraints, but at the
same time reduces preference in attachment. The
results of this is the gaussian-like distribution with
very few low degree nodes. The difference between
this thesaurus based topicality and that found in hu-
man association data is that human association data
only includes the most similar words.
5 Distributional Similarity Networks
Lexical semantic resources can be automatically ex-
tracted using distributional similarity. Here words
are projected into a vector space using the contexts
in which they appear as axes. Contexts can be as
 1e-05
 1e-04
 0.001
 0.01
 0.1
 1
 1  10  100  1000  10000
P(
k)
k
k=5
*k=5
k=50
*k=50
Figure 3: Degree distributions of Jaccard
wide as document (Landauer and Dumais, 1997)
or close as grammatical dependencies (Grefenstette,
1994). The distance between words in this space ap-
proximates the similarity measured by synonymy.
We use the noun similarities produced by Gor-
man and Curran (2006) using the weighted Jac-
card measure and the t-test weight and grammat-
ical relations extracted from their LARGE corpus,
the method found to perform best against their gold-
standard evaluation. Only words with a corpus fre-
quency higher than 100 are included. This method
is comparable to that used in LSA, although using
grammatical relations as context produces similar-
ity much more like synonymy than those taken at a
document level (Kilgarriff and Yallop, 2000).
Distributional similarity produces a list of vocab-
ulary words, their similar neighbours and the sim-
ilarity to the neighbours. These lists approximate
synonymy by measuring substitutability in context,
and do not only find synonyms as near neighbours
as both antonyms and hyponyms are frequently sub-
stitutable in a grammatical context (Weeds, 2003).
From this we generate graphs by taking either the k
nearest neighbours to each word (k-NN), or by us-
ing a threshold. To produce a threshold we take the
mean similarity of the kth neighbour of all words (*k-
NN). We compare both these methods.
Figure 3 compares the degree distributions of
these. Using k-NN produces a degree distribution
that is close to a Gaussian, where as *k-NN pro-
duces a distribution much more like that of our ex-
pert compiled resources. This is unsurprising when
the distribution of distributional distances is consid-
ered. Some words will have many near neighbours,
78
Roget?s WordNet Hyp k-NN *k-NN
n 15,517 11,746 118,264 35,592 19,642
?k? 8.97 4.58 6.61 8.26 13.86
L 6.5 9.8 6.3 6.2 6.4
D 21.4 27 16.4 12 25.6
C 0.74 0.63 0.74 0.18 0.37
Table 3: Comparing nouns in expert and distribu-
tional resources
and other few. In the first case, k-NN will fail to in-
clude some near neighbours, and in the second will
include some distant neighbours that are note se-
mantically related. This result is consistent between
k = 5 and 50. Introduction of random edges from
the noise of distant neighbours reduces the diameter
and missing near neighbours reduces the clustering
coefficient (Table 3).
In Table 3 we also compare these to noun syn-
onymy in Roget?s, and to synonymy and hyponymy
in WordNet. Distributional similarity (*k-NN) pro-
duces a network with similar degree, characteristic
path length and diameter. The clustering coefficient
is much less than that from expert resources, is still
several orders of magnitude larger than an equivalent
random graph (Table 1).
Figure 4 compares a distributional network to net-
works WordNet and Moby. We can see the same
broad-scale in the distributional and synonym net-
works, and a distinct difference with the scale-free
WordNet hyponym distribution.
The distributional similarity distribution is sim-
ilar to that found in networks formed from LSA
by Steyvers and Tenenbaum (2005). Steyvers and
Tenenbaum hypothesise that the distributions pro-
duced by LSA might be due more to frequency dis-
tribution effects that correct language modelling.
In light of our analysis of synonymy relations,
we propose a new explanation. Given that: dis-
tributional similarity has been shown to approx-
imate the semantic similarity in synonymy rela-
tions found in thesaurus type resources (Curran,
2004); distributional similarity produces networks
with similar statistical properties to those formed by
synonym and homonymy relations; and, the syn-
onym and homonymy relations found in thesauri
produce networks with different statistical proper-
ties to those found in the association networks anal-
ysed by Steyvers and Tenenbaum; it can be plausibly
 1e-04
 0.001
 0.01
 0.1
 1
 1  10  100  1000  10000
P(
k)
k
WordNet
Hyponym
Moby
*k=5
Figure 4: Degree distributions for nouns
hypothesised that distributional techniques are mod-
eling the acquisition of synonyms and homonyms,
rather than all semantic relationships.
This is given further credence by experimental
findings that acquisition of homonyms occurs at a
different rate to the acquisition of vocabulary. This
indicates that there are different mechanisms for
learning the meaning of lexical items and learning
to relate the meanings of lexical items. Any whole-
language model would then be composed of a com-
mon set of lexical items related by disparate rela-
tions, such as synonymy, homonymy and hyponymy.
This type of model is predicted by spreading activa-
tion (Collins and Loftus, 1975).
It is unfortunate that there is a lack of data
with which to validate this model, or our constraint
model, empirically. This should not prevent further
analysis of network models that distiguish semantic
relations, so long as this limitation is understood.
6 Conclusion
Semantic networks have been used successfully to
explain access to the mental lexicon. We use both
expert-compiled and automatically extracted seman-
tic resources, we compare the networks formed from
semantic association and synonymy and homonymy.
These relations produce small-world networks, but
do not share the same scale-free properties as for se-
mantic association.
We find that this difference can be explained using
a constrained attachment model informed by child-
hood language acquisition experiments. It is also
predicted by spreading-activation theories of seman-
79
tic access where a common set of lexical items is
connected by a disparate set of relations. We further
find that distributional models of language acquisi-
tion produce relations that approximate synonymy
and networks topologically similar to synonymy-
homonymy networks.
7 Acknowledgements
We would like to thank the anonymous reviewers
for their helpful feedback and corrections. This
work has been supported by the Australian Research
Council under Discovery Projects DP0453131 and
DP0665973.
References
Re?ka Albert, Hawoong Jeong, and Albert-La?szlo? Baraba?si.
2000. Error and attack tolerance of complex networks. Na-
ture, 406:378?381.
Lu??s A. Nunes Amaral, Antonio Scala, Marc Barthe?le?my, and
H. Eugene Stanley. 2000. Classes of small-world net-
works. Proceedings of the National Academy of Sciences,
97(21):11149?11152, October 10.
Ramon F. i Cancho and Ricard V. Sole?. 2001. The small
world of human language. Proceedings of The Royal Society
of London. Series B, Biological Sciences, 268(1482):2261?
2265, November.
Devin M. Casenhiser. 2005. Children?s resistance to
homonymy: an experimental study of pseudohomonyms.
Journal of Child Language, 32:319?343.
Allan M. Collins and Elizabeth F. Loftus. 1975. A spreading-
activation theory of semantic processing. Psychological re-
view, 82(6):407?428.
Allan M. Collins and M. Ross Quillian. 1969. Retrieval time
from semantic memory. Journal of Verbal Learning and Ver-
bal Behavior, 8:240?247.
James R. Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
Martin Doherty and Josef Perner. 1998. Metalinguistic aware-
ness and theory of mind: just two words for the same thing?
Congitive Development, 13:279?305.
Michalis Faloutsos, Petros Faloutsos, and Christos Faloutsos.
1999. On power-law relationships of the internet topology.
In Proceedings of the conference on Applications, technolo-
gies, architectures, and protocols for computer communica-
tion, pages 251?262.
Christiane Fellbaum, editor. 1998. WordNet: an electronic lex-
ical database. The MIT Press, Cambridge, MA, USA.
Wendy A. Garnham, Julie Brooks, Alan Garnham, and Anne-
Marie Ostenfeld. 2000. From synonyms to homonyms:
exploring the role of metarepresentation in language under-
standing. Developmental Science, 3(4):428?441.
James Gorman and James R. Curran. 2006. Scaling distribu-
tional similarity to large corpora. In Proceedings of the 44th
Annual Meeting of the Association for Computational Lin-
guistics, Sydney, Australia, 17?21 July.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston.
Evelyn F. Keller. 2005. Revisiting ?scale-free? networks.
Bioessays, 27(10):1060?1068, October.
Adam Kilgarriff and Colin Yallop. 2000. What?s in a the-
saurus? In Proceedings of the Second International Confer-
ence on Language Resources and Evaluation, pages 1371?
1379.
Thomas K. Landauer and Susan T. Dumais. 1997. A solu-
tion to plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240, April.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 1998. The university of south florida
word association, rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/.
L. John Old. 2003. The Semantic Structure of Roget?s, a Whole-
Language Thesaurus. Ph.D. thesis, Indiana University.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages 133?
142, 17?18 May.
Mariano Sigman and Guillermo A. Cecchi. 2002. Global or-
ganization of the WordNet lexicon. Proceedings of the Na-
tional Academy of Sciences, 99(3):1742?1747.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The large-
scale structure of semantic networks: statistical analyses and
a model of semantic growth. Cognitive Science, 29(1):41?
78.
Christopher Stokoe. 2005. Differentiating homonymy and
polysemy in information retrieval. In Proceedings of the
Conference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 403?410.
Amos Tversky. 1977. Features of similarity. Psychological
Review, 84(4):327?352, July.
Duncan J. Watts and Steven H. Strogatz. 1998. Collective dy-
namics of small-world networks, 393:440?442, 4 June.
Duncan J. Watts. 1999. Small Worlds: The Dynamics of Net-
works between Order and Randomness. Princeton Univer-
sity Press, Princeton, NJ, USA.
Julie E. Weeds. 2003. Measures and Applications of Lexical
Distributional Similarity. Ph.D. thesis, University of Sussex.
80
