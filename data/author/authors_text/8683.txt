Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 819?826, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Noun Phrase Query Segmentation
Shane Bergsma and Qin Iris Wang
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{bergsma,wqin}@cs.ualberta.ca
Abstract
Query segmentation is the process of tak-
ing a user?s search-engine query and di-
viding the tokens into individual phrases
or semantic units. Identification of these
query segments can potentially improve
both document-retrieval precision, by first
returning pages which contain the exact
query segments, and document-retrieval re-
call, by allowing query expansion or substi-
tution via the segmented units. We train and
evaluate a machine-learned query segmenta-
tion system that achieves 86% segmentation-
decision accuracy on a gold standard set of
segmented noun phrase queries, well above
recently published approaches. Key en-
ablers of this high performance are features
derived from previous natural language pro-
cessing work in noun compound bracketing.
For example, token association features be-
yond simple N-gram counts provide power-
ful indicators of segmentation.
1 Introduction
Billions of times every day, people around the world
communicate with Internet search engines via a
small text box on a web page. The user provides
a sequence of words to the search engine, and the
search engine interprets the query and tries to return
web pages that not only contain the query tokens,
but that are also somehow about the topic or idea
that the query terms describe.
Recent years have seen a widespread recognition
that the user is indeed providing natural language
text to the search engine; query tokens are not inde-
pendent, unordered symbols to be matched on a web
document but rather ordered words and phrases with
syntactic relationships. For example, Zhai (1997)
pointed out that indexing on single-word symbols is
not able to distinguish a search for ?bank terminol-
ogy? from one for ?terminology bank.? The reader
can submit these queries to a current search engine
to confirm that modern indexing does recognize the
effect of token order on query meaning in some way.
Accurately interpreting query semantics also de-
pends on establishing relationships between the
query tokens. For example, consider the query ?two
man power saw.? There are a number of possible
interpretations of this query, and these can be ex-
pressed through a number of different segmentations
or bracketings of the query terms:
1. [two man power saw]
2. [two man] [power saw]
3. [two] [man] [power saw]
4. [two] [man power] [saw], etc.
One simple way to make use of these interpretations
in search would be to put quotation marks around the
phrasal segments to require the search engine to only
find pages with exact phrase matches. If, as seems
likely, the searcher is seeking pages about the large,
mechanically-powered two-man saws used by lum-
berjacks and sawyers to cut big trees, then the first
segmentation is correct. Indeed, a phrasal search
for ?two man power saw? on Google does find the
device of interest. So does the second interpreta-
tion, but along with other, less-relevant pages dis-
cussing competitions involving ?two-man handsaw,
819
two-woman handsaw, power saw log bucking, etc.?
The top document returned for the third interpreta-
tion, meanwhile, describes a man on a rampage at a
subway station with two cordless power saws, while
the fourth interpretation finds pages about topics
ranging from hockey?s thrilling two-man power play
advantage to the man power situation during the
Second World War. Clearly, choosing the right seg-
mentation means finding the right documents faster.
Query segmentation can also help if insufficient
pages are returned for the original query. A tech-
nique such as query substitution or expansion (Jones
et al, 2006) can be employed using the segmented
units. For example, we could replace the sexist ?two
man? modifier with the politically-correct ?two per-
son? phrase in order to find additional relevant doc-
uments. Without segmentation, expanding via the
individual words ?two,? ?man,? ?power,? or ?saw?
could produce less sensible results.
In this paper, we propose a data-driven, machine-
learned approach to query segmentation. Similar to
previous segmentation approaches described in Sec-
tion 2, we make a decision to segment or not to seg-
ment between each pair of tokens in the query. Un-
like previous work, we view this as a classification
task where the decision parameters are learned dis-
criminatively from gold standard data. In Section 3,
we describe our approach and the features we use.
Section 4 describes our labelled data, as well as the
specific tools used for our experiments. Section 5
provides the results of our evaluation, and shows the
strong gains in performance possible using a wide
set of features within a discriminative framework.
2 Related Work
Query segmentation has previously been ap-
proached in an unsupervised manner. Risvik et
al. (2003) combine the frequency count of a seg-
ment and the mutual information (MI) between pairs
of words in the segment in a heuristic scoring func-
tion. The system chooses the segmentation with the
highest score as the output segmentation. Jones et
al. (2006) use MI between pairs of tokens as the sole
factor in deciding on segmentation breaks. If the MI
is above a threshold (optimized on a small training
set), the pair of tokens is joined in a segment. Oth-
erwise, a segmentation break is made.
Query segmentation is related to the task of noun
compound (NC) bracketing. NC bracketing deter-
mines the syntactic structure of an NC as expressed
by a binary tree, or, equivalently, a binary bracket-
ing (Nakov and Hearst, 2005a). Zhai (1997) first
identified the importance of syntactic query/corpus
parsing for information retrieval, but did not con-
sider query segmentation itself. In principle, as
N increases, the number of binary trees for an N -
token compound is much greater than the 2N?1 pos-
sible segmentations. In practice, empirical NC re-
search has focused on three-word compounds. The
computational problem is thus deciding whether the
three-word NC has a left or right-bracketing struc-
ture (Lauer, 1995). For the segmentation task,
analysing a three-word NC requires deciding be-
tween four different segmentations. For example,
there are two bracketings for ?used car parts,? the
left-bracketing ?[[used car] parts]? and the right-
bracketing ?[used [car parts]],? while there are four
segmentations, including the case where there is
only one segment, ?[used car parts]? and the base
case where each token forms its own segment,
?[used] [car] [parts].? Query segmentation thus nat-
urally handles the case where the query consists of
multiple, separate noun phrases that should not be
analysed with a single binary tree.
Despite the differences between the tasks, it is
worth investigating whether the information that
helps disambiguate left and right-bracketings can
also be useful for segmentation. In particular, we
explored many of the sources of information used
by Nakov and Hearst (2005a), as well as several
novel features that aid segmentation performance
and should also prove useful for NC analysis re-
searchers. Unlike all previous approaches that we
are aware of, we apply our features in a flexible
discriminative framework rather than a classification
based on a vote or average of features.
NC analysis has benefited from the recent trend
of using web-derived features rather than corpus-
based counts (Keller and Lapata, 2003). Lapata and
Keller (2004) first used web-based co-occurrence
counts for the bracketing of NCs. Recent inno-
vations have been to use statistics ?beyond the N-
gram,? such as counting the number of web pages
where a pair of words w, x participate in a genitive
relationship (?w?s x?), occur collapsed as a single
820
phrase (?wx?) (Nakov and Hearst, 2005a) or have
a definite article as a left-boundary marker (?the
w x?) (Nicholson and Baldwin, 2006). We show
strong performance gains when such features are
employed for query segmentation.
NC bracketing is part of a larger field of research
on multiword expressions including general NC in-
terpretation. NC interpretation explores not just
the syntactic dependencies among compound con-
stituents, but the semantics of the nominal relation-
ships (Girju et al, 2005). Web-based statistics have
also had an impact on these wider analysis tasks, in-
cluding work on interpretation of verb nominalisa-
tions (Nicholson and Baldwin, 2006) and NC coor-
dination (Nakov and Hearst, 2005b).
3 Methodology
3.1 Segmentation Classification
Consider a query x = {x1, x2, ..., xN} consistingof N query tokens. Segmentation is a mapping S :
x ? y ? YN , where y is a segmentation from theset YN . Since we can either have or not have a seg-mentation break at each of the N?1 spaces between
the N tokens, |YN | = 2N?1. Supervised machinelearning can be applied to derive the mapping S au-
tomatically, given a set of training examples con-
sisting of pairs of queries and their segmentations
T = {(xi,yi)}. Typically this would be done via aset of features ?(x,y) for the structured examples.
A set of weights w can be learned discriminatively
such that each training example (xi,yi) has a higherscore, Scorew(x,y) = w ? ?(x,y), than alterna-tive query-segmentation pairs, (xi, zi), zi 6= yi.1 Attest time, the classifier chooses the segmentation for
x that has the highest score according to the learned
parameterization: y? = argmaxy Scorew(x,y).Unlike many problems in NLP such as parsing or
part-of-speech tagging, the small cardinality of YNmakes enumerating all the alternative query segmen-
tations computationally feasible.
In our preliminary experiments, we used a Sup-
port Vector Machine (SVM) ranker (Joachims,
2002) to learn the structured classifier.2 We also in-
1See e.g. Collins (2002) for a popular training algorithm.
2A ranking approach was also used previously by Daume? III
and Marcu (2004) for the CoNLL-99 nested noun phrase iden-
tification task.
vestigated a Hidden Markov Model SVM (Altun et
al., 2003) to label the segmentation breaks using in-
formation from past segmentation decisions. Ulti-
mately, the mappings produced by these approaches
were not as accurate as a simple formulation that
creates a full query segmentation y as the combi-
nation of independent classification decisions made
between each pair of tokens in the query.3
In the classification framework, the input is a
query, x, a position in the query, i, where 0<i<N ,
and the output is a segmentation decision yes/no.
The training set of segmented queries is converted
into examples of decisions between tokens and
learning is performed on this set. At test time, N ?1
segmentation decisions are made for the N -length
query and an output segmentation y is produced.
Here, features depend only on the input query x and
the position in the query i. For a decision at position
i, we use features from tokens up to three positions
to the left and to the right of the decision location.
That is, for a decision between xL0 and xR0, we ex-tract features from a window of six tokens in the
query: {..., xL2, xL1, xL0, xR0, xR1, xR2, ...}. Wenow detail the features derived from this window.
3.2 Features
There are a number of possible indicators of whether
a segmentation break occurs between a pair of to-
kens. Some of these features fire separately for each
token x in our feature window, while others are de-
fined over pairs or sets of tokens in the window. We
first describe the features that are defined for the to-
kens around the decision boundary, xL0 and xR0,before describing how these same features are ex-
tended to longer phrases and other token pairs.
3.2.1 Decision-boundary features
Table 1 lists the binary features that fire if partic-
ular aspects of a token or pair of tokens are present.
For example, one of the POS-tags features will fire
if the pair?s part-of-speech tags are DT JJ , another
feature will fire if the position of the pair in the to-
3The structured learners did show large gains over the clas-
sification framework on the dev-set when using only the basic
features for the decision-boundary tokens (see Section 3.2.1),
but not when the full feature set was deployed. Also, features
only available to structured learners, e.g. number of segments
in query, etc., did improve the performance of the structured
approaches, but not above that of the simpler classifier.
821
Table 1: Indicator features.
Name Description
is-the token x = ?the?
is-free token x = ?free?
POS-tags Part-of-speech tags of pair xL0 xR0fwd-pos position from beginning, i
rev-pos position from end N ? i
ken is 2, etc. The two lexical features (for when the
token is ?the? and when the token is ?free?) fire sep-
arately for the left and right tokens around the deci-
sion boundary. They are designed to add discrimi-
nation for these common query words, motivated by
examples in our training set. For example, in the
training set, ?free? often occurs in its own segment
when it?s on the left-hand-side of a decision bound-
ary (e.g. ?free? ?online? ...), but may join into a
larger segment when it?s on the right-hand-side of a
collocation (e.g. ?sulfite free? or ?sugar free?). The
classifier can use the feature weights to encourage or
discourage segmentation in these specific situations.
For statistical features, previous work (Section 2)
suggests that the mutual information between the de-
cision tokens xL0 and xR0 may be appropriate. Thelog of the pointwise mutual information (Church and
Hanks, 1989) between the decision-boundary tokens
xL0, xR0 is:
MI(xL0, xR0) = log Pr(xL0xR0)Pr(xL0)Pr(xR0)
This is equivalent to the sum: log C(xL0xR0) +
log K ? log C(xL0) ? log C(xR0). For web-basedfeatures, the counts C(.) can be taken as a search en-
gine?s count of the number of pages containing the
term. The normalizer K is thus the total number of
pages on the Internet.
Represented as a summation, we can see that pro-
viding MI as the feature effectively ties the weights
on the logarithmic counts C(xL0xR0), C(xL0), and
C(xR0). Another approach would be to providethese logarithmic counts as separate features to our
learning algorithm, which can then set the weights
optimally for segmentation. We call this set of
counts the ?Basic? features. In Section 5, we con-
firm results on our development set that showed us-
ing the basic features untied increased segmentation
Table 2: Statistical features.
Name Description
web-count count of ?x? on the web
pair-count web count ?w x?
definite web count ?the w x?
collapsed web count ?wx? (one word)
and-count web count ?w and x?
genitive web count ?w?s x?
Qcount-1 Counts of ?x? in query database
Qcounts-2 Counts of ?w x? in database
performance by up to 4% over using MI ? an impor-
tant observation for all researchers using association
models as features in their discriminative classifiers.
Furthermore, with this technique, we do not need
to normalize the counts for the other pairwise statis-
tical features given in Table 2. We can simply rely
on our learning algorithm to increase or decrease the
weights on the logarithm of the counts as needed.
To illustrate how the statistical features work,
consider a query from our development set: ?star
wars weapons guns.? The phrase ?star wars? can
easily be interpreted as a phrase; there is a high
co-occurrence count (pair-count), and many pages
where they occur as a single phrase (collapsed),
e.g. ?starwars.com.? ?Weapons? and ?guns,? on the
other hand, should not be joined together. Although
they may have a high co-occurrence count, the coor-
dination feature (and-count) is high (?weapons and
guns?) showing these to be related concepts but not
phrasal constituents. Including this novel feature re-
sulted in noticeable gains on the development set.
Since this is a query-based segmentation, features
that consider whether sets of tokens occurred else-
where in the query database may provide domain-
specific discrimination. For each of the Qcount fea-
tures, we look for two quantities: the number of
times the phrase occurs as a query on its own and the
number of times the phrase occurs within another
query.4 Including both of these counts also resulted
in performance gains on the development set.
We also extensively investigated other corpus-
based features, such as the number of times the
phrase occurred hyphenated or capitalized, and the
4We exclude counts from the training, development, and
testing queries discussed in Section 4.1.
822
corpus-based distributional similarity (Lin, 1998)
between a pair of tokens. These features are
not available from search-engine statistics because
search engines disregard punctuation and capitaliza-
tion, and collecting page-count-based distributional
similarity statistics is computationally infeasible.
Unfortunately, none of the corpus-based features
improved performance on the development set and
are thus excluded from further consideration. This
is perhaps not surprising. For such a task that in-
volves real user queries, with arbitrary spellings and
sometimes exotic vocabulary, gathering counts from
web search engines is the only way to procure reli-
able and broad-coverage statistics.
3.2.2 Context Features
Although the tokens at the decision boundary
are of paramount importance, information from the
neighbouring tokens is also critical for segmentation
decision discrimination. We thus include features
that take into consideration the preceding and fol-
lowing tokens, xL1 and xR1, as context information.We gather all the token indicator features for each of
these tokens, as well as all pairwise features between
xL1 and xL0, and then xR0 and xR1. If context to-kens are not available at this position in the query,
a feature fires to indicate this. Also, if the context
features are available, we include trigram web and
query-database counts of ?xL1 xL0 xR0? and ?xL0
xR0 xR1?, and a fourgram spanning both contexts.Furthermore, if tokens xL2 and xR2 are available, wecollect relevant token-level, pairwise, trigram, and
fourgram counts including these tokens as well.
In Section 5, we show that context features are
very important. They allow our system to implic-
itly leverage surrounding segmentation decisions,
which cannot be accessed directly in an independent
segmentation-decision classifier. For example, con-
sider the query ?bank loan amoritization schedule.?
Although ?loan amoritization? has a strong connec-
tion, we may nevertheless insert a break between
them because ?bank loan? and ?amoritization sched-
ule? each have even stronger association.
3.2.3 Dependency Features
Motivated by work in noun phrase parsing, it
might be beneficial to check if, for example, token
xL0 is more likely to modify a later token, such as
xR1. For example, in ?female bus driver?, we mightnot wish to segment ?female bus? because ?female?
has a much stronger association with ?driver? than
with ?bus?. Thus, as features, we include the pair-
wise counts between xL0 and xR1, and then xL1 and
xR0. Features from longer range dependencies didnot improve performance on the development set.
4 Experimental Setup
4.1 Data
Our dataset was taken from the AOL search query
database (Pass et al, 2006), a collection of 35
million queries submitted to the AOL search en-
gine. Most punctuation has been removed from the
queries.5 Along with the query, each entry in the
database contains an anonymous user ID and the do-
main of the URL the user clicked on, if they selected
one of the returned pages. For our data, we used only
those queries with a click-URL. This subset has a
higher proportion of correctly-spelled queries, and
facilitates annotation (described below).
We then tagged the search queries using a max-
imum entropy part-of-speech tagger (Ratnaparkhi,
1996). As our approach was designed particularly
for noun phrase queries, we selected for our final ex-
periments those AOL queries containing only deter-
miners, adjectives, and nouns. We also only consid-
ered phrases of length four or greater, since queries
of these lengths are most likely to benefit from a seg-
mentation, but our approach works for queries of any
length. Future experiments will investigate applying
the current approach to phrasal verbs, prepositional
idioms and segments with other parts of speech.
We randomly selected 500 queries for training,
500 for development, and 500 for final testing.
These were all manually segmented by our annota-
tors. Manual segmentation was done with improv-
ing search precision in mind. Annotators were asked
to analyze each query and form an idea of what the
user was searching for, taking into consideration the
click-URL or performing their own online searches,
if needed. The annotators were then asked to seg-
ment the query to improve search retrieval, by forc-
ing a search engine to find pages with the segments
5Including, unfortunately, all quotation marks, precluding
our use of users? own segmentations as additional labelled ex-
amples or feature data for our system
823
occurring as unbroken units.
One annotator segmented all three data sets, and
these were used for all the experiments. Two ad-
ditional annotators also segmented the final test set
to allow inter-annotator agreement calculation. The
pairwise agreement on segmentation decisions (be-
tween each pair of tokens) was between 84.0% and
84.6%. The agreement on entire queries was be-
tween 57.6% and 60.8%. All three agreed com-
pletely on 219 of the 500 queries, and we use this
?intersected? set for a separate evaluation in our ex-
periments.6 If we take the proportion of segmenta-
tion decisions the annotators would be expected to
agree on by chance to be 50%, the Kappa statis-
tic (Jurafsky and Martin, 2000, page 315) is around
.69, below the .8 considered to be good reliability.
This observed agreement was lower than we an-
ticipated, and reflects both differences in query in-
terpretation and in the perceived value of differ-
ent segmentations for retrieval performance. An-
notators agreed that terms like ?real estate,? ?work
force,? ?west palm beach,? and ?private investiga-
tor? should be separate segments. These are colloca-
tions in the linguistics sense (Manning and Schu?tze,
1999, pages 183-187); we cannot substitute related
words for terms in these expressions nor apply syn-
tactic transformations or paraphrases (e.g. we don?t
say ?investigator of privates?). However, for a query
such as ?bank manager,? should we exclude web
pages that discuss ?manager of the bank? or ?branch
manager for XYZ bank?? If a user is searching for a
particular webpage, excluding such results could be
harmful. However, for query substitution or expan-
sion, identifying that ?bank manager? is a single unit
may be useful. We can resolve the conflicting objec-
tives of our two motivating applications by moving
to a multi-layer query bracketing scheme, first seg-
menting unbreakable collocations and then building
them into semantic units with a query segmentation
grammar. This will be the subject of future research.
4.2 Experiments
All of our statistical feature information was col-
lected using the Google SOAP Search API.7 For
training and classifying our data, we use the popular
6All queries and statistical feature information is available
at http://www.cs.ualberta.ca/?bergsma/QuerySegmentation/
7http://code.google.com/apis/soapsearch/
Support Vector Machine (SVM) learning package
SVMlight (Joachims, 1999). SVMs are maximum-
margin classifiers that achieve good performance on
a range of tasks. In each case, we learn a linear ker-
nel on the training set segmentation decisions and
tune the parameter that trades-off training error and
margin on the development set.
We use the following two evaluation criteria:
1. Seg-Acc: Segmentation decision accuracy: the
proportion of times our classifier?s decision to
insert a segment break or not between a pair of
tokens agrees with the gold standard decision.
2. Qry-Acc: Query segmentation accuracy: the
proportion of queries for which the complete
segmentation derived from our classifications
agrees with the gold standard segmentation.
5 Results
Table 3 provides our results for various configu-
rations of features and token-combinations as de-
scribed in Section 3.8 For comparison, a baseline
that always chooses a segmentation break achieves
44.8% Seg-Acc and 4.2% Qry-Acc, while a system
that inserts no breaks achieves 55.2% Seg-Acc and
4.0% Qry-Acc. Our comparison system is the MI
approach used by Jones et al (2006), which achieves
68% Seg-Acc and 26.6% Qry-Acc (Table 3). We let
the SVM set the threshold for MI on the training set.
Note that the Basic, Decision-Boundary system
(Section 3.2.1), which uses exactly the same co-
occurrence information as the MI system (in the
form of the Basic features) but allows the SVM to
discriminatively weight the logarithmic counts, im-
mediately increases Seg-Acc performance by 3.7%.
Even more strikingly, adding the Basic count infor-
mation for the Context tokens (Section 3.2.2) boosts
performance by another 8.5%, increasing Qry-Acc
by over 22%. Smaller, further gains arise by adding
Dependency token information (Section 3.2.3).
Also, notice that moving from Basic features for
the Decision-Boundary tokens to all of our indica-
tor (Table 1) and statistical (Table 2) features (re-
ferred to as All features) increases performance from
71.7% to 84.3%. These gains convincingly justify
8Statistically significant intra-row differences in Qry-Acc
are marked with an asterix (McNemar?s test, p<0.05)
824
Table 3: Segmentation Performance (%)
Feature Type Feature Span Test Set Intersection SetSeg-Acc Qry-Acc Seg-Acc Qry-Acc
MI Decision-Boundary 68.0 26.6 73.8 34.7
Basic Decision-Boundary 71.7 29.2 77.6 39.7
Basic Decision-Boundary, Context 80.2 52.0* 85.6 62.1*
Basic Decision-Boundary, Context, Dependency 81.1 53.2 86.2 64.8
All Decision-Boundary 84.3 57.8* 86.6 63.5
All Decision-Boundary, Context 86.3 63.8* 89.2 71.7*
All Decision-Boundary, Context, Dependency 85.8 61.0 88.7 69.4
our use of an expanded feature set for this task.
Including Context with the expanded features adds
another 2%, while adding Dependency information
actually seems to hinder performance slightly, al-
though gains were seen when adding Dependency
information on the development set.
Note, however, that these results must also be
considered in light of the low inter-annotator agree-
ment (Section 4.1). Indeed, results are lower if we
evaluate using the test-set labels from another an-
notator (necessarily training on the original anno-
tator?s labels). On the intersected set of the three
annotators, however, results are better still: 88.7%
Seg-Acc and 69.4% Qry-Acc on the intersected
queries for the full-featured system (Table 3). Since
high performance is dependent on consistent train-
ing and test labellings, it seems likely that develop-
ing more-explicit annotation instructions may allow
further improvements in performance as within-set
and between-set annotation agreement increases.
It would also be theoretically interesting, and of
significant practical importance, to develop a learn-
ing approach that embraces the agreement of the
annotations as part of the learning algorithm. Our
initial ranking formulation (Section 3.1), for exam-
ple, could learn a model that prefers segmentations
with higher agreement, but still prefers any anno-
tated segmentation to alternative, unobserved struc-
tures. As there is growing interest in making max-
imal use of annotation resources within discrimina-
tive learning techniques (Zaidan et al, 2007), devel-
oping a general empirical approach to learning from
ambiguously-labelled examples would be both an
important contribution to this trend and a potentially
helpful technique in a number of NLP domains.
6 Conclusion
We have developed a novel approach to search query
segmentation and evaluated this approach on actual
user queries, reducing error by 56% over a recent
comparison approach. Gains in performance were
made possible by both leveraging recent progress in
feature engineering for noun compound bracketing,
as well as using a flexible, discriminative incorpora-
tion of association information, beyond the decision-
boundary tokens. We have created and made avail-
able a set of manually-segmented user queries, and
thus provided a new testing platform for other re-
searchers in this area. Our initial formulation of
query segmentation as a structured learning prob-
lem, and our leveraging of association statistics be-
yond the decision boundary, also provides power-
ful tools for noun compound bracketing researchers
to both move beyond three-word compounds and to
adopt discriminative feature weighting techniques.
The positive results achieved on this important ap-
plication should encourage further inter-disciplinary
collaboration between noun compound interpreta-
tion and information retrieval researchers. For ex-
ample, analysing the semantics of multiword expres-
sions may allow for more-focused query expansion;
knowing to expand ?bank manager? to include pages
describing a ?manager of the bank,? but not doing
the same for non-compositional phrases like ?real
estate? or ?private investigator,? requires exactly the
kind of techniques being developed in the noun com-
pound interpretation community. Thus for query ex-
pansion, as for query segmentation, work in natural
language processing has the potential to make a real
and immediate impact on search-engine technology.
825
The next step in this research is to directly inves-
tigate how query segmentation affects search perfor-
mance. For such an evaluation, we would need to
know, for each possible segmentation (including no
segmentation), the document retrieval performance.
This could be the proportion of returned documents
that are deemed to be relevant to the original query.
Exactly such an evaluation was recently used by Ku-
maran and Allan (2007) for the related task of query
contraction. Of course, a dataset with queries and
retrieval scores may serve for more than evaluation;
it may provide the examples used by the learning
module. That is, the parameters of the contraction
or segmentation scoring function could be discrim-
inatively set to optimize the retrieval of the training
set queries. A unified framework for query contrac-
tion, segmentation, and expansion, all based on dis-
criminatively optimizing retrieval performance, is
a very appealing future research direction. In this
framework, the size of the training sets would not
be limited by human annotation resources, but by
the number of queries for which retrieved-document
relevance judgments are available. Generating more
training examples would allow the use of more pow-
erful, finer-grained lexical features for classification.
Acknowledgments
We gratefully acknowledge support from the Natu-
ral Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, the Alberta In-
genuity Center for Machine Learning, and the Al-
berta Informatics Circle of Research Excellence.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas Hofmann.
2003. Hidden markov support vector machines. In ICML.
Kenneth Ward Church and Patrick Hanks. 1989. Word associ-
ation norms, mutual information, and lexicography. In ACL,
pages 76?83.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In EMNLP, pages 1?8.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.
2005. On the semantics of noun compounds. Computer
Speech and Language, 19(4):479?496.
Hal Daume? III and Daniel Marcu. 2004. NP bracketing by
maximum entropy tagging and SVM reranking. In EMNLP,
pages 254?261.
Thorsten Joachims. 1999. Making large-scale Support Vector
Machine learning practical. In B. Scho?lkopf and C. Burges,
editors, Advances in Kernel Methods: Support Vector Ma-
chines, pages 169?184. MIT-Press.
Thorsten Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM Conference on Knowledge Dis-
covery and Data Mining, pages 133?142.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner.
2006. Generating query substitutions. In WWW, pages 387?
396.
Daniel Jurafsky and James H. Martin. 2000. Speech and lan-
guage processing. Prentice Hall.
Frank Keller and Mirella Lapata. 2003. Using the web to obtain
frequencies for unseen bigrams. Computational Linguistics,
29(3):459?484.
Giridhar Kumaran and James Allan. 2007. A case for shorter
queries, and helping users create them. In NAACL-HLT,
pages 220?227.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of NLP tasks. In HLT-NAACL, pages
121?128.
Mark Lauer. 1995. Corpus statistics meet the noun compound:
Some empirical results. In ACL, pages 47?54.
Dekang Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In COLING/ACL, pages 768?773.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press.
Preslav Nakov and Marti Hearst. 2005a. Search engine statis-
tics beyond the n-gram: Application to noun compound
bracketing. In CoNLL, pages 17?24.
Preslav Nakov and Marti Hearst. 2005b. Using the web as
an implicit training set: application to structural ambiguity
resolution. In HLT/EMNLP, pages 835?842.
Jeremy Nicholson and Timothy Baldwin. 2006. Interpretation
of compound nominalisations using corpus and web statis-
tics. In ACL Workshop on Multiword Expressions, pages
54?61.
Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A
picture of search. In The First International Conference on
Scalable Information Systems.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP, pages 133?142.
Knut Magne Risvik, Tomasz Mikolajewski, and Peter Boros.
2003. Query segmentation for web search. In WWW (Poster
Session).
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using
?annotator rationales? to improve machine learning for text
categorization. In NAACL-HLT, pages 260?267.
Chengxiang Zhai. 1997. Fast statistical parsing of noun phrases
for document indexing. In ANLP, pages 312?319.
826
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 5?8,
Rochester, April 2007. c?2007 Association for Computational Linguistics
Learning Structured Classiers for Statistical Dependency Parsing
Qin Iris Wang
Department of Computing Science
University of Alberta
Edmonton, Canada T6G 2E8
wqin@cs.ualberta.ca
Abstract
My research is focused on developing ma-
chine learning algorithms for inferring de-
pendency parsers from language data. By
investigating several approaches I have
developed a unifying perspective that al-
lows me to share advances between both
probabilistic and non-probabilistic meth-
ods. First, I describe a generative tech-
nique that uses a strictly lexicalised pars-
ing model, where all the parameters are
based on words and do not use any part-
of-speech (POS) tags nor grammatical cat-
egories. Then, I incorporate two ideas
from probabilistic parsing?word similar-
ity smoothing and local estimation?to
improve the large margin approach. Fi-
nally, I present a simpler and more ef-
ficient approach to training dependency
parsers by applying a boosting-like proce-
dure to standard training methods.
1 Introduction
Over the past decade, there has been tremendous
progress on learning parsing models from treebank
data (Magerman, 1995; Collins, 1999; Charniak,
1997; Ratnaparkhi, 1999; Charniak, 2000; Wang
et al, 2005; McDonald et al, 2005). Most of the
early work in this area was based on postulating
generative probability models of language that in-
cluded parse structures (Magerman, 1995; Collins,
1997; Charniak, 1997). Learning in this context
consisted of estimating the parameters of the model
with simple likelihood based techniques, but incor-
porating various smoothing and back-off estimation
tricks to cope with the sparse data problems (Collins,
1997; Bikel, 2004). Subsequent research began to
focus more on conditional models of parse structure
given the input sentence, which allowed discrimi-
native training techniques such as maximum con-
ditional likelihood (i.e. ?maximum entropy?) to be
applied (Ratnaparkhi, 1999; Charniak, 2000). Cur-
rently, the work on conditional parsing models ap-
pears to have culminated in large margin training
approaches (Taskar et al, 2004; McDonald et al,
2005), which demonstrates the state of the art per-
formance in English dependency parsing.
Despite the realization that maximum margin
training is closely related to maximum conditional
likelihood for conditional models (McDonald et
al., 2005), a sufficiently unified view has not yet
been achieved that permits the easy exchange of
improvements between the probabilistic and non-
probabilistic approaches. For example, smoothing
methods have played a central role in probabilistic
approaches (Collins, 1997; Wang et al, 2005), and
yet they are not being used in current large margin
training algorithms. Another unexploited connec-
tion is that probabilistic approaches pay closer at-
tention to the individual errors made by each compo-
nent of a parse, whereas the training error minimized
in the large margin approach?the ?structured mar-
gin loss? (McDonald et al, 2005)?is a coarse mea-
sure that only assesses the total error of an entire
parse rather than focusing on the error of any par-
ticular component. I have addressed both of these
issues, as well as others in my work.
2 Dependency Parsing Model
Given a sentence   	


 , I consider the
problem of computing an accurate directed depen-
5
dency tree,  , over   . Note that  consists of or-
dered pairs of words Proceedings of ACL-08: HLT, pages 532?540,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semi-supervised Convex Training for Dependency Parsing
Qin Iris Wang
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
wqin@cs.ualberta.ca
Dale Schuurmans
Department of Computing Science
University of Alberta
Edmonton, AB, Canada, T6G 2E8
dale@cs.ualberta.ca
Dekang Lin
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA, USA, 94043
lindek@google.com
Abstract
We present a novel semi-supervised training
algorithm for learning dependency parsers.
By combining a supervised large margin loss
with an unsupervised least squares loss, a dis-
criminative, convex, semi-supervised learning
algorithm can be obtained that is applicable
to large-scale problems. To demonstrate the
benefits of this approach, we apply the tech-
nique to learning dependency parsers from
combined labeled and unlabeled corpora. Us-
ing a stochastic gradient descent algorithm, a
parsing model can be efficiently learned from
semi-supervised data that significantly outper-
forms corresponding supervised methods.
1 Introduction
Supervised learning algorithms still represent the
state of the art approach for inferring dependency
parsers from data (McDonald et al, 2005a; McDon-
ald and Pereira, 2006; Wang et al, 2007). How-
ever, a key drawback of supervised training algo-
rithms is their dependence on labeled data, which
is usually very difficult to obtain. Perceiving the
limitation of supervised learning?in particular, the
heavy dependence on annotated corpora?many re-
searchers have investigated semi-supervised learn-
ing techniques that can take both labeled and unla-
beled training data as input. Following the common
theme of ?more data is better data? we also use both
a limited labeled corpora and a plentiful unlabeled
data resource. Our goal is to obtain better perfor-
mance than a purely supervised approach without
unreasonable computational effort. Unfortunately,
although significant recent progress has been made
in the area of semi-supervised learning, the perfor-
mance of semi-supervised learning algorithms still
fall far short of expectations, particularly in chal-
lenging real-world tasks such as natural language
parsing or machine translation.
A large number of distinct approaches to semi-
supervised training algorithms have been investi-
gated in the literature (Bennett and Demiriz, 1998;
Zhu et al, 2003; Altun et al, 2005; Mann and
McCallum, 2007). Among the most prominent ap-
proaches are self-training, generative models, semi-
supervised support vector machines (S3VM), graph-
based algorithms and multi-view algorithms (Zhu,
2005).
Self-training is a commonly used technique
for semi-supervised learning that has been ap-
532
plied to several natural language processing tasks
(Yarowsky, 1995; Charniak, 1997; Steedman et al,
2003). The basic idea is to bootstrap a supervised
learning algorithm by alternating between inferring
the missing label information and retraining. Re-
cently, McClosky et al (2006a) successfully applied
self-training to parsing by exploiting available un-
labeled data, and obtained remarkable results when
the same technique was applied to parser adaptation
(McClosky et al, 2006b). More recently, Haffari
and Sarkar (2007) have extended the work of Abney
(2004) and given a better mathematical understand-
ing of self-training algorithms. They also show con-
nections between these algorithms and other related
machine learning algorithms.
Another approach, generative probabilistic mod-
els, are a well-studied framework that can be ex-
tremely effective. However, generative models use
the EM algorithm for parameter estimation in the
presence of missing labels, which is notoriously
prone to getting stuck in poor local optima. More-
over, EM optimizes a marginal likelihood score that
is not discriminative. Consequently, most previous
work that has attempted semi-supervised or unsu-
pervised approaches to parsing have not produced
results beyond the state of the art supervised results
(Klein and Manning, 2002; Klein and Manning,
2004). Subsequently, alternative estimation strate-
gies for unsupervised learning have been proposed,
such as Contrastive Estimation (CE) by Smith and
Eisner (2005). Contrastive Estimation is a general-
ization of EM, by defining a notion of learner guid-
ance. It makes use of a set of examples (its neighbor-
hood) that are similar in some way to an observed
example, requiring the learner to move probability
mass to a given example, taking only from the ex-
ample?s neighborhood. Nevertheless, CE still suf-
fers from shortcomings, including local minima.
In recent years, SVMs have demonstrated state
of the art results in many supervised learning tasks.
As a result, many researchers have put effort on
developing algorithms for semi-supervised SVMs
(S3VMs) (Bennett and Demiriz, 1998; Altun et
al., 2005). However, the standard objective of an
S3VM is non-convex on the unlabeled data, thus
requiring sophisticated global optimization heuris-
tics to obtain reasonable solutions. A number of
researchers have proposed several efficient approx-
imation algorithms for S3VMs (Bennett and Demi-
riz, 1998; Chapelle and Zien, 2005; Xu and Schu-
urmans, 2005). For example, Chapelle and Zien
(2005) propose an algorithm that smoothes the ob-
jective with a Gaussian function, and then performs
a gradient descent search in the primal space to
achieve a local solution. An alternative approach is
proposed by Xu and Schuurmans (2005) who formu-
late a semi-definite programming (SDP) approach.
In particular, they present an algorithm for multi-
class unsupervised and semi-supervised SVM learn-
ing, which relaxes the original non-convex objective
into a close convex approximation, thereby allowing
a global solution to be obtained. However, the com-
putational cost of SDP is still quite expensive.
Instead of devising various techniques for cop-
ing with non-convex loss functions, we approach the
problem from a different perspective. We simply re-
place the non-convex loss on unlabeled data with an
alternative loss that is jointly convex with respect
to both the model parameters and (the encoding of)
the self-trained prediction targets. More specifically,
for the loss on the unlabeled data part, we substi-
tute the original unsupervised structured SVM loss
with a least squares loss, but keep constraints on
the inferred prediction targets, which avoids trivial-
ization. Although using a least squares loss func-
tion for classification appears misguided, there is
a precedent for just this approach in the early pat-
tern recognition literature (Duda et al, 2000). This
loss function has the advantage that the entire train-
ing objective on both the labeled and unlabeled data
now becomes convex, since it consists of a convex
structured large margin loss on labeled data and a
convex least squares loss on unlabeled data. As
we will demonstrate below, this approach admits an
efficient training procedure that can find a global
minimum, and, perhaps surprisingly, can systemat-
ically improve the accuracy of supervised training
approaches for learning dependency parsers.
Thus, in this paper, we focus on semi-supervised
language learning, where we can make use of both
labeled and unlabeled data. In particular, we in-
vestigate a semi-supervised approach for structured
large margin training, where the objective is a com-
bination of two convex functions, the structured
large margin loss on labeled data and the least
squares loss on unlabeled data. We apply the result-
533
 fundsInvestors continue to  pour cash into money
Figure 1: A dependency tree
ing semi-supervised convex objective to dependency
parsing, and obtain significant improvement over
the corresponding supervised structured SVM. Note
that our approach is different from the self-training
technique proposed in (McClosky et al, 2006a),
although both methods belong to semi-supervised
training category.
In the remainder of this paper, we first review
the supervised structured large margin training tech-
nique. Then we introduce the standard semi-
supervised structured large margin objective, which
is non-convex and difficult to optimize. Next we
present a new semi-supervised training algorithm for
structured SVMs which is convex optimization. Fi-
nally, we apply this algorithm to dependency pars-
ing and show improved dependency parsing accu-
racy for both Chinese and English.
2 Dependency Parsing Model
Given a sentence X = (x1, ..., xn) (xi denotes
each word in the sentence), we are interested in
computing a directed dependency tree, Y , over X.
As shown in Figure 1, in a dependency structure,
the basic units of a sentence are the syntactic re-
lationships (aka. head-child or governor-dependent
or regent-subordinate relations) between two indi-
vidual words, where the relationships are expressed
by drawing links connecting individual words (Man-
ning and Schutze, 1999). The direction of each link
points from a head word to a child word, and each
word has one and only one head, except for the head
of the sentence. Thus a dependency structure is ac-
tually a rooted, directed tree. We assume that a di-
rected dependency tree Y consists of ordered pairs
(xi ? xj) of words in X such that each word ap-
pears in at least one pair and each word has in-degree
at most one. Dependency trees are assumed to be
projective here, which means that if there is an arc
(xi ? xj), then xi is an ancestor of all the words
between xi and xj .1 Let ?(X) denote the set of all
the directed, projective trees that span on X. The
parser?s goal is then to find the most preferred parse;
that is, a projective tree, Y ? ?(X), that obtains
the highest ?score?. In particular, one would assume
that the score of a complete spanning tree Y for a
given sentence, whether probabilistically motivated
or not, can be decomposed as a sum of local scores
for each link (a word pair) (Eisner, 1996; Eisner and
Satta, 1999; McDonald et al, 2005a). Given this
assumption, the parsing problem reduces to find
Y ? = arg max
Y ??(X)
score(Y |X) (1)
= arg max
Y ??(X)
?
(xi?xj)?Y
score(xi ? xj)
where the score(xi ? xj) can depend on any mea-
surable property of xi and xj within the sentence X.
This formulation is sufficiently general to capture
most dependency parsing models, including proba-
bilistic dependency models (Eisner, 1996; Wang et
al., 2005) as well as non-probabilistic models (Mc-
Donald et al, 2005a).
For standard scoring functions, particularly those
used in non-generative models, we further assume
that the score of each link in (1) can be decomposed
into a weighted linear combination of features
score(xi ? xj) = ? ? f(xi ? xj) (2)
where f(xi ? xj) is a feature vector for the link
(xi ? xj), and ? are the weight parameters to be
estimated during training.
3 Supervised Structured Large Margin
Training
Supervised structured large margin training ap-
proaches have been applied to parsing and produce
promising results (Taskar et al, 2004; McDonald et
al., 2005a; Wang et al, 2006). In particular, struc-
tured large margin training can be expressed as min-
imizing a regularized loss (Hastie et al, 2004), as
shown below:
1We assume all the dependency trees are projective in our
work (just as some other researchers do), although in the real
word, most languages are non-projective.
534
min
?
?
2 ?
?? + (3)
?
i
max
Li,k
(?(Li,k, Yi)? diff(?, Yi, Li,k))
where Yi is the target tree for sentence Xi; Li,k
ranges over all possible alternative k trees in ?(Xi);
diff(?, Yi, Li,k) = score(?, Yi) ? score(?, Li,k);
score(?, Yi) =
?
(xm?xn)?Yi ? ? f(xm ? xn), as
shown in Section 2; and ?(Li,k, Yi) is a measure of
distance between the two trees Li,k and Yi. This is
an application of the structured large margin training
approach first proposed in (Taskar et al, 2003) and
(Tsochantaridis et al, 2004).
Using the techniques of Hastie et al (2004) one
can show that minimizing the objective (3) is equiv-
alent to solving the quadratic program
min
?,?
?
2 ?
?? + e?? subject to
?i,k ? ?(Li,k, Yi)? diff(?, Yi, Li,k)
?i,k ? 0
for all i, Li,k ? ?(Xi) (4)
where e denotes the vector of all 1?s and ? represents
slack variables. This approach corresponds to the
training problem posed in (McDonald et al, 2005a)
and has yielded the best published results for En-
glish dependency parsing.
To compare with the new semi-supervised ap-
proach we will present in Section 5 below, we re-
implemented the supervised structured large margin
training approach in the experiments in Section 7.
More specifically, we solve the following quadratic
program, which is based on Equation (3)
min
?
?
2 ?
?? +
?
i
max
L
k
?
m=1
k
?
n=1
?(Li,m,n, Yi,m,n)
? diff(?, Yi,m,n, Li,m,n) (5)
where diff(?, Yi,m,n, Li,m,n) = score(?, Yi,m,n) ?
score(?, Li,m,n) and k is the sentence length. We
represent a dependency tree as a k ? k adjacency
matrix. In the adjacency matrix, the value of Yi,m,n
is 1 if the word m is the head of the word n, 0 oth-
erwise. Since both the distance function ?(Li, Yi)
and the score function decompose over links, solv-
ing (5) is equivalent to solve the original constrained
quadratic program shown in (4).
4 Semi-supervised Structured Large
Margin Objective
The objective of standard semi-supervised struc-
tured SVM is a combination of structured large mar-
gin losses on both labeled and unlabeled data. It has
the following form:
min
?
?
2 ?
?? +
N
?
i=1
structured loss (?,Xi, Yi)
+ min
Yj
U
?
j=1
structured loss (?,Xj , Yj) (6)
where
structured loss (?,Xi, Yi)
= max
L
k
?
m=1
k
?
n=1
?(Li,m,n, Yi,m,n) (7)
?diff(?, Yi,m,n, Li,m,n)
N and U are the number of labeled and unlabeled
training sentences respectively, and Yj ranges over
guessed targets on the unsupervised data.
In the second term of the above objective shown in
(6), both ? and Yj are variables. The resulting loss
function has a hat shape (usually called hat-loss),
which is non-convex. Therefore the objective as a
whole is non-convex, making the search for global
optimal difficult. Note that the root of the optimiza-
tion difficulty for S3VMs is the non-convex property
of the second term in the objective function. We will
propose a novel approach which can deal with this
problem. We introduce an efficient approximation?
least squares loss?for the structured large margin
loss on unlabeled data below.
5 Semi-supervised Convex Training for
Structured SVM
Although semi-supervised structured SVM learning
has been an active research area, semi-supervised
structured SVMs have not been used in many real
applications to date. The main reason is that most
available semi-supervised large margin learning ap-
proaches are non-convex or computationally expen-
sive (e.g. (Xu and Schuurmans, 2005)). These tech-
niques are difficult to implement and extremely hard
to scale up. We present a semi-supervised algorithm
535
for structured large margin training, whose objective
is a combination of two convex terms: the super-
vised structured large margin loss on labeled data
and the cheap least squares loss on unlabeled data.
The combined objective is still convex, easy to opti-
mize and much cheaper to implement.
5.1 Least Squares Convex Objective
Before we introduce the new algorithm, we first in-
troduce a convex loss which we apply it to unlabeled
training data for the semi-supervised structured large
margin objective which we will introduce in Sec-
tion 5.2 below. More specifically, we use a struc-
tured least squares loss to approximate the struc-
tured large margin loss on unlabeled data. The cor-
responding objective is:
min
?,Yj
?
2 ?
?? + (8)
?
2
U
?
j=1
k
?
m=1
k
?
n=1
(
??f(Xj,m ? Xj,n)? Yj,m,n
)2
subject to constraints on Y (explained below).
The idea behind this objective is that for each pos-
sible link (Xj,m ? Xj,n), we intend to minimize the
difference between the link and the corresponding
estimated link based on the learned weight vector.
Since this is conducted on unlabeled data, we need
to estimate both ? and Yj to solve the optimization
problem. As mentioned in Section 3, a dependency
tree Yj is represented as an adjacency matrix. Thus
we need to enforce some constraints in the adjacency
matrix to make sure that each Yj satisfies the depen-
dency tree constraints. These constraints are critical
because they prevent (8) from having a trivial solu-
tion in Y. More concretely, suppose we use rows to
denote heads and columns to denote children. Then
we have the following constraints on the adjacency
matrix:
? (1) All entries in Yj are between 0 and 1
(convex relaxation of discrete directed edge in-
dicators);
? (2) The sum over all the entries on each col-
umn is equal to one (one-head rule);
? (3) All the entries on the diagonal are zeros
(no self-link rule);
? (4) Yj,m,n + Yj,n,m ? 1 (anti-symmetric
rule), which enforces directedness.
One final constraint that is sufficient to ensure that
a directed tree is obtained, is connectedness (i.e.
acyclicity), which can be enforced with an addi-
tional semidefinite constraint. Although convex, this
constraint is more expensive to enforce, therefore we
drop it in our experiments below. (However, adding
the semidefinite connectedness constraint appears to
be feasible on a sentence by sentence level.)
Critically, the objective (8) is jointly convex in
both the weights ? and the edge indicator variables
Y. This means, for example, that there are no local
minima in (8)?any iterative improvement strategy,
if it converges at all, must converge to a global min-
imum.
5.2 Semi-supervised Convex Objective
By combining the convex structured SVM loss on
labeled data (shown in Equation (5)) and the con-
vex least squares loss on unlabeled data (shown in
Equation (8)), we obtain a semi-supervised struc-
tured large margin loss
min
?,Yj
?
2 ?
?? +
N
?
i=1
structured loss (?,Xi, Yi) +
U
?
j=1
least squares loss (?,Xj , Yj) (9)
subject to constraints on Y (explained above).
Since the summation of two convex functions is
also convex, so is (9). Replacing the two losses with
the terms shown in Equation (5) and Equation (8),
we obtain the final convex objective as follows:
min
?,Yj
?
2N ?
?? +
N
?
i=1
max
L
k
?
m=1
k
?
n=1
?(Li,m,n, Yi,m,n)?
diff(?, Yi,m,n, Li,m,n) + ?2U ?
?? + (10)
?
2
U
?
j=1
k
?
m=1
k
?
n=1
(
??f(Xj,m ? Xj,n)? Yj,m,n
)2
subject to constraints on Y (explained above),
where diff(?, Yi,m,n, Li,m,n) = score(?, Yi,m,n) ?
536
score(?, Li,m,n), N and U are the number of labeled
and unlabeled training sentences respectively, as we
mentioned before. Note that in (10) we have split
the regularizer into two parts; one for the supervised
component of the objective, and the other for the
unsupervised component. Thus the semi-supervised
convex objective is regularized proportionally to the
number of labeled and unlabeled training sentences.
6 Efficient Optimization Strategy
To solve the convex optimization problem shown in
Equation (10), we used a gradient descent approach
which simply uses stochastic gradient steps. The
procedure is as follows.
? Step 0, initialize the Yj variables of each
unlabeled sentence as a right-branching (left-
headed) chain model, i.e. the head of each word
is its left neighbor.
? Step 1, pass through all the labeled training sen-
tences one by one. The parameters ? are up-
dated based on each labeled sentence.
? Step 2, based on the learned parameter weights
from the labeled data, update ? and Yj on each
unlabeled sentence alternatively:
? treat Yj as a constant, update ? on each
unlabeled sentence by taking a local gra-
dient step;
? treat ? as a constant, update Yj by call-
ing the optimization software package
CPLEX to solve for an optimal local so-
lution.
? Repeat the procedure of step 1 and step 2 until
maximum iteration number has reached.
This procedure works efficiently on the task of
training a dependency parser. Although ? and
Yj are updated locally on each sentence, progress
in minimizing the total objective shown in Equa-
tion (10) is made in each iteration. In our experi-
ments, the objective usually converges within 30 it-
erations.
7 Experimental Results
Given a convex approach to semi-supervised struc-
tured large margin training, and an efficient training
algorithm for achieving a global optimum, we now
investigate its effectiveness for dependency parsing.
In particular, we investigate the accuracy of the re-
sults it produces. We applied the resulting algorithm
to learn dependency parsers for both English and
Chinese.
7.1 Experimental Design
Data Sets
Since we use a semi-supervised approach, both la-
beled and unlabeled training data are needed. For
experiment on English, we used the English Penn
Treebank (PTB) (Marcus et al, 1993) and the con-
stituency structures were converted to dependency
trees using the same rules as (Yamada and Mat-
sumoto, 2003). The standard training set of PTB
was spit into 2 parts: labeled training data?the
first 30k sentences in section 2-21, and unlabeled
training data?the remaining sentences in section
2-21. For Chinese, we experimented on the Penn
Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004)
and we used the rules in (Bikel, 2004) for conver-
sion. We also divided the standard training set into
2 parts: sentences in section 400-931 and sentences
in section 1-270 are used as labeled and unlabeled
data respectively. For both English and Chinese,
we adopted the standard development and test sets
throughout the literature.
As listed in Table 1 with greater detail, we
experimented with sets of data with different sen-
tence length: PTB-10/CTB4-10, PTB-15/CTB4-15,
PTB-20/CTB4-20, CTB4-40 and CTB4, which
contain sentences with up to 10, 15, 20, 40 and all
words respectively.
Features
For simplicity, in current work, we only used two
sets of features?word-pair and tag-pair indicator
features, which are a subset of features used by
other researchers on dependency parsing (McDon-
ald et al, 2005a; Wang et al, 2007). Although
our algorithms can take arbitrary features, by only
using these simple features, we already obtained
very promising results on dependency parsing
using both the supervised and semi-supervised
approaches. Using the full set of features described
in (McDonald et al, 2005a; Wang et al, 2007) and
comparing the corresponding dependency parsing
537
English
PTB-10
Training(l/ul) 3026/1016
Dev 163
Test 270
PTB-15
Training 7303/2370
Dev 421
Test 603
PTB-20
Training 12519/4003
Dev 725
Test 1034
Chinese
CTB4-10
Training(l/ul) 642/347
Dev 61
Test 40
CTB4-15
Training 1262/727
Dev 112
Test 83
CTB4-20
Training 2038/1150
Dev 163
Test 118
CTB4-40
Training 4400/2452
Dev 274
Test 240
CTB4
Training 5314/2977
Dev 300
Test 289
Table 1: Size of Experimental Data (# of sentences)
results with previous work remains a direction for
future work.
Dependency Parsing Algorithms
For simplicity of implementation, we use a stan-
dard CKY parser in the experiments, although
Eisner?s algorithm (Eisner, 1996) and the Spanning
Tree algorithm (McDonald et al, 2005b) are also
applicable.
7.2 Results
We evaluate parsing accuracy by comparing the di-
rected dependency links in the parser output against
the directed links in the treebank. The parameters
? and ? which appear in Equation (10) were tuned
on the development set. Note that, during training,
we only used the raw sentences of the unlabeled
data. As shown in Table 2 and Table 3, for each
data set, the semi-supervised approach achieves a
significant improvement over the supervised one in
dependency parsing accuracy on both Chinese and
English. These positive results are somewhat sur-
prising since a very simple loss function was used on
Training Test length Supervised Semi-sup
Train-10 ? 10 82.98 84.50
Train-15 ? 10 84.80 86.93? 15 76.96 80.79
Train-20
? 10 84.50 86.32
? 15 78.77 80.57
? 20 74.89 77.85
Train-40
? 10 84.19 85.71
? 15 78.03 81.21
? 20 76.25 77.79
? 40 68.17 70.90
Train-all
? 10 82.67 84.80
? 15 77.92 79.30
? 20 77.30 77.24
? 40 70.11 71.90
all 66.30 67.35
Table 2: Supervised and Semi-supervised Dependency
Parsing Accuracy on Chinese (%)
Training Test length Supervised Semi-sup
Train-10 ? 10 87.77 89.17
Train-15 ? 10 88.06 89.31? 15 81.10 83.37
Train-20
? 10 88.78 90.61
? 15 83.00 83.87
? 20 77.70 79.09
Table 3: Supervised and Semi-supervised Dependency
Parsing Accuracy on English (%)
538
the unlabeled data. A key benefit of the approach is
that a straightforward training algorithm can be used
to obtain global solutions. Note that the results of
our model are not directly comparable with previous
parsing results shown in (McClosky et al, 2006a),
since the parsing accuracy is measured in terms of
dependency relations while their results are f -score
of the bracketings implied in the phrase structure.
8 Conclusion and Future Work
In this paper, we have presented a novel algorithm
for semi-supervised structured large margin training.
Unlike previous proposed approaches, we introduce
a convex objective for the semi-supervised learning
algorithm by combining a convex structured SVM
loss and a convex least square loss. This new semi-
supervised algorithm is much more computationally
efficient and can easily scale up. We have proved our
hypothesis by applying the algorithm to the signifi-
cant task of dependency parsing. The experimental
results show that the proposed semi-supervised large
margin training algorithm outperforms the super-
vised one, without much additional computational
cost.
There remain many directions for future work.
One obvious direction is to use the whole Penn Tree-
bank as labeled data and use some other unannotated
data source as unlabeled data for semi-supervised
training. Next, as we mentioned before, a much
richer feature set can be used in our model to get
better dependency parsing results. Another direc-
tion is to apply the semi-supervised algorithm to
other natural language problems, such as machine
translation, topic segmentation and chunking. In
these areas, there are only limited annotated data
available. Therefore semi-supervised approaches
are necessary to achieve better performance. The
proposed semi-supervised convex training approach
can be easily applied to these tasks.
Acknowledgments
We thank the anonymous reviewers for their useful
comments. Research is supported by the Alberta In-
genuity Center for Machine Learning, NSERC, MI-
TACS, CFI and the Canada Research Chairs pro-
gram. The first author was also funded by the Queen
Elizabeth II Graduate Scholarship.
References
S. Abney. 2004. Understanding the yarowsky algorithm.
Computational Linguistics, 30(3):365?395.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Proceedings of Advances in Neural In-
formation Processing Systems 18.
K. Bennett and A. Demiriz. 1998. Semi-supervised sup-
port vector machines. In Proceedings of Advances in
Neural Information Processing Systems 11.
D. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4).
O. Chapelle and A. Zien. 2005. Semi-supervised clas-
sification by low density separation. In Proceedings
of the Tenth International Workshop on Artificial In-
teligence and Statistics.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the Association for the Advancement of Artificial In-
telligence, pages 598?603.
R. Duda, P. Hart, and D. Stork. 2000. Pattern Classifica-
tion. Wiley, second edition.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
the International Conference on Computational Lin-
guistics.
G. Haffari and A. Sarkar. 2007. Analysis of semi-
supervised learning with the yarowsky algorithm. In
Proceedings of the Conference on Uncertainty in Arti-
ficial Intelligence.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. 2004.
The entire regularization path for the support vector
machine. Journal of Machine Learning Research,
5:1391?1415.
D. Klein and C. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
D. Klein and C. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proceedingsof the Annual Meeting of the
Association for Computational Linguistics.
G. S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In Proceedings of International Confer-
ence on Machine Learning.
C. Manning and H. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
539
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006a. Ef-
fective self-training for parsing. In Proceedings of the
Human Language Technology: the Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
Proceedings of the International Conference on Com-
putational Linguistics and the Annual Meeting of the
Association for Computational Linguistics.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of European Chapter of the Annual Meeting
of the Association for Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of Human Language
Technologies and Conference on Empirical Methods
in Natural Language Processing.
M. Palmer et al 2004. Chinese Treebank 4.0. Linguistic
Data Consortium.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the European Chapter of
the Annual Meeting of the Association for Computa-
tional Linguistics, pages 331?338.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Proceedings of Advances
in Neural Information Processing Systems 16.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
International Conference on Machine Learning.
Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly
lexical dependency parsing. In Proceedings of the In-
ternational Workshop on Parsing Technologies, pages
152?159.
Q. Wang, C. Cherry, D. Lizotte, and D. Schuurmans.
2006. Improved large margin dependency parsing via
local constraints and Laplacian regularization. In Pro-
ceedings of The Conference on Computational Natural
Language Learning, pages 21?28.
Q. Wang, D. Lin, and D. Schuurmans. 2007. Simple
training of dependency parsers via structured boosting.
In Proceedings of the International Joint Conference
on Artificial Intelligence, pages 1756?1762.
L. Xu and D. Schuurmans. 2005. Unsupervised and
semi-supervised multi-class support vector machines.
In Proceedings the Association for the Advancement of
Artificial Intelligence.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of the International Workshop on Parsing
Technologies.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the Annual Meeting of the Association for Com-
putational Linguistics, pages 189?196, Cambridge,
Massachusetts.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings of International Con-
ference on Machine Learning.
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical report, Computer Sciences, University
of Wisconsin-Madison.
540
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 152?159,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Strictly Lexical Dependency Parsing 
 
 
Qin Iris Wang and Dale Schuurmans Dekang Lin 
Department of Computing Science Google, Inc. 
University of Alberta 1600 Amphitheatre Parkway 
Edmonton, Alberta, Canada, T6G 2E8 Mountain View, California, USA, 94043 
{wqin,dale}@cs.ualberta.ca lindek@google.com 
 
 
 
 
Abstract 
We present a strictly lexical parsing 
model where all the parameters are based 
on the words. This model does not rely 
on part-of-speech tags or grammatical 
categories. It maximizes the conditional 
probability of the parse tree given the 
sentence. This is in contrast with most 
previous models that compute the joint 
probability of the parse tree and the sen-
tence. Although the maximization of 
joint and conditional probabilities are 
theoretically equivalent, the conditional 
model allows us to use distributional 
word similarity to generalize the ob-
served frequency counts in the training 
corpus. Our experiments with the Chi-
nese Treebank show that the accuracy of 
the conditional model is 13.6% higher 
than the joint model and that the strictly 
lexicalized conditional model outper-
forms the corresponding unlexicalized 
model based on part-of-speech tags. 
1 Introduction 
There has been a great deal of progress in statisti-
cal parsing in the past decade (Collins, 1996; 
Collins, 1997; Chaniak, 2000). A common charac-
teristic of these parsers is their use of lexicalized 
statistics. However, it was discovered recently that 
bi-lexical statistics (parameters that involve two 
words) actually played much smaller role than 
previously believed.  It was found in (Gildea, 
2001) that the removal of bi-lexical statistics from 
a state-of-the-art PCFG parser resulted very small 
change in the output. Bikel (2004) observed that 
the bi-lexical statistics accounted for only 1.49% 
of the bigram statistics used by the parser. When 
considering only bigram statistics involved in the 
highest probability parse, this percentage becomes 
28.8%. However, even when the bi-lexical statis-
tics do get used, they are remarkably similar to 
their back-off values using part-of-speech tags. 
Therefore, the utility of bi-lexical statistics be-
comes rather questionable. Klein and Manning 
(2003) presented an unlexicalized parser that 
eliminated all lexicalized parameters. Its perform-
ance was close to the state-of-the-art lexicalized 
parsers. 
We present a statistical dependency parser that 
represents the other end of spectrum where all 
statistical parameters are lexical and the parser 
does not require part-of-speech tags or grammati-
cal categories. We call this strictly lexicalized 
parsing. 
A part-of-speech lexicon has always been con-
sidered to be a necessary component in any natu-
ral language parser. This is true in early rule-based 
as well as modern statistical parsers and in de-
pendency parsers as well as constituency parsers. 
The need for part-of-speech tags arises from the 
sparseness of natural language data. They provide 
generalizations of words that are critical for pars-
ers to deal with the sparseness. Words belonging 
to the same part-of-speech are expected to have 
the same syntactic behavior. 
Instead of part-of-speech tags, we rely on dis-
tributional word similarities computed automati-
cally from a large unannotated text corpus. One of 
the benefits of strictly lexicalized parsing is that 
152
 fundsinvestors continue  to  pour cash into moneyMany? 
0 1 2 3 4 5 6 7 8 9
 
 
 
 
the parser can be trained with a treebank that only 
contains the dependency relationships between 
words. The annotators do not need to annotate 
parts-of-speech or non-terminal symbols (they 
don?t even have to know about them), making the 
construction of the treebank easier.  
Strictly lexicalized parsing is especially benefi-
cial for languages such as Chinese, where parts-
of-speech are not as clearly defined as English. In 
Chinese, clear indicators of a word's part-of-
speech such as suffixes -ment, -ous or function 
words such as the, are largely absent. In fact, 
monolingual Chinese dictionaries that are mainly 
intended for native speakers almost never contain 
part-of-speech information. 
In the next section, we present a method for 
modeling the probabilities of dependency trees. 
Section 3 applies similarity-based smoothing to 
the probability model to deal with data sparseness. 
We then present experimental results with the 
Chinese Treebank in Section 4 and discuss related 
work in Section 5.  
2 A Probabilistic Dependency Model 
Let S be a sentence. The dependency structure T 
of S is a directed tree connecting the words in S. 
Each link in the tree represents a dependency rela-
tionship between two words, known as the head 
and the modifier. The direction of the link is from 
the head to the modifier. We add an artificial root 
node (?) at the beginning of each sentence and a 
dependency link from ? to the head of the sen-
tence so that the head of the sentence can be 
treated in the same way as other words. Figure 1 
shows an example dependency tree. 
We denote a dependency link l by a triple (u, v, 
d), where u and v are the indices (u < v) of the 
words connected by l, and d specifies the direction 
of the link l. The value of d is either L or R. If d = 
L, v is the index of the head word; otherwise, u is 
the index of the head word.  
Dependency trees are typically assumed to be 
projective (without crossing arcs), which means 
that if there is an arc from h to m, h is an ancestor 
of all the words between h and m. Let F(S) be the 
set of possible directed, projective trees spanning 
on S. The parsing problem is to find  
 
( ) ( )STPSFT |maxarg ?  
 
 Generative parsing models are usually defined 
recursively from top down, even though the de-
coders (parsers) for such models almost always 
take a bottom-up approach. The model proposed 
here is a bottom-up one. Like previous ap-
proaches, we decompose the generation of a parse 
tree into a sequence of steps and define the prob-
ability of each step.  The probability of the tree is 
simply the product of the probabilities of the steps 
involved in the generation process. This scheme 
requires that different sequences of steps must not 
lead to the same tree. We achieve this by defining 
a canonical ordering of the links in a dependency 
tree. Each generation step corresponds to the con-
struction of a dependency link in the canonical 
order. 
Given two dependency links l and l' with the 
heads being h and h' and the modifiers being m 
and m', respectively, the order between l and l' are 
determined as follows: 
? If h ? h' and there is a directed path from one 
(say h) to the other (say h?), then l? precedes l. 
? If h ? h' and there does not exist a directed path 
between h and h?, the order between l and l? is 
determined by the order of h and h? in the sen-
tence (h precedes h? ? l precedes l?). 
? If h = h' and the modifiers m and m? are on dif-
ferent sides of h, the link with modifier on the 
right precedes the other. 
? If h = h' and the modifiers m and m? are on the 
same side of the head h, the link with its modi-
fier closer to h precedes the other one. 
 
Figure 1. An Example Dependency Tree. 
153
For example, the canonical order of the links in 
the dependency tree in Figure 1 is: (1, 2, L), (5, 6, 
R), (8, 9, L), (7, 9, R), (5, 7, R), (4, 5, R), (3, 4, 
R), (2, 3, L), (0, 3, L). 
The generation process according to the ca-
nonical order is similar to the head outward gen-
eration process in (Collins, 1999), except that it is 
bottom-up whereas Collins? models are top-down. 
Suppose the dependency tree T is constructed in 
steps G1, ?, GN in the canonical order of the de-
pendency links, where N is the number of words 
in the sentence. We can compute the probability 
of T as follows: 
( )
( )
( )? = ?=
=
N
i ii
N
GGSGP
SGGGP
STP
1 11
21
,...,,|
|,...,,
|
 
Following (Klein and Manning, 2004), we re-
quire that the creation of a dependency link from 
head h to modifier m be preceded by placing a left 
STOP and a right STOP around the modifier m 
and ?STOP between h and m. 
Let LwE  (and 
R
wE ) denote the event that there 
are no more modifiers on the left (and right) of a 
word w. Suppose the dependency link created in 
the step i is (u, v, d).  If d = L, Gi is the conjunc-
tion of the four events: RuE , 
L
uE , 
L
vE? and 
linkL(u, v). If d = R, Gi consists of four events: 
L
vE ,
R
vE , 
R
uE? and linkR(u, v).  
The event Gi is conditioned on 11,...,, ?iGGS , 
which are the words in the sentence and a forest of 
trees constructed up to step i-1. Let LwC  (and 
R
wC ) 
be the number of modifiers of w on its left (and 
right). We make the following independence as-
sumptions: 
? Whether there is any more modifier of w on 
the d side depends only on the number of 
modifiers already found on the d side of w. 
That is, dwE  depends only on w and 
d
wC .  
? Whether there is a dependency link from a 
word h to another word m depends only on the 
words h and m and the number of modifiers of 
h between m and h. That is,  
o linkR(u,v) depends only on u, v, and RuC . 
o linkL(u,v) depends only on u, v, and LvC . 
Suppose Gi corresponds to a dependency link (u, 
v, L). The probability ( )11,...,,| ?ii GGSGP  can be 
computed as: 
 
 
( )
( )( )
( ) ( )
( )( ) ( )( )LvLLvLv
R
u
R
u
L
u
L
u
iL
L
v
R
u
L
u
ii
CvuvulinkPCvEP
CuEPCuEP
GGSvulinkEEEP
GGSGP
,,|,,|1
,|,|
,...,,|,,,,
,...,,|
11
11
??
??=
?= ?
?
 
 
The events RwE  and 
L
wE  correspond to the 
STOP events in (Collins, 1999) and (Klein and 
Manning, 2004). They are crucial for modeling 
the number of dependents. Without them, the 
parse trees often contain some ?obvious? errors, 
such as determiners taking arguments, or preposi-
tions having arguments on their left (instead of 
right). 
Our model requires three types of parameters: 
? ( )dwdw CwEP ,| , where w is a word, d is a di-
rection (left or right). This is the probability of 
a STOP after taking dwC  modifiers on the d 
side. 
? ( )( )RuR CvuvulinkP ,,|,  is the probability of v 
being the ( 1+RuC )?th modifier of u on the 
right. 
? ( )( )LvL CvuvulinkP ,,|,  is the probability of u 
being the ( 1+LvC )?th modifier of v on the 
left. 
 
The Maximum Likelihood estimations of these 
parameters can be obtained from the frequency 
counts in the training corpus: 
? C(w, c, d): the frequency count of  w with c 
modifiers on the d side. 
? C(u, v, c, d): If d = L, this is the frequency 
count words u and v co-occurring in a sen-
tence and v has c modifiers between itself and 
u. If d = R, this is the frequency count words u 
and v co-occurring in a sentence and u has c 
modifiers between itself and v. 
? K(u, v, c, d): similar to C(u, v, c, d) with an 
additional constraint that linkd(u, v) is true. 
154
( ) ( )( )?
?
=
cc
d
w
d
w dcwC
dcwCCwEP
'
,',
,,,|  , where c = dwC ; 
( )( ) ( )( )RcvuC RcvuKCvuvulinkP RuR ,,, ,,,,,|, = ,   
where  c = RuC ; 
( )( ) ( )( )LcvuC LcvuKCvuvulinkP LvL ,,, ,,,,,|, = ,   
where  c = LvC . 
We compute the probability of the tree condi-
tioned on the words. All parameters in our model 
are conditional probabilities where the left sides of 
the conditioning bar are binary variables. In con-
trast, most previous approaches compute joint 
probability of the tree and the words in the tree. 
Many of their model parameters consist of the 
probability of a word in a given context. 
We use a dynamic programming algorithm 
similar to chart parsing as the decoder for this 
model. The algorithm builds a packed parse forest 
from bottom up in the canonical order of the 
parser trees. It attaches all the right children be-
fore attaching the left ones to maintain the canoni-
cal order as required by our model.  
3 Similarity-based Smoothing 
3.1 Distributional Word Similarity 
Words that tend to appear in the same contexts 
tend to have similar meanings. This is known as 
the Distributional Hypothesis in linguistics (Harris, 
1968). For example, the words test and exam are 
similar because both of them follow verbs such as 
administer, cancel, cheat on, conduct, ... and both of 
them can be preceded by adjectives such as aca-
demic, comprehensive, diagnostic, difficult, ... 
Many methods have been proposed to compute 
distributional similarity between words (Hindle, 
1990; Pereira et al, 1993; Grefenstette, 1994; Lin, 
1998). Almost all of the methods represent a word 
by a feature vector where each feature corre-
sponds to a type of context in which the word ap-
peared. They differ in how the feature vectors are 
constructed and how the similarity between two 
feature vectors is computed.   
We define the features of a word w to be the set 
of words that occurred within a small context win-
dow of w in a large corpus. The context window 
of an instance of w consists of the closest non-
stop-word on each side of w and the stop-words in 
between. In our experiments, the set of stop-words 
are defined as the top 100 most frequent words in 
the corpus. The value of a feature w' is defined as 
the point-wise mutual information between the w' 
and w: 
( ) ( )( ) ( )???
????
??=
'
',log',
wPwP
wwPwwPMI  
where P(w, w?) is the probability of w and w? co-
occur in a context window. 
The similarity between two vectors is computed 
as the cosine of the angle between the vectors. 
The following are the top similar words for the 
word keystone obtained from the English Giga-
word Corpus: 
centrepiece 0.28, figment 0.27, fulcrum 0.21, culmi-
nation 0.20, albatross 0.19, bane 0.19, pariahs 0.18, 
lifeblood 0.18, crux 0.18, redoubling 0.17, apotheo-
sis 0.17, cornerstones 0.17, perpetuation 0.16, fore-
runners 0.16, shirking 0.16, cornerstone 0.16, 
birthright 0.15, hallmark 0.15, centerpiece 0.15, evi-
denced 0.15, germane 0.15, gist 0.14, reassessing 
0.14, engrossed 0.14, Thorn 0.14, biding 0.14, nar-
rowness 0.14, linchpin 0.14, enamored 0.14, formal-
ised 0.14, tenths 0.13, testament 0.13, certainties 
0.13, forerunner 0.13, re-evaluating 0.13, antithetical 
0.12, extinct 0.12, rarest 0.12, imperiled 0.12, remiss 
0.12, hindrance 0.12, detriment 0.12, prouder 0.12, 
upshot 0.12, cosponsor 0.12, hiccups 0.12, premised 
0.12, perversion 0.12, destabilisation 0.12, prefaced 
0.11, ?? 
3.2 Similarity-based Smoothing 
The parameters in our model consist of condi-
tional probabilities P(E|C) where E is the binary 
variable linkd(u, v) or dwE  and the context C is 
either [ ]dwCw,  or [ ]dwCvu ,, , which involves one 
or two words in the input sentence. Due to the 
sparseness of natural language data, the contexts 
observed in the training data only covers a tiny 
fraction of the contexts whose probability distri-
bution are needed during parsing. The standard 
approach is to back off the probability to word 
classes (such as part-of-speech tags). We have 
taken a different approach. We search in the train-
155
ing data to find a set of similar contexts to C and 
estimate the probability of E based on its prob-
abilities in the similar contexts that are observed 
in the training corpus. 
Similarity-based smoothing was used in (Dagan 
et al, 1999) to estimate word co-occurrence prob-
abilities. Their method performed almost 40% 
better than the more commonly used back-off 
method. Unfortunately, similarity-based smooth-
ing has not been successfully applied to statistical 
parsing up to now.  
In (Dagan et al, 1999), the bigram probability 
P(w2|w1) is computed as the weighted average of 
the conditional probability of w2 given similar 
words of w1. 
( ) ( )( ) ( )( )??= 11' 121 1112 '|
',|
wSw
MLESIM wwPwnorm
wwsimwwP  
where ( )11 ', wwsim  denotes the similarity (or an 
increasing function of the similarity) between w1 
and w?1, S(w1) denote the set of words that are 
most similar to w1 and norm(w1) is the normaliza-
tion factor ( ) ( )
( )??= 11' 111 ',wSw wwsimwnorm .   
The underlying assumption of this smoothing 
scheme is that a word is more likely to occur after 
w1 if it tends to occur after similar words of w1.  
We make a similar assumption: the probability 
P(E|C) of event E given the context C is computed 
as the weight average of P(E|C?) where C? is a 
similar context of C and is attested in the training 
corpus:  
( ) ( )( ) ( )( )???= OCSC MLESIM CEPCnorm
CCsimCEP
'
'|',|  
where S(C) is the set of top-K most similar con-
texts of C (in the experiments reported in this pa-
per, K = 50); O is the set of contexts observed in 
the training corpus, sim(C,C?) is the similarity 
between two contexts  and  norm(C) is the nor-
malization factor.  
In our model, a context is either  [ ]dwCw,  or [ ]dwCvu ,, . Their similar contexts are defined as:  
[ ]( ) [ ] ( ){ }
[ ]( ) [ ]{ })('),(',',',,
',', '
vSvuSuCvuCvuS
wSwCwCwS
d
w
d
w
d
w
d
w
??=
?=
 
where S(w) is the set of top-K similar words of w 
(K = 50). 
Since all contexts used in our model contain at 
least one word, we compute the similarity be-
tween two contexts, sim(C, C?), as the geometric 
average of the similarities between corresponding 
words: 
[ ] [ ]( ) ( )
[ ] [ ]( ) ( ) ( )',',,',',,,
',,',,
'
'
vvsimuusimCvuCvusim
wwsimCwCwsim
d
w
d
w
d
w
d
w
?=
=
 
Similarity-smoothed probability is only neces-
sary when the frequency count of the context C in 
the training corpus is low. We therefore compute  
P(E | C) = ? PMLE(E | C) + (1 ? ?) PSIM(E | C) 
where the smoothing factor 
5||
1||
+
+=
C
C?  and |C| is 
the frequency count of the context C in the train-
ing data. 
A difference between similarity-based smooth-
ing in (Dagan et al, 1999) and our approach is 
that our model only computes probability distribu-
tions of binary variables. Words only appear as 
parts of contexts on the right side of the condition-
ing bar. This has two important implications. 
Firstly, when a context contains two words, we 
are able to use the cross product of the similar 
words, whereas (Dagan et al, 1999) can only use 
the similar words of one of the words. This turns 
out to have significant impact on the performance 
(see Section 4).  
Secondly, in (Dagan et al, 1999), the distribu-
tion P(?|w?1) may itself be sparsely observed. 
When ( )12 '| wwPMLE  is 0, it is often due to data 
sparseness. Their smoothing scheme therefore 
tends to under-estimate the probability values. 
This problem is avoided in our approach. If a con-
text did not occur in the training data, we do not 
include it in the average. If it did occur, the 
Maximum Likelihood estimation is reasonably 
accurate even if the context only occurred a few 
times, since the entropy of the probability distri-
bution is upper-bounded by log 2. 
4 Experimental Results 
We experimented with our parser on the Chinese 
Treebank (CTB) 3.0.  We used the same data split 
as (Bikel, 2004): Sections 1-270 and 400-931 as 
156
the training set, Sections 271-300 as testing and 
Sections 301-325 as the development set. The 
CTB contains constituency trees. We converted 
them to dependency trees using the same method 
and the head table as (Bikel, 2004).  Parsing Chi-
nese generally involve segmentation as a pre-
processing step. We used the gold standard seg-
mentation in the CTB.  
The distributional similarities between the Chi-
nese words are computed using the Chinese Gi-
gaword corpus. We did not segment the Chinese 
corpus when computing the word similarity.  
We measure the quality of the parser by the un-
directed accuracy, which is defined as the number 
of correct undirected dependency links divided by 
the total number of dependency links in the corpus 
(the treebank parse and the parser output always 
have the same number of links). The results are 
summarized in Table 1. It can be seen that the per-
formance of the parser is highly correlated with 
the length of the sentences. 
 
Max Sentence Length 10 15 20 40 
Undirected Accuracy 90.8 85.6 84.0 79.9 
Table 1. Evaluation Results on CTB 3.0 
 
We also experimented with several alternative 
models for dependency parsing. Table 2 summer-
izes the results of these models on the test corpus 
with sentences up to 40 words long. 
One of the characteristics of our parser is that it 
uses the similar words of both the head and the 
modifier for smoothing. The similarity-based 
smoothing method in (Dagan et al, 1999) uses the 
similar words of one of the words in a bigram. We 
can change the definition of similar context as 
follows so that only one word in a similar context 
of C may be different from a word in C (see 
Model (b) in Table 2): 
[ ]( )
[ ]{ } [ ]{ })(',',)(',,' ,, vSvCvuuSuCvu CvuS dwdw
d
w
???=  
where w is either v or u depending on whether d is 
L or R. This change led to a 2.2% drop in accuracy 
(compared with Model (a) in Table 2), which we 
attribute to the fact that many contexts do not have 
similar contexts in the training corpus.  
Since most previous parsing models maximize 
the joint probability of the parse tree and the sen-
tence P(T, S) instead of P(T | S),  we also imple-
mented a joint model (see Model (c) in Table 2): 
( ) ( ) ( )( )( ) ( )?= ??
??=
N
i
d
hii
d
hi
d
h
R
mi
R
m
L
mi
L
m
i
iii
iiii
ChmPChEP
CmEPCmEP
STP
1 ,|,|1
,|,|
,  
where hi and mi are the head and the modifier of 
the i'th dependency link. The probability ( )i
i
d
hii ChmP ,|  is smoothed by averaging the 
probabilities ( )i
i
d
hii ChmP ,'| , where h?i is a similar 
word of hi, as in (Dagan et al, 1999). The result 
was a dramatic decrease in accuracy from the con-
ditional model?s 79.9%. to 66.3%.  
 Our use of distributional word similarity can 
be viewed as assigning soft clusters to words. In 
contrast, parts-of-speech can be viewed as hard 
clusters of words. We can modify both the condi-
tional and joint models to use part-of-speech tags, 
instead of words. Since there are only a small 
number of tags, the modified models used MLE  
without any smoothing except using a small con-
stant as the probability of unseen events. Without 
smoothing, maximizing the conditional model is 
equivalent to maximizing the joint model. The 
accuracy of the unlexicalized models (see Model 
(d) and Model (e) in Table 2) is 71.1% which is 
considerably lower than the strictly lexicalized 
conditional model, but higher than the strictly 
lexicalized joint model. This demonstrated that 
soft clusters obtained through distributional word 
similarity perform better than the part-of-speech 
tags when used appropriately. 
 
Models Accuracy 
(a) Strictly lexicalized conditional model 79.9 
(b)   At most one word is different in a similar context 77.7 
(c)  Strictly lexicalized  joint model 66.3 
(d)  Unlexicalized conditional mod-els 71.1 
(e)  Unlexicalized joint models 71.1 
Table 2. Performance of Alternative Models 
 
157
5 Related Work  
Previous parsing models (e.g., Collins, 1997; 
Charniak, 2000) maximize the joint probability 
P(S, T) of a sentence S and its parse tree T. We 
maximize the conditional probability P(T | S). Al-
though they are theoretically equivalent, the use of 
conditional model allows us to take advantage of 
similarity-based smoothing. 
Clark et al (2002) also computes a conditional 
probability of dependency structures. While the 
probability space in our model consists of all pos-
sible non-projective dependency trees, their prob-
ability space is constrained to all the dependency 
structures that are allowed by a Combinatorial 
Category Grammar (CCG) and a category diction-
ary (lexicon). They therefore do not need the 
STOP markers in their model. Another major dif-
ference between our model and (Clark et al, 
2002) is that the parameters in our model consist 
exclusively of conditional probabilities of binary 
variables. 
Ratnaparkhi?s maximum entropy model (Rat-
naparkhi, 1999) is also a conditional model. How-
ever, his model maximizes the probability of the 
action during each step of the parsing process, 
instead of overall quality of the parse tree.  
Yamada and Matsumoto (2002) presented a de-
pendency parsing model using support vector ma-
chines. Their model is a discriminative model that 
maximizes the differences between scores of the 
correct parse and the scores of the top competing 
incorrect parses.  
In many dependency parsing models such as 
(Eisner, 1996) and (MacDonald et al, 2005), the 
score of a dependency tree is the sum of the scores 
of the dependency links, which are computed in-
dependently of other links. An undesirable conse-
quence of this is that the parser often creates 
multiple dependency links that are separately 
likely but jointly improbable (or even impossible). 
For example, there is nothing in such models to 
prevent the parser from assigning two subjects to 
a verb. In the DMV model (Klein and Manning, 
2004), the probability of a dependency link is 
partly conditioned on whether or not there is a 
head word of the link already has a modifier. Our 
model is quite similar to the DMV model, except 
that we compute the conditional probability of the 
parse tree given the sentence, instead of the joint 
probability of the parse tree and the sentence. 
There have been several previous approaches to 
parsing Chinese with the Penn Chinese Treebank 
(e.g., Bikel and Chiang, 2000; Levy and Manning, 
2003). Both of these approaches employed phrase-
structure joint models and used part-of-speech 
tags in back-off smoothing. Their results were 
evaluated with the precision and recall of the 
bracketings implied in the phrase structure parse 
trees. In contrast, the accuracy of our model is 
measured in terms of the dependency relation-
ships. A dependency tree may correspond to more 
than one constituency trees.  Our results are there-
fore not directly comparable with the precision 
and recall values in previous research. Moreover, 
it was argued in (Lin 1995) that dependency based 
evaluation is much more meaningful for the appli-
cations that use parse trees, since the semantic 
relationships are generally embedded in the de-
pendency relationships. 
6 Conclusion 
To the best of our knowledge, all previous natural 
language parsers have to rely on part-of-speech 
tags. We presented a strictly lexicalized model for 
dependency parsing that only relies on word sta-
tistics. We compared our parser with an unlexical-
ized parser that employs the same probabilistic 
model except that the parameters are estimated 
using gold standard tags in the Chinese Treebank. 
Our experiments show that the strictly lexicalized 
parser significantly outperformed its unlexicalized 
counter-part. 
An important distinction between our statistical 
model from previous parsing models is that all the 
parameters in our model are conditional probabil-
ity of binary variables. This allows us to take ad-
vantage of similarity-based smoothing, which has 
not been successfully applied to parsing before. 
Acknowledgements 
The authors would like to thank Mark Steedman 
for suggesting the comparison with unlexicalized 
parsing in Section 4 and the anonymous reviewers 
for their comments. This work was supported in 
part by NSERC, the Alberta Ingenuity Centre for 
Machine Learning and the Canada Research 
158
Chairs program. Qin Iris Wang was also sup-
ported by iCORE Scholarship. 
References  
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing 
Model. Computational Linguistics, 30(4), pp. 479-
511.  
Daniel M. Bikel and David Chiang. 2000. Two Statisti-
cal Parsing Models applied to the Chinese Treebank. 
In Proceedings of the second Chinese Language 
Processing Workshop, pp. 1-6.  
Eugene Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. In Proceedings of the Second Meeting of 
North American Chapter of Association for Compu-
tational Linguistics (NAACL-2000), pp. 132-139.  
Stephen Clark, Julia Hockenmaier and Mark Steedman. 
2002. Building Deep Dependency Structures with a 
Wide-Coverage CCG Parser. In Proceedings of the 
40th Annual Meeting of the ACL, pp. 327-334.  
Michael Collins. 1996. A New Statistical Parser Based 
on Bigram Lexical Dependencies. In Proceedings of 
the 34th Annual Meeting of the ACL, pp. 184-191. 
Santa Cruz.  
Michael Collins. 1997. Three Generative, Lexicalised 
Models for Statistical Parsing. In Proceedings of the 
35th Annual Meeting of the ACL (jointly with the 8th 
Conference of the EACL), pp. 16-23. Madrid.  
Michael Collins. 1999. Head-Driven Statistical Models 
for Natural Language Parsing. PhD Dissertation, 
University of Pennsylvania.  
Ido Dagan, Lillian Lee and Fernando Pereira. 1999. 
Similarity-based models of cooccurrence probabili-
ties. Machine Learning, Vol. 34(1-3) special issue 
on Natural Language Learning, pp. 43-69.  
Jason M. Eisner. 1996. Three new probabilistic models 
for dependency parsing: An exploration. In Proceed-
ings of COLING-96, pp. 340-345, Copenhagen.  
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Proceedings of EMNLP-2001, pp. 167-
202. Pittsburgh, PA.  
Gregory Grefenstette. 1994. Explorations in Automatic 
Thesaurus Discovery. Kluwer Academic Press, Bos-
ton, MA.  
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.  
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In Proceedings of ACL-
90, pp. 268-275. Pittsburg, Pennsylvania.  
Dan Klein and Chris Manning. 2002. Fast exact infer-
ence with a factored model for natural language 
parsing. In Proceedings of Neural Information 
Processing Systems.  
Dan Klein and Chris Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual 
Meeting of the ACL, pp. 423-430.  
Dan Klein and Chris Manning. 2004. Corpus-Based 
Induction of Syntactic Structure: Models of De-
pendency and Constituency. In Proceedings of the 
42nd Annual Meeting of the ACL, pp. 479-486.  
Roger Levy and Chris Manning. 2003. Is it harder to 
parse Chinese, or the Chinese Treebank?  In Pro-
ceedings of the 41st Annual Meeting of the ACL, pp. 
439-446. 
Dekang Lin. 1995. A dependency-based method for 
evaluating broad-coverage parsers. In Proceedings 
of IJCAI-95, pp.1420-1425.  
Dekang Lin. 1998. Automatic Retrieval and Clustering 
of Similar Words. In Proceeding of COLING-
ACL98, pp. 768-774. Montreal, Canada.  
Ryan McDonald, Koby Crammer, and Fernando 
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005, pp. 
91-98.  
Fernando Pereira, Naftali Z. Tishby, and Lillian Lee. 
1993. Distributional clustering of English words. In 
Proceedings of ACL-1993, pp. 183-190, Columbus, 
Ohio.  
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal Dependency Analysis with Support Vector Ma-
chines. In Proceedings of the 8th International 
Workshop on Parsing Technologies, pp.195-206.  
159
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 21?28, New York City, June 2006. c?2006 Association for Computational Linguistics
Improved Large Margin Dependency Parsing
via Local Constraints and Laplacian Regularization
Qin Iris Wang Colin Cherry Dan Lizotte Dale Schuurmans
Department of Computing Science
University of Alberta
 
wqin,colinc,dlizotte,dale  @cs.ualberta.ca
Abstract
We present an improved approach for
learning dependency parsers from tree-
bank data. Our technique is based on two
ideas for improving large margin train-
ing in the context of dependency parsing.
First, we incorporate local constraints that
enforce the correctness of each individ-
ual link, rather than just scoring the global
parse tree. Second, to cope with sparse
data, we smooth the lexical parameters ac-
cording to their underlying word similar-
ities using Laplacian Regularization. To
demonstrate the benefits of our approach,
we consider the problem of parsing Chi-
nese treebank data using only lexical fea-
tures, that is, without part-of-speech tags
or grammatical categories. We achieve
state of the art performance, improving
upon current large margin approaches.
1 Introduction
Over the past decade, there has been tremendous
progress on learning parsing models from treebank
data (Collins, 1997; Charniak, 2000; Wang et al,
2005; McDonald et al, 2005). Most of the early
work in this area was based on postulating gener-
ative probability models of language that included
parse structure (Collins, 1997). Learning in this con-
text consisted of estimating the parameters of the
model with simple likelihood based techniques, but
incorporating various smoothing and back-off esti-
mation tricks to cope with the sparse data problems
(Collins, 1997; Bikel, 2004). Subsequent research
began to focus more on conditional models of parse
structure given the input sentence, which allowed
discriminative training techniques such as maximum
conditional likelihood (i.e. ?maximum entropy?)
to be applied (Ratnaparkhi, 1999; Charniak, 2000).
In fact, recently, effective conditional parsing mod-
els have been learned using relatively straightfor-
ward ?plug-in? estimates, augmented with similar-
ity based smoothing (Wang et al, 2005). Currently,
the work on conditional parsing models appears to
have culminated in large margin training (Taskar
et al, 2003; Taskar et al, 2004; Tsochantaridis et
al., 2004; McDonald et al, 2005), which currently
demonstrates the state of the art performance in En-
glish dependency parsing (McDonald et al, 2005).
Despite the realization that maximum margin
training is closely related to maximum conditional
likelihood for conditional models (McDonald et
al., 2005), a sufficiently unified view has not yet
been achieved that permits the easy exchange of
improvements between the probabilistic and non-
probabilistic approaches. For example, smoothing
methods have played a central role in probabilistic
approaches (Collins, 1997; Wang et al, 2005), and
yet they are not being used in current large margin
training algorithms. However, as we demonstrate,
not only can smoothing be applied in a large mar-
gin training framework, it leads to generalization im-
provements in much the same way as probabilistic
approaches. The second key observation we make is
somewhat more subtle. It turns out that probabilistic
approaches pay closer attention to the individual er-
rors made by each component of a parse, whereas
the training error minimized in the large margin
approach?the ?structured margin loss? (Taskar et
al., 2003; Tsochantaridis et al, 2004; McDonald et
al., 2005)?is a coarse measure that only assesses
the total error of an entire parse rather than focusing
on the error of any particular component.
21
 funds?Investors?continue? to? pour?cash?into?money?
Figure 1: A dependency tree
In this paper, we make two contributions to the
large margin approach to learning parsers from su-
pervised data. First, we show that smoothing based
on lexical similarity is not only possible in the large
margin framework, but more importantly, allows
better generalization to new words not encountered
during training. Second, we show that the large mar-
gin training objective can be significantly refined to
assess the error of each component of a given parse,
rather than just assess a global score. We show that
these two extensions together lead to greater train-
ing accuracy and better generalization to novel input
sentences than current large margin methods.
To demonstrate the benefit of combining useful
learning principles from both the probabilistic and
large margin frameworks, we consider the prob-
lem of learning a dependency parser for Chinese.
This is an interesting test domain because Chinese
does not have clearly defined parts-of-speech, which
makes lexical smoothing one of the most natural ap-
proaches to achieving reasonable results (Wang et
al., 2005).
2 Lexicalized Dependency Parsing
A dependency tree specifies which words in a sen-
tence are directly related. That is, the dependency
structure of a sentence is a directed tree where the
nodes are the words in the sentence and links rep-
resent the direct dependency relationships between
the words; see Figure 1. There has been a grow-
ing interest in dependency parsing in recent years.
(Fox, 2002) found that the dependency structures
of a pair of translated sentences have a greater de-
gree of cohesion than phrase structures. (Cherry and
Lin, 2003) exploited such cohesion between the de-
pendency structures to improve the quality of word
alignment of parallel sentences. Dependency rela-
tions have also been found to be useful in informa-
tion extraction (Culotta and Sorensen, 2004; Yan-
garber et al, 2000).
A key aspect of a dependency tree is that it does
not necessarily report parts-of-speech or phrase la-
bels. Not requiring parts-of-speech is especially
beneficial for languages such as Chinese, where
parts-of-speech are not as clearly defined as En-
glish. In Chinese, clear indicators of a word?s part-
of-speech such as suffixes ?-ment?, ?-ous? or func-
tion words such as ?the?, are largely absent. One
of our motivating goals is to develop an approach to
learning dependency parsers that is strictly lexical.
Hence the parser can be trained with a treebank that
only contains the dependency relationships, making
annotation much easier.
Of course, training a parser with bare word-to-
word relationships presents a serious challenge due
to data sparseness. It was found in (Bikel, 2004) that
Collins? parser made use of bi-lexical statistics only
1.49% of the time. The parser has to compute back-
off probability using parts-of-speech in vast majority
of the cases. In fact, it was found in (Gildea, 2001)
that the removal of bi-lexical statistics from a state
of the art PCFG parser resulted in very little change
in the output. (Klein and Manning, 2003) presented
an unlexicalized parser that eliminated all lexical-
ized parameters. Its performance was close to the
state of the art lexicalized parsers.
Nevertheless, in this paper we follow the re-
cent work of (Wang et al, 2005) and consider a
completely lexicalized parser that uses no parts-of-
speech or grammatical categories of any kind. Even
though a part-of-speech lexicon has always been
considered to be necessary in any natural language
parser, (Wang et al, 2005) showed that distributional
word similarities from a large unannotated corpus
can be used to supplant part-of-speech smoothing
with word similarity smoothing, to still achieve state
of the art dependency parsing accuracy for Chinese.
Before discussing our modifications to large mar-
gin training for parsing in detail, we first present the
dependency parsing model we use. We then give
a brief overview of large margin training, and then
present our two modifications. Subsequently, we
present our experimental results on fully lexical de-
pendency parsing for Chinese.
3 Dependency Parsing Model
Given a sentence   
		 we are in-
terested in computing a directed dependency tree,
22
 , over  . In particular, we assume that a di-
rected dependency tree  consists of ordered pairs
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 7?8,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Recent Advances in Dependency Parsing
Qin Iris Wang, AT&T Interactive
Yue Zhang, Oxford
   Data-driven (statistical) approaches have been playing an
increasingly prominent role in parsing since the 1990s. In recent
years, there has been a growing interest in dependency-based as
opposed to constituency-based approaches to syntactic parsing, with
application to a wide range of research areas and different languages.
Graph-based and transition-based methods are the two dominant
data-driven approaches to dependency parsing. In a graph-based model,
it defines a space of candidate dependency trees for a given sentence.
Each candidate tree is scored via a local or global scoring function.
The parser (usually uses dynamic programming) outputs the
highest-scored tree. In contrast, in a transition-based model, it
defines a transition system for mapping a sentence to its dependency
tree.  It induces a model for predicting the next state transition,
given the transition history. Given the induced model, the output
parse tree is built deterministically upon the construction of the
optimal transition sequence.
   Both Graph-based and transition-based approaches have been used to
achieve state-of-the-art dependency parsing results for a wide range
of languages. Some researchers have used the combination of the two
models and it shows the performance of the combined model is
significantly better than the individual models. Another recent trend
is to apply online training to shift-reduce parsing in the
transition-based models. In this tutorial, we first introduce the two
main-stream data-driven dependency parsing models--- graph-based and
transition-based models. After comparing the differences between them,
we show how these two models can be combined in various ways to
achieve better results.
Outline
Part A: Introduction to Dependency Parsing
Part B: Graph-based Dependency Parsing Models
- Learning Algorithms (Local Learning vs. Global Learning)
- Parsing Algorithms (Dynamic Programming)
- Features (Static Features vs. Dynamic Features)
7
Part C: Transition-based Dependency Parsing Models
- Learning Algorithms (Local Learning vs. online Learning)
- Parsing Algorithms (Shift-reduce Parsing)
- Features
Part D: The Combined Models
- The stacking Method
- The ensemble Method
- Single-model Combination
Part E: Other Recent Trends in Dependency Parsing
- Integer Linear Programming
- Fast Non-Projective Parsing
Presenters
Qin Iris Wang
Email: qiniriswang@gmail.com
Qin Iris Wang is currently a Research Scientist at AT&T Interactive
(San Francisco). Qin obtained her PhD in 2008 from the University of
Alberta under Dekang Lin and Dale Schuurmans. Qin's research interests
include NLP (in particular dependency parsing), machine learning,
information retrieval, text mining and large scale data processing.
Qin's PhD studies was focused on Learning Structured Classifiers for
Statistical Dependency Parsing. Before joined AT&T, she was a research
scientist at Yahoo Labs. Qin was a teaching assistant for two years
during her PhD studies. In 2009, Qin organized a workshop on "
Semi-supervised Learning for Natural Language Processing" at
NAACL-HLT.
Yue Zhang
Email: yue.zhang@comlab.ox.ac.uk
Yue Zhang just defended his PhD thesis at the University of Oxford.
Yue's research interests include natural language processing (word
segmentation, parsing, machine translation), machine learning, etc.
More specifically, his research area is the syntactic analysis of the
Chinese language, using discriminative machine-learning approaches. He
has worked on word segmentation, joint word segmentation and
POS-tagging, phrase-structure parsing and dependency parsing. Yue
worked on Chinese-English machine-translation during MSc studies in
Oxford, and parallel computing during undergrad studies in Tsinghua
University.
8
