Modeling Context in Scenario Template Creation
Long Qiu, Min-Yen Kan, Tat-Seng Chua
Department of Computer Science
National University of Singapore
Singapore, 117590
{qiul,kanmy,chuats}@comp.nus.edu.sg
Abstract
We describe a graph-based approach to Sce-
nario Template Creation, which is the task
of creating a representation of multiple re-
lated events, such as reports of different hur-
ricane incidents. We argue that context is
valuable to identify important, semantically
similar text spans from which template slots
could be generalized. To leverage context,
we represent the input as a set of graphs
where predicate-argument tuples are ver-
tices and their contextual relations are edges.
A context-sensitive clustering framework is
then applied to obtain meaningful tuple clus-
ters by examining their intrinsic and extrin-
sic similarities. The clustering framework
uses Expectation Maximization to guide the
clustering process. Experiments show that:
1) our approach generates high quality clus-
ters, and 2) information extracted from the
clusters is adequate to build high coverage
templates.
1 Introduction
Scenario template creation (STC) is the problem of
generating a common semantic representation from
a set of input articles. For example, given multiple
newswire articles on different hurricane incidents,
an STC algorithm creates a template that may in-
clude slots for the storm?s name, current location, di-
rection of travel and magnitude. Slots in such a sce-
nario template are often to be filled by salient entities
in the scenario instance (e.g., ?Hurricane Charley?
or ?the coast area?) but some can also be filled by
prominent clauses, verbs or adjectives that describe
these salient entities. Here, we use the term salient
aspect (SA) to refer to any of such slot fillers that
people would regard as important to describe a par-
ticular scenario. Figure 1 shows such a manually-
built scenario template in which details about im-
portant actions, actors, time and locations are coded
as slots.
STC is an important task that has tangible bene-
fits for many downstream applications. In the Mes-
sage Understanding Conference (MUC), manually-
generated STs were provided to guide Information
Extraction (IE). An ST can also be viewed as reg-
ularizing a set of similar articles as a set of at-
tribute/value tuples, enabling multi-document sum-
marization from filled templates.
Despite these benefits, STC has not received
much attention by the community. We believe this
is because it is considered a difficult task that re-
quires deep NL understanding of the source articles.
A problem in applications requiring semantic simi-
larity is that the same word in different contexts may
have different senses and play different roles. Con-
versely, different words in similar contexts may play
similar roles. This problem makes approaches that
rely on word similarity alone inadequate.
We propose a new approach to STC that incor-
porates the use of contextual information to address
this challenge. Unlike previous approaches that con-
centrate on the intrinsic similarity of candidate slot
fillers, our approach explicitly models contextual ev-
idence. And unlike approaches to word sense disam-
biguation (WSD) and other semantic analyses that
157
use neighboring or syntactically related words as
contextual evidence, we define contexts by semantic
relatedness which extends beyond sentence bound-
aries. Figure 2 illustrates a case in point with two
excerpts from severe storm reports. Here, although
the intrinsic similarity of the main verbs ?hit? and
?land? is low, their contextual similarity is high as
both are followed by clauses sharing similar subjects
(hurricanes) and the same verbs. Our approach en-
codes such contextual information as graphs, map-
ping the STC problem into a general graph overlay
problem that is solvable by a variant of Expectation
Maximization (EM).
Our work also contributes resources for STC re-
search. Until now, few scenario templates have been
publicly available (as part of MUC), rendering any
potential evaluation of automated STC statistically
insignificant. As part of our study, we have com-
piled a set of input articles with annotations that we
are making available to the research community.
Scenario Template: Storm
Storm Name Charley
Storm Action landed
Location Florida?s Gulf coast
Time Friday at 1950GMT
Speed 145 mph
Victim Category 1 13 people
Action died
Victim Category 2 over one million
Action affected
Figure 1: An example scenario template (filled).
2 Related Work
A natural way to automate the process of STC is to
cluster similar text spans in the input article set. SAs
then emerge through clustering; if a cluster of text
spans is large enough, the aspects contained in it will
be considered as SAs. Subsequently, these SAs will
be generalized into one or more slots in the template,
depending on the definition of the text span. As-
suming scenarios are mainly defined by actions, the
focus should be on finding appropriate clusters for
text spans each of which represents an action. Most
of the related work (although they may not directly
address STC) shares this assumption and performs
Charley landed further south on Florida?s
Gulf coast than predicted, ... The hurricane
... has weakened and is moving over South
Carolina.
At least 21 others are missing after the storm
hit on Wednesday. .... But Tokage had
weakened by the time it passed over Japan?s
capital, Tokyo, where it left little damage be-
fore moving out to sea.
Figure 2: Contextual evidence of similarity. Curved
lines indicate similar contexts, providing evidence
that ?land? and ?hit? from two articles are semanti-
cally similar.
action clustering accordingly. While the target ap-
plication varies, most systems that need to group text
spans by similarity measures are verb-centric.
In addition to the verb, many systems expand
their representation by including named entity tags
(Collier, 1998; Yangarber et al, 2000; Sudo et al,
2003; Filatova et al, 2006), as well as restrict-
ing matches (using constraints on subtrees (Sudo et
al., 2003; Filatova et al, 2006), predicate argument
structures (Collier, 1998; Riloff and Schmelzen-
bach, 1998; Yangarber et al, 2000; Harabagiu and
Maiorano, 2002) or semantic roles).
Given these representations, systems then cluster
similar text spans. To our knowledge, all current
systems use a binary notion of similarity, in which
pairs of spans are either similar or not. How they de-
termine similarity is tightly coupled with their text
span representation. One criterion used is pattern
overlap: for example, (Collier, 1998; Harabagiu and
Lacatusu, 2005) judge text spans to be similar if they
have similar verbs and share the same verb argu-
ments. Working with tree structures, Sudo et al and
Filatova et al instead require shared subtrees.
Calculating text span similarity ultimately boils
down to calculating word phrase similarity. Ap-
proaches such as Yangarber?s or Riloff and
Schmelzenbach?s do not employ a thesaurus and
thus are easier to implement, but can suffer from
over- or under-generalization. In certain cases, ei-
ther the same actor is involved in different actions or
different verbs realize the same action. Other sys-
tems (Collier, 1998; Sudo et al, 2003) do employ
158
lexical similarity but threshold it to obtain binary
judgments. Systems then rank clusters by cluster
size and correlation with the relevant article set and
equate top clusters as output scenario slots.
3 Context-Sensitive Clustering (CSC)
Automating STC requires handling a larger degree
of variations than most previous work we have sur-
veyed. Note that the actors involved in actions in a
scenario generally differ from event to event, which
makes most related work on text span similarity cal-
culation unsuitable. Also, action participants are not
limited to named entities, so our approach needs to
process all NPs. As both actions and actors may be
realized using different words, a similarity thesaurus
is necessary. Our approach to STC uses a thesaurus
based on corpus statistics (Lin, 1998) for real-valued
similarity calculation. In contrast to previous ap-
proaches, we do not threshold word similarity re-
sults; we retain their fractional values and incorpo-
rate these values holistically. Finally, as the same
action can be realized in different constructions, the
semantic (not just syntactic) roles of verb arguments
must be considered, lest agent and patient roles be
confused. For these reasons, we use a semantic role
labeler (Pradhan et al, 2004) to provide and delimit
the text spans that contain the semantic arguments
of a predicate. We term the obtained text spans as
predicate argument tuples (tuples) throughout the
paper. The semantic role labeler reportedly achieves
an F 1 measure equal to 68.7% on identification-classification of predicates and core arguments on a
newswire text corpus (LDC, 2002). Within the con-
fines of our study, we find it is able to capture most
of the tuples of interest.
Our approach explicitly captures contextual ev-
idence. We define a tuple?s contexts as other tu-
ples in the same article segment where no topic shift
occurs. This definition refines the n-surrounding
word constraint commonly used in spelling correc-
tion (for example, (Hirst and Budanitsky, 2005)),
Word Sense Disambiguation ((Preiss, 2001), (Lee
and Ng, 2002), for instance), etc. while still en-
sures the relatedness between a tuple and its con-
texts. Specifically, a tuple is contextually related to
other tuples by two quantifiable contextual relations:
argument-similarity and position-similarity. For our
experiments, we use the leads of newswire articles
as they normally summarize the news. We also as-
sume a lead qualifies as a single article segment, thus
making all of its tuples as potential contexts to each
other.
from A2
from A1
weakened(storm)
v21
hit(storm)
v22
moving(storm)
v23
weakened(hurricane)
v11
landed(hurricane)
v12
moving(hurricane)
v13
e21,2
e22,1 e21,3
e23,1
e22,3
e23,2
e11,2
e12,1 e11,3
e13,1
e12,3
e13,2
Figure 3: Being similar contexts, ?weakened? and
?moving? provide contextual evidence that ?land?
and ?hit? are similar.
First, we split the input article leads into sentences
and perform semantic role labeling immediately af-
terwards. Our system could potentially benefit from
additional pre-processing such as co-reference reso-
lution. Currently these pre-processing steps have not
been properly integrated with the rest of the system,
and thus we have not yet measured their impact.
We then transform each lead Ai into a graph Gi =
{V i, Ei}. As shown in Figure 3, vertices V i =
{vij}(j = 1, ..., N) are the N predicate argumenttuples extracted from the ith article, and directed
edges Ei = {eim,n = (vim, vin)} reflect contextualrelations between tuple vim and vin. Edges only con-nect tuples from the same article, i.e., within each
graph Gi. We differentiate between two types of
edges. One is argument-similarity, where the two
tuples have semantically similar arguments. This
models tuple cohesiveness, where the edge weight is
determined by the similarity score of the most sim-
ilar inter-tuple argument pair. The other is position-
similarity, represented as the offset of the ending tu-
ple with respect to the other, measured in sentences.
This edge type is directional to account for simple
causality.
Given this set of graphs, the clustering task is to
find an optimal alignment of all graphs (i.e., super-
imposing the set of article graphs to maximize vertex
overlap, constrained by the edges). We adapt Expec-
tation Maximization (Dempster et al, 1977) to find
159
an optimal clustering. This process assigns tuples to
suitable clusters where they are semantically similar
and share similar contexts with other tuples. Algo-
rithm 1 outlines this alignment process.
Algorithm 1 Graph Alignment(G)
/*G is a set of graph {Gi}*/
T ? all tuples in G
C ? highly cohesive tuples clusters
other? remaining tuples semantically connected with C
C[C.length]? otherrepeat
/*E step*/for each i such that i < C.length dofor each j such that j < C.length doif i == j then
continue;
re-estimate parameters[C[i],C[j]] /*distribution
parameters of edges between two clusters*/
tupleReassigned = false /*reset*/
/*M step*/for each i such that i < T.length do
aBestLikelihood = T [i].likelihood; /*likelihood of
being in its current cluster*/for each tuple tcontxt that contextually related with
T [i] dofor each cluster ccand, any candidate cluster thatcontextually related with tcontxt.cluster do
P (T [i] ? ccand) = comb(Ps, Pc)
likelihood = log(P (T [i] ? ccand))if likelihood > aBestLikelihood then
aBestLikelihood = likelihood
T [i].cluster = ccand
tupleReassigned = trueuntil tupleReassigned == false /*alignment stable*/return
During initialization, tuples whose pairwise simi-
larity higher than a threshold ? are merged to form
highly cohesive seed clusters. To compute a con-
tinuous similarity Sim(ta, tb) of tuples ta and tb,we use the similarity measure described in (Qiu et
al., 2006), which linearly combines similarities be-
tween the semantic roles shared by the two tuples.
Some other tuples are related to these seed clus-
ters by argument-similarity. These related tuples are
temporarily put into a special ?other? cluster. The
cluster membership of these related tuples, together
with those currently in the seed clusters, are to be
further adjusted. The ?other? cluster is so called be-
cause a tuple will end up being assigned to it if it
is not found to be similar to any other tuple. Tuples
that are neither similar to nor contextually related by
argument-similarity to another tuple are termed sin-
gletons and excluded from being clustered.
We then iteratively (re-)estimate clusters of tuples
across the set of article graphs G. In the E-step of the
EM algorithm, all contextual relations between each
pair of clusters are collected as two set of edges.
Here we assume argument-similarity and position-
similarity are independent and thus we differenti-
ate them in the computation. Accordingly, there
are two sets: edgesas and edgesps. For simplicity,we assume independent normal distributions for the
strength of each set (inter-tuple argument similarity
for edgesas and sentence distance for edgesps). Theedge strength distribution parameters for both sets
between each pair of clusters are re-estimated based
on current edges in edgesas and edgesps.In the M-step, we examine each tuple?s fitness for
belonging to its cluster and relocate some tuples to
new clusters to maximize the likelihood given the
latest estimated edge strength distributions. In the
following equations, we denote the proposition that
predicate argument tuple ta belongs to cluster cm as
ta?cm; a typical tuple (the centroid) of the cluster
cm as tcm ; and the cluster of ta as cta . The objectivefunction to maximize is:
Obj(G) =
X
ta?G
log(P (ta?cta)), (1)
where P (ta?cm) = 2Ps(ta?cm) Pc(ta?cm)Ps(ta?cm) + Pc(ta?cm) . (2)
Equation 2 takes the harmonic mean of two factors:
a contextual factor Pc and and a semantic factor Ps:
Pc(ta?cm) = max{P (edges(ta, tb)|
tb:edges(ta,tb)6=null
edges(cm, ctb ))}, (3)
Ps(ta?cm) =
(
simdefault, cm = cother,
Sim(ta, tcm), otherwise. (4)
Here the contextual factor Pc models how likely
ta belongs to cm according to the contextual infor-mation, i.e., the conditional probability of the con-
textual relations between cm and ctb given the con-textual relations between ta and one particular con-text tb, which maximizes this probability. Accord-ing to Bayes? theorem, it is computed as shown in
Equation 3. In practice, we multiply two conditional
probabilities: P (edgeas(ta, tb)|edgesas(cm, ctb))and P (edgeps(ta, tb)|edgesps(cm, ctb)), assumingindependence between edgesas and edgesps.We assume there are still singleton tuples that are
not semantically similar to another tuple and should
belong to the special ?other? cluster. Given that they
160
are dissimilar to each other, we set simdefault toa small nonzero value in Equation 4 to prevent the
?other? cluster from expelling them based on their
low semantic similarity. Tuples? cluster member-
ships are recalculated, and the parameters describ-
ing the contextual relations between clusters are re-
estimated. New EM iterations are performed as long
as one or more tuple relocations occur. Once the
EM halts, clusters of equivalent tuples are formed.
Among these clusters, some correspond to salient
actions that, together with their actors, are all SAs
to be generalized into template slots. Cluster size
is a good indicator of salience, and each large clus-
ter (excluding the ?other? cluster) can be viewed as
containing instances of a salient action.
Formulating the clustering process as a variant of
iterative EM is well-motivated as we consider the
similarity scores as noisy and having missing obser-
vations. Calculating semantic similarity is at best
inaccurate. Thus it is difficult to cluster tuples cor-
rectly based only on their semantic similarity. Also
to check whether a tuple shares contexts with a clus-
ter of tuples, the cluster has to be relatively clean.
An iterative EM as we have proposed naturally im-
prove the cleanness of these tuple clusters gradually
as new similarity information comes to light.
4 Evaluation
For STC, we argue that it is crucial to cluster tuples
with high recall so that an SA?s various surface
forms can be captured and the size of clusters can
serve as a salience indicator. Meanwhile, precision
should not be sacrificed, as more noise will hamper
the downstream generalization process which
outputs template slots. We conduct experiments
designed to answer two relevant research questions:
1) Cluster Quality: Whether using contexts (in
CSC) produces better clustering results than ignor-
ing it (in the K-means baseline); and
2) Template Coverage: Whether slots generalized
from CSC clusters cover human-defined templates.
4.1 Data Set and Baseline
A straightforward evaluation of a STC system would
compare its output against manually-prepared gold
standard templates, such as those found in MUC.
Unfortunately, such scenario templates are severely
limited and do not provide enough instances for a
proper evaluation. To overcome this problem, we
have prepared a balanced news corpus, where we
have manually selected articles covering 15 scenar-
ios. Each scenario is represented by a total of 45 to
50 articles which describe 10 different events.
Our baseline is a standard K-means clusterer. Its
input is identical to that of CSC ? the tuples ex-
tracted from relevant news articles and are not ex-
cluded from being clustered by CSC in the initial-
ization stage (refer to Section 3) ? and employs the
same tuple similarity measure (Qiu et al, 2006). The
differentiating factor between CSC and K-means is
the use of contextual evidence. A standard K-means
clusterer requires a k to be specified. For each sce-
nario, we set its k as the number of clusters gener-
ated by CSC for direct comparison.
We fix the test set for each scenario as ten ran-
domly selected news articles, each reporting a dif-
ferent instance of the scenario; the development set
(which also serves as the training set for determin-
ing the EM initialization threshold ? and simdefaultin Equation 4) is a set of ten articles from the ?Air-
linerCrash? scenario, which are excluded from the
test set. Both systems analyze the first 15 sentences
of each article, and sentences generate 2 to 3 predi-
cate argument tuples on average, resulting in a total
of 10 ? 15 ? (2 to 3) = 300 to 450 tuples for each
scenario.
4.2 Cluster Quality
This experiment compares the clustering results of
CSC and K-means. We use the standard cluster-
ing metrics of purity and inverse purity (Hotho et
al., 2003). The first author manually constructed the
gold standard clusters for each scenario using a GUI
before conducting any experiments. A special clus-
ter, corresponding to the ?other? cluster in the CSC
clusters, was created to hold the singleton tuples for
each scenario. Table 1 shows this under the column
?#Gold Standard Clusters?.
Using the manual clusters as the gold standard, we
obtain the purity (P) and inverse purity (IP) scores
of CSC and K-means on each scenario. In Table 1,
we see that CSC outperforms K-means on 10 of 15
scenarios for both P and IP. For the remaining 5 sce-
narios, where CSC and K-means have comparable
161
P scores, the IP scores of CSC are all significantly
higher than that of K-means. This suggests clus-
ters tend to be split apart more in K-means than in
CSC when they have similar purity. One thing worth
mentioning here is that the ?other? cluster normally
is relatively large for each scenario, and thus may
skew the results. To remove this effect, we excluded
tuples belonging to the CSC ?other? cluster from the
K-means input, generating one fewer cluster. Run-
ning the evaluation again, the resulting P-IP scores
again show that CSC outperforms the baseline K-
means. We only report the results for all tuples in
our paper for simplicity.
#Gold Std. CSC K-meansScenario Clusters P IP P IP
AirlinerCrash 23 .61 .42 .52 .28
Earthquake 18 .60 .44 .53 .30
Election 10 .77 .49 .75 .21
Fire 14 .65 .44 .64 .26
LaunchEvent 12 .77 .37 .73 .22
Layoff 10 .71 .28 .70 .19
LegalCase 8 .75 .37 .75 .18
Nobel 6 .77 .28 .77 .19
Obituary 7 .85 .46 .81 .28
RoadAccident 20 .61 .49 .56 .40
SoccerFinal 5 .88 .39 .88 .15
Storm 14 .61 .31 .61 .22
Tennis 6 .87 .19 .87 .12
TerroristAttack 14 .64 .48 .62 .25
Volcano 16 .68 .38 .66 .17Average 12.2 .72 .39 .69 .23
Table 1: CSC outperforms K-means with respect to
the purity (P) and inverse purity (IP) scores.
A close inspection of the results reveals some
problematic cases. One issue worth mentioning is
that for certain actions both CSC and K-means pro-
duce split clusters. In the CSC case, we traced this
problem back to the thesaurus, where predicates for
one action seem to belong to two or more totally dis-
similar semantic categories. The corresponding tu-
ples are thus assigned to different clusters as their
low semantic similarity forces the tuples to remain
separate, despite the shared contexts trying to join
them. One example is ?blast (off)? and ?lift (off)? in
the ?Launch Event? scenario. The thesaurus shows
the two verbs are dissimilar and the corresponding
tuples end up being in two split clusters. This can
not be solved easily without an improved thesaurus.
We are considering adding a prior to model the op-
timal size for clusters, which may help to compact
such cases.
4.3 Template Coverage
We also assess how well the resulting, CSC-
generated tuple clusters serve in creating good sce-
nario template slots. We start from the top largest
clusters from each scenario, and decompose each
of them into six sets: the predicates, agents, pa-
tients, predicate modiers, agent modiers and pa-
tient modiers. For each of the first three sets for
each cluster, we create a generalized term to repre-
sent it using an extended version of a generaliza-
tion algorithm (Tseng et al, 2006). These terms
are deemed output slots, and are put into the tem-
plate with their agent-predicate-patient relations pre-
served. The size of the template may increase when
more clusters are generalized, as new slots may re-
sult.
We manually compare the slots that are output
from the system with those defined in existing sce-
nario templates in MUC. The results here are only
indicative and not conclusive, as there are only two
MUC7 templates available for comparison: Aviation
Disaster and Launch Event.
Template semantic role general term
action crash
cluster 1 agent aircraft
patient ?
action kill
cluster 2 agent heavier-than-
air-craft
patient people
Figure 4: Automated scenario template of ?Avia-
tionDisaster?.
Figure 4 shows an excerpt of the automatically
generated template ?AviationDisaster? (?Airliner-
Crash? in our corpus) where the semantic roles in
the top two biggest clusters have been generalized.
Their modifiers are quite semantically diverse, as
shown in Table 2. Thus, generalization (probably
after a categorization operation) remains as a chal-
lenging problem.
Nonetheless, the information contained in these
semantic roles and their modifiers covers human-
162
semantic role modifier head samples
agent:aircraft A, U.N., The, Swiss, Canadian-
built, AN, China, CRJ-200, mil-
itary, Iranian, Air, refueling, US,
...
action:crash Siberia, mountain, rain, Tues-
day, flight, Sharjah, flames, Sun-
day, board, Saturday, 225, Rock-
away, approach, United, moun-
tain, hillside
patient:people all, 255, 71
Table 2: Sample automatically detected modifier
heads of different semantic roles.
AviationDisaster LaunchEvent
* AIRCRAFT * VEHICLE
* AIRLINE * VEHICLE TYPE
DEPARTURE POINT * VEHICLE OWNER
DEPARTURE DATE * PAYLOAD
* AIRCRAFT TYPE PAYLOAD TYPE
* CRASH DATE PAYLOAD FUNC
* CRASH SITE * PAYLOAD OWNER
CAUSE INFO PAYLOAD ORIGIN
* VICTIMS NUM * LAUNCH DATE
* LAUNCH SITE
MISSION TYPE
MISSION FUNCTION
MISSION STATUS
Figure 5: MUC-7 template coverage: asterisks
marking all the slots that could be automatically
generated.
defined scenario templates quite well. The two
MUC7 templates are shown as a list of slots in Fig-
ure 5, where horizontal lines delimit slots about dif-
ferent semantic roles, and asterisks mark all the slots
that could be automatically generated by our system
once it has an improved generalizer. We can see
substantial amount of overlap, indicating that a STC
system powered by CSC is able to capture scenarios?
important facts.
5 Conclusion
We have introduced a new context-sensitive ap-
proach to the scenario template creation (STC) prob-
lem. Our method leverages deep NL processing, us-
ing semantic role labeler?s structured semantic tu-
ples as input. Despite the use of deeper semantics,
we believe that intrinsic semantic similarity by itself
is not sufficient for clustering. We have shown this
through examples and argue that an approach that
considers contextual similarity is necessary. A key
aspect of our work is the incorporation of such con-
textual information. Our approach uses a notion of
context that combines two aspects: positional simi-
larity (when two tuples are adjacent in the text), and
argument similarity (when they have similar argu-
ments). The set of relevant articles are represented
as graphs where contextual evidence is encoded.
By mapping our problem into a graphical formal-
ism, we cast the STC clustering problem as one of
multiple graph alignment. Such a graph alignment is
solved by an adaptation of EM, which handles con-
texts and real-valued similarity by treating both as
noisy and potentially unreliable observations.
While scenario template creation (STC) is a dif-
ficult problem, its evaluation is arguably more dif-
ficult due to the dearth of suitable resources. We
have compiled and released a corpus of over 700
newswire articles that describe different instances of
15 scenarios, as a suitable input dataset for further
STC research. Using this dataset, we have evaluated
and analyzed our context-sensitive approach. While
our results are indicative, they show that considering
contextual evidence improves performance.
Acknowledgments
The authors are grateful to Kathleen R. McKeown
and Elena Filatova at Columbia University for their
stimulating discussions and comments over different
stages of the preparation of this paper.
References
Robin Collier. 1998. Automatic Template Creation forInformation Extraction. Ph.D. thesis, University ofSheffield, UK.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-gorithm. JRSSB, 39:1?38.
Elena Filatova, Vasileios Hatzivassiloglou, and KathleenMcKeown. 2006. Automatic creation of domain tem-plates. In Proceedings of the COLING/ACL ?06.
Sanda M. Harabagiu and V. Finley Lacatusu. 2005.
Topic themes for multi-document summarization. InProceedings of SIGIR ?05.
163
Sanda M. Harabagiu and S. J. Maiorano. 2002. Multi-
document summarization with GISTEXTER. In Pro-ceedings of LREC ?02.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11(1).
Andreas Hotho, Steffen Staab, and Gerd Stumme. 2003.WordNet improves text document clustering. In Pro-ceedings of the SIGIR 2003 Semantic Web Workshop.
LDC. 2002. The aquaint corpus of english news text,
catalog no. LDC2002t31.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empiri-cal evaluation of knowledge sources and learning algo-rithms for word sense disambiguation. In Proceedingsof EMNLP ?02.
Dekang Lin. 1998. Automatic retrieval and clustering ofsimilar words. In Proceedings of COLING/ACL ?98.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-ings of HLT/NAACL ?04.
Judita Preiss. 2001. Local versus global context for wsdof nouns. In Proceedings of CLUK4.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.Paraphrase recognition via dissimilarity significanceclassification. In Proceedings of EMNLP ?06.
Ellen Riloff and M. Schmelzenbach. 1998. An empiri-
cal approach to conceptual case frame acquisition. InProceedings of WVLC ?98.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-ceedings of ACL ?03.
Yuen-Hsien Tseng, Chi-Jen Lin, Hsiu-Han Chen, and Yu-I Lin. 2006. Toward generic title generation for clus-
tered documents. In Proceedings of AIRS ?06.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen. 2000. Unsupervised discovery ofscenario-level patterns for information extraction. InProceedings of ANLP ?00.
164
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 18?26,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Paraphrase Recognition via Dissimilarity Signicance Classication
Long Qiu, Min-Yen Kan and Tat-Seng Chua
Department of Computer Science
National University of Singapore
Singapore, 117543
{qiul,kanmy,chuats}@comp.nus.edu.sg
Abstract
We propose a supervised, two-phase
framework to address the problem of para-
phrase recognition (PR). Unlike most PR
systems that focus on sentence similarity,
our framework detects dissimilarities be-
tween sentences and makes its paraphrase
judgment based on the significance of such
dissimilarities. The ability to differenti-
ate significant dissimilarities not only re-
veals what makes two sentences a non-
paraphrase, but also helps to recall addi-
tional paraphrases that contain extra but
insignificant information. Experimental
results show that while being accurate
at discerning non-paraphrasing dissimilar-
ities, our implemented system is able to
achieve higher paraphrase recall (93%), at
an overall performance comparable to the
alternatives.
1 Introduction
The task of sentence-level paraphrase recognition
(PR) is to identify whether a set of sentences (typ-
ically, a pair) are semantically equivalent. In such
a task, ?equivalence? takes on a relaxed meaning,
allowing sentence pairs with minor semantic dif-
ferences to still be considered as paraphrases.
PR can be thought of as synonym detection ex-
tended for sentences, and it can play an equally
important role in natural language applications.
As with synonym detection, applications such as
summarization can benefit from the recognition
and canonicalization of concepts and actions that
are shared across multiple documents. Automatic
construction of large paraphrase corpora could
mine alternative ways to express the same con-
cept, aiding machine translation and natural lan-
guage generation applications.
In our work on sentence-level PR, we have iden-
tified two main issues through observation of sam-
ple sentences. The first is to identify all discrete in-
formation nuggets, or individual semantic content
units, shared by the sentences. For a pair of sen-
tences to be deemed a paraphrase, they must share
a substantial amount of these nuggets. A trivial
case is when both sentences are identical, word
for word. However, paraphrases often employ dif-
ferent words or syntactic structures to express the
same concept. Figure 1 shows two sentence pairs,
in which the first pair is a paraphrase while the
second is not. The paraphrasing pair (also denoted
Paraphrase (+pp):
Authorities said a young man injured Richard Miller.
Richard Miller was hurt by a young man.Non-Paraphrase (-pp):
The technology-laced Nasdaq Composite Index
.IXIC added 1.92 points, or 0.12 percent, at 1,647.94.
The technology-laced Nasdaq Composite Index
.IXIC dipped 0.08 of a point to 1,646.
Figure 1: Examples: Paraphrasing & Non-
paraphrasing
as the +pp class) use different words. Focusing
just on the matrix verbs, we note differences be-
tween ?injured? and ?hurt?. A paraphrase recogni-
tion system should be able to detect such semantic
similarities (despite the different syntactic struc-
tures). Otherwise, the two sentences could look
even less similar than two non-paraphrasing sen-
tences, such as the two in the second pair. Also in
the paraphrasing pair, the first sentence includes an
extra phrase ?Authorities said?. Human annotators
tend to regard the pair as a paraphrase despite the
presence of this extra information nugget.
18
This leads to the second issue: how to recognize
when such extra information is extraneous with
respect to the paraphrase judgment. Such para-
phrases are common in daily life. In news articles
describing the same event, paraphrases are widely
used, possibly with extraneous information.
We equate PR with solving these two issues,
presenting a natural two-phase architecture. In the
first phase, the nuggets shared by the sentences
are identified by a pairing process. In the second
phase, any unpaired nuggets are classified as sig-
nificant or not (leading to ?pp and +pp classifica-
tions, respectively). If the sentences do not contain
unpaired nuggets, or if all unpaired nuggets are in-
significant, then the sentences are considered para-
phrases. Experiments on the widely-used MSR
corpus (Dolan et al, 2004) show favorable results.
We first review related work in Section 2. We
then present the overall methodology and describe
the implemented system in Section 3. Sections 4
and 5 detail the algorithms for the two phases re-
spectively. This is followed with our evaluation
and discussion of the results.
2 Related Work
Possibly the simplest approach to PR is an infor-
mation retrieval (IR) based ?bag-of-words? strat-
egy. This strategy calculates a cosine similar-
ity score for the given sentence set, and if the
similarity exceeds a threshold (either empirically
determined or learned from supervised training
data), the sentences are paraphrases. PR systems
that can be broadly categorized as IR-based in-
clude (Corley and Mihalcea, 2005; Brockett and
Dolan, 2005). In the former work, the authors
defined a directional similarity formula reflect-
ing the semantic similarity of one text ?with re-
spect to? another. A word contributes to the di-
rectional similarity only when its counterpart has
been identified in the opposing sentence. The as-
sociated word similarity scores, weighted by the
word?s specificity (represented as inverted docu-
ment frequency, idf ), sum to make up the direc-
tional similarity. The mean of both directions
is the overall similarity of the pair. Brockett
and Dolan (2005) represented sentence pairs as
a feature vector, including features (among oth-
ers) for sentence length, edit distance, number of
shared words, morphologically similar word pairs,
synonym pairs (as suggested by WordNet and a
semi-automatically constructed thesaurus). A sup-
port vector machine is then trained to learn the
{+pp,?pp} classifier.
Strategies based on bags of words largely ig-
nore the semantic interactions between words.
Weeds et al (2005) addressed this problem by
utilizing parses for PR. Their system for phrasal
paraphrases equates paraphrasing as distributional
similarity of the partial sub-parses of a candidate
text. Wu (2005)?s approach relies on the genera-
tive framework of Inversion Transduction Gram-
mar (ITG) to measure how similar two sentences
arrange their words based on edit distance.
Barzilay and Lee (2003) proposed to apply
multiple-sequence alignment (MSA) for tradi-
tional, sentence-level PR. Given multiple articles
on a certain type of event, sentence clusters are
first generated. Sentences within the same clus-
ter, presumably similar in structure and content,
are then used to construct a lattice with ?back-
bone? nodes corresponding to words shared by the
majority and ?slots? corresponding to different re-
alization of arguments. If sentences from differ-
ent clusters have shared arguments, the associated
lattices are claimed to be paraphrase. Likewise,
Shinyama et al (2002) extracted paraphrases from
similar news articles, but use shared named enti-
ties as an indication of paraphrasing. It should be
noted that the latter two approaches are geared to-
wards acquiring paraphrases rather than detecting
them, and as such have the disadvantage of requir-
ing a certain level of repetition among candidates
for paraphrases to be recognized.
All past approaches invariably aim at a proper
similarity measure that accounts for all of the
words in the sentences in order to make a judg-
ment for PR. This is suitable for PR where in-
put sentences are precisely equivalent semanti-
cally. However, for many people the notion of
paraphrases also covers cases in which minor or
irrelevant information is added or omitted in can-
didate sentences, as observed in the earlier ex-
ample. Such extraneous content should not be a
barrier to PR if the main concepts are shared by
the sentences. Approaches that focus only on the
similarity of shared contents may fail when the
(human) criteria for PR include whether the un-
matched content is significant or not. Correctly
addressing this problem should increase accuracy.
In addition, if extraneous portions of sentences
can be identified, their confounding influence on
the sentence similarity judgment can be removed,
19
leading to more accurate modeling of semantic
similarity for both recognition and acquisition.
3 Methodology
As noted earlier, for a pair of sentences to be a
paraphrase, they must possess two attributes:
1. similarity: they share a substantial amount of
information nuggets;
2. dissimilarities are extraneous: if extra infor-
mation in the sentences exists, the effect of
its removal is not significant.
A key decision for our two-phase PR framework
is to choose the representation of an information
nugget. A simple approach is to use representative
words as information nuggets, as is done in the
SimFinder system (Hatzivassiloglou et al, 2001).
Instead of using words, we choose to equate in-
formation nuggets with predicate argument tuples.
A predicate argument tuple is a structured repre-
sentation of a verb predicate together with its argu-
ments. Given a sentence from the example in Fig-
ure 1, its predicate argument tuple form in Prop-
Bank (Kingsbury et al, 2002) format is:
target(predicate): hurtarg0: a young manarg1: Richard Miller
We feel that this is a better choice for the repre-
sentation of a nugget as it accounts for the action,
concepts and their relationships as a single unit.
In comparison, using fine-grained units such as
words, including nouns and verbs may result in in-
accuracy (sentences that share vocabulary may not
be paraphrases), while using coarser-grained units
may cause key differences to be missed. In the rest
of this paper, we use the term tuple for conciseness
when no ambiguity is introduced.
An overview of our paraphrase recognition sys-
tem is shown in Figure 2. A pair of sentences is
first fed to a syntactic parser (Charniak, 2000) and
then passed to a semantic role labeler (ASSERT;
(Pradhan et al, 2004)), to label predicate argu-
ment tuples. We then calculate normalized tuple
similarity scores over the tuple pairs using a met-
ric that accounts for similarities in both syntactic
structure and content of each tuple. A thesaurus
constructed from corpus statistics (Lin, 1998) is
utilized for the content similarity.
We utilize this metric to greedily pair together
the most similar predicate argument tuples across
Figure 2: System architecture
sentences. Any remaining unpaired tuples repre-
sent extra information and are passed to a dissim-
ilarity classifier to decide whether such informa-
tion is significant. The dissimilarity classifier uses
supervised machine learning to make such a deci-
sion.
4 Similarity Detection and Pairing
We illustrate this advantage of using predicate ar-
gument tuples from our running example. In Ta-
ble 1, one of the model sentences is shown in the
middle column. Two edited versions are shown on
the left and right columns. While it is clear that
the left modification is an example of a paraphrase
and the right is not, the version on the left in-
volves more changes in its syntactic structure and
vocabulary. Standard word or syntactic similar-
ity measures would assign the right modification a
higher similarity score, likely mislabeling one or
both modifications.
In contrast, semantic role labeling identifies the
dependencies between predicates and their argu-
ments, allowing a more precise measurement of
sentence similarity. Assuming that the arguments
in predicate argument tuples are assigned the same
role when their roles are comparable1 , we define
the similarity score of two tuples Ta and Tb asthe weighted sum of the pairwise similarities of
all their shared constituents C={(ca, cb)} (c beingeither the target or one of the arguments that both
1ASSERT, which is trained on the Propbank, only guaran-
tees consistency of arg0 and arg1 slots, but we have found in
practice that aligning arg2 and above arguments do not cause
problems.
20
Modification 1: paraphrase Model Sentence Modification 2: non-paraphrase
Sentence Richard Miller was hurt by a
young man.
Authorities said a young man in-
jured Richard Miller.
Authorities said Richard Miller injured
a young man.
(Paired)
Tuples
target: saidarg0: Authoritiesarg1: a young man injured
Richard Miller
target: saidarg0: Authoritiesarg1: Richard Miller injured a
young mantarget: hurtarg0: a young manarg1: Richard Miller
target: injuredarg0: a young manarg1: Richard Miller
target: injuredarg0: Richard Millerarg1: a young man
Table 1: Similarity Detection: pairing of predicate argument tuples
tuples have):
Sim(Ta, Tb) =
1
?
X
Sim(ca, cb)?
c={target,argshared}
wc==targettarget (1)
where normalization factor ? is the sum of the
weights of constituents in C , i.e.:
? = ?{argshared}? + wtarget (2)
In our current implementation we reduce tar-
gets and their arguments to their syntactic head-
words. These headwords are then directly com-
pared using a corpus-based similarity thesaurus.
As we hypothesized that targets are more impor-
tant for predicate argument tuple similarity, we
multiply the target?s similarity by a weighting fac-
tor wtarget , whose value we have empirically de-termined as 1.7, based on a 300-pair development
set from the MSR training set.
We then proceed to pair tuples in the two sen-
tences using a greedy iterative algorithm. The al-
gorithm locates the two most similar tuples from
each sentence, pairs them together and removes
them from futher consideration. The process stops
when subsequent best pairings are below the simi-
larity threshold or when all possible tuples are ex-
hausted. If unpaired tuples still exist in a given
sentence pair, we further examine the copular con-
structions and noun phrases in the opposing sen-
tence for possible pairings2 . This results in a one-
2Copular constructions are not handled by ASSERT. Such
constructions account for a large proportion of the semantic
meaning in sentences. Consider the pair ?Microsoft rose 50
cents? and ?Microsoft was up 50 cents?, in which the second
is in copular form. Similarly, NPs can often be equivalent
to predicate argument tuples when actions are nominalized.
Consider an NP that reads ?(be blamed for) frequent attacks
on soldiers? and a predicate argument tuple: ?(be blamed for)
attacking soldiers?. Again, identical information is conveyed
but not captured by semantic role labeling. In such cases,
they can be paired if we allow a candidate tuple to pair with
the predicative argument (e.g., 50 cents) of a copula, or (the
head of) an NP in the opposing sentence. As these heuristic
matches may introduce errors, we resort to these methods of
matching tuple only in the contingency when there are un-
paired tuples.
to-one mapping with possibly some tuples left un-
paired. The curved arrows in Table 1 denote the
correct results of similarity pairing: two tuples are
paired up if their target and shared arguments are
identical or similar respectively, otherwise they re-
main unpaired even if the ?bag of words? they con-
tain are the same.
5 Dissimilarity Significance
Classification
If some tuples remain unpaired, they are dissimilar
parts of the sentence that need to be labeled by the
dissimilarity classifier. Such unpaired informa-
tion could be extraneous or they could be semanti-
cally important, creating a barrier for paraphrase.
We frame this as a supervised machine learning
problem in which a set of features are used to
inform the classifier. A support vector machine,
SVMLight, was chosen as the learning model as it
has shown to yield good performance over a wide
application range. We experimented with a wide
set of features of unpaired tuples, including inter-
nal counts of numeric expressions, named entities,
words, semantic roles, whether they are similar
to other tuples in the same sentence, and contex-
tual features like source/target sentence length and
paired tuple count. Currently, only two features
are correlated in improved classification, which
we detail now.
Syntactic Parse Tree Path: This is a series of
features that reflect how the unpaired tuple con-
nects with the context: the rest of the sentence.
It models the syntactic connection between the
constituents on both ends of the path (Gildea and
Palmer, 2002; Pradhan et al, 2004). Here, we
model the ends of the path as the unpaired tuple
and the paired tuple with the closest shared ances-
tor, and model the path itself as a sequence of con-
stituent category tags and directions to reach the
destination (the paired target) from the source (the
21
unpaired target) via the the shared ancestor. When
no tuples have been paired in the sentence pair,
the destination defaults to the root of the syntactic
parse tree. For example, the tuples with target ?in-
jured? are unpaired when the model sentence and
the non-paraphrasing modification in Table 1 are
being compared. A path ??V BD, ?V P , ?S , ?SBAR
, ?V P , ?V BD? links a target ?injured? to the pairedtarget ?said?, as shown in Figure 3.
VP`````     VBD
said
SBAR
SXXXXNPaa!!NNP
Richard
NNP
Miller
VP
bb""VBD
injured
NP
Figure 3: Syntactic parse tree path
The syntactic path can act as partial evidence
in significance classification. In the above exam-
ple, the category tag ?V BD? assigned to ?injured?indicates that the verb is in its past tense. Such
a predicate argument tuple bears the main con-
tent of the sentence and generally can not be ig-
nored if its meaning is missing in the opposing
sentence. Another example is shown in Figure
4. The second sentence has one unpaired target
?proposed? while the rest all find their counter-
part. The path we get from the syntactic parse tree
reads ??V BN , ?NP , ?S , ...?, showing that the un-paired tuple (consisting of a single predicate) is a
modifier contained in an NP. It can be ignored if
there is no contradiction in the opposing sentence.
We represent a syntactic path by a set of n-gram
(n ? 4) features of subsequences of category tags
found in the path, along with the respective direc-
tion. We require these n-gram features to be no
more than four category tags away from the un-
paired target, as our primary concern is to model
what role the target plays in its sentence.
Sheena Young of Child, the national infertility sup-
port network, hoped the guidelines would lead to a more
?fair and equitable? service for infertility sufferers.
Sheena Young, a spokesman for Child, the national
infertility support network, said the proposed guide-
lines should lead to a more ?fair and equitable? service
for infertility sufferers.
Figure 4: Unpaired predicate argument tuple as
modifier in a paraphrase
Predicate: This is the lexical token of predi-
cate argument tuple?s target, as a text feature. As
this feature is liable to run into sparse data prob-
lems, the semantic category of the target would be
a more suitable feature. However, verb similar-
ity is generally regarded as difficult to measure,
both in terms of semantic relatedness as well as
in finding a consistent granularity for verb cate-
gories. We investigated using WordNet as well as
Levin?s classification (Levin, 1993) as additional
features on our validation data, but currently find
that using the lexical form of the target works best.
5.1 Classifier Training Set Acquisition
Currently, no training corpus for predicate argu-
ment tuple significance exists. Such a corpus is in-
dispensable for training the classifier. Rather than
manually annotating training instances, we use
an automatic method to construct instances from
paraphrase corpora. This is possible as the para-
phrase judgments in the corpora can imply which
portion of the sentence(s) are significant barriers
to paraphrasing or not. Here, we exploit the simi-
larity detector implemented for the first phase for
this purpose. If unpaired tuples exist after greedy
pairing, we classify them along two dimensions:
whether the sentence pair is a (non-)paraphrase,
and the source of the unpaired tuples:
1. [PS] paraphrasing pairs and unpaired predicate argu-
ment tuples are only from a single sentence;
2. [NS] non-paraphrasing pairs and only one single un-
paired predicate argument tuple exists;
3. [PM] paraphrasing pairs and unpaired predicate argu-
ment tuples are from multiple (both) sentences;
4. [NM] non-paraphrasing pairs and multiple unpaired
predicate argument tuples (from either one or both sen-
tences) exist.
Assuming that similarity detector pairs tuples
correctly, for the first two categories, the para-
phrasing judgment is directly linked to the un-
paired tuples. PS tuple instances are therefore
used as insignificant class instances, and NS as
significant ones. The last two categories can-
not be used for training data, as it is unclear which
of the unpaired tuples is responsible for the (non-)
paraphrasing as the similarity measure may mis-
takenly leave some similar predicate argument tu-
ples unpaired.
6 Evaluation
The goal of our evaluation is to show that our sys-
tem can reliably determine the cause(s) of non-
22
MSR Corpus Label +pp -pp
system prediction correct? T F T F total
# sentence pairs (s-ps) 85 23 55 37 200
# labelings (H&C agree) 80 19 53 35 187
# tuple pairs (t-ps) (S) 80 6 36 35 157
# correct t-ps (H&S agree) 74 6 34 30 144
# missed t-ps (H) 11 10 5 5 31
# sig. unpaired tuples(H) 6 4 69 51 130
# sig. unpaired tuples(S) 0 32 70 0 102
# sig. unpaired tuples(H&S) 0 4 43 0 47
# -pp for other reasons 0 0 5 2 7
Table 2: (H)uman annotations vs. (C)orpus anno-
tations and (S)ystem output
paraphrase examples, while maintaining the per-
formance level of the state-of-the-art PR systems.
For evaluation, we conduct both component
evaluations as well as a holistic one, resulting in
three separate experiments. In evaluating the first
tuple pairing component, we aim for high preci-
sion, so that sentences that have all tuples paired
can be safely assumed to be paraphrases. In evalu-
ating the dissimilarity classifier, we simply aim for
high accuracy. In our overall system evaluation,
we compare our system versus other PR systems
on standard corpora.
Experimental Data Set. For these experi-
ments, we utilized two widely-used corpora for
paraphrasing evaluation: the MSR and PASCAL
RTE corpora. The Microsoft Research Paraphrase
coupus (Dolan et al, 2004) consists of 5801
newswire sentence pairs, 3900 of which are an-
notated as semantically equivalent by human an-
notators. It reflects ordinary paraphrases that peo-
ple often encounter in news articles, and may be
viewed as a typical domain-general paraphrase
recognition task that downstream NLP systems
will need to deal with. The corpus comes divided
into standard training (70%) and testing (30%) di-
visions, a partition we follow in our experiments.
ASSERT (the semantic role labeler) shows for this
corpus a sentence contains 2.24 predicate argu-
ment tuples on average. The second corpus is
the paraphrase acquisition subset of the PASCAL
Recognizing Textual Entailment (RTE) Challenge
corpus (Dagan et al, 2005). This is much smaller,
consisting of 50 pairs, which we employ for test-
ing only to show portability.
To assess the component performance, we need
additional ground truth beyond the {+pp, ?pp}
labels provided by the corpora. For the first eval-
uation, we need to ascertain whether a sentence
pair?s tuples are correctly paired, misidentified or
mispaired. For the second, which tuple(s) (if any)
are responsible for a ?pp instance. However, cre-
ating ground truth by manual annotation is expen-
sive, and thus we only sampled the data set to get
an indicative assessment of performance. We sam-
pled 200 random instances from the total MSR
testing set, and first processed them through our
framework. Then, five human annotators (two au-
thors and three volunteers) annotated the ground
truth for tuple pairing and the semantic signifi-
cance of the unpaired tuples, while checking sys-
tem output. They also independently came up with
their own {+pp,-pp} judgment so we could assess
the reliability of the provided annotations.
The results of this annotation is shown in Ta-
ble 2. Examining this data, we can see that the
similarity detector performs well, despite its sim-
plicity and assumption of a one-to-one mapping.
Out of the 157 predicate argument tuple pairs
identified through similarity detection, annotators
agreed that 144 (92%) are semantically similar or
equivalent. However, 31 similar pairs were missed
by the system, resulting in 82% recall. We defer
discussion on the other details of this table to Sec-
tion 7.
To assess the dissimilarity classifier, we focus
on the ?pp subset of 55 instances recognized by
the system. For 43 unpaired tuples from 40 sen-
tence pairs (73% of 55), the annotators? judgments
agree with the classifier?s claim that they are sig-
nificant. For these cases, the system is able to both
recognize that the sentence pair is not a paraphrase
and further correctly establish a cause of the non-
paraphrase.
In addition to this ground truth sampled evalu-
ation, we also show the effectiveness of the clas-
sifier by examining its performance on PS and NS
tuples in the MSR corpus as described in Section
5. The test set consists of 413 randomly selected
PS and NS instances among which 145 are signif-
icant (leading to non-paraphrases). The classifier
predicts predicate argument tuple significance at
an accuracy of 71%, outperforms a majority clas-
sifier (65%), a gain which is marginally statisti-
cally significant (p < .09).
significant insignificant
112 263 insignificant by classifier
33 5 significant by classifier
We can see the classifier classifies the majority
of insignificant tuples correctly (by outputting a
23
709 Sentence Pairs Without 1016 Sentence Pairs With
Unpaired Tuples Unpaired Tuples Overall
Algorithm (41.1% of Test set) (58.9% of Test set) (100% of Test set)
Acc R P Acc R P Acc R P F1
Majority Classifier 79.5% 100% 79.5% 57.4% 100% 57.4% 66.5% 100% 66.5% 79.9%
SimFinder 82.2% 92.2% 86.4% 66.3% 84.9% 66.1% 72.9% 88.5% 75.1% 81.3%
CM05 - - - - - - 71.5% 92.5% 72.3% 81.2%
Our System 79.5% 100% 79.5% 66.7% 87.0% 66.0% 72.0% 93.4% 72.5% 81.6%
Table 3: Results on MSR test set
17 Sentence Pairs Without 33 Sentence Pairs With
Algorithm Unpaired Tuples Unpaired Tuples Overall
(34% of Test set) (66% of Test set) (100% of Test set)
Acc R P Acc R P Acc R P F1
Majority Classifier 65% 100% 65% 42% 100% 42% 50% 100% 50% 67%
SimFinder 71% 91% 71% 42% 21% 27% 52% 52% 52% 52%
Our System 65% 100% 65% 48% 64% 43% 54% 80% 53% 64%
Table 4: Results on PASCAL PP test set
score greater than zero), which is effectively a
98% recall of insignificant tuples. However, the
precision is less satisfatory. We suspect this is par-
tially due the tuples that fail to be paired up with
their counterpart. Such noise is found among the
automatically collected PS instances used in train-
ing.
0
20
40
60
80
100
120
140
160
180
> 
-
.
5
-
0.5
 
-
 
-
0.2
5
-
.
25
 
-
 
0
0 -
 
.
25
.
25
 
-
 
.
5
.
5 -
 
.
75
.
75
 
-
 
1
< 
1
SVM Prediction
Fr
eq
u
en
cy
Insignificant
Significant
Figure 5: Dissimilarity classifier performance
For the final system-wide evaluation, we imple-
mented two baseline systems: a majority classifier
and SimFinder (Hatzivassiloglou et al, 2001), a
bag-of-words sentence similarity module incorpo-
rating lexical, syntactic and semantic features. In
Table 3, precision and recall are measured with re-
spect to the paraphrasing class. The table shows
sentence pairs falling under the column ?pairs
without unpaired tuples? are more likely to be
paraphrasing than an arbitrary pair (79.5% ver-
sus 66.5%), providing further validation for using
predicate argument tuples as information nuggets.
The results for the experiment benchmarking the
overall system performance are shown under the
?Overall? column: our approach performs compa-
rably to the baselines at both accuracy and para-
phrase recall. The system performance reported in
(CM05; (Corley and Mihalcea, 2005)), which is
among the best we are aware of, is also included
for comparison.
We also ran our system (trained on the MSR
corpus) on the 50 instances in the PASCAL para-
phrase acquisition subset. Again, the system per-
formance (as shown in Table 4) is comparable to
the baseline systems.
7 Discussion
We have just shown that when two sentences have
perfectly matched predicate argument tuples, they
are more likely to be a paraphrase than a random
sentence pair drawn from the corpus. Further-
more, in the sampled human evaluation in Table
2, among the 88 non-paraphrasing instances with
whose MSR corpus labels our annotators agreed
(53 correctly and 35 incorrectly judged by our sys-
tem), the cause of the ?pp is correctly attributed
in 81 cases to one or more predicate argument tu-
ples. The remaining 7 cases (as shown in the last
row) are caused by phenomenon that are not cap-
tured by our tuple representation. We feel this jus-
tifies using predicate argument tuples as informa-
tion nuggets, but we are currently considering ex-
panding our representation to account for some of
these cases.
The evaluation also confirms the difficulty of
making paraphrase judgements. Although the
24
MSR corpus used strict means of resolving inter-
rater disagreements during its construction, the an-
notators agreed with the MSR corpus labels only
93.5% (187/200) of the time.
One weakness of our system is that we rely on a
thesaurus (Lin, 1998) for word similarity informa-
tion for predicate argument tuple pairing. How-
ever, it is designed to provide similarity scores
between pairs of individual words (rather than
phrases). If a predicate argument tuple?s target or
one argument is realized as a phrase (borrow ?
check out, for instance), the thesaurus is unable to
provide an accurate similarity score. For similarity
between predicate argument tuples, negation and
modality have yet to be addressed, although they
account for only a tiny fraction of the corpus.
We further estimated how the similarity detec-
tor can be affected when the semantic role labeler
makes mistakes (by failing to identify arguments
or assigning incorrect role names). Checking 94
pairs ground-truth similar tuples, we found that the
system mislabels 43 of them. The similarity detec-
tor failed to pair around 30% of them. In compar-
sion, all the tuple pairs free of labeling errors are
correctly paired. A saving grace is that labeling
errors rarely lead to incorrect pairing (one pairing
in all the examined sentences). The labeling er-
rors impact the whole system in two ways: 1) they
caused similar tuples that should have been paired
up to be added as noise in that dissimilarity clas-
sifier?s training set and 2) paired tuples with label-
ing errors erroneously increase the target weight
in Equation (1).
Some example paraphrasing cases that are prob-
lematic for our current system are:
1. Non-literal language issues such as implica-
ture, idiom, metaphor, etc. are not addressed in
our current system. When predicate argument tu-
ples imply each other, they are less similar than
what our system currently is trained for, causing
the pairing to fail:
+pp, Later in the day, a standoff developed between French
soldiers and a Hema battlewagon that attempted to pass the
UN compound.
French soldiers later threatened to open fire on a Hema bat-
tlewagon that tried to pass near the UN compound.
2. A paraphrasing pair may exceed the systems?
threshold for syntactic difference:
+pp, With the exception of dancing, physical activity did not
decrease the risk.
Dancing was the only physical activity associated with a
lower risk of dementia.
3. One or more unpaired tuples exist, but their
significance is not inferred correctly:
+pp, Inhibited children tend to be timid with new people,
objects, and situations, while uninhibited children sponta-
neously approach them.
Simply put, shy individuals tend to be more timid with new
people and situations.
In the MSR corpus, the first error case is more
frequent than the other two. We identify these as
challenging cases where idiomatic processing is
needed.
Below we show some unpaired predicate ar-
gument tuples (underlined) that are significant
enough to be paraphrase barriers. These examples
give an indicative categorization of what signifi-
cant tuples are and their corpus frequency (when
predicate argument tuples are the reasons; we ex-
amined 40 such cases for this estimation). There
is one thing in common: every case involves sub-
stantial information that is difficult to infer from
context. Such tuples appear as:
1. (40%) The nucleus of the sentence (often the
matrix tuple):
Michael Hill, a Sun reporter who is a member of the
Washington-Baltimore Newspaper Guild?s bargaining com-
mittee, estimated meetings to last late Sunday.
2. (30%) A part of a coordination:
Security lights have also been installed and police have
swept the grounds for booby traps.
3. (13%) A predicate of a modifying clause:
Westermayer was 26 then, and a friend and former manager
who knew she was unhappy in her job tipped her to another
position.
4. (7%) An adjunct:
While waiting for a bomb squad to arrive, the bomb exploded,
killing Wells.
5. (7%) An embedded sentence:
Dean told reporters traveling on his 10-city ?Sleepless
Summer? tour that he considered campaigning in Texas a
challenge.
6. (3%) Or factual content that conflicts with
the opposing sentence:
Total sales for the period declined 8.0 percent to USD1.99
billion from a year earlier.
Wal-Mart said sales at stores open at least a year rose 4.6
percent from a year earlier.
8 Conclusions
We have proposed a new approach to the para-
phrase recognition (PR) problem: a supervised,
25
two-phase framework emphasizing dissimilarity
classification. To emulate human PR judgment
in which insignificant, extraneous information
nuggets are generally allowed for a paraphrase,
we estimate whether such additional information
nuggets affect the final paraphrasing status of a
sentence pair. This approach, unlike previous PR
approaches, has the key benefit of explaining the
cause of a non-paraphrase sentence pair.
In the first, similarity detection module, using
predicate argument tuples as the unit for compar-
ison, we pair them up in a greedy manner. Un-
paired tuples thus represent additional information
unrepresented in the opposing sentence. A second,
dissimilarity classification module uses the lexical
head of the predicates and the tuples? path of at-
tachment as features to decide whether such tuples
are barriers to paraphrase.
Our evaluations show that the system obtains 1)
high accuracy for the similarity detector in pairing
predicate argument tuples, 2) robust dissimilar-
ity classification despite noisy training instances
and 3) comparable overall performance to current
state-of-the-art PR systems. To our knowledge this
is the first work that tackles the problem of identi-
fying what factors stop a sentence pair from being
a paraphrase.
We also presented corpus examples that illus-
trate the categories of errors that our framework
makes, suggesting future work in PR. While we
continue to explore more suitable representation
of unpaired predicate argument tuples, we plan to
augment the similarity measure for phrasal units
to reduce the error rate in the first component. An-
other direction is to detect semantic redundancy in
a sentence. Unpaired tuples that are semantically
redundant should also be regarded as insignificant.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL 2003.
Chris Brockett and Bill Dolan. 2005. Support vector ma-
chines for paraphrase identification and corpus construc-
tion. In Proceedings of the 3rd International Workshop onParaphrasing.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Annual Meeting of theNorth American Chapter of the Association for Computa-tional Linguistics (NAACL?2000).
Courtney Corley and Rada Mihalcea. 2005. Measuring the
semantic similarity of texts. In Proceedings of the ACLWorkshop on Empirical Modeling of Semantic Equiva-lence and Entailment, pages 13?18, Ann Arbor, USA.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. InPASCAL Proceedings of the First Challenge Workshop?Recognizing Textual Entailment, Southampton,UK.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the20th International Conference on Computational Linguis-tics, Geneva, Switzerland.
Daniel Gildea and Martha Palmer. 2002. The necessity of
parsing for predicate argument recognition. In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics, Philadelphia, USA.
Vassileios Hatzivassiloglou, Judith Klavans, Melissa Hol-
combe, Regina Barzilay, Min-Yen Kan, and Kathleen
McKeown. 2001. Simfinder: A flexible clustering tool
for summarization. In Proceedings of the NAACL Work-shop on Automatic Summarization, pages 41?49.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the penn treebank. In Pro-ceedings of the Human Language Technology Conference,
San Diego, USA.
Beth Levin. 1993. English verb classes and alternations: A
preliminary investigation. University of Chicago Press.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL ?98, pages
768?774, Montreal, Canada.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Dan Jurafsky. 2004. Shallow semantic pars-
ing using support vector machines. In Proceedings ofHLT/NAACL, Boston, USA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph
Grishman. 2002. Automatic paraphrase acquisition from
news articles. In Proceedings of the Human LanguageTechnology Conference, pages 40?46, San Diego, USA.
Julie Weeds, David Weir, and Bill Keller. 2005. The dis-
tributional similarity of sub-parses. In Proceedings of theACL Workshop on Empirical Modeling of Semantic Equiv-alence and Entailment, pages 7?12, Ann Arbor, USA.
Dekai Wu. 2005. Recognizing paraphrases and textual en-
tailment using inversion transduction grammars. In Pro-ceedings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment, Ann Arbor, USA.
26
