Translation using Information on Dialogue Participants 
Setsuo Yamada, E i i ch i ro  Sumi ta  and  H idek i  Kashioka 
ATR Interpreting Telecommunications Research Laboratories* 
2-2, Hikaridai, Seika-cho, Soraku-gun, 
Kyoto, 619-0288, JAPAN 
{ syamada, sumita, kashioka} @itl.atr.co.jp t 
Abstract 
This paper proposes a way to improve the trans- 
lation quality by using information on dialogue 
participants that is easily obtained from out- 
side the translation component. We incorpo- 
rated information on participants' ocial roles 
and genders into transfer ules and dictionary 
entries. An experiment with 23 unseen dia- 
logues demonstrated a recall of 65% and a preci- 
sion of 86%. These results howed that our sim- 
ple and easy-to-implement method is effective, 
and is a key technology enabling smooth con- 
versation with a dialogue translation system. 
1 I n t roduct ion  
Recently, various dialogue translation systems 
have been proposed (Bub and others, 1997; 
Kurematsu and Morimoto, 1996; Rayner and 
Carter, 1997; Ros~ and Levin, 1998; Sumita 
and others, 1999; Yang and Park, 1997; Vi- 
dal, 1997). If we want to make a conversation 
proceed smoothly using these translation sys- 
tems, it is important o use not only linguis- 
tic information, which comes from the source 
language, but also extra-linguistic nformation, 
which does not come from the source language, 
but, is shared between the participants of the 
conversation. 
Several dialogue translation methods that 
use extra-linguistic information have been pro- 
posed. Horiguchi outlined how "spoken lan- 
guage pragmatic information" can be trans- 
lated (Horiguchi, 1997). However, she did not 
apply this idea to a dialogue translation system. 
LuperFoy et al proposed a software architec- 
*Current affiliation is ATR Spoken Language Trans- 
lation Research Laboratories 
Current mail addresses are 
{ setsuo.yarnada, eiichiro.sumita, hideki.kashioka} 
@slt. atr. co.jp 
ture that uses '% pragmatic adaptation" (Lu- 
perFoy and others, 1998), and Mima et al pro- 
posed a method that uses "situational informa- 
tion" (Mima and others, 1997). LuperFoy et al 
simulated their method on man-machine inter- 
faces and Mima et al preliminarily evaluated 
their method. Neither study, however, applied 
its proposals to an actual dialogue translation 
system. 
The above mentioned methods will need time 
to work in practice, since it is hard to obtain 
the extra-linguistic nformation on which they 
depend. 
We have been paying special attention to "po- 
liteness," because a lack of politeness can inter- 
fere with a smooth conversation between two 
participants, uch as a clerk and a customer. It 
is easy for a dialogue translation system to know 
which participant is the clerk and which is the 
customer from the interface (such as the wires 
to the microphones). 
This paper describes a method of "polite- 
ness" selection according to a participant's so- 
cial role (a clerk or a customer), which is eas- 
ily obtained from the extra-linguistic environ- 
ment. We incorporated each participant's so- 
cial role into transfer ules and transfer dictio- 
nary entries. We then conducted an experiment 
with 23 unseen dialogues (344 utterances). Our 
method achieved a recall of 65% and a preci- 
sion of 86%. These rates could be improved to 
86% and 96%, respectively (see Section 4). It 
is therefore possible to use a "participant's so- 
cial role" (a clerk or a customer in this case) 
to appropriately make the translation results 
"polite," and to make the conversation proceed 
smoothly with a dialogue translation system. 
Section 2 analyzes the relationship between a
particular participant's social role (a clerk) and 
politeness in Japanese. Section 3 describes our 
proposal in detail using an English-to-Japanese 
37 
translation system. Section 4 shows an exper- 
iment and results, followed by a discussion in 
Section 5. Finally, Section 6 concludes this pa- 
per. 
2 A Par t i c ipant ' s  Soc ia l  Ro le  and  
Po l i teness  
This section focuses on one participant's social 
role. We investigated Japanese outputs of a di- 
alogue translation system to see how many ut- 
terances hould be polite expressions in a cur- 
rent translation system for travel arrangement. 
We input 1,409 clerk utterances into a Transfer 
Driven Machine Translation system (Sumita 
and others, 1999) (TDMT for short). The in- 
puts were closed utterances, meaning the sys- 
tem already knew the utterances, enabling the 
utterances to be transferred at a good quality. 
Therefore, we used closed utterances as the in- 
puts to avoid translation errors. 
As a result, it was shown that about 70% 
(952) of all utterances should be improved to use 
polite expressions. This result shows that a cur- 
rent translation system is not enough to make 
a conversation smoothly. Not surprisingly, if all 
expressions were polite, some Japanese speakers 
would feel insulted. Therefore, Japanese speak- 
ers do not have to use polite expression in all 
utterances. 
We classified the investigated ata into dif- 
ferent ypes of English expressions for Japanese 
politeness, i.e., into honorific titles, parts of 
speech such as verbs, and canned phrases, 
as shown in Table 1; however, not all types 
appeared in the data. For example, when 
the clerk said "How will you be paying, Mr. 
Suzuki," the Japanese translation was made 
polite as "donoyouni oshiharaininarimasu-ka 
suzuki-sama" in place of the standard expres- 
sion "donoyouni shiharaimasu-ka suzuki-san." 
Table 1 shows that there is a difference in 
how expressions should be made more polite ac- 
cording to the type, and that many polite ex- 
pressions can be translated by using only local 
information, i.e., transfer rules and dictionary 
entries. In the next section, we describe how to 
incorporate the information on dialogue partic- 
ipants, such as roles and genders, into transfer 
rules and dictionary entries in a dialogue trans- 
lation system. 
3 A Method  of  Us ing  In fo rmat ion  
on  D ia logue  Par t i c ipants  
This section describes how to use information 
on dialogue participants, such as participants' 
social roles and genders. First, we describe 
TDMT, which we also used in our experiment. 
Second, we mention how to modify transfer 
rules and transfer dictionary entries according 
to information on dialogue participants. 
3.1 Transfer  Dr iven  Mach ine  
Trans la t ion  
TDMT uses bottom-up left-to-right chart pars- 
ing with transfer rules as shown in Figure 1. 
The parsing determines the best structure and 
best transferred result locally by performing 
structural disambiguation using semantic dis- 
tance calculations, in parallel with the deriva- 
tion of possible structures. The semantic dis- 
tance is defined by a thesaurus. 
(source pattern) 
==~ 
J ((target pattern 1) 
((source xample 1) 
(source xample 2) 
? "- ) 
(target pattern 2) 
?o* ) 
Figure 1: Transfer ule format 
A transfer ule consists of a source pattern, 
a target pattern, and a source example. The 
source pattern consists of variables and con- 
stituent boundaries (Furuse and Iida, 1996). 
A constituent boundary is either a functional 
word or the part-of-speech of a left constituent's 
last word and the part-of-speech of a right con- 
stituent's first word. In Example (1), the con- 
stituent boundary IV-CN) is inserted between 
"accept" and "payment," because "accept" is 
a Verb and "payment" is a Common Noun. 
The target pattern consists of variables that cor- 
respond to variables in the source pattern and 
words of the target language. The source exam- 
ple consists of words that come from utterances 
referred to when a person creates transfer ules 
(we call such utterances closed utterances). 
Figure 2 shows a transfer ule whose source 
pattern is (X (V-CN) Y). Variable X corre- 
sponds to x, which is used in the target pat- 
tern, and Y corresponds to y, which is also 
38 
Table 1: Examples of polite expressions 
Type: verb, title 
Eng: How will you be paying, Mr. Suzuki 
Standard: donoyouni shiharaimasu-ka suzuki-san 
Polite: donoyouni o_shiharaininarimasu-ka suzuki-sama 
Gloss: How pay-QUESTION suzuki-Mr. 
Type: verb, common noun 
Eng: We have two types of rooms available 
Standard: aiteiru ni-shurui-no heya-ga ariraasu 
Polite: aiteiru ni-shurui-no oheya-ga gozaimasu 
Gloss: available two-types-of room-TOP have 
Type: auxiliary verb 
Eng: You can shop for hours 
Standard: suujikan kaimono-wo surukotogadekimasu 
Polite: suujikan kaimono-wo shiteitadakemasu 
Gloss: for hours make-OBJ can 
Type: pronoun 
Eng: Your room number, please 
Standard: anatano heya bangou-wo 
Polite: okyakusamano heya bangou-wo 
Gloss: Your room number-so obj 
onegaishirnasu 
onegaishimasu 
please 
Type: canned phrase 
Eng: How can I help you 
Standard: dou shimashitaka 
Polite: douitta goyoukendeshouka 
Gloss: How can I help you 
Example (1) 
Eng: We accept payment by credit card 
Standard: watashitachi-wa kurejitlo-kaado-deno shiharai-wo ukelsukemasu 
Polite: watashidomo-wa kurejitto-kaado-deno o_shiharai-wo ukeshimasu 
Gloss: We-TOP credit-card-by payment-OBJ accept 
used in the target pattern. The source exam- 
ple (("accept") ("payment")) comes from Ex- 
ample (1), and the other source examples come 
from the other closed utterances. This transfer 
rule means that if the source pattern is (X (V- 
CN) Y) then (y "wo" x) or (y "ni" x) is selected 
as the target pattern, where an input word pair 
corresponding to X and Y is semantically the 
most similar in a thesaurus to, or exactly the 
same as, the source example. For example, if 
an input word pair corresponding to X and Y 
is semantically the most similar in a thesaurus 
to, or exactly the same as, (("accept") ("pay- 
ment")), then the target pattern (y "wo" x) is 
selected in Figure 2. As a result, an appropriate 
target pattern is selected. 
After a target pattern is selected, TDMT cre- 
ates a target structure according to the pattern 
(X (V-CN) Y) 
((y "wo" x) 
((("accept") ("payment")) 
(("take") ("picture"))) 
(y "hi" x) 
((("take") ("bus")) 
(("get") ("sunstroke"))) 
) 
Figure 2: Transfer ule example 
by referring to a transfer dictionary, as shown 
in Figure 3. If the input is "accept (V -CN)  
payment," then this part is translated into "shi- 
harai wo uketsukeru." "wo" is derived from the 
target pattern (y "wo" x), and "shiharai" and 
"uketsukeru" are derived from the transfer dic- 
tionary, as shown in Figure 4. 
39 
(source pattern) 
(((target pattern 11) :pattern-cond 11
(target pattern 12) :pattern-cond 12 
itarget pattern In) :default) 
((source xample 1) 
? oo ) 
(((source xample 1) ~ (target word lt) :word-cond 11 
(source example 1) --* (target word 12) :word-cond 12 
?? .  
(source example 1) --* (target word lm) :default) 
o . "  ) 
(((target pattern 21) :pattern-cond 21 
. . .  ) ) )  
Figure 5: Transfer ule format with information on dialogue participants 
(((source word 1) --* (target word 11) :cond 11 I 
(source word 1) -* (target word 12) :cond 12 I 
I . . .  
(source word 1) -~ (target word lk) :default)\[ 
o*.  ) I 
Figure 6: Dictionary format with information on dialogue participants 
((source word) ~ (target word) 
? " .  ) 
Figure 3: Transfer dictionary format 
(("accept") --* ("uketsukeru') I ("payment") --* ("shiharai"))  
Figure 4: Transfer dictionary example 
(X "sama") 
((("Mr." x) :h-gender male 
("Ms." x) :h-gender female 
("Mr-ms." x)) 
(("room number"))) 
) 
Figure 7: Transfer ule example with the par- 
ticipant's gender 
3.2 Transfer Rules and Entr ies 
according to Information on 
Dialogue Part ic ipants 
For this research, we modified the transfer ules 
and the transfer dictionary entries, as shown in 
Figures 5 and 6. In Figure 5, the target pattern 
"target pattern 11" and the source word "source 
example 1" are used to change the translation 
according to information on dialogue partici- 
pants. For example, if ":pattern-cond 11" is de- 
fined as ":h-gender male" as shown in Figure 7, 
then "target pattern 11" is selected when the 
hearer is a male, that is, "("Mr." x)" is selected. 
Moreover, if ":word-cond 11" is defined as ":s- 
role clerk" as shown in Figure 8, then "source 
example 1" is translated into "target word 11" 
when the speaker is a clerk, that is, "accept" is 
translated into "oukesuru." Translations uch 
as "target word 11" are valid only in the source 
pattern; that is, a source example might not 
always be translated into one of these target 
words. If we always want to produce transla- 
tions according to information on dialogue par- 
ticipants, then we need to modify the entries 
in the transfer dictionary like Figure 6 shows. 
Conversely, if we do not want to always change 
the translation, then we should not modify the 
entries but modify the transfer ules. Several 
conditions can also be given to ":word-cond" 
and ":pattern-cond." For example, ":s-role cus- 
tomer and :s-gender female," which means the 
speaker is a customer and a female, can be 
given. In Figure 5, ":default" means the de- 
40 
fault target pattern or word if no condition is 
matched. The condition is checked from up to 
down in order; that is, first, ":pattern-cond 11," 
second, ":pattern-cond 1~," ... and so on. 
(X (V-CN) Y) 
((y "wo" x) 
((("accept") ("payment")) 
(("take") ("picture"))) 
((("accept") -~ ("oukesuru"):s-role clerk 
( "accept" ) --+ ( "uketsukeru" ) )) 
) 
Figure 8: Transfer ule example with a partici- 
pant's role 
((("payment") --~ ("oshiharai") :s-role clerk 
( "payment" ) ---* ( "shiharai" )) 
(("we") --* ("watashidomo") :s-role clerk 
("we") --~ ("watashltachi"))) 
Figure 9: Transfer dictionary example with a 
speaker's role 
Even though we do not have rules and en- 
tries for pattern conditions and word condi- 
tions according to another participant's infor- 
mation, such as ":s-role customer'(which means 
the speaker's role is a customer) and ":s-gender 
male" (which means the speaker's gender is 
male), TDMT can translate xpressions corre- 
sponding to this information too. For example, 
"Very good, please let me confirm them" will 
be translated into "shouchiitashimasita kakunin 
sasete itadakimasu" when the speaker is a clerk 
or "soredekekkoudesu kakunin sasete kudasai" 
when the speaker is a customer, as shown in 
Example (2). 
By making a rule and an entry like the ex- 
amples shown in Figures 8 and 9, the utter- 
ance of Example (1) will be translated into 
"watashidomo wa kurejitto kaado deno oshi- 
harai wo oukeshimasu" when the speaker is a 
clerk. 
4 An  Exper iment  
The TDMT system for English-to-Japanese at 
the time Of the experiment had about 1,500 
transfer ules and 8,000 transfer dictionary en- 
tries. In other words, this TDMT system was 
capable of translating 8,000 English words into 
Japanese words. About 300 transfer ules and 
40 transfer dictionary entries were modified to 
improve the level of "politeness." 
We conducted an experiment using the trans- 
fer rules and transfer dictionary for a clerk with 
23 unseen dialogues (344 utterances). Our input 
was off-line, i.e., a transcription of dialogues, 
which was encoded with the participant's social 
role. In the on-line situation, our system can 
not infer whether the participant's social role is 
a clerk or a customer, but can instead etermine 
the role without error from the interface (such 
as a microphone or a button). 
In order to evaluate the experiment, we clas- 
sifted the Japanese translation results obtained 
for the 23 unseen dialogues (199 utterances from 
a clerk, and 145 utterances from a customer, 
making 344 utterances in total) into two types: 
expressions that had to be changed to more po- 
lite expressions, and expressions that did not. 
Table 2 shows the number of utterances that in- 
cluded an expression which had to be changed 
into a more polite one (indicated by "Yes") and 
those that did not (indicated by "No"). We ne- 
glected 74 utterances whose translations were 
too poor to judge whether to assign a "Yes" or 
"No." 
Table 2: The number of utterances to be 
changed or not 
Necessity | The number 
of change I of utterances 
Yes 104 
No 166 
Out of scope 74 
Total \[ 344 
* 74 translations were too poor to handle for the 
"politeness" problem, and so they are ignored in this 
paper. 
The translation results were evaluated to see 
whether the impressions of the translated re- 
sults were improved or not with/without mod- 
ification for the clerk from the viewpoint of 
"politeness." Table 3 shows the impressions 
obtained according to the necessity of change 
shown in Table 2. 
The evaluation criteria are recall and preci- 
sion, which are defined as follows: 
Recall = 
number of utterances whose impression is better 
number of utterances which should be more polite 
41 
Example (2) 
Eng: Very good, please let me confirm them 
Standard: wakarimasita kakunin sasete 
Clerk: shouchiitashimasita kakunin sase~e 
Customer: soredekekkoudesu kakunin sasete 
Gloss: very good con:firm let me 
kudasai 
itadakimasu 
kudasai 
please 
Table 3: Evaluation on using the speaker's role 
Necessity 
of change 
Yes 
(lo4) 
No 
(166) 
~ Impression 
better 
same 
worse  
no-diff 
better 
s alTle 
worse  
no-diff 
The number 
of utterances 
68 
5 
3 
28 
0 
3 
0 
163 
bet ter :  Impression of a translation is better. 
same:  Impression of a translation has not changed. 
worse: Impression of a translation is worse. 
no-diff: There is no difference between the two 
translations. 
Precision = 
number of utterances whose impression is better 
number of utterances whose expression has been 
changed by the modified rules and entries 
The recall was 65% (= 68 - (68 + 5 + 3 + 28)) 
and the precision was 86% (= 68 -: (68 + 5 + 3 + 
0+3+0)).  
There are two main reasons which bring down 
these rates. One reason is that TDMT does not 
know who or what the agent of the action in 
the utterance is; agents are also needed to se- 
lect polite expressions. The other reason is that 
there are not enough rules and transfer dictio- 
nary entries for the clerk. 
It is easier to take care of the latter problem 
than the former problem. If we resolve the lat- 
ter problem, that is, if we expand the transfer 
rules and the transfer dictionary entries accord- 
ing to the "participant's social role" (a clerk and 
a customer), then the recall rate and the preci- 
sion rate can be improved (to 86% and 96%, 
respectively, as we have found). As a result, we 
can say that our method is effective for smooth 
conversation with a dialogue translation system. 
5 D iscuss ion  
In general, extra-linguistic information is hard 
to obtain. However, some extra-linguistic infor- 
mation can be easily obtained: 
(1) One piece of information is the participant's 
social role, which can be obtained from the in- 
terface such as the microphone used. It was 
proven that a clerk and customer as the social 
roles of participants are useful for translation 
into Japanese. However, more research is re- 
quired on another participant's social role. 
(2) Another piece of information is the par- 
ticipant's gender, which can be obtained by a 
speech recognizer with high accuracy (Takezawa 
and others, 1998; Naito and others, 1998). We 
have considered how expressions can be useful 
by using the hearer's gender for Japanese-to- 
English translation. 
Let us consider the Japanese honorific title 
"sama" or "san." If the heater's gender is male, 
then it should be translated "Mr." and if the 
hearer's gender is female, then it should be 
translated "Ms." as shown in Figure 7. Ad- 
ditionally, the participant's gender is useful for 
translating typical expressions for males or fe- 
males. For example, Japanese "wa" is often at- 
tached at the end of the utterance by females. 
It is also important for a dialogue translation 
system to use extra-linguistic information which 
the system can obtain easily, in order to make 
a conversation proceed smoothly and comfort- 
ably for humans using the translation system. 
We expect hat other pieces of usable informa- 
tion can be easily obtained in the future. For 
example, age might be obtained from a cellular 
telephone if it were always carried by the same 
person and provided with personal information. 
In this case, if the system knew the hearer was a 
child, it could change complex expressions into 
easier ones. 
6 Conc lus ion  
We have proposed a method of translation us- 
ing information on dialogue participants, which 
42 
is easily obtained from outside the translation 
component, and applied it to a dialogue trans- 
lation system for travel arrangement. This 
method can select a polite expression for an 
utterance according to the "participant's social 
role," which is easily determined by the inter- 
face (such as the wires to the microphones). For 
example, if the microphone is for the clerk (the 
speaker is a clerk), then the dialogue translation 
system can select a more polite expression. 
In an English-to-Japanese translation system, 
we added additional transfer ules and transfer 
dictionary entries for the clerk to be more po- 
lite than the customer. Then, we conducted an 
experiment with 23 unseen dialogues (344 ut- 
terances). We evaluated the translation results 
to see whether the impressions of the results im- 
proved or not. Our method achieved a recall of 
65% and a precision of 86%. These rates could 
easily be improved to 86% and 96%, respec- 
tively. Therefore, we can say that our method 
is effective for smooth conversation with a dia- 
logue translation system. 
Our proposal has a limitation in that if the 
system does not know who or what the agent 
of an action in an utterance is, it cannot ap- 
propriately select a polite expression. We are 
considering ways to enable identification of the 
agent of an action in an utterance and to ex- 
pand the current framework to improve the level 
of politeness even more. In addition, we intend 
to apply other extra-linguistic nformation to a 
dialogue translation system. 
References  
Thomas Bub et al 1997. Verbmobih The 
combination of deep and shallow processing 
for spontaneous speech translation. In the 
1997 International Conference on Acoustics, 
Speech, and Signal Processing: ICASSP 97, 
pages 71-74, Munich. 
Osamu Furuse and Hitoshi Iida. 1996. In- 
cremental translation utilizing constituent 
boundary patterns. In Proceedings of 
COLING-96, pages 412-417, Copenhagen. 
Keiko Horiguchi. 1997. Towards translating 
spoken language pragmatics in an analogical 
framework. In Proceedings ofA CL/EA CL-97 
workshop on Spoken Language Translation, 
pages 16-23, Madrid. 
Akira Kurematsu and Tsuyoshi Morimoto. 
1996. Automatic Speech Translation. Gordon 
and Breach Publishers. 
Susann LuperFoy et al 1998. An architecture 
for dialogue management, context tracking, 
and pragmatic adaptation i  spoken dialogue 
system. In Proceedings of COLING-A CL'98, 
pages 794-801, Montreal. 
Hideki Mima et al 1997. A situation-based 
approach to spoken dialogue translation be- 
tween different social roles. In Proceedings of
TMI-97, pages 176-183, Santa Fe. 
Masaki Naito et al 1998. Acoustic and lan- 
guage model for speech translation system 
ATR-MATRIX. In the Proceedings of the 
1998 Spring Meeting of the Acoustical Soci- 
ety of Japan, pages 159-160 (in Japanese). 
Manny Rayner and David Carter. 1997. Hy- 
brid language processing in the spoken lan- 
guage translator. In the 1997 International 
Conference on Acoustics, Speech, and Signal 
Processing: ICASSP 97, pages 107-110, Mu- 
nich. 
Carolyn Penstein Ros~ and Lori S. Levin. 1998. 
An interactive domain independent approach 
to robust dialogue interpretation. In Proceed- 
ings of COLING-ACL'98, pages 1129-1135, 
Montreal. 
Eiichiro Sumita et al 1999. Solutions to prob- 
lems inherent in spoken-language translation: 
The ATR-MATRIX approach. In the Ma- 
chine Translation Summit VII, pages 229- 
235, Singapore. 
Toshiyuki Takezawa et al 1998. A Japanese- 
to-English speech translation system: ATR- 
MATRIX. In the 5th International Con- 
ference On Spoken Language Processing: 
ICSLP-98, pages 2779-2782, Sydney. 
Enrique Vidal. 1997. Finite-state speech-to- 
speech translation. In the 1997 International 
Conference on Acoustics, Speech, and Signal 
Processing: ICASSP 97, pages 111-114, Mu- 
nich. 
Jae-Woo Yang and Jun Park. 1997. An exper- 
iment on Korean-to-English and Korean-to- 
Japanese spoken language translation. In the 
1997 International Conference on Acoustics, 
Speech, and Signal Processing: ICASSP 97, 
pages 87-90, Munich. 
43 
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 39?46,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Searching for Sentences Expressing Opinions 
by using Declaratively Subjective Clues 
 
 
 Nobuaki Hiroshima, Setsuo Yamada, Osamu Furuse and Ryoji Kataoka 
NTT Cyber Solutions Laboratories, NTT Corporation 
1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan 
hiroshima.nobuaki@lab.ntt.co.jp 
 
  
 
Abstract 
This paper presents a method for search-
ing the web for sentences expressing 
opinions. To retrieve an appropriate 
number of opinions that users may want 
to read, declaratively subjective clues are 
used to judge whether a sentence ex-
presses an opinion. We collected declara-
tively subjective clues in opinion-
expressing sentences from Japanese web 
pages retrieved with opinion search que-
ries. These clues were expanded with the 
semantic categories of the words in the 
sentences and were used as feature pa-
rameters in a Support Vector Machine to 
classify the sentences. Our experimental 
results using retrieved web pages on 
various topics showed that the opinion 
expressing sentences identified by the 
proposed method are congruent with sen-
tences judged by humans to express 
opinions. 
1 Introduction 
Readers have an increasing number of opportu-
nities to read opinions (personal ideas or beliefs), 
feelings (mental states), and sentiments (positive 
or negative judgments) that have been written or 
posted on web pages such as review sites, per-
sonal web sites, blogs, and BBSes. Such subjec-
tive information on the web can often be a useful 
basis for finding out what people think about a 
particular topic or making a decision. 
A number of studies on automatically extract-
ing and analyzing product reviews or reputations 
on the web have been conducted (Dave et al, 
2003; Morinaga et al, 2002; Nasukawa and Yi, 
2003; Tateishi et al, 2004; Kobayashi et al, 
2004). These studies focus on using sentiment 
analysis to extract positive or negative informa-
tion about a particular product. Different kinds 
of subjective information, such as neutral opin-
ions, requests, and judgments, which are not ex-
plicitly associated with positive/negative as-
sessments, have not often been considered in 
previous work. Although sentiments provide 
useful information, opinion-expressing sentences 
like ?In my opinion this product should be 
priced around $15,? which do not express ex-
plicitly positive or negative judgments (unlike 
sentiments) can also be informative for a user 
who wants to know others? opinions about a 
product. When a user wants to collect opinions 
about an event, project, or social phenomenon, 
requests and judgments can be useful as well as 
sentiments. With open-domain topics, sentences 
expressing sentiments should not be searched 
exclusively; other kinds of opinion expressing 
sentences should be searched as well. 
The goal of our research is to achieve a web 
search engine that locates opinion-expressing 
sentences about open-domain topics on products, 
persons, events, projects, and social phenomena. 
Sentence-level subjectivity/objectivity classifica-
tion in some of the previous research (Riloff and 
Wiebe, 2003; Wiebe and Riloff, 2005) can iden-
tify subjective statements that include specula-
tion in addition to positive/negative evaluations. 
In these efforts, the subjectivity/objectivity of a 
current sentence is judged based on the existence 
of subjective/objective clues in both the sentence 
itself and the neighboring sentences. The subjec-
tive clues, some adjective, some noun, and some 
verb phrases, as well as other collocations, are 
learned from corpora (Wiebe, 2000; Wiebe et al, 
2001). Some of the clues express subjective 
meaning unrestricted to positive/negative meas-
urements. The sentence-level subjectivity ap-
39
proach suggests a way of searching for opinion 
expressing sentences in the open domain.  
The problem of applying sentence-level sub-
jectivity classification to opinion-expressing sen-
tence searches is the likelihood of collecting too 
many sentences for a user to read. According to 
the work of Wiebe et al (2001), 70% of sen-
tences in opinion-expressing articles like editori-
als and 44% of sentences in non-opinion ex-
pressing articles like news reports were judged 
to be subjective. In analyzing opinions (Cardie 
et al, 2003; Wilson et al, 2004), judging docu-
ment-level subjectivity (Pang et al, 2002; Tur-
ney, 2002), and answering opinion questions 
(Cardie et al, 2003; Yu and Hatzivassiloglou, 
2003), the output of a sentence-level subjectivity 
classification can be used without modification. 
However, in searching opinion-expressing sen-
tences, it is necessary to designate criteria for 
opinion-expressing sentences that limit the num-
ber of retrieved sentences so that a user can sur-
vey them without difficulty. While it is difficult 
to formally define an opinion, it is possible to 
practically tailor the definition of an opinion to 
the purpose of the application (Kim and Hovy, 
2004).  
This study introduces the notion of declara-
tively subjective clues as a criterion for judging 
whether a sentence expresses an opinion and 
proposes a method for finding opinion-
expressing sentences that uses these clues. De-
claratively subjective clues such as the subjec-
tive predicate part of the main clause and subjec-
tive sentential adverb phrases suggest that the 
writer is the source of the opinion. We hypothe-
size that a user of such an ?opinion-expressing 
sentence? search wants to read the writer?s opin-
ions and that explicitly stated opinions are pre-
ferred over quoted or implicational opinions. We 
suppose that writer?s ideas or beliefs are explic-
itly declared in a sentence with declaratively 
subjective clues whereas sentences without de-
claratively subjective clues mainly describe 
things. The number of sentences with declara-
tively subjective clues is estimated to be less 
than the number of subjective sentences defined 
in the previous work. We expect that the opinion 
expressing sentences identified with our method 
will be appropriate from the both qualitative and 
quantitative viewpoints. 
Section 2 describes declaratively subjective 
clues and explains how we collected them from 
opinion-expressing sentences on Japanese web 
pages retrieved with opinion search queries. Sec-
tion 3 explains our strategy for searching opin-
ion-expressing sentences by using declaratively 
subjective clues. Section 4 evaluates the pro-
posed method and shows how the opinion-
expressing sentences found by the proposed 
method are congruent with the sentences judged 
by humans to be opinions. 
2 Declaratively Subjective Clues 
Declaratively subjective clues are a basic crite-
rion for judging whether a sentence expresses an 
opinion. We extracted the declaratively subjec-
tive clues from Japanese sentences that evalua-
tors judged to be opinions. 
2.1 Opinion-expressing Sentence Judgment 
We regard a sentence to be ?opinion expressing? 
if it explicitly declares the writer?s idea or belief 
at a sentence level. We define as a ?declaratively 
subjective clue?, the part of a sentence that con-
tributes to explicitly conveying the writer?s idea 
or belief in the opinion-expressing sentence. For 
example, "I am glad" in the sentence "I am glad 
to see you" can convey the writer?s pleasure to a 
reader, so we regard the sentence as an ?opinion-
expressing sentence? and ?I am glad? as a ?de-
claratively subjective clue.? Another example of 
a declaratively subjective clue is the exclamation 
mark in the sentence "We got a contract!" It con-
veys the writer?s emotion about the event to a 
reader. 
If a sentence only describes something ab-
stract or concrete even though it has word-level 
or phrase-level subjective parts, we do not con-
sider it to be opinion expressing. On the other 
hand, some word-level or phrase-level subjective 
parts can be declaratively subjective clues de-
pending on where they occur in the sentence. 
Consider the following two sentences. 
 
(1) This house is beautiful. 
(2) We purchased a beautiful house. 
 
Both (1) and (2) contain the word-level subjec-
tive part "beautiful". Our criterion would lead us 
to say that sentence (1) is an opinion, because 
"beautiful" is placed in the predicate part and (1) 
is considered to declare the writer?s evaluation 
of the house to a reader. This is why ?beautiful? 
in (1) is eligible as a declaratively subjective 
clue. On the other hand, sentence (2) is not 
judged to contain an opinion, because "beauti-
ful" is placed in the noun phrase, i.e., the object 
of the verb ?purchase,? and (2) is considered to 
report the event of the house purchase rather ob-
40
jectively to a reader. Sentence (2) partially con-
tains subjective information about the beauty of 
the house; however this information is unlikely 
to be what a writer wants to emphasize. Thus, 
"beautiful" in (2) does not work as a declara-
tively subjective clue. 
These two sentences illustrate the fact that the 
presence of a subjective word (?beautiful?) does 
not unconditionally assure that the sentence ex-
presses an opinion. Additionally, these examples 
do suggest that sentences containing an opinion 
can be judged depending on where such word-
level or phrase-level subjective parts as evalua-
tive adjectives are placed in the predicate part. 
Some word-level or phrase-level subjective 
parts such as subjective sentential adverbs can be 
declaratively subjective clues depending on 
where they occur in the sentence. In sentence (3), 
?amazingly? expresses the writer?s feeling about 
the event. Sentence (3) is judged to contain an 
opinion because there is a subjective sentential 
adverb in its main clause. 
 
(3) Amazingly, few people came to my party. 
 
The existence of some idiomatic collocations 
in the main clause also affects our judgment as 
to what constitutes an opinion-expressing sen-
tence. For example, sentence (4) can be judged 
as expressing an opinion because it includes ?my 
wish is?. 
 
(4) My wish is to go abroad. 
 
Thus, depending on the type of declaratively 
subjective clue, it is necessary to consider where 
the expression is placed in the sentence to judge 
whether the sentence is an opinion. 
 
2.2 Clue Expression Collection 
We collected declaratively subjective clues in 
opinion-expressing sentences from Japanese web 
pages. Figure 1 illustrates the flow of collection 
of eligible expressions. 
 
type query?s topic 
Product cell phone, car, beer, cosmetic 
Entertainment sports, movie, game, animation 
Facility  museum, zoo, hotel, shop 
Politics diplomacy, election 
Phenomena diction, social behavior 
Event firework, festival 
Culture artwork, book, music 
Organization company 
Food cuisine, noodle, ice cream 
Creature bird 
Table 1: Topic Examples 
 
First, we retrieved Japanese web pages from 
forty queries covering a wide range of topics 
such as products, entertainment, facilities, and 
phenomena, as shown in Table 1. We used que-
ries on various topics because we wanted to ac-
quire declaratively subjective clues for open-
domain opinion web searches. Most of the que-
ries contain proper nouns. These queries corre-
spond to possible situations in which a user 
wants to retrieve opinions from web pages about 
a particular topic, such as ?Cell phone X,? ?Y 
museum,? and ?Football coach Z?s ability?, 
where X, Y, and Z are proper nouns. 
Next, opinion-expressing sentences were ex-
tracted from the top twenty retrieved web pages 
in each query, 800 pages in total. There were 
75,575 sentences in these pages.  
Figure 1: Flow of Clue Expression Collection 
41
 Three evaluators judged whether each sen-
tence contained an opinion or not. The 13,363 
sentences judged to do so by all three evaluators 
were very likely to be opinion expressing. The 
number of sentences which three evaluators 
agreed on as non-opinion expressing was 
42,346.1 Out of the 13,363 opinion expressing 
sentences, 8,425 were then used to extract de-
claratively subjective clues and learn positive 
examples in a Support Vector Machine (SVM), 
and 4,938 were used to assess the performance 
of opinion expressing sentence search (Section 
4). Out of the 42,346 non-opinion sentences, 
26,340 were used to learn negative examples, 
and 16,006 were used to assess, keeping the 
number ratio of the positive and negative exam-
ple sentences in learning and assessing. 
One analyst extracted declaratively subjective 
clues from 8,425 of the 13,363 opinion-
expressing sentences, and another analyst 
checked the result. The number of declaratively 
                                                 
1 Note that not all of these opinion-expressing sentences 
retrieved were closely related to the query because some of 
the pages described miscellaneous topics.  
subjective clues obtained was 2,936. These clues 
were classified into fourteen types as shown in 
Table 2, where the underlined expressions in 
example sentences are extracted as declaratively 
subjective clues. The example sentences in Table 
2 are Japanese opinion-expressing sentences and 
their English translations. Although some Eng-
lish counterparts of Japanese clue expressions 
might not be cogent because of the characteristic 
difference between Japanese and English, the 
clue types are likely to be language-independent. 
We can see that various types of expressions 
compose opinion-expressing sentences. 
As mentioned in Section 2.1, it is important to 
check where a declaratively subjective clue ap-
pears in the sentence in order to apply our crite-
rion of whether the sentence is an opinion or not. 
The clues in the types other than (b), (c) and (l) 
usually appear in the predicate part of a main 
clause.  
The declaratively subjective clues in Japanese 
examples are placed in the rear parts of sen-
tences except in types (b), (c) and (l). This re-
flects the heuristic rule that Japanese predicate 
 type example sentence (English translation of Japanese sentence) 
(a) Thought Kono hon wa kare no dato omou. 
(I think this book is his.) 
(b) Declarative adverb Tabun rainen yooroppa ni iku. 
(I will possibly go to Europe next year.) 
(c) Interjection Waa, suteki. 
(Oh, wonderful.) 
(d) Intensifier Karera wa totemo jouzu ni asonda. 
(They played extremely well) 
(e) Impression Kono yougo wa yayakoshii. 
(This terminology is confusing.) 
(f) Emotion Oai dekite ureshii desu. 
(I am glad to see you.) 
(g) Positive/negative judgment Anata no oodio kiki wa sugoi. 
(Your audio system is terrific.) 
(h) Modality about propositional attitude Sono eiga wo miru beki da. 
(You should go to the movie.) 
(i) Value judgment Kono bun wa imi fumei da. 
(This sentence makes no sense.) 
(j) Utterance-specific sentence form Towa ittemo,ima wa tada no yume dakedo. 
(Though, it's literally just a dream now.) 
(k) Symbol Keiyaku wo tottazo! 
(We got a contract!)
(l) Idiomatic collocation Ii nikui. 
(It's hard to say.) 
(m) Uncertainty Ohiru ni nani wo tabeyou kanaa. 
(I am wondering what I should eat for lunch.) 
(n) Imperative Saizen wo tukushi nasai. 
(Give it your best.) 
Table 2: Clue Types
42
parts are in principle placed in the rear part of a 
sentence. 
 
3 Opinion-Sentence Extraction 
In this section, we explain the method of classi-
fying each sentence by using declaratively sub-
jective clues. 
The simplest method for automatically judging 
whether a sentence is an opinion is a rule-based 
one that extracts sentences that include declara-
tively subjective clues. However, as mentioned 
in Section 2, the existence of declaratively sub-
jective clues does not assure that the sentence 
expresses an opinion. It is a daunting task to 
write rules that describe how each declaratively 
subjective clue should appear in an opinion-
expressing sentence. A more serious problem is 
that an insufficient collection of declaratively 
subjective clues will lead to poor extraction per-
formance. 
For that reason, we adopted a learning method 
that binarily classifies sentences by using de-
claratively subjective clues and their positions in 
sentences as feature parameters of an SVM. 
With this method, a consistent framework of 
classification can be maintained even if we add 
new declaratively subjective clues, and it is pos-
sible that we can extract the opinion-expressing 
sentences which have unknown declaratively 
subjective clues. 
3.1 Augmentation by Semantic Categories 
Before we can use declaratively subjective clues 
as feature parameters, we must address two is-
sues: 
? Cost of building a corpus:  It is costly 
to provide a sufficient amount of tagged 
corpus of opinion-expressing-sentence la-
bels to ensure that learning achieves a 
high-performance extraction capability. 
? Coverage of words co-occurring with 
declaratively subjective clues:  Many of 
the declaratively subjective clue expres-
sions have co-occurring words in the 
opinion-expressing sentence. Consider the 
following two sentences. 
(5) The sky is high. 
(6) The quality of this product is high. 
 
Both (5) and (6) contain the word "high" 
in the predicate part. Sentence (5) is con-
sidered to be less of an opinion than (6) 
because an evaluator might judge (5) to be 
the objective truth, while all evaluators are 
likely to judge (6) to be an opinion. The 
adjective "high" in the predicate part can 
be validated as a declaratively subjective 
clue depending on co-occurring words. 
However, it is not realistic to provide all 
possible co-occurring words with each 
declaratively subjective clue expression. 
Semantic categories can be of help in dealing 
with the above two issues. Declaratively subjec-
tive clue expressions can be augmented by se-
mantic categories of the words in the expressions. 
An augmentation involving both declaratively 
subjective clues and co-occurrences will increase 
feature parameters. In our implementation, we 
adopted the semantic categories proposed by 
Ikehara et al (1997). Utilization of semantic 
categories has another effect: it improves the 
extraction performance. Consider the following 
two sentence patterns: 
 
(7) X is beautiful. 
(8) X is pretty. 
 
The words "beautiful" and "pretty" are adjec-
tives in the common semantic category, "appear-
ance", and the degree of declarative subjectivity 
of these sentences is almost the same regardless 
of what X is. Therefore, even if "beautiful" is 
learned as a declaratively subjective clue but 
"pretty" is not, the semantic category "appear-
ance" that the learned word "beautiful" belongs 
to, enables (8) to be judged opinion expressing 
as well as (7). 
3.2 Feature Parameters to Learn 
We implemented our opinion-sentence extrac-
tion method by using a Support Vector Machine 
(SVM) because an SVM can efficiently learn the 
model for classifying sentences into opinion-
expressing and non-opinion expressing, based on 
the combinations of multiple feature parameters. 
The following are the crucial feature parameters 
of our method. 
? 2,936 declaratively subjective clues 
? 2,715 semantic categories that words in 
a sentence can fall into 
If the sentence has a declaratively subjective 
clue of type (b), (c) or (l) in Table 2, the feature 
parameter about the clue is assigned a value of 1; 
if not, it is assigned 0. If the sentence has de-
claratively subjective clues belonging to types 
43
other than (b), (c) or (l) in the predicate part, the 
feature parameter about the clue is assigned 1; if 
not, it is assigned 0. 
The feature parameters for the semantic cate-
gory are used to compensate for the insufficient 
amount of declaratively subjective clues pro-
vided and to consider co-occurring words with 
clue expressions in the opinion-expressing sen-
tences, as mentioned in Section 3.1. 
The following are additional feature parame-
ters. 
? 150 frequent words 
? 13 parts of speech 
Each feature parameter is assigned a value of 1 if 
the sentence has any of the frequent words or 
parts of speech. We added these feature parame-
ters based on the hypotheses that some frequent 
words in Japanese have the function of changing 
the degree of declarative subjectivity, and that 
the existence of such parts of speech as adjec-
tives and adverbs possibly influences the de-
clarative subjectivity. The effectiveness of these 
additional feature parameters was confirmed in 
our preliminary experiment. 
4 Experiments 
We conducted three experiments to assess the 
validity of the proposed method: comparison 
with baseline methods, effectiveness of position 
information in SVM feature parameters, and ef-
fectiveness of SVM feature parameters such as 
declaratively subjective clues and semantic cate-
gories. 
All experiments were performed using the 
Japanese sentences described in Section 2.1.  We 
used 8,425 opinion expressing sentences, which 
were used to collect declaratively subjective 
clues as a training set, and used 4,938 opinion-
expressing sentences as a test set. We also used 
26,340 non-opinion sentences as a training set 
and used 16,006 non-opinion sentences as a test 
set. The test set was divided into ten equal sub-
sets. The experiments were evaluated with the 
following measures following the variable 
scheme in Table 3: 
ba
a
Pop +=   ca
a
Rop +=  
opop
opop
op RP
RP
F +=
2
 
dc
d
P opno +=_  db
d
R opno +=_  
opnoopno
opnoopno
opno RP
RP
F
__
__
_
2
+=  
 
dcba
da
A +++
+=  
 
We evaluated ten subsets with the above 
measures and took the average of these results. 
4.1  Comparison with Baseline Methods 
We first performed an experiment comparing 
two baseline methods with our proposed method. 
We prepared a baseline method that regards a 
sentence as an opinion if it contains a number of 
declaratively subjective clues that exceeds a cer-
tain threshold. The best threshold was set 
through trial and error at five occurrences. We 
also prepared another baseline method that 
learns a model and classifies a sentence using 
only features about a bag of words. 
The experimental results are shown in Table 4. 
It can be seen that our method performs better 
than the two baseline methods. Though the dif-
ference between our method?s results and those 
of the bag-of-words method seems rather small, 
the superiority of the proposed method cannot be 
rejected at the significance level of 5% in t-test. 
Answer 
System 
Opinion No opinion 
Opinion a b 
No opinion c d 
Opinion No opinion 
Method 
Precision Recall F-measure Precision Recall F-measure 
Accuracy 
Occurrences of DS clues 
(baseline 1) 
66.4% 35.3% 46.0% 82.6% 94.5% 88.1% 80.5% 
Bag of words 
(baseline 2) 
80.9% 64.2% 71.6% 89.6% 95.3% 92.4% 88.0% 
Proposed 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% 
Table 4: Results for comparison with baseline methods 
Table 3: Number of sentences in a test set 
44
 4.2 Feature Parameters with Position In-
formation 
We inspected the effect of position information 
of 2,936 declaratively subjective clues based on 
the heuristic rule that a Japanese predicate part 
almost always appears in the last ten words in a 
sentence. Instead of more precisely identifying 
predicate position from parsing information, we 
employed this heuristic rule as a feature parame-
ter in the SVM learner for practical reasons. 
Table 5 lists the experimental results. "All 
words" indicates that all feature parameters are 
permitted at any position in the sentence. "Last 
10 words" indicates that all feature parameters 
are permitted only if they occur within the last 
ten words in the sentence.  
We can see that feature parameters with posi-
tion information perform better than those with-
out position information in all evaluations. This 
result confirms our claim that the position of the 
feature parameters is important for judging 
whether a sentence is an opinion or not. 
However, the difference did not indicate supe-
riority between the two results at the significance 
level of 5%. In the ?last 10 word? experiment, 
we restricted the position of 422 declaratively 
subjective clues like (b), (c) and (l) in Table 2, 
which appear in any position of a sentence, to 
the same conditions as with the other types of 
2,514 declaratively subjective clues. The fact 
that the equal position restriction on all declara-
tively subjective clues slightly improved per-
formance suggests there will be significant im-
provement in performance from assigning the 
individual position condition to each declara-
tively subjective clue. 
4.3 Effect of Feature Parameters 
The third experiment was designed to ascertain 
the effects of declaratively subjective clues and 
semantic categories. The declaratively subjective 
clues and semantic categories were employed as 
feature parameters for the SVM learner. The ef-
fect of each particular feature parameter can be 
seen by using it without the other feature pa-
rameter, because the feature parameters are in-
dependent of each other. 
The experimental results are shown in Table 6. 
The first row shows trials using only frequent 
words and parts of speech as feature parameters. 
"Y" in the first and second columns indicates 
exclusive use of declaratively subjective clues 
and semantic categories as the feature parame-
ters, respectively. For instance, we can deter-
mine the effect of declaratively subjective clues 
by comparing the first row with the second row. 
The results show the effects of declaratively 
subjective clues and semantic categories. The 
results of the first row show that the method us-
ing only frequent words and parts of speech as 
the feature parameters cannot precisely classify 
subjective sentences. Additionally, the last row 
of the results clearly shows that using both de-
claratively subjective clues and semantic catego-
ries as the feature parameters is the most effec-
tive. The difference between the last row of the 
results and the other rows cannot be rejected 
even at the significance level of 5%. 
Feature sets Opinion No opinion 
DS 
clues 
Semantic 
categories 
Precision Recall F-
measure
Precision Recall F-
measure 
Accuracy 
  71.4% 53.2% 60.9% 87.7% 94.1% 90.8% 85.2% 
Y  79.9% 64.3% 71.2% 89.6% 95.0% 92.2% 87.8% 
 Y 76.1% 68.9% 72.2% 90.7% 93.3% 92.0% 87.5% 
Y Y 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% 
Opinion No opinion Position 
Precision Recall F-measure Precision Recall F-measure 
Accuracy 
All words 76.8% 70.6% 73.5% 91.2% 93.4% 92.3% 88.0% 
Last 10 words 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% 
Table 5: Results for feature parameters with position information 
Table 6: Results for effect of feature parameters 
45
5 Conclusion and Future Work 
 We proposed a method of extracting sentences 
classified by an SVM as opinion-expressing that 
uses feature sets of declaratively subjective clues 
collected from opinion-expressing sentences in 
Japanese web pages and semantic categories of 
words obtained from a Japanese lexicon. The 
first experiment showed that our method per-
formed better than baseline methods. The second 
experiment suggested that our method performed 
better when extraction of features was limited to 
the predicate part of a sentence rather than al-
lowed anywhere in the sentence. The last ex-
periment showed that using both declaratively 
subjective clues and semantic categories as fea-
ture parameters yielded better results than using 
either clues or categories exclusively. 
Our future work will attempt to develop an 
open-domain opinion web search engine. To 
succeed, we first need to augment the proposed 
opinion-sentence extraction method by incorpo-
rating the query relevancy mechanism. Accord-
ingly, a user will be able to retrieve opinion-
expressing sentences relevant to the query. Sec-
ond, we need to classify extracted sentences in 
terms of emotion, sentiment, requirement, and 
suggestion so that a user can retrieve relevant 
opinions on demand. Finally, we need to sum-
marize the extracted sentences so that the user 
can quickly learn what the writer wanted to say.  
References 
Claire Cardie, Janyce Wiebe, Theresa Wilson, and 
Diane J. Litman. 2003. Combining Low-Level and 
Summary Representations of Opinions. for Multi-
Perspective Question Answering. Working Notes - 
New Directions in Question Answering (AAAI 
Spring Symposium Series) . 
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: Opinion extraction 
and semantic classification of product reviews. Pro-
ceedings of the 12th International World Wide Web 
Conference, 519-528. 
Satoru Ikehara, Masahiro Miyazaki, Akio Yokoo, Sato-
shi Shirai, Hiromi Nakaiwa, Kentaro Ogura, Yoshi-
fumi Ooyama, and Yoshihiko Hayashi. 1997. Ni-
hongo Goi Taikei ? A Japanese Lexicon. Iwanami 
Shoten. 5 volumes. (In Japanese). 
Soo-Min Kim and Eduard Hovy. 2004. Determining the 
Sentiment of Opinions. Proceedings of the. COLING-
04. 
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, 
Kenji Tateishi, and Toshikazu Fukushima. 2004. Col-
lecting Evaluative Expressions for Opinion Extrac-
tion. Proceedings of the First International Joint Con-
ference on Natural Language Processing (IJCNLP-
04), 584-589. 
Satoshi Morinaga, Kenji Yamanishi, and Kenji Tateishi. 
2002. Mining Product Reputations on the Web. Pro-
ceedings of the eighth ACM SIGKDD International 
Conference on Knowledge Discovery and Data Min-
ing (KDD 2002).  
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), 76-86. 
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment 
Analysis: Capturing Favorability Using Natural 
Language Processing. Proceedings of the 2nd Inter-
national Conference on Knowledge Capture(K-CAP 
2003). 
Ellen Riloff and Janyce Wiebe. 2003. Learning Extrac-
tion Patterns for Subjective Expressions. Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-03), 105-112. 
Kenji Tateishi, Yoshihide Ishiguro, and Toshikazu Fu-
kushima, 2004. A Reputation Search Engine that 
Collects People?s Opinions by Information Extrac-
tion Technology, IPSJ Transactions Vol. 45 
No.SIG07, 115-123. 
Peter Turney. 2002. Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. Proceedings of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-2002), 417-424. 
Janyce Wiebe. 2000. Learning Subjective Adjectives 
from Corpora. Proceedings of the 17th National Con-
ference on Artificial Intelligence (AAAI -2000).  
Janyce Wiebe, Theresa Wilson, and Matthew Bell. 2001. 
Identifying Collocations for Recognizing Opinions. 
Proceedings of ACL/EACL 2001 Workshop on Col-
location.  
Janyce Wiebe and Ellen Riloff. 2005. Creating Subjec-
tive and Objective Sentence Classifiers from Unanno-
tated Texts. Proceedings of Sixth International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing-2005), 486-497.  
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa, 2004. 
Just how mad are you? Finding strong and weak 
opinion clauses. Proceeding of the AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text: Theories and Applications. 
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards Answering Opinion Questions: Separating 
Facts from Opinions and Identifying the Polarity of 
Opinion Sentences. Proceedings of the Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP-2003). 
 
46
