Why Nitpicking Works: Evidence for Occam?s Razor in Error Correctors
Dekai WU?1 Grace NGAI?2 Marine CARPUAT?
dekai@cs.ust.hk csgngai@polyu.edu.hk marine@cs.ust.hk
? Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
? Hong Kong Polytechnic University
Department of Computing
Kowloon
Hong Kong
Abstract
Empirical experience and observations have shown us when
powerful and highly tunable classifiers such as maximum en-
tropy classifiers, boosting and SVMs are applied to language
processing tasks, it is possible to achieve high accuracies, but
eventually their performances all tend to plateau out at around
the same point. To further improve performance, various error
correction mechanisms have been developed, but in practice,
most of them cannot be relied on to predictably improve per-
formance on unseen data; indeed, depending upon the test set,
they are as likely to degrade accuracy as to improve it. This
problem is especially severe if the base classifier has already
been finely tuned.
In recent work, we introduced N-fold Templated Piped Cor-
rection, or NTPC (?nitpick?), an intriguing error corrector that
is designed to work in these extreme operating conditions. De-
spite its simplicity, it consistently and robustly improves the ac-
curacy of existing highly accurate base models. This paper in-
vestigates some of the more surprising claims made by NTPC,
and presents experiments supporting an Occam?s Razor argu-
ment that more complex models are damaging or unnecessary
in practice.
1 Introduction
The investigation we describe here arose from a very
commonly discussed experience, apparently triggered by
the recent popularity of shared task evaluations that have
opened opportunities for researchers to informally com-
pare their experiences ?with a common denominator?, so
to speak.
Among the perennial observations which are made
during the analysis of the results is that (1) methods de-
signed to ?fine-tune? the high-accuracy base classifiers
behave unpredictably, their success or failure often ap-
pearing far more sensitive to where the test set was drawn
from, rather than on any true quality of the ?fine-tuning?,
and consequently, (2) the resulting system rankings are
often unpredictable, especially as they are typically con-
ducted only on a single new test set, often drawn from
a single arbitrary new source of a significantly different
nature than the training sets. One could argue that such
evaluations do not constitute a fair test, but in fact, this is
1The author would like to thank the Hong Kong Research Grants
Council (RGC) for supporting this research in part through research
grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.
2The author would like to thank the Hong Kong Polytechnic Univer-
sity for supporting this research in part through research grants A-PE37
and 4-Z03S.
where computational linguistics modeling diverges from
machine learning theory, since for any serious NLP ap-
plication, such evaluations constitute a much more accu-
rate representation of the real world.
We believe one primary reason for this common ex-
perience is that the models involved are typically al-
ready operating well beyond the limits of accuracy of
the models? assumptions about the nature of distributions
from which testing samples will be drawn. For this rea-
son, even ?sophisticated? discriminative training crite-
ria, such as maximum entropy, minimum error rate, and
minimum Bayes risk, are susceptible to these stability
problems. There has been much theoretical work done
on error correction, but in practice, any error correction
usually lowers the performance of the combined system
on unseen data, rather than improving it. Unfortunately,
most existing theory simply does not apply.
This is especially true if the base model has been
highly tuned. For the majority of tasks, the performance
of the trained models, after much fine tuning, tend to
plateau out at around the same point, regardless of the
theoretical basis of the underlying model. This holds
true with most highly accurate classifiers, including max-
imum entropy classifiers, SVMs, and boosting models.
In addition, even though data analysis gives us some gen-
eral idea as to what kinds of feature conjunctions might
help, the classifiers are not able to incorporate those
into their model (usually because the computational cost
would be infeasible), and any further post-processing
tends to degrade accuracy on unseen data. The common
practice of further improving accuracy at this point is to
resort to ad hoc classifier combination methods, which
are usually not theoretically well justified and, again, un-
predictably improve or degrade performance?thus con-
suming vast amounts of experimental resources with rel-
atively low expected payoff, much like a lottery.
There are a variety of reasons for this, ranging from
the aforementioned validity of the assumptions about the
distribution between the training and test corpora, to the
absence of a well justified stopping point for error cor-
rection. The latter problem is much more serious than it
seems at first blush, since without a well-justified stop-
ping criterion, the performance of the combined model
will be much more dependent upon the distribution of
the test set, than on any feature engineering. Empirical
evidence for this argument can be seen from the result of
the CoNLL shared tasks (Tjong Kim Sang, 2002)(Tjong
Kim Sang and Meulder, 2003), where the ranking of the
participating systems changes with the test corpora.
Inspired by the repeated observations of this phe-
nomenon by many participants, we decided to stop
?sweeping the issue under the rug?, and undertook to
confront it head-on. Accordingly, we challenged our-
selves to design an error corrector satisfying the follow-
ing criteria, which few if any existing models actually
meet: (1) it would leverage off existing base models,
while targeting their errors; (2) it would consistently im-
prove accuracy, even on top of base models that already
deliver high accuracy; (3) it would be robust and con-
servative, so as to almost never accidentally degrade ac-
curacy; (4) it would be broadly applicable to any classi-
fication or recognition task, especially high-dimensional
ones such as named-entity recognition and word-sense
disambiguation; and (5) it would be template-driven and
easily customizable, which would enable it to target er-
ror patterns beyond the base models? representation and
computational complexity limitations.
Our goal in this undertaking was to invent as little as
possible. We expected to make use of relatively sophis-
ticated error-minimization techniques. Thus the results
were surprising: the simplest models kept outperform-
ing the ?sophisticated? models. This paper attempts to
investigate some of the key reasons why.
To avoid reinventing the wheel, we originally
considered adapting an existing error-driven method,
transformation-based learning (TBL) for this purpose.
TBL seems well suited to the problem as it is inherently
an error corrector and, on its own, has been shown to
achieve high accuracies on a variety of problems (see
Section 4). Our original goal was to adapt TBL for
error correction of high-performing models (Wu et al,
2004a), with two main principles: (1) since it is not clear
that the usual assumptions made about the distribution
of the training/test data are valid in such extreme oper-
ating ranges, empirical observations would take prece-
dence over theoretical models, which implies that (2) any
model would have to be empirically justified by testing
on a diverse range of data. Experimental observations,
however, increasingly drove us toward different goals.
Our resulting error corrector, NTPC, was instead con-
structed on the principle of making as few assumptions as
possible in order to robustly generalize over diverse situ-
ations and problems. One observation made in the course
of experimentation, after many attempts at fine-tuning
model parameters, was that many of the complex theo-
retical models for error correction often do not perform
consistently. This is perhaps not too surprising upon fur-
ther reflection, since the principle of Occam?s Razor does
prefer simpler hypotheses over more complex ones.
NTPC was introduced in (Wu et al, 2004b), where the
controversial issues it raised generated a number of in-
teresting questions, many of which were were directed at
NTPC?s seeming simplicity, which seems in opposition
to the theory behind many other error correcting models.
In this paper, we investigate the most commonly-asked
questions. We illuminate these questions by contrasting
NTPC against the more powerful TBL, presenting ex-
periments that show that NTPC?s simple model is indeed
Figure 1: Piped architecture with n-fold partitioning.
key to its robustness and reliability.
The rest of the paper is laid out as follows: Section 2
presents an introduction to NTPC, including an overview
of its architecture. Section 3 addresses key questions re-
lated to NTPC?s architecture and presents empirical re-
sults justifying its simplicity.
2 N-fold Templated Piped Correction
N-fold Templated Piped Correction or NTPC, is a
model that is designed to robustly improve the accuracy
of existing base models in a diverse range of operating
conditions. As was described above, the most challeng-
ing situations for any error corrector is when the base
model has been finely tuned and the performance has
reached a plateau. Most of the time, any further feature
engineering or error correction after that point will end
up hurting performance rather than improving it.
2.1 The architecture of NTPC is surprisingly
simple
One of the most surprising things about NTPC lies in
the fact that despite its simplicity, it outperforms math-
ematically much more ?sophisticated? methods at error
correcting. Architecturally, it relies on a simple rule-
learning mechanism and cross-partitioning of the train-
ing data to learn very conservative, cautious rules that
make only a few corrections at a time.
Figure 1 illustrates the NTPC architecture. Prior to
learning, NTPC is given (1) a set of rule templates which
describe the types of rules that it is allowed to hypothe-
size, (2) a single base learning model, and (3) an anno-
tated training set.
The NTPC architecture is essentially a sequen-
tially chained piped ensemble that incorporates cross-
validation style n-fold partition sets generated from the
base model. The training set is partitioned n times in
order to train n base models. Subsequently the n held-
out validation sets are classified by the respective trained
base models, with the results combined into a ?reconsti-
tuted? training set. The reconstituted training set is used
by Error Corrector Learner, which learns a set of rules.
Rule hypotheses are generated according to the given set
of allowable templates:
R = {r| r ? H ? ? (r) > ?min ?  (r) = 0} (1)
?(r) =
?X
j=1
?
r(xj ,y?j) 6=?
?(r(xj , y?j), yj) (2)
(r) =
?X
j=1
?
r(xj ,y?j) 6=?
1? ?(r(xj , y?j), yj)(3)
where X is a sequence of X training examples xi, Y
is a sequence of reference labels yi for each example
respectively, Y? is a sequence of labels y?i as predicted
by the base model for each example respectively, H is
the hypothesis space of valid rules implied by the tem-
plates, and ?min is a confidence threshold. Setting ?min
to a relatively high value (say 15) implements the re-
quirement of high reliability. R is subsequently sorted
by the ?i value of each rule ri into an ordered list of rules
R? = (r?0, . . . , r
?
i?1).
During the evaluation phase, depicted in the lower por-
tion of Figure 1, the test set is first labeled by the base
model. The error corrector?s rules r?i are then applied in
the order of R? to the evaluation set. The final classifica-
tion of a sample is then the classification attained when
all the rules have been applied.
2.2 NTPC consistently and robustly improves
accuracy of highly-accurate base models
In previous work (Wu et al, 2004b), we presented ex-
periments on named-entity identification and classifica-
tion across four diverse languages, using Adaboost.MH
as the base learner, which showed that NTPC was ca-
pable of robustly and consistently improving upon the
accuracy of the already-highly-accurate boosting model;
correcting the errors committed by the base model but
not introducing any of its own.
Table 1 compares results obtained with the base Ad-
aboost.MH model (Schapire and Singer, 2000) and the
NTPC-enhanced model for a total of eight different
named-entity recognition (NER) models. These experi-
ments were performed on the CoNLL-2002 and CoNLL-
2003 shared task data sets. It can be seen that the Ad-
aboost.MH base models clearly already achieve high ac-
curacy, setting the bar very high for NTPC to improve
upon. However, it can also be seen that NTPC yields fur-
ther F-Measure gains on every combination of task and
language, including English NE bracketing (Model M2)
for which the base F-Measure is the highest.
An examination of the rules (shown in the Appendix)
can give an idea as to why NTPC manages to identify
and correct errors which were overlooked by the highly
tuned base model. NTPC?s advantage comes from two
aspects: (1) its ability to handle complex conjunctions
of features, which often reflect structured, linguistically
motivated expectations, in the form of rule templates;
and (2) its ability to ?look forward? at classifications
from the right context, even when processing the sen-
tence in a left-to-right direction. The base classifier is
unable to incorporate these two aspects, because (1) in-
cluding complex conjunctions of features would raise the
computational cost of searching the feature space to a
point where it would be infeasible, and (2) most classi-
fiers process a sentence from left-to-right, deciding on
the class label for each word before moving on to the
next one. Rules that exploit these advantages are eas-
ily picked out in the table; many of the rules (especially
those in the top 5 for both English and Spanish) consist of
complex conjunctions of features; and rules that consider
the right context classifications can be identified by the
string ?ne <num>?, where <num> is a positive integer
(indicating how many words to the right).
3 Experiments
The most commonly-raised issues about NTPC relate
to the differences between NTPC and TBL (though the
conceptual issues are much the same as for other error-
minimization criteria, such as minimum error rate or
minimum Bayes risk). This is expected, since it was
one of our goals to reinvent as little as possible. As
a result, NTPC does bear a superficial resemblance to
TBL, both of them being error-driven learning methods
that seek to incrementally correct errors in a corpus by
learning rules that are determined by a set of templates.
One of the most frequently asked questions is whether
the Error Corrector Learner portion of NTPC could be
replaced by a transformation-based learner. This section
will investigate the differences between NTPC and TBL,
and show the necessity of the changes that were incorpo-
rated into NTPC.
The experiments run in this section were performed
on the data sets used in the CoNLL-2002 and CoNLL-
2003 Named Entity Recognition shared tasks. The
high-performing base model is based on AdaBoost.MH
(Schapire and Singer, 2000), the multi-class generaliza-
tion of the original boosting algorithm, which imple-
ments boosting on top of decision stump classifiers (de-
cision trees of depth one).
3.1 Any Error is Bad
The first main difference between NTPC and TBL, and
also what seems to be an extreme design decision on
the part of NTPC, is the objective scoring function. To
be maximally certain of not introducing any new errors
with its rules, the first requirement that NTPC?s objective
function places onto any candidate rules is that they must
not introduce any new errors ( (r) = 0). This is called
the zero error tolerance principle.
To those who are used to learners such as
transformation-based learning and decision lists, which
allow for some degree of error tolerance, this design prin-
ciple seems overly harsh and inflexible. Indeed, for al-
Table 1: NTPC consistently yields improvements on all eight different high-accuracy NER base models, across every
combination of task and language.
Model Task Language Model Precision Recall F-Measure1
M1 Bracketing Dutch Base 87.27 91.48 89.33
Base w/ NTPC 87.44 92.04 89.68
M2 Bracketing English Base 95.01 93.98 94.49
Base w/ NTPC 95.23 94.05 94.64
M3 Bracketing German Base 83.44 65.86 73.62
Base w/ NTPC 83.43 65.91 73.64
M4 Bracketing Spanish Base 89.46 87.57 88.50
Base w/ NTPC 89.77 88.07 88.91
M5 Classification + Bracketing Dutch Base 70.26 73.64 71.91
Base w/ NTPC 70.27 73.97 72.07
M6 Classification + Bracketing English Base 88.64 87.68 88.16
Base w/ NTPC 88.93 87.83 88.37
M7 Classification + Bracketing German Base 75.20 59.35 66.34
Base w/ NTPC 75.19 59.41 66.37
M8 Classification + Bracketing Spanish Base 74.11 72.54 73.32
Base w/ NTPC 74.43 73.02 73.72
most all models, there is an implicit assumption that the
scoring function will be based on the difference between
the positive and negative applications, rather than on an
absolute number of corrections or mistakes.
Results for eight experiments are shown in Figures 2
and 3. Each experiment compares NTPC against other
variants that allow relaxed  (r) ? max conditions for
various max ? {1, 2, 3, 4,?}. The worst curve in each
case is for max = ?? in other words, the system that
only considers net performance improvement, as TBL
and many other rule-based models do. The results con-
firm empirically that the  (r) = 0 condition (1) gives the
most consistent results, and (2) generally yields accura-
cies among the highest, regardless of how long training is
allowed to continue. In other words, the presence of any
negative application during the training phase will cause
the error corrector to behave unpredictably, and the more
complex model of greater error tolerance is unnecessary
in practice.
3.2 Rule Interaction is Unreliable
Another key difference between NTPC and TBL is the
process of rule interaction. Since TBL allows a rule to
use the current classification of a sample and its neigh-
bours as features, and a rule updates the current state of
the corpus when it applies to a sample, the application
of one rule could end up changing the applicability (or
not) of another rule. From the point of view of a sam-
ple, its classification could depend on the classification
of ?nearby? samples. Typically, these ?nearby? samples
are those found in the immediately preceding or succeed-
ing words of the same sentence. This rule interaction is
permitted in both training and testing.
NTPC, however, does not allow for this kind of rule in-
teraction. Rule applications only update the output clas-
sification of a sample, and do not update the current state
of the corpus. In other words, the feature values for a
Figure 2: NTPC?s zero tolerance condition yields less
fluctuation and generally higher accuracy than the re-
laxed tolerance variations, in bracketing experiments.
(bold = NTPC, dashed = relaxed tolerance)
sample are initialized once, at the beginning of the pro-
gram, and not changed again thereafter. The rationale for
making this decision is the hypothesis that rule interac-
tion is in nature unreliable, since the high-accuracy base
model provides sparse opportunities for rule application
and thus much sparser opportunities for rule interaction,
making any rule that relies on rule interaction suspect.
As a matter of fact, by considering only rules that make
no mistake during the learning phase, NTPC?s zero error
tolerance already eliminates any correction of labels that
results from rule interaction?since a label correction on
a sample that results from the application of more than
one rule necessarily implies that at least one of the rules
made a mistake.
Since TBL is a widely used error-correcting method,
Figure 3: NTPC?s zero tolerance condition yields less
fluctuation and generally higher accuracy than the re-
laxed tolerance variations, in bracketing + classification
experiments. (bold = NTPC, dashed = relaxed tolerance)
Figure 4: Unpredictable fluctuations on the bracket-
ing task show that allowing TBL-style rule interaction
does not yield reliable improvement over NTPC. (bold =
NTPC, dashed = rule interaction)
Figure 5: Unpredictable fluctuations on the bracket-
ing + classification task show that allowing TBL-style
rule interaction does not yield reliable improvement over
NTPC. (bold = NTPC, dashed = rule interaction)
it is natural to speculate that NTPC?s omission of rule in-
teraction is a weakness. In order to test this question, we
implemented an iterative variation of NTPC that allows
rule interaction, where each iteration targets the residual
error from previous iterations as follows:
1. i? 0,X0 ? X
2. r?i ? null, s?i ? 0
3. foreach r ? H such that i (r) = 0
? if ?i (r) > ??i then r?i ? r, ??i ? ?i (r)
4. if ??i < ?min then return
5. Xi+1 ? result of applying r?i to Xi
6. i? i + 1
7. goto Step 3
where
?i(r) =
?X
j=1
?
r(xij ,y?j) 6=?
?(r(xij , y?j), yj)
i(r) =
?X
j=1
?
r(xij ,y?j) 6=?
1? ?(r(xij , y?j), yj)
Here, incremental rule interaction is a natural conse-
quence of arranging the structure of the algorithm to ob-
serve the right context features coming from the base
model, as with transformation-based learning. In Step
5 of the algorithm, the current state of the corpus is up-
dated with the latest rule on each iteration. That is, in
each given iteration of the outer loop, the learner consid-
ers the corrected training data obtained by applying rules
learned in the previous iterations, so the learner has ac-
cess to the labels that result from applying the previous
rules. Since these rules may apply anywhere in the cor-
pus, the learner is not restricted to using only labels from
the left context.
The time complexity of this variation is an order of
magnitude more expensive than NTPC, due to the need
to allow rule interaction using nested loops. The ordered
list of output rules r?0 , . . . , r?i?1is learned in a greedy
fashion, to progressively improve upon the performance
of the learning algorithm on the training set.
Results for eight experiments on this variation, shown
in Figures 4 and 5, demonstrate that this expensive extra
capability is rarely useful in practice and does not reli-
ably guarantee that accuracy will not be degraded. This
is yet another illustration of the principle that, in high-
accuracy error correction problems, at least, more simple
modes of operation should be preferred over more com-
plex arrangements.
3.3 NTPC vs. N-fold TBL
Another question on NTPC that is frequently raised is
whether or not ordinary TBL, which is after all, intrinsi-
cally an error-correcting model, can be used in place of
NTPC to perform better error correction. Figure 6 shows
the results of four sets of experiments evaluating this ap-
proach on top of boosting. As might be expected from
extrapolation from the foregoing experiments that inves-
tigated their individual differences, NTPC outperforms
the more complex TBL in all cases, regardless of how
long training is allowed to continue.
Figure 6: NTPC consistently outperforms error correc-
tion using TBL even when n-fold partitioning is used.
(bold = NTPC, dashed = TBL with n-fold partitioning)
Table 2: The more complex partition-based voted er-
ror corrector degrades performance, while NTPC helps
(bracketing + classification, English).
Model Precision Recall F-Measure1
Base 95.01 93.98 94.49
Partition-Based
Voting
95.07 93.79 94.43
Base w/ NTPC 95.14 94.05 94.59
3.4 NTPC vs. Partition-Based Voting
Another valid question would be to ask if the way that
NTPC combines the results of the n-fold partitioning is
oversimplistic and could be improved upon. As was pre-
viously stated, the training corpus for the error correc-
tor in NTPC is the ?reconstituted training set? gener-
ated by combining the held-out validation sets after they
have labeled with initial classifications by their respec-
tive trained base models. To investigate if NTPC could
benefit from a more complex model, we employed vot-
ing, a commonly-used technique in machine learning and
natural language processing. As before, the training set
was partitioned and multiple base learners were trained
and evaluated on the multiple training and validation sets,
respectively. However, instead of recombining the vali-
dation sets into a reconstituted training set, multiple er-
ror corrector models were trained on the n partition sets.
During the evaluation phase, all n error correctors were
evaluated on the evaluation set after it had been labeled
by the base model, and they voted on the final output.
Table 2 shows the results of using such an approach
for the bracketing + classification task on English. The
empirical results clearly show that the more complex and
time-consuming voting model not only does not outper-
fom NTPC, but in fact again degrades the performance
from the base boosting-only model.
3.5 Experiment Summary
In our experiments, we set out to investigate whether
NTPC?s operating parameters were overly simple, and
whether more complex arrangements were necessary or
desirable. However, empirical evidence points to the fact
that, in this problem of error correction in high accuracy
ranges, at least, simple mechanisms will suffice to pro-
duce good results?in fact, the more complex operations
end up degrading rather than improving accuracy.
A valid question is to ask why methods such
as decision list learning (Rivest, 1987) as well as
transformation-based learning benefit from these more
complex mechanisms. Though structurally similar to
NTPC, these models operate in a very different environ-
ment, where many initially poorly labeled examples are
available to drive rule learning with. Hence, it is pos-
sibly advantageous to trade off some corrections with
some mistakes, provided that there is an overall posi-
tive change in accuracy. However, in an error-correcting
situation, most of the samples are already correctly la-
beled, errors are few and far in between and the sparse
data problem is exacerbated. In addition, the idea of er-
ror correction implies that we should, at the very least,
not do any worse than the original algorithm, and hence
it makes sense to err on the side of caution and minimize
any errors created, rather than hoping that a later rule ap-
plication will undo mistakes made by an earlier one.
Finally, note that the same point applies to many other
models where training criteria like minimum error rate
are used, since such criteria are functions of the trade-
off between correctly and incorrectly labeled examples,
without zero error tolerance to compensate for the sparse
data problem.
4 Previous Work
4.1 Boosting and NER
Boosting (Freund and Schapire, 1997) has been success-
fully applied to several NLP problems. In these NLP
systems boosting is typically used as the ultimate stage
in a learned system. For example, Shapire and Singer
(2000) applied it to Text Categorization while Escud-
ero et al(2000) used it to obtain good results on Word
Sense Disambiguation. More closely relevant to the ex-
periments described here in, two of the best-performing
three teams in the CoNLL-2002 Named Entity Recog-
nition shared task evaluation used boosting as their base
system (Carreras et al, 2002)(Wu et al, 2002).
However, precedents for improving performance af-
ter boosting are few. At the CoNLL-2002 shared task
session, Tjong Kim Sang (unpublished) described an ex-
periment using voting to combine the NER outputs from
the shared task participants which, predictably, produced
better results than the individual systems. A couple of
the individual systems were boosting models, so in some
sense this could be regarded as an example.
Tsukamoto et al(2002) used piped AdaBoost.MH
models for NER. Their experimental results were some-
what disappointing, but this could perhaps be attributable
to various reasons including the feature engineering or
not using cross-validation sampling in the stacking.
Appendix
The following examples show the top 10 rules learned for English and Spanish on the bracketing + classification task.
(Models M6 and M8)
English
ne -2=ZZZ ne -1=ZZZ word:[1,3]=21 nonnevocab 0=inNonNeVocab nevocab 0=inNeVocab captype 0=firstword-firstupper => ne=I-ORG
ne 1=O ne 2=O word -1=ZZZ nonnevocab 0=inNonNeVocab nevocab 0=not-inNeVocab captype 0=firstword-firstupper => ne=O
captype 0=notfirstword-firstupper captype -1=firstword-firstupper captype 1=number nonnevocab 0=inNonNeVocab nevocab 0=inNeVocab ne 0=I-LOC => ne=I-
ORG
ne -1=ZZZ ne 0=I-ORG word 1=, nonnevocab 0=not-inNonNeVocab nevocab 0=not-inNeVocab captype 0=allupper => ne=I-LOC
ne 0=I-PER word:[1,3]=0 nonnevocab 0=not-inNonNeVocab nevocab 0=not-inNeVocab captype 0=notfirstword-firstupper => ne=I-ORG
ne 0=I-ORG ne 1=O ne 2=O nonnevocab 0=inNonNeVocab nevocab 0=not-inNeVocab captype 0=alllower => ne=O
ne 0=I-PER ne 1=I-ORG => ne=I-ORG
ne -1=ZZZ ne 0=I-PER word:[-3,-1]=ZZZ word:[1,3]=1 => ne=I-ORG
ne 0=I-ORG word:[-3,-1]=spd => ne=B-ORG
ne -1=I-ORG ne 0=I-PER word:[1,3]=1 => ne=I-ORG
Spanish
wcaptype 0=alllower ne -1=I-ORG ne 0=I-ORG ne 1=O => ne=O
captypeLex -1=inLex captypeGaz -1=not-inGaz wcaptype -1=alllower ne -1=O ne 0=O captypeLex 0=not-inLex captypeGaz 0=not-inGaz wcaptype 0=noneed-
firstupper => ne=I-ORG
wcaptype 0=noneed-firstupper wcaptype -1=noneed-firstupper wcaptype 1=alllower captypeLex 0=not-inLex captypeGaz 0=not-inGaz ne 0=O => ne=I-ORG
ne 0=O word 0=efe => ne=I-ORG
ne -1=O ne 0=O word 1=Num word 2=. captypeLex 0=not-inLex captypeGaz 0=not-inGaz wcaptype 0=allupper => ne=I-MISC
pos -1=ART pos 0=NCF wcaptype 0=noneed-firstupper ne -1=O ne 0=O => ne=I-ORG
wcaptype 0=alllower ne 0=I-PER ne 1=O ne 2=O => ne=O
ne 0=O ne 1=I-MISC word 2=Num captypeLex 0=not-inLex captypeGaz 0=not-inGaz wcaptype 0=allupper => ne=I-MISC
ne 0=I-LOC word:[-3,-1]=universidad => ne=I-ORG
ne 1=O ne 2=O word 0=de captypeLex 0=not-inLex captypeGaz 0=inGaz wcaptype 0=alllower => ne=O
The AdaBoost.MH base model?s high accuracy sets
a high bar for error correction. Aside from brute-force
en masse voting of the sort at CoNLL-2002 described
above, we do not know of any existing post-boosting
models that improve rather than degrade accuracy. We
aim to further improve performance, and propose using
a piped error corrector.
4.2 Transformation-based Learning
Transformation-based learning (Brill, 1995), or TBL, is
one of the most successful rule-based machine learning
algorithms. The central idea of TBL is to learn an or-
dered list of rules, each of which evaluates on the re-
sults of those preceding it. An initial assignment is made
based on simple statistics, and then rules are greedily
learned to correct the mistakes, until no net improvement
can be made.
Transformation-based learning has been used to tackle
a wide range of NLP problems, ranging from part-of-
speech tagging (Brill, 1995) to parsing (Brill, 1996) to
segmentation and message understanding (Day et al,
1997). In general, it achieves state-of-the-art perfor-
mances and is fairly resistant to overtraining.
5 Conclusion
We have investigated frequently raised questions about
N-fold Templated Piped Correction (NTPC), a general-
purpose, conservative error correcting model, which has
been shown to reliably deliver small but consistent gains
on the accuracy of even high-performing base models
on high-dimensional NLP tasks, with little risk of acci-
dental degradation. Experimental evidence shows that
when error-correcting high-accuracy base models, sim-
ple models and hypotheses are more beneficial than com-
plex ones, while the more complex and powerful models
are surprisingly unreliable or damaging in practice.
References
Eric Brill. Transformation-based error-driven learning and natural language pro-
cessing: A case study in part of speech tagging. Computational Linguistics,
21(4):543?565, 1995.
Eric Brill. Recent Advances in Parsing Technology, chapter Learning to Parse
with Transformations. Kluwer, 1996.
Xavier Carreras, Llu??s Ma`rques, and Llu??s Padro?. Named entity extraction using
adaboost. In Dan Roth and Antal van den Bosch, editors, Proceedings of
CoNLL-2002, pages 167?170. Taipei, Taiwan, 2002.
David Day, John Aberdeen, Lynette Hirshman, Robyn Kozierok, Patricia Robin-
son, and Marc Vilain. Mixed initiative development of language processing
systems. In Proceedings of the Fifth Conference on Applied Natural Language
Processing, Washington, D.C., March 1997. Association of Computational
Linguistics.
Gerard Escudero, Lluis Marquez, and German Rigau. Boosting applied to word
sense disambiguation. In European Conference on Machine Learning, pages
129?141, 2000.
Yoram Freund and Robert E. Schapire. A decision-theoretic generalization of
on-line learning and an application to boosting. In Journal of Computer and
System Sciences, 55(1), pages 119?139, 1997.
Ronald L. Rivest. Learning decision lists. Machine Learning, 2(3):229?246,
1987.
Robert E. Schapire and Yoram Singer. Boostexter: A boosting-based system for
text categorization. Machine Learning, 2(3):135?168, 2000.
Erik Tjong Kim Sang and Fien Meulder. Introduction to the conll-2003 shared
task: Language-independent named entity recognition. In Walter Daelemans
and Miles Osborne, editors, Proceedings of CoNLL-2003. Edmonton, Canada,
2003.
Erik Tjong Kim Sang. Introduction to the conll-2002 shared task: Language-
independent named entity recognition. In Dan Roth and Antal van den Bosch,
editors, Proceedings of CoNLL-2002, pages 155?158. Taipei, Taiwan, 2002.
Koji Tsukamoto, Yutaka Mitsuishi, and Manabu Sassano. Learning with multiple
stacking for named entity recognition. In Dan Roth and Antal van den Bosch,
editors, Proceedings of CoNLL-2002, pages 191?194. Taipei, Taiwan, 2002.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang.
Boosting for named entity recognition. In Dan Roth and Antal van den Bosch,
editors, Proceedings of CoNLL-2002, pages 195?198. Taipei, Taiwan, 2002.
Dekai Wu, Grace Ngai, and Marine Carpuat. N-fold templated piped correc-
tion. In First International Joint Conference on Natural Language Processing
(IJCNLP-2004), pages 632?637. Hainan Island, China, March 2004.
Dekai Wu, Grace Ngai, and Marine Carpuat. Raising the bar: Stacked conserva-
tive error correction beyond boosting. In Fourth International Conference on
Language Resources and Evaluation (LREC-2004). Lisbon, May 2004.
A Stacked, Voted, Stacked Model for Named Entity Recognition
Dekai Wu?? and Grace Ngai? and Marine Carpuat?
? Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
{dekai,marine}@cs.ust.hk
? Hong Kong Polytechnic University
Department of Computing
Kowloon
Hong Kong
csgngai@polyu.edu.hk
Abstract
This paper investigates stacking and voting
methods for combining strong classifiers like
boosting, SVM, and TBL, on the named-entity
recognition task. We demonstrate several ef-
fective approaches, culminating in a model that
achieves error rate reductions on the develop-
ment and test sets of 63.6% and 55.0% (En-
glish) and 47.0% and 51.7% (German) over the
CoNLL-2003 standard baseline respectively,
and 19.7% over a strong AdaBoost baseline
model from CoNLL-2002.
1 Introduction
We describe multiple stacking and voting methods that
effectively combine strong classifiers such as boosting,
SVM, and TBL, for the named-entity recognition (NER)
task. NER has emerged as an important step for many
natural language applications, including machine trans-
lation, information retrieval and information extraction.
Much of the research in this field was pioneered in
the Message Understanding Conference (MUC) (Sund-
heim, 1995), which performed detailed entity extraction
and identification on English documents. As a result,
most current NER systems which have impressive per-
formances have been specially constructed and tuned for
English MUC-style documents. It is unclear how well
they would perform when applied to another language.
Our system was designed for the CoNLL-2003 shared
task, the goal of which is to identify and classify four
types of named entities: PERSON, LOCATION, ORGA-
NIZATION and MISCELLANEOUS. The task specifi-
cations were that two languages would be involved. We
*The author would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research in
part through two research grants (RGC 6083/99E and RGC
6256/00E).
were given about a month to develop our system on the
first language, which was English, but only two weeks to
adapt it to the surprise language, which was German.
Given the goal of the shared task, we designed our
system to achieve a high performance without relying
too heavily on knowledge that is very specific for a par-
ticular language or domain. In the spirit of language-
independence, we avoided using features and information
which would not be easily obtainable for almost any ma-
jor language.
2 Classification Methods
To carry out the stacking and voting experiments, we con-
structed a number of relatively strong individual compo-
nent models of the following kinds.
2.1 Boosting
The main idea behind boosting algorithms is that a set
of many weak classifiers can be effectively combined to
yield a single strong classifier. Each weak classifier is
trained sequentially, increasingly focusing more heavily
on the instances that the previous classifiers found diffi-
cult to classify.
For the boosting framework, our system uses Ada-
Boost.MH (Freund and Schapire, 1997), an n-ary classifi-
cation variant of the original binary AdaBoost algorithm.
It performs well on a number of natural language process-
ing problems, including text categorization (Schapire and
Singer, 2000) and word sense disambiguation (Escudero
et al, 2000). In particular, it has also been demonstrated
that boosting can be used to build language-independent
NER models that perform exceptionally well (Wu et al,
2002; Carreras et al, 2002).
2.2 Support Vector Machines
Support Vector Machines (SVMs) have gained a con-
siderable following in recent years (Boser et al, 1992),
particularly in dealing with high-dimensional spaces
such as commonly found in natural language prob-
lems like text categorization (Joachims, 1998). SVMs
have shown promise when applied to chunking (Kudo
and Matsumoto, 2001) and named entity recognition
(Sassano and Utsuro, 2000; McNamee and Mayfield,
2002), though performance is quite sensitive to param-
eter choices.
2.3 Transformation-based Learning
Transformation-based learning (Brill, 1995) (TBL) is a
rule-based machine learning algorithm that was first in-
troduced by Brill and used for part-of-speech tagging.
The central idea of transformation-based learning is to
learn an ordered list of rules which progressively improve
upon the current state of the training set. An initial as-
signment is made based on simple statistics, and then
rules are greedily learned to correct the mistakes, until
no net improvement can be made.
The experiments presented in this paper were per-
formed using the fnTBL toolkit (Ngai and Florian, 2001),
which implements several optimizations in rule learning
to drastically speed up the time needed for training.
3 Data Resources
3.1 Preprocessing the Data
The data that was provided by the CoNLL organizers was
sentence-delimited and tokenized, and hand-annotated
with named entity chunks. The English data was au-
tomatically labeled with part-of-speech and chunk tags
from the memory-based tagger and chunker (Daelemans
et al, 1996), and the German data was labelled with the
decision-tree-based TreeTagger (Schmidt, 1994). We re-
placed the English part-of-speech tags with those gener-
ated by a transformation-based learner (Ngai and Florian,
2001). The chunk tags did not appear to help in either
case and were discarded.
As we did not want to overly rely on characteristics
which were specific to the Indo-European language fam-
ily, we did not perform detailed morphological analysis;
but instead, an approximation was made by simply ex-
tracting the prefixes and suffixes of up to 4 characters
from all the words.
In order to let the system generalize over word types,
we normalized the case information of all the words in
the corpus by converting them to uniform lower case. To
recapture the lost information, each word was annotated
with a tag that specified if it was in all lower case, all
upper case, or was of mixed case.
3.2 Gazetteers
Apart from the training and test data, the CoNLL orga-
nizers also provided two lists of named entities, one in
English and one in Dutch. Part of the challenge for this
year?s shared task was to find ways of using this resource
in the system.
To supplement the provided gazetteers, a large col-
lection of names and words was downloaded from var-
ious web sources. This collection was used to compile
a gazetteer of 120k uncategorized English proper names
and a lexicon of 500k common English words. As there
were no supplied gazetteers for German, we also com-
piled a gazetteer of 8000 German names, which were
mostly personal first and last names and geographical lo-
cations, and a lexicon of 32k common German words.
Named entities in the corpus which appeared in the
gazetteers were identified lexically or using a maximum
forward match algorithm similar to that used in Chinese
word segmentation. Once named entities have been iden-
tified in this preprocessing step, each word can then be
annotated with an NE chunk tag corresponding to the out-
put from the system. The learner can view the NE chunk
tag as an additional feature.
The variations in this approach come from resolving
conflicts between different possible type information for
the same NE. The different ways that we dealt with the
problem were: (1) Rank all the NE types by frequency in
the training corpus. In the case of a conflict, default to
the more common NE. (2) Give all the possible NEs to
the boosting learner as a set of possible NE chunk tags.
(3) Discard the NE type information and annotate each
word with a tag indicating whether it is inside an NE.
4 Classifier Combination
It is a well-known fact that if several classifiers are avail-
able, they can be combined in various ways to create
a system that outperforms the best individual classifier.
Since we had several classifiers available to us, it was rea-
sonable to investigate combining them in different ways.
4.1 Stacking
Like voting, stacking is a learning paradigm that con-
structs a combined model from several classifiers. The
basic concept behind stacking is to train two or more clas-
sifiers sequentially, with each successive classifier incor-
porating the results of the previous ones in some fashion.
4.1.1 Integration of External Resources
As mentioned above, at the most basic level, lexicon
and gazetteer information was integrated into our classi-
fiers by including them as additional features. However
we also experimented with several different ways of in-
corporating this information via stacking?one possible
approach was to view the gazetteers as a separate system
that would produce an output and then implement stack-
ing to combine their outputs.
4.1.2 Division into Subtasks
One of the most straightforward approaches to stack-
ing can be applied to tasks that are naturally divisible into
hierarchically ordered subtasks. An example approach,
which was taken by several of the participating teams in
the CoNLL-2002 shared task, is to split the NER task into
the identification phase, where named entities are identi-
fied in the text; and the classification phase, where the
identified named entities are categorized into the various
subtypes. Provided that the performance of the individ-
ual classifier is fairly high (otherwise errors made in the
earlier stages could propagate down the chain), this has
the advantage of reducing the complexity of the task for
each individual classifier.
To construct such a system, we trained a stacked Ada-
Boost.MH classifier to perform NE reclassification on
boundaries identified in the base model. The output of the
initial models are postprocessed to remove all NE type
information and then passed to this stacked classifier. As
Table 1 shows, stacking the boosting models yields a sig-
nificant gain in performance.
English devel. Precision Recall F?=1
(Boost) Base 88.64% 87.68% 88.16
(Boost) Base + Stacked 89.26% 88.29% 88.77
Table 1: Improving classification of NE types via stacked
AdaBoost.MH.
4.1.3 Error Correction
Another approach to stacking that we investigated in
this work involves a closer interaction between the mod-
els. The general overview of this approach is for a given
model to use the output of another trained model as its
initial state, and to improve beyond it. The idea is that
the second model, with a different learning and represen-
tation bias, will be able to move out of the local maxima
that the previous model has settled into.
To accomplish this we introduced Stacked TBL
(STBL), a variant of TBL tuned for this purpose (Wu et
al., 2003). We found TBL to be an appropriate point of
departure since it starts from an initial state of classifi-
cation and learns rules to iteratively correct the current
labeling. We aimed to use STBL to improve the base
model from the preceding section.
STBL proved quite effective; in fact it yielded the best
base model performance among all our models. Table 2
shows the result of stacking STBL on the boosting base
model.
4.2 Voting
The simplest approach to combining classifiers is through
voting, which examines the outputs of the various mod-
English devel. Precision Recall F?=1
(Boost) Base 88.64% 87.68% 88.16
(Boost + STBL) Base 87.83% 88.79% 88.31
Table 2: Improving the above AdaBoost.MH base model,
via Stacked TBL (STBL).
els and selects the classifications which have a weight
exceeding some threshold, where the weight is depen-
dent upon the models that proposed this particular clas-
sification. The variations in this approach stem from the
method by which weights are attached to the models. It
is possible to assign varying weights to the models, in ef-
fect giving one model more ?say? than the others. In our
system, however, we simply assigned each model equal
weight, and selected classifications which were proposed
by a majority of models.
Voting was thus used to further improve the base
model. Four models chosen for heterogeneity partici-
pated in the voting: two variants of the AdaBoost.MH
model, the SVM model, and the Stacked TBL model.
As before, the stacked AdaBoost.MH reclassifier was
applied to the voted result, yielding a final stacked voted
stacked model.
This model gave the best overall results on the task as
a whole. Table 3 shows the results of our system.
English devel. Precision Recall F?=1
Boost1 + Stacked 89.26% 88.29% 88.77
Boost2 + Stacked 82.98% 85.62% 84.28
SVM + Stacked 84.41% 85.71% 85.05
Boost + STBL + Stacked 89.09% 88.07% 88.57
Voted + Stacked 90.18% 88.86% 89.51
Table 3: Improving classification of NE types, via stacked
voted stacked AdaBoost.MH, STBL, and SVM models.
5 Overall Results
Complete results on the development and test sets, for
both English and German, are shown in Table 4.
6 Conclusion
This paper has presented an overview of our entry to
the CoNLL-2003 shared task. As individual component
models, we constructed strong AdaBoost.MH models,
SVM models, and Stacked TBL models, and provided
them with detailed features on the data.
We then demonstrated several stacking and voting
models that proved capable of improving performance
further. This was non-trivial since the individual compo-
nent models were all quite strong to begin with. Because
of this the vast majority of classifier combination models
we tested actually turned out to degrade performance, or
showed zero improvement. The models presented here
worked well because they were each motivated by de-
tailed analyses.
We did investigate a number of ways in which
gazetteers could be incorporated. The gazetteer supplied
for the shared task was found not to improve perfor-
mance significantly, because our models were already ad-
equately powerful to correctly identify most of the named
entities supplied by the gazetteer. However, minimal ef-
fort to augment the gazetteers did result in a performance
boost. Moreover, performance was further improved by
the inclusion of a common word lexicon not containing
any named entities.
Inspection revealed that some errors found in the
output of the system stemmed from either erroneous
sentence boundaries in the test data, or difficult-to-avoid
inconsistencies in the the gold standard annotations. For
example, in the following:
1. . . . [ Panamanian ]MISC boxing legend . . .
2. . . . [ U.S. ]LOC collegiate national champion . . .
both ?Panamanian? and ?U.S.? are used as modifiers, but
one is annotated as a MISC-type NE while the other is
considered a LOC-type.
The stacked voted stacked model obtained an improve-
ment of 4.83 F-Measure points on the English devel-
opment set over our best model from the CoNLL-2002
shared task which we took as our baseline, resulting in
a substantial 19.7% error rate reduction. The system
achieves this respectable performance using very little
in the way of outside resources?only a part-of-speech
tagger and some common wordlists?which can be ob-
tained easily for almost any major language. Most fea-
tures we used can also be used for uninflected and non-
Indo-European languages such as Chinese, where the pre-
fixes and suffixes can be replaced by decomposing the
words at the character level. This is in keeping with the
the language-independent spirit of the shared task.
References
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. 1992. A training al-
gorithm for optimal margin classifiers. In David Haussler, editor, Proceedings
of the 4th Workshop on Computational Learning Theory, pages 144?152, San
Mateo, CA. ACM Press.
Eric Brill. 1995. Transformation-based error-driven learning and natural lan-
guage processing: A case study in part of speech tagging. Computational
Linguistics, 21(4):543?565.
Xavier Carreras, Llu?is Ma`rquez, and Llu?is Padro?. 2002. Named entity extrac-
tion using adaboost. In Proceedings of CoNLL-2002, pages 167?170. Taipei,
Taiwan.
Walter Daelemans, Jakub Zavrel, and Peter Berck. 1996. MBT: A memory-based
part of speech tagger-generator.
Gerard Escudero, Llu?is Ma`rquez, and German Rigau. 2000. Boosting applied to
word sense disambiguation. In European Conference on Machine Learning,
pages 129?141.
English devel. Precision Recall F?=1
LOC 91.88% 92.98% 92.42
MISC 91.53% 83.19% 87.16
ORG 86.43% 80.76% 83.50
PER 90.39% 93.49% 91.91
Overall 90.18% 88.86% 89.51
English test Precision Recall F?=1
LOC 86.27% 85.91% 86.09
MISC 75.88% 77.07% 76.47
ORG 78.19% 72.97% 75.49
PER 83.94% 87.26% 85.57
Overall 82.02% 81.39% 81.70
German devel. Precision Recall F?=1
LOC 75.13% 61.39% 67.57
MISC 75.46% 45.05% 56.42
ORG 80.05% 47.86% 59.91
PER 74.19% 61.96% 67.52
Overall 75.92% 54.67% 63.56
German test Precision Recall F?=1
LOC 74.94% 60.97% 67.23
MISC 72.77% 51.04% 60.00
ORG 61.22% 42.69% 50.30
PER 83.68% 73.39% 78.20
Overall 75.20% 59.35% 66.34
Table 4: Overall results of stacked voted stacked model
on development and test sets.
Yoav Freund and Rob E. Schapire. 1997. A decision-theoretic generalization of
on-line learning and an application to boosting. In Journal of Computer and
System Sciences, 55(1), pages 119?139.
Thorsten Joachims. 1998. Text categorization with support vector machines:
learning with many relevant features. In Claire Ne?dellec and Ce?line Rou-
veirol, editors, Proceedings of ECML-98, 10th European Conference on Ma-
chine Learning, number 1398, pages 137?142, Chemnitz, DE. Springer Ver-
lag, Heidelberg, DE.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of NAACL-2001.
Paul McNamee and James Mayfield. 2002. Entity extraction without language-
specific resources. In Proceedings of CoNLL-2002, pages 183?186. Taipei,
Taiwan.
Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast
lane. In Proceedings of NAACL?01, pages 40?47, Pittsburgh, PA. ACL.
Manabu Sassano and Takehito Utsuro. 2000. Named entity chunking techniques
in supervised learning for Japanese named entity recognition. In Proceedings
of COLING-2000, pages 705?711.
Rob E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system
for text categorization. In Machine Learning, 39(2/3, pages 135?168.
Helmut Schmidt. 1994. Probabilistic part-of-speech tagging using decision trees.
In International Conference on New Methods in Natural Language Process-
ing, pages 44?49, Manchester, U.K.
Beth Sundheim. 1995. MUC6 named entity task defnition, version 2.1. In Pro-
ceedings of the Sixth Message Understanding Conference (MUC6).
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang.
2002. Boosting for named entity recognition. In Proceedings of CoNLL-2002,
pages 195?198. Taipei, Taiwan.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. Forthcoming.
Combining Optimal Clustering and Hidden Markov Models for
Extractive Summarization
Pascale Fung

Human Language Technology Center,
Dept. of Electrical & Electronic
Engineering,
University of Science & Technology
(HKUST)
Clear Water Bay, Hong Kong
pascale@ee.ust.hk
Grace Ngai

Dept. of Computing,
Hong Kong Polytechnic
University,
Kowloon, Hong Kong
csgngai@polyu.edu.hk
CHEUNG, Chi-Shun

Human Language Technology Center,
Dept. of Electrical & Electronic
Engineering,
University of Science & Technology
(HKUST)
Clear Water Bay, Hong Kong
eepercy@ee.ust.hk
Abstract
We propose Hidden Markov models with
unsupervised training for extractive sum-
marization. Extractive summarization se-
lects salient sentences from documents to
be included in a summary. Unsupervised
clustering combined with heuristics is a
popular approach because no annotated
data is required. However, conventional
clustering methods such as K-means do
not take text cohesion into consideration.
Probabilistic methods are more rigorous
and robust, but they usually require su-
pervised training with annotated data. Our
method incorporates unsupervised train-
ing with clustering, into a probabilistic
framework. Clustering is done by modi-
fied K-means (MKM)--a method that
yields more optimal clusters than the con-
ventional K-means method. Text cohesion
is modeled by the transition probabilities
of an HMM, and term distribution is
modeled by the emission probabilities.
The final decoding process tags sentences
in a text with theme class labels. Parame-
ter training is carried out by the segmental
K-means (SKM) algorithm. The output of
our system can be used to extract salient
sentences for summaries, or used for topic
detection. Content-based evaluation
shows that our method outperforms an ex-
isting extractive summarizer by 22.8% in
terms of relative similarity, and outper-
forms a baseline summarizer that selects
the top N sentences as salient sentences
by 46.3%.
1 Introduction
Multi-document summarization (MDS) is the
summarization of a collection of related documents
(Mani (1999)). Its application includes the summa-
rization of a news story from different sources
where document sources are related by the theme
or topic of the story. Another application is the
tracking of news stories from the single source
over different time frame. In this case, documents
are related by topic over time.

Multi-document summarization is also an exten-
sion of single document summarization. One of the
most robust and domain-independent summariza-
tion approaches is extraction-based or shallow
summarization (Mani (1999)). In extraction-based
summarization, salient sentences are automatically
extracted to form a summary directly  (Kupiec et.
al, (1995), Myaeng & Jang (1999), Jing et. al,
(2000), Nomoto & Matsumoto (2001,2002), Zha
(2002), Osborne (2002)), or followed by a synthe-
sis stage to generate a more natural summary
(McKeown & Radev (1999), Hovy & Lin (1999)).
Summarization therefore involves some theme or
topic identification and then extraction of salient
segments in a document. 

Story segmentation, document and sentence and
classification can often be accomplished by unsu-
pervised, clustering methods, with little or no re-
quirement of human labeled data (Deerwester
(1991), White & Cardie (2002), Jing et. al (2000)).
Unsupervised methods or hybrids of supervised
and unsupervised methods for extractive summari-
zation have been found to yield promising results
that are either comparable or superior to supervised
methods (Nomoto & Matsumoto (2001,2002)). In
these works, vector space models are used and
document or sentence vectors are clustered to-
gether according to some similarity measure
(Deerwester (1991), Dagan et al (1997)).

The disadvantage of clustering methods lies in
their ad hoc nature. Since sentence vectors are con-
sidered to be independent sample points, the sen-
tence order information is lost. Various heuristics
and revision strategies have been applied to the
general sentence selection schema to take into con-
sideration text cohesion (White & Cardie (2002),
Mani and Bloedorn (1999), Aone et. al (1999), Zha
(2002), Barzilay et al, (2001)). We would like to
preserve the natural linear cohesion of sentences in
a text as a baseline prior to the application of any
revision strategies.

To compensate for the ad hoc nature of vector
space models, probabilistic approaches have re-
gained some interests in information retrieval in
recent years (Knight & Marcu (2000), Berger &
Lafferty (1999), Miller et al, (1999)).  These re-
cent probabilistic methods in information retrieval
are largely inspired by the success of probabilistic
models in machine translation in the early 90s
(Brown et. al), and regard information retrieval as
a noisy channel problem. Hidden Markov Models
proposed by Miller et al (1999), and have shown
to outperform tf, idf in TREC information retrieval
tasks. The advantage of probabilistic models is that
they provide a more rigorous and robust frame-
work to model query-document relations than ad
hoc information retrieval. Nevertheless, such prob-
abilistic IR models still require annotated training
data.

In this paper, we propose an iterative unsupervised
training method for multi-document extractive
summarization, combining vectors space model
with a probabilistic model. We iteratively classify
news articles, then paragraphs within articles, and
finally sentences within paragraphs into common
story themes, by using modified K-means (MKM)
clustering and segmental K-means (SKM) decod-
ing. We obtain an initial clustering of article
classes by MKM, which determines the inherent
number of theme classes of all news articles. Next,
we use SKM to classify paragraphs and then sen-
tences. SKM iterates between a k-means clustering
step, and a Viterbi decoding step, to obtain a final
classification of sentences into theme classes. Our
MKM-SKM paradigm combines vector space clus-
tering model with a probabilistic framework, pre-
serving some of the natural sentence cohesion,
without the requirement of annotated data. Our
method also avoids any arbitrary or ad hoc setting
of parameters.

In section 2, we introduce the modified K-means
algorithm as a better alternative than conventional
K-means for document clustering. In section 3 we
present the stochastic framework of theme classifi-
cation and sentence extraction. We describe the
training algorithm in section 4, where details of the
model parameters and Viterbi scoring are pre-
sented. Our sentence selection algorithm is de-
scribed in Section 5. Section 6 describes our
evaluation experiments. We discuss the results and
conclude in section 7.
2 Story Segmentation using Modified K-
means (MKM) Clustering
The first step in multi-document summarization is
to segment and classify documents that have a
common theme or story. Vector space models can
be used to compare documents (Ando et al (2000),
Deerwester et al (1991)). K-means clustering is
commonly used to cluster related document or sen-
tence vectors together. A typical k-means cluster-
ing algorithm for summarization is as follows:

1. Arbitrarily choose K vectors as initial centroids;
2. Assign vectors closest to each centroid to its cluster;
3. Update centroid using all vectors assigned to each
cluster;
4. Iterate until average intra-cluster distance falls be-
low a threshold;

We have found three problems with the standard k-
means algorithm for sentence clustering. First, the
initial number of clusters k, has to be set arbitrarily
by humans. Second, the initial partition of a cluster
is arbitrarily set by thresholding. Hence, the initial
set of centroids is arbitrary. Finally, during cluster-
ing, the centroids are selected as the sentence
among a group of sentences that has the least aver-
age distance to other sentences in the cluster. All
these characteristics of K-means can be the cause
of a non-optimal cluster configuration at the final
stage.

To avoid the above problems, we propose using
modified K-means (MKM) clustering algorithm
(Wilpon & Rabiner(1985)), coupled with virtual
document centroids. MKM starts from a global
centroid and splits the clusters top down until the
clusters stabilize:

1. Compute the centroid of the entire training set;
2. Assign vectors closest to each centroid to its cluster;
3. Update centroid using all vectors assigned to each
cluster;
4. Iterate 2-4 until vectors stop moving between clus-
ters;
5. Stop if clusters stabilizes, and output final clusters,
else goto step 6;
6. Split the cluster with largest intra-cluster distance
into two by finding the pair of vectors with largest
distance in the cluster. Use these two vectors as new
centroids, and repeat steps 2-5.

In addition, we do not use any existing document
in the collection as the selected centroid. Rather,
we introduce virtual centroids that contain the ex-
pected value of all documents in a cluster. An ele-
ment of the centroid is the average weight of the
same index term in all documents within that clus-
ter:

M
w
M
m
m
i
i

=
=
1?

The vectors are document vectors in this step. The
number of clusters is determined after the clusters
are stabilized. The resultant cluster configuration is
more optimal and balanced than that from using
conventional k-means clustering.   Using the MKM
algorithm with virtual centroids, we segment the
collection of news articles into clusters of related
articles. Articles covering the same story from dif-
ferent sources now carry the same theme label.
Articles from the same source over different time
period also carry the same theme label. In the next
stage, we iteratively re-classify each paragraph,
and then re-classify each sentence in each para-
graph into final theme classes.

3 A Stochastic Process of Theme Classifi-
cation
After we have obtained story labels of each article,
we need to classify the paragraphs and then the
sentences according to these labels. Each para-
graph in the article is assigned the cluster number
of that article, as we assume all paragraphs in the
same article share the same story theme.

We suggest that the entire text generation process
can be considered as a stochastic process that starts
in some theme class, generates sentences one after
another for that theme class, then goes to the next
theme and generates the sentences, so on and so
forth, until it reaches the final theme class in a
document, and finishes generating sentences in that
class. This is an approximation of the authoring
process where a writer thinks of a certain structure
for his/her article, starts from the first section,
writes sentences in that section, proceeds to the
next section, etc., until s/he finishes the last sen-
tence in the last section.

Given a document of sentences, the task of sum-
mary extraction involves discovering the underly-
ing theme class transitions at the sentence
boundaries, classify each sentence according to
these theme concepts, and then extract the salient
sentences in each theme class cluster.

We want to find )|(maxarg DCP
C

  where D

 is a
document consisting of linearly ordered sentence
sequences ))(,),(,),2(),1(( TstsssD 

= , and C

is
a theme class sequence which consists of the class
labels of all the sentences in D

 ,
)))((,)),((,)),2(()),1((( TsctscscscC 

= .

Following Bayes Rule gives us
)(/)()|()|( DPCPCDPDCP  = . We assume )(DP  is
equally likely for all documents, so that finding the
best class sequence becomes:

)))(()),((,)),2(()),1(((
)))((),()),((),(,)),2((),2()),1((),1((maxarg
)()|(maxarg)|(maxarg
TsctscscscP
TscTstsctsscsscsP
CPCDPDCP
C
CC





?=
?

Note that the total number of theme classes is far
fewer than the total number of sentences in a
document and the mapping is not one-to-one. Our
task is similar to the concept of discourse parsing
(Marcu (1997)), where discourse structures are
extracted from the text. In our case, we are carry-
ing out discourse tagging, whereby we assign the
class labels or tags to each sentence in the docu-
ment.

We use Hidden Markov Model for this stochastic
process, where the classes are assumed to be hid-
den states.

We make the following assumptions:

? The probability of the sentence given its past only
depends on its theme class (emission probabilities);
? The probability of the theme class only depends on
the theme classes of the previous N sentences (tran-
sition probabilities).


The above assumptions lead to a Hidden Markov
Model with M states representing M different
theme classes. Each state can generate different
sentences according to some probability distribu-
tion?the emission probabilities. These states are
hidden as we only observe the generated sentences
in the text.  Every theme/state can transit to any
other theme/state, or to itself, according to some
probabilities?the transition probabilities.

2C
1C 3C
jC
?
=
L
i
jt CiSP
0
)|)(( 



Figure 1: An ergodic HMM for theme tagging

Our theme tagging task then becomes a search
problem for HMM: Given the observation se-
quence ))(,),(,),2(),1(( TstsssD 

= , and the
model ? , how do we choose a corresponding state
sequence )))((,)),((,)),2(()),1((( TsctscscscC 

=  ,
that best explains the sentence sequence?


To train the model parameter ? , we need to solve
another problem in HMM: How do we adjust the
model parameters ),,( pi? BA= , the transition,
emission and initial probabilities, to maximize the
likelihood of the observation sentence sequences
given our model?

In a supervised training paradigm, we can obtain
human labeled class-sentence pairs and carry out a
relative frequency count for training the emission
and transition probabilities. However, hand label-
ing some large collection of texts with theme
classes is very tedious. One main reason is that
there is a considerable amount of disagreement
between humans on manual annotation of themes
and topics. How many themes should there be?
Where should each theme start and end? 

It is therefore desirable to decode the hidden theme
or topic states using an unsupervised training
method without manually annotated data. Conse-
quently, we only need to cluster and label the ini-
tial document according to cluster number. In the
HMM framework, we then improve upon this ini-
tial clustering by iteratively estimate ),,( pi? BA= ,
and maximize )|( DCP   using a Viterbi decoder.

3.1 Sentence Feature Vector and Similarity
Measure
Prior to the training process, we need to define sen-
tence feature vector and the similarity measure for
comparing two sentence vectors.

As we consider a document D

 to be a sequence of
sentences, the sentences themselves are repre-
sented as feature vectors )(ts  of length L, where t
is the position of the sentence in the document and
L is the size of the vocabulary. Each element of the
vector )(ts  is an index term in the sentence,
weighted by its text frequency (tf) and inverse
document frequency (idf) where tf is defined as the
frequency of the word in that particular sentence,
and idf  is the inverse frequency of the word in the
larger document collection
N
dflog? where df is
the number of sentences this particular word ap-
pears in and N is the total number of sentences in
the training corpus.  We select the sentences as
documents in computing the tf and idf because we
are comparing sentence against sentence.

In the initial clustering and subsequent Viterbi
training process, sentence feature vectors need to
be compared to the centroid of each cluster. Vari-
ous similarity measures and metrics include the
cosine measure, Dice coefficient, Jaccard coeffi-
cient, inclusion coefficient, Euclidean distance, KL
convergence, information radius, etc (Manning &
Sch   tze (1999), Dagan et al (1997), Salton and
McGill (1983)).  We chose the cosine similarity
measure for its ease in computation:

 

= =
=
?
?
=
L
i
L
i
ii
L
i
ii
vsts
vsts
vsts
1 1
22
1
))(())((
)()(
))(),(cos(
4 Segmental K-means Clustering for Pa-
rameter Training
In this section, we describe an iterative training
process for estimation of our HMM parameters.
We consider the output of the MKM clustering
process in Section 2 as an initial  segmentation of
text into class sequences. To improve upon this
initial segmentation, we use an iterative Viterbi
training method that is similar to the segmental k-
means clustering for speech processing (Rabiner &
Juang(1993)). All articles in the same story cluster
are processed as follows: 

1. Initialization: All paragraphs in the same story class
are clustered again. Then all sentences in the same
paragraph shares the same class label as that para-
graph. This is the initial class-sentence segmentation.
Initial class transitions are counted.
2. (Re-)clustering: Sentence vectors with their class
labels are repartitioned into K clusters (K is obtained
from the MKM step previously) using the K-means
algorithm. This step is iterated until the clusters sta-
bilize.
3. (Re-)estimation of probabilities: The centroids of
each cluster are estimated. Update emission prob-
abilities from the new clusters.
4. (Re-)classification by decoding: the updated set of
model parameters from step 2 are used to rescore the
(unlabeled) training documents into sequences of
class given sentences, using Viterbi decoding. Up-
date class transitions from this output.
5. Iteration: Stop if convergence conditions are met,
else repeat steps 2-4.

The segmental clustering algorithm is iterated until
the decoding likelihood converges. The final
trained Viterbi decoder is then used to tag un-
annotated data sets into class-sentence pairs. 

In the following Sections 4.1 and 4.2, we discuss in
more detail steps 3 and 4.
4.1 Estimation of Probabilities
We need to train the parameters of our HMM such
that the model can best describe the training data.
During the iterative process, the probabilities are
estimated from class-sentence pair sequences ei-
ther from the initialization stage or the re-
classification stage.
4.1.1 Transition Probabilities: Text Cohesion
and Text Segmentation
Text cohesion (Halliday and Hasan (1996)) is an
important concept in summarization as it under-
lines the theme of a text segment based on connec-
tivity patterns between sentences (Mani (2002)).
When an author writes from theme to theme in a
linear text, s/he generates sentences that are tightly
linked together within a theme. When s/he pro-
ceeds to the next theme, the sentences that are gen-
erated next are quite separate from the previous
theme of sentences but are they themselves tightly
linked again.

As mentioned in the introduction, most extraction-
based summarization approaches give certain con-
sideration to the linearity between sentences in a
text. For example, Mani (1999) uses spread activa-
tion weight between sentence links,  (Barzilay et al
2001) uses a cohesion constraint that led to im-
provement in summary quality. Anone et al (1999)
uses linguistic knowledge such as aliases, syno-
nyms, and morphological variations to link lexical
items together across sentences.

Term distribution has been studied by many NLP
researchers. Manning & Sch?tze (1999) gives a
good overview of various probability distributions
used to describe how a term appears in a text. The
distributions are in general non-Gaussian in nature.

Our Hidden Markov Model provides a unified
framework to incorporate text cohesion and term
distribution information in the transition probabili-
ties of theme classes. The class of a sentence de-
pends on the class labels of the previous N
sentences. The linearity of the text is hence pre-
served in our model.  In the preliminary experi-
ment, we set N to be one, that is, we are using a bi-
gram class model.

4.1.2 Emission Probabilities: Poisson distribu-
tion of terms
For the emission probabilities, there are a number
of possible formulations. We cannot use relative
frequency counts of number of sentences in clus-
ters divided by the total sentences in the cluster
since most sentences occur only once in the entire
corpus. Looking at the sentence feature vector, we
take the view that the probability of a sentence
vector being generated by a particular cluster is the
product of the probabilities of the index terms in
the sentence occurring in that cluster according to
some distribution, and that these term distribution
probabilities are independent of each other.

For a sentence vector of length L, where L is the
total size of the vocabulary, its elements?the in-
dex terms?have certain probability density func-
tion (pdf). In speech processing, spectral features
are assumed to follow independent Gaussian dis-
tributions. In language processing, several models
have been proposed for term distribution, including
the Poisson distribution, the two-Poisson model for
content and non-content words (Bookstein and
Swanson (1975)), the negative binomial (Mosteller
and Wallace (1984), Church and Gale (1995)) and
Katz?s k-mixture (Katz (1996)). We adopt two
schemes for comparison (1) the unigram distribu-
tion of each index term in the clusters; (2) the Pois-
son distribution as pdf. for modeling the term
emission probabilities:
!
);(
k
ekp
k
i
i
i
?? ??=

At each estimation step of the training process, the
? for the Poisson distribution is estimated from the
centroid of each theme cluster. 1
                                                         
1

Strictly speaking, we ought to re-estimate the IDF in the k-mixture
during each iteration by using the re-estimated clusters from the k-
means step as the documents. However, we simplify the process by
using the pre-computed IDF from all training documents.
4.2 Viterbi Decoding: Re-classification with
sentence cohesion

After each re-estimation, we use a Viterbi decoder
to find the best class sequence given a document
containing sentence sequences.  The ?time se-
quence? corresponds to the sequence of sentences
in a document whereas the states are the theme
classes.

At each node of the trellis, the probability of a sen-
tence given any class state is computed from the
transition probabilities and the emission probabili-
ties. After Viterbi backtracking, the best class se-
quence of a document is found and the sentences
are relabeled by the class tags.

5 Salient Sentence Extraction
The SKM algorithm is iterated until the decoding
likelihood converges. The final trained Viterbi de-
coder is then used to tag un-annotated data sets
into class-sentence pairs. We can then extract sali-
ent sentences from each class to be included in a
summary, or for question-answering.

To evaluate the effectiveness of our method as a
foundation for extractive summarization, we ex-
tract sentences from each theme class in each
document using four features, namely:
(1) the position of the sentence
n
p 1= -- the further it is
from the title, the less important it is supposed to be;
(2) the cosine similarity of the sentence with the centroid of
its class ?1;
(3) its similarity with the first sentence in the article ?2; and
(4) the so-called Z model (Zechner (1996), Nomoto & Ma-
tsumoto (2000)), where the mass of a sentence is com-
puted as the sum of tf, idf values of index terms in that
sentence and the center of mass  is chosen as the salient
sentence to be included in a summary.

))(())))((log(1(maxarg
1
tsidftstfz i
L
i
i
s

=
?+= 

The above features are linearly combined to yield a
final saliency score for every sentence:

zwwwpwsw ?+?+?+?= 423121)( ??
                                                                                         

Our features are similar to those in an existing sys-
tem (Radev 2002), with the difference in the cen-
troid computation (and cluster definition), resulting
from our stochastic system.

6 Experiments
Many researchers have proposed various evalua-
tion methods for summarization. We find that ex-
trinsic, task-oriented evaluation method to be most
easily automated, and quantifiable (Radev 2000).
We choose to evaluate our stochastic theme classi-
fication system (STCS) on a multi-document
summarization task, among other possible tasks
We choose a content-based method to evaluate the
summaries extracted by our system, compared to
those by another extraction-based system MEAD
(Radev 2002), and against a baseline system that
chooses the top N sentence in each document as
salient sentences.  All three systems are considered
unsupervised.

The evaluation corpus we use is a segment of the
English part of HKSAR news from the LDC, con-
sisting of 215 articles. We first use MEAD to ex-
tract summaries from 1%-20% compression ratio.
We then use our system to extract the same num-
ber of salient sentences as MEAD, according to the
sentence weights. The baseline system also ex-
tracts the same amount of data as the other two
systems. We plot the cosine similarities of the
original 215 documents with each individual ex-
tracted summaries from these three systems. The
following figure shows a plot of cosine similarity
scores against compression ratio of each extracted
summary. In terms of relative similarity score, our
system is 22.8% higher on average than MEAD,
and 46.3% higher on average than the base-
line.
0
0. 1
0. 2
0. 3
0. 4
0. 5
0. 6
0. 7
0. 8
0. 9
0% 10% 20% 30%
Our  syst em
MEAD
TOP-N Sent

Figure 2: Our system outperforms an existing multi-document
summarizer (MEAD) by 22.8% on average, and outperforms
the baseline top-N sentence selection system by 46.3% on
average.

We would like to note that in our comparative
evaluation, it is necessary to normalize all variable
factors that might affect the system performance,
other than the intrinsic algorithm in each system.
For example, we ensure that the sentence segmen-
tation function is identical in all three systems. In
addition, index term weights need to be properly
trained within their own document clusters. Since
MEAD discards all sentences below the length 9,
the other two systems also discard such sentences.
The feature weights in both our system and MEAD
are all set to the default value one. Since all other
features are the same between our system and
MEAD, the difference in performance is attributed
to the core clustering and centroid computation
algorithms in both systems.
7 Conclusion and Discussion
We have presented a stochastic HMM framework
with modified K-means and segmental K-means
algorithms for extractive summarization. Our
method uses an unsupervised, probabilistic ap-
proach to find class centroids, class sequences and
class boundaries in linear, unrestricted texts in or-
der to yield salient sentences and topic segments
for summarization and question and answer tasks.
We define a class to be a group of connected sen-
tences that corresponds to one or multiple topics in
the text. Such topics can be answers to a user
query, or simply one concept to be included in the
summary. We define a Markov model where the
states correspond to the different classes, and the
observations are continuous sequences of sen-
tences in a document. Transition probabilities are
the class transitions obtained from a training cor-
pus. Emission probabilities are the probabilities of
an observed sentence given a specific class, fol-
lowing a Poisson distribution.  Unlike conventional
methods where texts are treated as independent
sentences to be clustered together, our method in-
corporates text cohesion information in the class
transition probabilities. Unlike other HMM and
noisy channel, probabilistic approaches for infor-
mation retrieval, our method does not require an-
notated data as it is unsupervised.

We also suggest using modified K-means cluster-
ing algorithm to avoid ad hoc choices of initial
cluster set as in the conventional K-means algo-
rithm. For unsupervised training, we use a segmen-
tal K-means training method to iteratively improve
the clusters. Experimental results show that the
content-based performance of our system is 22.8%
above that of an existing extractive summarization
system, and 46.3% above that of simple top-N sen-
tence selection system.  Even though the evalua-
tion on the training set is not a close evaluation
since the training is unsupervised, we will also
evaluate on testing data not included in the training
set as our trained decoder can be used to classify
sentences in unseen texts. Our framework serves as
a foundation for future incorporation of other sta-
tistical and linguistic information as vector features,
such as part-of-speech tags, name aliases, syno-
nyms, and morphological variations.
References
Chinatsu Aone, James Gorlinsky, Bjornar Larsen, and Mary Ellen Oku-
rowski, 1999. A trainable summarizer with knowledge acquired from
robust NLP techniques. In Advances in automatic text summarization,
ed. Inderjeet Mani and Mark T. Maybury. pp 71-80.
Michele Banko, Vibhu O. Mittal & Michael J. Witbrock. 2000. Headline
Generation Based on Statistical Translation. In Proc. Of the Associa-
tion for Computational Linguistics. 
Regina Barzilay,  Noemie Elhadad & Kathleen R. McKeown. 2001. Sen-
tence Ordering in Multi-document Summarization. In  Proceedings of
the 1st Human Language Technology Conference. pp 149-156. San
Diego, CA, US.
Adam Berger & John Lafferty. 1999. Information Retrieval as Statistical
Translation. .  . In Proc. Of the 22nd  ACM SIGIR Conference (SIGIR-
99). pp 222-229. Berkeley, CA, USA.
Branimir Boguraev & Christopher Kennedy. 1999. Salience-Based Content
Characterisation of Text Documents.  In Advances in automatic text
summarization / edited. Inderjeet Mani and Mark T. Maybury. pp 99-
110.
Kenneth Ward Church. 1988. A Stochastic Parts Program and Noun Phrase
Parser for Unrestricted Text.  In  Proceedings of the Second Conference
on Applied Natural Language Processing. pp 136--143.  Austin, TX.
Ido Dagan, Lillian Lee, & Fernando Pereira. 1997. Similarity-based meth-
ods for word sense disambiguation. In Proc. Of the 32nd  Conference of
the Association of Computational Linguistics, pp 56-63.
Halliday & Hasan, 1976. Cohesion in English. London: Longman.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation of       Expository
Text.  In  Proc. Of the Association for Computational Linguistics. pp 9-
16.  Las Cruces, NM.
Eduard Hovy & Chin-Yew Lin. 1999. Automated Text Summarization in
SUMMARIST In Advances in automatic text summarization / edited.
Inderjeet Mani and Mark T. Maybury.  pp 81-97.
H. Jing. Dragomir R. Radev and M. Budzikowska. Centroid-based summa-
rization of multiple documents: sentence extaction, utility-based
evaluation and user studies. In Proceedings of ANLP/NAACL-2000.
Slava M. Katz. 1996. Distribution of content words and phrases in text and
language modeling. In Natural Language Engineering, Vol.2 Part.1,
pp15-60.
Kevin Knight & Daniel Marcu. 2000. Statistics-Based Summarization ?
Step One: Sentence Compression. In Proc. Of the 17th Annual Confer-
ence of the American Association for Artificial Intelligence. pp  703-
710. Austin, Texas, US.
Julian Kupiec, Jan Pedersen & Francine Chen. 1995. A Trainable Docu-
ment Summarizer. In Proc. Of the 18th ACM-SIRGIR Conference. pp
68-73.
Inderjeet Mani & Eric Bloedorn. 1999. Summarizing Similarities and
Differences Among Related Documents.  In Information Retrieval, 1.
pp 35-67.
Christopher D. Manning & Hinrich Sch   tze 1999. Foundations of statisti-
cal natural language processing. The MIT Press  Cambridge, Massachu-
setts. London, England.
Daniel Marcu. 1997. The Rhetorical Parsing of Natural Language Texts.
In Proc. of the 35th Annual Meeting of Association for Computational
Linguistics and 8th Conference of European Chapter of Association for
Computational Linguistics. pp 96-103.  Madrid, Spain.
Bernard Merialdo. 1994. Tagging English Text with a Probabilistic Model.
In Computational Linguistics, 20.2. pp 155-172.
David R.H. Miller, Tim Leek & Richard M. Schwartz. 1999. A Hidden
Markov Model Information Retrieval System. In Proc. Of the
SIGIR?99   pp 214?221.  Berkley, CA, US.
Sung Hyon Myaeng & Dong-Hyun Jang. 1999. Development and Evalua-
tion of a Statistically-Based Document Summarization System. In Ad-
vances in automatic text summarization / edited. Inderjeet Mani and
Mark T. Maybury. MIT Press. pp 61-70.
Tadashi Nomoto & Yuji Matsumoto. 2002. Supervised ranking in open
domain summarization. In Proc. Of the 40th  Conference of the Associa-
tion of Computational Linguistics, pp.. Pennsylvania, US.
Tadashi Nomoto & Yuji Matsumoto.  2001.  A New Approach to Unsu-
pervised Text Summarization. In  Proc. Of the SIGIR?01, pp 26-34  New
Orleans, Louisiana, USA.
Jahna C. Otterbacher, Dragomir R. Radev & Airong Luo. 2002. Revision
that Improve Cohesion in Multi-document Summaries. In  Proc. Of the
Workshop on Automatic Summarization (including DUC 2002), pp 27-
36. Association for Computational Linguistics. Philadelphia, US.
Miles Osborne. 2002. Using Maximum Entropy for Sentence Extraction.
In  Proc. Of the Workshop on Automatic Summarization (including
DUC 2002), pp 1-8. Association for Computational Linguistics. Phila-
delphia, US.
Dragomir Radev, Adam Winkel & Michael Topper . 2002. Multi-
Document Centroid-based Text Summarization.. In Proc. Of the ACL-
02 Demonstration Session, pp112-113. Pennsylvania, US.
Michael White & Claire Cardie. 2002. Selecting Sentences for Multidocu-
ment Summaries using Randomized Local Search.  In Proc. Of the
Workshop on Automatic Summarization (including DUC 2002), pp 9-
18. Association for Computational Linguistics. Philadelphia, US.
J.G. Wilpon & L.R. Rabiner 1985. A modified K-means clustering algo-
rithm for use in isolated word recognition. In IEEE Trans. Acoustics,
Speech, Signal Proc. ASSP-33(3), pp 587-594.
K. Zechner. 1996. Fast generation of abstracts from general domain text
corpora by extracting relevant sentences. In Proc. Of the 16th  Interna-
tional Conference on Computational Linguistics, pp 986-989. Copen-
hagen, Denmark.
Hongyuan Zha. 2002. Generic Summarization and Keyphrase Extraction
Using Mutual Reinforcement Principle and Sentence Clustering.  In
Proc. Of the SIGIR?02. pp 113-120.  Tampere, Finland Endre Boros,
Paul B. Kantor & David J. Neu. 2001. A Clustering Based Approach to
Creating Multi-Document Summaries.
Semantic Role Labeling with
Boosting, SVMs, Maximum Entropy, SNOW, and Decision Lists
Grace NGAI?1 , Dekai WU?2
Marine CARPUAT? , Chi-Shing WANG? , Chi-Yung WANG?
? Dept. of Computing
HK Polytechnic University
Hong Kong
? HKUST, Dept of Computer Science
Human Language Technology Center
Hong Kong
csgngai@polyu.edu.hk, dekai@cs.ust.hk
marine@cs.ust.hk, wcsshing@netvigator.com, cscywang@comp.polyu.edu.hk
Abstract
This paper describes the HKPolyU-HKUST sys-
tems which were entered into the Semantic Role La-
beling task in Senseval-3. Results show that these
systems, which are based upon common machine
learning algorithms, all manage to achieve good
performances on the non-restricted Semantic Role
Labeling task.
1 Introduction
This paper describes the HKPolyU-HKUST sys-
tems which participated in the Senseval-3 Semantic
Role Labeling task. The systems represent a diverse
array of machine learning algorithms, from decision
lists to SVMs to Winnow-type networks.
Semantic Role Labeling (SRL) is a task that
has recently received a lot of attention in the NLP
community. The SRL task in Senseval-3 used
the Framenet (Baker et al, 1998) corpus: given a
sentence instance from the corpus, a system?s job
would be to identify the phrase constituents and
their corresponding role.
The Senseval-3 task was divided into restricted
and non-restricted subtasks. In the non-restricted
subtask, any and all of the gold standard annotations
contained in the FrameNet corpus could be used.
Since this includes information on the boundaries
of the parse constituents which correspond to some
frame element, this effectively maps the SRL task
to that of a role-labeling classification task: given a
constituent parse, identify the frame element that it
belongs to.
Due to the lack of time and resources, we chose to
participate only in the non-restricted subtask. This
enabled our systems to take the classification ap-
proach mentioned in the previous paragraph.
1The author would like to thank the Hong Kong Polytechnic
University for supporting this research in part through research
grants A-PE37 and 4-Z03S.
2The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
2 Experimental Features
This section describes the features that were used
for the SRL task. Since the non-restricted SRL task
is essentially a classification task, each parse con-
stituent that was known to correspond to a frame
element was considered to be a sample.
The features that we used for each sample have
been previously shown to be helpful for the SRL
task (Gildea and Jurafsky, 2002). Some of these
features can be obtained directly from the Framenet
annotations:
? The name of the frame.
? The lexical unit of the sentence ? i.e. the lex-
ical identity of the target word in the sentence.
? The general part-of-speech tag of the target
word.
? The ?phrase type? of the constituent ? i.e. the
syntactic category (e.g. NP, VP) that the con-
stituent falls into.
? The ?grammatical function? (e.g. subject, ob-
ject, modifier, etc) of the constituent, with re-
spect to the target word.
? The position (e.g. before, after) of the con-
stituent, with respect to the target word.
In addition to the above features, we also ex-
tracted a set of features which required the use of
some statistical NLP tools:
? Transitivity and voice of the target word ?
The sentence was first part-of-speech tagged
and chunked with the fnTBL transformation-
based learning tools (Ngai and Florian, 2001).
Simple heuristics were then used to deduce the
transitivity voice of the target word.
? Head word (and its part-of-speech tag) of the
constituent ? After POS tagging, a syntactic
parser (Collins, 1997) was then used to ob-
tain the parse tree for the sentence. The head
word (and the POS tag of the head word) of
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the syntactic parse constituent whose span cor-
responded most closely to the candidate con-
stituent was then assumed to be the head word
of the candidate constituent.
The resulting training data set consisted of 51,366
constituent samples with a total of 151 frame ele-
ment types. These ranged from ?Descriptor? (3520
constituents) to ?Baggage? and ?Carrier? (1 con-
stituent each). This training data was randomly par-
titioned into a 80/20 ?development training? and
?validation? set.
3 Methodology
The previous section described the features that
were extracted for each constituent. This section
will describe the experiment methodology as well
as the learning systems used to construct the mod-
els.
Our systems had originally been trained on the
entire development training (devtrain) set, gener-
ating one global model per system. However, on
closer examination of the task, it quickly became
evident that distinguishing between 151 possible
outcomes was a difficult task for any system. It
was also not clear that there was going to be a
lot of information that could be generalized across
frame types. We therefore partitioned the data by
frame, so that one model would be trained for each
frame. (This was also the approach taken by (Gildea
and Jurafsky, 2002).) Some of our individual sys-
tems tried both approaches; the results are com-
pared in the following subsections. For compar-
ison purposes, a baseline model was constructed
by simply classifying all constituents with the most
frequently-seen (in the training set) frame element
for the frame.
In total, five individual systems were trained for
the SRL task, and four ensemble models were gen-
erated by using various combinations of the indi-
vidual systems. With one exception, all of the indi-
vidual systems were constructed using off-the-shelf
machine learning software. The following subsec-
tions describe each system; however, it should be
noted that some of the individual systems were not
officially entered as competing systems; therefore,
their scores are not listed in the final rankings.
3.1 Boosting
The most successful of our individual systems is
based on boosting, a powerful machine learning
algorithm which has been shown to achieve good
results on NLP problems in the past. Our sys-
tem was constructed around the Boostexter soft-
ware (Schapire and Singer, 2000), which imple-
Model Prec. Recall Attempted
Single Model 0.891 0.795 89.2%
Frame Separated 0.894 0.798 89.2%
Baseline 0.444 0.396 89.2%
Table 1: Boosting Models: Validation Set Results
ments boosting on top of decision stumps (decision
trees of one level), and was originally designed for
text classification. The same system also partici-
pated in the Senseval-3 lexical sample tasks for Chi-
nese and English, as well as the Multilingual lexical
sample task (Carpuat et al, 2004).
Table 1 compares the results of training one sin-
gle overall boosting model (Single) versus training
separate models for each frame (Frame). It can be
seen that training frame-specific models produces
a small improvement over the single model. The
frame-specific model was used in all of the ensem-
ble systems, and was also entered into the competi-
tion as an individual system (hkpust-boost).
3.2 Support Vector Machines
The second of our individual systems was based
on support vector machines, and implemented using
the TinySVM software package (Boser et al, 1992).
Since SVMs are binary classifiers, we used a
one-against-all method to reduce the SRL task to
a binary classification problem. One model is con-
structed for each possible frame element and the
task of the model is to decide, for a given con-
stituent, whether it should be classified with that
frame element. Since it is possible for all the bi-
nary classifiers to decide on ?NOT-<element>?, the
model is effectively allowed to pass on samples that
it is not confident about. This results in a very pre-
cise model, but unfortunately at a significant hit to
recall.
A number of kernel parameter settings were in-
vestigated, and the best performance was achieved
with a polynomial kernel of degree 4. The rest of
the parameters were left at the default values. Table
2 shows the results of the best SVM model on the
validation set. This model participated in the all of
the ensemble systems, and was also entered into the
competition as an individual system.
System Prec. Recall Attempted
SVM 0.945 0.669 70.8%
Baseline 0.444 0.396 89.2%
Table 2: SVM Models: Validation Set Results
3.3 Maximum Entropy
The third of our individual systems was based on
the maximum entropy model, and implemented on
top of the YASMET package (Och, 2002). Like the
boosting model, the maximum entropy system also
participated in the Senseval-3 lexical sample tasks
for Chinese and English, as well as the Multilingual
lexical sample task (Carpuat et al, 2004).
Our maximum entropy models can be classi-
fied into two main approaches. Both approaches
used the frame-partitioned data. The more conven-
tional approach (?multi?) then trained one model
per frame; that model would be responsible for clas-
sifying a constituent belonging to that frame with
one of several possible frame elements. The second
approach (binary) used the same approach as the
SVM models, and trained one binary one-against-
all classifier for each frame type-frame element
combination. (Unlike the boosting models, a single
maximum entropy model could not be trained for all
possible frame types and elements, since YASMET
crashed on the sheer size of the feature space.)
System Prec. Recall Attempted
multi 0.856 0.764 89.2%
binary 0.956 0.539 56.4%
Baseline 0.444 0.396 89.2%
Table 3: Maximum Entropy Models: Validation Set
Results
Table 3 shows the results for the maximum en-
tropy models. As would have been expected, the
binary model achieves very high levels of precision,
but at considerable expense of recall. Both systems
were eventually used in the some of the ensemble
models but were not submitted as individual contes-
tants.
3.4 SNOW
The fourth of our individual systems is based on
SNOW ? Sparse Network Of Winnows (Mun?oz et
al., 1999).
The development approach for the SNOW mod-
els was similar to that of the boosting models. Two
main model types were generated: one which gener-
ated a single overall model for all the possible frame
elements, and one which generated one model per
frame type. Due to a bug in the coding which was
not discovered until the last minute, however, the
results for the frame-separated model were invali-
dated. The single model system was eventually used
in some of the ensemble systems, but not entered as
an official contestant. Table 4 shows the results.
System Prec. Recall Attempted
Single Model 0.764 0.682 89.2%
Baseline 0.444 0.396 89.2%
Table 4: SNOW Models: Validation Set Results
3.5 Decision Lists
The final individual system was a decision list im-
plementation contributed from the Swarthmore Col-
lege team (Wicentowski et al, 2004), which partic-
ipated in some of the lexical sample tasks.
The Swarthmore team followed the frame-
separated approach in building the decision list
models. Table 5 shows the result on the validation
set. This system participated in some of the final
ensemble systems as well as being an official par-
ticipant (hkpust-swat-dl).
System Prec. Recall Attempted
DL 0.837 0.747 89.2%
Baseline 0.444 0.396 89.2%
Table 5: Decision List Models: Validation Set Re-
sults
3.6 Ensemble Systems
Classifier combination, where the results of differ-
ent models are combined in some way to make a
new model, has been well studied in the literature.
A successful combined classifier can result in the
combined model outperforming the best base mod-
els, as the advantages of one model make up for the
shortcomings of another.
Classifier combination is most successful when
the base models are biased differently. That condi-
tion applies to our set of base models, and it was
reasonable to make an attempt at combining them.
Since the performances of our systems spanned
a large range, we did not want to use a simple ma-
jority vote in creating the combined system. Rather,
we used a set of heuristics which trusted the most
precise systems (the SVM and the binary maximum
entropy) when they made a prediction, or a combi-
nation of the others when they did not.
Table 6 shows the results of the top-scoring com-
bined systems which were entered as official con-
testants. As expected, the best of our combined sys-
tems outperformed the best base model.
4 Test Set Results
Table 7 shows the test set results for all systems
which participated in some way in the official com-
petition, either as part of a combined system or as
an individual contestant.
Model Prec. Recall Attempted
svm, boosting, maxent (binary) (hkpolyust-all(a)) 0.874 0.867 99.2%
boosting (hkpolyust-boost) 0.859 0.852 0.846%
svm, boosting, maxent (binary), DL (hkpolyust-swat(a)) 0.902 0.849 94.1%
svm, boosting, maxent (binary), DL, snow (hkpolyust-swat(b)) 0.908 0.846 93.2%
svm, boosting, maxent (multi), DL, snow (hkpolyust-all(b)) 0.905 0.846 93.5%
decision list (hkpolyust-swat-dl) 0.819 0.812 99.2%
maxent (multi) 0.827 0.735 88.8%
svm (hkpolyust-svm) 0.926 0.725 76.1%
snow 0.713 0.499 70.0%
maxent (binary) 0.935 0.454 48.6%
Baseline 0.438 0.388 88.6%
Table 7: Test set results for all our official systems, as well as the base models used in the ensemble system.
Base Models Prec. Recall Attempted
svm, boosting,
maxent (bin)
0.901 0.803 89.2%
svm, boosting,
maxent (bin), snow
0.938 0.8 85.2%
svm, boosting,
maxent (bin), DL
0.926 0.783 84.6%
svm, boosting,
maxent (multi),
DL, snow
0.935 0.797 85.2%
Baseline 0.444 0.396 89.2%
Table 6: Combined Models: Validation Set Results
The top-performing system is the combined sys-
tem that uses the SVM, boosting and the binary im-
plementation of maximum entropy. Of the individ-
ual systems, boosting performs the best, even out-
performing 3 of the combined systems. The SVM
suffers from its high-precision approach, as does the
binary implementation of maximum entropy. The
rest of the systems fall somewhere in between.
5 Conclusion
This paper presented the HKPolyU-HKUST sys-
tems for the non-restricted Semantic Role Labeling
task for Senseval-3. We mapped the task to that
of a simple classification task, and used features
and systems which were easily extracted and con-
structed. Our systems achieved good performance
on the SRL task, easily beating the baseline.
6 Acknowledgments
The ?hkpolyust-swat-*? systems are the result of
joint work between our team and Richard Wicen-
towski?s team at Swarthmore College. The authors
would like to express their immense gratitude to the
Swarthmore team for providing their decision list
system as one of our models.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Chris-
tian Boitet and Pete Whitelock, editors, Proceedings
of the Thirty-Sixth Annual Meeting of the Associa-
tion for Computational Linguistics and Seventeenth
International Conference on Computational Linguis-
tics, pages 86?90, San Francisco, California. Morgan
Kaufmann Publishers.
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vap-
nik. 1992. A training algorithm for optimal margin
classifiers. In Computational Learing Theory, pages
144?152.
Marine Carpuat, Weifeng Su, and Dekai Wu. 2004.
Augmenting Ensemble Classification for Word Sense
Disambiguation with a Kernel PCA Model. In Pro-
ceedings of Senseval-3, Barcelona.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the ACL (jointly with the 8th
Conference of the EACL), Madrid.
Daniel Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):256?288.
Marcia Mun?oz, Vasin Punyakanok, Dan Roth, and Dav
Zimak. 1999. A learning approach to shallow pars-
ing. In Proceedings of EMNLP-WVLC?99, pages
168?178, College Park. Association for Computa-
tional Linguistics.
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proceedings of the 39th
Conference of the Association for Comp utational Lin-
guistics, Pittsburgh, PA.
Franz Josef Och. 2002. Yet Another Small Max-
ent Toolkit: Yasmet. http://www-i6.informatik.rwth-
aachen.de/Colleagues/och.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Richard Wicentowski, Emily Thomforde, and Adrian
Packel. 2004. The Swarthmore College Senseval-3
system. In Proceedings of Senseval-3, Barcelona.
Joining forces to resolve lexical ambiguity:
East meets West in Barcelona
Richard WICENTOWSKI*, Grace NGAI?1 , Dekai WU?2
Marine CARPUAT? , Emily THOMFORDE*, Adrian PACKEL*
*Swarthmore College
Swarthmore, PA
USA
? Dept. of Computing
? HK Polytechnic University
Hong Kong
? HKUST, Dept of Computer Science
Human Language Technology Center
Hong Kong
richardw@cs.swarthmore.edu, csgngai@polyu.edu.hk, dekai@cs.ust.hk
marine@cs.ust.hk, ethomfo1@cs.swarthmore.edu, packel@cs.swarthmore.edu
Abstract
This paper describes the component models and
combination model built as a joint effort be-
tween Swarthmore College, Hong Kong PolyU, and
HKUST. Though other models described elsewhere
contributed to the final combination model, this pa-
per focuses solely on the joint contributions to the
?Swat-HK? effort.
1 Introduction
This paper describes the two joint component mod-
els of the Swat-HK systems entered into four of
the word sense disambiguation lexical sample tasks
in Senseval-3: Basque, Catalan, Italian and Roma-
nian, as well as a combination model for each lan-
guage. The feature engineering (and construction of
three other component models which are described
in (Wicentowski et al, 2004)) was performed at
Swarthmore College, while the Hong Kong team
constructed two component models based on well-
known machine learning algorithms. The combina-
tion model, which was constructed at Swarthmore,
uses voting to combine all five models.
2 Experimental Features
A full description of the experimental features for
all four tasks can be found in the report submitted
by the Swarthmore College Senseval team (Wicen-
towski et al, 2004). Briefly, the systems used lexi-
cal and syntactic features in the context of the target
word:
? The ?bag of words (and lemmas)? in the con-
text of the ambiguous word.
? Bigrams and trigrams of words (and lemmas,
1The author would like to thank the Hong Kong Polytechnic
University for supporting this research in part through research
grants A-PE37 and 4-Z03S.
2The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
part-of-speech tags, and, for Basque, case in-
formation) surrounding the ambiguous word.
? The topic (or code) of the document containing
the current instance of the word was extracted.
(Basque and Catalan only.)
These features have been shown to be effective
in previous WSD research. Since our systems were
all supervised, all the data used was provided by the
Senseval organizers; no additional (unlabeled) data
was included.
3 Methodology
The systems that were constructed by this team in-
cluded two component models: a boosting model
and a maximum entropy model as well as a com-
bination system. The component models were also
used in other Senseval-3 tasks: Semantic Role La-
beling (Ngai et al, 2004) and the lexical sample
tasks for Chinese and English, as well as the Multi-
lingual task (Carpuat et al, 2004).
To perform parameter tuning for the two compo-
nent models, 20% of the samples from the training
set were held out into a validation set. Since we
did not expect the senses of different words to share
any information, the training data was partitioned by
the ambiguous word in question. A model was then
trained for each ambiguous word type. In total, we
had 40 models for Basque, 27 models for Catalan,
45 models for Italian and 39 models for Romanian.
3.1 Boosting
Boosting is a powerful machine learning algorithm
which has been shown to achieve good results on
a variety of NLP problems. One known property
of boosting is its ability to handle large numbers of
features. For this reason, we felt that it would be
well suited to the WSD task, which is known to be
highly lexicalized with a large number of possible
word types.
Our system was constructed around the Boostex-
ter software (Schapire and Singer, 2000), which im-
plements boosting on top of decision stumps (deci-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
sion trees of one level), and was originally designed
for text classification.
Tuning a boosting system mainly lies in modify-
ing the number of iterations, or the number of base
models it would learn. Larger number of iterations
contribute to the boosting model?s power. However,
they also make it more prone to overfitting and in-
crease the training time. The latter, a simple dis-
advantage in another problem, becomes a real issue
for Senseval, since large numbers of models (one for
each word type) need to be trained in a short period
of time.
Since the available features differed from lan-
guage to language, the optimal number of iterations
also varied. Table 1 shows the performance of the
model on the validation set with respect to the num-
ber of iterations per language.
Accuracy
Number of iterations
Language 500 1000 2000
Basque 66.12% 67.07% 67.08%
Catalan 84.77% 84.89% 85.02%
Italian 51.11% 50.93%
Romanian 64.68% 64.52%
Table 1: Boosting models on the validation sets.
The final systems for the languages used 2000 it-
erations for Basque and Catalan and 500 iterations
for Italian and Romanian. The test set results are
shown in Table 4
3.2 Maximum Entropy
The other individual system was based on the maxi-
mum entropy model, another machine learning al-
gorithm which has been successfully applied to
many NLP problems. Our system was implemented
on top of the YASMET package (Och, 2002).
Due to lack of time, we did not manage to fine-
tune the maximum entropy model. The YASMET
package does provide a number of easily variable
parameters, but we were only able to try varying the
feature selection count threshold and the smoothing
parameter, and only on the Basque data.
Experimentally, however, smoothing did not
seem to make a difference. The only change in per-
formance was caused by varying the feature selec-
tion count threshold, which controls the number of
times a feature has to be seen in the training set in
order to be considered. Table 2 shows the perfor-
mances of the system on the Basque validation set,
with count thresholds of 0, 1 and 2.
Since word sense disambiguation is known to be
Threshold
0 1 2
Accuracy 55.62% 66.13% 65.68%
Table 2: Maximum Entropy Models on Basque val-
idation set.
a highly lexicalized task involving many feature val-
ues and sparse data, it is not too surprising that set-
ting a low threshold of 1 proves to be the most effec-
tive. The final system kept this threshold, smooth-
ing was not done and the GIS iterations allowed to
proceed until it converged on its own. These param-
eters were used for all four languages.
The maximum entropy model was not entered
into the competition as an official contestant; how-
ever, it did participate in the combined system.
3.3 Combined System
Ensemble methods have been widely studied in
NLP research, and it is well-known that a set of
systems will often combine to produce better re-
sults than those achieved by the best individual sys-
tem alone. The final system contributed by the
Swarthmore-Hong Kong team was such an ensem-
ble. In addition to the boosting and maximum en-
tropy models mentioned earlier, three other models
were included: a nearest-neighbor clustering model,
a decision list, and a Na??ve Bayes model. The five
models were then combined by a simple weighted
majority vote, with an ad-hoc weight of 1.1 given
to the boosting and decision lists systems, and 1.0
otherwise, with ties broken arbitrarily.
Due to an unfortunate error with the input data of
the voting algorithm (Wicentowski et al, 2004), the
official submitted results for the combined system
were poorer than they should have been. Table 3
compares the official (submitted) results to the cor-
rected results on the test set. The decrease in per-
formance caused by the error ranged from 0.9% to
3.3%.
Language official corrected net gain
Basque 67.0% 67.9% 0.9%
Catalan 79.5% 80.4% 0.9%
Italian 51.4% 54.7% 3.3%
Romanian 72.4% 73.3% 0.9%
Table 3: Ensemble system results on the test set.
Both official and corrected results are included.
System
Description Name Acc. (%)
Basque
Boosting basque-swat hk-bo 71.1
Combined swat-hk-basque 67.0 (67.9)
NNC 66.0
DL 64.6
Maxent 62.1
NB 60.4
Baseline 55.8
Catalan
Boosting catalan-swat hk-bo 79.6
DL 80.6
Combined swat-hk-catalan 79.5 (80.4)
NNC 77.5
NB 71.3
Maxent 70.9
Baseline 66.4
Italian
Combined swat-hk-italian 51.4 (54.7)
DL 50.3
Boosting italian-swat hk 48.3
Maxent 46.9
NNC 44.9
NB 42.1
Baseline 23.7
Romanian
Boosting romanian-swat hk-bo 72.7
Combined swat-hk-romanian 72.4 (73.3)
DL 70.9
NNC 67.9
Maxent 66.5
NB 62.8
Baseline 58.4
Table 4: Test set results on 4 languages. Offi-
cial contestants are in bold; corrected voting results
are in parentheses. Key: NB: Na??ve Bayes, NNC:
Nearest-Neighbor Clustering, DL: Decision List
4 Test Set Results
Final results from all the systems are shown in Ta-
ble 4. As a reference, the results of a simple base-
line system which assigns the most frequent sense
as seen in the training set is also provided.
Due to the error in the voting system, the offi-
cial results for the combination system were lower
than they should have been ? as a result, boosting
was officially the top ranked system for 3 of the 4
languages. With the corrected results, however, the
combined system outperforms the individual mod-
els, as expected. The only exception is Basque,
where the booster had an exceptionally strong per-
formance. This is probably due to the fact that
Basque has a much richer feature set than the other
languages, which boosting was better able to take
advantage of.
The poor performance of the maximum entropy
model was also unexpected at first; however, it is
perhaps not too surprising, given the lack of time
spent on fine-tuning the model. As a result, most of
the parameters were left at their default values.
One thing worth noting is the fact that the sys-
tems were combined as ?closed systems? ? i.e. all
that was known about them was the output result,
and nothing else. The result was that no confidence
measures from the boosting and maximum entropy
could be used in the combined system. It is likely
that the performance could have been further im-
proved if more information had been available.
5 Conclusions and Discussion
This paper describes the ?Swat-HK? systems which
were the result of collaborative work between
Swarthmore College, Hong Kong Polytechnic Uni-
versity and HKUST. Several base systems were con-
structed on the same feature set, and a weighed ma-
jority voting system was used to combine the re-
sults. The individual systems all achieve good re-
sults, easily beating the baseline. As expected, the
combined system outperforms the best individual
system for the majority of the tasks.
References
Marine Carpuat, Weifeng Su, and Dekai Wu. 2004.
Augmenting Ensemble Classification for Word
Sense Disambiguation with a Kernel PCA Model.
In Proceedings of Senseval-3, Barcelona.
Grace Ngai, Dekai Wu, Marine Carpuat, Chi-Shing
Wang, and Chi-Yung Wang. 2004. Semantic
Role Labeling with Boosting, SVMs, Maximum
Entropy, SNOW, and Decision Lists. In Proceed-
ings of Senseval-3, Barcelona.
Franz Josef Och. 2002. Yet Another Small
Maxent Toolkit: Yasmet. http://www-
i6.informatik.rwth-aachen.de/Colleagues/och.
Robert E. Schapire and Yoram Singer. 2000. Boos-
texter: A boosting-based system for text catego-
rization. Machine Learning, 39(2/3):135?168.
Richard Wicentowski, Emily Thomforde, and
Adrian Packel. 2004. The Swarthmore College
Senseval-3 system. In Proceedings of Senseval-
3, Barcelona.
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 40?47,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Clustering Approach for  Unsupervised Chinese Coreference Resolution 
Chi-shing Wang Grace NGAI Department of Computing Hong Kong Polytechnic University Kowloon, HONG KONG {cscswang, csgngai}@comp.polyu.edu.hk   Abstract Coreference resolution is the process of identifying expressions that refer to the same entity. This paper presents a clustering algo-rithm for unsupervised Chinese coreference resolution. We investigate why Chinese coreference is hard and demonstrate that techniques used in coreference resolution for English can be extended to Chinese. The proposed system exploits clustering as it has advantages over traditional classification methods, such as the fact that no training data is required and it is easily extended to accommodate additional features. We con-duct a set of experiments to investigate how noun phrase identification and feature selec-tion can contribute to coreference resolution performance. Our system is evaluated on an annotated version of the TDT3 corpus using the MUC-7 scorer, and obtains comparable performance. We believe that this is the first attempt at an unsupervised approach to Chi-nese noun phrase coreference resolution. 1 INTRODUCTION  Noun phrase coreference resolution is the proc-ess of detecting noun phrases (NPs) in a docu-ment and determining whether the NPs refer to the same entity, where an entity is defined as ?a construct that represents an abstract identity?. The NPs that refer to the entity are known as mentions. Mentions can be antecedents or ana-phors. An anaphor is an expression that refers back to a previous expression in a discourse. In Figure 1, ????? (President Clinton) refers to ??? (Clinton) and is described as an ana-
phoric reference to??? (Clinton). ????? (President Clinton) is described as the antece-dent of ? (he). ??? (Clinton), ????? (President Clinton) and the second ? (he) are all mentions of the same entity that refers to former U.S. president Bill Clinton. 
NP coreference resolution is an important sub-task in natural language processing (NLP) appli-cations such as text summarization, information extraction, data mining and question answering. This task has attracted much attention in recent years (Cardie and Wagstaff, 1999; Harabagiu et al, 2001; Soon et al, 2001; Ng and Cardie, 2002; Yang et al, 2004; Florian et al, 2004; Zhou et al, 2005), and has been included as a subtask in the MUC (Message Understanding Conferences) and ACE (Automatic Content Extraction) competitions.  Coreference resolution is a difficult task for various reasons. Firstly, a list of features can play a role to support coreference resolution such as 
 
Figure 1: An excerpt from the text, with core-ferring noun phrases annotated. English trans-lation in italics. 
[???1]???????????[??2]??????[???3]?[???1]????????[?3]???[?????1]???????[?1]?????[??2]????????	 [Clinton1] said that Washington would progres-sively follow through on economic aid to [Ko-rea2]. [Kim Dae-Jung3] applauded [Clinton1]?s speech. [He1] said, ?[President Clinton1] reiter-ated in the talks that [he1] would provide solid support for [Korea2] to shake off the economic crisis. 
40
gender agreement, number agreement, head noun matches, semantic class, positional information, contextual information, appositive, abbreviation etc. Ng and Cardie (2002) found 53 features which are useful for this problem. However, no single feature is completely reliable since there are always exceptions: e.g. the number agree-ment test returns false when ????  (this army, singular) is matched against ??? (army members, plural), despite the two phrases being coreferential. Secondly, identifying features automatically and accurately is hard. Features such as semantic class come from named entity recognition (NER) systems and ontologies and gazetteers, but they are not always accurate, es-pecially where new terms are concerned. Thirdly, coreference resolution subsumes the pronoun resolution problem, which is already difficult since pronouns carry limited lexical and semantic information. In addition to the aforementioned, Chinese coreference resolution is also made more diffi-cult due to the lack of morphological and ortho-graphic clues. Chinese words contain less exte-rior information than words in many Indoeuro-pean languages. For example, in English, number agreement can be detected through word inflec-tions and part-of-speech (POS) tags, but there are no simple rules in Chinese to distinguish whether a word is singular or plural. Proper name and abbreviations are identified by capitalization in English, but Chinese does not use capitalization. Moreover, written Chinese does not have word boundaries, so word segmentation is a crucial problem, as we cannot get the true meaning of the sentence based on characters alone. A simple sentence can be segmented in several different ways to get different meanings.  This characteris-tic affects the performance of all parts and leads to irrecoverable errors. In addition, there are very few Chinese coreference data sets available for research purposes (none of them freely available) and as a result, no easily obtainable benchmark-ing dataset for training and measuring perform-ance. Building a reasonably large coreference corpus is a labor-consuming task. To our knowledge, there have only been two Chinese coreference systems in previously pub-lished work: Florian et al (2004), which presents a statistical framework and reports experiment results on Chinese texts; and Zhou et al (2005), which proposed a unified transformation based learning framework for Chinese entity detection and tracking. It consists of two models: the de-tection model locates possibly coreferring NPs 
and the tracking model links the coreference re-lations. This paper presents research performed on Chinese noun phrase coreference resolution.  Since there are no freely available Chinese coreference resources, we used an unsupervised method that partially borrows from Cardie and Wagstaff?s (1999) clustering-based technique, with features that are specially designed for Chi-nese. In addition, we perform and present the results of experiments designed to investigate the contribution of each feature. 2 Experiment Setup Identifying coreferent NPs in an unannotated document actually involves two tasks: mention detection, which identifies the anaphors and an-tecedents in a document, followed by noun phrase coreference resolution. In order to reduce the complexity of the final system, we follow the usual approach in handling these two phases separately.  2.1 Corpus Even though we are using an unsupervised ap-proach, a gold standard corpus is still needed for experiment evaluation. Since we did not have access to the ACE multilingual entity tracking corpus, we created our own corpus by selecting 30 documents from the TDT3 Chinese corpus. This resulted in a corpus of approximately 36K Chinese characters, about the same size as the MUC dryrun test sets. We then had our corpus annotated by a native Chinese speaker following the MUC-7 (Hirschman and Chinchor, 1997) and ACE Chinese entity guidelines (LDC, 2004) by picking out noun phrase mentions corresponding to one of the following nine types of entities: Person, Organization, Location, Geo-Political Entity (GPE), Facility, Vehicle, Weapon, Date and Money, and for each pair of mentions, decid-ing whether they refer to the same entity follow-ing MUC-7 definitions.  According to the guide-lines, each mention participates in exactly one entity, and all mentions in the same entity are coreferent. The NPs that are marked include proper nouns, nominal nouns and pronouns and the entity types are a superset of those used in the MUC and ACE competitions. The resulting cor-pus includes 1640 mentions, referring to 410 en-tities.  Once our corpus had been determined, the first step was to determine the possible mentions in a plain text. We first used a dictionary-based word 
41
segmentation system (Lancashire, 2005) to seg-ment the Chinese characters into words. The segmented words are then labeled with POS tags by a statistical POS tagging system (Fung et al, 2004).   3 Mention Detection After the corpus has been preprocessed, mention detection involves the identification of NPs in the corpus that refer to some entity. Most of these NPs correspond to non-recursive NPs, which makes this task simpler as most syntactic parsers identify NPs as part of the parsing proc-ess. This approach, however, suffers from two problems: firstly, the parser itself is unlikely to be 100% accurate; and secondly, the boundaries of the NPs identified by the parser may not cor-respond exactly with those of the entities identi-fied by the human annotator. Another approach is simply to use heuristics based on the POS tag sequence to identify poten-tial NPs of interest. The advantage of this method is that the NPs thus extracted should be closer to the human-annotated entities since the heuristics will be constructed specifically for this task. To investigate the effect of different ap-proaches on the result of the coreference resolu-tion, we applied both methods separately to our corpus. The corpus was parsed with a state-of-the-art multilingual statistical parser (Bikel 2004), which is trained on the Chinese Penn Treebank. After parsing, we extracted all non-recursive NP chunks tagged by the parser as pos-sible mentions.  For the heuristic-based approach, we applied a few simple heuristics, which had been previ-ously developed during unrelated work for Eng-lish named-entity resolution (i.e. they were not written with foreknowledge of the gold standard entities) and which are based on the part-of-speech tags of the words.  Some examples of our heuristics were to look for pronouns, or to extract all noun sequences, or sequences of determiners followed by adjectives and nouns.  Table 1 shows the performance of the pars-ing-based approach versus the heuristic-based approach. The parser-based approach suffers 
mainly because the NPs that it extracts tend to be on the long side, resulting in recall errors when the boundaries of the parser-identified NPs mis-match with the human-annotated entities. In ad-dition, the parser also tends to extract more NPs than needed, which results in a hit to precision. 4 Coreference Resolution The final step after the mention detection phase is to determine which of the extracted phrases refer to the same entity, or are coreferent.  The small size of our corpus made it quite ob-vious that we would not be able to perform su-pervised learning, as there would not be enough data for generalization purposes.  Therefore we chose to use an unsupervised clustering approach for this step.  Clustering is a natural choice as it partitions the data into groups; used on corefer-ence resolution, we expect to gather coreferrent NPs into the same cluster.  Furthermore, most clustering methods can easily incorporate both context-dependent and independent constraints into their features. 4.1 Features Our features use both lexical and syntactic in-formation designed to capture both the content of the phrase and its role within the sentence. With the exception of the last three features, which are defined with respect to a noun phrase pair, all our features describe various aspects of a single noun phrase:  Lexical String ? This is just simply the string of words in the phrase. Head Noun ? The head noun in a phrase is the noun that is not a modifier for another noun. Sentence Position ? This measures the position of the phrase within the document. Gender ? For each phrase, we use a gazetteer to assign it a gender. The possible values are male (e.g. ??, mister), female (e.g. ??, miss), ei-ther (e.g. ??, leader) and neither (e.g. ??, factory).  Number ? A phrase can be either singular (e.g. ???, one cat), plural (e.g. ???, two dogs), either (e.g. ??, product) or neither (e.g. ??, safety). 
 Recall Precision F-Measure Heuristics 83 59.3 69.2 Parser-Based 62.7 28.7 39.4 Table 1: Mention Detection Results  
42
Semantic Class ? To give the system more in-formation on each phrase, we generated our own gazetteer from a combination of gazetteers com-piled from web sources and heuristics.  Our gaz-etteer consists of 4700 entries, each of which is labeled with one of the following semantic classes: person, organization, location, facility, GPE, date, money, vehicle and weapon. Phrases in the corpus that are found in the gazetteer are given the same semantic class label; phrases not in the gazetteer are marked as UNKNOWN.  Proper Name ? The part-of-speech tag ?NR? and a list of common proper names were used to label each noun phrase as to whether it is a proper name (values: true/false). Pronoun ? Determined by the part-of-speech ?PN?. Values: true/false.  Demonstrative Noun Phrase ? A demonstrative noun phrase is a phrase that consists of a noun phrases preceded by one of the characters [???] (this/that/some).  Appositive ? Two noun phrases are in apposition when the first phrase is headed by a common noun while the second one is a proper name with no space or punctuation between them. e.g. [?
??? ][??? ]????????? ([US president] [Clinton] visited Pyongyang last week.) This differs from English where two nouns are considered to be in apposition when one of them is an anaphor and separated by a comma from the other phrase, which is the most immediate proper name. (e.g. ?Bill Gates, the chairman of Microsoft Corp?) Abbreviation ? A noun phrase is an abbrevia-tion when it is formed by using part of another noun phrase, e.g. ??????? (Pyongyang Central Communications Office) is commonly abbreviated as ???. Since name abbreviations in Chinese are often given in an ad-hoc manner, it would be infeasible to generate a list of nam and abbreviations in advance. We therefore use the following heuristic: given two phrases, we test if one is an abbreviation of another by ex-tracting each successive character from the shorter phrase and testing to see if it is included in the corresponding word from the longer phrase. Intuitively, we know that this is a com-mon way of abbreviating terms; empirically, it usually gives us a correct result. Edit Distance ? Abbreviations and nicknames Feature f Function Noun Phrase Match -1 if the string of NPi matches the string of NPj; else 0 Head Noun Match -1 if head noun of NPi matches the head noun of NPj; else 0 
Sentence Distance 0 if NPi and NPj are in the same sentence;  For non-pronouns: 1/10 if they are one sentence apart; and so on with maximum value 1;  For pronouns: if more than two sentences apart, then 1 Gender Agreement 1 if they do not match in gender; else 0 Number Agreement 1 if they do not match in number; else 0 Semantic Agreement 1 if they do not match in semantic class or unknown; else 0 
Proper Name Agreement 1 if both are proper names, but mismatch on every word; else 0 
Pronoun Agreement 1 if either NPi or NPj is pronoun and mismatch in gender or number; else 0 Demonstrative Noun Phrase -1 if NPi is demonstrative and NPi contains NPj; else 0 Appositive -1 if NPi and NPj are in an appositive relationship; else 0 Abbreviation -1 if NPi and NPj are in an abbreviative relationship; else 0 Edit Distance 0 if NPi and NPj are the same, 1/(length of longer string) if one edit is needed to transform one to another, and so on. Table 2: Features and functions used in clustering algorithm  
43
are very commonly used in Chinese and even though the previous feature will work on most of them, there are some common exceptions. To make sure that we catch those as well, we intro-duced a Chinese-specific feature as a further test. Since abbreviations and nicknames are not usu-ally substrings of the original strings, but will still share some common characters, we measure the Levenshtein distance, defined as the number of character insertions, deletions and substitu-tions, between every potential antecedent-anaphor pair. 4.2 Distance Metric In order for the clustering algorithm to be able to group instances together by similarity, we need to determine a distance metric between two in-stances ? in our case, two noun phrases. For our system, we borrowed a simple distance metric from Cardie and Wagstaff (1999) that sums up the results of a series of functions over the two phrases: 
( , ) ( , )
i j f i j
f F
dist NP NP function NP NP
?
=
?
 Table 2 presents the features and the correspond-ing functions that were used in our system. Each function calculates a distance between the two phrases that is an indicator of the degree of in-compatibility between the two phrases with re-spect to a particular feature. The NOUN PHRASE, HEAD NOUN, DEMONSTRATIVE, APPOSITIVE and ABBREVIATIVE functions test for compatibility and return a negative value when the two phrases are compatible for that term?s feature. The reason for the negative value returned is that if the two phrases match on this particular feature, then it is a strong indicator of coreference. Therefore, we reduce the distance between two phrases, making it more likely that they will be clustered together into the same en-tity.  When there is a mismatch, however, it does not necessarily indicate that the two NPs are non-coreferential, so we leave the distance between the NPs unchanged. Conversely, there are some features where a mismatch would indicate that the two NPs are absolutely non-compatible and will definitely not refer to the same entity.  The DISTANCE, GENDER, NUMBER, SEMANTIC, PROPER NAME, PRONOUN and EDIT_DISTANCE functions return a positive value when the two phrases mismatch on that particular feature. A positive value results in a greater distance be-tween two phrases, which makes it less likely for them to be clustered together.  
4.3 Clustering Algorithm Most of the previous work in clustering-based noun phrase coreference resolution has centered around the use of bottom-up clustering methods, where each noun phrase is initially assigned to a singleton cluster by itself, and clusters which are ?close enough? to each other are merged (Cardie & Wagstaff, 1999; Angheluta et al, 2004). In our system, we use a method called modi-fied k-means clustering (Wilpon & Rabiner 1985), which takes the opposite approach and uses a top-down approach to split clusters, inter-leaved with a k-means iterative phase.  Modified k-means clustering has been successfully applied to speech recognition and it has the advantage of always being able to come to the optimal cluster-ing (i.e. it is not dependent upon the starting state or merging order).   Modified k-means starts off with all the in-stances in one big cluster.  The system then itera-tively performs the following steps: 1. For each cluster, find its centroid, de-fined as the instance which is the closest to all other instances in the same cluster. 2. For each instance: a. Calculate its distance to all the centroids.   b. Find the centroid with the mini-mum distance, and join its clus-ter. 3. Iterate 1-2 until instances stop moving between clusters. 4. Find the cluster with the largest intra-cluster distance.  (Call this Clustermax and its centroid, Centroidmax.) If this distance is smaller than some threshold r, stop. 5. From the instances inside Clustermax, find the pair that are the furthest apart from each other. a. Add the pair of instances to the list of centroids and remove Centroidmax from the list.  b. Repeat from Step 2. The algorithm thus alternates traditional k-means clustering with a step that adds new clusters to the pool of existing ones.  Used for coreference resolution, it splits up the instances into clusters in which the instances are more similar to each other than to instances in other clusters.  The only thing left to do is to determine a suit-able threshold. As functions that check for com-patibility return negative values while positive distances indicate incompatibility, a threshold of 0 would separate compatible and incompatible 
44
elements.  However, since the feature extraction will not be totally accurate, (especially for the GENDER and NUMBER features which test for incompatibility) we chose to be more lenient with deciding whether two phrases should be clustered together, and used a threshold of r = 1 to allow for possible errors. 5 Evaluation Evaluation of coreference resolution systems has traditionally been performed with precision and recall.  The MUC competition defines recall as follows (Vilain et al, 1995): 
(| | | ( ) |)
(| | 1)
i i
i
C p C
R
C
?
=
?
?
?
 
Each Ci is a gold standard cluster (i.e. a set of phrases which we know refer to the same entity), and p(Ci) is the partitioning of Ci by the auto-matically-generated clusters. For precision, the role of the automatic and gold standard clusters are reversed. Our results were evaluated using the MUC scoring program which reports recall, precision and F-measure, where the F-measure is defined as the harmonic mean of precision and recall: 
? 
F =
2PR
P + R
 Table 3 presents the results of our coreference resolution system on the outputs of both the pars-ing-based and heuristic-based entity detection systems, as measured by the MUC-7 scoring program. For the purposes of comparison, we also present results of our clustering algorithm on the gold standard entities.  This gives us a sense of the upper bound that we could poten-tially achieve if we got 100% accuracy on our mention detection phase.  An additional baseline is generated by implementing a system that as-sumes that all phrases refer to the same entity ? i.e. it takes all the heuristically-generated phrases and puts them into one big cluster. This gives us an upper bound on the recall of the system. Yet another baseline, to see how easy the task is, is to merge mentions together if the ?Noun Phrase Match? function tests true.   
From the results, it can be seen that our system achieves a performance gain of over 10 F-Measure points over the simplest baseline, and over 8 F-Measure points over the more sophisti-cated baseline. Unfortunately, due to corpus dif-ferences, we cannot conduct a comparison with results found in previous work. An interesting observation is the fact that the heuristic-based entity recognizer achieves better performance than the one based on statistical parsing.  The parser is trained on the Chinese Penn Treebank, which tends to have relatively longer noun phrases, and as result, the phrases generated by the parser also tend to be on the long side. This causes errors at the entity recog-nition phase, which results in a performance hit for the overall system.  6 Analysis One interesting question to ask about the results is the contribution of any given individual fea-ture to the result of the overall system. We have already investigated the effect of entity recogni-tion, and in this section, we take a look at the features for the clustering algorithm. Error! Reference source not found. presents the results of a series of experiments in which one feature at a time was removed from the clustering algo-rithm. The last entry in the table shows the re-sults of the full system; the drop in performance when a feature is removed is indicative of its contribution. Judging from the results, the 3 fea-tures that contribute the most to performance are the NOUN PHRASE MATCH, SEMANTIC AGREEMENT and EDIT DISTANCE features. Two out of the three, NOUN PHRASE and EDIT DISTANCE, operate on lexical informa-tion.  The importance of string matching to coreference resolution is consistent with findings in previous work (Yang et al 2004), which ar-rived at the same conclusion for English.  In addition, we note that the two Chinese-specific features that were introduced, ABBRE-VIATION and EDIT DISTANCE, both contrib-ute significantly (as measured by a student?s t-test) to the performance of the final system. 
 Recall Precision F-Measure Gold Standard Entities 78 88.5 82.9 Baseline (Heuristic-based Entities) 80.9 44.1 57.1 Baseline (Noun Phrase Match Only) 50.9 77.2 61.3 Heuristic-Based Entity Recognition 62.9 77.1 69.3 Parsing-Based Entity Recognition 42.5 62.9 50.7 Table 3: Coreference Resolution Performance  
45
Of our features, those that contribute the least to the overall performance are the GENDER, NUMBER and DEMONSTRATIVE NOUN PHRASE features. For DEMONSTRATIVE NOUN PHRASE, the reason is because of data sparsity ? there are just simply not enough ex-amples that it would make any significant im-pact.  For the GENDER and NUMBER features, we find that the problem is mostly with errors in feature generation. To our knowledge, this is the first published result on unsupervised Chinese coreference reso-lution. Due to differences in data, it is not possi-ble to conduct a comparison of our work with previous results.  7 Related Work Coreference resolution has attracted much atten-tion in recent years, especially as a result of the MUC and ACE competitions. The approaches taken have exhibited a shift from knowledge-based approaches to learning-based approaches. Many of the learning-based approaches recast coreference resolution as a binary classification task, which, given a pair of NPs, uses a trained classifier to determine whether they are corefer-ent. Soon et al (2001) used this approach with a 12-feature decision tree-based classifier and Ng and Cardie (2002) extended this approach with extra machine learning frameworks and a larger set of features. Yang et al (2004) extended this approach into an NP-cluster based approach, which considers the relationships between phrases and coreferential clusters.  In addition, several unsupervised approaches have been proposed. Cardie and Wagstaff (1999) re-cast the problem as a clustering task which applied a set of incompatibility functions and 
weights in the distance metric. Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.  All of the previously mentioned work has been for English. There has been relatively little work in Chinese: Florian et al (2004) provides results using a language-independent framework on the Entity Detection and Tracking task (EDT). They formulate the detection subtask as a classification problem using a Robust Risk Minimization clas-sifier combined with a Maximum Entropy classi-fier. Their system performs significantly well on English, Chinese and Arabic, however, the sys-tem suffers from small amount of training data (90K characters for Chinese, in contrast with 340K words for English). Their system obtained an ACE value of 58.8 on the ACE evaluation data on Chinese.  Finally, Zhou et al (2005) pro-posed a unified Transformation-Based Learning framework on Chinese EDT. The TBL tracking model looks at pairs of NPs at a time and classi-fies them as being coreferent or not based on the values of six features. They report an ACE score of 63.3 on their dataset. 8 Conclusions and Future Work In this paper, we have presented an unsupervised approach to Chinese coreference resolution. Our approach performs resolution by clustering, with the advantage that no annotated training data is needed. We evaluated our approach using a cor-pus which we developed using standard annota-tion schemes, and find that our system achieves an error reduction rate of almost 30% over the baseline. We also analyze the performance of our system by investigating the contribution of indi-vidual features to our system. The analysis illus-
Removed feature Recall Precision F-measure Noun Phrase Match 59.8 75.9 66.9 Head Noun Match 60.4 76.2 67.4 Sentence Distance 63.2 73.3 67.8 Gender Agreement 62.9 76.3 68.9 Number Agreement 63.2 75.9 69 Semantic Agreement 60.5 73 66.2 Proper Name Agreement 63 76.2 69 Pronoun Agreement 61.3 76.9 68.2 Demonstrative Noun Phrase 62.2 77.9 69.2 Appositive 60.1 76.9 67.5 Abbreviation 61.6 77 68.4 Edit Distance 62.4 72.8 67.2 None (All Features) 62.9 77.1 69.3 Table 4: Contribution of individual features to overall performance.  
46
trates the contribution of the new language-specific features. While the results produced by our system are impressive, it should be noted that all our fea-tures consider only the mention phrase itself. We consider this to be a rather simplistic and incom-plete. In future work, we plan to investigate the use of more sophisticated features, including contextual features, to improve the performance of our system. References ANGHELUTA R., JEUNIAUX P., RUDRADEB M., MOENS M.F. 2004. Clustering Algorithms for Noun Phrase Coreference Resolution. Proceedings of the 7th International Conference on the Statisti-cal Analysis of Textual Data. BEAN, D. and RILOFF, E. 2004. Unsupervised learn-ing of contextual role knowledge for coreference resolution. In Proc. of HLT/NAACL, pages 297?304. BIKEL, D. M. 2004. A Distributional Analysis of a Lexicalized Statistical Parsing Model. In Proceed-ings of EMNLP, Barcelona CARDIE, C. and WAGSTAFF, K. 1999. Noun phrase coreference as clustering. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 82-89. FLORIAN, R., HASSAN, H., ITTYCHERIAH, A., JING, H., KAMBHATLA, N., LUO, X., NICOLOV, N., and ROUKOS, S. 2004. Statistical Model for Multilingual Entity Detection and Tracking. In Proceedings of 2004 annual meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004). FUNG, P., NGAI, G., YANG, Y. S., and CHEN, B.F. 2004. A maximum-entropy Chinese parser aug-mented by Transformation-Based Learning. ACM Transactions on Asian Language Information Processing (TALIP), 3(2), pp 159-168. GAO J.F., LI M. and HUANG C.N. 2003. Improved souce-channel model for Chinese wordsegmenta-tion. In Proc. of ACL2003. HARABAGIU, S., BUNESCU, R.,and MAIORANO, S. 2001. Text and Knowledge Mining for Corefer-ence Resolution, in Proceedings of the 2nd Meet-ing of the North American Chapter of the Associa-tion of Computational Linguistics (NAACL-2001). HIRSCHMAN, L. and CHINCHOR, N. 1997. MUC7 Coreference Task Definition, http://www.itl.nist.gov/iaui/894.02/related_projects/muc/proceedings/co_task.html. 
LANCASHIRE, D. 2005. Adsotrans Chinese-English annotation. http://www.adsotrans.com/. LDC. 2004. Chinese Annotation Guidelines for Entity Detection and Tracking. http://www.ldc.upenn.edu/Projects/ACE/Data. MUC-7. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7). Morgan Kaufmann, San Francisco, CA. NG V. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL), 2005. NG, V. and CARDIE, C. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40rd Annual Meeting of the As-sociation for Computational Linguistics, Pages 104-111. NG V. and CARDIE C. 2003. Bootstrapping Corefer-ence Classifiers with Multiple Machine Learning Algorithms. Proceedings of the 2003 Conference on Empirical Methods in Natural Language Proc-essing (EMNLP-2003), Association for Computa-tional Linguistics, 2003. SOON, W., NG, H., and LIM, D. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521-544. VILAIN, M., BURGER, J., ABERDEEN, J., CON-NOLLY, D., and HIRSCHMAN, L. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 45?52, San Francisco, CA. Morgan Kaufmann. WILPON, J., AND RABINER, L. 1985. A modified K-means clustering algorithm for use in isolated word recognition. In IEEE Transactions on Acous-tics, Speech, Signal Processing. ASSP-33(3), 587-594. YANG, X., ZHOU, G., SU, J., and TAN, C. L. 2004. An NP-Cluster Based Approach to Coreference Resolution. Proceedings of the 20th International Conference on Computational Linguistics (COL-ING2004). ZHOU Y., HUANG C., GAO J., WU L. 2005. Trans-formation Based Chinese Entity Detection and Tracking. Proceedings of the Second International Joint Conference on Natural Language Processing. 
47
Inducing Multilingual Text Analysis Tools
via Robust Projection across Aligned Corpora
David Yarowsky
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
yarowsky@cs.jhu.edu
Grace Ngai
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
gyn@cs.jhu.edu
Richard Wicentowski
Dept. of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
richardw@cs.jhu.edu
ABSTRACT
This paper describes a system and set of algorithms for automati-
cally inducing stand-alone monolingual part-of-speech taggers, base
noun-phrase bracketers, named-entity taggers and morphological
analyzers for an arbitrary foreign language. Case studies include
French, Chinese, Czech and Spanish.
Existing text analysis tools for English are applied to bilingual
text corpora and their output projected onto the second language
via statistically derived word alignments. Simple direct annotation
projection is quite noisy, however, even with optimal alignments.
Thus this paper presents noise-robust tagger, bracketer and lemma-
tizer training procedures capable of accurate system bootstrapping
from noisy and incomplete initial projections.
Performance of the induced stand-alone part-of-speech tagger
applied to French achieves 96% core part-of-speech (POS) tag ac-
curacy, and the corresponding induced noun-phrase bracketer ex-
ceeds 91% F-measure. The induced morphological analyzer achie-
ves over 99% lemmatization accuracy on the complete French ver-
bal system.
This achievement is particularly noteworthy in that it required
absolutely no hand-annotated training data in the given language,
and virtually no language-specific knowledge or resources beyond
raw text. Performance also significantly exceeds that obtained by
direct annotation projection.
Keywords
multilingual, text analysis, part-of-speech tagging, noun phrase brac-
keting, named entity, morphology, lemmatization, parallel corpora
1. TASK OVERVIEW
A fundamental roadblock to developing statistical taggers, brack-
eters and other analyzers for many of the world?s 200+ major lan-
guages is the shortage or absence of annotated training data for the
large majority of these languages. Ideally, one would like to lever-
.
[][ ]
IN NNP NNP VBG VBG NNS NNSJJ JJ JJ
HongIn
Kong
national law(s)implementing of
109876543210
National   laws  applying  in  Hong  Kong
2 4 53
[ [] ]JJ VBG IN NNP NNP
0 1
NNS
un  producteur  important   de   petrole  brut 
DT[
NN
20
NN ]a  significant  producer    for    crude  oil[ ]
22 23
NNJJIN
21
JJ
1918
12
JJJJ NN
13 14 151110[ [ ]]
DT
[PLACE]
[PLACE]
Annotations From Existing English Tools
Annotations From Existing English Tools
Induced Annotations for Chinese
Induced Annotations for French
Figure 1: Projecting part-of-speech tags, named-entity tags and
noun-phrase structure from English to Chinese and French.
            	 
 
    

     	 


croyaient croissant croire croitre
believe growing growbelievingbelieved
French RootsFrench Inflections
BELIEVE
   
GROW
English Bridge Lemmas
V
   


  
 
Figure 2: French morphological analysis via English
age the large existing investments in annotated data and tools for
resource-rich languages (such as English and Japanese) to over-
come the annotated resource shortage in other languages.
To show the broad potential of our approach and methods, this
paper will investigate four fundamental language analysis tasks:
POS tagging, base noun phrase (baseNP) bracketing, named en-
tity tagging, and inflectional morphological analysis, as illustrated
in Figures 1 and 2. These bedrock tools are important components
of the language analysis pipelines for many applications, and their
low cost extension to new languages, as described here, can serve
as a broadly useful enabling resource.
2. BACKGROUND
Previous research on the word alignment of parallel corpora has
tended to focus on their use in translation model training for MT
rather than on monolingual applications. One exception is bilin-
gual parsing. Wu (1995, 1997) investigated the use of concurrent
parsing of parallel corpora in a transduction inversion framework,
helping to resolve attachment ambiguities in one language by the
coupled parsing state in the second language. Jones and Havrilla
(1998) utilized similar joint parsing techniques (twisted-pair gram-
mars) for word reordering in target language generation.
However, with these exceptions in the field of parsing, to our
knowledge no one has previously used linguistic annotation pro-
jection via aligned bilingual corpora to induce traditional stand-
alone monolingual text analyzers in other languages. Thus both
our proposed projection and induction methods, and their applica-
tion to multilingual POS tagging, named-entity classification and
morphological analysis induction, appears to be highly novel.
3. DATA RESOURCES
The data sets used in these experiments included the English-
French Canadian Hansards, the English-Chinese Hong Kong Han-
sards, and parallel Czech-English Reader?s Digest collection. In
addition, multiple versions of the Bible were used, including the
French Douay-Rheims Bible, Spanish Reina Valera Bible, and three
English Bible Versions (King James, New International and Re-
vised Standard), automatically verse-aligned in multiple pairings.
All corpora were automatically word-aligned by the now publicly
available EGYPT system (Al-Onaizan et al, 1999), based on IBM?s
Model 3 statistical MT formalism (Brown et al, 1990). The tag-
ging and bracketing tasks utilized approximately 2 million words
in each language, with the sample sizes for morphology induc-
tion given in Table 3. All word alignments utilized strictly raw-
word-based model variants for English/French/Spanish/Czech and
character-based model variants for Chinese, with no use of mor-
phological analysis or stemming, POS-tagging, bracketing or dic-
tionary resources.
4. PART-OF-SPEECHTAGGER INDUCTION
Part-of-speech tagging is the first of four applications covered in
this paper. The goal of this work is to project POS analysis capabil-
ities from one language to another via word-aligned parallel bilin-
gual corpora. To do so, we use an existing POS tagger (e.g. Brill,
1995) to annotate the English side of the parallel corpus. Then,
as illustrated in Figure 1 for Chinese and French, the raw tags are
transferred via the word alignments, yielding an extremely noisy
initial training set for the 2nd language. The third crucial step is to
generalize from these noisy projected annotations in a robust way,
yielding a stand-alone POS tagger for the new language that is con-
siderably more accurate than the initial projected tags.
Additional details of this algorithm are given in Yarowsky and
Ngai (2001). Due to lack of space, the following sections will serve
primarily as an overview of the algorithm and its salient issues.
4.1 Part-of-speech Projection Issues
First, because of considerable cross-language differences in fine-
grained tag set inventories, this work focuses on accurately assign-
ing core POS categories (e.g. noun, verb, adverb, adjective, etc.),
with additional distinctions in verb tense, noun number and pro-
noun type as captured in the English tagset inventory. Although
impoverished relative to some languages, and incapable of resolv-
ing details such as grammatical gender, this Brown-corpus-based
tagset granularity is sufficient for many applications. Furthermore,
many finer-grained part-of-speech distinctions are resolved primar-
ily by morphology, as handled in Section 7. Finally, if one desires
to induce a finer-grained tagging capability for case, for example,
one should project from a reference language such as Czech, where
case is lexically marked.
Figure 3 illustrates six scenarios encountered when projecting
POS tags from English to a language such as French. The first
two show straightforward 1-to-1 projections, which are encoun-
tered in roughly two-thirds of English words. Phrasal (1-to-N)
alignments offer greater challenges, as typically only a subset of
the aligned words accept the English tag. To distinguish these
cases, we initially assign position-sensitive phrasal parts-of-speech
via subscripting (e.g. Les/NNS  lois/NNS  ), and subsequently learn
a probablistic mapping to core, non-phrasal parts of speech (e.g.
P  DT  NNS  ) that is used along with tag sequence and lexical prior
models to re-tag these phrasal POS projections.
French
Induced Tags NN
O    ... salon ...Les lois ...
DT NNS
Tagger Output
English ... living room ...
VBG NN
The laws ...
DT NNS
... veterans ...
... anciens combattants ...
NNS NNS
NNS
(JJ) (NNS)
... potatoes ...
... pommes   de     terre ...
NNS
NNS NNSNNS
(IN) (NN)(NNS)
Tagger Output
English
French
Induced Tag
Correct Tag
Les   lois ...
NNS
(DT) (NNS)
O    Laws ...
NNS
O
Les     lois ...
Laws ...
NNS NNS
(DT) (NNS)
NNS
b a ba bc a
Figure 3: French POS tag projection scenarios
4.2 Noise-robust POS Tagger Training
Even at the relatively low tagset granularity of English, direct
projection of core POS tags onto French achieves only 76% ac-
curacy using EGYPT?s automatic word alignments (as shown in
Table 1). Part of this deficiency is due to word-alignment error;
when word alignments were manually corrected, direct projection
core-tag accuracy increased to 85%. Also, standard bigram taggers
trained on the automatically projected data achieve only modest
success at generalization (86% when reapplied to the noisy train-
ing data). More highly lexicalized learning algorithms exhibit even
greater potential for overmodeling the specific projection errors of
this data.
Thus our research has focused on noise-robust techniques for
distilling a conservative but effective tagger from this challenging
raw projection data. In particular, we modify standard n-gram mod-
eling to separate the training of the tag sequence model   	

	Coaxing Confidences from an Old Friend: 
Probabilistic Classifications from Transformation Rule Lists 
Radu F lo r ian*  John  C.  Henderson  t Grace  Nga i*  
*Department of Computer  Science 
Johns Hopkins University 
Baltimore, MD 21218, USA 
{rf lorian,gyn}@cs.jhu.edu 
tThe MITRE Corporat ion  
202 Bur l ington Road 
Bedford,  MA 01730, USA 
jhndrsn@mitre .org 
Abst rac t  
Transformation-based l arning has been success- 
fully employed to solve many natural language 
processing problems. It has many positive fea- 
tures, but one drawback is that it does not provide 
estimates of class membership probabilities. 
In this paper, we present a novel method for 
obtaining class membership robabilities from a 
transformation-based rule list classifier. Three ex- 
periments are presented which measure the model- 
ing accuracy and cross-entropy ofthe probabilistic 
classifier on unseen data and the degree to which 
the output probabilities from the classifier can be 
used to estimate confidences in its classification 
decisions. 
The results of these experiments show that, for 
the task of text chunking 1, the estimates produced 
by this technique are more informative than those 
generated by a state-of-the-art decision tree. 
1 In t roduct ion  
In natural language processing, a great amount of 
work has gone into the development of machine 
learning algorithms which extract useful linguistic 
information from resources such as dictionaries, 
newswire feeds, manually annotated corpora and 
web pages. Most of the effective methods can 
be roughly divided into rule-based and proba- 
bilistic algorithms. In general, the rule-based 
methods have the advantage of capturing the 
necessary information in a small and concise set 
of rules. In part-of-speech tagging, for exam- 
ple, rule-based and probabilistic methods achieve 
comparable accuracies, but rule-based methods 
capture the knowledge in a hundred or so simple 
rules, while the probabilistic methods have a 
very high--dimensional parameter space (millions 
of parameters). 
One of the main advantages of probabilistic 
methods, on the other hand, is that they include a 
measure of uncertainty in their output. This can 
take the form of a probability distribution over 
potential outputs, or it may be a ranked list of 
IA11 the experiments are performed on text chnnklng. 
The technique presented is general-purpose, however, and 
can be applied to many tasks for which transformation- 
based learning performs well, without changing the inter- 
rials of the learner. 
candidate outputs. These uncertainty measures 
are useful in situations where both the classifi- 
cation of an sample and the system's confidence 
in that classification are needed. An example of 
this is a situation in an ensemble system where 
ensemble members disagree and a decision must 
be made about how to resolve the disagreement. 
A similar situation arises in pipeline systems, such 
as a system which performs parsing on the output 
of a probabilistic part-of-speech tagging. 
Transformation-based learning (TBL) (Brill, 
1995) is a successful rule-based machine learning 
algorithm in natural language processing. It has 
been applied to a wide variety of tasks, including 
part of speech tagging (Roche and Schabes, 1995; 
Brill, 1995), noun phrase chvnklng (Ramshaw and 
Marcus, 1999), parsing (Brill, 1996; Vilain and 
Day, 1996), spelling correction (Mangu and Brill, 
1997), prepositional phrase attachment (Brill and 
Resnik, 1994), dialog act tagging (Samuel et 
al., 1998), segmentation and message understand- 
ing (Day et al, 1997), often achieving state- 
of-the-art performance with a small and easily- 
understandable list of rules. 
In this paper, we describe a novel method 
which enables a transformation-based classifier to 
generate a probability distribution on the class 
labels. Application of the method allows the 
transformation rule list to retain the robustness of 
the transformation-based algorithms, while bene- 
fitting from the advantages ofa probabilistic clas- 
sifter. The usefulness of the resulting probabilities 
is demonstrated bycomparison with another state- 
of-the-art classifier, the C4.5 decision tree (Quin- 
lan, 1993). The performance of our algorithm 
compares favorably across many dimensions: it 
obtains better perplexity and cross-entropy; an 
active learning algorithm using our system outper- 
forms a similar algorithm using decision trees; and 
finally, our algorithm has better rejection curves 
than a similar decision tree. Section 2 presents the 
transformation based learning paradigm; Section 
3 describes the algorithm for construction of the 
decision tree associated with the transformation 
based list; Section 4 describes the experiments 
in detail and Section 5 concludes the paper and 
outlines the future work. 
26 
2 Trans format ion  ru le  l i s t s  
The central idea of transformation-based l arn- 
ing is to learn an ordered list of rules which 
progressively improve upon the current state of 
the training set. An initial assignment is made 
based on simple statistics, and then rules are 
greedily learned to correct he mistakes, until no 
net improvement can be made. 
These definitions and notation will be used 
throughout the paper: 
? X denotes the sample space; 
? C denotes the set of possible classifications of 
the samples; 
? The state space is defined as 8 = X x C. 
? 7r will usually denote a predicate defined on 
X; 
? A rule r is defined as a predicate - class label 
- time tuple, (~r,c,t), c E C,t E N, where t is 
the learning iteration in which when the rule 
was learned, its position in the list. 
? A rule r = (~r, c, t) applies to a state (z, y) if 
7r(z) = true and c # y. 
Using a TBL framework to solve a problem as- 
sumes the existence of: 
? An initial class assignment (mapping from X 
to ,.9). This can be as simple as the most 
common class label in the training set, or it 
can be the output from another classifier. 
? A set of allowable templates for rules. These 
templates determine the predicates the rules 
will test, and they have the biggest influence 
over the behavior of the system. 
? An objective function for learning. Unlike in 
many other learning algorithms, the objective 
function for TBL will typically optimize the 
evaluation function. An often-used method is 
the difference in performance resulting from 
applying the rule. 
At the beginning of the learning phase, the 
training set is first given an initial class assign- 
ment. The system then iteratively executes the 
following steps: 
1. Generate all productive rules. 
2. For each rule: 
(a) Apply to a copy of the most recent state 
of the training set. 
(b) Score the result using the objective func- 
tion. 
3. Select he rule with the best score. 
4. Apply the rule to the current state of the 
training set, updating it to reflect his change. 
5. Stop if the score is smaller than some pre-set 
threshold T. 
6. Repeat from Step 1. 
The system thus learns a list of rules in a greedy 
fashion, according to the objective function. When 
no rule that improves the current state of the 
training set beyond the pre-set threshold can 
be found, the training phase ends. During the 
evaluation phase, the evaluation set is initialized 
with the same initial class assignment. Each rule 
is then applied, in the order it was learned, to the 
evaluation set. The final classification is the one 
attained when all rules have been applied. 
3 Probab i l i ty  es t imat ion  w i th  
t rans format ion  ru le  l i s t s  
Rule lists are infamous for making hard decisions, 
decisions which adhere entirely to one possibility, 
excluding all others. These hard decisions are 
often accurate and outperform other types of 
classifiers in terms of exact-match accuracy, but 
because they do not have an associated proba- 
bility, they give no hint as to when they might 
fail. In contrast, probabilistic systems make soft 
decisions by assigning a probability distribution 
over all possible classes. 
There are many applications where soft deci- 
sions prove useful. In situations such as active 
learning, where a small number of samples are 
selected for annotation, the probabilities can be 
used to determine which examples the classifier 
was most unsure of, and hence should provide the 
most extra information. A probabilistic system 
can also act as a filter for a more expensive 
system or a human expert when it is permitted 
to reject samples. Soft decision-making is also 
useful when the system is one of the components 
in a larger decision-malting process, as is the case 
in speech recognition systems (Bald et al, 1989), 
or in an ensemble system like AdaBoost (Freund 
and Schapire, 1997). There are many other 
applications in which a probabilistic lassifier is 
necessary, and a non-probabHistic classifier cannot 
be used instead. 
3.1 Estimation via conversion to decision 
tree 
The method we propose to obtain probabilis- 
tic classifications from a transformation rule list 
involves dividing the samples into equivalence 
classes and computing distributions over each 
equivalence class. At any given point in time i, 
each sample z in the training set has an associated 
state si(z) = (z,~l). Let R (z )  to be the set of rules 
r~ that applies to the state el(z), 
R(z) = {ri ~ 7~Ir~ applies to si(z)} 
An  equivalence class consists of all the samples 
z that have the same R(z). Class probability 
assignments are then estimated using statistics 
computed on the equivalence classes. 
27 
An illustration of the conversion from a rule 
list to a decision tree is shown below. Table 1 
shows an example transformation rule list. It is 
straightforward to convert this rule list into a de- 
cision pylon (Bahl et al, 1989)~. which can be used 
to represent all the possible sequences of labels 
assigned to a sample during the application of the 
TBL  algorithm. The decision pylon associated 
with this particular rule list is displayed on the left 
side of Figure 1. The decision tree shown on the 
right side of Figure 1 is constructed such that the 
samples stored in any leaf have the same class label 
sequence as in the displayed decision pylon. In 
the decision pylon, "no" answers go straight down; 
in the decision tree, "yes" answers take the right 
branch. Note that a one rule in the transformation 
rule list can often correspond to more than one 
node in the decision tree. 
Initial label = A 
I f  Q1 and label=A then  label+-B 
I f  Q2 and label=A then  labele-B 
I f  Q3 and label=B then  label~A 
Table I: Example of a Transformation Rule List. 
Figure 1: Converting the transformation rule list 
from Table 1 to a decision tree, 
The conversion from a transformation rule list 
to a decision tree is presented as a recursive 
procedure. The set of samples in the training set 
is transformed to a set of states by applying the 
initial class assignments. A node n is created for 
each of the initial class label assignments c and all 
states labeled c are assigned to n. 
The following recursive procedure is invoked 
with an initial "root" node, the complete set of 
states (from the corpus) and the whole sequence 
of rules learned uring training: 
A lgor i thm:  Ru leL is tToDec is ionTree  
(RLTDT)  
Input :  
* A set/3 of N states ((Zl, Yl) --- (ZN, YN)) with 
labels Yi E C; 
? A set 7~ of M rules (ro,rl . . .rM) where ri = 
Do:  
1. If 7~ is empty, the end of the rule list has been 
reached. Create a leaf node, n, and estimate 
the probability class distribution based on the 
true classifications of the states in 13. Return 
n.  
2. Let rj  = (Ir j ,yj , j )  be the lowest-indexed rule 
in 7~. Remove it from 7~. 
3. Split the data in/3 using the predicate 7rj and 
the current hypothesis uch that samples on 
which 7rj returns true are on the right of the 
split: 
BL = {x E BlTrj(x ) = false} 
/3R = {x E/31 j(x) = true} 
4. If IBLI > K and IBRI > K,  the split is 
acceptable: 
(a) Create a new internal node, n; 
(b) Set the question: q(n) = 7rj; 
(c) Create the left child of n using a recursive 
call to RLTDT(BL, 7~); 
(d) Create the right child of n using a recur- 
sive call to RLTDT(BR, 7~); 
(e) Return node n. 
Otherwise, no split is performed using rj.  
Repeat from Step 1. 
The parameter K is a constant that determines the 
minimum weight that a leaf is permitted to have, 
effectively pruning the tree during construction. 
In all the experiments, K was set to 5. 
3.2 Fur ther  growth  o f  the  decis ion t ree  
When a rule list is converted into a decision tree, 
there are often leaves that are inordinately heavy 
because they contain a large number of samples. 
Examples of such leaves are those containing 
samples which were never transformed by any 
of the rules in the rule list. These populations 
exist either because they could not be split up 
during the rule list learning without incurring a 
net penalty, or because any rule that acts on them 
has an objective function score of less than the 
threshold T. This is sub-optimal for estimation 
because when a large portion of the corpus falls 
into the same equivalence class, the distribution 
assigned to it reflects only the mean of those 
samples. The undesirable consequence is that all 
of those samples are given the same probability 
distribution. 
To ameliorate this problem, those samples are 
partitioned into smaller equivalence classes by 
further growing the decision tree. Since a decision 
tree does not place all the samples with the same 
current label into a single equivalence class, it does 
not get stuck in the same situation as a rule list 
m in which no change in the current state of 
corpus can be made without incurring a net loss 
in performance. 
28 
Continuing to grow the decision tree that was 
converted from a rule list can be viewed from 
another angle. A highly accurate prefix tree 
for the final decision tree is created by tying 
questions together during the first phase of the 
growth process (TBL). Unlike traditional decision 
trees which select splitting questions for a node 
by looking only at the samples contained in the 
local node, this decision tree selects questions by 
looking at samples contained in all nodes on the 
frontier whose paths have a suM< in common. An 
illustration of this phenomenon can be seen in 
Figure 1, where the choice to split on Question 
3 was made from samples which tested false 
on the predicate of Question 1, together with 
samples which tested false on the predicate of 
Question 2. The result of this is that questions 
are chosen based on a much larger population than 
in standard decision tree growth, and therefore 
have a much greater chance of being useful and 
generalizable. This alleviates the problem of over- 
partitioning of data, which is a widely-recognized 
concern during decision tree growth. 
The decision tree obtained from this conversion 
can be grown further. When the rule list 7~ is 
exhausted at Step 1, instead of creating a leaf 
node, continue splitting the samples contained in 
the node with a decision tree induction algorithm. 
The splitting criterion used in the experiments is 
the information gain measure. 
4 Exper iments  
Three experiments that demonstrate the effec- 
tiveness and appropriateness of our probability 
estimates are presented in this section. The 
experiments are performed on text chunking, a 
subproblem ofsyntactic parsing. Unlike full pars- 
ing, the sentences are divided into non-overlapping 
phrases, where each word belongs to the lowest 
parse constituent that dominates it. 
The data used in all of these experiments i  
the CoNLL-2000 phrase chunking corpus (CoNLL, 
2000). The corpus consists of sections 15-18 and 
section 20 of the Penn Treebank (Marcus et al, 
1993), and is pre-divided into a 8936-sentence 
(211727 tokens) training set and a 2012-sentence 
(47377 tokens) test set. The chunk tags are 
derived from the parse tree constituents, and the 
part-of-speech tags were generated by the Brill 
tagger (Brill, 1995). 
As was noted by Ramshaw & Marcus (1999), 
text chunking can be mapped to a tagging task, 
where each word is tagged with a chunk tag 
representing the phrase that it belongs to. An 
example sentence from the corpus is shown in 
Table 4. As a contrasting system, our results 
are compared with those produced by a C4.5 
decision tree system (henceforth C4.5). The 
reason for using C4.5 is twofold: firstly, it is a 
widely-used algorithm which achieves state-.of-the- 
art performance on a broad variety of tasks; and 
Word 
A.P. 
Green 
currently 
has 
2,664,098 
shares 
outstanding 
POS tag 
NNP 
NNP 
RB 
VBZ 
CD 
NNS 
JJ 
Chunk Tag 
B-NP 
I-NP 
B-ADVP 
B-VP 
B-NP 
I-NP 
B-ADJP 
O 
Table 2: Example of a sentence with chunk tags 
secondly, it belongs to the same class of classifiers 
as our converted transformation-based rule list 
(henceforth TBLDT). 
To perform a fair evaluation, extra care was 
taken to ensure that both C4.5 and TBLDT 
explore as similar a sample space as possible. The 
systems were allowed to consult the word, the 
part-of-speech, and the chunk tag of all examples 
within a window of 5 positions (2 words on either 
side) of each target example. 2 Since multiple 
features covering the entire vocabulary of the 
training set would be too large a space for C4.5 
to deal with, in all of experiments where TBLDT 
is directly compared with C4.5, the word types 
that both systems can include in their predicates 
are restricted to the most "ambiguous" 100 words 
in the training set, as measured by the number of 
chunk tag types that are assigned to them. The 
initial prediction was made for both systems using 
a class assignment based solely on the part-of- 
speech tag of the word. 
Considering chunk tags within a contextual win- 
dow of the target word raises a problem with C4.5. 
A decision tree generally trains on independent 
samples and does not take into account changes 
of any features in the context. In our case, the 
samples are dependent; the classification ofsample 
i is a feature for sample i + 1, which means that 
changing the classification for sample i affects 
the context of sample i + 1. To address this 
problem, the C4.5 systems are trained with the 
correct chlmk~ in the left context. When the 
system is used for classification, input is processed 
in a left-to-right manner;and the output of the 
system is fed forward to be used as features 
in the left context of following samples. Since 
C4.5 generates probabilities for each classification 
decision, they can be redirected into the input for 
the next position. Providing the decision treewith 
this confidence information effectively allows it to 
perform a limited search over the entire sentence. 
C4.5 does have one advantage over TBLDT, 
however. A decision tree can be trained using the 
subsetting feature, where questions asked are of 
the form: "does feature f belong to the set FT'. 
This is not something that a TBL can do readily, 
2The TBL templates are similar to those used in 
l~am.~haw and Marcus (1999). 
29 
but since the objective is in comparing TBLDT to 
another state-of-the-art system, this feature was 
enabled. 
4.1 Evaluation Measures 
The most commonly used measure for evaluating 
tagging tasks is tag accuracy, lit is defined as 
Accuracy = # of correctly tagged examples 
of examples 
In syntactic parsing, though, since the task is 
to identify the phrasal components, it is more 
appropriate o measure the precision and recall: 
# of correct proposed phrases 
Precision = 
# of proposed phrases 
# of correct proposed phrases 
Recall = # of correct phrases 
To facilitate the comparison of systems with dif- 
ferent precision and recall, the F-measure metric 
is computed as a weighted harmonic mean of 
precision and recall: 
(82 + 1) ? Precision x Recall 
= 
82 x Precision + Recall 
The ~ parameter is used to give more weight to 
precision or recall, as the task at hand requires. 
In all our experiments, ~ is set to 1, giving equal 
weight o precision and recall. 
The reported performances are all measured 
with the evaluation tool provided with the CoNLL 
corpus (CoNLL, 2000). 
4.2 Active Learning 
To demonstrate the usefulness of obtaining proba- 
bilities from a transformation rule list, this section 
describes an application which utilizes these prob- 
abilities, and compare the resulting performance 
of the system with that achieved by C4.5. 
Natural language processing has traditionally 
required large amounts of annotated ata from 
which to extract linguistic properties. However, 
not all data is created equal: a normal distribu- 
tion of aunotated ata contains much redundant 
information. Seung et al (1992) and Freund et 
al. (1997) proposed a theoretical ctive learning 
approach, where samples are intelligently selected 
for annotation. By eliminating redundant infor- 
mation, the same performance can be achieved 
while using fewer resources. Empirically, active 
learning has been applied to various NLP tasks 
such as text categorization (Lewis and Gale, 1994; 
Lewis and Catlett, 1994; Liere and Tadepalli, 
1997), part-of-speech tagging (Dagan and Engel- 
son, 1995; Engelson and Dagan, 1996), and base 
noun phrase chunbiug (Ngai and Yarowsky, 2000), 
resulting in significantly large reductions in the 
quantity of data needed to achieve comparable 
performance. 
This section presents two experimental results 
which show the effectiveness of the probabilities 
generated by the TBLDT. The first experiment 
compares the performance achieved by the active 
learning algorithm using TBLDT with the perfor- 
mance obtained by selecting samples equentially 
from the training set. The second experiment 
compares the performances achieved by TBLDT 
and C4.5 training on samples elected by active 
learning. 
The following describes the active learning algo- 
rithm used in the experiments: 
1. Label an initial T1 sentences ofthe corpus; 
2. Use the machine learning algorithm (G4.5 or 
TBLDT) to obtain chunk probabilities on the 
rest of the training data; 
3. Choose T2 samples from the rest of the train- 
ing set, specifically the samples that optimize 
an evaluation function f ,  based on the class 
distribution probability of each sample; 
4. Add the samples, including their "true" classi- 
fication 3 to the training pool and retrain the 
system; 
5. If a desired number of samples is reached, 
stop, otherwise repeat from Step 2. 
The evaluation function f that was used in our 
experiments is:
where H(UIS, i ) is the entropy of the chllnk 
probability distribution associated with the word 
index i in sentence S. 
Figure 2 displays the performance (F-measure 
and chllnk accuracy) of a TBLDT system trained 
on samples elected by active learning and the 
same system trained on samples elected sequen- 
tially from the corpus versus the number of words 
in the annotated tralniug set. At each step of 
the iteration, the active learning-trained TBLDT 
system achieves a higher accuracy/F-measure, or, 
conversely, is able to obtain the same performance 
level with less training data. Overall, our system 
can yield the same performance as the sequential 
system with 45% less data, a significant reduction 
in the annotation effort. 
Figure 3 shows a comparison between two active 
learning experiments: one using TBLDT and the 
other using C4.5. 4 For completeness, a sequential 
run using C4.5 is also presented. Even though 
C4.5 examines a larger space than TBLDT by 
SThe true (reference or gold standard) classification is 
available in this experiment. In an annotation situation, 
the samples are sent o human annotators for labeling. 
4As mentioned arlier, both the TBLDT and C4.5 were 
limited to the same 100 most ambiguous words in the 
corpus to ensure comparability. 
3O 
84 
AL?TBLDT ' ~ ' ' ' 
i i i I i 
(a) F-measure vs. number of words in trrdniug set 
Oil 
AL* 'mI .~ ' - -  . . . .  
i I 
(b) Chunk Accuracy vs. number of words in training 
set 
Figure 2: Performance of the TBLDT system versus sequential choice. 
87 
86 
| -  
81 
...=2- ' 
, . r .~ . . . r  - - I~ ' ' ' '~ ' '~"  
\ [~"  
i 
I i L i 
(a) F-measure vs. number of words in tr~inln s set 
31 
AL?'nBL (I0? ~ )  ~ '  
J i i ~ i i 
(b) Accuracy vs. number of words in training set 
Figure 3: Performance of the TBLDT system versus the DT system 
utilizing the feature subset predicates, TBLDT 
still performs better. The difference in accuracy at 
26200 words (at the end of the active learning run 
for TBLDT) is statistically significant at a 0.0003 
level. 
As a final remark on this experiment, note that 
at an annotation level of 19000 words, the fully 
lexicalized TBLDT outperformed the C4.5 system 
by making 15% fewer errors. 
4.3 Re jec t ion  curves 
It is often very useful for a classifier to be able 
to offer confidence scores associated with its deci- 
sions. Confidence scores are associated with the 
probability P(C(z) correct\[z) where C(z) is the 
classification of sample z. These scores can be 
used in real-life problems to reject samples that 
the the classifier is not sure about, in which case 
a better observation, or a human decision, might 
be requested. The performance of the classifier 
is then evaluated on the samples that were not 
rejected. This experiment framework is well- 
established in machine learning and optimization 
research (Dietterich and Bakiri, 1995; Priebe et 
al., 1999). 
Since non-probabilistic classifiers do not offer 
any insights into how sure they are about a 
particular classification, it is not easy to obtain 
confidence scores from them. A probabilistic 
classifier, in contrast, offers information about the 
class probability distribution of a given sample. 
Two measures that can be used in generating 
confidence scores are proposed in this section. 
The first measure, the entropy H of the class 
probability distribution of a sample z, C(z) = 
{p(CllZ),p(c2\[z)...p(cklZ)}, i s  a measure  of the 
uncertainty in the distribution: 
k 
HCCCz)) = - I=) log2 pC Iz) 
i=I 
The higher the entropy of the distribution of 
class probability estimates, the more uncertain the 
0.99 
0.98 
0.97 
g~ 
0.~ 
~0.95  
0.94 
0.93 
///.._.-.--f" 
/ / C4.5 (hard d=fisions)__ i 
I / / / . -  ..... ~ % _ .. ~.; 
':" \] 
I I I I I I I I I 
0J O2 O3 0.4 0.5 O.6 0,7 O.8 O.9 Z 
l~c~t of rej~xaed ~
(a)  Subcorpus  (batch)  re jec t ion  
0~ 1 i i 1 
0.985 " 
0~8 
O.975 
097 
0.965 
0.96 - TBL-DT 
0.955 
095 - C4_5 (soft decisi 
0.945 ..- .... 
0.94 .-.:.-.-.r.-. r.~---.':.'.':.'. "
0.935 \[ 
0 0.2 0.4 0.6 ~8 1 
Probability of th~ most lflmly tag 
(b)  Thresho ld  (on l ine)  re jec t ion  
Figure 4: Rejection curves. 
classifier is of its classification. The samples e- 
lected for rejection are chosen by sorting the data 
using the entropies of the estimated probabilities, 
and then selecting the ones with highest entropies. 
The resulting curve is a measure of the correlation 
between the true probability distribution and the 
one given by the classifier. 
Figure 4(a) shows the rejection curves for the 
TBLDT system and two C4.5 decision trees - one 
which receives a probability distribution as input 
("soft" decisions on the left context) , and one 
which receives classifications ("hard" decisions on 
all fields). At the left of the curve, no samples 
are rejected; at the right side, only the samples 
about which the classifiers were most certain are 
kept (the samples with minimum entropy). Note 
that the y-values on the right side of the curve are 
based on less data, effectively introducing wider 
variance in the curve as it moves right. 
As shown in Figure 4(a), the C4.5 classifier 
that has access to the left context chunk tag 
probability distributions behaves better than the 
other C4.5 system, because this information about 
the surrounding context allows it to effectively 
perform a shallow search of the classification 
space. The TBLDT system, which also receives 
a probability distribution on the chunk tags in 
the left context, clearly outperforms both C4.5 
systems at all rejection levels. 
The second proposed measure is based on the 
probability of the most likely tag. The assumption 
here is that this probability is representative of 
how certain the system is about the classifica- 
tion. The samples are put in bins based on 
the probability of the most likely chnnk tag, and 
accuracies are computed for each bin (these bins 
are cumulative, meaning that a sample will be 
included in all the bins that have a lower threshold 
than the probability of its most likely chnnl? 
tag). At each accuracy level, a sample will be 
rejected if the probability of its most likely chnn~ 
Cross Entropy 
TBLDT 1.2944 0.2580 
DT+probs 1.4150 0.3471 
DT 1.4568 0.3763 
Table 3: Cross entropy and perplexities for two 
C4.5 systems and the TBLDT system 
is below the accuracy level. The resulting curve 
is a measure of the correlation between the true 
distribution probability and the probability of the 
most likely chunk tag, i.e. how appropriate those 
probabilities are as confidence measures. Unlike 
the first measure mentioned before, a threshold 
obtained using this measure can be used in an 
online manner to identify the samples of whose 
classification the system is confident. 
Figure 4(b) displays the rejection curve for 
the second measure and the same three systems. 
TBLDT again outperforms both C4.5 systems, at 
all levels of confidence. 
In summary, the TBLDT system outperforms 
both C4.5 systems presented, resulting in fewer re- 
jections for the same performance, or, conversely, 
better performance at the same rejection rate. 
4.4 Perp lex i ty  and  Cross Ent ropy  
Cross entropy is a goodness measure for probabil- 
ity estimates that takes into account he accuracy 
of the estimates as well as the classification accu- 
racy of the system. It measures the performance 
of a system trained on a set of samples distributed 
according to the probability distribution p when 
tested on a set following a probability distribution 
q. More specifically, we utilize conditional cross 
entropy, which is defined as 
n (C lX)  = - q (=) -  q(cl=) ? log2 pC@:) 
zEX ?EC 
where X is the set of examples and C is the set of 
chnnlr tags, q is the probabil i ty distribution on the 
32 
Chunk  
Type  
A c c u r a c y  
(%) 
Precisionl Recall 
(%) I (%) 
Overall 95.23 92.02 92.50 
ADJP  - 75.69 68.95 
ADVP - 80.88 78.64 
CONJP  - 40.00 44.44 
INTJ - 50.00 50.00 
LST - 0.00 0.00 
NP  - 92.18 92.72 
PP  95.89 97.90 
PRT  - 67.80 75.47 
SBAR 88.71 82.24 
VP 92.00 92.87 
Fi 
92.26! 
72.16 
79.74 
42.11 
50.00 
0.00 
92.45 
96.88 
71.43 
85.35 
92.44 
Table 4: Performance of TBLDT on the CoNLL 
Test Set 
test document and p is the probability distribution 
on the train corpus. 
The cross entropy metric fails if any outcome is 
given zero probability by the estimator. To avoid 
this problem, estimators are "smoothed", ensuring 
that novel events receive non-zero probabilities. 
A very simple smoothing technique (interpolation 
with a constant) was used for all of these systems. 
A closely related measure is perplexity, defined 
as 
P = 2~(cl x) 
The cross entropy and perplexity results for the 
various estimation schemes are presented in Table 
? 3. The TBLDT outperforms both C4.5 systems, 
obtaining better cross-entropy and chunk tag per- 
plexity. This shows that the overall probability 
distribution obtained from the TBLDT system 
better matches the true probability distribution. 
This strongly suggests hat probabilities generated 
this way can be used successfully in system com- 
bination techniques such as voting or boosting. 
4.5 Chunk ing  per formance 
It is worth noting that the transformation-based 
system used in the comparative graphs in Figure 
3 was not r, uning at full potential. As described 
earlier, the TBLDT system was only allowed to 
consider words that C4.5 had access to. However, 
a comparison between the corresponding TBLDT 
curves in Figures 2 (where the system is given 
access to all the words) and 3 show that a 
transformation-based system given access to all 
the words performs better than the one with a 
restricted lexicon, which in turn outperforms the 
best C4.5 decision tree system both in terms of 
accuracy and F-measure. 
Table 4 shows the performance of the TBLDT 
system on the full CoNLL  test set, broken down 
by chunk type. Even though the TBLDT results 
could not be compared with other published re- 
sults on the same task and data (CoNLL  will 
not take place until September 2000), our system 
significantly outperforms a similar system trained 
with a C4.5 decision tree, shown in Table 5, both 
in chunk accuracy and F-measure. 
Chunk 
Type 
Accuracy 
(%) 
ADVP 
CONJP  
IJrecision 
(%) 
Recall 
(%) 
Overall 93.80 90.02 90.26 
ADJP 65.58 64.38 
74.14 76.79 
33.33 
INTJ 50.00 50.00 
LST 0.00 0.00 
NP  91.00 90.93 
PP  92.70 96.36 
PRT  71.13 65.09 
SBAR 86.35 61.50 
VP  90.71 91.22 
I Fz 
90.14 
64.98 
75.44 
33.33 
50.00 
0.00 
90.96 
94.50 
67.98 
71.83 
90.97 
Table 5: Performance of C4.5 on the CoNLL Test 
Set 
5 Conclus ions 
In this paper we presented a novel way to convert 
transformation rule lists, a common paradigm in 
natural anguage processing, into a form that is 
equivalent in its classification behavior, but is 
capable of providing probability estimates. Using 
this approach, favorable properties of transfor- 
mation rule lists that makes them popular for 
language processing are retained, while the many 
advantages of a probabilistic system axe gained. 
To demonstrate he efficacy of this approach, 
the resulting probabilities were tested in three 
ways: directly measuring the modeling accuracy 
on the test set via cross entropy, testing the 
goodness of the output probabilities in a active 
learning algorithm, and observing the rejection 
curves attained from these probability estimates. 
The experiments clearly demonstrate that the 
resulting probabilities perform at least as well as 
the ones generated by C4.5 decision trees, resulting 
in better performance in all cases. This proves that 
the resulting probabilistic lassifier is as least as 
good as other state-of-the-art p obabilistic models. 
The positive results obtained suggest hat the 
probabilistic lassifier obtained from transforma- 
tion rule lists can be successfully used in machine 
learning algorithms that require soft-decision clas- 
sifters, such as boosting or voting. Future research 
will include testing the behavior of the system 
under AdaBoost (Freund and Schapire, 1997). We 
also intend to investigate the effects that other 
decision tree growth and smoothing techniques 
may have on continued refinement of the converted 
rule list. 
6 Acknowledgements  
We thank Eric Brill, Fred Jelinek and David 
Yaxowsky for their invaluable advice and sugges- 
tions. In addition we would like to thank David 
Day, Ben Weliner and the anonymous reviewers 
for their useful comments and suggestions on the 
paper . . . .  
The views expressed in this paper are those of 
the authors and do not necessarily reflect he views 
33 
of the MITRE Corporation. It  was performed 
as a collaborative ffort at \]both MITRE and 
the Center for Language and ',Speech Processing, 
Johns Hopkins University, Baltimore, MD. It was 
supported by NSF grants numbered IRI-9502312 
and IRI-9618874, as well as the MITRE-Sponsored 
Research program. 
References 
L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1989. 
A tree-based statistical language model for natural 
language speech recognition. IEEE Transactions on 
Acoustics, Speech and Signal Processing, 37:1001- 
1008. 
E. BriU and P. Resnik. 1994. A rule-based approach 
to prepositional phrase attachment disambiguation. 
In Proceedings of the Fifteenth International Con- 
ference on Computational Linguistics (COLING- 
199~), pages 1198--1204, Kyoto. 
E. BrllL 1995. Transformation-based rror-driven 
learning and natural language processing: A case 
study in part of speech tagging. Computational 
Linguistics, 21(4):543-565. 
E. Brill, 1996. Learning to Parse with Transforma- 
tions. In H. Bunt and M. Tomita (eds.) Recent 
Advances in Parsing Technology, Kluwer. 
CoNLL. 2000. Shared task for computational natu- 
ral language learning (CoNLL), 2000. http://lcg- 
ww w.uia.ac.be/conU2000/chunking. 
I. Dagan and S. Engelson. 1995. Committee-based 
sampling for training probabilistic lassifiers. In 
Proceedings ofInternational Conference on Machine 
Learning (ICML) 1995, pages 150-157. 
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, 
P. Robinson, and M. Vllaln. 1997. Mixed-initiative 
development of language processing systems. In 
Fifth Conference on Applied Natural Language Pro- 
cessing, pages 348-355. Association for Computa- 
tional Linguistics, March. 
T. G. Dietterich and G. Bakiri. 1995. Solving multi- 
class learning problems via error-correcting output 
codes. Journal of Artificial Intelligence Research, 
2:263-286. 
S. Engelson and I. Dagan. 1996. Minlmi~.ing manual 
annotation cost in supervised training fxom corpora. 
In Proceedings of ACL 1996, pages 319-326, Santa 
Cruz, CA. Association for Computational Linguis- 
tics. 
Y. Freund and R.E. Schapire. 1997. A decision- 
theoretic generalization of on-fine learning and an 
application to boosting. Journal of Computer and 
System Sciences, 55(1):119--139. 
Y. Fremad, H. S. Senng, E. Shamir, and N. Tishby. 
1997. Selective sampling using the query by com- 
mittee algorithm. Machine Learning, 28:133-168. 
D. Lewis and J. Catlett. 1994. Heterogeneous n- 
certainty sampling for supervised learning. In Pro- 
ceedings of the 11th International Conference on 
Machine Learning, pages 139---147. 
D. Lewis and W. Gale. 1994. A sequential algorithm 
for training text classifiers. In Proceedings ofA CM- 
SIGIR 1994, pages 3-12. ACM-SIGIR. 
R. Liere and P. Tadepalli. 1997. Active learning with 
committees for text categorization. In Proceedings 
of the Fourteenth National Conference on Artificial 
Intelligence, pages 591-596. AAAI. 
L. Mangu and E. Brill. 1997. Automatic rule acquisi- 
tion for spelling correction. In Proceedings of the 
Fourteenth International Conference on Machine 
Learning, pages 734-741, Nashville, Tennessee. 
M. P. Marcus, B. Santorini, and M. A. Mareinkiewicz. 
1993. Building a large annotated corpus of english: 
The Penn Treebank. Computational Linguistics, 
19(2):313-330. 
G. Ngai and D. Yarowsky. 2000. Rule writing or 
annotation: Cost-efficient resource usage for base 
noun phrase chunking. In Proceedings ofA CL 2000. 
Association for Computational Linguistics. 
C. E. Priebe, J.-S. Pang, and T. Olson. 1999. Opt lmiT. 
ing mine classification performance. In Proceedings 
of the JSM. American Statistical Association. 
J. R. Qnlnlan. 1993. C~.5: Programs for machine 
learning. Morgan Kanfmann, San Mateo, CA. 
L. Ramshaw and M. Marcus, 1999. Text Chunk- 
ing Using Transformation-based Learning. In S. 
Armstrong, K.W. Church, P. Isabelle, S. Mauzi, 
E. Tzoukermann and D. Yarowsky (eds.) Natural 
Language Processing Using Very Large Corpora, 
Kluwer. 
E. Roche and Y. Schabes. 1995. Computational 
linguistics. Deterministic Part of Speech Tagging 
with Finite State Transducers, 21(2):227-253. 
K. Samuel, S. Carberry, and K. Vijay-Shanker. 1998. 
Dialogue act tagging with transformation-based 
learning. In Proceedings of the 17th International 
Conference on Computational Linguistics and the 
36th Annual Meeting of the Association for Com- 
putational Linguistics, pages 1150-1156, Montreal, 
Quebec, Canada. 
H. S. Senng, M. Opper, and H. Sompolinsky. 1992. 
Query by committee. In Proceedings of the Fifth 
Annual A CM Workshop on Computational Learning 
Theory, pages 287-294. ACM. 
M. Vilain and D. Day. 1996. Finite-state parsing 
by rule sequences. In International Conference on 
Computational Linguistics, pages 274-279, Copen- 
hagen, Denmark, August. 
34 
Identifying Concepts Across Languages:
A First Step towards a Corpus-based Approach to Automatic Ontology
Alignment
Grace Ngai
 
Marine Carpuat

Pascale Fung
  
grace@intendi.com eemarine@ust.hk pascale@ee.ust.hk
 
Intendi Inc.
Hong Kong

Human Language Technology Center
HKUST
Clear Water Bay, Hong Kong
1 Introduction
The growing importance of multilingual informa-
tion retrieval and machine translation has made mul-
tilingual ontologies an extremely valuable resource.
Since the construction of an ontology from scratch
is a very expensive and time consuming undertak-
ing, it is attractive to consider ways of automatically
aligning monolingual ontologies, which already ex-
ist for many of the world?s major languages.
This paper presents a first step towards the cre-
ation of a bilingual ontology through the alignment
of two monolingual ontologies: the American En-
glish WordNet and the Mandarin Chinese HowNet.
These two ontologies have structures which are very
different from each other, as well as being con-
structed for two very different languages, which
makes this an appropriate and challenging task for
our algorithm.
2 Alignment of Ontologies
In this paper, we address the problem of automatic
multilingual ontology alignment. Multilingual on-
tologies are very useful, but are also very time-
consuming and expensive to build. For example,
Euro WordNet (Vossen, 1998), a multilingual on-
tology for 8 European languages, involved 11 aca-
demic and commercial institutions and took 3 years
to complete. Furthermore, for many of the world?s
major languages, monolingual ontologies already
exist in some shape or form. Therefore, it is reason-
able and attractive to investigate whether a multi-
lingual ontology could be quickly and robustly con-
structed from monolingual resources.
Given the easy availability of bilingual dictionar-
ies, the task might seem easy at a first blush. How-
ever, given two independently constructed ontolo-
gies, there always exists some difference in their
structure that makes it difficult to perform a purely
structural alignment. These differences arise from
different approaches and philosophies taken during
the construction of the ontology; and for ontologies
in different languages, differences which stem from
dissimilarities between the languages concerned.
In addition, multilingual ontology alignment also
has to deal with machine translation issues. Since
an ontology arranges words in a semantic hierar-
chy, it is possible for a word to appear in several
different places in the hierarchy depending on its
semantic sense. However, words and concepts in a
given language do not always translate cleanly into a
second language; a word often has multiple transla-
tions, and they do not always share the same mean-
ings. In the absence of any ambiguity resolution,
synonym sets in one ontology will be erroneously
aligned to multiple synonym sets in the second on-
tology. This is a serious problem: an investigative
experiment with two ontologies, the American En-
glish WordNet and the Mandarin Chinese HowNet,
found that, in the absence of any word sense disam-
biguation, each HowNet definition (the equivalent
of a synonym set from WordNet) corresponded to
an average of 8.1 WordNet synonym sets.
The approach taken in this paper works upon the
assumption that even though a word may have dif-
ferent translations that correspond to different se-
mantic senses, it is not likely that its synonyms will
have the same exact set of translations. Given a syn-
onym set, or synset, in one ontology, our approach
considers the average similarity between its words
and words from all potential alignment candidates:
Given two ontologies  and  , a synonym set
(synset) 	
 , and a similarity score  	
ffTransformation-Based Learning in the Fast Lane
Grace Ngai
y;z
and Radu Florian
y
{gyn,rorian}@cs.jhu.edu
y
Johns Hopkins University
Baltimore, MD 21218, USA
z
Weniwen Technologies
Hong Kong
Abstract
Transformation-based learning has been successfully
employed to solve many natural language process-
ing problems. It achieves state-of-the-art perfor-
mance on many natural language processing tasks
and does not overtrain easily. However, it does have
a serious drawback: the training time is often in-
torelably long, especially on the large corpora which
are often used in NLP. In this paper, we present a
novel and realistic method for speeding up the train-
ing time of a transformation-based learner without
sacricing performance. The paper compares and
contrasts the training time needed and performance
achieved by our modied learner with two other
systems: a standard transformation-based learner,
and the ICA system (Hepple, 2000). The results of
these experiments show that our system is able to
achieve a signicant improvement in training time
while still achieving the same performance as a stan-
dard transformation-based learner. This is a valu-
able contribution to systems and algorithms which
utilize transformation-based learning at any part of
the execution.
1 Introduction
Much research in natural language processing has
gone into the development of rule-based machine
learning algorithms. These algorithms are attractive
because they often capture the linguistic features of
a corpus in a small and concise set of rules.
Transformation-based learning (TBL) (Brill,
1995) is one of the most successful rule-based ma-
chine learning algorithms. It is a exible method
which is easily extended to various tasks and do-
mains, and it has been applied to a wide variety of
NLP tasks, including part of speech tagging (Brill,
1995), noun phrase chunking (Ramshaw and Mar-
cus, 1999), parsing (Brill, 1996), phrase chunking
(Florian et al, 2000), spelling correction (Mangu
and Brill, 1997), prepositional phrase attachment
(Brill and Resnik, 1994), dialog act tagging (Samuel
et al, 1998), segmentation and message understand-
ing (Day et al, 1997). Furthermore, transformation-
based learning achieves state-of-the-art performance
on several tasks, and is fairly resistant to overtrain-
ing (Ramshaw and Marcus, 1994).
Despite its attractive features as a machine learn-
ing algorithm, TBL does have a serious draw-
back in its lengthy training time, especially on the
larger-sized corpora often used in NLP tasks. For
example, a well-implemented transformation-based
part-of-speech tagger will typically take over 38
hours to nish training on a 1 million word cor-
pus. This disadvantage is further exacerbated when
the transformation-based learner is used as the base
learner in learning algorithms such as boosting or
active learning, both of which require multiple it-
erations of estimation and application of the base
learner. In this paper, we present a novel method
which enables a transformation-based learner to re-
duce its training time dramatically while still retain-
ing all of its learning power. In addition, we will
show that our method scales better with training
data size.
2 Transformation-based Learning
The central idea of transformation-based learning
(TBL) is to learn an ordered list of rules which
progressively improve upon the current state of the
training set. An initial assignment is made based on
simple statistics, and then rules are greedily learned
to correct the mistakes, until no net improvement
can be made.
The following denitions and notations will be
used throughout the paper:
 The sample space is denoted by S;
 C denotes the set of possible classications of
the samples;
 C[s] denotes the classication associated with a
sample s, and T [s] denotes the true classica-
tion of s;
 p will usually denote a predicate dened on S;
 A rule r is dened as a predicate - class label
pair, (p; t), where t 2 C is called the target of r;
 R denotes the set of all rules;
 If r = (p; t), p
r
will denote p and t
r
will denote
t;
 A rule r = (p
r
; t
r
) applies to a sample s if
p
r
(s) = true and t
r
6= C[s]; the resulting sam-
ple is denoted by r(s).
Using the TBL framework to solve a problem as-
sumes the existence of:
 An initial class assignment. This can be as sim-
ple as the most common class label in the train-
ing set, or it can be the output of another clas-
sier.
 A set of allowable templates for rules. These
templates determine the types of predicates the
rules will test; they have the largest impact on
the behavior of the system.
 An objective function f for learning. Unlike
in many other learning algorithms, the objec-
tive function for TBL will directly optimize the
evaluation function. A typical example is the
dierence in performance resulting from apply-
ing the rule:
f (r) = good (r)   bad (r)
where
good (r) = jfsjC [s] 6= T [s] ^ C [r (s)] = T [s]gj
bad (r) = jfsjC [s] = T [s] ^ C [r (s)] 6= T [s]gj
Since we are not interested in rules that have a nega-
tive objective function value, only the rules that have
a positive good (r) need be examined. This leads to
the following approach:
1. Generate the rules (using the rule template set)
that correct at least an error (i.e. good (r) > 0),
by examining all the incorrect samples (s s.t.
C [s] 6= T [s]);
2. Compute the values bad () for each rule r such
that good(r) > f(b) , storing at each point in
time the rule b that has the highest score; while
computing bad(r), skip to the next rule when
f (r) < f (b)
The system thus learns a list of rules in a greedy
fashion, according to the objective function. When
no rule that improves the current state of the train-
ing set beyond a pre-set threshold can be found, the
training phase ends. During the application phase,
the evaluation set is initialized with the initial class
assignment. The rules are then applied sequentially
to the evaluation set in the order they were learned.
The nal classication is the one attained when all
rules have been applied.
2.1 Previous Work
As was described in the introductory section, the
long training time of TBL poses a serious prob-
lem. Various methods have been investigated to-
wards ameliorating this problem, and the following
subsections detail two of the approaches.
2.1.1 The Ramshaw & Marcus Approach
One of the most time-consuming steps in
transformation-based learning is the updating
step. The iterative nature of the algorithm requires
that each newly selected rule be applied to the
corpus, and the current state of the corpus updated
before the next rule is learned.
Ramshaw & Marcus (1994) attempted to reduce
the training time of the algorithm by making the up-
date process more ecient. Their method requires
each rule to store a list of pointers to samples that
it applies to, and for each sample to keep a list of
pointers to rules that apply to it. Given these two
sets of lists, the system can then easily:
1. identify the positions where the best rule applies
in the corpus; and
2. update the scores of all the rules which are af-
fected by a state change in the corpus.
These two processes are performed multiple times
during the update process, and the modication re-
sults in a signicant reduction in running time.
The disadvantage of this method consists in the
system having an unrealistically high memory re-
quirement. For example, a transformation-based
text chunker training upon a modestly-sized corpus
of 200,000 words has approximately 2 million rules
active at each iteration. The additional memory
space required to store the lists of pointers associ-
ated with these rules is about 450 MB, which is a
rather large requirement to add to a system.
1
2.1.2 The ICA Approach
The ICA system (Hepple, 2000) aims to reduce the
training time by introducing independence assump-
tions on the training samples that dramatically re-
duce the training time with the possible downside of
sacricing performance.
To achieve the speedup, the ICA system disallows
any interaction between the learned rules, by enforc-
ing the following two assumptions:
 Sample Independence  a state change in a
sample (e.g. a change in the current part-
of-speech tag of a word) does not change the
context of surrounding samples. This is cer-
tainly the case in tasks such as prepositional
phrase attachment, where samples are mutually
independent. Even for tasks such as part-of-
speech tagging where intuition suggests it does
not hold, it may still be a reasonable assump-
tion to make if the rules apply infrequently and
sparsely enough.
1
We need to note that the 200k-word corpus used in this
experiment is considered small by NLP standards. Many of
the available corpora contain over 1 million words. As the
size of the corpus increases, so does the number of rules and
the additional memory space required.
 Rule Commitment  there will be at most one
state change per sample. In other words, at
most one rule is allowed to apply to each sample.
This mode of application is similar to that of a
decision list (Rivest, 1987), where an sample is
modied by the rst rule that applies to it, and
not modied again thereafter. In general, this
assumption will hold for problems which have
high initial accuracy and where state changes
are infrequent.
The ICA system was designed and tested on the
task of part-of-speech tagging, achieving an impres-
sive reduction in training time while suering only
a small decrease in accuracy. The experiments pre-
sented in Section 4 include ICA in the training time
and performance comparisons
2
.
2.1.3 Other Approaches
Samuel (1998) proposed a Monte Carlo approach
to transformation-based learning, in which only a
fraction of the possible rules are randomly selected
for estimation at each iteration. The -TBL sys-
tem described in Lager (1999) attempts to cut down
on training time with a more ecient Prolog imple-
mentation and an implementation of lazy learning.
The application of a transformation-based learning
can be considerably sped-up if the rules are compiled
in a nite-state transducer, as described in Roche
and Schabes (1995).
3 The Algorithm
The approach presented here builds on the same
foundation as the one in (Ramshaw and Marcus,
1994): instead of regenerating the rules each time,
they are stored into memory, together with the two
values good (r) and bad (r).
The following notations will be used throughout
this section:
 G (r) = fs 2 Sjp
r
(s) = true and C[s] 6=
t
r
and t
r
= T [s]g  the samples on which the
rule applies and changes them to the correct
classication; therefore, good(r) = jG(r)j.
 B (r) = fs 2 Sjp
r
(s) = true and C[s] 6=
t
r
and C[s] = T [s]g  the samples on which
the rule applies and changes the classication
from correct to incorrect; similarly, bad(r) =
jB(r)j.
Given a newly learned rule b that is to be applied
to S, the goal is to identify the rules r for which at
least one of the sets G (r) ; B (r) is modied by the
application of rule b. Obviously, if both sets are not
modied when applying rule b, then the value of the
objective function for rule r remains unchanged.
2
The algorithm was implemented by the the authors, fol-
lowing the description in Hepple (2000).
The presentation is complicated by the fact that,
in many NLP tasks, the samples are not indepen-
dent. For instance, in POS tagging, a sample is de-
pendent on the classication of the preceding and
succeeding 2 samples (this assumes that there ex-
ists a natural ordering of the samples in S). Let
V (s) denote the vicinity of a sample  the set of
samples on whose classication the sample s might
depend on (for consistency, s 2 V (s)); if samples are
independent, then V (s) = fsg.
3.1 Generating the Rules
Let s be a sample on which the best rule b applies
(i.e. [b (s)] 6= C [s]). We need to identify the rules
r that are inuenced by the change s ! b (s). Let
r be such a rule. f (r) needs to be updated if and
only if there exists at least one sample s
0
such that
s
0
2 G (r) and b (s
0
) =2 G (r) or (1)
s
0
2 B (r) and b (s
0
) =2 B (r) or (2)
s
0
=2 G (r) and b (s
0
) 2 G (r) or (3)
s
0
=2 B (r) and b (s
0
) 2 B (r) (4)
Each of the above conditions corresponds to a spe-
cic update of the good (r) or bad (r) counts. We
will discuss how rules which should get their good or
bad counts decremented (subcases (1) and (2)) can
be generated, the other two being derived in a very
similar fashion.
The key observation behind the proposed algo-
rithm is: when investigating the eects of applying
the rule b to sample s, only samples s
0
in the set
V (s) need to be checked. Any sample s
0
that is not
in the set
[
fsjb changes sg
V (s)
can be ignored since s
0
= b(s
0
).
Let s
0
2 V (s) be a sample in the vicinity of s.
There are 2 cases to be examined  one in which b
applies to s
0
and one in which b does not:
Case I: c (s
0
) = c (b (s
0
)) (b does not modify the
classication of sample s
0
). We note that the
condition
s
0
2 G (r) and b (s
0
) =2 G (r)
is equivalent to
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^
t
r
= T [s
0
] ^ p
r
(b (s
0
)) = false
(5)
and the formula
s
0
2 B (r) and b (s
0
) =2 B (r)
is equivalent to
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^
C [s
0
] = T [s
0
] ^ p
r
(b (s
0
)) = false
(6)
(for the full details of the derivation, inferred from
the denition of G (r) and B (r), please refer to
Florian and Ngai (2001)).
These formulae oer us a method of generating
the rules r which are inuenced by the modication
s
0
! b (s
0
):
1. Generate all predicates p (using the predicate
templates) that are true on the sample s
0
.
2. If C [s
0
] 6= T [s
0
] then
(a) If p (b (s
0
)) = false then decrease good (r),
where r is the rule created with predicate
p s.t. target T [s
0
];
3. Else
(a) If p (b (s
0
)) = false then for all the rules
r whose predicate is p
3
and t
r
6= C [s
0
] de-
crease bad (r);
The algorithm for generating the rules r that need
their good counts (formula (3)) or bad counts (for-
mula (4)) increased can be obtained from the formu-
lae (1) (respectively (2)), by switching the states s
0
and b (s
0
), and making sure to add all the new pos-
sible rules that might be generated (only for (3)).
Case II: C [s
0
] 6= C [b (s
0
)] (b does change the clas-
sication of sample s
0
). In this case, the formula (5)
is transformed into:
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^ t
r
= T [s
0
] ^
(p
r
(b (s
0
)) = false _ t
r
= C [b (s
0
)])
(7)
(again, the full derivation is presented in Florian and
Ngai (2001)). The case of (2), however, is much
simpler. It is easy to notice that C [s
0
] 6= C [b (s
0
)]
and s
0
2 B (r) implies that b (s
0
) =2 B (r); indeed,
a necessary condition for a sample s
0
to be in a set
B (r) is that s
0
is classied correctly, C [s
0
] = T [s
0
].
Since T [s
0
] 6= C [b (s
0
)], results C [b (s
0
)] 6= T [s
0
] and
therefore b (s
0
) =2 B (r). Condition (3) is, therefore,
equivalent to
p
r
(s
0
) = true ^ C [s
0
] 6= t
r
^ C [s
0
] = T [s
0
]
(8)
The algorithm is modied by replacing the test
p (b (s
0
)) = false with the test p
r
(b (s
0
)) = false _
C [b (s)] = t
r
in formula (1) and removing the test
altogether for case of (2). The formulae used to gen-
erate rules r that might have their counts increased
(equations (3) and (4)) are obtained in the same
fashion as in Case I.
3.2 The Full Picture
At every point in the algorithm, we assumed that all
the rules that have at least some positive outcome
(good (r) > 0) are stored, and their score computed.
3
This can be done eciently with an appropriate data
structure - for example, using a double hash.
For all samples s that satisfy C [s] 6= T [s], generate all rules
r that correct the classication of s; increase good (r).
For all samples s that satisfy C [s] = T [s] generate all pred-
icates p s.t. p (s) = true; for each rule r s.t. p
r
= p and
t
r
6= C [s] increase bad (r).
1: Find the rule b = argmax
r2R
f (r).
If (f (b) < Threshold or corpus learned to completion) then
quit.
For each predicate p, let R (p) be the rules whose predicate
is p (p
r
= r).
For each samples s; s
0
s.t. C [s] 6= C [b (s)] and s
0
2 V (s):
If C [s
0
] = C [b (s
0
)] then
 for each predicate p s.t. p (s
0
) = true
 If C [s
0
] 6= T [s
0
] then
 If p (b (s
0
)) = false then decrease good (r),
where r = [p; T [s
0
]], the rule created with
predicate p and target T [s
0
];
Else
 If p (b (s
0
)) = false then for all the rules
r 2 R (p) s.t. t
r
6= C [s
0
] decrease bad (r);
 for each predicate p s.t. p (b (s
0
)) = true
 If C [b (s
0
)] 6= T [s
0
] then
 If p (s
0
) = false then increase good (r),
where r = [p; T [s
0
]];
Else
 If p (s
0
) = false then for all rules r 2 R (p)
s.t. t
r
6= C [b (s
0
)] increase bad (r);
Else
 for each predicate p s.t. p (s
0
) = true
 If C [s
0
] 6= T [s
0
] then
 If p (b (s
0
)) = false _ C [b (s
0
)] = t
r
then
decrease good (r), where r = [p; T [s
0
]];
Else
For all the rules r 2 R(p) s.t. t
r
6= C [s
0
]
decrease bad (r);
 for each predicate p s.t. p (b (s
0
)) = true
 If C [b (s
0
)] 6= T [s
0
] then
 If p (s
0
) = false _ C [s
0
] = t
r
then increase
good (r), where r = [p; T [s
0
]];
Else
For all rules r 2 R (p) s.t. t
r
6= C [b (s
0
)]
increase bad (r);
Repeat from step 1:
Figure 1: FastTBL Algorithm
Therefore, at the beginning of the algorithm, all the
rules that correct at least one wrong classication
need to be generated. The bad counts for these rules
are then computed by generation as well: in every
position that has the correct classication, the rules
that change the classication are generated, as in
Case 4, and their bad counts are incremented. The
entire FastTBL algorithm is presented in Figure 1.
Note that, when the bad counts are computed, only
rules that already have positive good counts are se-
lected for evaluation. This prevents the generation
of useless rules and saves computational time.
The number of examined rules is kept close to the
minimum. Because of the way the rules are gen-
erated, most of them need to modify either one of
their counts. Some additional space (besides the one
needed to represent the rules) is necessary for repre-
senting the rules in a predicate hash  in order to
have a straightforward access to all rules that have a
given predicate; this amount is considerably smaller
than the one used to represent the rules. For exam-
ple, in the case of text chunking task described in
section 4, only approximately 30Mb additional mem-
ory is required, while the approach of Ramshaw and
Marcus (1994) would require approximately 450Mb.
3.3 Behavior of the Algorithm
As mentioned before, the original algorithm has a
number of deciencies that cause it to run slowly.
Among them is the drastic slowdown in rule learning
as the scores of the rules decrease. When the best
rule has a high score, which places it outside the tail
of the score distribution, the rules in the tail will be
skipped when the bad counts are calculated, since
their good counts are small enough to cause them
to be discarded. However, when the best rule is in
the tail, many other rules with similar scores can no
longer be discarded and their bad counts need to be
computed, leading to a progressively longer running
time per iteration.
Our algorithm does not suer from the same prob-
lem, because the counts are updated (rather than
recomputed) at each iteration, and only for the sam-
ples that were aected by the application of the lat-
est rule learned. Since the number of aected sam-
ples decreases as learning progresses, our algorithm
actually speeds up considerably towards the end of
the training phase. Considering that the number
of low-score rules is a considerably higher than the
number of high-score rules, this leads to a dramatic
reduction in the overall running time.
This has repercussions on the scalability of the al-
gorithm relative to training data size. Since enlarg-
ing the training data size results in a longer score dis-
tribution tail, our algorithm is expected to achieve
an even more substantial relative running time im-
provement over the original algorithm. Section 4
presents experimental results that validate the su-
perior scalability of the FastTBL algorithm.
4 Experiments
Since the goal of this paper is to compare and con-
trast system training time and performance, extra
measures were taken to ensure fairness in the com-
parisons. To minimize implementation dierences,
all the code was written in C++ and classes were
shared among the systems whenever possible. For
each task, the same training set was provided to each
system, and the set of possible rule templates was
kept the same. Furthermore, extra care was taken
to run all comparable experiments on the same ma-
chine and under the same memory and processor
load conditions.
To provide a broad comparison between the sys-
tems, three NLP tasks with dierent properties
were chosen as the experimental domains. The
rst task, part-of-speech tagging, is one where the
commitment assumption seems intuitively valid and
the samples are not independent. The second
task, prepositional phrase attachment, has examples
which are independent from each other. The last
task is text chunking, where both independence and
commitment assumptions do not seem to be valid.
A more detailed description of each task, data and
the system parameters are presented in the following
subsections.
Four algorithms are compared during the follow-
ing experiments:
 The regular TBL, as described in section 2;
 An improved version of TBL, which makes ex-
tensive use of indexes to speed up the rules' up-
date;
 The FastTBL algorithm;
 The ICA algorithm (Hepple, 2000).
4.1 Part-of-Speech Tagging
The goal of this task is to assign to each word
in the given sentence a tag corresponding to its
part of speech. A multitude of approaches have
been proposed to solve this problem, including
transformation-based learning, Maximum Entropy
models, Hidden Markov models and memory-based
approaches.
The data used in the experiment was selected from
the Penn Treebank Wall Street Journal, and is the
same used by Brill and Wu (1998). The training set
contained approximately 1M words and the test set
approximately 200k words.
Table 1 presents the results of the experiment
4
.
All the algorithms were trained until a rule with
a score of 2 was reached. The FastTBL algorithm
performs very similarly to the regular TBL, while
running in an order of magnitude faster. The two
assumptions made by the ICA algorithm result in
considerably less training time, but the performance
is also degraded (the dierence in performance is sta-
tistically signicant, as determined by a signed test,
at a signicance level of 0:001). Also present in Ta-
ble 1 are the results of training Brill's tagger on the
same data. The results of this tagger are presented
to provide a performance comparison with a widely
used tagger. Also worth mentioning is that the tag-
ger achieved an accuracy of 96:76% when trained on
the entire data
5
; a Maximum Entropy tagger (Rat-
naparkhi, 1996) achieves 96:83% accuracy with the
same training data/test data.
4
The time shown is the combined running time for both
the lexical tagger and the contextual tagger.
5
We followed the setup from Brill's tagger: the contextual
tagger is trained only on half of the training data. The train-
ing time on the entire data was approximately 51 minutes.
Brill's tagger Regular TBL Indexed TBL FastTBL ICA (Hepple)
Accuracy 96:61% 96:61% 96:61% 96:61% 96:23%
Running time 5879 mins, 46 secs 2286 mins, 21 secs 420 mins, 7 secs 17 mins, 21 secs 6 mins, 13 secs
Time ratio 0:4 1:0 5:4 131:7 367:8
Table 1: POS tagging: Evaluation and Running Times
Regular TBL Indexed TBL Fast TBL ICA (Hepple)
Accuracy 81:0% 81:0% 81:0% 77:8%
Running time 190 mins, 19 secs 65 mins, 50 secs 14 mins, 38 secs 4 mins, 1 sec
Time Ratio 1:0 2:9 13 47:4
Table 2: PP Attachment:Evaluation and Running Times
4.2 Prepositional Phrase Attachment
Prepositional phrase attachment is the task of decid-
ing the point of attachment for a given prepositional
phrase (PP). As an example, consider the following
two sentences:
1. I washed the shirt with soap and water.
2. I washed the shirt with pockets.
In Sentence 1, the PP with soap and water de-
scribes the act of washing the shirt. In Sentence 2,
however, the PP with pockets is a description for
the shirt that was washed.
Most previous work has concentrated on situa-
tions which are of the form VP NP1 P NP2. The
problem is cast as a classication task, and the sen-
tence is reduced to a 4-tuple containing the preposi-
tion and the non-inected base forms of the head
words of the verb phrase VP and the two noun
phrases NP1 and NP2. For example, the tuple cor-
responding to the two above sentences would be:
1. wash shirt with soap
2. wash shirt with pocket
Many approaches to solving this this problem have
been proposed, most of them using standard ma-
chine learning techniques, including transformation-
based learning, decision trees, maximum entropy
and backo estimation. The transformation-based
learning system was originally developed by Brill
and Resnik (1994).
The data used in the experiment consists of ap-
proximately 13,000 quadruples (VP NP1 P NP2 )
extracted from Penn Treebank parses. The set is
split into a test set of 500 samples and a training set
of 12,500 samples. The templates used to generate
rules are similar to the ones used by Brill and Resnik
(1994) and some include WordNet features. All the
systems were trained until no more rules could be
learned.
Table 2 shows the results of the experiments.
Again, the ICA algorithm learns the rules very fast,
but has a slightly lower performance than the other
two TBL systems. Since the samples are inherently
independent, there is no performance loss because
of the independence assumption; therefore the per-
formance penalty has to come from the commitment
assumption. The Fast TBL algorithm runs, again,
in a order of magnitude faster than the original TBL
while preserving the performance; the time ratio is
only 13 in this case due to the small training size
(only 13000 samples).
4.3 Text Chunking
Text chunking is a subproblem of syntactic pars-
ing, or sentence diagramming. Syntactic parsing at-
tempts to construct a parse tree from a sentence by
identifying all phrasal constituents and their attach-
ment points. Text chunking simplies the task by
dividing the sentence into non-overlapping phrases,
where each word belongs to the lowest phrasal con-
stituent that dominates it. The following exam-
ple shows a sentence with text chunks and part-of-
speech tags:
[NP A.P.
NNP
Green
NNP
] [ADVP
currently
RB
] [VP has ] [NP 2,664,098
CD
shares
NNS
] [ADJP outstanding
JJ
] .
The problem can be transformed into a classication
task. Following Ramshaw & Marcus' (1999) work in
base noun phrase chunking, each word is assigned
a chunk tag corresponding to the phrase to which
it belongs . The following table shows the above
sentence with the assigned chunk tags:
Word POS tag Chunk Tag
A.P. NNP B-NP
Green NNP I-NP
currently RB B-ADVP
has VBZ B-VP
2,664,098 CD B-NP
shares NNS I-NP
outstanding JJ B-ADJP
. . O
The data used in this experiment is the CoNLL-
2000 phrase chunking corpus (Tjong Kim Sang and
Buchholz, 2000). The training corpus consists of
sections 15-18 of the Penn Treebank (Marcus et al,
1993); section 20 was used as the test set. The chunk
tags are derived from the parse tree constituents,
Regular TBL Indexed TBL Fast TBL ICA (Hepple)
F-measure 92.30 92.30 92.30 86.20
Running Time 19211 mins, 40 secs 2056 mins, 4secs 137 mins, 57 secs 12 mins, 40 secs
Time Ratio 1:0 9:3 139:2 1516:7
Table 3: Text Chunking: Evaluation and Running Times
and the part-of-speech tags were generated by Brill's
tagger (Brill, 1995). All the systems are trained to
completion (until all the rules are learned).
Table 3 shows the results of the text chunking ex-
periments. The performance of the FastTBL algo-
rithm is the same as of regular TBL's, and runs in an
order of magnitude faster. The ICA algorithm again
runs considerably faster, but at a cost of a signi-
cant performance hit. There are at least 2 reasons
that contribute to this behavior:
1. The initial state has a lower performance than
the one in tagging; therefore the independence
assumption might not hold. 25% of the samples
are changed by at least one rule, as opposed to
POS tagging, where only 2.5% of the samples
are changed by a rule.
2. The commitment assumption might also not
hold. For this task, 20% of the samples that
were modied by a rule are also changed again
by another one.
4.4 Training Data Size Scalability
A question usually asked about a machine learning
algorithm is how well it adapts to larger amounts
of training data. Since the performance of the Fast
TBL algorithm is identical to that of regular TBL,
the issue of interest is the dependency between the
running time of the algorithm and the amount of
training data.
The experiment was performed with the part-of-
speech data set. The four algorithms were trained
on training sets of dierent sizes; training times were
recorded and averaged over 4 trials. The results are
presented in Figure 2(a). It is obvious that the Fast
TBL algorithm is much more scalable than the reg-
ular TBL  displaying a linear dependency on the
amount of training data, while the regular TBL has
an almost quadratic dependency. The explanation
for this behavior has been given in Section 3.3.
Figure 2(b) shows the time spent at each iteration
versus the iteration number, for the original TBL
and fast TBL systems. It can be observed that the
time taken per iteration increases dramatically with
the iteration number for the regular TBL, while for
the FastTBL, the situation is reversed. The con-
sequence is that, once a certain threshold has been
reached, the incremental time needed to train the
FastTBL system to completion is negligible.
5 Conclusions
We have presented in this paper a new and im-
proved method of computing the objective function
for transformation-based learning. This method al-
lows a transformation-based algorithm to train an
observed 13 to 139 times faster than the original
one, while preserving the nal performance of the
algorithm. The method was tested in three dier-
ent domains, each one having dierent characteris-
tics: part-of-speech tagging, prepositional phrase at-
tachment and text chunking. The results obtained
indicate that the algorithmic improvement gener-
ated by our method is not linked to a particular
task, but extends to any classication task where
transformation-based learning can be applied. Fur-
thermore, our algorithm scales better with training
data size; therefore the relative speed-up obtained
will increase when more samples are available for
training, making the procedure a good candidate for
large corpora tasks.
The increased speed of the Fast TBL algorithm
also enables its usage in higher level machine learn-
ing algorithms, such as adaptive boosting, model
combination and active learning. Recent work (Flo-
rian et al, 2000) has shown how a TBL frame-
work can be adapted to generate condences on the
output, and our algorithm is compatible with that
framework. The stability, resistance to overtraining,
the existence of probability estimates and, now, rea-
sonable speed make TBL an excellent candidate for
solving classication tasks in general.
6 Acknowledgements
The authors would like to thank David Yarowsky
for his advice and guidance, Eric Brill and John
C. Henderson for discussions on the initial ideas of
the material presented in the paper, and the anony-
mous reviewers for useful suggestions, observations
and connections with other published material. The
work presented here was supported by NSF grants
IRI-9502312, IRI-9618874 and IIS-9985033.
References
E. Brill and P. Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment disam-
biguation. In Proceedings of the Fifteenth Interna-
tional Conference on Computational Linguistics
(COLING-1994), pages 11981204, Kyoto.
E. Brill and J. Wu. 1998. Classier combination for
05000
10000
15000
20000
25000
100000 150000 200000 250000 300000 350000 400000 450000 500000 550000
Ru
nn
in
g 
Ti
m
e (
mi
nu
tes
)
Training Set Size (words)
ICA FastTBL
Indexed TBL
Regular TBL
(a) Running Time versus Training Data Size
Ru
nn
in
g 
Ti
m
e (
sec
on
ds
)
Iteration Number
Regular TBL
FastTBL
Indexed TBL
0
500
1000
1500
2000
0 200 400 600 800 1000
(b) Running Time versus Iteration Number
Figure 2: Algorithm Scalability
improved lexical disambiguation. Proceedings of
COLING-ACL'98, pages 191195, August.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21(4):543565.
E. Brill, 1996. Recent Advances in Parsing Technol-
ogy, chapter Learning to Parse with Transforma-
tions. Kluwer.
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok,
P. Robinson, and M. Vilain. 1997. Mixed-
initiative development of language processing sys-
tems. In Fifth Conference on Applied Natural
Language Processing, pages 348355. Association
for Computational Linguistics, March.
R. Florian and G. Ngai. 2001. Transformation-
based learning in the fast lane. Technical report,
Johns Hopkins University, Computer Science De-
partment.
R. Florian, J.C. Henderson, and G. Ngai. 2000.
Coaxing condence from an old friend: Probabilis-
tic classications from transformation rule lists.
In Proceedings of SIGDAT-EMNLP 2000, pages
2643, Hong Kong, October.
M. Hepple. 2000. Independence and commitment:
Assumptions for rapid training and execution of
rule-based pos taggers. In Proceedings of the 38th
Annual Meeting of the ACL, pages 278285, Hong
Kong, October.
T. Lager. 1999. The -tbl system: Logic pro-
gramming tools for transformation-based learn-
ing. In Proceedings of the 3rd International Work-
shop on Computational Natural Language Learn-
ing, Bergen.
L. Mangu and E. Brill. 1997. Automatic rule acqui-
sition for spelling correction. In Proceedings of the
Fourteenth International Conference on Machine
Learning, pages 734741, Nashville, Tennessee.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of english: The Penn Treebank.
Computational Linguistics, 19(2):313330.
L. Ramshaw and M. Marcus. 1994. Exploring the
statistical derivation of transformational rule se-
quences for part-of-speech tagging. In The Bal-
ancing Act: Proceedings of the ACL Workshop on
Combining Symbolic and Statistical Approaches to
Language, pages 128135, New Mexico State Uni-
versity, July.
L. Ramshaw and M. Marcus, 1999. Natural Lan-
guage Processing Using Very Large Corpora, chap-
ter Text Chunking Using Transformation-based
Learning, pages 157176. Kluwer.
A. Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the First Con-
ference on Empirical Methods in NLP, pages 133
142, Philadelphia, PA.
R. Rivest. 1987. Learning decision lists. Machine
Learning, 2(3):229246.
E. Roche and Y. Schabes. 1995. Computational
linguistics. Deterministic Part of Speech Tagging
with Finite State Transducers, 21(2):227253.
K. Samuel, S. Carberry, and K. Vijay-Shanker.
1998. Dialogue act tagging with transformation-
based learning. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics
and the 36th Annual Meeting of the Association
for Computational Linguistics, pages 11501156,
Montreal, Quebec, Canada.
K. Samuel. 1998. Lazy transformation-based
learning. In Proceedings of the 11th Intera-
tional Florida AI Research Symposium Confer-
ence, pages 235239, Florida, USA.
E. Tjong Kim Sang and S. Buchholz. 2000. In-
troduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL-
2000, pages 127132, Lisbon, Portugal.
 	
 

 
	
