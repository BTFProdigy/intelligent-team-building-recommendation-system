Coling 2008: Companion volume ? Posters and Demonstrations, pages 23?26
Manchester, August 2008
A Scalable MMR Approach to Sentence Scoring
for Multi-Document Update Summarization
Florian Boudin
\
and Marc El-B
`
eze
\
\
Laboratoire Informatique d?Avignon
339 chemin des Meinajaries, BP1228,
84911 Avignon Cedex 9, France.
florian.boudin@univ-avignon.fr
marc.elbeze@univ-avignon.fr
Juan-Manuel Torres-Moreno
\,[
[
?
Ecole Polytechnique de Montr?eal
CP 6079 Succ. Centre Ville H3C 3A7
Montr?eal (Qu?ebec), Canada.
juan-manuel.torres@univ-avignon.fr
Abstract
We present SMMR, a scalable sentence
scoring method for query-oriented up-
date summarization. Sentences are scored
thanks to a criterion combining query rele-
vance and dissimilarity with already read
documents (history). As the amount of
data in history increases, non-redundancy
is prioritized over query-relevance. We
show that SMMR achieves promising re-
sults on the DUC 2007 update corpus.
1 Introduction
Extensive experiments on query-oriented multi-
document summarization have been carried out
over the past few years. Most of the strategies
to produce summaries are based on an extrac-
tion method, which identifies salient textual seg-
ments, most often sentences, in documents. Sen-
tences containing the most salient concepts are se-
lected, ordered and assembled according to their
relevance to produce summaries (also called ex-
tracts) (Mani and Maybury, 1999).
Recently emerged from the Document Under-
standing Conference (DUC) 2007
1
, update sum-
marization attempts to enhance summarization
when more information about knowledge acquired
by the user is available. It asks the following ques-
tion: has the user already read documents on the
topic? In the case of a positive answer, producing
an extract focusing on only new facts is of inter-
est. In this way, an important issue is introduced:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
Document Understanding Conferences are conducted
since 2000 by the National Institute of Standards and Tech-
nology (NIST), http://www-nlpir.nist.gov
redundancy with previously read documents (his-
tory) has to be removed from the extract.
A natural way to go about update summarization
would be extracting temporal tags (dates, elapsed
times, temporal expressions...) (Mani and Wilson,
2000) or to automatically construct the timeline
from documents (Swan and Allan, 2000). These
temporal marks could be used to focus extracts on
the most recently written facts. However, most re-
cently written facts are not necessarily new facts.
Machine Reading (MR) was used by (Hickl et
al., 2007) to construct knowledge representations
from clusters of documents. Sentences contain-
ing ?new? information (i.e. that could not be in-
ferred by any previously considered document)
are selected to generate summary. However, this
highly efficient approach (best system in DUC
2007 update) requires large linguistic resources.
(Witte et al, 2007) propose a rule-based system
based on fuzzy coreference cluster graphs. Again,
this approach requires to manually write the sen-
tence ranking scheme. Several strategies remain-
ing on post-processing redundancy removal tech-
niques have been suggested. Extracts constructed
from history were used by (Boudin and Torres-
Moreno, 2007) to minimize history?s redundancy.
(Lin et al, 2007) have proposed a modified Max-
imal Marginal Relevance (MMR) (Carbonell and
Goldstein, 1998) re-ranker during sentence selec-
tion, constructing the summary by incrementally
re-ranking sentences.
In this paper, we propose a scalable sentence
scoring method for update summarization derived
from MMR. Motivated by the need for relevant
novelty, candidate sentences are selected accord-
ing to a combined criterion of query relevance and
dissimilarity with previously read sentences. The
rest of the paper is organized as follows. Section 2
23
introduces our proposed sentence scoring method
and Section 3 presents experiments and evaluates
our approach.
2 Method
The underlying idea of our method is that as the
number of sentences in the history increases, the
likelihood to have redundant information within
candidate sentences also increases. We propose
a scalable sentence scoring method derived from
MMR that, as the size of the history increases,
gives more importance to non-redundancy that to
query relevance. We define H to represent the pre-
viously read documents (history), Q to represent
the query and s the candidate sentence. The fol-
lowing subsections formally define the similarity
measures and the scalable MMR scoring method.
2.1 A query-oriented multi-document
summarizer
We have first started by implementing a simple
summarizer for which the task is to produce query-
focused summaries from clusters of documents.
Each document is pre-processed: documents are
segmented into sentences, sentences are filtered
(words which do not carry meaning are removed
such as functional words or common words) and
normalized using a lemmas database (i.e. inflected
forms ?go?, ?goes?, ?went?, ?gone?... are replaced
by ?go?). An N -dimensional term-space ? , where
N is the number of different terms found in the
cluster, is constructed. Sentences are represented
in ? by vectors in which each component is the
term frequency within the sentence. Sentence scor-
ing can be seen as a passage retrieval task in Infor-
mation Retrieval (IR). Each sentence s is scored by
computing a combination of two similarity mea-
sures between the sentence and the query. The first
measure is the well known cosine angle (Salton et
al., 1975) between the sentence and the query vec-
torial representations in ? (denoted respectively ~s
and
~
Q). The second similarity measure is based
on the Jaro-Winkler distance (Winkler, 1999). The
original Jaro-Winkler measure, denoted JW, uses
the number of matching characters and transposi-
tions to compute a similarity score between two
terms, giving more favourable ratings to terms that
match from the beginning. We have extended this
measure to calculate the similarity between the
sentence s and the query Q:
JW
e
(s,Q) =
1
|Q|
?
?
q?Q
max
m?S
?
JW(q,m) (1)
where S
?
is the term set of s in which the terms
m that already have maximized JW(q,m) are re-
moved. The use of JW
e
smooths normalization and
misspelling errors. Each sentence s is scored using
the linear combination:
Sim
1
(s,Q) = ? ? cosine(~s,
~
Q)
+ (1? ?) ? JW
e
(s,Q) (2)
where ? = 0.7, optimally tuned on the past DUCs
data (2005 and 2006). The system produces a list
of ranked sentences from which the summary is
constructed by arranging the high scored sentences
until the desired size is reached.
2.2 A scalable MMR approach
MMR re-ranking algorithm has been successfully
used in query-oriented summarization (Ye et al,
2005). It strives to reduce redundancy while main-
taining query relevance in selected sentences. The
summary is constructed incrementally from a list
of ranked sentences, at each iteration the sentence
which maximizes MMR is chosen:
MMR = argmax
s?S
[ ? ? Sim
1
(s,Q)
? (1? ?) ?max
s
j
?E
Sim
2
(s, s
j
) ] (3)
where S is the set of candidates sentences and E
is the set of selected sentences. ? represents an
interpolation coefficient between sentence?s rele-
vance and non-redundancy. Sim
2
(s, s
j
) is a nor-
malized Longest Common Substring (LCS) mea-
sure between sentences s and s
j
. Detecting sen-
tence rehearsals, LCS is well adapted for redun-
dancy removal.
We propose an interpretation of MMR to tackle
the update summarization issue. Since Sim
1
and
Sim
2
are ranged in [0, 1], they can be seen as prob-
abilities even though they are not. Just as rewriting
(3) as (NR stands for Novelty Relevance):
NR = argmax
s?S
[ ? ? Sim
1
(s,Q)
+ (1? ?) ? (1? max
s
h
?H
Sim
2
(s, s
h
)) ] (4)
We can understand that (4) equates to an OR com-
bination. But as we are looking for a more intu-
itive AND and since the similarities are indepen-
dent, we have to use the product combination. The
24
scoring method defined in (2) is modified into a
double maximization criterion in which the best
ranked sentence will be the most relevant to the
query AND the most different to the sentences in
H .
SMMR(s) = Sim
1
(s,Q)
?
(
1? max
s
h
?H
Sim
2
(s, s
h
)
)
f(H)
(5)
Decreasing ? in (3) with the length of the sum-
mary was suggested by (Murray et al, 2005) and
successfully used in the DUC 2005 by (Hachey
et al, 2005), thereby emphasizing the relevance
at the outset but increasingly prioritizing redun-
dancy removal as the process continues. Sim-
ilarly, we propose to follow this assumption in
SMMR using a function denoted f that as the
amount of data in history increases, prioritize non-
redundancy (f(H)? 0).
3 Experiments
The method described in the previous section has
been implemented and evaluated by using the
DUC 2007 update corpus
2
. The following subsec-
tions present details of the different experiments
we have conducted.
3.1 The DUC 2007 update corpus
We used for our experiments the DUC 2007 up-
date competition data set. The corpus is composed
of 10 topics, with 25 documents per topic. The up-
date task goal was to produce short (?100 words)
multi-document update summaries of newswire ar-
ticles under the assumption that the user has al-
ready read a set of earlier articles. The purpose
of each update summary will be to inform the
reader of new information about a particular topic.
Given a DUC topic and its 3 document clusters: A
(10 documents), B (8 documents) and C (7 doc-
uments), the task is to create from the documents
three brief, fluent summaries that contribute to sat-
isfying the information need expressed in the topic
statement.
1. A summary of documents in cluster A.
2. An update summary of documents in B, un-
der the assumption that the reader has already
read documents in A.
2
More information about the DUC 2007 corpus is avail-
able at http://duc.nist.gov/.
3. An update summary of documents in C, un-
der the assumption that the reader has already
read documents in A and B.
Within a topic, the document clusters must be pro-
cessed in chronological order. Our system gener-
ates a summary for each cluster by arranging the
high ranked sentences until the limit of 100 words
is reached.
3.2 Evaluation
Most existing automated evaluation methods work
by comparing the generated summaries to one or
more reference summaries (ideally, produced by
humans). To evaluate the quality of our generated
summaries, we choose to use the ROUGE
3
(Lin,
2004) evaluation toolkit, that has been found to be
highly correlated with human judgments. ROUGE-
N is a n-gram recall measure calculated between
a candidate summary and a set of reference sum-
maries. In our experiments ROUGE-1, ROUGE-2
and ROUGE-SU4 will be computed.
3.3 Results
Table 1 reports the results obtained on the DUC
2007 update data set for different sentence scor-
ing methods. cosine + JW
e
stands for the scor-
ing method defined in (2) and NR improves it
with sentence re-ranking defined in equation (4).
SMMR is the combined adaptation we have pro-
posed in (5). The function f(H) used in SMMR is
the simple rational function
1
H
, where H increases
with the number of previous clusters (f(H) = 1
for cluster A,
1
2
for cluster B and
1
3
for cluster C).
This function allows to simply test the assumption
that non-redundancy have to be favoured as the
size of history grows. Baseline results are obtained
on summaries generated by taking the leading sen-
tences of the most recent documents of the cluster,
up to 100 words (official baseline of DUC). The
table also lists the three top performing systems at
DUC 2007 and the lowest scored human reference.
As we can see from these results, SMMR out-
performs the other sentence scoring methods. By
ways of comparison our system would have been
ranked second at the DUC 2007 update competi-
tion. Moreover, no post-processing was applied to
the selected sentences leaving an important margin
of progress. Another interesting result is the high
performance of the non-update specific method
(cosine+ JW
e
) that could be due to the small size
3
ROUGE is available at http://haydn.isi.edu/ROUGE/.
25
of the corpus (little redundancy between clusters).
ROUGE-1 ROUGE-2 ROUGE-SU4
Baseline 0.26232 0.04543 0.08247
3
rd
system 0.35715 0.09622 0.13245
2
nd
system 0.36965 0.09851 0.13509
cosine+ JW
e
0.35905 0.10161 0.13701
NR 0.36207 0.10042 0.13781
SMMR 0.36323 0.10223 0.13886
1
st
system 0.37032 0.11189 0.14306
Worst human 0.40497 0.10511 0.14779
Table 1: ROUGE average recall scores computed
on the DUC 2007 update corpus.
4 Discussion and Future Work
In this paper we have described SMMR, a scal-
able sentence scoring method based on MMR that
achieves very promising results. An important as-
pect of our sentence scoring method is that it does
not requires re-ranking nor linguistic knowledge,
which makes it a simple and fast approach to the
issue of update summarization. It was pointed out
at the DUC 2007 workshop that Question Answer-
ing and query-oriented summarization have been
converging on a common task. The value added
by summarization lies in the linguistic quality. Ap-
proaches mixing IR techniques are well suited for
query-oriented summarization but they require in-
tensive work for making the summary fluent and
coherent. Among the others, this is a point that we
think is worthy of further investigation.
Acknowledgments
This work was supported by the Agence Nationale
de la Recherche, France, project RPM2.
References
Boudin, F. and J.M. Torres-Moreno. 2007. A Co-
sine Maximization-Minimization approach for User-
Oriented Multi-Document Update Summarization.
In Recent Advances in Natural Language Processing
(RANLP), pages 81?87.
Carbonell, J. and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 335?336.
ACM Press New York, NY, USA.
Hachey, B., G. Murray, and D. Reitter. 2005. The
Embra System at DUC 2005: Query-oriented Multi-
document Summarization with a Very Large Latent
Semantic Space. In Document Understanding Con-
ference (DUC).
Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC?s
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. In Document Understanding
Conference (DUC).
Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and
S. Ye. 2007. NUS at DUC 2007: Using Evolu-
tionary Models of Text. In Document Understanding
Conference (DUC).
Lin, C.Y. 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Workshop on Text Sum-
marization Branches Out, pages 25?26.
Mani, I. and M.T. Maybury. 1999. Advances in Auto-
matic Text Summarization. MIT Press.
Mani, I. and G. Wilson. 2000. Robust temporal pro-
cessing of news. In 38th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 69?76.
Association for Computational Linguistics Morris-
town, NJ, USA.
Murray, G., S. Renals, and J. Carletta. 2005. Extractive
Summarization of Meeting Recordings. In Ninth Eu-
ropean Conference on Speech Communication and
Technology. ISCA.
Salton, G., A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18(11):613?620.
Swan, R. and J. Allan. 2000. Automatic generation
of overview timelines. In 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 49?56.
Winkler, W. E. 1999. The state of record linkage and
current research problems. In Survey Methods Sec-
tion, pages 73?79.
Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
ing Update Summaries for DUC 2007. In Document
Understanding Conference (DUC).
Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS
at DUC 2005: Understanding documents via con-
cept links. In Document Understanding Conference
(DUC).
26
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 108?115,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Positional Language Models for Clinical Information Retrieval
Florian Boudin
DIRO, Universite? de Montre?al
CP. 6128, succ. Centre-ville
H3C 3J7 Montre?al, Canada
boudinfl@iro.umontreal.ca
Jian-Yun Nie
DIRO, Universite? de Montre?al
CP. 6128, succ. Centre-ville
H3C 3J7 Montre?al, Canada
nie@iro.umontreal.ca
Martin Dawes
Department of Family Medicine
McGill University, 515 Pine Ave
H2W 1S4 Montre?al, Canada
martin.dawes@mcgill.ca
Abstract
The PECO framework is a knowledge repre-
sentation for formulating clinical questions.
Queries are decomposed into four aspects,
which are Patient-Problem (P), Exposure (E),
Comparison (C) and Outcome (O). However,
no test collection is available to evaluate such
framework in information retrieval. In this
work, we first present the construction of a
large test collection extracted from system-
atic literature reviews. We then describe an
analysis of the distribution of PECO elements
throughout the relevant documents and pro-
pose a language modeling approach that uses
these distributions as a weighting strategy. In
our experiments carried out on a collection of
1.5 million documents and 423 queries, our
method was found to lead to an improvement
of 28% in MAP and 50% in P@5, as com-
pared to the state-of-the-art method.
1 Introduction
In recent years, the volume of health and biomedi-
cal literature available in electronic form has grown
exponentially. MEDLINE, the authoritative reposi-
tory of citations from the medical and bio-medical
domain, contains more than 18 million citations.
Searching for clinically relevant information within
this large amount of data is a difficult task that med-
ical professionals are often unable to complete in a
timely manner. A better access to clinical evidence
represents a high impact application for physicians.
Evidence-Based Medicine (EBM) is a widely ac-
cepted paradigm for medical practice (Sackett et al,
1996). EBM is defined as the conscientious, explicit
and judicious use of current best evidence in making
decisions about patient care. Practice EBM means
integrating individual clinical expertise with the best
available external clinical evidence from systematic
research. It involves tracking down the best evi-
dence from randomized trials or meta-analyses with
which to answer clinical questions. Richardson et
al. (1995) identified the following four aspects as the
key elements of a well-built clinical question:
? Patient-problem: what are the patient charac-
teristics (e.g. age range, gender, etc.)? What is
the primary condition or disease?
? Exposure-intervention: what is the main in-
tervention (e.g. drug, treatment, duration, etc.)?
? Comparison: what is the exposure compared
to (e.g. placebo, another drug, etc.)?
? Outcome: what are the clinical outcomes (e.g.
healing, morbidity, side effects, etc.)?
These elements are known as the PECO elements.
Physicians are educated to formulate their clinical
questions in respect to this structure. For example, in
the following question: ?In patients of all ages with
Parkinson?s disease, does a Treadmill training com-
pared to no training allows to increase the walking
distance?? one can identify the following elements:
? P: Patients of all ages with Parkinson?s disease
? E: Treadmill training
? C: No treadmill training
? O: Walking distance
In spite of this well-defined question structure,
physicians still use keyword-based queries when
they search for clinical evidence. An explanation of
108
that is the almost total absence of PECO search in-
terfaces. PubMed1, the most used search interface,
does not allow users to formulate PECO queries
yet. For the previously mentioned clinical question,
a physician would use the query ?Treadmill AND
Parkinson?s disease?. There is intuitively much to
gain by using a PECO structured query in the re-
trieval process. This structure specifies the role of
each concept in the desired documents, which is
a clear advantage over a keyword-based approach.
One can for example differentiate two queries in
which a disease would be a patient condition or a
clinical outcome. This conceptual decomposition of
queries is also particularly useful in a sense that it
can be used to balance the importance of each ele-
ment in the search process.
Another important factor that prevented re-
searchers from testing approaches to clinical infor-
mation retrieval (IR) based on PECO elements is
the lack of a test collection, which contains a set of
documents, a set of queries and the relevance judg-
ments. The construction of such a test collection is
costly in manpower. In this paper, we take advan-
tage of the systematic reviews about clinical ques-
tions from Cochrane. Each Cochrane review ex-
amines in depth a clinical question and survey all
the available relevant publications. The reviews are
written for medical professionals. We transformed
them into a TREC-like test collection, which con-
tains 423 queries and 8926 relevant documents ex-
tracted from MEDLINE. In a second part of this pa-
per, we present a model integrating the PECO frame-
work in a language modeling approach to IR. An in-
tuitive method would try to annotate the concepts
in documents into PECO categories. One can then
match the PECO elements in the query to the ele-
ments detected in documents. However, as previous
studies have shown, it is very difficult to automat-
ically annotate accurately PECO elements in docu-
ments. To by-pass this issue, we propose an alter-
native that relies on the observed positional distri-
bution of these elements in documents. We will see
that different types of element have different distri-
butions. By weighting words according to their posi-
tions, we can indirectly weigh the importance of dif-
ferent types of element in search. As we will show
1www.pubmed.gov
in this paper, this approach turns out to be highly
effective.
This paper is organized as follows. We first briefly
review the previous work, followed by a description
of the test collection we have constructed. Next,
we give the details of the method we propose and
present our experiments and results. Lastly, we con-
clude with a discussion and directions for further
work.
2 Related work
The need to answer clinical questions related to a
patient care using IR systems has been well stud-
ied and documented (Hersh et al, 2000; Niu et al,
2003; Pluye et al, 2005). There are a limited but
growing number of studies trying to use the PECO
elements in the retrieval process. (Demner-Fushman
and Lin, 2007) is one of the few such studies, in
which a series of knowledge extractors is used to
detect PECO elements in documents. These ele-
ments are later used to re-rank a list of retrieved ci-
tations from PubMed. Results reported indicate that
their method can bring relevant citations into higher-
ranking positions, and from these abstracts gener-
ate responses that answer clinicians? questions. This
study demonstrates the value of the PECO frame-
work as a method for structuring clinical questions.
However, as the focus has been put on the post-
retrieval step (for question-answering), it is not clear
whether PECO elements are useful at the retrieval
step. Intuitively, the integration of PECO elements
in the retrieval process can also lead to higher re-
trieval effectiveness.
The most obvious scenario for testing this would
be to recognize PECO elements in documents prior
to indexing. When a PECO-structured query is for-
mulated, it is matched against the PECO elements
in the documents (Dawes et al, 2007). Neverthe-
less, the task of automatically identifying PECO el-
ements is a very difficult one. There are two major
reasons for that. First, previous studies have indi-
cated that there is a low to moderate agreement rate
among humans for annotating PECO elements. This
is due to the lack of standard definition for the el-
ement? boundaries (e.g. can be words, phrases or
sentences) but also to the existence of several lev-
els of annotation. Indeed, there are a high number
109
of possible candidates for each element and one has
to choose if it is a main element (i.e. playing a ma-
jor role in the clinical study) or secondary elements.
Second is the lack of sufficient annotated data that
can be used to train automatic tagging tools.
Despite all these difficulties, several efficient
detection methods have been proposed (Demner-
Fushman and Lin, 2007; Chung, 2009). Nearly all
of them are however restricted to a coarse-grain an-
notation level (i.e. tagging entire sentences as de-
scribing one element). This kind of coarser-grain
identification is more robust and more feasible than
the one at concept level, and it could be sufficient in
the context of IR. In fact, for IR purposes, what is
the most important is to correctly weight the words
in documents and queries. From this perspective,
an annotation at the sentence level may be suffi-
cient. Notwithstanding, experiments conducted us-
ing a collection of documents that were annotated at
a sentence-level only showed a small increase in re-
trieval accuracy (Boudin et al, 2010b) compared to
a traditional bag-of-words approach.
More recently, Boudin et al (2010a) proposed an
alternative to the PECO detection issue that relies
on assigning different weights to words according to
their positions in the document. A location-based
weighting strategy is used to emphasize the most
informative parts of documents. They show that
a large improvement in retrieval effectiveness can
be obtained this way and indicate that the weights
learned automatically are correlated to the observed
distribution of PECO elements in documents. In this
work, we propose to go one step further in this direc-
tion by analyzing the distribution of PECO elements
in a large number of documents and define the posi-
tional probabilities of PECO elements accordingly.
These probabilities will be integrated in the docu-
ment language model.
3 Construction of the test collection
Despite the increasing use of search engines by med-
ical professionals, there is no standard test collection
for evaluating clinical IR. Constructing such a re-
source from scratch would require considerable time
and money. One way to overcome this obstacle is
to use already available systematic reviews. Sys-
tematic reviews try to identify, appraise, select and
synthesize all high quality research evidence rele-
vant to a clinical question. The best-known source
of systematic reviews in the healthcare domain is the
Cochrane collaboration2. It consists of a group of
over 15,000 specialists who systematically identify
and review randomized trials of the effects of treat-
ments. In particular, a review contains a reference
section, listing all the relevant studies to the clinical
question. These references can be considered as rel-
evant documents. In our work, we propose to use
these reviews as a way to semi-automatically build a
test collection. As the reviews are made by special-
ists in the area independently from our study, we can
avoid bias in our test collection.
We gathered a subset of Cochrane systematic re-
views and asked a group of annotators, one professor
and four Master students in family medicine, to cre-
ate PECO-structured queries corresponding to the
clinical questions. As clinical questions answered
in these reviews cover various aspects of one topic,
multiple variants of precise PECO queries were gen-
erated for each review. Moreover, in order to be able
to compare a PECO-based search strategy to a real
world scenario, this group have also provided the
keyword-based queries that they would have used
to search with PubMed. Below is an example of
queries generated from the systematic review about
?Aspirin with or without an antiemetic for acute mi-
graine headaches in adults?:
Keyword-based query
[aspirin and migraine]
PECO-structured queries
1. [adults 18 years or more with migraine]P
[aspirin alone]E
[placebo]C
[pain free]O
2. [adults 18 years or more with migraine]P
[aspirin plus an antiemetic]E
[placebo]C
[pain free]O
3. [adults 18 years or more with migraine]P
[aspirin plus metoclopramide]E
[active comparator]C
[use of rescue medication]O
2www.cochrane.org
110
All the citations included in the ?References? sec-
tion of the systematic review were extracted and
selected as relevant documents. These citations
were manually mapped to PubMed unique identi-
fiers (PMID). This is a long process that was under-
taken by two different workers to minimize the num-
ber of errors. At this step, only articles published in
journals referenced in PubMed are considered (e.g.
conference proceedings are not included).
0 20 40 60 80 100 120
Number of references in each review
0
5
10
15
20
25
Nu
mb
er
 of
 sy
ste
ma
tic
 re
vie
ws
Figure 1: Histogram of the number of queries versus the
number of relevant documents.
We selected in sequential order from the set
of new systematic reviews3 and processed 156
Cochrane reviews. There was no restriction about
the topics covered or the number of included refer-
ences. The resulting test collection is composed of
423 queries and 8926 relevant citations (2596 differ-
ent citations). This number reduces to 8138 citations
once we remove the citations without any text in the
abstract (i.e. certain citations, especially old ones,
only contain a title). Figure 1 shows the statistics
derived from the number of relevant documents by
query. In this test collection, the average number of
documents per query is approximately 19 while the
average length of a document is 246 words.
4 Distribution of PECO elements
The observation that PECO elements are not evenly
distributed throughout the documents is not new. In
fact, most existing tagging methods used location-
based features. This information turns out to be very
useful because of the standard structure of medical
citations. Actually, many scientific journals explic-
itly recommend authors to write their abstracts in
3http://mrw.interscience.wiley.com/
cochrane/cochrane clsysrev new fs.html
compliance to the ordered rhetorical structure: In-
troduction, Methods, Results and Discussion. These
rhetorical categories are highly correlated to the dis-
tributions of PECO elements, as some elements are
more likely to occur in certain categories (e.g. clin-
ical outcomes are more likely to appear in the con-
clusion). The position is thus a strong indicator of
whether a text segment contains a PECO element or
not.
To the best of our knowledge, the first analysis
of the distribution of PECO elements in documents
was described in(Boudin et al, 2010a). A small col-
lection of manually annotated abstracts was used to
compute the probability that a PECO element oc-
curs in a specific part of the documents. This study
is however limited by the small number of anno-
tated documents (approximately 50 citations) and
the moderate agreement rate among human annota-
tors. Here we propose to use our test collection to
compute more reliable statistics.
The idea is to use the pairs of PECO-structured
query and relevant document, assuming that if a doc-
ument is relevant then it should contain the same
elements as the query. Of course, this is obvi-
ously not always the case. Errors can be introduced
by synonyms or homonyms and relevant documents
may not contain all of the elements described in the
query. But, with more than 8100 documents, it is
quite safe to say that this method produce fairly reli-
able results. Moreover, a filtering process is applied
to queries removing all non-informative words (e.g.
stopwords, numbers, etc.) from being counted.
There are several ways to look at the distribution
of PECO elements in documents. One can use the
rhetorical structure of abstracts to do that. However,
the high granularity level of such analysis would
make it less precise for IR purposes. Furthermore,
most of the citations available in PubMed are de-
void of explicitly marked sections. It is possible to
automatically detect these sections but only with a
non-negligible error rate (McKnight and Srinivasan,
2003). In our study, we chose to use a fixed num-
ber of partitions by dividing documents into parts of
equal length. This choice is motivated by its repeata-
bility and ease to implement, but also for compari-
son with previous studies.
We divided each relevant document into 10 parts
of equal length on a word level (from P1 to P10). We
111
computed statistics on the number of query words
that occur in each of these parts. For each PECO el-
ement, the distribution of query words among the
parts of the documents is not uniform (Figure 2).
We observe distinctive distributions, especially for
Patient-Problem and Exposure elements, indicating
that first and last parts of the documents have higher
chance to contain these elements. This gives us a
clear and robust indication on which specific parts
should be enhanced when searching for a given el-
ement. Our proposed model will exploit the typical
distributions of PECO elements in documents.
P1 P2 P3 P4 P5 P6 P7 P8 P9 P100.00
0.05
0.10
0.15
0.20
0.25 P elements
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10
E elements
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10                                           Parts of the documents0.00
0.05
0.10
0.15
0.20
0.25
      
      
      
      
      
      
 Pro
por
tion
 of P
ECO
 ele
men
ts in
 par
t
C elements
P1 P2 P3 P4 P5 P6 P7 P8 P9 P10
O elements
Figure 2: Distribution of each PECO element throughout
the different parts of the documents.
5 Retrieval Method
In this work, we use the language modeling ap-
proach to information retrieval. This approach as-
sumes that queries and documents are generated
from some probability distribution of text (Ponte and
Croft, 1998). Under this assumption, ranking a doc-
ument D as relevant to a query Q is seen as estimat-
ing P(Q|D), the probability thatQwas generated by
the same distribution as D. A typical way to score
a document D as relevant to a query Q is to com-
pute the Kullback-Leibler divergence between their
respective language models:
score(Q,D) =
?
w?Q
P(w|Q) ? logP(w|D) (1)
Under the traditional bag-of-words assumption,
i.e. assuming that there is no need to model term de-
pendence, a simple estimate for P(w|Q) can be ob-
tained by computing Maximum Likelihood Estima-
tion (MLE). It is calculated as the number of times
the word w appears in the query Q, divided by its
length:
P(w|Q) =
count(w,Q)
|Q|
A similar method is employed for estimating
P(w|D). Bayesian smoothing using Dirichlet pri-
ors is however applied to the maximum likelihood
estimator to compensate for data sparseness (i.e.
smoothing probabilities to remove zero estimates).
Given ? the prior parameter and C the collection of
documents, P(w|D) is computed as:
P(w|D) =
count(w,D) + ? ? P(w|C)
|D| + ?
5.1 Model definition
In our model, we propose to use the distribution of
PECO elements observed in documents to empha-
size the most informative parts of the documents.
The idea is to get rid of the problem of precisely
detecting PECO elements by using a positional lan-
guage model. To integrate position, we estimate
a series of probabilities that constraints the word
counts to a specific part of the documents instead of
the entire document. Each document D is ranked by
a weighted linear interpolation. Given a document
D divided in 10 parts p ? [P1, P2 ? ? ?P10], P(w|D)
in equation 1 is redefined as:
P ?(w|D) = ? ? P(w|D) + ? ? Ptitle(w|D)
+? ?
?
pi?D
?e ? Ppi(w|D) (2)
where the ?e weights for each type of element e
are empirically fixed to the values of the distribution
of PECO elements observed in documents. We then
redefine the scoring function to integrate the PECO
query formulation. The idea is to use the PECO
structure as a way to balance the importance of each
element in the retrieval step. The final scoring func-
tion is defined as:
scorefinal(Q,D) =
?
e?PECO
?e ? score(Qe, D)
112
In our model, there are a total of 7 weighting pa-
rameters, 4 corresponding to the PECO elements in
queries (?P, ?E, ?C and ?O) and 3 for the document
language models (?, ? and ?). These parameters
will be determined by cross-validation.
6 Results
In this section, we first describe the details of our
experimental protocol. Then, we present the results
obtained by our model on the constructed test col-
lection.
6.1 Experimental settings
As a collection of documents, we gathered 1.5 mil-
lions of citations from PubMed. We used the fol-
lowing constraints: citations with an abstract, hu-
man subjects, and belonging to one of the follow-
ing publication types: randomized control trials, re-
views, clinical trials, letters, editorials and meta-
analyses. The set of queries and relevance judg-
ments described in Section 3 is used to evaluate
our model. Relevant documents were, if not al-
ready included, added to the collection. Because
each query is generated from a systematic literature
review completed at a time t, we placed an addi-
tional restriction on the publication date of the re-
trieved documents: only documents published be-
fore time t are considered. Before indexing, each
citation is pre-processed to extract its title and ab-
stract text and then converted into a TREC-like doc-
ument format. Abstracts are divided into 10 parts of
equal length (the ones containing less than 10 words
are discarded). The following fields are marked in
each document: title, P1, P2 ? ? ? P10. The following
evaluation measures are used:
? Precision at rank n (P@n): precision computed
on the n topmost retrieved documents.
? Mean Average Precision (MAP): average of
precision measures computed at the point of
each relevant document in the ranked list.
? Number of relevant documents retrieved
All retrieval tasks are performed using an ?out-
of-the-shelf? version of the Lemur toolkit4. We use
the embedded tokenization algorithm along with the
4www.lemurproject.org
standard Porter stemmer. The number of retrieved
documents is set to 1000 and the Dirichlet prior
smoothing parameter to ? = 2000. In all our exper-
iments, we use the KL divergence scoring function
(equation 1) as baseline. Statistical significance is
computed using the well-known Student?s t-test. To
determine reasonable weights and avoid overtuning
the parameters, we use a 10-fold cross-validation op-
timizing the MAP values.
6.2 Experiments
We first investigated the impact of using PECO-
structured queries on the retrieval performance. As
far as we know, no quantitative evaluation of the
increase or decrease of performance in comparison
with a keyword-based search strategy has been re-
ported. Schardt et al (2007) presented a compari-
son between PubMed and a PECO search interface
but failed to demonstrate any significant difference
between the two search protocols. The larger num-
ber of words in PECO-structured queries, on aver-
age 18.8 words per query compared to 4.3 words for
keyword queries, should capture more aspects of the
information need. But, it may also be a disadvan-
tage due to the fact that more noise can be brought
in, causing query-drift issues.
We propose two baselines using the keyword-
based queries. The first baseline (named Baseline-
1) uses keyword queries with the traditional lan-
guage modeling approach. This is one of the state-
of-the-art approaches in current IR research. This
retrieval model considers each word in a query as
an equal, independent source of information. In the
second baseline (named Baseline-2), we consider
multiword phrases. In our test collection, queries
are often composed of multiword phrases such as
?low back pain? or ?early pregnancy?. It is clear
that finding the exact phrase ?heart failure? is a
much stronger indicator of relevance than just find-
ing ?heart? and ?failure? scattered within a docu-
ment. The Indri operator #1 is used to perform
phrase-based retrieval. Phrases are already indicated
in queries by the conjunction and (e.g. vaccine and
hepatitis B). A simple regular expression is used to
recognize the phrases.
Results are presented in Table 1. As expected,
phrase-based retrieval leads to some increase in re-
trieval precision (P@5). However, the number of
113
relevant documents retrieved is decreased. This is
due to the fact that we use exact phrase matching
that can reduce query coverage. One solution would
be to use unordered window features (Indri operator
#uwn) that would require words to be close together
but not necessarily in an exact sequence order (Met-
zler and Croft, 2005).
The PECO queries use PECO-structured queries
as a bag of words. We observe that PECO queries
do not enhance the average precision but increase
the P@5 significantly. The number of relevant doc-
uments retrieved is also larger. These results indi-
cate that formulating clinical queries according to
the PECO framework enhance the retrieval effec-
tiveness.
Model MAP P@5 #rel. ret.
Baseline-1 0.129 0.151 5369
Baseline-2 0.128 0.161? 4645
PECO-queries 0.126 0.172? 5433
Table 1: Comparing the performance measures of
keyword-based and PECO-structured queries in terms of
MAP, precision at 5 and number of relevant documents
retrieved (#rel. ret.). (?: t.test < 0.05)
In a second series of experiments, we evaluated
the model we proposed in Section 5 . We compared
two variants of our model. The first variant (named
Model-1) uses a global ?e distribution fixed accord-
ing to the average distribution of all PECO elements
(i.e. the observed probability that a PECO element
occurs in a document? part, no matter which element
it is). The second variant (named Model-2) uses a
differentiated ?e distribution for each type of PECO
element. The idea is to see if, given the fact that
PECO elements have different distributions in docu-
ments, using an adapted weight distribution for each
element can improve the retrieval effectiveness.
Previous studies have shown that assigning a dif-
ferent weight to each PECO element in the query
leads to better results (Demner-Fushman and Lin,
2007; Boudin et al, 2010a). In order to compare
our model with a similar method, we defined another
baseline (named Baseline-3) by fixing the parame-
ters ? = 0 and ? = 0 in equation 2. We performed
a grid search (from 0 to 1 by step of 0.1) to find
the optimal ? weights. Regarding the last three pa-
rameters in our full models, namely ?, ? and ?, we
conducted a second grid search to find their optimal
values. Performance measures obtained in 10-fold
cross-validation (optimizing the MAP measure) by
these models are presented in Table 2.
A significant improvement is obtained by
the Baseline-3 over the keyword-based approach
(Baseline-2). The PECO decomposition of queries
is particularly useful to balance the importance of
each element in the scoring function. We observe a
large improvement in retrieval effectiveness for both
models over the two baselines. This strongly indi-
cates that a weighting scheme based on the word po-
sition in documents is effective. These results sup-
port our assumption that the distribution of PECO
elements in documents can be used to weight words
in the document language model.
However, we do not observe meaningful differ-
ences between Model-1 and Model-2. This tend to
suggest that a global distribution is likely more ro-
bust for IR purposes than separate distributions for
each type of element. Another possible reason is that
our direct mapping from positional distribution to
probabilities may not be the most appropriate. One
may think about using a different transformation, or
performing some smoothing. We will leave this for
our future work.
7 Conclusion
This paper first presented the construction of a test
collection for evaluating clinical information re-
trieval. From a set of systematic reviews, a group
of annotators were asked to generate structured clin-
ical queries and collect relevance judgments. The
resulting test collection is composed of 423 queries
and 8926 relevant documents. This test collection
provides a basis for researchers to experiment with
PECO-structured queries in clinical IR. The test col-
lection introduced in this paper, along with the man-
ual given to the group of annotators, will be available
for download5.
In a second step, this paper addressed the prob-
lem of using the PECO framework in clinical IR. A
straightforward idea is to identify PECO elements in
documents and use the elements in the retrieval pro-
cess. However, this approach does not work well be-
5http://www-etud.iro.umontreal.ca/?boudinfl/pecodr/
114
Model MAP % rel. P@5 % rel. #rel. ret.
Baseline-2 0.128 - 0.161 - 4645
Baseline-3 0.144 +12.5%? 0.196 +21.7%? 5780
Model-1 0.164 +28.1%? 0.241 +49.7%? 5768
Model-2 0.163 +27.3%? 0.240 +49.1%? 5770
Table 2: 10-fold cross validation scores for the Baseline-2, Baseline-3 and the two variants of our proposed model
(Model-1 and Model-2). Relative increase over the Baseline-2 is given, #rel. ret. is the number of relevant documents
retrieved. (?: t.test < 0.01, ?: t.test < 0.05)
cause of the difficulty to automatically detect these
elements. Instead, we proposed a less demanding
approach that uses the distribution of PECO ele-
ments in documents to re-weight terms in the doc-
ument model. The observation of variable distribu-
tions in our test collection led us to believe that the
position information can be used as a robust indica-
tor of the presence of a PECO element. This strategy
turns out to be promising. On a data set composed
of 1.5 million citations extracted with PubMed, our
best model obtains an increase of 28% for MAP
and nearly 50% for P@5 over the classical language
modeling approach.
In future work, we intend to expand our analy-
sis of the distribution of PECO elements to a larger
number of citations. One way to do that would
be to automatically extract PubMed citations that
contain structural markers associated to PECO cate-
gories (Chung, 2009).
References
Florian Boudin, Jian-Yun Nie, and Martin Dawes. 2010a.
Clinical Information Retrieval using Document and
PICO Structure. In Proceedings of the HLT-NAACL
2010 conference, pages 822?830.
Florian Boudin, Lixin Shi, and Jian-Yun Nie. 2010b. Im-
proving Medical Information Retrieval with PICO El-
ement Detection. In Proceedings of the ECIR 2010
conference, pages 50?61.
Grace Y. Chung. 2009. Sentence retrieval for abstracts
of randomized controlled trials. BMC Medical Infor-
matics and Decision Making, 9(1).
Thomas Owens Sheri Keitz Connie Schardt, Martha
B Adams and Paul Fontelo. 2007. Utilization of the
PICO framework to improve searching PubMed for
clinical questions. BMC Medical Informatics and De-
cision Making, 7(1).
Martin Dawes, Pierre Pluye, Laura Shea, Roland Grad,
Arlene Greenberg, and Jian-Yun Nie. 2007. The iden-
tification of clinically important elements within med-
ical journal abstracts: PatientPopulationProblem, Ex-
posureIntervention, Comparison, Outcome, Duration
and Results (PECODR). Informatics in Primary care,
15(1):9?16.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statistical
techniques. Computational Linguistics, 33(1):63?103.
William R. Hersh, Katherine Crabtree, David H. Hickam,
Lynetta Sacherek, Linda Rose, and Charles P. Fried-
man. 2000. Factors associated with successful an-
swering of clinical questions using an information re-
trieval system. Bulletin of the Medical Library Asso-
ciation, 88(4):323?331.
Larry McKnight and Padmini Srinivasan. 2003. Catego-
rization of sentence types in medical abstracts. Pro-
ceedings of the AMIA annual symposium.
Donald Metzler and W. Bruce Croft. 2005. A Markov
random field model for term dependencies. In Pro-
ceedings of the SIGIR conference, pages 472?479.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia
Rodriguez-Gianolli. 2003. Answering clinical ques-
tions with role identification. In Proceedings of the
ACL 2003 Workshop on Natural Language Processing
in Biomedicine, pages 73?80.
Pierre Pluye, Roland M. Grad, Lynn G. Dunikowski,
and Randolph Stephenson. 2005. Impact of clinical
information-retrieval technology on physicians: a lit-
erature review of quantitative, qualitative and mixed
methods studies. International Journal of Medical In-
formatics, 74(9):745?768.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the SIGIR conference, pages 275?281.
Scott W. Richardson, Mark C. Wilson, Jim Nishikawa,
and Robert S. Hayward. 1995. The well-built clini-
cal question: a key to evidence-based decisions. ACP
Journal Club, 123(3):A12?13.
David L. Sackett, William Rosenberg, J. A. Muir Gray,
Brian Haynes, and W. Scott Richardson. 1996. Ev-
idence based medicine: what it is and what it isn?t.
British medical journal, 312:71?72.
115
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 822?830,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Clinical Information Retrieval using Document and PICO Structure
Florian Boudin and Jian-Yun Nie
DIRO, Universite? de Montre?al
CP. 6128, succursale Centre-ville
Montre?al, H3C 3J7 Que?bec, Canada
{boudinfl,nie}@iro.umontreal.ca
Martin Dawes
Department of Family Medicine
McGill University, 515 Pine Ave W
Montre?al, H2W 1S4 Que?bec, Canada
martin.dawes@mcgill.ca
Abstract
In evidence-based medicine, clinical questions
involve four aspects: Patient/Problem (P), In-
tervention (I), Comparison (C) and Outcome
(O), known as PICO elements. In this pa-
per we present a method that extends the lan-
guage modeling approach to incorporate both
document structure and PICO query formu-
lation. We present an analysis of the distri-
bution of PICO elements in medical abstracts
that motivates the use of a location-based
weighting strategy. In experiments carried out
on a collection of 1.5 million abstracts, the
method was found to lead to an improvement
of roughly 60% in MAP and 70% in P@10 as
compared to state-of-the-art methods.
1 Introduction
As the volume of published medical literature con-
tinues to grow exponentially, there is more and more
research for physicians to assess and evaluate and
less time to do so. Evidence-based medicine (EBM)
(Sackett et al, 1996) is a widely accepted paradigm
in medical practice that relies on evidence from
patient-centered clinical research to make decisions.
Taking an evidence-based approach to searching
means doing a systematic search of all the available
literature, individually critically appraising each re-
search study and then applying the findings in clini-
cal practice. However, this is a time consuming ac-
tivity. One way to facilitate searching for a precise
answer is to formulate a well-focused and structured
question (Schardt et al, 2007).
Physicians are educated to formulate their clinical
questions according to several well defined aspects
in EBM: Patient/Problem (P), Intervention (I),
Comparison (C) and Outcome (O), which are called
PICO elements. In many documents in medical lit-
erature (e.g. MEDLINE), one can find the elements
of the PICO structure, but rarely explicitly anno-
tated (Dawes et al, 2007). To identify documents
corresponding to a patient?s state, physicians also
construct their queries according to the PICO struc-
ture. For example, in the question ?In children with
pain and fever how does paracetamol compared
with ibuprofen affect levels of pain and fever?? one
can identify the following PICO elements:
Patient/Problem: children/pain and fever
Intervention: paracetamol
Comparison: ibuprofen
Outcome: levels of pain and fever
Very little work, if any, has been carried out on the
use of these elements in the Information Retrieval
(IR) process. There are several reasons for that. It
is not easy to identify PICO elements in documents,
as well as in the question if these are not explicitly
separated in it. Several studies have been performed
on identifying PICO elements in abstracts (Demner-
Fushman and Lin, 2007; Hansen et al, 2008; Chung,
2009). However, all of them are reporting coarse-
grain (sentence-level) tagging methods that have not
yet been shown to be sufficient for the purpose of
IR. Moreover, there is currently no standard test col-
lection of questions in PICO structure available for
evaluation. On the other hand, the most critical as-
pect in IR is term weighting. One of the purpose
of tagging PICO elements is to assign appropriate
weights to these elements during the retrieval pro-
cess. From this perspective, a semantic tagging of
PICO elements may be a task that goes well beyond
822
that is required for IR. It may be sufficient to have
a method that assigns appropriate weights to ele-
ments rather than recognizing their semantic roles.
In this paper, we will propose an approach to deter-
mine term weights according to document structure.
This method will be compared to that using tagging
of PICO elements.
In this paper, we first report an attempt to manu-
ally annotate the PICO elements in documents by
physicians and use them as training data to build
an automatic tagging tool. It turns out that there
is a high disagreement rate between human anno-
tators. The utilization of the automatic tagging tool
in an IR experiment shows only a small gain in re-
trieval effectiveness. We therefore propose an alter-
native to PICO element detection that uses the struc-
tural information of documents. This solution turns
out to be robust and effective. The alternative ap-
proach is motivated by a strong trend that we ob-
serve in the distribution of PICO elements in docu-
ments. We then make use of both PICO query and
document structure to extend the classical language
modeling approach to IR. Specifically, we investi-
gate how each element of a PICO query should be
weighted and how a location-based weighting strat-
egy can be used to emphasize the most informative
parts (i.e. containing the most PICO elements) of
documents.
The paper is organized as follows. We first briefly
review the previous work, followed by a description
of the method we propose. Next, we present our
experiments and results. Lastly, we conclude with a
discussion and directions for future work.
2 Related work
There have been only a few studies trying to use
PICO elements in the retrieval process. (Demner-
Fushman and Lin, 2007) is one of the few such stud-
ies. The method they describe consists in re-ranking
an initial list of retrieved citations. To this end, the
relevance of a document is scored by the use of de-
tected PICO elements, among other things. Several
other studies aimed to build a Question-Answering
system for clinical questions (Demner-Fushman and
Lin, 2006; Andrenucci, 2008). But again, the focus
has been set on the post-retrieval step, while the doc-
ument retrieval step only uses a standard approach.
In this paper, we argue that IR has much to gain by
using PICO elements.
The task of identifying PICO elements has how-
ever gain more attention. In their paper, (Demner-
Fushman and Lin, 2007) presented a method that
uses either manually crafted pattern-matching rules
or a combination of basic classifiers to detect PICO
elements in medical abstracts. Prior to that, biomed-
ical concepts are labelled by Metamap (Aronson,
2001) while relations between these concepts are
extracted with SemRep (Rindflesch and Fiszman,
2003). Recently, supervised classification using
Support Vector Machines (SVM) was proposed by
(Hansen et al, 2008) to extract the number of trial
participants. In a later study, (Chung, 2009) ex-
tended this work to other elements using Conditional
Random Fields. Although these studies are report-
ing interesting results, they are limited in several as-
pects. First, many are restricted to some segments
of the medical documents (e.g. Method section)
(Chung, 2009), and in most cases, the test collection
is very small (a few hundreds abstracts). Second, the
precision and granularity of these methods have not
yet been shown to be sufficient for the purpose of IR.
The structural information provided by markup
languages (e.g. XML) has been successfully used
to improve the IR effectiveness (INEX, 2002 2009).
For such documents, the structure information can
be used to emphasize some particular parts of the
document. Thereby, a given word should not have
the same importance depending on its position in the
document structure.
Taking into account the structure can be done ei-
ther at the step of querying or at the step of index-
ing. One way to integrate the structure at querying
is to adapt query languages (Fuhr and Gro?johann,
2001). These approaches follow the assumption that
the user knows where the most relevant information
is located. However, (Kamps et al, 2005) showed
that it is preferable to use structure as a search hint,
and not as a strict search requirement
The second approach consists in integrating the
document structure at the indexing step by introduc-
ing a structure weighting scheme (Wilkinson, 1994).
In such a scheme, the weight assigned to a word is
not only based on its frequency but also on its posi-
tion in the document. The structure of a document
can be defined in terms of tags (e.g. title, section),
823
each of those having a weight chosen either empiri-
cally or automatically by the use of optimizing tech-
niques such as genetic algorithms (Trotman, 2005).
3 Using PICO elements in retrieval
In this section, we present an experiment on the
manual annotation of PICO elements. We then de-
scribe an approach to detect these elements in doc-
uments and give some results on the use of these
tagged elements in the retrieval process.
3.1 Manual annotation of PICO elements
We asked medical professionals to manually anno-
tate the PICO elements in a small collection of ab-
stracts from PubMed1. The instructions given to
the annotators were fairly simple. They were asked
to precisely annotate all PICO elements in abstracts
with no restriction about the size of the elements (i.e.
they could be words, phrases or sentences). More
than 50 abstracts were manually annotated this way
by at least two different annotators. Two annotations
by two annotators are considered to agree if they
share some words (i.e. they overlap). We computed
the well known Cohen?s kappa measure as well as an
ad-hoc measure called loose. The latter uses PICO
elements as units and estimates the proportion of el-
ements that have been annotated by both raters.
Measure P-element I/C-element O-element
kappa 0.687 0.539 0.523
loose 0.363 0.136 0.140
Table 1: Agreement measures computed for each ele-
ment. Cohen?s kappa and loose agreement are presented.
We can observe that there is a very low agree-
ment rate between human annotators. The loose
measure indicates that less than 15% of the I, C and
O elements have been marked by both annotators.
This fact shows that such human annotations can be
hardly used to develop an automatic tagging tool for
PICO elements, which requires consistent training
data. We therefore try to develop a coarser-grained
tagging method.
1www.pubmed.gov, PubMed is a service of the US Na-
tional Library of Medicine that includes over 19 million cita-
tions from MEDLINE and other life science journals.
3.2 Automatic detection of PICO elements
Similarly to previous work, we propose a sentence-
level detection method. The identification of PICO
elements can be seen as a classification task. Even
for a coarser-grain classification task, we are still
lack of annotated data. One solution is to use the
structural information embedded in some medical
abstracts for which the authors have clearly stated
distinctive sentence headings. Some recent ab-
stracts in PubMed do contain explicit headings such
as ?PATIENTS?, ?SAMPLE? or ?OUTCOMES?,
that can be used to locate sentences correspond-
ing to PICO elements. Using that information, we
extracted three sets of abstracts: Patient/Problem
(14 279 abstracts), Intervention/Comparison (9 095)
and Outcome (2 394).
Tagging each document goes through a three steps
process. First, the document is segmented into plain
sentences. Then each sentence is converted into a
feature vector using statistical (e.g. position, length)
and knowledge-based features (e.g. MeSH semantic
type). Knowledge-based features were derived ei-
ther from manually crafted cue-words/verbs lists or
semantic types within the MeSH ontology2. Finally,
each vector is submitted to multiple classifiers, one
for each element, allowing to label the correspond-
ing sentence. We use several algorithms imple-
mented in the Weka toolkit3: decision trees, SVM,
multi-layer perceptron and Naive Bayes. Combin-
ing multiple classifiers using a weighted linear com-
bination of their prediction scores achieves the best
results with a f-measure score of 86.3% for P, 67%
for I/C and 56.6% for O in 10-fold cross-validation.
3.3 Use of detected elements in IR
We use language modeling approach to IR in this
work. The idea is that a document is a good match to
a query if its language model is likely to generate the
query (Ponte and Croft, 1998). It is one of the state-
of-the-art approaches in current IR research. Most
language modeling work in IR use unigram lan-
guage models ?also called bags-of-words models?
assuming that there is no structure in queries or doc-
uments. A typical way to score a document d as
relevant to a query q is to use the Kullback-Leibler
2www.nlm.nih.gov/mesh/
3www.cs.waikato.ac.nz/ml/index.html
824
divergence between their respective LMs:
score(q, d) =
?
w?q
P(w | Mq) ? log P(w | Md) (1)
? ?KL(Mq || Md)
whereMq is the LM of the query andMd the LM of
the document. P(w | M?) estimates the probability
of the word w given the language model M?. The
most direct way to estimate these models is to use
Maximum Likelihood estimation over the words:
P(w | M?) =
count(w, ?)
| ? |
where ? is the observed document, count(w, ?) the
number of times the wordw occurs in ? and | ? | the
length of the document. Bayesian smoothing using
Dirichlet priors is then applied to the maximum like-
lihood estimator to compensate for data sparseness.
We propose an approach that extend the basic
LM approach to take into consideration the PICO
element annotation. We assume that each ele-
ment in the document has a different importance
weight. Four more LMs are created, one for each
elements. Given?e the weight of the PICO element
e, P(w | Md) in equation 1 is re-defined as:
P1(w | Md) ? P(w | Md) +
?
e?[P,I,C,O]
?e ? P(w | Me)
The right hand of the above equation is not a prob-
ability function. We could use a normalization to
transform it. However, for the purpose of document
ranking, this will not make any difference. There-
fore, we will keep the un-normalized value.
We performed an extensive series of experiments
using this model on the test collection described in
Section 5. The results are shown in Table 2. It turns
out that the best improvement we were able to obtain
is very small (0.5% of MAP increase). There may
be several reasons for that. First, the accuracy of
the automatic document tagging may be insufficient.
Second, even if elements are correctly identified in
documents, if queries are treated as bags-of-words
then any PICO element can match with any identi-
cal word in the query, whether it describe the same
element or not. However, we also tested a na??ve ap-
proach that matches the PICO elements in queries
with the corresponding elements in documents. But
this approach quickly turns out to be too restrictive
and leads to bad results.
Measure
Weighted elements
P I / C O Best?
MAP increase 0.0% ?0.2% ?0.1% +0.5%
Table 2: Results using the PICO elements automatically
detected in documents (?: wP = 0.5, wI = 0.2).
As we can see, this approach only brings limited
improvement in retrieval effectiveness. This rises
the question of the usability of such tagging method
in its current performance state. We will see in the
next section an alternative solution to this problem
that relies on the distribution of PICO elements in
documents.
4 Method
4.1 Distribution of PICO elements
PICO elements are not evenly distributed in medical
documents, which often follow some implicit writ-
ing convention. An intuitive method is to weight
higher a segment that is more probable to con-
tain PICO elements. The distribution of PICO el-
ements is likely to correlate to the position within
the document. This intuition has been used in most
of the supervised PICO detection methods which
use location-based features. There has been sev-
eral studies that cover the PICO extraction problem.
However, as far as we know, none of them analyses
and uses the positional ditribution of these elements
within the documents for the purpose of IR. Biomed-
ical abstracts can be typically represented by four or-
dered rhetorical categories which are Introduction,
Methods, Results and Discussion (IMRAD) (Sollaci
and Pereira, 2004). The reason is found in the need
for speed when reviewing literature, as this format
allows readers to pick those parts of particular in-
terest. Besides, many scientific journals explicitly
recommended this ordered structure.
The PICO dispersion is highly correlated to these
rhetorical categories as some elements are more
likely to occur in certain categories. For example,
outcomes are more likely to appear in Results and/or
Discussion parts. One could also expect to infer the
825
role played by PICO elements in a clinical study. For
example, the drug pioglitazone has not the same role
in a clinical study if it appears as the main interven-
tion (likely to occur in all parts) or as a comparative
treatment (Methods and/or Results parts).
Instead of analysing the dispersion of PICO ele-
ments into the four IMRAD categories, we choose to
the use automatically splitted parts. There are sev-
eral reasons for that. First, the IMRAD categories
are not explicitely marked in abstracts. An auto-
matic tagging of these would surely result in some
errors. Second, using a low granularity approach
would provide more precise statistics. Furthermore,
if one would use the dispersion of elements as a cri-
terion to estimate how important each part is, an au-
tomatic partition would be a good choice because of
its repeatability and ease to implement.
We divided each manually annotated abstract into
10 parts of equal length (P1 being the begining and
P10 the ending) and computed statistics on the num-
ber of elements than occur in each of these parts.
The Figure 1 shows the proportion of elements for
each part. We can observe that PICO elements are
not evenly distributed throughout the abstracts. Uni-
versally accepted rules that govern medical writing
styles would be the first reason for that. It is clear
that the beginning and ending parts of abstracts do
contain most of the PICO elements. This gives us a
clear indication on which parts should be enhanced
when searching for these elements.
8 9 10 11 12 13% of PICO elements
P10
P9
P8
P7
P6
P5
P4
P3
P2
P1
Par
ts o
f th
e ab
stra
cts
Figure 1: Proportion of PICO elements computed for
each different part of abstracts.
Therefore, there may be several levels of granu-
larity when using the PICO framework in IR. One
can identify each PICO element in the document,
whether it is described by a word, a phrase or a com-
plete sentence. One can also use a coarser-grain
approach, estimating from the distribution across
documents the probability that each part contains a
PICO element. As attempts to precisely locate PICO
elements have shown that this task is particularly
difficult, we propose to get rid this issue by using
the second method.
4.2 Model definitions
We propose three different models that extend the
classical language modeling approach. The first uses
the structural information of documents, the second
takes advantage of the PICO query structure while
the third simply combine the first two models.
Model-1
Attempts to precisely locate PICO elements in doc-
uments have shown that this task is particularly dif-
ficult. We propose to get around this issue by intro-
ducing structural markers to convey document struc-
ture and use them as a means of providing location
information. Accordingly, each document is repre-
sented as a series of successive parts. To integrate
document structure into the ranking function, we es-
timate a series of probabilities that constraints the
word counts to a specific part instead of the entire
document. Each document d is then ranked by a
weighted linear interpolation. Intuitively, the weight
of a part should depend on how much information is
conveyed by its words. Given ?p the weight of the
part p ? [TITLE, P1 ? ? ? P10], P(w | Md) in equation
1 is re-defined as:
P2(w |Md) ? P(w |Md)+
?
p?d
?p ?P(w ? p |Md)
Model-2
The PICO formulation of queries provides informa-
tion about the role of each query word. One idea
is to use this structural decomposition to thoroughly
balance elements in the ranking function. For exam-
ple, the weight given to the drug fluoxetine should be
different depending on whether it refers to the inter-
vention or comparison concept. The same goes for
obesity which can be a problem or an outcome. To
826
integrate this in the ranking function, we define a pa-
rameter ?e that represents the weight given to query
words belonging to the element e ? [P, I, C, O].
f(w, e) = 1 if w ? e, 0 otherwise. We re-defined
P(w | Md) in equation 1 as:
P3(w |Mq) ? P(w |Mq)+
?
e?[P,I,C,O]
?e ?f(w, e)?P(w |Mq)
Model-1+2
This is the combination of the two previously de-
scribed models. We re-defined the scoring function
as:
score(q, d) =
?
w?q
P3(w | Mq) ? log P2(w | Md)
5 Experiments
In this section, we describe the details of our exper-
imental protocol. We then present the results ob-
tained with the three proposed models.
Experimental settings
We gathered a collection of nearly 1.5 million ab-
stracts from PubMed with the following require-
ments: with abstract, humans subjects, in english
and selecting the following publication types: RCT,
reviews, clinical trials, letters, practice guidelines,
editorials and meta-analysis. Prior to the index con-
struction, each abstract is automatically divided into
10 parts of equal length, abstracts containing less
than 10 words are discarded. The following fields
are then marked: TITLE, P1, P2, ... P10 with P1 be-
ing the begining of the document and P10 the end-
ing.
Unfortunately, there is no standard test collection
appropriate for testing the use of PICO in IR and
we had to manually create one. For queries, we use
the Cochrane systematic reviews4 on 10 clinical
questions about different aspects of ?diabetes?.
These reviews contain the best available infor-
mation about an healthcare intervention and are
designed to facilitate the choices that doctors face
in health care. All the documents in the ?Included
studies? section are judged to be relevant for the
4www.cochrane.org/reviews/
question. These included studies are selected by
the reviewers (authors of the review article) and
judged to be highly related to the clinical question.
In our experiments, we consider these documents
as relevant ones. From the 10 selected questions,
professors in family medicine have formulated a set
of 52 queries, each of which was manually anno-
tated according to the PICO structure. The resulting
testing corpus is composed of 52 queries (average
length of 14.7 words) and 378 relevant documents.
Below are some of the alternative formulations of
queries for the question ?Pioglitazone for type 2
diabetes mellitus?:
In patients with type 2 diabetes (P) | does pioglita-
zone (I) | compared to placebo (C) | reduce stroke
and myocardial infarction (O)
In patients with type 2 diabetes who have a high risk
of macrovascular events (P) | does pioglitazone (I) |
compared to placebo (C) | reduce mortality (O)
We use cross-validation to determine reasonable
weights and avoid over-fitting. We have divided the
queries into two groups of 26 queries: Qa and Qb.
The best parameters found for Qa are used to test
on Qb, and vice versa. In our experiments, we use
the KL divergence ranking (equation 1) as baseline.
The following evaluation measures are considered
relevant:
Precision at n (P@n). Precision computed on only
the n topmost retrieved documents.
Mean Average Precision (MAP). Average of preci-
sions computed at the point of each relevant docu-
ment in the ranked list of retrieved documents.
MAP is a popular measure that gives a global
quality score of the entire ranked list of retrieved
documents. In the case of clinical searches, one
could also imagine this scenario: a search performed
by a physician who does not have the time to look
into large sets of results, but for whom it is impor-
tant to have relevant results in the top 10. In such
case, P@10 is also an appropriate measure.
Student?s t-test is performed to determine statis-
tical significance. The Lemur Toolkit5 was used for
5www.lemurproject.org
827
all retrieval tasks. Experiments were performed with
an ?out-of-the-box? version of Lemur, using its tok-
enization algorithm and porter stemmer. The Dirich-
let prior smoothing parameter was set to its default
value ? = 2500.
Experiments with model-1
We first investigated whether assigning a weight to
each part of the document can improve the retrieval
accuracy. It is however difficult to determine a set
of reasonable values for all the parts together, as the
value of one part will affect those of the others. In
this study, we perform a two pass tuning. First, we
consider the ?p weights to be independent. By doing
so, searching for the optimal weight distribution can
be seen as tuning the weight of each part separately.
When searching the optimal weight of a part, the
weight for other parts is assigned 0. Second, these
approximations of the optimum values are used as
initial weights prior to the second pass. The final
weight distribution is obtained by searching for the
best weight combination around the initial values.
The Figure 2 shows the optimal weight distri-
butions along with the best relative MAP increase
for each part. A noticeable improvement is ob-
tained by increasing the weights associated to the ti-
tle/introduction and conclusion of documents. This
is consistent with the results observed on the dis-
tribution of PICO elements in abstracts. Boosting
middle parts of documents seems to have no impact
at all. We can see that the two ?p weight distribu-
tions (1-pass and 2-pass) are very close.
Performance measures obtained by model-1 are
presented in Table 3. With 1-pass tuning, we ob-
serve a MAP score increase of 37.5% and a P@10
increase of 64.1%. After the second pass, scores are
lower with 35% and 60.5% for MAP and P@10 re-
spectively. This result indicates that there is possibly
overfitting when we perform the two pass parameter
tuning. It could also be caused by the limited num-
ber of query in our test collection. However, we can
determine reasonable weights by tuning each part
weight separately.
Experiments with model-2
We have seen that a large improvement could come
from weighting each part accordingly. In a second
series of experiments, we try to assign a different
10
20
30
0.2
0.4
0.6
0.8
1.0
                            Weight parameter ?p
Q26A1-pass2-pass
Title P1 P2 P3 P4 P5 P6 P7 P8 P9 P100
10
20
30
      
      
      
      
      
     M
AP 
incr
eas
e (%
)
Different part of the documents
0.0
0.2
0.4
0.6
0.8
1.0Q26B1-pass2-pass
Figure 2: Best MAP increase for each part p (bar charts),
corresponding 1 and 2-pass ?p weights are also given.
weight to each PICO element in queries. A grid
search was used to find the optimal ?e weights com-
bination. The results are shown in Table 3.
We observe a MAP score increase of 22.5% and
an increase of 11% in P@10. Though the use of
a PICO weighting scheme increases the retrieval
accuracy, there is clearly much to gain by using
the document structure. The optimal [?p, ?i, ?c, ?o]
weights distribution is [0.3, 1.2, 0, 0.1] for Qa and
[0.2, 1, 0, 0.2] for Qb. That means that the most im-
portant words in queries belong to the Intervention
element. This supports the manual search strategy
proposed by (Weinfeld and Finkelstein, 2005), in
which they suggested that I and P elements should
be used first to construct queries, and only if too
many results are obtained that other elements should
be considered.
It is interesting to see that query words belonging
to the Comparison element have to be considered
as the least important part of a query. Even more
so because they are in the same semantic group as
the Intervention words. A reason for that could be
the use of vague words such as ?no-intervention? or
?placebo?. The methodology employed to construct
the queries is also responsible. Indeed, physicians
have focused on producing alternative formulations
of 10 general clinical questions by predominantly
modifying the one of the PICO elements. As a re-
sult, some of them do share the same vague Com-
parison words.
828
Experiments
MAP P@10
Qb?Qa Qa?Qb % Avg. Qb?Qa Qa?Qb % Avg.
Baseline 0.118 0.131 0.219 0.239
Model-1 / 1pass 0.165 0.176 +37.5%? 0.377 0.373 +64.1%?
Model-1 / 2pass 0.165 0.170 +35.0%? 0.354 0.381 +60.5%?
Model-2 0.149 0.168 +22.5%? 0.250 0.258 +11.0%
Model-1+2 0.198 0.202 +61.5%? 0.385 0.392 +70.0%?
Table 3: Cross-validation (train?test) scores for the baseline (Kullback-Leibler divergence), model-1 with 1 and 2-
pass tuning, model-2 and their combination (model-1+2). Relative increase over the baseline is also given (averaged
between Qa and Qb). (?: t.test < 0.01)
Experiments with model-1+2
We have seen that both the use of a location-based
weighting and a PICO-structure weighting scheme
increase the retrieval accuracy. In this last series of
experiments, we analyse the results of their com-
bination. We can observe that fusing model-1 and
model-2 allows us to obtain the best retrieval ac-
curacy with a MAP score increase of 61.5% and a
P@10 increase of 70.0%. It is a large improvement
over the baseline as it means that instead of about
two relevant documents in the top 10, our system
can retrieve nearly four. These results confirm that
both PICO framework and document structure can
be very helpful for the IR process.
6 Conclusion
We presented a language modeling approach that in-
tegrates document and PICO structure for the pur-
pose of clinical IR. A straightforward idea is to de-
tect PICO elements in documents and use the ele-
ments in the retrieval process. However, this ap-
proach does not work well because of the diffi-
culty to arrive at a consistent tagging of these ele-
ments. Instead, we propose a less demanding ap-
proach which assigns different weights to different
parts of a document.
We first analysed the distribution of PICO el-
ements in a manually annotated abstracts collec-
tion. The observed results led us to believe that a
location-based weighting scheme can be used in-
stead of a PICO detection approach. We then ex-
plored whether this strategy can be used as an in-
dicator to refine document relevance. We also pro-
posed a model to integrate the PICO information
provided in queries and investigated how each el-
ement should be balanced in the ranking function.
On a data set composed of 1.5 million abstracts ex-
tracted from PubMed, our method obtains an in-
crease of 61.5% for MAP and 70% for P@10 over
the classical language modeling approach.
This work can be much improved in the future.
For example, the location-based weighting method
can be improved in order to model a different weight
distribution for each PICO element. As the distri-
bution in abstracts is not the same among PICO el-
ements, it is expected that differentiated weighting
schemes could result in better retrieval effectiveness.
In a similar perspective, we are continuing our ef-
forts to construct a larger manually annotated col-
lection of abstracts. It will be thereafter conceiv-
able to use this data to infer the structural weighting
schemes or to train a more precise PICO detection
method. The focused evaluation described in this
paper is a first step. Although the queries are limited
to diabetes, this does not affect the general PICO
structure in queries. We plan to extend the coverage
of queries to other topics in the future.
Acknowledgements
The work described in this paper was funded by
the Social Sciences and Humanities Research Coun-
cil (SSHRC). The authors would like to thank Dr.
Ann McKibbon, Dr. Dina Demner-Fushman, Lorie
Kloda, Laura Shea, Lucas Baire and Lixin Shi for
their contribution in the project.
829
References
A. Andrenucci. 2008. Automated Question-Answering
Techniques and the Medical Domain. In International
Conference on Health Informatics, volume 2, pages
207?212.
A.R. Aronson. 2001. Effective Mapping of Biomedical
Text to the UMLS Metathesaurus: The MetaMap Pro-
gram. In AMIA Symposium.
G. Chung. 2009. Sentence retrieval for abstracts of ran-
domized controlled trials. BMC Medical Informatics
and Decision Making, 9(1):10.
M. Dawes, P. Pluye, L. Shea, R. Grad, A. Green-
berg, and J.Y. Nie. 2007. The identification of
clinically important elements within medical jour-
nal abstracts: Patient-Population-Problem, Exposure-
Intervention, Comparison, Outcome, Duration and
Results (PECODR). Informatics in Primary care,
15(1):9?16.
D. Demner-Fushman and J. Lin. 2006. Answer extrac-
tion, semantic clustering, and extractive summariza-
tion for clinical question answering. In ACL.
D. Demner-Fushman and J. Lin. 2007. Answering
clinical questions with knowledge-based and statistical
techniques. Computational Linguistics, 33(1):63?103.
N. Fuhr and K. Gro?johann. 2001. XIRQL: A query
language for information retrieval in XML documents.
In SIGIR, pages 172?180.
M.J. Hansen, N.O. Rasmussen, and G. Chung. 2008. A
method of extracting the number of trial participants
from abstracts describing randomized controlled trials.
Journal of Telemedicine and Telecare, 14(7):354?358.
INEX. 2002-2009. Proceedings of the INitiative for the
Evaluation of XML Retrieval (INEX) workshop.
J. Kamps, M. Marx, M. de Rijke, and B. Sigurbjo?rnsson.
2005. Structured queries in XML retrieval. In CIKM,
pages 4?11.
J.M. Ponte and W.B. Croft. 1998. A language model-
ing approach to information retrieval. In SIGIR, pages
275?281.
T.C. Rindflesch and M. Fiszman. 2003. The interac-
tion of domain knowledge and linguistic structure in
natural language processing: interpreting hypernymic
propositions in biomedical text. Journal of Biomedical
Informatics, 36(6):462?477.
D.L. Sackett, W. Rosenberg, J.A. Gray, R.B. Haynes, and
W.S. Richardson. 1996. Evidence based medicine:
what it is and what it isn?t. British medical journal,
312(7023):71.
C. Schardt, M. Adams, T. Owens, S. Keitz, and
P. Fontelo. 2007. Utilization of the PICO frame-
work to improve searching PubMed for clinical ques-
tions. BMC Medical Informatics and Decision Mak-
ing, 7(1):16.
L.B. Sollaci and M.G. Pereira. 2004. The introduction,
methods, results, and discussion (IMRAD) structure:
a fifty-year survey. Journal of the Medical Library
Association, 92(3):364.
A. Trotman. 2005. Choosing document structure
weights. Information Processing and Management,
41(2):243?264.
J.M. Weinfeld and K. Finkelstein. 2005. How to answer
your clinical questions more efficiently. Family prac-
tice management, 12(7):37.
R. Wilkinson. 1994. Effective retrieval of structured doc-
uments. In SIGIR, pages 311?317.
830
Proceedings of NAACL-HLT 2013, pages 298?305,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression
Florian Boudin and Emmanuel Morin
LINA - UMR CNRS 6241, Universite? de Nantes, France
{florian.boudin,emmanuel.morin}@univ-nantes.fr
Abstract
Multi-Sentence Compression (MSC) is the
task of generating a short single sentence sum-
mary from a cluster of related sentences. This
paper presents an N-best reranking method
based on keyphrase extraction. Compression
candidates generated by a word graph-based
MSC approach are reranked according to the
number and relevance of keyphrases they con-
tain. Both manual and automatic evaluations
were performed using a dataset made of clus-
ters of newswire sentences. Results show that
the proposed method significantly improves
the informativity of the generated compres-
sions.
1 Introduction
Multi-Sentence Compression (MSC) can be broadly
described as the task of generating a short single sen-
tence summary from a cluster of related sentences.
It has recently attracted much attention, mostly be-
cause of its relevance to single or multi-document
extractive summarization. A standard way to gen-
erate summaries consists in ranking sentences by
importance, cluster them by similarity and select a
sentence from the top ranked clusters (Wang et al,
2008). One difficulty is then to generate concise,
non-redundant summaries. Selected sentences al-
most always contain additional information specific
to the documents from which they came, leading to
readability issues in the summary.
Sentence Compression (SC), i.e. the task of
summarizing a sentence while retaining most of
the informational content and remaining grammat-
ical (Jing, 2000), is a straightforward solution to this
problem. Another solution would be to create, for
each cluster of related sentences, a concise and flu-
ent fusion of information, reflecting facts common
to all sentences. Originally defined as sentence fu-
sion (Barzilay and McKeown, 2005), MSC is a text-
to-text generation process in which a novel sentence
is produced as a result of summarizing common in-
formation across a set of similar sentences.
Most of the previous MSC approaches rely on
syntactic parsers for producing grammatical com-
pressions, e.g. (Filippova and Strube, 2008; El-
sner and Santhanam, 2011). Recently, (Filippova,
2010) proposed a word graph-based approach which
only requires a Part-Of-Speech (POS) tagger and a
list of stopwords. The key assumption behind her
approach is that redundancy within the set of related
sentences provides a reliable way of generating in-
formative and grammatical sentences. Although this
approach seemingly works well, 48% to 60% of the
generated sentences are missing important informa-
tion about the set of related sentences. In this study,
we aim at producing more informative sentences by
maximizing the range of topics they cover.
Keyphrases are words that capture the main top-
ics of a document. Extracting keyphrases can benefit
various Natural Language Processing tasks such as
summarization, information retrieval and question-
answering (Kim et al, 2010). In summarization,
keyphrases provide semantic metadata that represent
the content of a document. Sentences containing the
most relevant keyphrases are used to generate the
summary (D?Avanzo and Magnini, 2005). In the
same way, we hypothesize that keyphrases can be
used to better generate sentences that convey the gist
298
of the set of related sentences.
In this paper, we present a reranking method
of N-best multi-sentence compressions based on
keyphrase extraction and describe a series of experi-
ments conducted on a manually constructed evalua-
tion corpus. More precisely, the main contributions
of our work are as follows:
? We extend Filippova (2010)?s word graph-
based MSC approach to produce well-
punctuated and more informative compres-
sions.
? We investigate the use of automatic Machine
Translation (MT) and summarization evalua-
tion metrics to evaluate MSC performance.
? We introduce a French evaluation dataset made
of 40 sets of related sentences along with refer-
ence compressions composed by humans.
The rest of this paper is organized as follows. We
first briefly review the previous work, followed by
a description of the method we propose. Next, we
give the details of the evaluation dataset we have
constructed and present our experiments and results.
Lastly, we conclude with a discussion and directions
for further work.
2 Related work
2.1 Multi-sentence compression
MSC have received much attention recently and
many different approaches have been proposed. The
pioneering work of (Barzilay and McKeown, 2005)
introduced the framework used by many subsequent
works: input sentences are represented by depen-
dency trees, some words are aligned to merge the
trees into a lattice, and the lattice is linearized using
tree traversal to produce fusion sentences. (Filip-
pova and Strube, 2008) cast MSC as an integer linear
program, and show promising results for German.
Later, (Elsner and Santhanam, 2011) proposed a su-
pervised approach trained on examples of manually
fused sentences.
Previously described approaches require the use
of a syntactic parser to control the grammatical-
ity of the output. As an alternative, several word
graph-based approaches that only require a POS
tagger were proposed. The key assumption is
that redundancy provides a reliable way of gen-
erating grammatical sentences. First, a directed
word graph is constructed from the set of input sen-
tences in which nodes represent unique words, de-
fined as word and POS tuples, and edges express
the original structure of sentences (i.e. word order-
ing). Sentence compressions are obtained by find-
ing commonly used paths in the graph. Word graph-
based MSC approaches were used in different tasks,
such as guided microblog summarization (Sharifi
et al, 2010), opinion summarization (Ganesan et
al., 2010) and newswire summarization (Filippova,
2010).
2.2 Keyphrase extraction
Keyphrases are words that are representative of the
main content of documents. Extracting keyphrases
can benefit various Natural Language Processing
tasks such as summarization, information retrieval
and question-answering (Kim et al, 2010). Previ-
ous works fall into two categories: supervised and
unsupervised methods. The idea behind supervised
methods is to recast keyphrase extraction as a binary
classification task. A model is trained using anno-
tated data to determine whether a given phrase is a
keyphrase or not (Frank et al, 1999; Turney, 2000).
Unsupervised approaches proposed so far have in-
volved a number of techniques, including language
modeling (Tomokiyo and Hurst, 2003), graph-based
ranking (Mihalcea and Tarau, 2004; Wan and Xiao,
2008) and clustering (Liu et al, 2009). While super-
vised approaches have generally proven more suc-
cessful, the need for training data and the bias to-
wards the domain on which they are trained remain
two critical issues.
3 Method
In this section, we first describe Filippova (2010)?s
word graph-based MSC approach. Then, we present
the keyphrase extraction approach we use and our
method for reranking generated compressions.
3.1 Description of Filippova?s approach
Let G = (V, E) be a directed graph with the set
of vertices (nodes) V and a set of directed edges E,
where E is a subset of V ? V . Given a set of re-
lated sentences S = {s1, s2, ..., sn}, a word graph
is constructed by iteratively adding sentences to it.
299
world
's
pinta
tortoise
island
the
last kind
has
to
be
known
as
giant
away
-end-
his
lonesome
george
a
died
the
believed
of
passed
-start-
the
...
...
(3)
(2)
Figure 1: Word graph constructed from the set of related sentences, a possible compression path is also given.
Figure 1 is an illustration of the word graph con-
structed from the following sentences. For clarity,
edge weights are omitted and italicized fragments
from the sentences are replaced with dots.
1. Lonesome George, the world?s last Pinta Island
giant tortoise, has passed away.
2. The giant tortoise known as Lonesome George
died Sunday at the Galapagos National Park in
Ecuador.
3. He was only about a hundred years old, but
the last known giant Pinta tortoise, Lonesome
George, has passed away.
4. Lonesome George, a giant tortoise believed to
be the last of his kind, has died.
At the first step, the graph simply represents one
sentence plus the start and end symbols (?start? and
?end? in Figure 1). A node is added to G for each
word in the sentence, and words adjacent in the sen-
tence are connected with directed edges. A word
from the following sentences is mapped onto an ex-
isting node in the graph if they have the same lower-
cased word form and POS and that no word from this
sentence has already been mapped onto this node. A
new node is created if there is no suitable candidate
in the graph.
Words are added to the graph in the following or-
der:
i. non-stopwords for which no candidate exists in
the graph or for which an unambiguous map-
ping is possible;
ii. non-stopwords for which there are either sev-
eral possible candidates in the graph or which
occur more than once in the sentence;
iii. stopwords.
For the last two groups of words where mapping
is ambiguous (i.e. there are two or more nodes in
the graph that refer to the same word/POS tuple),
the immediate context (the preceding and following
words in the sentence and the neighboring nodes in
the graph) or the frequency (i.e. the node which has
words mapped onto it) are used to select the candi-
date node. We use the stopword list included in nltk1
extended with temporal nouns (e.g. monday, yester-
day).
In Filippova?s approach, punctuation marks are
excluded. To generate well-punctuated compres-
sions, we simply added a fourth step for adding
punctuation marks in the graph. When mapping is
ambiguous, we select the candidate which has the
same immediate context.
Once the words from a sentence are added to the
graph, words adjacent in the sentence are connected
with directed edges. Edge weights are calculated us-
ing the weighting function defined in Equation 1.
1http://nltk.org/
300
w(i, j) =
cohesion(i, j)
freq(i)? freq(j)
(1)
cohesion(i, j) =
freq(i) + freq(j)
?
s?S d(s, i, j)?1
(2)
where freq(i) is the number of words mapped to the
node i. The function d(s, i, j) refers to the distance
between the offset positions of words i and j in sen-
tence s.
The purpose of this function is two fold: i. to
generate a grammatical compression, links between
words which appear often in this order are favored
(see Equation 2); ii. to generate an informative com-
pression, the weight of edges connecting salient
nodes is decreased.
A K-shortest paths algorithm is then used to find
the 50 shortest paths from start to end nodes in the
graph. Paths shorter than eight words or that do not
contain a verb are filtered. The remaining paths are
reranked by normalizing the total path weight over
its length. The path which has the lightest average
edge weight is then considered as the best compres-
sion.
3.2 Reranking paths using keyphrases
The main difficulty of MSC is to generate sentences
that are both informative and grammatically correct.
Here, redundancy within the set of input sentences
is used to identify important words and salient links
between words. Although this approach seemingly
works well, important information is missing in 48%
to 60% of the generated sentences (Filippova, 2010).
One of the reasons for this is that node salience
is estimated only with the frequency measure. To
tackle this issue, we propose to rerank the N-best list
of compressions using keyphrases extracted from
the set of related sentences. Intuitively, an infor-
mative sentence should contain the most relevant
keyphrases. We propose to rerank generated com-
pressions according to the number and relevance of
keyphrases they contain.
An unsupervised method based on (Wan and
Xiao, 2008) is used to extract keyphrases from each
set of related sentences. This method is based on
the assumption that a word recommends other co-
occurring words, and the strength of the recommen-
dation is recursively computed based on the im-
portance of the words making the recommendation.
Keyphrase extraction can be divided into two steps.
First, a weighted graph is constructed from the set
of related sentences, in which nodes represent words
defined as word and POS tuples. Two nodes (words)
are connected if their corresponding lexical units co-
occur within a sentence. Edge weights are the num-
ber of times two words co-occur. TextRank (Mihal-
cea and Tarau, 2004), a graph-based ranking algo-
rithm that takes into account edge weights, is ap-
plied for computing a salience score for each node.
The score for node Vi is initialized with a default
value and is computed in an iterative manner until
convergence using this equation:
S(Vi) = (1?d)+d?
?
Vj?adj(Vi)
wji
?
Vk?adj(Vi)wjk
S(Vi)
where adj(Vi) denotes the neighbors of Vi and d is
the damping factor set to 0.85.
The second step consists in generating and scor-
ing keyphrase candidates. Sequences of adja-
cent words satisfying a specific syntactic pattern
are collapsed into multi-word phrases. We use
(ADJ)*(NPP|NC)+(ADJ)* for French, in which
ADJ are adjectives, NPP are proper nouns and NC
are common nouns.
The score of a candidate keyphrase k is computed
by summing the salience scores of the words it con-
tains normalized by its length + 1 to favor longer
n-grams (see equation 3).
score(k) =
?
w?k TextRank(w)
length(k) + 1
(3)
The small vocabulary size as well as the high
redundancy within the set of related sentences are
two factors that make keyphrase extraction easier
to achieve. On the other hand, a large number
of the generated keyphrases are redundant. Some
keyphrases may be contained within larger ones,
e.g. giant tortoise and Pinta Island giant tortoise. To
solve this problem, generated keyphrases are clus-
tered using word overlap. For each cluster, we then
select the keyphrase with the highest score. This fil-
tering process enables the generation of a smaller
subset of keyphrases while having a better coverage
of the cluster content.
301
Reranking techniques can suffer from the limited
scope of the N-best list, which may rule out many
potentially good candidates. For this reason, we use
a larger number of paths than the one in (Filippova,
2010). Accordingly, the K-shortest paths algorithm
is used to find the 200 shortest paths. We rerank the
paths by normalizing the total path weight over its
length multiplied by the sum of keyphrase scores it
contains. The score of a sentence compression c is
given by:
score(c) =
?
i,j?path(c)w(i,j)
length(c)?
?
k?c score(k)
(4)
4 Experimental settings
4.1 Construction of the evaluation dataset
To our knowledge, there is no dataset available to
evaluate MSC in an automatic way. The perfor-
mance of the previously described approaches was
assessed by human judges. In this work, we intro-
duce a new evaluation dataset made of 40 sets of re-
lated sentences along with reference compressions
composed by human assessors. The purpose of this
dataset is to investigate the use of existing automatic
evaluation metrics for the MSC task.
Similar to (Filippova, 2010), we collected news
articles presented in clusters on the French edition of
Google News2 over a period of three months. Clus-
ters composed of at least 20 news articles and con-
taining one single prevailing event were manually
selected. To obtain the sets of related sentences, we
extracted the first sentences from each article in the
cluster, removing duplicates. Leading sentences in
news articles are known to provide a good summary
of the article content and are used as a baseline in
summarization (Dang, 2005).
The resulting dataset contains 618 sentences (33
tokens on average) spread over 40 clusters. The
number of sentences within each cluster is on av-
erage 15, with a minimum of 7 and a maximum of
36. The word redundancy rate within the dataset,
computed as the number of unique words over the
number of words for each cluster, is 38.8%.
Three reference compressions were manually
composed for each set of sentences. Human an-
notators, all native French speakers, were asked to
2http://news.google.fr
carefully read the set of sentences, extract the most
salient facts and generate a sentence (compression)
that summarize the set of sentences. Annotators
were also told to introduce as little new vocabu-
lary as possible in their compressions. The purpose
of this guideline is to reduce the number of possi-
ble mismatches, as existing evaluation metrics are
based on n-gram comparison. Reference compres-
sions have a compression rate of 60%.
4.2 Automatic evaluation
The use of automatic methods for evaluating
machine-generated text has gradually become the
mainstream in Computational Linguistics. Well
known examples are the ROUGE (Lin, 2004) and
BLEU (Papineni et al, 2002) evaluation metrics used
in the summarization and MT communities. These
metrics assess the quality of a system output by com-
puting its similarity to one or more human-generated
references.
Prior work in sentence compression use the F1
measure over grammatical relations to evaluate can-
didate compressions (Riezler et al, 2003). It was
shown to correlate significantly with human judg-
ments (Clarke and Lapata, 2006) and behave sim-
ilarly to BLEU (Unno et al, 2006). However,
this metric is not entirely reliable as it depends on
parser accuracy and the type of dependency relations
used (Napoles et al, 2011). In this work, the fol-
lowing evaluation measures are considered relevant:
BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams)
and ROUGE-SU4 (bigrams with skip distance up to
4 words)4. ROUGE measures are computed using
stopword removal and French stemming 5.
4.3 Manual evaluation
The quality of the generated compressions was as-
sessed in an experiment with human raters. Two as-
pects were considered: grammaticality and informa-
tivity. Following previous work (Barzilay and McK-
eown, 2005), we asked raters to assess grammati-
cality on a 3-points scale: perfect (2 pts), if the com-
pression is a complete grammatical sentence; almost
3ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a.pl
4We use the version 1.5.5 of the ROUGE package available
from http://www.berouge.com
5http://snowball.tartarus.org/
302
(1 pt), if it requires minor editing, e.g. one mistake
in articles, agreement or punctuation; ungrammati-
cal (0 pts), if it is none of the above. Raters were ex-
plicitly asked to ignore lack of capitalization while
evaluating grammaticality.
Informativity is evaluated according to the 3-
points scale defined in (Filippova, 2010): perfect (2
pts), if the compression conveys the gist of the main
event and is more or less like the summary the per-
son would produce himself; related (1 pt), if it is
related to the the main theme but misses something
important; unrelated (0 pts), if the compression is
not related to the main theme.
Three raters, all native French speakers, were
hired to assess the generated compressions.
5 Results
To evaluate the effectiveness of our method, we
compare the compressions generated with Filip-
pova?s approach (denoted as baseline) against the
ones obtained by reranking paths using keyphrases
(denoted as KeyRank). We evaluated the agreement
between the three raters using Fleiss?s kappa (Art-
stein and Poesio, 2008). The ? value is 0.56 which
denotes a moderate agreement.
Table 1 presents the average grammaticality and
informativity scores. Results achieved by the base-
line are consistent with the ones presented in (Fil-
ippova, 2010). We observe a significant improve-
ment in informativity for KeyRank. Grammaticality
scores are, however, slightly decreased. One reason
for that is the reranking we added to the shortest path
method that outputs longer compressions. The aver-
age length for our method is nevertheless drastically
shorter than the average length of the input sentences
(19 vs. 33 tokens). This corresponds to a compres-
sion rate (58%) that is close to the one observed on
reference compressions (60%).
Table 2 shows the distributions over the three
scores for both grammaticality and informativity.
We observe that 97.5% of the compressions gener-
ated with KeyRank are related to the main theme
of the cluster, and 62.5% convey the very gist of
it without missing any important information. This
represents an absolute increase of 19.2% over the
baseline. Although our reranking method has lower
grammaticality scores, 65% of the generated sen-
Method Gram. Info.
Length
CompR
Avg. Std.Dev.
Baseline 1.63 1.33 16.3 4.8 50%
KeyRank 1.53 1.60? 19 6.1 58%
Table 1: Average ratings over all clusters and raters along
with average compression length (in tokens), standard de-
viation and corresponding compression rate (? indicates
significance at the 0.01 level using Student?s t-test).
tences are perfectly grammatical.
Method
Gram. Info.
0 1 2 0 1 2
Baseline 9.2% 18.3% 72.5% 10.0% 46.7% 43.3%
KeyRank 11.7% 23.3% 65.0% 2.5% 35.0% 62.5%
Table 2: Distribution over possible manual ratings for
grammaticality and informativity. Ratings are expressed
on a scale of 0 to 2.
Table 3 shows the performance of the baseline
and our reranking method in terms of ROUGE and
BLEU scores. KeyRank significantly outperforms
the baseline according to the different ROUGE met-
rics. This indicates an improvement in informativity
for the compressions generated using our method.
We observe a large but not significant increase in
BLEU scores. The slightly decreased grammatical-
ity scores could be a reason for this. BLEU is essen-
tially a precision metric, and it measures how well a
compression candidate overlaps with multiple refer-
ences. Longer n-grams used by BLEU6 tend to score
for grammaticality rather than content.
Metric Baseline KeyRank
ROUGE-1 0.57441 0.65677?
ROUGE-2 0.39212 0.44140?
ROUGE-SU4 0.37004 0.43443?
BLEU 0.61560 0.65770
Table 3: Automatic evaluation scores (? and ? indicate
significance at the 0.01 and 0.001 levels respectively us-
ing Student?s t-test)
To assess the effectiveness of automatic evalua-
6BLEU measures are computed using 4-grams.
303
tion metrics, we compute the Pearson?s correlation
coefficient between ROUGE and BLEU scores and
averaged manual ratings. According to Table 4, re-
sults show medium to strong correlation between
ROUGE scores and informativity ratings. On the
other hand, BLEU scores better correlate with gram-
maticality ratings. Overall, automatic evaluation
metrics are not highly correlated with manual rat-
ings. One reason for that may be that the manual
score assignments are arbitrary (i.e. 0, 1, 2), and that
a score of one is in fact closer to two than to zero.
Results suggest that automatic metrics do give an in-
dication of the compression quality, but can not re-
place manual evaluation.
Metric Gram. Info.
ROUGE-1 0.402 0.591
ROUGE-2 0.432 0.494
ROUGE-SU4 0.386 0.542
BLEU 0.444 0.401
Table 4: Pearson correlation coefficients for automatic
metrics vs. average human ratings.
6 Conclusion
This paper presented a multi-sentence compres-
sion approach that uses keyphrases to generate
more informative compressions. We extended Fil-
ippova (2010)?s word graph-based MSC approach
by adding a re-reranking step that favors compres-
sions that contain the most relevant keyphrases of
the input sentence set. An implementation of the
proposed multi-sentence compression approach is
available for download7. We constructed an eval-
uation dataset made of 40 sets of related sentences
along with reference compressions composed by hu-
mans. This dataset is freely available for download8.
We performed both manual and automatic evalua-
tions and showed that our method significantly im-
proves the informativity of the generated compres-
sions. We also investigated the correlation between
manual and automatic evaluation metrics and found
that ROUGE and BLEU have a medium correlation
with manual ratings.
7https://github.com/boudinfl/takahe
8https://github.com/boudinfl/lina-msc
In future work, we intend to examine how gram-
maticality of the generated compressions can be en-
hanced. Similar to the work of Hasan et al (2006) in
the Machine Translation field, we plan to experiment
with high order POS language models reranking.
Acknowledgments
The authors would like to thank Sebastia?n Pen?a Sal-
darriaga and Ophe?lie Lacroix for helpful comments
on this work. We thank the anonymous reviewers for
their useful comments. This work was supported by
the French Agence Nationale de la Recherche under
grant ANR-12-CORD-0027 and by the French Re-
gion Pays de Loire in the context of the DEPART
project (http://www.projetdepart.org/).
References
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 377?384, Sydney, Australia, July. Association
for Computational Linguistics.
Hoa Trang Dang. 2005. Overview of duc 2005. In Pro-
ceedings of the Document Understanding Conference.
Ernesto D?Avanzo and Bernardo Magnini. 2005. A
keyphrase-based approach to summarization: the lake
system at duc-2005. In Proceedings of the Document
Understanding Conference.
Micha Elsner and Deepak Santhanam. 2011. Learning to
fuse disparate sentences. In Proceedings of the Work-
shop on Monolingual Text-To-Text Generation, pages
54?63, Portland, Oregon, June. Association for Com-
putational Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 177?185, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Katja Filippova. 2010. Multi-Sentence Compression:
Finding Shortest Paths in Word Graphs. In Proceed-
304
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 322?330,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A Graph Based Approach to Abstrac-
tive Summarization of Highly Redundant Opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 340?
348, Beijing, China, August. Coling 2010 Organizing
Committee.
S. Hasan, O. Bender, and H. Ney. 2006. Reranking trans-
lation hypotheses using structural properties. In Pro-
ceedings of the EACL Workshop on Learning Struc-
tured Information in Natural Language Applications,
pages 41?48.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the Sixth
Conference on Applied Natural Language Processing,
pages 310?315, Seattle, Washington, USA, April. As-
sociation for Computational Linguistics.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010. Semeval-2010 task 5 : Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 21?26, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.
2009. Clustering to find exemplar terms for keyphrase
extraction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 257?266, Singapore, August. Association
for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 404?411,
Barcelona, Spain, July. Association for Computational
Linguistics.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of the Workshop on Monolingual Text-To-Text Gener-
ation, pages 91?97, Portland, Oregon, June. Associa-
tion for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 118?125. Association for Computational Lin-
guistics.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing Microblogs Automatically. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 685?
688, Los Angeles, California, June. Association for
Computational Linguistics.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33?40, Sapporo, Japan, July. Association
for Computational Linguistics.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303?336.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming cfg parse trees
for sentence compression using machine learning ap-
proaches. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 850?857,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Xiaojun Wan and Jianguo Xiao. 2008. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 969?976, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding.
2008. Multi-document Summarization via Sentence-
Level Semantic Analysis and Symmetric Matrix Fac-
torization. In Proceedings of the 31st annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?08, pages
307?314, New York, NY, USA. ACM.
305
JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 61?68,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Participation du LINA ? DEFT 2012
Florian Boudin Amir Hazem Nicolas Hernandez Prajol Shrestha
Universit? de Nantes
pr?nom.nom@univ-nantes.fr
R?SUM?
Cet article pr?sente la participation de l??quipe TALN du LINA au d?fi fouille de textes (DEFT)
2012. D?velopp? sp?cifiquement pour la seconde piste du d?fi, notre syst?me combine les sorties
de trois diff?rentes m?thodes d?extraction de mots cl?s. Notre syst?me s?est class? ? la 2i e`me place
sur un total de 9 syst?mes avec une f-mesure de 21,3%.
ABSTRACT
LINA at DEFT 2012
This article presents the participation of the TALN group at LINA to the d?fi fouille de textes
(DEFT) 2012. Developed specifically for the second task, our system combines the outputs of
three different keyword extraction methods. Our system ranked 2nd out of 9 systems with a
f-measure of 21,3%.
MOTS-CL?S : extraction de mots cl?s, deft 2012, combinaison de m?thodes.
KEYWORDS: keyword extraction, deft 2012, combining methods.
1 Introduction
L?indexation automatique consiste ? identifier un ensemble de mots cl?s (e.g. mots, termes) qui
d?crit le contenu d?un document. Les mots cl?s peuvent ensuite ?tre utilis?s, entre autres, pour
faciliter la recherche d?information ou la navigation dans les collections de documents. L??dition
2012 du d?fi fouille de textes (DEFT) porte sur l?extraction automatique de mots cl?s ? partir
d?articles scientifiques parus dans le domaine des Sciences Humaines et Sociales (SHS).
L?objectif du d?fi est de retrouver, ? partir du contenu des documents (i.e. articles scientifiques),
les mots cl?s qui ont pu ?tre choisis par les auteurs. Deux diff?rentes pistes ont ?t? propos?es.
La premi?re piste consiste ? identifier dans une terminologie, les mots cl?s qui ont ?t? assign?s
aux documents. Cette terminologie regroupe l?ensemble des mots cl?s utilis?s dans la collection.
La seconde piste, de prime abord plus complexe, consiste ? extraire les mots cl?s directement ?
partir du contenu des documents. Cet article d?crit notre participation ? la seconde piste du d?fi.
Le reste de cet article est organis? comme suit. La section 2 d?crit l?ensemble de donn?es utilis?
pour la campagne d??valuation. La section 3 pr?sente les diff?rentes m?thodes que nous avons
d?velopp?es sp?cifiquement pour la seconde piste du d?fi. Nous d?crivons ensuite en section
4 nos r?sultats exp?rimentaux avant de pr?senter les m?thodes que nous avons test?es et qui
61
qui ont eu un impact nul ou n?gatif sur les r?sultats. La section 6 conclut cet article et donne
quelques perspectives de travaux futurs.
2 Description de la campagne DEFT 2012
L?ensemble de documents utilis? pour le d?fi 2012 est constitu? de 234 articles scientifiques parus
dans le domaine des SHS. Ces articles ont ?t? publi?s entre 2001 et 2008 dans quatre revues
diff?rentes. L?ensemble d?apprentissage contient 60% des documents (soit 141 articles), et celui
de test contient les 40% restants (soit 93 articles). La r?partition des quatre diff?rentes revues
dans les deux ensembles est uniforme.
Du point de vue technique, les articles sont au format XML. Ils sont structur?s en deux parties : le
r?sum? et le corps de l?article. Chaque article contient ?galement le nombre de mots cl?s indexant
son contenu. Les mots cl?s assign?s ? chaque article sont disponibles pour chacun des articles de
l?ensemble d?entra?nement.
Les syst?mes participant au d?fi sont ?valu?s ? l?aide des mesures classiques de pr?cision, rappel
et f-mesure. Pour chaque article, les mots cl?s g?n?r?s par les syst?mes sont compar?s aux mots
cl?s de r?f?rence (assign?s par les auteurs). Afin de limiter les probl?mes li?s aux diff?rentes
variations orthographiques, plusieurs traitements de normalisation (i.e. normalisation de la casse
et lemmatisation) sont appliqu?s au pr?alable aux mots cl?s. Chaque participant peut soumettre
jusqu?? trois ex?cutions par piste.
La liste ci-dessous pr?sente quelques unes des difficult?s que nous avons identifi?es dans les
articles de l?ensemble d?entra?nement.
? Articles diff?rents ayant le m?me r?sum?, e.g. les articles as_2002_007048ar et as_2002_
007053ar.
? Contenu des articles dans des langues diff?rentes et/ou m?lang?es, e.g. fran?ais et anglais
dans ttr_2008_037494ar, espagnol dans meta_2005_019927ar.
? Contenu des articles tr?s bruit? avec des probl?mes de ponctuation, de caract?res unicodes et
de segmentation en paragraphes, e.g. ci-dessous un extrait de l?article meta_2005_019840ar.
<p>Ce langage est au c.ur des pr?occupations des juristes , qui
nous rappellent r?guli?rement </p>
<p>que le droit est affaire de mots. Et cela dans tout l.univers
du droit , vers quelque c?t? que l.on se</p>
<p>tourne , dans le monde juridique anglophone - o?, pour
Mellinkoff (1963& amp;#x00A0 ;: vii), &amp;#x00A0;The law is a</p>
<p></p>
<p>dans son ensemble , la technique juridique aboutit , pour la
plus grande part , ? une question de</p>
<p>terminologie&amp;# x00A0;. Chacun pourra le v?rifier par la
consultation d.ouvrages parmi les plus r?cents et</p>
62
3 Approches
Les diff?rentes m?thodes que nous avons d?velopp?es utilisent le mot comme unit? principale.
Nous avons donc appliqu? un ensemble commun de pr?-traitements aux documents : segmenta-
tion en phrases, d?coupage en mots et ?tiquetage morpho-syntaxique. L?information structurelle
pr?sente dans chacun des documents (i.e. r?sum?, corps de l?article et paragraphes) est pr?serv?e.
Chaque paragraphe est segment? en phrases en utilisant la m?thode PUNKT de d?tection de
changement de phrases (Kiss et Strunk, 2006) mise en ?uvre dans la bo?te ? outils NLTK (Bird et
Loper, 2004). La tokenisation des phrases est effectu?e avec un outil d?velopp? en interne utili-
sant le lexique des formes fl?chies du fran?ais (lefff)1 pour l?indentification des unit?s lexicales
complexes (e.g. mots compos?s). L??tiquetage morpho-syntaxique est obtenu ? l?aide du Stanford
POS Tagger (Toutanova et al, 2003)2 entrain?e sur le French Treebank (Abeill? et al, 2003).
3.1 Syst?me 1
Ce syst?me est bas? sur du T F? IDF et trois r?gles issues du corpus d?apprentissage. La principale
question qui se pose ici est : qu?est ce qu?un mot cl? ? ou autrement dit, qu?est ce qui fait qu?un
terme a plus de chances d??tre un mot cl? qu?un autre ?
En analysant les documents du corpus d?apprentissage, nous avons relev? trois particularit?s li?es
aux mots cl?s. La premi?re concerne leur localisation dans les documents. Chaque document ?tant
divis? en deux parties qui sont : le r?sum? (ABSTRACT) et le corps du document (BODY), nous
nous sommes donc int?ress?s ? la position des mots cl?s par rapport ? ce d?coupage. Nous avons
pu constater qu?un terme apparaissant ? la fois dans le r?sum? et dans le corps du document avait
plus de chances d??tre un mot cl?. Ainsi, nous avons utilis? cette information comme premi?re
r?gle de notre syst?me (nous appellerons cette r?gle : R1). Deux strat?gies utilisant cette r?gle
ont ?t? adopt?es, la premi?re consiste ? ne s?lectionner que des termes qui apparaissent ? la
fois dans le r?sum? et dans le corps du document (nous appellerons cette strat?gie : S1), la
deuxi?me consiste ? donner la priorit? aux termes respectant la strat?gie S1 en utilisant une
pond?ration par un param?tre ? fix? empiriquement (nous appellerons cette strat?gie : S2).
Les diff?rents tests conduits ont montr? que l?utilisation de la strat?gie S1 donnait de meilleurs
r?sultats que l?utilisation de la strat?gie S2. Intuitivement, nous aurions tendance ? penser le
contraire (la strat?gie S2 devrait ?tre meilleur que S1), car ?liminer des termes n?apparaissant
que dans le r?sum? ou que dans le corps du document nous ferait sans doute perdre des mots
cl?s. L?explication et que la strat?gie S1 corrige sans doute les faiblesses de notre syst?me qui
renverrait plus de faux positifs que de vrais n?gatifs.
La deuxi?me particularit? relative aux n-grammes, d?coule de la question suivante : est ce qu?un
terme simple (1-gramme) a plus de chances d??tre un mot cl? qu?un terme compos? (n-grammes
avec n > 1) ? De part le corpus d?apprentissage nous avons pu constater qu?il y avait 70% de
termes simples et 30% de termes compos?s. Ainsi, nous avons voulu donner une plus grande
importance aux termes simples extraits par notre syst?me. De la m?me mani?re que pour la
strat?gie S2, nous avons introduit un param?tre de pond?ration ? afin de prioriser les termes
simples (nous appellerons cette r?gle R2).
1
http://www.labri.fr/perso/clement/lefff/2Nous utilison la version 3.1.0 avec les param?tres par d?faut.
63
La troisi?me particularit? rel?ve de la simple observation que la quasi totalit? des mots cl?s
?taient soit des noms, soit des adjectifs. ? partir de cette constatation, nous avons introduit une
troisi?me r?gle (R3) qui filtre les verbes.
3.2 Syst?me 2
Ce syst?me repose sur l?exploitation d?un existant ? savoir l?approche KEA (Keyphrase Extraction
Algorithm) de (Witten et al, 1999). KEA permet d?une part de mod?liser les expressions significa-
tives (compos?es d?un ou plusieurs mots) du contenu de textes ? l?aide de textes et d?expressions
cl?s associ?es et d?autre part d?extraire les expressions cl?s d?une collection de textes ? l?aide
d?une mod?lisation construite a priori. L?approche utilise un classifieur bay?sien na?f pour calculer
un score de probabilit? de chaque expression cl? candidate. La construction requiert un ensemble
d?expressions cl?s class?es positivement pour chaque texte du corpus d?apprentissage. L?extraction
se r?alise sur un corpus de domaine similaire au domaine du corpus d?apprentissage.
Les phases de mod?lisation ou d?extraction des expressions cl?s fonctionnent toutes deux ? la
suite de deux phases ?l?mentaires : l?extraction de candidats et le calcul de traits descriptifs
des candidats. Les candidats s?obtiennent par extraction de n-grammes de taille pr?d?finie ne
d?butant pas et ne finissant pas par un mot outil.
Les traits utilis?s pour d?crire chaque candidat au sein d?un document sont les suivants : le
T F ? IDF (mesure de sp?cificit? du candidat pour le document), la position de la premi?re
occurrence (pourcentage du texte pr?c?dent l?occurrence), le nombre de mots qui compose le
candidat. Les candidats ayant un haut T F ? IDF , apparaissant au d?but d?un texte et comptant
le plus de mots sont ainsi consid?r?s comme ?tant de bons descripteurs du contenu d?un texte.
Les derni?res ?volutions de KEA permettent d?exploiter des lexiques contr?l?s de type th?saurus
dans la construction de la mod?lisation (Medelyan et Witten, 2006).
Nous n?avons pas exploit? de ressources ext?rieures de type lexiques contr?l?s dans la construction
de notre mod?lisation. Nous avons utilis? la version 5.0 de l?impl?mentation de KEA3 disponible
sous licence GNU ; en pratique nous avons utilis? les fonctionnalit?s d?extraction ?libre? pr?sentes
d?s la version 3.0. Les fonctionnalit?s d?velopp?es ult?rieurement concernent l?exploitation de
lexiques contr?l?s. Nos candidats ?taient au maximum de taille 5. Nous avons exploit? le corpus
d?apprentissage fourni pour la seconde piste pour construire notre mod?lisation. Chaque texte
(r?sum? et corps) a ?t? consid?r? comme une unit? documentaire.
L?approche KEA est facilement portable ? diff?rentes langues du fait qu?elle n?cessite peu de
ressources. En particulier elle ne requiert pas une pr?-analyse syntaxique pour s?lectionner des
candidats. Nous avons n?anmoins port? une certaine attention ? nos traitements pr?liminaires
et nous avons constat? qu?une pr?-segmentation en token mots ainsi que l?utilisation d?une liste
multilingue de mots outils augmentaient la qualit? de l?extraction des expressions cl?s lorsque
nous ?valuions l?approche par validation crois?e sur le corpus d?apprentissage. Concernant la
liste des mots outils, nous avons fusionn? les listes fournies par KEA pour le fran?ais, l?anglais et
l?espagnol. Nous l?avons compl?t?e des formes des mots outils pouvant subir une ?lision du e final
en fran?ais (e.g. ?le? s?est vu compl?t? de la forme ?l??, de m?me pour ?lorsque? avec ?lorsqu??. . .).
Ces formes ?taient en effet reconnues par notre segmenteur en mots.
3
http://www.nzdl.org/Kea
64
3.3 Syst?me 3
Ce syst?me est bas? sur une approche par classification supervis?e. La t?che d?extraction de mots
cl?s est ici consid?r?e comme une t?che de classification binaire. La premi?re ?tape consiste ?
g?n?rer tous les mots cl?s candidats ? partir du document. Pour ce faire, nous commen?ons par
extraire tous les n-grammes de mots jusqu?? n = 4. Des contraintes syntaxiques sont ensuite
utilis?es pour filtrer les candidats. Ainsi, seuls les n-grammes compos?s uniquement de noms,
d?adjectifs et de mots outils (except? en premier/dernier mot du n-gramme) sont gard?s.
Pour chaque candidat, nous calculons les traits suivants :
? Poids T F ? IDF
? Nombre de mots du n-gramme
? Patron syntaxique du n-gramme (e.g. ?Nom Adjectif?)
? Position relative de la premi?re occurence dans le document
? Section(s) o? apparait le n-gramme (r?sum?, corps ou les deux)
? Nombre de documents de la collection dans lesquels le n-gramme apparait
? Score de saillance dans l?arbre de d?pendances de coh?sion lexicale du texte (voir ci-dessous)
Nous construisons ce que nous appelons un ?arbre de d?pendances de coh?sion lexicale? selon
une approche d?crite par Choi ? la section 6.3.1. de sa th?se (Choi, 2002). Une d?pendance
est pr?suppos?e exister entre deux phrases cons?cutives si celles-ci ont des mots en commun ;
l?hypoth?se est de consid?rer la seconde phrase comme une ?laboration de la premi?re. En
pratique, notre algorithme ne reconna?t pas syst?matiquement une relation de d?pendance
entre deux phrases cons?cutives qui partagent des mots en commun. En effet notre algorithme
recherche, pour chaque phrase du texte, la phrase la plus haute dans la cha?ne de d?pendance
de la phrase pr?c?dente avec laquelle elle partage des mots en commun. L?arbre est construit
en prenant le texte dans son ensemble (r?sum? et corps) pr?alablement lemmatis?. Un score de
saillance est calcul? pour chaque phrase en fonction du nombre de ses d?pendances (directes
et transitives) normalis? par le nombre de d?pendances maximal qu?une phrase peut avoir sur
le texte donn?. Chaque expression candidate h?rite alors du score de la phrase o? apparait sa
premi?re occurence.
Nous utilisons la combinaison par vote de trois algorithmes de classification disponibles dans la
boite ? outils Weka (Hall et al, 2009) : NaiveBayes, J48 et RandomForest. Les mots-cl?s candidats
sont ensuite tri?s selon leurs scores de pr?diction.
3.4 Combinaison des syst?mes
Les trois syst?mes que nous avons d?velopp?s utilisent diff?rentes m?thodes pour capturer
l?importance d?un mot cl? par rapport ? un document. Une combinaison des sorties de ces
derniers est donc pertinente.
Nous disposons pour chaque document, de trois listes pond?r?es de mots cl?s. La m?thode la
plus simple consisterait ? utiliser la somme des scores des trois syst?mes. Cependant, les scores
calcul?s par chacun des syst?mes ne sont pas directement comparables. ? la place du score, nous
utilisons pour chaque mot cl? candidat, l?inverse de son rang dans la liste ordonn?e.
Deux strat?gies de combinaison ont ?t? utilis?es. La premi?re, COMBI1 consiste ? assigner la
65
somme de l?inverse des rangs d?un mot cl? dans les listes ordonn?es des trois syst?mes. Pour la
seconde strat?gie, COMBI2, nous ne consid?rons que les mots cl?s apparaissant dans les sorties
des trois syst?mes. L?id?e est de filtrer les mots cl?s consid?r?s comme important par seulement
un ou deux des trois syst?mes.
4 R?sultats
Nous pr?sentons dans cette section les r?sultats officiels de la campagne DEFT 2012. Nous
avons soumis trois ex?cutions pour chacune des deux pistes. Pour la premi?re piste, nous avons
simplement utilis? le Syst?me 1 (d?crit dans la section 3.1) et filtr? les mots cl?s candidats ? l?aide
de la terminologie. Le nombre de mots cl?s retourn?s est fix? ? 7 pour la premi?re ex?cution et ?
6 pour les deux autres. Les trois configurations utilisent la r?gle R3.
La premi?re ex?cution utilise la r?gle R2 (? = 0,6). La seconde ex?cution utilise la r?gle R2
(? = 0, 65). La troisi?me ex?cution utilise la r?gle R1 avec la strat?gie S2 (?= 0, 65) et la r?gle
R2 (? = 0,65). Pour la seconde piste, nous avons soumis les ex?cutions de deux combinaisons
(COMBI1 et COMBI2) ainsi que du syst?me 3 (d?crit dans la section 3.3). Le nombre de mots cl?s
retourn?s est fix? ? 130% du nombre de mots cl?s de r?f?rence pour COMBI1 et COMBI2 et ?
110% pour le syst?me 3. Ces nombres permettent d?obtenir les meilleurs r?sultats sur l?ensemble
d?entra?nement.
La table 1 pr?sente les r?sultats de nos trois ex?cutions pour la premi?re piste. Les r?sultats
obtenus par les trois ex?cutions sont moins bons que ceux obtenus sur l?ensemble d?entrainement
(f-mesure=0,44 pour la premi?re ex?cution). Nous constatons que la variation du rappel sur les
trois ex?cutions est faible. La chute de la pr?cision pour la troisi?me ex?cution s?explique par
l?application de la r?gle R1 qui limite le nombre de candidats possibles.
Syst?me Pr?cision Rappel f-mesure
1 0,3812 0,4004 0,3906
2 0,3759 0,3948 0,3851
3 0,3343 0,4097 0,3682
TAB. 1 ? R?sultats de nos trois ex?cutions pour la premi?re piste.
La table 2 montre les r?sultats de nos trois ex?cutions pour la seconde piste. Nous pouvons voir
que la performance de COMBI2 est largement en dessous de COMBI1. Nous avions constat? le
ph?nom?ne inverse sur les donn?es d?entra?nement. Ceci est du au fait que le nombre de mots
cl?s retourn?s par COMBI2 peut dans certains cas ?tre inf?rieur au seuil que nous avons fix?. En
effet, l?intersection des listes des 100 meilleurs mots cl?s candidats de chaque syst?me est tr?s
restrainte pour quelque uns des documents de l?ensemble de test. Nous constatons que les scores
du syst?me 3, ayant obtenu les meilleurs r?sultats sur l?ensemble d?entra?nement parmis nos trois
syst?mes, sont faibles en comparaison des deux combinaisons. Ce r?sultat semble indiquer un
probl?me de sur-entra?nement et illustre bien l?utilit? de la combinaison.
La table 3 pr?sente, pour chacune des deux pistes, le classement des diff?rentes ?quipes sur la
base de la meilleure soumission. Notre soumission est class?e au rang 5 sur 10 pour la premi?re
66
Syst?me Pr?cision Rappel f-mesure
COMBI1 0,1949 0,2355 0,2133
COMBI2 0,1788 0,2128 0,1943
Syst?me 3 0,1643 0,1880 0,1753
TAB. 2 ? R?sultats de nos trois ex?cutions pour la seconde piste.
piste et au rang 2 sur 9 pour la seconde piste. Les r?sultats obtenus par l??quipe 16 sont bien
au dessus de toutes les autres ?quipes et montrent qu?une marge de progression importante est
possible pour notre syst?me.
Rang Piste 1 Piste 2
1 ?quipe 16 (0,9488) ?quipe 16 (0,5874)
2 ?quipe 05 (0,7475) ?quipe 06 (0,2133)
3 ?quipe 04 (0,4417) ?quipe 05 (0,2087)
4 ?quipe 02 (0,3985) ?quipe 02 (0,1921)
5 ?quipe 06 (0,3906) ?quipe 01 (0,1901)
6 ?quipe 01 (0,2737) ?quipe 13 (0,1632)
7 ?quipe 13 (0,1378) ?quipe 04 (0,1270)
8 ?quipe 17 (0,1079) ?quipe 17 (0,0895)
9 ?quipe 03 (0,0857) ?quipe 03 (0,0785)
10 ?quipe 18 (0,0428) -
TAB. 3 ? Classement de DEFT 2012 sur la base de la meilleure soumission de chaque ?quipe pour
chacune des deux pistes. Notre classement est indiqu? en gras (?quipe 06).
5 Ce qui n?a pas march?
Nous d?crivons ici les m?thodes qui ont eu un impact nul ou n?gatif sur les r?sultats.
Traits ayant un impact n?gatif sur la performance du syst?me 3 : la dispersion d?un mot cl?
dans le document, mots appartenant ? des phrases contenant des citations, noms des auteurs les
plus cit?s dans le document (sp?cifique aux articles commen?ant par ?as?).
Suppression de la redondance : nous avons constat? un niveau de redondance important
des mots cl?s dans les sorties de nos syst?mes. Par exemple, les mots cl?s ?jardins collectifs?,
?jardins? et ?collectifs? sont tous les trois pr?sents dans le top 10, ce qui fait baisser le rappel.
Plusieurs strat?gies ont ?t? exp?riment?es pour supprimer cette redondance (e.g. suppression
d?un n-gramme si tous les mots qui le composent sont ?galement pr?sents parmi les 10 meilleurs
candidats). Une d?gradation des r?sultats est cependant observ?e indiquant que la strat?gie ?
adopter est d?pendante des documents.
Mod?le de pond?ration ? base de graphe : nous avons impl?ment? l?approche propos?e
dans (Mihalcea et Tarau, 2004). Il s?agit de repr?senter chaque document sous la forme d?un
67
graphe de mots connect?s par des relations de co-occurrences. Des algorithmes de centralit? sont
ensuite appliqu?s pour extraire les mots les plus caract?ristiques. Les r?sultats obtenus par cette
m?thode sont inf?rieurs ? ceux obtenus ? l?aide d?une pond?ration par la mesure T F ? IDF .
6 Conclusions
Nous avons d?crit la participation du LINA ? DEFT 2012. Notre syst?me est le r?sultat de la
combinaison des sorties de trois diff?rentes m?thodes d?extraction de mots cl?s. Les r?sultats
obtenus par ce dernier sont toujours meilleurs que ceux obtenus par chacune des trois m?thodes
individuellement. Pour la seconde piste, notre syst?me s?est class? ? la 2i e`me place sur un total de
9 syst?mes avec une f-mesure de 21,3%.
La strat?gie que nous avons employ?e pour combiner les sorties des diff?rentes m?thodes n?est
cependant pas optimale. Nous envisageons d??tendre ce travail en proposant d?autres strat?gies
comme par exemple l?utilisation d?un meta-classifieur.
R?f?rences
ABEILL?, A., CL?MENT, L. et TOUSSENEL, F. (2003). Building a treebank for French. Treebanks :
building and using parsed corpora, pages 165?188.
BIRD, S. et LOPER, E. (2004). NLTK : The natural language toolkit. In ACL, Barcelone, Espagne.
CHOI, F. Y. Y. (2002). Content-based Text Navigation. Th?se de doctorat, Department of Computer
Science, University of Manchester.
HALL, M., FRANK, E., HOLMES, G., PFAHRINGER, B., REUTEMANN, P. et WITTEN, I. (2009). The weka
data mining software : an update. ACM SIGKDD Explorations Newsletter, 11(1):10?18.
KISS, T. et STRUNK, J. (2006). Unsupervised multilingual sentence boundary detection. Compu-
tational Linguistics, 32(4):485?525.
MEDELYAN, O. et WITTEN, I. H. (2006). Thesaurus based automatic keyphrase indexing. In
Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, JCDL ?06, pages 296?297,
New York, NY, USA. ACM.
MIHALCEA, R. et TARAU, P. (2004). Textrank : Bringing order into texts. In LIN, D. et WU,
D., ?diteurs : Proceedings of EMNLP 2004, pages 404?411, Barcelona, Spain. Association for
Computational Linguistics.
TOUTANOVA, K., KLEIN, D., MANNING, C. et SINGER, Y. (2003). Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the 3rd Conference of the North American
Chapter of the ACL (NAACL 2003), pages 173?180. Association for Computational Linguistics.
WITTEN, I. H., PAYNTER, G. W., FRANK, E., GUTWIN, C. et NEVILL-MANNING, C. G. (1999). Kea :
Practical automatic keyphrase extraction. CoRR, cs.DL/9902007.
68
