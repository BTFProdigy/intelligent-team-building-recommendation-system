Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013?1023,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Collective Cross-Document Relation Extraction Without Labelled Data
Limin Yao Sebastian Riedel Andrew McCallum
University of Massachusetts, Amherst
{lmyao,riedel,mccallum}@cs.umass.edu
Abstract
We present a novel approach to relation ex-
traction that integrates information across doc-
uments, performs global inference and re-
quires no labelled text. In particular, we
tackle relation extraction and entity identifi-
cation jointly. We use distant supervision to
train a factor graph model for relation ex-
traction based on an existing knowledge base
(Freebase, derived in parts from Wikipedia).
For inference we run an efficient Gibbs sam-
pler that leads to linear time joint inference.
We evaluate our approach both for an in-
domain (Wikipedia) and a more realistic out-
of-domain (New York Times Corpus) setting.
For the in-domain setting, our joint model
leads to 4% higher precision than an isolated
local approach, but has no advantage over a
pipeline. For the out-of-domain data, we ben-
efit strongly from joint modelling, and observe
improvements in precision of 13% over the
pipeline, and 15% over the isolated baseline.
1 Introduction
Relation Extraction is the task of predicting seman-
tic relations over entities expressed in structured or
semi-structured text. This includes, for example,
the extraction of employer-employee relations men-
tioned in newswire, or protein-protein interactions
expressed in biomedical papers. It also includes the
prediction of entity types such as country, citytown
or person, if we consider entity types as unary rela-
tions.
A particularly attractive approach to relation ex-
traction is based on distant supervision.1 Here in
1Also called self training, or weak supervision.
place of annotated text, only an existing knowl-
edge base (KB) is needed to train a relation extrac-
tor (Mintz et al, 2009; Bunescu and Mooney, 2007;
Riedel et al, 2010). The facts in the KB are heuris-
tically aligned to an unlabelled training corpus, and
the resulting alignment is the basis for learning the
extractor.
Naturally, the predictions of a distantly supervised
relation extractor will be less accurate than those of
a supervised one. While facts of existing knowledge
bases are inexpensive to come by, the heuristic align-
ment to text will often lead to noisy patterns in learn-
ing. When applied to unseen text, these patterns will
produce noisy facts. Indeed, we find that extraction
precision still leaves much room for improvement.
This room is not as large as in previous work (Mintz
et al, 2009) where target text and training KB are
closely related. However, when we use the knowl-
edge base Freebase (Bollacker et al, 2008) and the
New York Times corpus (Sandhaus, 2008), we ob-
serve very low precision. For example, the preci-
sion of the top-ranked 50 nationality relation
instances is only 28%.
On inspection, it turns out that many of the errors
can be easily identified: they amount to violations
of basic compatibility constraints between facts. In
particular, we observe unsatisfied selectional pref-
erences of relations towards particular entity types
as types of their arguments. An example is the fact
that the first argument of nationality is always
a person while the second is a country. A sim-
ple way to address this is a pipeline: first predict
entity types, and then condition on these when pre-
dicting relations. However, this neglects the fact that
relations could as well be used to help entity type
prediction.
1013
While there is some existing work on enforcing
such constraints in a joint fashion (Roth and Yih,
2007; Kate and Mooney, 2010; Riedel et al, 2009),
they are not directly applicable here. The difference
is the amount of facts they take into account at the
same time. They focus on single sentence extrac-
tions, and only consider very few interacting facts.
This allows them to work with exact optimization
techniques such as (Integer) Linear Programs and
still remain efficient.2 However, when working on
a sentence level they fail to exploit the redundancy
present in a corpus. Moreover, the fewer facts they
consider at the same time, the lower the chance that
some of these will be incompatible, and that mod-
elling compatibility will make a difference.
In this work we present a novel approach that
performs relation extraction across documents, en-
forces selectional preferences, and needs no labelled
data. It is based on an undirected graphical model
in which variables correspond to facts, and factors
between them measure compatibility. In order to
scale up, we run an efficient Gibbs-Sampler at in-
ference time, and train our model using SampleR-
ank (Wick et al, 2009). In practice this leads to a
runtime behaviour that is linear in the size of the cor-
pus. For example, 200,000 documents take less than
three hours for training and testing.
For evaluation we consider two scenarios. First
we follow Mintz et al (2009), use Freebase as
source of distant supervision, and employ Wikipedia
as source of unlabelled text?we will call this an
in-domain setting. This scenario is somewhat arti-
ficial in that Freebase itself is partially derived from
Wikipedia, and in practice we cannot expect text and
training knowledge base to be so close. Hence we
also evaluate our approach on the New York Times
corpus (out-of-domain setting).
For in-domain data we make the following find-
ing. When we compare to an isolated baseline that
makes no use of entity types, our joint model im-
proves average precision by 4%. However, it does
not outperform a pipelined system. In the out-of-
domain setting, our collective model substantially
outperforms both other approaches. Compared to
the isolated baseline, we achieve a 15% increase in
2The pyramid algorithm of Kate and Mooney (2010) may
scale well, but it is not clear how to apply their scheme to cross-
document extraction.
precision. With respect to the pipeline approach, the
increase is 13%.
In the following we will first give some back-
ground information on relation extraction with dis-
tant supervision. Then we will present our graphi-
cal model as well as the inference and learning tech-
niques we apply. After discussing related work, we
present our empirical results and conclude.
2 Background
In this section we will introduce the terminology and
concepts we use throughout the paper. We will also
give a brief introduction to relation extraction, in
particular in the context of distant supervision.
2.1 Relations
We seek to extract facts about entities. Example en-
tities would be the company founder BILL GATES,
the company MICROSOFT, and the country USA.
A relation R is a set of tuples c over entities. We
will follow (Mintz et al, 2009) and call the term
R (c1, . . . cn) with c ? R a relation instance.3 It
denotes the membership of the tuple c in the re-
lation R. For example, founded (BILL GATES,
MICROSOFT) is a relation instance denoting that
BILL GATES and MICROSOFT are related in the
founded relation.
In the following we will always consider some set
of candidate tuples C that may or may not be re-
lated. We define Cn ? C to be set of all n-ary tu-
ples in C. Note that while our definition considers
general n-nary relations, in practice we will restrict
us to unary and binary relations C1 and C2.
Following previous work (Mintz et al, 2009; Ze-
lenko et al, 2003; Culotta and Sorensen, 2004) we
make one more simplifying assumption: every can-
didate tuple can be member of at most one relation.
2.2 Entity Types
An entity can be of one or several entity types. For
example, BILL GATES is a person, and a company
founder. Entity types correspond to the special
case of relations with arity one, and will be treated
as such in the following.
3Other commonly used terms are relational facts, ground
facts, ground atoms, and assertions.
1014
We care about entity types for two reasons. First,
they can be important for downstream applications:
if consumers of our extracted facts know the type
of entities, they can find them more easily, visu-
alize them more adequately, and perform opera-
tions specific to these types (write emails to persons,
book a hotel in a city, etc.). Second, they are use-
ful for extracting binary relations due to selectional
preferences?see section 2.6.
2.3 Mentions
In natural language text spans of tokens are used to
refer to entities. We call such spans entity mentions.
Consider, for example, the following sentence snip-
pet:
(1) Political opponents of President Evo Morales
of Bolivia have in recent days stepped up...
Here ?Evo Morales? is an entity mention of pres-
ident EVO MORALES, and ?Bolivia? a mention of
the country BOLIVIA he is the president of.
People often express relations between entities in
natural language texts by mentioning the participat-
ing entities in specific syntactic and lexical patterns.
We will refer to any tuple of mentions of entities
(e1, . . . en) in a sentence as candidate mention tu-
ple. If such a candidate expresses the relation R,
then it is a relation mention of the relation instance
R (e1, . . . , en).
Consider again example 1. Here the pair of en-
tity mentions (?Evo Morales?, ?Bolivia?) is a candi-
date mention tuple. In fact, in this case the candidate
is indeed a relation mention of the relation instance
nationality (EVO MORALES, BOLIVIA).
2.4 Relation Extraction
We define the task of relation extraction as follows.
We are given a corpus of documents and a set of
target relations. Then we are asked to predict all re-
lation instances I so that for each R (c) ? I there
exists at least one relation mention in the given cor-
pus.
The above definition covers a range of existing
approaches by varying over what we define as tar-
get corpus. On one end, we have extractors that
process text on a per sentence basis (Zelenko et al,
2003; Culotta and Sorensen, 2004). On the other
end, we have methods that take relation mentions
from several documents and use these as input fea-
tures (Mintz et al, 2009; Bunescu and Mooney,
2007).
There is a compelling reason for performing re-
lation extraction within a larger scope that consid-
ers mentions across documents: redundancy. Often
facts are mentioned in several sentences and doc-
uments. Some of these mentions may be difficult
to parse, or they use unseen patterns. But the more
mentions we consider, the higher the probability that
one does parse, and fits a pattern we have seen in the
training data.
Note that for relation extraction that considers
more than a single mention we have to solve the
coreference problem in order to determine which
mentions refer to the same entity. In the follow-
ing we will assume that coreference clusters are pro-
vided by a preprocessing step.
2.5 Distant Supervision
In relation extraction we often encounter a lack of
explicitly annotated text, but an abundance of struc-
tured data sources such as company databases or col-
laborative knowledge bases like Freebase. In order
to exploit this, many approaches use simple but ef-
fective heuristics to align existing facts with unla-
belled text. This labelled text can then be used as
training material of a supervised learner.
One heuristic is to assume that each candidate
mention tuple of a training fact is indeed expressing
the corresponding relation (Bunescu and Mooney,
2007). Mintz et al (2009) refer to this as the dis-
tant supervision assumption.
Clearly, this heuristic can fail. Let us again
consider the nationality relation between EVO
MORALES and BOLIVIA. In an 2007 article of the
New York Times we find this relation mention can-
didate:
(2) ...the troubles faced by Evo Morales in
Bolivia...
This sentence does not directly express that EVO
MORALES is a citizen of BOLIVIA, and hence vi-
olates the distant supervision assumption. The prob-
lem with this observation is that at training time
we may learn a relatively large weight for the
feature ?<Entity1> in <Entity2>? associated with
1015
nationality. When testing our model we then
encounter a sentence such as
(3) Arrest Warrant Issued for Richard Gere in
India.
that leads us to extract that RICHARD GERE is a cit-
izen of INDIA.
2.6 Global Consistency of Facts
As discussed above, distant supervision can lead to
noisy extractions. However, such noise can often be
easily identified by testing how compatible the ex-
tracted facts are to each other. In this work we are
concerned with a particular type of compatibility:
selectional preferences.
Relations require, or prefer, their arguments to be
of certain types. For example, the nationality
relation requires the first argument to be a person,
and the second to be a country. On inspection,
we find that these preferences are often not satis-
fied in a baseline distant supervision system akin to
Mintz et al (2009). This often results from patterns
such as ?<Entity1> in <Entity2>? that fire in many
cases where <Entity2> is a location, but not a
country.
3 Model
Our observations in the previous section suggest
that we should (a) explicitly model compatibil-
ity between extracted facts, and (b) integrate ev-
idence from several documents to exploit redun-
dancy. In this work we choose a Conditional Ran-
dom Field (CRF) to achieve this. CRFs are a natural
fit for this task: They allow us to capture correlations
in an explicit fashion, and to incorporate overlapping
input features from multiple documents.
The hidden output variables of our model areY =
(Yc)c?C . That is, we have one variable Yc for each
candidate tuple c ? C . This variable can take as
value any relation in C with the same arity as c. See
example relation variables in figure 1.
The observed input variablesX consists of a fam-
ily of variables Xc =
(
X1c, . . .X
m
c
)
m?M for each
candidate tuple c. Here Xic stores relevant observa-
tions we make for the i-th candidate mention tuple of
c in the corpus. For example, X1BILL GATES,MICROSOFT
in figure 1 would contain, among others, the pattern
?[M2] was founded by [M1]?.
3.1 Factor Templates
Our conditional probability distribution over vari-
ables X and Y is defined using using a set T of
factor templates. Each template Tj ? T defines
a set of factors {(yi,xi)}, a set Kj of feature in-
dices, parameters
{
?jk
}
k?Kj
and feature functions
{
f jk
}
k?Kj
. Together they define the following con-
ditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
P
k?Kj
?jkf
j
k(yi,xi)
(4)
In our case the set T consists of four templates
we will describe below. We construct this graphical
model using FACTORIE (McCallum et al, 2009), a
probabilistic programming language that simplifies
the construction process, as well as inference and
learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template
is unrolled, it creates one factor per variable Yc for
candidate tuple c ? C. The template also consists of
one weight ?Biasr and feature function f
Bias
r for each
possible relation r. fBiasr fires if the relation associ-
ated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need
to model the correlation between relation instances
and their mentions in text. For this purpose we de-
fine the template TMen that connects each relation
instance variable Yc with its observed mention vari-
ables Xc. Crucially, this template gathers mentions
from multiple documents, and enables us to exploit
redundancy.
The feature functions of this template are taken
from Mintz et al (2009). This includes features that
inspect the lexical content between entity mentions
in the same sentence, and the syntactic path between
them. One example is
fMen101 (yc,xc)
def
=
?
??
??
1 yc = founded ? ?i with
"M2 was founded by M1" ? xic
0 otherwise
.
1016
founder
Microsoft was 
founded by Bill Gates...
person
company
nationality
country
With Microsoft chairman 
Bill Gates soon relinquishing...
Bill Gates was 
born in the USA  in 1955
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1
?
j
k
f
j
k
(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need to
model the correlation between relation instances and
their mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
from (Mintz et al, 2009b) (with minor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Crucially, these templates function on a cross-
document level. They gather all mentions of the can-
didate tuple c and extract features from all of these.
3.1.3 Se ectional Preference Templates
To capture the correlations between entity types
and the relations the entities participate in, we in-
troduce the joint template TJoint. It connects a re-
lation instance variable Ye1,...,ea to the entity type
variables Ye1 , . . . , Yen . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The feature fires when the variables are in the
state r, t1 . . . ta. After training we would expect
a weight ?Joint
founder,person,company to be larger than
?Joint
founder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t that fire if ei is
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1 ?
j
kf
j
k(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need to
model the correlation between relation instances and
t ir mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
fr m (Mintz et al, 2009b) (with inor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Crucially, these templates function on a cross-
document level. They gather all mentions of the can-
didate tuple c and extract features from all of these.
3.1.3 Selectional Preference Templates
To capture the correlations between entity types
and the relations the entities participate in, we in-
troduce th j int tem late TJoint. It connects a re-
la ion instance variable Ye1,...,ea to the entity type
variables Ye1 , . . . , Ye . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The feature fires when the variables are in the
state r, t1 . . . ta. After training we would expect
a weight ?Jointfounder,person,company to be larger than
?Jointfounder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t that fire if ei is
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1 ?
j
kf
j
k(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we ne d to
model the correlation between relation instances and
their mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
from (Mintz et al, 2009b) (with minor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Cruci lly, these templates function on a cross-
document level. Th y gather all mentions of the can-
didate tupl c and extract features from all o these.
3.1.3 Selectional Preference Templates
To capture the correlations between entity types
and the relations he entities participate in, we in-
troduce the joint template TJoint. It connects a re-
lation instance variable Ye1,...,e to the entity type
variables Ye1 , . . . , Yen . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r, 1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The featur fires when the variables are i the
state r, t1 . . . ta. After training we would xpect
a weight ?Jointfounder,person,company to be larger than
?Jointfounder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t tha fire if ei is
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i )
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,; exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
Figure 1: Factor Graph of our model that captures selectional preferences and functionality constraints. For
readability we only label a subsets of equivalent variables and factors. Note that the graph shows an example
assignment to variables.
It tests whether for any mentions of the candidate
tuple the phrase "founded by" appears between the
mentions of the argument entities.
3.1.3 S lectional Preference T mplates
To capture the correlations between entity types
and relations the entities participate in, we introduce
the template TJoint. It connects a relation instance
variable Ye1,...,en to the individual entity type vari-
ables Ye1 , . . . , Yen . To measure the compatibility
between relation and entity variables, we use one
feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta) for each com-
bination of relation and entity types r, t1 . . . ta.
f Jointr,t1...ta fires when the factor variables are in the
state r, t1 . . . ta. For example, f Jointfounded,person,company
fires if Ye1 is in state person, Ye2 in state company,
and Ye1,e2 in state founded.
We also add a template TPair that measures the
pairwise compatibility between the relation variable
Ye1,...,ea and each entity variable Yei in isolation.
Here we use features fPairi,r,t that fire if ei is the i-th ar-
gument of c, has the entity type t and the candidate
tuple c is labelled as instance of relation r. For ex-
ample, fPair1,founded,person fires if Ye1(argument i = 1)
is in state person, and Ye1,e2 in state founded, re-
gardless of the state of Ye2 .
3.2 Inference
There are two types of inference we have to perform:
sampling from the posterior during training (see sec-
tion 3.3), and finding the most likely configuration
(aka MAP inference). In both settings we employ a
Gibbs sampler (Geman and Geman, 1990) that ran-
domly picks a variable Yc and samples its relation
value conditioned on its Markov Blanket. At test
time we decrease the temperature of our sampler in
order to find an approximation of the MAP solution.
3.3 Training
Most learning methods need to calculate the model
expectations (Lafferty et al, 2001) or the MAP con-
figuration (Collins, 2002) before making an update
to the parameters. This step of inference is usually
the bottleneck for learning, even when performed
approximately.
SampleRank (Wick et al, 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within MCMC infer-
ence. Every pair of consecutive samples in the
MCMC chain is ranked according to the model and
the ground truth, and the parameters are updated
when the rankings disagree. This update can fol-
low different schemes, here we use MIRA (Cram-
mer and Singer, 2003). This allows the learner to
acquire more supervision per instance, and has led
to efficient training for models in which inference
1017
is expensive and generally intractable (Singh et al,
2009).
4 Related Work
Distant Supervision Learning to extract relations
by using distant supervision has raised much interest
in recent years. Our work is inspired by Mintz et al
(2009) who also use Freebase as distant supervision
source. We also heuristically align our knowledge
base to text by making the distant supervision as-
sumption (Bunescu and Mooney, 2007; Mintz et al,
2009). However, in contrast to these previous ap-
proaches, and other related distant supervision meth-
ods (Craven and Kumlien, 1999; Weld et al, 2009;
Hoffmann et al, 2010), we perform relation extrac-
tion collectively with entity type prediction.
Schoenmackers et al (2008) use entailment rules
on assertion extracted by TextRunner to increase re-
call. They also perform cross-document probabilis-
tic inference based on Markov Networks. However,
they do not infer the types of entities and work in an
open IE setting.
Selectional Preferences In the context of super-
vised relation extraction, selectional preferences
have been applied. For example, Roth and Yih
(2007) have used Linear Programming to enforce
consistency between entity types and extracted re-
lations. Kate and Mooney (2010) use a pyramid
parsing scheme to achieve the same. Riedel et al
(2009) use Markov Logic to model interactions be-
tween event-argument relations for biomedical event
extraction. However, their work is (a) supervised,
and (b) performs extraction on a per-sentence basis.
Carlson et al (2010) also use selectional prefer-
ences. However, instead of exploiting them for train-
ing a graphical model using distant supervision, they
use selectional preferences to improve a bootstrap-
ping process. Here in each iteration of bootstrap-
ping, extracted facts that violate compatibility con-
straints will not be used to generate additional pat-
terns in the next iteration.
5 Experiments
We set up experiments to answer the following ques-
tions: (i) Does the explicit modelling of selectional
preferences improve accuracy? (ii) Can we also per-
form joint entity and relation extraction in a pipeline
and achieve similar results? (iii) How does our
cross-document approach scale?
To answer these questions we carry out experi-
ments on two data sets, Wikipedia and New York
Times articles, and use Freebase as distant supervi-
sion source for both.
5.1 Experimental Setup
We follow Mintz et al (2009) and perform two types
of evaluation: held-out and manual. In both cases
we have a training and a test corpus of documents,
and training and test sets of entities. For held-out
evaluation we split the set of entities in Freebase into
training and test sets. For manual evaluation we use
all Freebase entities during training. For testing we
use all entities that appear in the test document cor-
pus.
For both training and testing we then choose the
candidate tuples C that may or may not be relation
instances. To pick the entities C1 we want to predict
entity types for, we choose all entities that are men-
tioned at least once in the train/test corpus. To pick
the entity pairs C2 that we want to predict the rela-
tions of, we choose those that appear at least once
together in a sentence.
The set of candidates C will contain many tuples
which are not related in any Freebase relations. For
efficiency, we filter out a large fraction of these neg-
ative candidates for training. The number of neg-
ative examples we keep is chosen to be about 10
times the number of positive candidates. This num-
ber stems from trading-off the accuracy it leads to
and the increased training time it requires.
For both manual and held-out evaluation we rank
extracted test relation instances in the MAP state of
the network. This state is found by sampling 20 iter-
ations with a low temperature of 0.00001. The rank-
ing is done according to the log linear score that the
assigned relation for a candidate tuple gets from the
factors in its Markov Blanket. For optimal perfor-
mance, the score is normalized by the number of re-
lation mentions.
For manual evaluation we pick the top ranked 50
relation instances for the most frequent relations.
We ask three annotators to inspect the mentions of
these relation instances to decide whether they are
correct. Upon disagreement, we use majority vote.
To summarize precisions across relations, we take
1018
their average, and their average weighted by the pro-
portion of predicted instances for the given relation.
5.1.1 Data preprocessing
We preprocess our textual data as follows:
We first use the Stanford named entity recog-
nizer (Finkel et al, 2005) to find entity mentions in
the corpus. The NER tagger segments each docu-
ment into sentences and classifies each token into
four categories: PERSON, ORGANIZATION, LO-
CATION and NONE. We treat consecutive tokens
which share the same category as single entity men-
tion. Then we associate these mentions with Free-
base entities. This is achieved by performing a
string match between entity mention phrases and the
canonical names of entities as present in Freebase.
For each candidate tuple c with arity 2 and each
of its mention tuples iwe extract a set of featuresXic
similar to those used in (Mintz et al, 2009): lexical,
Part-Of-Speech (POS), named entity and syntactic
features, i.e. features obtained from the dependency
parsing tree of a sentence. We use the openNLP POS
tagger4 to obtain POS tags and employ the Malt-
Parser (Nivre et al, 2004) for dependency parsing.
For candidate tuples with arity 1 (entity types) we
use the following features: the entity?s word form,
the POS sequence, the head of the entity in the de-
pendency parse tree, the Stanford named entity tag,
and the left and right words to the current entity
mention phrase.
5.1.2 Configurations
We apply the following configurations of our fac-
tor graphs. As our baseline, and roughly equivalent
to previous work (Mintz et al, 2009), we pick the
templates TBias and TMen. These describe a fully dis-
connected graph, and we will refer to this configu-
ration as isolated. Next, we add the templates TJoint
and TPair to model selectional preferences, and refer
to this setting as joint.
In addition, we evaluate howwell selectional pref-
erences can be captured with a simple pipeline. For
this pipeline we first train an isolated system for en-
tity type prediction. Then we use the output of the
entity type prediction system as input for the relation
extraction system.
4available at http://opennlp.sourceforge.net/
5.1.3 Entity types and Relation types
Freebase contains many relation types and only
a subset of those relation types occur frequently
in the corpus. Since classes with very few
training instances are generally hard to learn,
we restrict ourselves to the 54 most frequently
mentioned relations. These include, for ex-
ample, nationality, contains, founded
and place_of_birth. Note that we con-
vert two Freebase non-binary temporal relations
to binary relations: employment_tenure and
place_lived. In both cases we simply disregard
the temporal information in the Freebase data.
As our main focus is relation extraction, we re-
strict ourselves to entity types compatible with our
selected relations. To this end we inspect the Free-
base schema information provided for each relation,
and include those entity types that are declared as
arguments of our relations. This leads to 10 entity
types including person, citytown, country,
and company.
Note that a Freebase entity can have several types.
We pick one of these by choosing the most specific
one that is a member of our entity type subset, or
MISC if no such member exists.
5.2 Wikipedia
In our first set of experiments we train and test using
Wikipedia as the text corpus. This is a comparatively
easy scenario because the facts in Freebase are partly
derived from Wikipedia, hence there is an increased
chance of properly aligning training facts and text.
This is similar to the setting of Mintz et al (2009).
5.2.1 Held Out Evaluation
We split 1,300,000 Wikipedia articles into train-
ing and test sets. Table 1 shows the statistics for this
split. The last row provides the number of negative
relation instances (candidates which are not related
according to Freebase) associated with each data set.
Figure 2 shows the precision-recall curves of re-
lation extraction for held-out data of various config-
urations. We notice a slight advantage of the joint
approach in the low recall area. Moreover, the joint
model predicts more relation instances, as can be
seen by its longer line in the graph.
For higher recall, the joint model performs
slightly worse. On closer inspection, we find that
1019
Wikipedia NYT
Train Test Train Test
#Documents 900K 400K 177K 39K
#Entities 213K 137K 56K 27K
#Positive 36K 24K 5K 2K
#Negative 219K 590K 64K 94K
Table 1: The statistics of held-out evaluation on
Wikipedia and New York Times.
0.0 0.1 0.2 0.3 0.4
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Recall
Pre
cisio
n
joint
pipe
isolated
Figure 2: Precision-recall curves for various setups
in Wikipedia held-out setting.
this observation is somewhat misleading. Many of
the predictions of the joint model are not in the
held-out test set derived from Freebase, but never-
theless correct. Hence, to understand if one system
really outperforms another, we need to rely on man-
ual evaluation.
Note that the figure only considers binary
relations?for entity types all configurations per-
form similarly.
5.2.2 Manual Evaluation
As mentioned above, held-out evaluation in this
context suffers from false negatives in Freebase. Ta-
ble 2 therefore shows the results of our manual eval-
uation. They are based on the average, and weighted
average, of the precisions for the relation instances
of the most frequent relations. We notice that here
Isolated Pipeline Joint
Wikipedia 0.82 0.87 0.86
Wikipedia (w) 0.95 0.94 0.95
NYT 0.63 0.65 0.78
NYT (w) 0.78 0.82 0.94
Table 2: Average and weighted (w) average preci-
sion over frequent relations for New York Times and
Wikipedia data, based on manual evaluation.
all systems perform comparably for weighted aver-
age precision. For average precision we see an ad-
vantage for both the pipeline and the joint model
over the isolated system.
One reason for similar weighted average preci-
sions is the fact that all approaches accurately pre-
dict a large number of contains instances. This is
due to very regular and simple patterns inWikipedia.
For example, most articles on towns start with ?A is
a municipality in the district of B in C, D.? For these
sentences, the relative position of two location men-
tions is a very good predictor of contains. When
used as a feature, it leads to high precision for all
models. And since contains instances are most
frequent, and we take the weighted average, results
are generally close to each other.
To summarize: in this in-domain setting, mod-
elling compatibility between entity types and rela-
tions helps to improve average precision, but not
weighted average precision. This holds for both the
joint and the pipeline model. However, we will see
how this changes substantially when moving to an
out-of-domain scenario.
5.3 New York Times
For our second set of experiments we use New
York Times data as training and test corpora. As
we argued before, this is expected to be the more
difficult?and more realistic?scenario.
5.3.1 Held-out Evaluation
We choose all articles of the New York times dur-
ing 2005 and 2006 as training corpus. As test corpus
we use the first 6 months of 2007.
Figure 3 shows precision-recall curves for our var-
ious setups. We see that jointly modelling entity
1020
0.00 0.05 0.10 0.15 0.20
0.2
0.4
0.6
0.8
1.0
Recall
Pre
cisio
n
joint
pipe
isolated
Figure 3: Precision-recall curves for various setups
in New York Times held-out setting.
types and relations helps to improve precision.
Due to the smaller overlap between Freebase and
NYT data, figure 3 also has to be taken with more
caution. The systems may predict correct relation
instances that just do not appear in Freebase. Hence
manual evaluation is even more important.
When evaluating entity precision we find that for
both models it is about 84%. This raises the ques-
tion why the joint entity type and relation extrac-
tion model outperforms the pipeline on relations.
We take a close look at the entities which partici-
pate in relations and find that joint model performs
better on most entity types, for example, country
and citytown. We also look at the relation in-
stances which are predicted by both systems and find
that the joint model does predict correct entity types
when the pipeline mis-predicts. And exactly these
mis-predictions lead the pipeline astray. Consider-
ing binary relation instances where the pipeline fails
but the joint model does not, we observe an entity
precision of 76% for the pipeline and 86% for our
joint approach. The joint model fails to correctly
predict some entity types that the pipeline gets right,
but these tend to appear in contexts where relation
instances are easy to extract without considering en-
Relation Type Iso. Pipe Joint
contains 0.92 0.98 0.96
nationality 0.28 0.64 0.82
plc_lived 0.88 0.70 0.96
plc_of_birth 0.32 0.20 0.25
works_for 0.96 0.98 0.98
plc_of_death 0.24 0.40 0.42
children 1.00 0.92 0.98
founded 0.42 0.34 0.71
Table 3: Precision at 50 for the most frequent rela-
tions on New York Times
tity types.5
5.3.2 Manual Evaluation
Manually evaluated precision for New York
Times data can be seen in table 2. In contrast to the
Wiki setting, here modelling entity types and rela-
tions jointly makes a substantial difference. For av-
erage precision, our joint model improves over the
isolated baseline by 15%, and over the pipeline by
13%. Similar improvements can be observed for
weighted average precision.
Let us look at a break-down of precisions with
respect to different relations shown in table 3. We
see dramatic improvements for nationality and
founded when applying the joint model. Note that
the nationality relation takes a larger part in
the predicted relation instances of the joint model
and hence contributes significantly to the weighted
average precision.
5.4 Scalability
We propose to perform joint inference for large scale
information extraction. An obvious concern in this
scenario is scalability. In practice we find that infer-
ence (and hence learning) in our model scales lin-
early with the number of candidate tuples. This can
be seen in figure 4a. It is to be expected since the
number of candidates equals the number of variables
the sampler has to process in each iteration.
The above observation also means that our ap-
proach scales linearly with corpus size. To illustrate
5Note that our learned preferences are soft, and hence can
be violated in case of wrong entity type predictions.
1021
1e+05 2e+05 3e+05 4e+05 5e+05
200
300
400
500
600
700
800
Number of Candidate Tuples
Time pe
r iteratio
n (seco
nds)
(a) CPU time
0 5000 10000 15000 20000 25000
1e+05
2e+05
3e+05
4e+05
5e+05
6e+05
Number of Documents
Numbe
r of Can
didate T
uples
(b) Candidate tuples
Figure 4: CPU time for one iteration per candidate
tuple, and candidate tuples per document.
this, figure 4b shows how the number of candidates
scales with the number of documents. Again we ob-
serve a linear behavior. Since both are linear, we can
say that our joint approach is linear in the number of
documents.
Total training and test times are moderate, too.
For example, the held-out experiments with 200,000
NYT documents finish within three hours.
6 Conclusion
This paper presents a novel approach to extracting
relational facts from text. Akin to previous work in
relation extraction with distant supervision, we re-
quire no annotated text. However, instead extract-
ing facts in isolation, we model interactions between
facts in order to improve precision. In particular, we
capture selectional preferences of relations. These
preferences are modelled in a cross-document fash-
ion using a large scale factor graph. We show in-
ference and learning can be efficiently performed
in linear time by Gibbs Sampling and SampleRank.
When applied to out-of-domain text, this approach
leads to a 15% increase in precision over an isolated
baseline, and a 13% improvement over a pipelined
system.
A crucial aspect of our approach is its extensibil-
ity. Since it is exclusively framed in terms of an
undirected graphical model, it is conceptually easy
to extend it to other types of compatibilities, such
as functionality constraints. It could also be ex-
tended to tackle coreference resolution. Eventually
we seek to model the complete process of the au-
tomatic construction of KB within this framework,
and capture dependencies between extractions in a
joint and principled fashion. As we have seen here,
in particular when learning is less supervised and
extractions are noisy, capturing such interactions is
paramount.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and in part by UPenn
NSF medium IIS-0803847. The University of Mas-
sachusetts also gratefully acknowledges the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ?08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247?1250, New York, NY, USA.
ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Andrew Carlson, Justin Betteridge, Richard Wang, Es-
tevam Hruschka, and Tom Mitchell. 2010. Cou-
pled semi-supervised learning for information extrac-
tion. In Third ACM International Conference on Web
Search and Data Mining (WSDM ?10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical methods in natural lan-
guage processing (EMNLP ?02), volume 10, pages 1?
8.
1022
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77?86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
S. Geman and D. Geman. 1990. Stochastic relaxation,
gibbs distributions, and the bayesian restoration of im-
ages. pages 452?472.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In ACL.
Rohit J. Kate and Raymond J. Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the 12th Conference on Com-
putational Natural Language Learning (CoNLL? 10).
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In International Conference on Machine Learn-
ing (ICML).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Y. Bengio, D. Schuur-
mans, J. Lafferty, C. K. I. Williams, and A. Culotta, ed-
itors, Advances in Neural Information Processing Sys-
tems 22, pages 1249?1257.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ?09),
pages 1003?1011. Association for Computational Lin-
guistics.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ?09), pages 41?49.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ?08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Sameer Singh, Karl Schultz, and Andrew McCallum.
2009. Bi-directional joint inference for entity res-
olution and segmentation using imperatively-defined
factor graphs. In European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML PKDD), pages 414?
429.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using wikipedia to bootstrap open information extrac-
tion. In ACM SIGMOD Record.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009. Samplerank: Learning
preferences from atomic gradients. In Neural Infor-
mation Processing Systems (NIPS), Workshop on Ad-
vances in Ranking.
Dimitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. JMLR, 3(6):1083 ? 1106.
1023
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456?1466,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Relation Discovery using Generative Models
Limin Yao? Aria Haghighi+ Sebastian Riedel? Andrew McCallum?
? Department of Computer Science, University of Massachusetts at Amherst
+ CSAIL, Massachusetts Institute of Technology
{lmyao,riedel,mccallum}@cs.umass.edu
{aria42}@csail.mit.edu
Abstract
We explore unsupervised approaches to rela-
tion extraction between two named entities;
for instance, the semantic bornIn relation be-
tween a person and location entity. Con-
cretely, we propose a series of generative
probabilistic models, broadly similar to topic
models, each which generates a corpus of ob-
served triples of entity mention pairs and the
surface syntactic dependency path between
them. The output of each model is a cluster-
ing of observed relation tuples and their as-
sociated textual expressions to underlying se-
mantic relation types. Our proposed models
exploit entity type constraints within a relation
as well as features on the dependency path be-
tween entity mentions. We examine effective-
ness of our approach via multiple evaluations
and demonstrate 12% error reduction in preci-
sion over a state-of-the-art weakly supervised
baseline.
1 Introduction
Many NLP applications would benefit from large
knowledge bases of relational information about
entities. For instance, knowing that the entity
Steve Balmer bears the leaderOf relation to the
entity Microsoft, would facilitate question answer-
ing (Ravichandran and Hovy, 2002), data mining,
and a host of other end-user applications. Due to
these many potential applications, relation extrac-
tion has gained much attention in information ex-
traction (Kambhatla, 2004; Culotta and Sorensen,
2004; Mintz et al, 2009; Riedel et al, 2010; Yao et
al., 2010). We propose a series of generative prob-
abilistic models, broadly similar to standard topic
models, which generate a corpus of observed triples
of entity mention pairs and the surface syntactic de-
pendency path between them. Our proposed mod-
els exploit entity type constraints within a relation
as well as features on the dependency path between
entity mentions. The output of our approach is a
clustering over observed relation paths (e.g. ?X was
born in Y? and ?X is from Y?) such that expressions
in the same cluster bear the same semantic relation
type between entities.
Past work has shown that standard supervised
techniques can yield high-performance relation de-
tection when abundant labeled data exists for a
fixed inventory of individual relation types (e.g.
leaderOf ) (Kambhatla, 2004; Culotta and Sorensen,
2004; Roth and tau Yih, 2002). However, less ex-
plored are open-domain approaches where the set
of possible relation types are not fixed and little to
no labeled is given for each relation type (Banko et
al., 2007; Banko and Etzioni, 2008). A more re-
lated line of research has explored inducing rela-
tion types via clustering. For example, DIRT (Lin
and Pantel, 2001) aims to discover different repre-
sentations of the same semantic relation using dis-
tributional similarity of dependency paths. Poon
and Domingos (2008) present an Unsupervised se-
mantic parsing (USP) approach to partition depen-
dency trees into meaningful fragments (or ?parts?
to use their terminology). The combinatorial nature
of this dependency partition model makes it difficult
for USP to scale to large data sets despite several
necessary approximations during learning and infer-
1456
ence. Our work is similar to DIRT and USP in that
we induce relation types from observed dependency
paths, but our approach is a straightforward and
principled generative model which can be efficiently
learned. As we show empirically, our approach out-
performs these related works when trained with the
same amount of data and further gains are observed
when trained with more data.
We evaluate our approach using ?intrinsic? clus-
tering evaluation and ?extrinsic? evaluation settings.1
The former evaluation is performed using subset of
induced clusters against Freebase relations, a large
manually-built entity and relational database. We
also show some clusters which are not included as
Freebase relations, as well as some entity clusters
found by our approach. The latter evaluation uses
the clustering induced by our models as features for
relation extraction in distant supervision framework.
Empirical results show that we can find coherent
clusters. In relation extraction, we can achieve 12%
error reduction in precision over a state-of-the-art
weakly supervised baseline and we show that using
features from our proposed models can find more
facts for a relation without significant accuracy loss.
2 Problem and Experimental Setup
The task of relation extraction is mapping surface
textual relations to underlying semantic relations.
For instance, the textual expression ?X was born in
Y? indicates a semantic relation bornIn between en-
tities ?X? and ?Y?. This relation can be expressed
textually in several ways: for instance, ?X, a native
of Y? or ?X grew up in Y?. There are several com-
ponents to a coherent relation type, including a tight
small number of textual expressions as well as con-
straints on the entities involved in the relation. For
instance, in the bornIn relation ?X? must be a person
entity and ?Y? a location (typically a city or nation).
In this work, we present an unsupervised probabilis-
tic generative model for inducing clusters of relation
types and recognizing their textual expressions. The
set of relation types is not pre-specified but induced
from observed unlabeled data. See Table 4 for ex-
amples of learned semantic relations.
Our observed data consists of a corpus of docu-
ments and each document is represented by a bag
1See Section 4 for a fuller discussion of evaluation.
of relation tuples. Each tuple represents an ob-
served syntactic relationship between two Named
Entities (NE) and consists of three components: the
dependency path between two NE mentions, the
source argument NE, and the destination argument
NE. A dependency path is a concatenation of depen-
dency relations (edges) and words (nodes) along a
path in a dependency tree. For instance, the sentence
?John Lennnon was born in Liverpool? would yield
the relation tuple (Lennon, [? ?nsubjpass, born, ?
?in], Liverpool). This relation tuple reflects a se-
mantic bornIn relation between the John Lennon and
Liverpool entities. The dependency path in this ex-
ample corresponds to the ?X was born in Y? textual
expression given earlier. Note that for the above ex-
ample, the bornIn relation can only occur between a
person and a location. The relation tuple is the pri-
mary observed random variable in our model and we
construct our models (see Section 3) so that clusters
consist of textual expressions representing the same
underlying relation type.
3 Models
We propose three generative models for modeling
tuples of entity mention pairs and the syntactic de-
pendency path between them (see Section 2). The
first two models, Rel-LDA and Rel-LDA1 are sim-
ple extensions of the standard LDA model (Blei et
al., 2003). At the document level, our model is iden-
tical to standard LDA; a multinomial distribution
is drawn over a fixed number of relation types R.
Changes lie in the observations. In standard LDA,
the atomic observation is a word drawn from a la-
tent topic distribution determined by a latent topic
indicator variable for that word position. In our ap-
proach, a document consists of an exchangeable set
of relation tuples. Each relation tuple is drawn from
a relation type ?topic? distribution selected by a la-
tent relation type indicator variable. Relation tuples
are generated using a collection of independent fea-
tures drawn from the underlying relation type distri-
bution. These changes to standard LDA are intended
to have the effect that instead of representing seman-
tically related words, the ?topic? latent variable rep-
resents a relation type.
Our third model exploits entity type constraints
within a relation and induces clusters of relations
1457
and entities jointly. For each tuple, a set of rela-
tion level features and two latent entity type indica-
tors are drawn independently from the relation type
distribution; a collection of entity mention features
for each argument is drawn independently from the
entity type distribution selected by the entity type
indicator.
Path X, made by Y
Source Gamma Knife
Dest Elekta
Trigger make
Lex , made by the Swedish
medical technology firm
POS , VBN IN DT JJ JJ NN NN
NER pair MISC-ORG
Sync pair partmod-pobj
Table 1: The features of tuple ?(Gamma Knife, made
by, Elekta)? in sentence ?Gamma Knife, made by the
Swedish medical technology firm Elekta, focuses low-
dosage gamma radiation ...?
3.1 Rel-LDA Model
This model is an extension to the standard LDA
model. At the document level, a multinomial dis-
tribution over relations ?doc is drawn from a prior
Dir(?). To generate a relation tuple, we first draw a
relation ?topic? r from Multi(?). Then we generate
each feature f of a tuple independently from a multi-
nomial distribution Multi(?rf ) selected by r. In this
model, each tuple has three features, i.e. its three
components, shown in the first three rows in Table 1.
Figure 1 shows the graphical representation of Rel-
LDA. Table 2 lists all the notation used in describing
our models.
The learning process of the models is an EM pro-
cess. The procedure is similar to that used by the
standard topic model. In the variational E-step (in-
ference), we sample the relation type indicator for
each tuple using p(r|f):
P (r|f(p, s, d)) ? p(r)?f p(f |r)
? (?r + nr|d)
?
f
?f+nf |rP
f ? (?f ?+nf ?|r)
|R| Number of relations
|D| Number of documents
r A relation
doc A document
p, s, d Dep path, source and dest args
f A feature/feature type
T Entity type of one argument
? Dirichlet prior for ?doc
?x Dirichlet prior for ?rx
? Dirichlet prior for ?t
?doc p(r|doc)
?rx p(x|r)
?t p(fs|T ), p(fd|T )
Table 2: The notation used in our models
      
  
             |R|  
      
  
  
......
                                        N 
r
f
?
?
rf
?
?
f
f
      
  
  
                           
                           
                                             |D|  
Figure 1: Rel-LDA model. Shaded circles are observa-
tions, and unshaded ones are hidden variables. A docu-
ment consists of N tuples. Each tuple has a set of fea-
tures. Each feature of a tuple is generated independently
from a hidden relation variable r.
p(r) and p(f |r) are estimated in the M-step:
?doc =
?+ nr|doc?
r?(?+ nr?|doc)
?rf =
?f + nf |r?
f ?(?f ? + nf ?|r)
where nf |r indicates the number of times a feature f
is assigned with r.
3.2 Rel-LDA1
Looking at results of Rel-LDA, we find the clus-
ters sometimes are in need of refinement, and we
can address this by adding more features. For in-
stance, adding trigger features can encourage spar-
sity over dependency paths. We define trigger words
as all the words on the dependency path except stop
words. For example, from path ?X, based in Y?,
?base? is extracted as a trigger word. The intuition
1458
for using trigger words is that paths sharing the same
set of trigger words should go to one cluster. Adding
named entity tag pair can refine the cluster too. For
example, a cluster found by Rel-LDA contains ?X
was born in Y? and ?X lives in Y?; but it also con-
tains ?X, a company in Y?. In this scenario, adding
features ?PER-LOC? and ?ORG-LOC? can push the
model to split the clusters into two and put the third
case into a new cluster.
Hence we propose Rel-LDA1. It is similar to
Rel-LDA, except that each tuple is represented with
more features. Besides p, s, and d, we introduce
trigger words, lexical pattern, POS tag pattern, the
named entity pair and the syntactic category pair fea-
tures for each tuple. Lexical pattern is the word se-
quence between the two arguments of a tuple and
POS tag pattern is the POS tag sequence of the lexi-
cal pattern. See Table 1 as an example.
Following typical EM learning(Charniak and El-
sner, 2009), we start with a much simpler genera-
tive model, expose the model to fewer features first,
and iteratively add more features. First, we train a
Rel-LDA model, i.e. the model only generates the
dependency path, source and destination arguments.
After each interval of 10 iterations, we introduce one
additional feature. We add the features in the order
of trigger, lexical pattern, POS, NER pair, and syn-
tactic pair.
3.3 Type-LDA model
We know that relations can only hold between
certain entity types, known as selectional prefer-
ences (Ritter et al, 2010; Seaghdha, 2010; Kozareva
and Hovy, 2010). Hence we propose Type-LDA
model. This model can capture the selectional pref-
erences of relations to their arguments. In the mean
time, it clusters tuples into relational clusters, and
arguments into different entity clusters. The entity
clusters could be interesting in many ways, for ex-
ample, defining fine-grained entity types and finding
new concepts.
We split the features of a tuple into relation level
features and entity level features. Relation level fea-
tures include the dependency path, trigger, lex and
POS features; entity level features include the entity
mention itself and its named entity tag.
The generative storyline is as follows. At the doc-
ument level, a multinomial distribution over rela-
      
  
  
                                        N   
      
  
  
        
                      
                                                   |D|  
      
  
             |R|  
r
f
f
s
?
?
rf
?
t
f
d
     
  
              |R| 
?
rt2
?
?
t2
?
?
f
T
1
T
2
     
  
              |T| 
     
  
              |R| 
?
rt1
?
t1
Figure 2: Type-LDA model. Each document consists of
N tuples. Each tuple has a set of features, relation level
features f and entity level features of source argument fs
and destination argument fd. Relation level features and
two hidden entity types T1 and T2 are generated from
hidden relation variable r independently. Source entity
features are generated from T1 and destination features
are generated from T2.
tions ?doc is drawn from a Dirichlet prior. A doc-
ument consists of N relation tuples. Each tuple is
represented by relation level features (f ) and entity
level features of source argument (fs) and destina-
tion argument (fd). For each tuple, a relation r is
drawn from Multi(?doc). The relation level features
and two hidden entity types T1 and T2 are indepen-
dently generated from r. Features fs are generated
from T1 and fd from T2. Figure 2 shows the graphi-
cal representation of this model.
At inference time, we sample r, T1 and T2 for
each tuple. For efficient inference, we first initialize
the model without T1 and T2, i.e. all the features are
generated directly from r. Here the model degener-
ates to Rel-LDA1. After some iterations, we intro-
duce T1 and T2. We sample the relation variable (r)
and two mention types variables (T1,T2) iteratively
for each tuple. We can sample them together, but
this is not very efficient. In addition, we found that
it does not improve performance.
4 Experiments
Our experiments are carried out on New York Times
articles from year 2000 to 2007 (Sandhaus, 2008).
We filter out some noisy documents, for example,
1459
obituary content, lists and so on. Obituary arti-
cles often contain syntax that diverges from stan-
dard newswire text. This leads to parse errors with
WSJ-trained parsers and in turn, makes extraction
harder. We also filter out documents that contain
lists or tables of items (such as books, movies) be-
cause this semi-structured information is not the fo-
cus of our current work. After filtering we are left
with approximately 428K documents. They are pre-
processed in several steps. First we employ Stanford
tools to tokenize, sentence-split and Part-Of-Speech
tag (Toutanova et al, 2003) a document. Next we
recognize named entities (Finkel et al, 2005) by
labelling tokens with PERSON, ORGANIZATION,
LOCATION, MISC and NONE tags. Consecutive
tokens which share the same category are assembled
into entity mentions. They serve as source and des-
tination arguments of the tuples we seek to model.
Finally we parse each sentence of a document using
MaltParser (Nivre et al, 2004) and extract depen-
dency paths for each pair of named entity mentions
in one sentence.
Following DIRT (Lin and Pantel, 2001), we fil-
ter out tuples that do not satisfy the following con-
straints. First, the path needs to be shorter than
10 edges, since longer paths occur less frequently.
Second, the dependency relations in the path should
connect two content words, i.e. nouns, verbs, ad-
jectives and adverbs. For example, in phrase ?solve
a problem?, ?obj(solve, problem)? is kept, while
?det(problem, a)? is discarded. Finally, the de-
pendency labels on the path must not be: ?conj?,
?ccomp?, ?parataxis?, ?xcomp?, ?pcomp?, ?advcl?,
?punct?, and ?infmod?. This selection is based on the
observation that most of the times the corresponding
dependency relations do not explicitly state a rela-
tion between two candidate arguments.
After all entity mentions are generated and paths
are extracted, we have nearly 2.5M tuples. After
clustering (inference), each of these tuple will be-
long to one cluster/relation and is associated with its
clusterID.
We experimented with the number of clusters and
find that in a range of 50-200 the performance does
not vary significantly with different numbers. In our
experiments, we cluster the tuples into 100 relation
clusters for all three models. For Type-LDA model,
we use 50 entity clusters.
We evaluate our models in two ways. The first
aims at measuring the clustering quality by mapping
clusters to Freebase relations. The second seeks to
assess the utility of our predicted clusters as features
for relation extraction.
4.1 Relations discovered by different models
Looking closely at the clusters we predict, we find
that some of them can be mapped to Freebase rela-
tions. We discover clusters that roughly correspond
to the parentCom (parent company relation), filmDi-
rector, authorOf, comBase (base of a company rela-
tion) and dieIn relations in Freebase. We treat Free-
base annotations as ground truth and measure recall.
We count each tuple in a cluster as true positive if
Freebase states the corresponding relation between
its argument pair. We find that precision numbers
against Freebase are low, below 10%. However,
these numbers are not reliable mainly because many
correct instances found by our models are missing
in Freebase. One reason why our predictions are
missing in Freebase is coreference. For example,
we predict parentCom relation between ?Linksys?
and ?Cisco?, while Freebase only considers ?Cisco
Systems, Inc.? as the parent company of ?Linksys?.
It does not corefer ?Cisco? to ?Cisco Systems, Inc.?.
Incorporating coreference in our model may fix this
problem and is a focus of future work. Instead of
measuring precision against Freebase, we ask hu-
mans to label 50 instances for each cluster and report
precision according to this annotated data. Table 3
shows the scores.
We can see that in most cases Rel-LDA1 and
Type-LDA substantially outperform the Rel-LDA
model. This is due to the fact that both models can
exploit more features to make clustering decisions.
For example, in Rel-LDA1 model, the NER pair fea-
ture restricts the entity types the two arguments can
take.
In the following, we take parentCom relation as
an example to analyze the behaviors of different
models. Rel-LDA includes spurious instances such
as ?A is the chief executive of B?, while Rel-LDA1
has fewer such instances due to the NER pair fea-
ture. Similarly, by explicitly modeling entity type
constraints, Type-LDA makes fewer such errors. All
our models make mistakes when sentences have co-
ordination structures on which the parser has failed.
1460
Rel. Sys. Rec. Prec.
parentCom
Rel-LDA 51.4 76.0
Rel-LDA1 49.5 78.0
Type-LDA 55.3 72.0
filmDirector
Rel-LDA 42.5 32.0
Rel-LDA1 70.5 40.0
Type-LDA 74.2 26.0
comBase
Rel-LDA 31.5 12.0
Rel-LDA1 54.2 22.0
Type-LDA 57.1 30.0
authorOf
Rel-LDA 25.2 84.0
Rel-LDA1 46.9 86.0
Type-LDA 20.2 68.0
dieIn
Rel-LDA 26.5 34.0
Rel-LDA1 55.9 40.0
Type-LDA 50.2 28.0
Table 3: Clustering quality evaluation (%), Rec. is mea-
sured against Freebase, Prec. is measured according to
human annotators
For example, when a sentence has the following pat-
tern ?The winners are A, a part of B; C, a part of
D; E, a part of F?, our models may predict parent-
Com(A,F), because the parser connects A with F via
the pattern ?a part of?.
Some clusters found by our models cannot be
mapped to Freebase relations. Consider the Free-
base relation worksFor as one example. This re-
lation subsumes all types of employment relation-
ships, irrespective of the role the employee plays for
the employer. By contrast, our models discover clus-
ters such as leaderOf, editorOf that correspond to
more specific roles an employee can have. We show
some example relations in Table 4. In the table, the
2nd row shows a cluster of employees of news media
companies; the 3rd row shows leaders of companies;
the last one shows birth and death places of persons.
We can see that the last cluster is noisy since we
do not handle antonyms in our models. The argu-
ments of the clusters have noise too. For example,
?New York? occurs as a destination argument in the
2nd cluster. This is because ?New York? has high
frequency in the corpus and it brings noise to the
clustering results. In Table 5 some entity clusters
found by Type-LDA are shown. We find different
types of companies, such as financial companies and
news companies. We also find subclasses of person,
for example, reviewer and politician, because these
different entity classes participate in different rela-
tions. The last cluster shown in the table is a mix-
ture of news companies and government agencies.
This may be because this entity cluster is affected
by many relations.
4.2 Distant Supervision based Relation
Extraction
Our generative models detect clusters of dependency
paths and their arguments. Such clusters are inter-
esting in their own right, but we claim that they can
also be used to help a supervised relation extractor.
We validate this hypothesis in the context of relation
extraction with distant supervision using predicted
clusters as features.
Following previous work (Mintz et al, 2009), we
use Freebase as our distant supervision source, and
align related entity pairs to the New York Times arti-
cles discussed earlier. Our training and test instances
are pairs of entities for which both arguments appear
in at least one sentence together. Features of each
instance are extracted from all sentences in which
both entities appear together. The gold label for each
instance comes from Freebase. If a pair of entities
is not related according to Freebase, we consider it
a negative example. Note that this tends to create
some amount of noise: some pairs may be related,
but their relationships are not yet covered in Free-
base.
After filtering out relations with fewer than 10 in-
stances we have 65 relations and an additional ?O?
label for unrelated pairs of entities. We call related
instances positive examples and unrelated instances
negative examples.
We train supervised classifiers using maximum
entropy. The baseline classifier employs features
that Mintz et al (2009) used. To extract features
from the generative models we proceed as follows.
For each pair of entities, we collect all tuples asso-
ciated with it. For each of these tuples we extract its
clusterID, and use this ID as a binary feature.
The baseline system without generative model
features is called Distant. The classifiers with ad-
ditional features from generative models are named
after the generative models. Thus we have Rel-LDA,
Rel-LDA1 and Type-LDA classifiers. We compare
1461
Source New York, Euro RSCG Worldwide, BBDO Worldwide, American, DDB Worldwide
Path X, a part of Y; X, a unit of Y; X unit of Y; X, a division of Y; X is a part of Y
Dest Omnicom Group, Interpublic Group of Companies, WPP Group, Publicis Groupe
Source Supreme Court, Anna Wintour, William Kristol, Bill Keller, Charles McGrath
Path X, an editor of Y; X, a publisher of Y; X, an editor at Y; X, an editor in chief of Y; X is an editor of Y;
Dest The Times, The New York Times, Vogue, Vanity Fair, New York
Source Kenneth L. Lay, L. Dennis Kozlowski, Bernard J. Ebbers, Thomas R. Suozzi, Bill Gates
Path X, the executive of Y; X, Y?s executive; X, Y executive; X, the chairman of Y; X, Y?s chairman
Dest Enron, Microsoft, WorldCom, Citigroup, Nassau County
Source Paul J. Browne, John McArdle, Tom Cocola, Claire Buchan, Steve Schmidt
Path X, a spokesman for Y; X, a spokeswoman for Y; X, Y spokesman; X, Y spokeswoman; X, a commissioner of Y
Dest White House, Justice Department, Pentagon, United States, State Department
Source United Nations, Microsoft, Intel, Internet, M. D. Anderson
Path X, based in Y; X, which is based in Y; X, a company in Y; X, a company based in Y; X, a consultant in Y
Dest New York, Washington, Manhattan, Chicago, London
Source Army, Shiite, Navy, John, David
Path X was born in Y; X die at home in Y; X die in Y; X, son of Y; X die at Y
Dest Manhattan, World War II, Brooklyn, Los Angeles, New York
Table 4: The path, source and destination arguments of some relations found by Rel-LDA1.
Company Microsoft, Enron, NBC, CBS, Disney
FinanceCom Merrill Lynch, Morgan Stanley, Goldman Sachs, Lehman Brothers, Credit Suisse First Boston
News Notebook, New Yorker, Vogue, Vanity Fair, Newsweek
SportsTeam Yankees, Mets, Giants, Knicks, Jets
University University of California, Harvard, Columbia University, New York University, University of Penn.
Art Reviewer Stephen Holden, Ken Johnson, Roberta Smith, Anthony Tommasini, Grace Glueck
Games World Series, Olympic, World Cup, Super Bowl, Olympics
Politician Eliot Spitzer, Ari Fleischer, Kofi Annan, Scott McClellan, Karl Rove
Gov. Agency Congress, European Union, NATO, Federal Reserve, United States Court of Appeals
News/Agency The New York Times, The Times, Supreme Court, Security Council, Book Review
Table 5: The entity clusters found by Type-LDA
these against Distant and the DIRT database. For
the latter we parse our data using Minipar (Lin,
1998) and extract dependency paths between pairs
of named entity mentions. For each path, the top 3
similar paths are extracted from DIRT database. The
Minipar path and the similar paths are used as addi-
tional features.
For held-out evaluation, we construct the training
data from half of the positive examples and half of
the negative examples. The remaining examples are
used as test data. Note that the number of negative
instances is more than 10 times larger than the num-
ber of positive instances. At test time, we rank the
predictions by the conditional probabilities obtained
from the Maximum Entropy classifier. We report
precision of top ranked 50 instances for each relation
in table 6. From the table we can see that all systems
using additional features outperform the Distant sys-
tem. In average, our best model achieves 4.1%
improvement over the distant supervision baseline,
12% error reduction. The precision of bornIn is low
because in most cases we predict bornIn instances
as liveIn.
We expect systems using generative model fea-
tures to have higher recall than the baseline. This
is difficult to measure, but precision in the high re-
call area is a signal. We look at top ranked 1000
instances of each system and show the precision in
the last row of the table. We can see that our best
model Type-LDA outperforms the distant supervi-
sion baseline by 4.5%.
Why do generative model features help to im-
1462
Relation Dist Rel Rel1 Type DIRT
worksFor 80.0 92.0 86.0 90.0 84.0
authorOf 98.0 98.0 98.0 98.0 98.0
containedBy 92.0 96.0 96.0 92.0 96.0
bornIn 16.0 18.0 22.0 24.0 10.0
dieIn 28.0 30.0 28.0 24.0 24.0
liveIn 50.0 52.0 54.0 54.0 56.0
nationality 92.0 94.0 90.0 90.0 94.0
parentCom 94.0 96.0 96.0 96.0 90.0
founder 65.2 76.3 61.2 64.0 68.3
parent 52.0 54.0 50.0 52.0 52.0
filmDirector 54.0 60.0 60.0 64.0 62.0
Avg 65.6 69.7 67.4 68.0 66.8
Prec@1K 82.8 85.8 85.3 87.3 82.8
Table 6: Precision (%) of some frequent relations
prove relation extraction? One reason is that gen-
erative models can transfer information from known
patterns to unseen patterns. For example, given
?Sidney Mintz, the great food anthropologist at
Johns Hopkins University?, we want to predict the
relation between ?Sidney Mintz? and ?Johns Hopkins
University?. The distant supervision system incor-
rectly predicts the pair as ?O? since it has not seen
the path ?X, the anthropologist at Y? in the training
data. By contrast, Rel-LDA can predict this pair cor-
rectly as worksFor because the dependency path of
this pair is in a cluster which contains the path ?X, a
professor at Y?.
In addition to held-out evaluation we also carry
out manual evaluation. To this end, we use all the
positive examples and randomly select five times
the number of positive examples as negative ex-
amples to train a classifier. The remaining nega-
tive examples are candidate instances. We rank the
predicted instances according to their classification
scores. For each relation, we ask human annotators
to judge its top ranked 50 instances.
Table 7 lists the manual evaluation results for
some frequent relations. We also list how many in-
stances are found for each relation. For almost all
the relations, systems using generative model fea-
tures find more instances. In terms of precision, our
models perform comparatively to the baseline, even
better for some relations.
We also notice that clustering quality is not con-
sistent with distant supervision performance. Rel-
LDA1 can find better clusters than Rel-LDA but it
has lower precision in held-out evaluation. Type-
LDA underperforms Rel-LDA in average precision
but it gets higher precision in a higher recall area, i.e.
precision at 1K. One possible reason for the incon-
sistency is that the baseline distant supervision sys-
tem already employs features that are used in Rel-
LDA1. Another reason may be that the clusters do
not overlap with Freebase relations very well, see
section 4.1.
4.3 Comparing against USP
We also try to compare against USP (Poon and
Domingos, 2008). Due to memory requirements of
USP, we are only able to run it on a smaller data
set consisting of 1,000 NYT documents; this is three
times the amount of data Poon and Domingos (2008)
used to train USP.2 For distant supervision based re-
lation extraction, we only match about 500 Freebase
instances to this small data set.
USP provides a parse tree for each sentence and
for each mention pair we can extract a path from
the tree. Since USP provides clusters of words and
phrases, we use the USP clusterID associated with
the words on the path as binary features in the clas-
sifier.
All models are less accurate when trained on this
smaller dataset; we can do as well as USP does,
even a little better. USP achieves 8.6% in F1, Rel-
LDA 8.7%, Rel-LDA1 10.3%, Type-LDA 8.9% and
Distant 10.3%. Of course, given larger datasets,
the performance of Rel-LDA, Rel-LDA1, and Type-
LDA improves considerably. In summary, compar-
ing against USP, our approach scales much more
easily to large data.
5 Related Work
Many approaches have been explored in relation ex-
traction, including bootstrapping, supervised classi-
fication, distant supervision, and unsupervised ap-
proaches.
Bootstrapping employs a few labeled examples
for each relation, iteratively extracts patterns from
the labeled seeds, and uses the patterns to extract
2Using the publicly released USP code, training a model
with 1,000 documents resulted in about 45 gigabytes of heap
space in the JVM.
1463
Relation Top 50 (%) #InstancesDist Rel Type Dist Rel Type
worksFor 100.0 100.0 100.0 314 349 349
authorOf 94.0 94.0 96.0 185 208 229
containedBy 98.0 98.0 98.0 670 714 804
bornIn 82.6 88.2 88.0 46 36 56
dieIn 100.0 100.0 100.0 167 176 231
liveIn 98.0 98.0 94.0 77 86 109
nationality 78.0 82.0 76.0 84 92 114
parentCom 79.2 77.4 85.7 24 31 28
founder 80.0 80.0 50.0 5 5 14
parent 97.0 92.3 94.7 33 39 38
filmDirector 92.6 96.9 97.1 27 32 34
Table 7: Manual evaluation, Precision and recall of some frequent relations
more seeds (Brin, 1998). This approach may suffer
from low recall since the patterns can be too specific.
Supervised learning can discover more general
patterns (Kambhatla, 2004; Culotta and Sorensen,
2004). However, this approach requires labeled data,
and most work only carry out experiments on small
data set.
Distant supervision for relation extraction re-
quires no labeled data. The approach takes some
existing knowledge base as supervision source,
matches its relational instances against the text cor-
pus to build the training data, and extracts new in-
stances using the trained classifiers (Mintz et al,
2009; Bunescu and Mooney, 2007; Riedel et al,
2010; Yao et al, 2010).
All these approaches can not discover new rela-
tions and classify instances which do not belong to
any of the predefined relations. Other past work has
explored inducing relations using unsupervised ap-
proaches.
For example, DIRT (Lin and Pantel, 2001) aims
to discover different representations of the same se-
mantic relation, i.e. similar dependency paths. They
employ the distributional similarity based approach
while we use generative models. Both DIRT and our
approach take advantage of the arguments of depen-
dency paths to find semantic relations. Moreover,
our approach can cluster the arguments into differ-
ent types.
Unsupervised semantic parsing (USP) (Poon and
Domingos, 2008) discovers relations by merging
predicates which have similar meanings; it proceeds
to recursively cluster dependency tree fragments (or
?parts?) to best explain the observed sentence. It is
not focused on capturing any particular kind of re-
lation between sentence constituents, but to capture
repeated patterns. Our approach differs in that we
are focused on capturing a narrow range of binary
relations between named entities; some of our mod-
els (see Section 3) utilize entity type information to
constraint relation type induction. Also, our models
are built to be scalable and trained on a very large
corpus. In addition, we use a distant supervision
framework for evaluation.
Relation duality (Bollegala et al, 2010) employs
co-clustering to find clusters of entity pairs and pat-
terns. They identify each cluster of entity pairs as a
relation by selecting representative patterns for that
relation. This approach is related to our models,
however, it does not identify any entity clusters.
Generative probabilistic models are widely em-
ployed in relation extraction. For example, they are
used for in-domain relation discovery while incorpo-
rating constraints via posterior regularization (Chen
et al, 2011). We are focusing on open domain re-
lation discovery. Generative models are also ap-
plied to selectional preference discovery (Ritter et
al., 2010; Seaghdha, 2010). In this scenario, the
authors assume relation labels are given while we
automatically discover relations. Generative models
are also used in unsupervised coreference (Haghighi
and Klein, 2010).
1464
Clustering is also employed in relation extraction.
Hasegawa et al (2004) cluster pairs of named en-
tities according to the similarity of context words
intervening between them. Their approach is not
probabilistic. Researchers also use topic models to
perform dimension reduction on features when they
cluster relations (Hachey, 2009). However, they do
not explicitly model entity types.
Open information extraction aims to discover re-
lations independent of specific domains and rela-
tions (Banko et al, 2007; Banko and Etzioni, 2008).
A self-learner is employed to extract relation in-
stances but the systems do not cluster the instances
into relations. Yates and Etzioni (2009) present RE-
SOLVER for discovering relational synonyms as a
post processing step. Our approach integrates entity
and relation discovery in a probabilistic model.
6 Conclusion
We have presented an unsupervised probabilistic
generative approach to relation extraction between
two named entities. Our proposed models exploit
entity type constraints within a relation as well
as features on the dependency path between entity
mentions to cluster equivalent textual expressions.
We demonstrate the effectiveness of this approach
by comparing induced relation clusters against a
large knowledge base. We also show that using clus-
ters of our models as features in distant supervised
framework yields 12% error reduction in precision
over a weakly supervised baseline and outperforms
other state-of-the art relation extraction techniques.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and the Uni-
versity of Massachusetts gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181, ITR#1, and NSF
MALLET. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the US govern-
ment. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors? and do not necessarily reflect those of the
sponsor.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI2007.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2010. Relational duality: Unsupervised ex-
traction of semantic relations between entities on the
web. In Proceedings of WWW.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proc. of WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Eugene Charniak and Micha Elsner. 2009. Em works for
pronoun anaphora resolution. In Proceedings of ACL.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of ACL.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
Benjamin Hachey. 2009. Towards Generic Relation Ex-
traction. Ph.D. thesis, University of Edinburgh.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for extracting relations. In Proceedings of ACL.
1465
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of ACL 10.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of
inference rules from text. In Proceedings of KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Hoifung Poon and Pedro Domingos. 2008. Unsuper-
vised semantic parsing. In Proceedings of the Confer-
ence on Empirical methods in natural language pro-
cessing (EMNLP).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of ACL10.
Dan Roth and Wen tau Yih. 2002. Probabilistic reason-
ing for entity and relation recognition. In Proceedings
of Coling.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings of ACL 10.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL, pages 252?259.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1013?1023, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
1466
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 729?732,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Constraint-Driven Rank-Based Learning for Information Extraction
Sameer Singh Limin Yao Sebastian Riedel Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst MA 01003
{sameer,lmyao,riedel,mccallum}@cs.umass.edu
Abstract
Most learning algorithms for undirected
graphical models require complete inference
over at least one instance before parameter up-
dates can be made. SampleRank is a rank-
based learning framework that alleviates this
problem by updating the parameters during in-
ference. Most semi-supervised learning algo-
rithms also perform full inference on at least
one instance before each parameter update.
We extend SampleRank to semi-supervised
learning in order to circumvent this compu-
tational bottleneck. Different approaches to
incorporate unlabeled data and prior knowl-
edge into this framework are explored. When
evaluated on a standard information extraction
dataset, our method significantly outperforms
the supervised method, and matches results of
a competing state-of-the-art semi-supervised
learning approach.
1 Introduction
Most supervised learning algorithms for undirected
graphical models require full inference over the
dataset (e.g., gradient descent), small subsets of the
dataset (e.g., stochastic gradient descent), or at least
a single instance (e.g., perceptron, Collins (2002))
before parameter updates are made. Often this is the
main computational bottleneck during training.
SampleRank (Wick et al, 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within inference. Ev-
ery pair of samples generated during inference is
ranked according to the model and the ground truth,
and the parameters are updated when the rankings
disagree. SampleRank has enabled efficient learn-
ing for massive information extraction tasks (Culotta
et al, 2007; Singh et al, 2009).
The problem of requiring a complete inference it-
eration before parameters are updated also exists in
the semi-supervised learning scenario. Here the sit-
uation is often considerably worse since inference
has to be applied to potentially very large unlabeled
datasets. Most semi-supervised learning algorithms
rely on marginals (GE, Mann and McCallum, 2008)
or MAP assignments (CODL, Chang et al, 2007).
Calculating these is computationally inexpensive for
many simple tasks (such as classification and re-
gression). However, marginal and MAP inference
tends to be expensive for complex structured pre-
diction models (such as the joint information extrac-
tion models of Singh et al (2009)), making semi-
supervised learning intractable.
In this work we employ a fast rank-based learning
algorithm for semi-supervised learning to circum-
vent the inference bottleneck. The ranking function
is extended to capture both the preference expressed
by the labeled data, and the preference of the domain
expert when the labels are not available. This allows
us to perform SampleRank as is, without sacrificing
its scalability, which is crucial for future large scale
applications of semi-supervised learning.
We applied our method to a standard information
extraction dataset used for semi-supervised learning.
Empirically we demonstrate improvements over the
supervised model, and closely match the results of a
competing state-of-the-art semi-supervised learner.
2 Background
Conditional random fields (Lafferty et al, 2001) are
undirected graphical models represented as factor
729
graphs. A factor graph G = {?i} defines a prob-
ability distribution over assignments y to a set of
output variables, conditioned on an observation x.
A factor ?i computes the inner product between
the vector of sufficient statistics f(xi,yi) and pa-
rameters ?. Let Z(x) be the data-dependent par-
tition function used for normalization. The proba-
bility distribution defined by the graph is:
p(y|x,?) =
1
Z(x)
?
?i?G
e??f(xi,yi)
2.1 Rank-Based Learning
SampleRank (Wick et al, 2009) is a rank-based
learning framework for that performs parameter up-
dates within MCMC inference. Every pair of con-
secutive samples in the MCMC chain is ranked ac-
cording to the model and the ground truth, and the
parameters are updated when the rankings disagree.
This allows the learner to acquire more supervision
per sample, and has led to efficient training of mod-
els for which inference is very expensive (Singh
et al, 2009).
SampleRank considers two ranking functions: (1)
the unnormalized conditional probability (model
ranking), and (2) a truth function F(y) (objective
ranking) which is defined as ?L(y,yL), the neg-
ative loss between the possible assignment y and
the true assignment yL. The truth function can take
different forms, such as tokenwise accuracy or F1-
measure with respect to some labeled data.
In order to learn the parameters for which model
rankings are consistent with objective rankings,
SampleRank performs the following update for each
consecutive pair of samples ya and yb of the MCMC
chain. Let ? be the learning rate, and ? =
f(xi,yai )? f(xi,y
b
i ), then ? is updated as follows:
?
+
?
?
??
??
?? if p(y
a|x)
p(yb|x) < 1 ? F(y
a) > F(yb)
??? if p(y
a|x)
p(yb|x) > 1 ? F(y
a) < F(yb)
0 otherwise.
This update is usually fast: in order to calculate
the required model ratio, only factors that touch
changed variables have to be taken into account.
SampleRank has been incorporated into the FAC-
TORIE toolkit for probabilistic programming with
imperatively-defined factor graphs (McCallum et al,
2009).
3 Semi-Supervised Rank-Based Learning
To apply SampleRank to the semi-supervised set-
ting, we need to specify the truth function F over
both labeled and unlabeled data. For labeled data
YL, we can use the true labels. These are not avail-
able for unlabeled data YU , and we present alterna-
tive ways of defining a truth function FU : YU ? <
for this case.
3.1 Self-Training
Self-training, which uses predictions as truth, fits di-
rectly into our SampleRank framework. After per-
forming SampleRank on training data (using FL),
MAP inference is performed on the unlabeled data.
The prediction y?U is used as the ground truth for
the unlabeled data. Thus the self-training objective
function Fs over the unlabeled data can be defined
as Fs(y) = ?L(y, y?U ).
3.2 Encoding Constraints
Constraint-driven semi-supervised learning uses
constraints to incorporate external domain knowl-
edge when labels are missing (Chang et al, 2007;
Mann and McCallum, 2008; Bellare et al, 2009).
Constraints prefer certain label configurations over
others. For example, one constraint may be that oc-
currences of the word ?California? are preferred to
have the label ?location?.
We can encode constraints directly into the objec-
tive function FU . Let a constraint i be specified as
?pi, ci?, where ci(y) denotes whether assignment y
satisfies the constraint i (+1), violates it (?1), or the
constraint does not apply (0), and pi is the constraint
strength. Then the objective function is:
Fc(y) =
?
i
pici(y)
3.3 Incorporating Model Predictions
When the objective function Fc is used, every pre-
diction on unlabeled data is ranked only according to
the constraints, and thus the model is trained to sat-
isfy all the constraints. This is a problem when the
constraints prefer a wrong solution while the model
favors the correct solution, resulting in SampleR-
ank updating the model away from the true solution.
To avoid this, the ranking function needs to balance
preferences of the constraints and the current model.
730
One option is to incorporate the self-training ob-
jective function Fs. A new objective function that
combines self-training with constraints can be de-
fined as:
Fsc(y) = Fs(y) + ?sFc(y)
= ?L(y, y?U ) + ?s
?
i
pici(y)
This objective function has at least two limita-
tions. First, self-training involves a complete infer-
ence step to obtain y?U . Second, the model might
have low confidence in its prediction (this is the case
when the underlying marginals are almost uniform),
but the self-training objective des not take this into
account. Hence, we also propose an objective func-
tion that incorporates the model score directly, i.e.
Fmc(y) = log p(y|x,?) + logZ(x) + ?mFc(y)
=
?
?i
? ? f(xi,yi) + ?m
?
i
pici(y)
This objective does not require inference, and also
takes into account model confidence.
In both objective functions Fsc and Fmc, ? con-
trols the relative contribution of the constraint pref-
erences to the objective function. With higher ?,
SampleRank will make updates that never try to vi-
olate constraints, while with low ?, SampleRank
trusts the model more. ? corresponds to constraint
satisfaction weights ? used in (Chang et al, 2007).
4 Related Work
Chang et al propose constraint-driven learn-
ing (CODL, Chang et al, 2007) which can be in-
terpreted as a variation of self-training: Instances
are selected for supervision based not only on the
model?s prediction, but also on their consistency
with a set of user-defined constraints. By directly in-
corporating the model score and the constraints (as
inFmc in Section 3.3) we follow the same approach,
but avoid the expensive ?Top-K? inference step.
Generalized expectation criterion (GE, Mann and
McCallum, 2008) and Alternating Projections (AP,
Bellare et al, 2009) encode preferences by speci-
fying constraints on feature expectations, which re-
quire expensive inference. Although AP can use on-
line training, it still involves full inference over each
instance. Furthermore, these methods only support
constraints that factorize according to the model.
Li (2009) incorporates prior knowledge into con-
ditional random fields as variables. They require full
inference during learning, restricting the application
to simple models. Furthermore, higher-order con-
straints are specified using large cliques in the graph,
which slow down inference. Our approach directly
incorporates these constraints into the ranking func-
tion, with no impact on inference time.
5 Experiments
We carried out experiments on the Cora citation
dataset. The task is to segment each citation into
different fields, such as ?author? and ?title?. We use
300 instances as training data, 100 instances as de-
velopment data, and 100 instances as test data. Some
instances from the training data are selected as la-
beled instances, and the remaining data (including
development) as unlabeled. We use the same token-
label constraints as Chang et al (2007).
We use the objective functions defined in Sec-
tion 3, specifically self-training (Self:Fs), direct
constraints (Cons:Fc), the combination of the two
(Self+Cons:Fsc), and combination of the model
score and the constraints (Model+Cons:Fmc). We
set pi = 1.0, ? = 1.0, ?s = 10, and ?m = 0.0001.
Average token accuracy for 5 runs is reported and
compared with CODL1 in Table 1. We also report
supervised results from (Chang et al, 2007) and
SampleRank. All of our methods show vast im-
provement over the supervised method for smaller
training sizes, but this difference decreases as the
training size increases. When the complete training
data is used, additional unlabeled data hurts our per-
formance. This is not observed in CODL since they
use more unlabeled data, which may also explain
their slightly higher accuracy. Note that Self+Cons
performs better than Self or Cons individually.
Model+Cons also performs competitively, and
may potentially outperform other methods if a bet-
ter ?m is chosen. Note, however, that ?m is much
harder to tune than ?s since ?m weighs the contri-
bution of the unnormalized model score, the range
1We report inference without constraints results from
CODL. Their results that incorporated constraints were higher,
but we do not implement this alternative due to the difficulty in
balancing the model score and constraint weights.
731
Method 5 10 15 20 25 300
Sup. (CODL) 55.1 64.6 68.7 70.1 72.7 86.1
SampleRank 66.5 74.6 75.6 77.6 79.5 90.7
CODL 71 76.7 79.4 79.4 82 88.2
Self 67.6 75.1 75.8 78.6 80.4 88
Cons 67.2 75.3 77.5 78.6 79.4 88.3
Self+Cons 71.3 77 77.5 79.5 81.1 87.4
Model+Cons 69.8 75.4 75.7 79.3 79.3 90.6
Table 1: Tokenwise Accuracy: for different methods as we vary the size of the labeled data
of which depends on many different factors such as
properties of the data, the learning rate, number of
samples, proposal function, etc. For self+cons (?s),
the ranges of the predictions and constraint penalties
are fixed and known, making the task simpler.
Self training takes 90 minutes to run on average,
while Self+Cons and Model+Cons need 100 min-
utes. Since the Cons method skips the inference
step over unlabeled data, it takes only 30 minutes
to run. As the size of the model and unlabeled data
set grows, this saving will become more significant.
Running time of CODL was not reported.
6 Conclusion
This work extends the rank-based learning frame-
work to semi-supervised learning. By integrating
the two paradigms, we retain the computational effi-
ciency provided by parameter updates within infer-
ence, while utilizing unlabeled data and prior knowl-
edge. We demonstrate accuracy improvements on a
real-word information extraction dataset.
We believe that the method will be of greater ben-
efit to learning in complex factor graphs such as
joint models over multiple extraction tasks. In future
work we will investigate our approach in such set-
tings. Additionally, various sensitivity, convergence,
and robustness properties of the method need to be
analyzed.
Acknowledgments
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by SRI Inter-
national subcontract #27-001338 and ARFL prime
contract #FA8750-09-C-0181, and in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation under
NSF grant #IIS-0326249. Any opinions, findings
and conclusions or recommendations expressed in
this material are the authors? and do not necessarily
reflect those of the sponsor.
References
Kedar Bellare, Gregory Druck, and Andrew McCallum.
Alternating projections for learning with expectation
constraints. In UAI, 2009.
Mingwei Chang, Lev Ratinov, and Dan Roth. Guiding
semi-supervision with constraint-driven learning. In
ACL, 2007.
Michael Collins. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithm. In ACL, 2002.
Aron Culotta, Michael Wick, and Andrew McCallum.
First-order probabilistic models for coreference reso-
lution. In NAACL/HLT, 2007.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: probabilistic models for
segmenting and labeling sequence data. In ICML,
2001.
Xiao Li. On the use of virtual evidence in conditional
random fields. In EMNLP, 2009.
Gideon S. Mann and Andrew McCallum. Generalized ex-
pectation criteria for semi-supervised learning of con-
ditional random fields. In ACL, 2008.
Andrew McCallum, Karl Schultz, and Sameer Singh.
FACTORIE: probabilistic programming via impera-
tively defined factor graphs. In NIPS, 2009.
Sameer Singh, Karl Schultz, and Andrew McCallum.
Bi-directional joint inference for entity resolution
and segmentation using imperatively-defined factor
graphs. In ECML/PKDD, 2009.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. SampleRank: Learning pref-
erences from atomic gradients. In NIPS Workshop on
Advances in Ranking, 2009.
732
Proceedings of NAACL-HLT 2013, pages 74?84,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Relation Extraction with Matrix Factorization and Universal Schemas
Sebastian Riedel
Department of Computer Science
University College London
s.riedel@ucl.ac.uk
Limin Yao, Andrew McCallum, Benjamin M. Marlin
Department of Computer Science
University of Massachusetts at Amherst
{lmyao,mccallum,marlin}@cs.umass.edu
Abstract
Traditional relation extraction predicts rela-
tions within some fixed and finite target
schema. Machine learning approaches to this
task require either manual annotation or, in
the case of distant supervision, existing struc-
tured sources of the same schema. The need
for existing datasets can be avoided by us-
ing a universal schema: the union of all in-
volved schemas (surface form predicates as in
OpenIE, and relations in the schemas of pre-
existing databases). This schema has an al-
most unlimited set of relations (due to surface
forms), and supports integration with existing
structured data (through the relation types of
existing databases). To populate a database of
such schema we present matrix factorization
models that learn latent feature vectors for en-
tity tuples and relations. We show that such
latent models achieve substantially higher ac-
curacy than a traditional classification ap-
proach. More importantly, by operating simul-
taneously on relations observed in text and in
pre-existing structured DBs such as Freebase,
we are able to reason about unstructured and
structured data in mutually-supporting ways.
By doing so our approach outperforms state-
of-the-art distant supervision.
1 Introduction
Most previous work in relation extraction uses a pre-
defined, finite and fixed schema of relation types
(such as born-in or employed-by). Usually some tex-
tual data is labeled according to this schema, and
this labeling is then used in supervised training of
an automated relation extractor, e.g. Culotta and
Sorensen (2004). However, labeling textual rela-
tions is time-consuming and difficult, leading to sig-
nificant recent interest in distantly-supervised learn-
ing. Here one aligns existing database records with
the sentences in which these records have been ?ren-
dered???effectively labeling the text?and from this
labeling we can train a machine learning system as
before (Craven and Kumlien, 1999; Mintz et al,
2009; Bunescu and Mooney, 2007; Riedel et al,
2010). However, this method relies on the availabil-
ity of a large database that has the desired schema.
The need for pre-existing datasets can be avoided
by using language itself as the source of the schema.
This is the approach taken by OpenIE (Etzioni et al,
2008). Here surface patterns between mentions of
concepts serve as relations. This approach requires
no supervision and has tremendous flexibility, but
lacks the ability to generalize. For example, Ope-
nIE may find FERGUSON?historian-at?HARVARD
but does not know FERGUSON?is-a-professor-at?
HARVARD. OpenIE has traditionally relied on a
large diversity of textual expressions to provide good
coverage. But this diversity is not always available,
and, in any case, the lack of generalization greatly
inhibits the ability to support reasoning.
One way to gain generalization is to cluster tex-
tual surface forms that have similar meaning (Lin
and Pantel, 2001; Pantel et al, 2007; Yates and
Etzioni, 2009; Yao et al, 2011). While the clus-
ters discovered by all these methods usually contain
semantically related items, closer inspection invari-
ably shows that they do not provide reliable impli-
cature. For example, a typical representative clus-
ter may include historian-at, professor-at, scientist-
at, worked-at. Although these relation types are in-
deed semantically related, note that scientist-at does
not necessarily imply professor-at, and worked-at
74
certainly does not imply scientist-at. In fact, we
contend that any relational schema would inherently
be brittle and ill-defined??having ambiguities, prob-
lematic boundary cases, and incompleteness.1 For
example, Freebase, in spite of its extensive effort to-
wards high coverage, has no critized nor scientist-at
relation.
In response to this problem, we present a new ap-
proach: implicature with universal schemas. Here
we embrace the diversity and ambiguity of original
inputs; we avoid forcing textual meaning into pre-
defined boxes. This is accomplished by defining
our schema to be the union of all source schemas:
original input forms, e.g. variants of surface pat-
terns similarly to OpenIE, as well as relations in
the schemas of many available pre-existing struc-
tured databases. But then, unlike OpenIE, our fo-
cus lies on learning asymmetric implicature among
relations. This allows us to probabilistically ?fill
in? inferred unobserved entity-entity relations in
this union. For example, after observing FERGU-
SON?historian-at?HARVARD our system infers that
FERGUSON?professor-at?HARVARD, but not vice
versa.
At the heart of our approach is the hypothesis that
we should concentrate on predicting source data??a
relatively well defined task that can be evaluated and
optimized??as opposed to modeling semantic equiv-
alence, which we believe will always be illusive.
Note that by operating simultaneously on rela-
tions observed in text and in pre-existing structured
databases such as Freebase, we are able to reason
about unstructured and structured data in mutually-
supporting ways. For example, we can predict sur-
face pattern relations that effectively serve as addi-
tional features when predicting Freebase relations,
hence improving generalization. Also notice that
users of our system will not have to study and un-
derstand the complexities of a particular schema in
order to issue queries; they can ask in whatever form
naturally occurs to them, and our system will likely
already have that relation in our universal schema.
Our technical approach is based on extensions
to probabilistic models of matrix factorization and
1At NAACL 2012 Lucy Vanderwende asked ?Where do the
relation types come from?? There was no satisfying answer. At
the same meeting, and in line with Brachman (1983), Ed Hovy
stated ?We don?t even know what is-a means.?
collaborative filtering (Collins et al, 2001; Koren,
2008; Rendle et al, 2009). We represent the prob-
abilistic knowledge base as a matrix with entity-
entity pairs in the rows and relations in the columns
(see figure 1). The rows come from running cross-
document entity resolution across pre-existing struc-
tured databases and textual corpora. The columns
come from the union of surface forms and DB rela-
tions. We present a series of models that learn lower
dimensional manifolds for tuples, relations and enti-
ties, and a set of weights that capture direct correla-
tions between relations. Weights and lower dimen-
sional representations act, through dot products, as
the natural parameters of a single log-linear model
to derive per-cell probabilities.
In experiments we show that our models can ac-
curately predict surface patterns relationships which
do not appear explicitly in text, and that learning la-
tent representations of entities, tuples and relations
substantially improves results over a traditional clas-
sifier approach. Moreover, we can improve accu-
racy by simultaneously operating on relations ob-
served in the New York Times corpus and in Free-
base. In particular, our model outperforms the cur-
rent state-of-the-art distant supervision method (Sur-
deanu et al, 2012) by 10% points Mean Average
Precision through joint implicature among surface
patterns and Freebase relations.
2 Model
Before we present our approach in more detail, we
briefly introduce some notation. We use R to de-
note the set of relations we seek to predict (such as
works-written in Freebase, or the X?historian-at?Y
pattern), and T to denote the set of input tuples. For
simplicity we assume each relation to be binary, al-
though our approach can be easily generalized to the
n-ary case. Given a relation r ? R and a tuple t ? T
the pair ?r, t? is a fact, or relation instance. The in-
put to our model is a set of observed facts O, and
the observed facts for a given tuple is denoted by
Ot := {?r, t? ? O}.
Our goal is a model that can estimate, for a
given relation r (such as X?historian-at?Y) and a
given tuple t (such as <FERGUSON,HARVARD>),
the probability p (yr,t = 1) where yr,t is a binary
random variable that is true iff t is in relation r. We
75
Tr
a
i
n
0.95
T
e
s
t
Surface Patterns KB Relations
X-professor-at-Y
1
1
0.05
X-historian-at-Y employee(X,Y) member(X,Y)
1 1
1
1
0.97
Rel. Extraction
1 0.93
0.97
Cluster Align
Reasoning with Universal Schema
F
e
r
g
u
s
o
n
,
H
a
r
v
a
r
d
O
m
a
n
,
O
x
f
o
r
d
F
i
r
t
h
,
O
x
f
o
r
d
G
?
d
e
l
,
P
r
i
n
c
e
t
o
n
0.95
Figure 1: Filling up a database of universal schema.
Dark circles are observed facts, shaded circles are in-
ferred facts. Relation Extraction (RE) maps surface pat-
tern relations (and other features) to structured relations.
Surface form clustering models correlations between pat-
terns, and can be fed into RE (Yao et al, 2011). Database
alignment and integration models correlations between
structured relations (not done in this work). Reasoning
with the universal schema incorporates these tasks in a
joint fashion.
introduce a series of exponential family models that
estimate this probability using a natural parameter
?r,t and the logistic function:
p (yr,t = 1|?r,t) := ? (?r,t) =
1
1 + exp (??r,t)
.
We will first describe our models through differ-
ent definitions of the natural parameter ?r,t. In each
case ?r,t will be a function of r, t and a set of weights
and/or latent feature vectors. In section 2.5 we will
then show how these weights and vectors can be es-
timated based on the observed facts O.
Notice that we can interpret p (yr,t = 1) as the
probability that a customer t likes product r. This
analogy allows us to draw from a large body of work
in collaborative filtering, such as work in probabilis-
tic matrix factorization and implicit feedback.
2.1 Latent Feature Model
One way to define ?r,t is through a latent feature
model F. Here we measure compatibility between
relation r and tuple t as dot product of two latent
feature representations of size KF: ar for relation r,
and vt for tuple t. This gives:
?Fr,t :=
KF?
k
ar,kvt,k.
This corresponds to generalized PCA (Collins et al,
2001), a model were the matrix ? = (?r,t) of natural
parameters is defined as the low rank factorization
AV.
Notice that we intentionally omit any per-relation
bias-terms. In section 4 we evaluate ranked answers
to queries on a per-relation basis, and a per-relation
bias term will have no effect on ranking facts of the
same relation. Also consider that such latent feature
models can capture asymmetry by assigning more
peaked vectors to specific relations, and more uni-
form vectors to general relations.
2.2 Neighborhood Model
We can interpolate the confidence for a given tuple
and relation based on the trueness of other similar
relations for the same tuple. In collaborative filter-
ing this is referred to as a neighborhood-based ap-
proach (Koren, 2008). In terms of our natural pa-
rameter, we implement a neighborhood model N via
a set of weights wr,r? , where each corresponds to a
directed association strength between relations r and
r?. For a given tuple t and relation r we then sum
up the weights corresponding to all relations r? that
have been observed for tuple t:
?Nr,t :=
?
(r?,t)?O\{(r,t)}
wr,r? .
Notice that the neighborhood model amounts to
a collection of local log-linear classifiers, one for
each relation r with feature functions fr,r? (t) =
I [r? 6= r ? (r?, t) ? O] and weights wr. This means
that in contrast to model F, this model cannot har-
ness any synergies between textual and pre-existing
DB relations.
76
2.3 Entity Model
Relations have selectional preferences: they allow
only certain types in their argument slots. While
knowledge bases such as Freebase or DBPedia have
extensive ontologies of types of entities, these are of-
ten not sufficiently fine to allow relations to discrim-
inate (Yao et al, 2012b). Hence, instead of using a
predetermined set of entity types, in our entity model
E we learn a latent entity representation from data.
More concretely, for each entity e we introduce a la-
tent feature vector te of dimension KE. In addition,
for each relation r and argument slot i we introduce
a feature vector di of the same dimension. For ex-
ample, binary relations have feature representations
d1 for argument 1, and d2 for argument 2. Mea-
suring compatibility of an entity tuple and relation
amounts to measuring, and summing up, compati-
bility between each argument slot representation and
the corresponding entity representation. This leads
to:
?Er,t :=
arity(r)?
i=1
KE?
k
di,ktti,k.
Note that due to entity resolution, tuples may
share entities, and hence parameters are shared
across rows.
2.4 Combined Model
In practice all the above models can capture impor-
tant aspects of the data. Hence we also use various
combinations, such as:
?NFEr,t := ?
N
r,t + ?
F
r,t + ?
E
r,t.
2.5 Parameter Estimation
Our models are parametrized through weights and
latent component vectors. We could estimate these
parameters by maximizing the loglikelihood of the
observed data akin to Collins et al (2001). How-
ever, as we do not have access to negative facts, the
model would simply learn to predict all facts to be
true. In our initial attempt to overcome this issue
we sampled a set of unobserved facts as designated
negative facts, as is done in related distant supervi-
sion approaches. However, we found that (a) our
results were sensitive to the choice of negative data
and (b) runtime was increased substantially because
of a large number of required negative facts.
In collaborative filtering positive-only data is also
known as implicit feedback. This type of feedback
arises, for example, when users buy but not rate
items. One successful approach to learning with im-
plicit feedback is based on the observation that the
actual task is not necessarily one of prediction (here:
to predict a number between 0 and 1) but one of
(generally simpler) ranking: to give true ?user-item?
cells higher scores than false ones. Bayesian Person-
alized Ranking (BPR) uses a variant of this ranking:
giving observed true facts higher scores than unob-
served (true or false) facts (Rendle et al, 2009). This
relaxed constraint is to be contrasted with the log-
likelihood setting that essentially requires (randomly
sampled) negative facts to score below a globally de-
fined threshold.
2.5.1 Objective
We first create a dataset of ranked pairs: for each
relation r and each observed fact f+ := ?r, t+? ? O
we choose all tuples t? such that f? := ?r, t?? /?
O?that is, tuples we have not observed to be in
relation r. For each pair of facts f+ and f? we
want p (f+) > p (f?) and hence ?f+ > ?f? . In
BPR this is achieved by maximizing a sum terms of
the form Objf+,f? := log
(
?
(
?f+ ? ?f?
))
, one for
each ranked pair:
Obj :=
?
?r,t+??O
?
?r,t??/?O
Obj?r,t+?,?r,t??. (1)
Notice that this objective differs slightly from the
one used by Rendle et al (2009). Consider tuples
as users and items as relations. We rank different
users with respect to the same item, while BPR ranks
items with respect to the same user. Also notice that
the BPR objective is an approximation to the per-
relation AUC (area under the ROC curve), and hence
directly correlated to what we want to achieve: well-
ranked tuples per relation.
Note that all parameters are regularized with
quadratic penalty which we omit here for brevity.
2.5.2 Optimization
To maximize the objective2 in equation 1 we fol-
low Rendle et al (2009) and employ Stochastic Gra-
dient Descent (SGD). In particular, in each epoch
2This objective is non-convex for all models excluding the
N model.
77
we sample |O| facts with replacement from O. For
each sampled fact ?r, t+? we then sample a tuple
t? ? T such that ?r, t?? /? O is not an observed
fact. This gives us |O| fact pairs ?f+, f??, and for
each pair we do an SGD update using the corre-
sponding gradients of Objf+,f? . For the F model
the gradients correspond to those presented by Ren-
dle et al (2009). The remaining gradients are easy
to derive; we omit details for brevity.
3 Related Work
This work extends a previous workshop paper (Yao
et al, 2012a) by introducing the neighborhood and
entity model, by working with the BPR objective,
and by more extensive experiments.
Relational Clustering There is a large body of
work aiming to discover latent relations by clus-
tering surface patterns (Hasegawa et al, 2004;
Shinyama and Sekine, 2006; Kok and Domingos,
2008; Yao et al, 2011; Takamatsu et al, 2011), or
by inducing synonymy relationships between pat-
terns independently of the entities (Yates and Et-
zioni, 2009; Pantel et al, 2007; Lin and Pantel,
2001). Our approach has a fundamentally different
objective: we are not (primarily) interested in clus-
ters of patterns or their semantic representation, but
in predicting patterns where they are not observed.
Moreover, these related methods rely on a symmetric
notion of synonymy in which clustered patterns are
assumed to have the same meaning. Our approach
rejects this assumption in favor of a model which
learns that certain patterns, or combinations thereof,
entail others in one direction, but not necessarily the
other. This is similar in spirit to work on learning
entailment rules (Szpektor et al, 2004; Zanzotto et
al., 2006; Szpektor and Dagan, 2008). However, for
us even entailment rules are just a by-product of our
goal to improve prediction, and it is this goal we di-
rectly optimize for and evaluate.
Matrix Factorization Our approach is also re-
lated to work on factorizing YAGO to predict new
links (Nickel et al, 2012). The primary differences
are that we include surface patterns in our schema,
use a ranking objective, and learn latent vectors for
entities and tuples. Likewise, matrix factorization in
various flavors has received significant attention in
the lexical semantics community, from LSA to re-
cent work on non-negative sparse embeddings (Mur-
phy et al, 2012). In our problem columns corre-
spond to relations, and rows correspond to entity tu-
ples. By contrast, there columns are words, and rows
are contextual features such as ?words in a local win-
dow.? Consequently, our objective is to complete
the matrix, whereas their objective is to learn better
latent embeddings of words (which by themselves
again cannot capture any sense of asymmetry).
OpenIE Open IE (Etzioni et al, 2008) extracts
facts mentioned in text, but does not predict poten-
tial facts not mentioned in text. Finding answers
requires explicit mentions, and hence suffers from
lower recall for not-so-frequently mentioned facts.
Methods that learn rules between textual patterns in
OpenIE aim at a similar goal as our proposed ap-
proach (Schoenmackers et al, 2008; Schoenmack-
ers et al, 2010). However, their approach is sub-
stantially more complex, requires a categorization
of entities into fine grained entity types, and needs
inference in high tree-width Markov Networks. By
contrast, our approach is based on a single unified
model, requires no entity types, and for us inferring
a fact amounts to not more than a few dot products.
In addition, in our Universal Schema approach Ope-
nIE surface patterns are just one kind of relations,
and our aim is populate relations of all kinds. In the
future we may even include relations between enti-
ties and continuous attributes (say, gene expression
measurements).
Distant Supervision In Distant Supervision (DS)
a set of facts from pre-existing structured sources
is aligned with surface patterns mentioned in
text (Bunescu and Mooney, 2007; Mintz et al, 2009;
Riedel et al, 2010; Hoffmann et al, 2011; Surdeanu
et al, 2012), and this alignment is then used to train
a relation extractor. A core difference to our ap-
proach is the number of target relations: In DS it
is the relatively small schema size of the knowledge
base, while we also include surface patterns. This
allows us to answer more expressive queries. More-
over, by learning from surface-pattern correlations,
our latent models induce feature representations for
patterns that do not appear in the DS training set. As
we will see in section 4, this allows us to outperform
state-of-the-art DS models.
78
Never-Ending Learning and Bootstrapping Our
latent feature models are capable of never-ending
learning (Carlson et al, 2010). That is, we can con-
tinue to train these models with incoming data, even
if no structured annotation is available. In bootstrap-
ping approaches the current model is used to predict
new relations, and these hypothesized relations are
used as new supervision targets (i.e. self-training).
By contrast, our model only strengthens the correla-
tions between incoming co-occurring observations.
This has the advantage that wrong predictions are
less likely be reinforced, hence reducing the risk of
semantic drift.
4 Experiments
How accurately can we fill a database of Universal
Schema, and does reasoning jointly across a uni-
versal schema help to improve over more isolated
approaches? In the following we seek to answer
this question empirically. To this end we train our
models on observed facts in a newswire corpus and
Freebase, and then manually evaluate ranked predic-
tions: first for structured relations and then for sur-
face form relations.
4.1 Data
Following previous work (Riedel et al, 2010),
our documents are taken from the NYTimes cor-
pus (Sandhaus, 2008). Articles after 2000 are used
as training corpus, articles from 1990 to 1999 as
test corpus. We also split Freebase facts 50/50 into
train and test facts, and their corresponding tuples
into train and test tuples. Then we align training tu-
ples with the training corpus, and test tuples with the
test corpus. This alignment relies on a preprocessing
step that links NER mentions in text with entities in
Freebase. In our case we use a simple string-match
heuristic to find this linking. Now we align an entity
tuple ?t1, t2? with a pair of mentions ?m1,m2? in
the same sentence if m1 is linked to t1 and m2 to t2.
Based on this alignment we filter out all relations for
which we find fewer than 10 tuples with mentions in
text.
The above alignment and filtering process reduces
the total number of tuples related according to Free-
base to 16k: approximately 8k tuples with facts
mentioned in the training set, and approximately 8k
such tuples for the test set. In addition we have a
set of approximately 200k training tuples for which
both arguments appear in the same sentence and
both can be linked to Freebase entities, but for which
no Freebase fact is recorded. This can either be be-
cause they are not related, or simply because Free-
base does not contain the relationship yet. We also
have about 200k such tuples in the test set. To sim-
plify evaluation, we create a subsampled test set by
randomly choosing 10k of the original test set tuples.
The above alignment allows us to determine, for
each tuple t, the observed facts Ot as follows. To
find the surface pattern facts OPATt for the tuple t =
?t1, t2? we extract, for each mention m = ?m1,m2?
of t, the lexicalized dependency path p between m1
and m2. Then we add ?p, t? to OPATt . For example,
we get ?<-subj<-head->obj->? for ?M1 heads M2.?
Filtering out patterns with fewer than 10 mentions
in text yields approximately 4k patterns. For train-
ing tuples we add as Freebase facts OFBt all facts
?r, t? that appear in Freebase, and for which r has
not been filtered out beforehand. For the test setOFBt
remains empty. The total set of observed facts Ot is
OFBt ?O
PAT
t , and their union over all tuples forms the
set of observed facts O.
4.2 Evaluation
For evaluation we use collections of relations: sur-
face patterns in one experiment and Freebase re-
lations in the other. In either case we compare
the competing systems with respect to their ranked
results for each relation in the collection. Given
this ranking task, our evaluation is inspired by the
TREC competitions and work in information re-
trieval (Manning et al, 2008). That is, we treat
each relation as query and receive the top 1000 (run
depth) entity pairs from each system. Then we pool
the top 100 (pool depth) answers from each system
and manually judge their relevance or ?truth.? This
gives a set of relevant results that we can use to cal-
culate recall and precision measures. In particular,
we can use these annotations to measure an average
precision across the precision-recall curve, and an
aggregate mean average precision (MAP) across all
relations. This metric has shown to be very robust
and stable (Manning et al, 2008). In addition we
also present a weighted version of MAP (weighted
MAP) in which the average precision for each re-
79
Relation # MI09 YA11 SU12 N F NF NFE
person/company 103 0.67 0.64 0.70 0.73 0.75 0.76 0.79
location/containedby 74 0.48 0.51 0.54 0.43 0.68 0.67 0.69
author/works_written 29 0.50 0.51 0.52 0.45 0.61 0.63 0.69
person/nationality 28 0.14 0.40 0.13 0.13 0.19 0.18 0.21
parent/child 19 0.14 0.25 0.62 0.46 0.76 0.78 0.76
person/place_of_death 19 0.79 0.79 0.86 0.89 0.83 0.85 0.86
person/place_of_birth 18 0.78 0.75 0.82 0.50 0.83 0.81 0.89
neighborhood/neighborhood_of 12 0.00 0.00 0.08 0.43 0.65 0.66 0.72
person/parents 7 0.24 0.27 0.58 0.56 0.53 0.58 0.39
company/founders 4 0.25 0.25 0.53 0.24 0.77 0.80 0.68
film/directed_by 4 0.06 0.15 0.25 0.09 0.26 0.26 0.30
sports_team/league 4 0.00 0.43 0.18 0.21 0.59 0.70 0.63
team/arena_stadium 3 0.00 0.06 0.06 0.03 0.08 0.09 0.08
team_owner/teams_owned 2 0.00 0.50 0.70 0.55 0.38 0.61 0.75
roadcast/area_served 2 1.00 0.50 1.00 0.58 0.58 0.83 1.00
structure/architect 2 0.00 0.00 1.00 0.27 1.00 1.00 1.00
composer/compositions 2 0.00 0.00 0.00 0.50 0.67 0.83 0.12
person/religion 1 0.00 1.00 1.00 0.50 1.00 1.00 1.00
film/produced_by 1 1.00 1.00 1.00 1.00 0.50 0.50 0.33
MAP 0.32 0.42 0.56 0.45 0.61 0.66 0.63
Weighted MAP 0.48 0.52 0.57 0.52 0.66 0.67 0.69
Table 1: Average and (weighted) Mean Average Precisions for Freebase relations based on pooled results. The #
column shows the number of true facts in the pool. NFE is statistically different to all but NF and F according to the
sign test. Bold faced are winners per relation, italics indicate ties.
lation is weighted by the relation?s number of true
facts.
Notice that we deviate from previous work in dis-
tant supervision that (a) combines the results from
several relations in a single precision recall curve,
and (b) uses held-out evaluation to measure how
well the predictions match existing Freebase facts.
This has several benefits. First, when aggregating
across relations results are often dominated by a few
very frequent relations, such as containedby, provid-
ing little information about how the models perform
across the board. Second, evaluating with Freebase
held-out data is biased. For example, we find that
frequently mentioned entity pairs are more likely to
have relations in Freebase. Systems that rank such
tuples higher receives higher precision than those
that do not have such bias, regardless of how cor-
rect their predictions are. Third, we can aggregate
per-relation comparisons to establish statistical sig-
nificance, for example via the sign test.
Also note that while we run our models on the
complete training and test set, evaluation is re-
stricted to the subsampled test set.
4.3 Predicting Freebase Relations
Table 1 shows our results for Freebase relations,
omitting those for which none of the systems can
find any relevant facts. Our first baseline is MI09,
a distantly supervised classifier based on the work
of Mintz et al (2009). This classifier only learns
from observed pattern-relation pairs in the training
set (of which we only have about 8k). By contrast,
our latent feature models can learn pattern-pattern
correlations both on the unlabeled training and test
set (comparable to bootstrapping). We hence also
compare against YA11, a version of MI09 that uses
preprocessed cluster features according to Yao et al
(2011). The third baseline is SU12, the state-of-the-
art Multi-Instance Multi-Label system by Surdeanu
et al (2012).
The remaining systems are our neighborhood
80
 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0.9 1
 0  0.2  0.4  0.6  0.8  1Precision Recall
Averaged 11-point Precision/Recall MI09YA11SU12NFNFNFE
Figure 2: Averaged 11-point precision recall curve for
Freebase relations in table 1.
model (N), the factorized model (F), their combi-
nation (NF) and the combined model with a latent
entity representation (NFE). For all our models we
use the same number of components when applica-
ble (KF = KE = 100), 1000 epochs, and 0.01 as
regularizer for component weights and 0.1 for neigh-
borhood weights.
Table 1 shows that adding pattern cluster features
(and hence incorporating more data) helps YA11
to improve over MI09. Likewise, we see that the
factorized model F improves over N, again learn-
ing from unlabeled data. This improvement is big-
ger than the corresponding change between MI09
and YA11, possibly indicating that our latent rep-
resentations are optimized directly towards improv-
ing prediction performance. The combination of N,
F and E outperforms all other models in terms of
weighted MAP, indicating the power of selectional
preferences learned from data. Note that NFE is
significantly different (p  0.05 in sign test) to all
but the NF and F models. In terms of MAP the NF
model outperforms NFE, indicating that it does not
do as well for frequent relations, but better for infre-
quent ones.
Figure 2 shows an averaged 11-point precision re-
call graph (Manning et al, 2008) for Freebase re-
lations. We notice that our latent models outper-
form all remaining models across all recall levels,
and that combining neighborhood and latent models
is helpful. This finding is consistent with our MAP
results. Figure 3 shows the recall-precision curve for
the works_written relation with respect to our three
baselines and the NFE model. Observe how preci-
 0 0.2 0.4 0.6 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Precision Recall
Recall/Precision MI09YA11SU12NFE
Figure 3: Precision and recall for works_written(X,Y).
Relation # N F NF NFE
visit 80 0.19 0.68 0.49 0.42
attend 69 0.23 0.10 0.07 0.10
base 61 0.46 0.87 0.81 0.68
head 38 0.47 0.67 0.70 0.68
scientist 36 0.25 0.84 0.79 0.73
support 18 0.16 0.29 0.32 0.38
adviser 11 0.19 0.15 0.19 0.28
criticize 9 0.09 0.60 0.67 0.64
praise 4 0.01 0.03 0.05 0.10
vote 3 0.18 0.18 0.34 0.34
MAP 0.22 0.44 0.44 0.43
Weighted MAP 0.28 0.56 0.50 0.46
Table 2: Average and (weighted) Mean Average Preci-
sions for surface patterns.2
sion drops for both MI09 and SU12 at about 50%
recall. At this point the remaining unretrieved facts
have patterns that have not been seen together with
works_written in the training set. By using cluster
features, YA11 can overcome this problem partly,
but not as dramatically as NFE?a pattern we ob-
serve for many relations.
All our models are fast to train. The slowest
model trains in just 45 minutes. By contrast, training
the topic model in YA11 alone takes 4 hours. Train-
ing SU12 takes two hours (on less data). Also notice
that our models not only learn to predict Freebase
relations, but also approximately 4k surface pattern
relations.
4.4 Predicting Surface Patterns
Table 2 presents a comparison of our models with re-
spect to 10 surface pattern relations. These relations
81
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0.9 1
 0  0.2  0.4  0.6  0.8  1Precision Recall
Averaged 11-point Precision/Recall NFNFNFE
Figure 4: Averaged 11-point precision recall curve for
surface pattern relations in table 2.
were chosen according to what we believe are inter-
esting questions not currently captured in Freebase.
We again see that learning a latent representation (F,
NF and NFE) from additional data helps quite sub-
stantially over the N model. For in the weighted
MAP metric we note that incorporating entity rep-
resentations (in the NFE model) in fact hurts total
performance.3 One reason may be the fact that Free-
base relations are typed?they require very specific
types of entities as arguments. By contrast, for a
surface pattern like ?X visits Y? X could be a person
or organization, and Y could be a location, organi-
zation or person. However, in terms of MAP score
this time there is no obvious winner among the la-
tent models. This is also confirmed by the averaged
11-point precision recall curve in figure 4.
Notice that we can accurately predict the X?
scientist-at?Y surface pattern relation in table 2,
as well as the more general person/company (em-
ployedBy) relation in table 1. This indicates that
our models can capture asymmetry?a symmetric
model would either over-predict X?scientist-at?Y
or under-predict person/company.
5 Conclusion
We present relation extraction into universal
schemas. Such schemas contain surface patterns
as relations, as well as relations from structured
sources. By predicting missing tuples for surface
pattern relations we can populate a database with-
out any labelled data, and answer questions not sup-
3Due to the small set of relations only N is significantly dif-
ferent to F, NF and NFE (p 0.05 in sign test).
ported by the structured schema alone. By predict-
ing missing tuples in the structured schema we can
expand a knowledge base of fixed schema, and only
require a set of existing facts from this schema. Cru-
cially, by predicting and modeling both surface pat-
terns and structured relations simultaneously we can
improve performance. We show this experimentally
by contrasting a series of the popular weakly super-
vised models to our collaborative filtering models
that learn latent feature representations across sur-
face patterns and structured relations. Moreover, our
models are computationally efficient, requiring less
time than comparable methods, while learning more
relations.
Reasoning with universal schemas is not merely a
tool for information extraction. It can also serve as
a framework for various data integration tasks. For
example, we could integrate facts from one schema
(say, Freebase) into another (say, the TAC KBP
schema) by adding both sets of relations to the set
of surface patterns. Reasoning with this schema
will mean populating each database with facts from
the other, and would leverage information in surface
patterns to improve integration. In future work we
also plan to integrate universal entity types and at-
tributes into the model.
The source code of our system, its output, and
all data annotations are available at http://www.
riedelcastro.org/uschema.
Acknowledgments
We thank the reviewers for very helpful comments.
This work was supported in part by the Center for In-
telligent Information Retrieval and the University of
Massachusetts, in part by UPenn NSF medium IIS-
0803847, in part by DARPA under agreement num-
ber FA8750-13-2-0020 and FA8750-09-C-0181, and
in part by an award from Google. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the view of DARPA,
AFRL, or the US government.
References
Ronald J. Brachman. 1983. What is-a is and isn t:
An analysis of taxonomic links in semantic networks.
IEEE Computer, 16(10):30?36.
82
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proceedings of the 25th AAAI Con-
ference on Artificial Intelligence (AAAI ?10).
Michael Collins, Sanjoy Dasgupta, and Robert E.
Schapire. 2001. A generalization of principal com-
ponent analysis to the exponential family. In Proceed-
ings of NIPS.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77?86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, Barcelona, Spain.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from Large
Corpora. Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL
?04), pages 415?422.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of ACL.
Stanley Kok and Pedro Domingos. 2008. Extracting Se-
mantic Networks from Text Via Relational Clustering.
In ECML.
Yehuda Koren. 2008. Factorization meets the neighbor-
hood: a multifaceted collaborative filtering model. In
Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?08, pages 426?434, New York, NY, USA.
ACM.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining, pages 323?328.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ?09),
pages 1003?1011. Association for Computational Lin-
guistics.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In COLING, pages 1933?1950.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st
international conference on World Wide Web, WWW
?12, pages 271?280, New York, NY, USA. ACM.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,
and Lars Schmidt-Thieme. 2009. Bpr: Bayesian per-
sonalized ranking from implicit feedback. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, UAI ?09, pages 452?461, Ar-
lington, Virginia, United States. AUAI Press.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ?08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, HLT-NAACL ?06, pages 304?
311, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D. Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
83
of the Conference on Empirical methods in natural
language processing (EMNLP ?12), pages 455?465.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 849?856,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2011. Probabilistic matrix factorization leveraging
contexts for unsupervised relation discovery. In Pro-
ceedings of PAKDD.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?11), July.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012a. Probabilistic databases of universal schema.
In Proceedings of the AKBC-WEKEX Workshop at
NAACL 2012, June.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012b. Unsupervised relation discovery with sense
disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?12), July.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In Proceedings of the 44th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?06).
84
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 712?720,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Relation Discovery with Sense Disambiguation
Limin Yao Sebastian Riedel Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{lmyao,riedel,mccallum}@cs.umass.edu
Abstract
To discover relation types from text, most
methods cluster shallow or syntactic patterns
of relation mentions, but consider only one
possible sense per pattern. In practice this
assumption is often violated. In this paper
we overcome this issue by inducing clusters
of pattern senses from feature representations
of patterns. In particular, we employ a topic
model to partition entity pairs associated with
patterns into sense clusters using local and
global features. We merge these sense clus-
ters into semantic relations using hierarchical
agglomerative clustering. We compare against
several baselines: a generative latent-variable
model, a clustering method that does not dis-
ambiguate between path senses, and our own
approach but with only local features. Exper-
imental results show our proposed approach
discovers dramatically more accurate clusters
than models without sense disambiguation,
and that incorporating global features, such as
the document theme, is crucial.
1 Introduction
Relation extraction (RE) is the task of determin-
ing semantic relations between entities mentioned in
text. RE is an essential part of information extraction
and is useful for question answering (Ravichandran
and Hovy, 2002), textual entailment (Szpektor et al,
2004) and many other applications.
A common approach to RE is to assume that rela-
tions to be extracted are part of a predefined ontol-
ogy. For example, the relations are given in knowl-
edge bases such as Freebase (Bollacker et al, 2008)
or DBpedia (Bizer et al, 2009). However, in many
applications, ontologies do not yet exist or have low
coverage. Even when they do exist, their mainte-
nance and extension are considered to be a substan-
tial bottleneck. This has led to considerable inter-
est in unsupervised relation discovery (Hasegawa et
al., 2004; Banko and Etzioni, 2008; Lin and Pantel,
2001; Bollegala et al, 2010; Yao et al, 2011). Here,
the relation extractor simultaneously discovers facts
expressed in natural language, and the ontology into
which they are assigned.
Many relation discovery methods rely exclusively
on the notion of either shallow or syntactic patterns
that appear between two named entities (Bollegala et
al., 2010; Lin and Pantel, 2001). Such patterns could
be sequences of lemmas and Part-of-Speech tags, or
lexicalized dependency paths. Generally speaking,
relation discovery attempts to cluster such patterns
into sets of equivalent or similar meaning. Whether
we use sequences or dependency paths, we will en-
counter the problem of polysemy. For example, a
pattern such as ?A beat B? can mean that person A
wins over B in competing for a political position,
as pair ?(Hillary Rodham Clinton, Jonathan Tasini)?
in ?Sen Hillary Rodham Clinton beats rival Jonathan
Tasini for Senate.? It can also indicate that an athlete
A beat B in a sports match, as pair ?(Dmitry Tur-
sunov, Andy Roddick)? in ?Dmitry Tursunov beat
the best American player Andy Roddick.? More-
over, it can mean ?physically beat? as pair ?(Mr.
Harris, Mr. Simon)? in ?On Sept. 7, 1999, Mr. Har-
ris fatally beat Mr. Simon.? This is known as poly-
semy. If we work with patterns alone, our extractor
will not be able to differentiate between these cases.
Most previous approaches do not explicitly ad-
dress this problem. Lin and Pantel (2001) assumes
only one sense per path. In (Pantel et al, 2007),
they augment each relation with its selectional pref-
712
erences, i.e. fine-grained entity types of two ar-
guments, to handle polysemy. However, such fine
grained entity types come at a high cost. It is difficult
to discover a high-quality set of fine-grained entity
types due to unknown criteria for developing such
a set. In particular, the optimal granularity of en-
tity types depends on the particular pattern we con-
sider. For example, a pattern like ?A beat B? could
refer to A winning a sports competition against B, or
a political election. To differentiate between these
senses we need types such as ?Politician? or ?Ath-
lete?. However, for ?A, the parent of B? we only
need to distinguish between persons and organiza-
tions (for the case of the sub-organization relation).
In addition, there are senses that just cannot be de-
termined by entity types alone: Take the meaning
of ?A beat B? where A and B are both persons; this
could mean A physically beats B, or it could mean
that A defeated B in a competition.
In this paper we address the problem of polysemy,
while we circumvent the problem of finding fine-
grained entity types. Instead of mapping entities to
fine-grained types, we directly induce pattern senses
by clustering feature representations of pattern con-
texts, i.e. the entity pairs associated with a pattern.
This allows us to employ not only local features such
as words, but also global features such as the docu-
ment and sentence themes.
To cluster the entity pairs of a single relation pat-
tern into senses, we develop a simple extension to
Latent Dirichlet Allocation (Blei et al, 2003). Once
we have our pattern senses, we merge them into
clusters of different patterns with a similar sense.
We employ hierarchical agglomerative clustering
with a similarity metric that considers features such
as the entity arguments, and the document and sen-
tence themes.
We perform experiments on New York Times ar-
ticles and consider lexicalized dependency paths as
patterns in our data. In the following we shall use
the term path and pattern exchangeably. We com-
pare our approach with several baseline systems, in-
cluding a generative model approach, a clustering
method that does not disambiguate between senses,
and our approach with different features. We per-
form both automatic and manual evaluations. For
automatic evaluation, we use relation instances in
Freebase as ground truth, and employ two clustering
metrics, pairwise F-score and B3 (as used in cofer-
ence). Experimental results show that our approach
improves over the baselines, and that using global
features achieves better performance than using en-
tity type based features. For manual evaluation, we
employ a set intrusion method (Chang et al, 2009).
The results also show that our approach discovers re-
lation clusters that human evaluators find coherent.
2 Our Approach
We induce pattern senses by clustering the entity
pairs associated with a pattern, and discover seman-
tic relations by clustering these sense clusters. We
represent each pattern as a list of entity pairs and
employ a topic model to partition them into different
sense clusters using local and global features. We
take each sense cluster of a pattern as an atomic clus-
ter, and use hierarchical agglomerative clustering to
organize them into semantic relations. Therefore, a
semantic relation comprises a set of sense clusters of
patterns. Note that one pattern can fall into different
semantic relations when it has multiple senses.
2.1 Sense Disambiguation
In this section, we discuss the details of how we dis-
cover senses of a pattern. For each pattern, we form
a clustering task by collecting all entity pairs the pat-
tern connects. Our goal is to partition these entity
pairs into sense clusters. We represent each pair by
the following features.
Entity names: We use the surface string of the en-
tity pair as features. For example, for pattern ?A play
B?, pairs which contain B argument ?Mozart? could
be in one sense, whereas pairs which have ?Mets?
could be in another sense.
Words: The words between and around the two
entity arguments can disambiguate the sense of a
path. For example, ?A?s parent company B? is dif-
ferent from ?A?s largest company B? although they
share the same path ?A?s company B?. The former
describes the sub-organization relationship between
two companies, while the latter describes B as the
largest company in a location A. The two words to
the left of the source argument, and to the right of the
destination argument also help sense discovery. For
example, in ?Mazurkas played by Anna Kijanowska,
pianist?, ?pianist? tells us pattern ?A played by B?
713
takes the ?music? sense.
Document theme: Sometimes, the same pattern
can express different relations in different docu-
ments, depending on the document?s theme. For
instance, in a document about politics, ?A defeated
B? is perhaps about a politician that won an elec-
tion against another politician. While in a document
about sports, it could be a team that won against an-
other team in a game, or an athlete that defeated an-
other athlete. In our experiments, we use the meta-
descriptors of a document as side information and
train a standard LDA model to find the theme of a
document. See Section 3.1 for details.
Sentence theme: A document may cover several
themes. Moreover, sometimes the theme of a doc-
ument is too general to disambiguate senses. We
therefore also extract the theme of a sentence as a
feature. Details are in 3.1.
We call entity name and word features local, and
the two theme features global.
We employ a topic model to discover senses for
each path. Each path pi forms a document, and it
contains a list of entity pairs co-occurring with the
path in the tuples. Each entity pair is represented
by a list of features fk as we described. For each
path, we draw a multinomial distribution ? over top-
ics/senses. For each feature of an entity pair, we
draw a topic/sense from ?pi . Formally, the gener-
ative process is as follows:
?pi ? Dirichlet(?)
?z ? Dirichlet(?)
ze ? Multinomial(?pi)
fk ? Multinomial(?ze)
Assume we have m paths and l entity pairs for each
path. We denote each entity pair of a path as e(pi) =
(f1, . . . , fn). Hence we have:
P (e1(pi), e2(pi), . . . , el(pi)|z1, z2, . . . , zl)
=
l?
j=1
n?
k=1
p(fk|zj)p(zj)
We assume the features are conditionally indepen-
dent given the topic assignments. Each feature is
generated from a multinomial distribution ?. We
use Dirichlet priors on ? and ?. Figure 1 shows the
graphical representation of this model.
S
p
?
e(p)
f
?
?
z
?
n
Figure 1: Sense-LDA model.
This model is a minor variation on standard LDA
and the difference is that instead of drawing an ob-
servation from a hidden topic variable, we draw
multiple observations from a hidden topic variable.
Gibbs sampling is used for inference. After infer-
ence, each entity pair of a path is assigned to one
topic. One topic is one sense. Entity pairs which
share the same topic assignments form one sense
cluster.
2.2 Hierarchical Agglomerative Clustering
After discovering sense clusters of paths, we employ
hierarchical agglomerative clustering (HAC) to dis-
cover semantic relations from these sense clusters.
We apply the complete linkage strategy and take co-
sine similarity as the distance function. The cutting
threshold is set to 0.1.
We represent each sense cluster as one vector by
summing up features from each entity pair in the
cluster. The weight of a feature indicates how many
entity pairs in the cluster have the feature. Some
features may get larger weights and dominate the co-
sine similarity. We down-weigh these features. For
example, we use binary features for word ?defeat?
in sense clusters of pattern ?A defeat B?. The two
theme features are extracted from generative mod-
els, and each is a topic number.
Our approach produces sense clusters for each
path and semantic relation clusters of the whole data.
Table 1 and 2 show some example output.
3 Experiments
We carry out experiments on New York Times ar-
ticles from years 2000 to 2007 (Sandhaus, 2008).
Following (Yao et al, 2011), we filter out noisy doc-
uments and use natural language packages to anno-
tate the documents, including NER tagging (Finkel
et al, 2005) and dependency parsing (Nivre et al,
2004). We extract dependency paths for each pair of
named entities in one sentence. We use their lemmas
714
Path 20:sports 30:entertainment 25:music/art
A play B
Americans, Ireland Jean-Pierre Bacri, Jacques Daniel Barenboim, recital of Mozart
Yankees, Angels Rita Benton, Gay Head Dance Mr. Rose, Ballade
Ecuador, England Jeanie, Scrabble Gil Shaham, Violin Romance
Redskins, Detroit Meryl Streep, Leilah Ms. Golabek, Steinways
Red Bulls, F.C. Barcelona Kevin Kline, Douglas Fairbanks Bruce Springsteen, Saints
doc theme sports music books television music theater
sen theme game yankees theater production book film show music reviews opera
lexical words beat victory num-num won played plays directed artistic director conducted production
entity names - r:theater r:theater r:hall r:york l:opera
Table 1: Example sense clusters produced by sense disambiguation. For each sense, we randomly sample 5 entity
pairs. We also show top features for each sense. Each row shows one feature type, where ?num? stands for digital
numbers, and prefix ?l:? for source argument, prefix ?r:? for destination argument. Some features overlap with each
other. We manually label each sense for easy understanding. We can see the last two senses are close to each other.
For two theme features, we replace the theme number with the top words. For example, the document theme of the
first sense is Topic30, and Topic30 has top words ?sports?.
relation paths
entertainment A, who play B:30; A play B:30; star A as B:30
sports
lead A to victory over B:20; A play to B:20; A play B:20; A?s loss to B:20; A beat B:20; A trail B:20;
A face B:26; A hold B:26; A play B:26; A acquire (X) from B:26; A send (X) to B:26;
politics
A nominate B:39; A name B:39; A select B:39; A name B:42; A select B:42;
A ask B:42; A choose B:42; A nominate B:42; A turn to B:42;
law A charge B:39; A file against B:39; A accuse B:39; A sue B:39
Table 2: Example semantic relation clusters produced by our approach. For each cluster, we list the top paths in it,
and each is followed by ?:number?, indicating its sense obtained from sense disambiguation. They are ranked by the
number of entity pairs they take. The column on the left shows sense of each relation. They are added manually by
looking at the sense numbers associated with each path.
for words on the dependency paths. Each entity pair
and the dependency path which connects them form
a tuple.
We filter out paths which occur fewer than 200
times and use some heuristic rules to filter out paths
which are unlikely to represent a relation, for exam-
ple, paths in with both arguments take the syntac-
tic role ?dobj? (direct objective) in the dependency
path. In such cases both arguments are often part
of a coordination structure, and it is unlikely that
they are related. In summary, we collect about one
million tuples, 1300 patterns and half million named
entities. In terms of named entities, the data is very
sparse. On average one named entity occurs four
times.
3.1 Feature Extraction
For the entity name features, we split each entity
string of a tuple into tokens. Each token is a fea-
ture. The source argument tokens are augmented
with prefix ?l:?, and the destination argument tokens
with prefix ?r:?. We use tokens to encourage overlap
between different entities.
For the word features, we extract all the words be-
tween the two arguments, removing stopwords and
the words with capital letters. Words with capital
letters are usually named entities, and they do not
tend to indicate relations. We also extract neigh-
boring words of source and destination arguments.
The two words to the left of the source argument are
added with prefix ?lc:?. Similarly the two words to
the right of the destination arguments are added with
prefix ?rc:?.
Each document in the NYT corpus is associated
with many descriptors, indicating the topic of the
document. For example, some documents are la-
beled as ?Sports?, ?Dallas Cowboys?, ?New York
Giants?, ?Pro Football? and so on. Some are labeled
715
as ?Politics and Government?, and ?Elections?. We
shall extract a theme feature for each document from
these descriptors. To this end we interpret the de-
scriptors as words in documents, and train a standard
LDA model based on these documents. We pick the
most frequent topic as the theme of a document.
We also train a standard LDA model to obtain
the theme of a sentence. We use a bag-of-words
representation for a document and ignore sentences
from which we do not extract any tuples. The LDA
model assigns each word to a topic. We count the
occurrences of all topics in one sentence and pick
the most frequent one as its theme. This feature
captures the intuition that different words can indi-
cate the same sense, for example, ?film??, ?show?,
?series? and ?television? are about ?entertainment?,
while ?coach?, ?game?, ?jets?, ?giants? and ?sea-
son? are about ?sports?.
3.2 Sense clusters and relation clusters
For the sense disambiguation model, we set the
number of topics (senses) to 50. We experimented
with other numbers, but this setting yielded the best
results based on our automatic evaluation measures.
Note that a path has a multinomial distribution over
50 senses but only a few senses have non-zero prob-
abilities.
We look at some sense clusters of paths. For
path ?A play B?, we examine the top three senses,
as shown in Table 1. The last two senses ?enter-
tainment? and ?music? are close. Randomly sam-
pling some entity pairs from each of them, we find
that the two sense clusters are precise. Only 1% of
pairs from the sense cluster ?entertainment? should
be assigned to the ?music? sense. For the path ?play
A in B? we discover two senses which take the
most probabilities: ?sports? and ?art?. Both clus-
ters are precise. However, the ?sports? sense may
still be split into more fine-grained sense clusters. In
?sports?, 67% pairs mean ?play another team in a
location? while 33% mean ?play another team in a
game?.
We also closely investigate some relation clusters,
shown in Table 2. Both the first and second relation
contain path ?A play B? but with different senses.
For the second relation, most paths state ?play? re-
lations between two teams, while a few of them
express relations of teams acquiring players from
other teams. For example, the entity pair ?(Atlanta
Hawks, Dallas Mavericks)? mentioned in sentence
?The Atlanta Hawks acquired point guard Anthony
Johnson from the Dallas Mavericks.? This is due to
that they share many entity pairs of team-team.
3.3 Baselines
We compare our approach against several baseline
systems, including a generative model approach and
variations of our own approach.
Rel-LDA: Generative models have been suc-
cessfully applied to unsupervised relation extrac-
tion (Rink and Harabagiu, 2011; Yao et al, 2011).
We compare against one such model: An extension
to standard LDA that falls into the framework pre-
sented by Yao et al (2011). Each document con-
sists of a list of tuples. Each tuple is represented by
features of the entity pair, as listed in 2.1, and the
path. For each document, we draw a multinomial
distribution over relations. For each tuple, we draw
a relation topic and independently generate all the
features. The intuition is that each document dis-
cusses one domain, and has a particular distribution
over relations.
In our experiments, we test different numbers of
relation topics. As the number goes up, precision in-
creases whereas recall drops. We report results with
300 and 1000 relation topics.
One sense per path (HAC): This system uses
only hierarchical clustering to discover relations,
skipping sense disambiguation. This is similar to
DIRT (Lin and Pantel, 2001). In DIRT, each path
is represented by its entity arguments. DIRT cal-
culates distributional similarities between different
paths to find paths which bear the same semantic re-
lation. It does not employ global topic model fea-
tures extracted from documents and sentences.
Local: This system uses our approach (both sense
clustering with topic models and hierarchical clus-
tering), but without global features.
Local+Type This system adds entity type features to
the previous system. This allows us to compare per-
formance of using global features against entity type
features. To determine entity types, we link named
entities to Wikipedia pages using the Wikifier (Rati-
nov et al, 2011) package and extract categories from
the Wikipedia page. Generally Wikipedia provides
many types for one entity. For example, ?Mozart? is
716
a person, musician, pianist, composer, and catholic.
As we argued in Section 1, it is difficult to determine
the right granularity of the entity types to use. In our
experiments, we use all of them as features. In hier-
archical clustering, for each sense cluster of a path,
we pick the most frequent entity type as a feature.
This approach can be seen as a proxy to ISP (Pantel
et al, 2007), since selectional preferences are one
way of distinguishing multiple senses of a path.
Our Approach+Type This system adds Wikipedia
entity type features to our approach. The Wikipedia
feature is the same as used in the previous system.
4 Evaluations
4.1 Automatic Evaluation against Freebase
We evaluate relation clusters discovered by all ap-
proaches against Freebase. Freebase comprises a
large collection of entities and relations which come
from varieties of data sources, including Wikipedia
infoboxes. Many users also contribute to Freebase
by annotating relation instances. We use coreference
evaluation metrics: pairwise F-score and B3 (Bagga
and Baldwin, 1998). Pairwise metrics measure how
often two tuples which are clustered in one seman-
tic relation are labeled with the same Freebase label.
We evaluate approximately 10,000 tuples which oc-
cur in both our data and Freebase. Since our sys-
tem predicts fine-grained clusters comparing against
Freebase relations, the measure of recall is underes-
timated. The precision measure is more reliable and
we employ F-0.5 measure, which places more em-
phasis on precision.
Matthews correlation coefficient (MCC) (Baldi et
al., 2000) is another measure used in machine learn-
ing, which takes into account true and false positives
and negatives and is generally regarded as a bal-
anced measure which can be used when the classes
are of very different sizes. In our case, the true nega-
tive number is 100 times larger than the true positive
number. Therefor we also employ MCC, calculated
as
MCC = TP?TN?FP?FN?
(TP+FP )(TP+FN)(TN+FP )(TN+FN)
The MCC score is between -1 and 1. The larger the
better. In perfect predictions, FP and FN are 0, and
the MCC score is 1. A random prediction results in
score 0.
Table 3 shows the results of all systems. Our ap-
proach achieves the best performance in most mea-
sures. Without using sense disambiguation, the per-
formance of hierarchical clustering decreases signif-
icantly, losing 17% in precision in the pairwise mea-
sure, and 15% in terms ofB3. The generative model
approach with 300 topics achieves similar precision
to the hierarchical clustering approach. With more
topics, the precision increases, however, the recall
of the generative model is much lower than those
of other approaches. We also show the results of
our approach without global document and sentence
theme features (Local). In this case, both precision
and recall decrease. We compare global features
(Our approach) against Wikipedia entity type fea-
tures (Local+Type). We see that using global fea-
tures achieves better performance than using entity
type based features. When we add entity type fea-
tures to our approach, the performance does not in-
crease. The entity type features do not help much
is due to that we cannot determine which particular
type to choose for an entity pair. Take pair ?(Hillary
Rodham Clinton, Jonathan Tasini)? as an example,
choosing politician for both arguments instead of
person will help.
We should note that these measures provide com-
parison between different systems although they
are not accurate. One reason is the following:
some relation instances should have multiple la-
bels but they have only one label in Freebase.
For example, instances of a relation that a per-
son ?was born in? a country could be labeled
as ?/people/person/place of birth? and as ?/peo-
ple/person/nationality?. This decreases the pairwise
precision. Further discussion is in Section 4.3.
4.2 Path Intrusion
We also evaluate coherence of relation clusters pro-
duced by different approaches by creating path in-
trusion tasks (Chang et al, 2009). In each task, some
paths from one cluster and an intruding path from
another are shown, and the annotator?s job is to iden-
tify one single path which is out of place. For each
path, we also show the annotators one example sen-
tence. Three graduate students in natural language
processing annotate intruding paths. For disagree-
ments, we use majority voting. Table 4 shows one
example intrusion task.
717
System
Pairwise B3
Prec. Rec. F-0.5 MCC Prec. Rec. F-0.5
Rel-LDA/300 0.593 0.077 0.254 0.191 0.558 0.183 0.396
Rel-LDA/1000 0.638 0.061 0.220 0.177 0.626 0.160 0.396
HAC 0.567 0.152 0.367 0.261 0.523 0.248 0.428
Local 0.625 0.136 0.364 0.264 0.626 0.225 0.462
Local+Type 0.718 0.115 0.350 0.265 0.704 0.201 0.469
Our Approach 0.736 0.156 0.422 0.314 0.677 0.233 0.490
Our Approach+Type 0.682 0.110 0.334 0.250 0.687 0.199 0.460
Table 3: Pairwise and B3 evaluation for various systems. Since our systems predict more fine-grained clusters than
Freebase, the recall measure is underestimated.
Path Example sentence
A beat B Dmitry Tursunov beat the best American player, Andy Roddick
A, who lose to B Sluman, Loren Roberts (who lost a 1994 Open playoff to Ernie Els at Oakmont ...
A, who beat B ... offender seems to be the Russian Mariya Sharapova, who beat Jelena Dokic
A, a broker at B Robert Bewkes, a broker at UBS for 12 years
A meet B Howell will meet Geoff Ogilvy, Harrington will face Davis Love III
Table 4: A path intrusion task. We show 5 paths and ask the annotator to identify one path which does not belong to
the cluster. And we show one example sentence for each path. The entities (As and Bs) in the sentences are bold. And
the italic row here indicates the intruder.
System Correct
Rel-LDA/300 0.737
Rel-LDA/1000 0.821
HAC 0.852
Local+Type 0.773
Our approach 0.887
Table 5: Results of intruding tasks of all systems.
From Table 5, we see that our approach achieves
the best performance. We concentrate on some in-
trusion tasks and compare the clusters produced by
different systems.
The clusters produced by HAC (without sense dis-
ambiguation) is coherent if all the paths in one rela-
tion take a particular sense. For example, one task
contains paths ?A, director at B?, ?A, specialist at
B?, ?A, researcher at B?, ?A, B professor? and ?A?s
program B?. It is easy to identify ?A?s program B?
as an intruder when the annotators realize that the
other four paths state the relation that people work
in an educational institution. The generative model
approach produces more coherent clusters when the
number of relation topics increases.
The system which employs local and entity type
features (Local+Type) produces clusters with low
coherence because the system puts high weight on
types. For example, (United States, A talk with B,
Syria) and (Canada, A defeat B, United States) are
clustered into one relation since they share the argu-
ment types ?country?-?country?. Our approach us-
ing the global theme features can correct such errors.
4.3 Error Analysis
We also closely analyze the pairwise errors that we
encounter when comparing against Freebase labels.
Some errors arise because one instance can have
multiple labels, as we explained in Section 4.1. One
example is the following: Our approach predicts that
(News Corporation, buy, MySpace) and (Dow Jones
& Company, the parent of, The Wall Street Journal)
are in one relation. In Freebase, one is labeled as
?/organization/parent/child?, the other is labeled as
?/book/newspaper owner/newspapers owned?. The
latter is a sub-relation of the former. We can over-
come this issue by introducing hierarchies in relation
labels.
Some errors are caused by selecting the incorrect
sense for an entity pair of a path. For instance, we
put (Kenny Smith, who grew up in, Queens) and
(Phil Jackson, return to, Los Angeles Lakers) into
718
the ?/people/person/place of birth? relation cluster
since we do not detect the ?sports? sense for the en-
tity pair ?(Phil Jackson, Los Angeles Lakers)?.
5 Related Work
There has been considerable interest in unsupervised
relation discovery, including clustering approach,
generative models and many other approaches.
Our work is closely related to DIRT (Lin and Pan-
tel, 2001). Both DIRT and our approach represent
dependency paths using their arguments. Both use
distributional similarity to find patterns representing
similar semantic relations. Based on DIRT, Pantel
et al (2007) addresses the issue of multiple senses
per path by automatically learning admissible argu-
ment types where two paths are similar. They cluster
arguments to fine-grained entity types and rank the
associations of a relation with these entity types to
discover selectional preferences. Selectional prefer-
ences discovery (Ritter et al, 2010; Seaghdha, 2010)
can help path sense disambiguation, however, we
show that using global features performs better than
entity type features.
Our approach is also related to feature parti-
tioning in cross-cutting model of lexical seman-
tics (Reisinger and Mooney, 2011). And our sense
disambiguation model is inspired by this work.
There they partition features of words into views and
cluster words inside each view. In our case, each
sense of a path can be seen as one view. However,
we allow different views to be merged since some
views overlap with each other.
Hasegawa et al (2004) cluster pairs of named en-
tities according to the similarity of context words in-
tervening between them. Hachey (2009) uses topic
models to perform dimensionality reduction on fea-
tures when clustering entity pairs into relations. Bol-
legala et al (2010) employ co-clustering to find clus-
ters of entity pairs and patterns jointly. All the ap-
proaches above neither deal with polysemy nor in-
corporate global features, such as sentence and doc-
ument themes.
Open information extraction aims to discover re-
lations independent of specific domains (Banko et
al., 2007; Banko and Etzioni, 2008). They employ
a self-learner to extract relation instances, but no
attempt is made to cluster instances into relations.
Yates and Etzioni (2009) present RESOLVER for
discovering relational synonyms as a post process-
ing step. Our approach falls into the same category.
Moreover, we explore path senses and global fea-
tures for relation discovery.
Many generative probabilistic models have been
applied to relation extraction. For example, vari-
eties of topic models are employed for both open
domain (Yao et al, 2011) and in-domain relation
discovery (Chen et al, 2011; Rink and Harabagiu,
2011). Our approach employs generative models
for path sense disambiguation, which achieves better
performance than directly applying generative mod-
els to unsupervised relation discovery.
6 Conclusion
We explore senses of paths to discover semantic re-
lations. We employ a topic model to partition en-
tity pairs of a path into different sense clusters and
use hierarchical agglomerative clustering to merge
senses into semantic relations. Experimental results
show our approach discovers precise relation clus-
ters and outperforms a generative model approach
and a clustering method which does not address
sense disambiguation. We also show that using
global features improves the performance of unsu-
pervised relation discovery over using entity type
based features.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and the Uni-
versity of Massachusetts gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
719
Pierre Baldi, S?ren Brunak, Yves Chauvin, Claus A. F.
Andersen, and Henrik Nielsen. 2000. Assessing the
accuracy of prediction algorithms for classification: an
overview. Bioinformatics, 16:412?424.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI2007.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Se-
bastian Hellmann. 2009. DBpedia - a crystallization
point for the web of data. Journal of Web Semantics:
Science, Services and Agents on the World Wide Web,
pages 154?165.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research, 3:993?1022, January.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ?08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247?1250, New York, NY, USA.
ACM.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2010. Relational duality: Unsupervised ex-
traction of semantic relations between entities on the
web. In Proceedings of WWW.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of ACL.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
Benjamin Hachey. 2009. Towards Generic Relation Ex-
traction. Ph.D. thesis, University of Edinburgh.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proceedings of KDD.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to Wikipedia. In Proceedings of ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Joseph Reisinger and Raymond J. Mooney. 2011. Cross-
cutting models of lexical semantics. In Proceedings of
EMNLP.
Bryan Rink and Sanda Harabagiu. 2011. A generative
model for unsupervised discovery of relations and ar-
gument classes from clinical texts. In Proceedings of
EMNLP.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for Selectional Pref-
erences. In Proceedings of ACL10.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings of ACL 10.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of EMNLP.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
720
