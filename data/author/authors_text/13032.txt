Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
A Pattern-Based Analyzer for French in the Context of Spoken
Language Translation: First Prototype and Evaluation
Herv? BLANCHON
GETA-CLIPS (IMAG)
BP 53
38041 Grenoble Cedex 9, France
herve.blanchon@imag.fr
Abstract
In this paper, we describe a first prototype of
a pattern-based analyzer developed in the
context of a speech-to-speech translation
project using a pivot-based approach (the
pivot is called IF). The chosen situation
involves a French client talking to an Italian
travel agent (both in their own language) to
organize a stay in the Trentino area.
An IF consists of a dialogue act, and a list,
possibly empty, of argument values. The
analyzer applies a "phrase spotting"
mechanism on the output of the speech
recognition module. It finds well-formed
phrases corresponding to argument values.
A dialogue act is then built according to the
instantiated arguments and some other
features of the input.
The current version of the prototype has
been involved in an evaluation campaign on
an unseen corpus of four dialogues
consisting of 235 speech turns. The results
are given and commented in the last part of
the paper. We think they pave the way for
future enhancements to both the coverage
and the development methodology.
R?sum?
Dans cet article, nous d?crivons la premi?re
version d'un analyseur fond? sur des patrons
dans le contexte d'un projet de traduction de
parole utilisant une technique de traduction
par pivot (le pivot est appel? IF). Dans la
situation choisie, un client fran?ais parle
avec un agent italien (chacun dans sa langue
maternelle) pour organiser un s?jour dans la
r?gion du Trentin en Italie.
Une IF se compose d'un acte de dialogue et
d'une liste, ?ventuellement vide, de valeurs
d'arguments. L'analyseur met en ?uvre un
m?canisme de reconnaissance de syntagmes
sur la sortie du module de reconnaissance de
la parole. Cela permet de trouver des
syntagmes bien form?s qui correspondent ?
des valeurs d'arguments. L'acte de parole est
alors construit en utilisant les arguments
instanci?s ainsi que d'autres caract?ristiques
de l'entr?e.
Cette version du prototype a ?t? mis en
?uvre lors d'une ?valuation sur un corpus de
quatre dialogues, non utilis?s pour le
d?veloppement, compos? de 235 tours de
parole du client. Les r?sultats sont donn?s
dans la derni?re section de cet article. Nous
pensons qu'ils ouvrent la voix pour de
futures am?liorations de la couverture ainsi
que de la m?thodologie de d?veloppement.
Introduction
In the framework of the NESPOLE! project
[Besacier, L., & al., 2001; Lazzari, G., 2000]
funded by the EU and the NSF we are exploring
future applications of automatic speech-to-
speech translation in the e-commerce and e-
services areas. For the actual translation we are
using a pivot-based approach (the pivot is called
IF for Interchange Format). Thus, we have to
develop the analysis from the textual output of
an automatic speech recognition module towards
the IF and the generation from the IF towards a
text-to-speech input text.
In this context, the analyzer has to be robust
against ill-formed input (in terms of syntax) and
recognition errors, both likely to be quite
common. To cope with these problems several
families of methods may be used: a rule-based
approaches with rules being relaxed if needed, a
?let the number do every thing? approaches
(using aligned source language inputs and
their pivot representations), a pattern-
based approaches (focusing on important
features of the input), and finally a
mixture of the previous ones.
Taking into account the way the pivot
represents the information present in the input
and the possible methods, we chose to
investigate a pattern-based approach (as in
[Zong, C., & al., 2000]). In this paper we will
focus on the first prototype of an analysis
module from French to a pivot called IF
(Interchange Format). We will justify our
choices with regard to the pivot specification
and describe the realization; finally we will give
several numbers about an evaluation this
analyzer was involved in.
1 Context and choices
1.1 Interchange Format
The IF we are currently using is an extension of
the one used in the C-STAR II context [Levin,
L., & al., 2000; Levin, L., & al., 1998]. It is
designed to abstract away from peculiarities of
any particular language in order to allow for
translation that are non-literal but capture the
speaker?s intent.
The IF is based on domain actions (DAs) that
consist of speech act and concepts. We have
currently defined 62 general speech acts (e.g.
acknowledge, introduce-self, g i v e -
information,). The concepts are split into 9
attitudes (e.g. disposition, feasibility,
obligation), and 97 main predications or
predication participants (e.g. price , room,
activity).
In addition to the DA, an IF representation may
contain arguments (e.g. disposition, price,
room-spec). The arguments have values that
represent information about the speech acts and
the concepts. There is currently about 280
arguments, 150 of them being top-level (e.g.
disposition, p r i c e , room-spec). The
others arguments do not exist on their own, they
are embedded within the top-level arguments
(e.g. quantity, currency, identifiability).
For an utterance meaning "je voudrais la
chambre ? 70 euros"1 the IF would be:
c: indicates that the client is speaking. give-
information+disposition+price+room is
the DA. disposition, price, room-spec are
the top-level arguments. quantity, currency,
identifiability are embedded arguments.
1.2 Constaints
The IF specification is giving constraints on the
construction of an IF at each levels.
Speech acts are defined with their possible
concept continuations (e.g. disposition,
price, availability concepts may follow
give-information ) and their licensed
arguments (rhetorical relations, e.g. cause ,
conjunction, disjunction). Concepts are
also defined with their possible continuations
(e.g. accommodation, room , activity
concepts may follow price) and arguments
(e.g. for-whom, price, time arguments may
be arguments of price).
Arguments are defined by their possible value,
relations and attributes. The value may be
question (the argument is questioned) or a set
of actual values (e.g. double, single, twin
for a room-spec). It is also possible to handle
relatives and pronouns. Relations define links
between two concepts (e.g. bed-spec,
location , p r i c e  for a room-spec).
Attributes define links between a concept and a
set of values (e.g. quantity, identifiability).
An attribute is defined only with a value and
attributes, no relation.
1.3 Choices
We had to choose a methodology for the
analysis from a speech recognition output
towards an IF and the generation form an IF to a
French text.
For the generation it was decided to apply a rule-
based approach under Ariane-G5 [Boitet, C.,
1997]. We are using the IF specification files to
                                                      
1 "I would like the room that costs 70 euros"
c:give-information+disposition+price+room
  ( disposition=(who=i, desire), 
    price=(quantity=70, currency=euro),
    room-spec=(identifiability=yes, room)
  )
automatically produce parts of the dictionaries
and the grammars. The IF is parsed into a
French linguistic tree passed to a general-
purpose French generation module. This
approach forces us to develop a specification as
clean as possible describing all the possible
"events". At the end, every potential IF input
should be covered. The drawbacks of this
approach are the bootstrapping process, which
takes a huge amount of time, and the continuous
changes in the IF specification we have to cope
with. For the generation, we are also
experimenting a pattern-based approach in
which DA families are associated with template
sentences (a fill in the blanks sentence). If
possible, the blanks are filled, with the French
phrase associated with the right argument value.
For the analysis we are also pursuing two tracks.
When the generation module will be available
we are thinking of reversing the process to build
an analysis module (an analyzer realized for C-
STAR II is described in [Blanchon, H. and
Boitet, C., 2000, Boitet, C. and Guilbaud, J.-P.,
2000]). We are also using a pattern-based
approach that is very convenient to deal with the
output that may be produced by the speech
recognition module. The operation of the
analyzer can be viewed as "phrase spotting"
among the input so that insertion, deletion, and
wrong agreements and word can be dealt with.
The source code is written in Tcl with an
intensive use of regular expressions matching.
The next section is dedicated to this module.
2 Overall process
The French to IF process is divided in four main
steps. The input text is first split into semantic
dialogue units (SDUs2). The topic of each SDU
is then searched out. According to the topic, the
possible arguments are then instantiated. Finally,
the Dialogue Act is built using the instantiated
arguments and some other features of the SDU.
2.1 Turns splitting into SDU
To split the output of the ASR we are using
boundaries: simple phrases and articulations.
                                                      
2 An SDU is a part of an utterance that can be
analyzed into an unique IF.
Simple phrases give speech acts without
continuation. They are classified into:
- affirmations (e.g. oui c'est ?a, bien s?r)3,
- negations (e.g. non pas du tout, non pas tr?s
bien, non)4,
- acknowledgments (e.g. c'est d'accord, c'est
ok, c'est bien, ok, oui)5,
- apologies (e.g. excusez-moi, d?sol?, pas de
quoi)6,
- exclamations (e.g. c'est excellent, tr?s bien,
oh)7,
- greetings (e.g. bonjour, au revoir, bonne
journ?e)8,
- dialogue management (e.g. all?, j'entends,
j'?coute)9,
- thanks (e.g. merci bien, merci)10, and some
others.
Articulations are realizations of the rhetorical
arguments (e.g. simple conjunctions,
conjunction phrases) followed by a pronoun or
the beginning of a question (e.g. et donc je, et
puis il, et j', donc on, est-ce que, quel est)11.
Some thirty regular expressions are used for the
splitting. Examples are given in annex 1.
2.2 Topic detection
Here, the goal is to find either the terminal
speech act for the SDU (e.g. apologize,
contradict, exclamation, greeting) or
what is the SDU talking about (e.g.
a c c o m m o d a t i o n , r o o m , a c t i v i t y,
attraction, price).
A list of expressions and/or words is associated
with the terminal speech acts (e.g. bonjour,
salut, ? bient?t, bonsoir , au revoir,
enchant?, ? plus tard, ? plus, bonne
journ?e for the greeting)12 and with the
                                                      
3 yes that it, of course
4 no not at all, no not very well, no
5 it's ok, it's ok, that's right, ok, yes
6  excuse me, sorry, you are welcome
7 that's excellent, very good, oh
8 hello, good bye, have a good day
9 hello, I can hear, I am listening
10 thank you very much, thank you
11 and thus I, and then I, and I, so we, is ?, what is
12 hello, hi, see you soon, good evening, good bye,
delighted, see you later, see you later (casual),
have a good day
other topics (e.g. place de camping, salle
de conf?rence, chambre double, chambre
simple, suite, ? for room)13.
The instantiated topic is the first matched one in
the utterance. If there is no match, the topic is
set to ?unknown?. The latter concerns fragments
with no explicit topic given (e.g. an utterance
containing only "1 3 5 7") or topics not handled
yet. There are currently 30 topics defined.
2.3 Arguments filling
A Topic2If function is then in charge of finding
the instantiated arguments among the possible
ones for the given speech act and/or topic. For
example, for the room  topic some of the
possible arguments are room-spec, location,
and duration.
An argument filling function (Argument2If) is
associated with each defined argument. Those
functions are in charge of finding a possible
realization for its argument in the input. It takes
into account the value, the relations and
attributes. For example for a room-spec the
RoomSpec2If function (given in annex 2) is
trying to locate an identifiability, a
quantity and a value (the type of room). This
is done by trying to match a sequence made of
an identifier (e.g. une, un, des, plusieurs)14, a
number and a room type. The smallest
acceptable sequence being a room-type. If
found, the French room type is translated into an
IF room type through the RoomSpec2If
function defined in the fifservdico space
name. The result is the IF-encoded value of the
argument or an empty string.
Up to now we are covering one fifth of the top-
level arguments (32 over 150, the most common
ones) given that the 21 rhetorical arguments and
8 out of the 9 attitude arguments are not handled
yet. Considering the total number of arguments
one fourth of them (73 over 281) is handled,
knowing that among the 281 arguments, there is
35 synonymous definitions.
2.4 Dialogue act construction
The dialogue act is built concatenating the
speech act,  the attitudes (currently
                                                      
13 camping lot, conference room, twin room, single
room, suite
14 a (feminine), a (masculine), some, several
disposition only), the main predication and
the predication participants.
The speech act is calculated using information
about ? the verbal construction (affirmation,
question, and rejection), ? the potential negation
of the predicate, ? the potential verification or
request for verification of the predicate.
verify- request-verification-
negate- negate-
affirmation
accept
give-information
introduce-topic
offer
resume-topic
suggest
suggest-action
question
request-accept
request-action
request-information
request-reject
request-suggestion
rejection
reject
optional path
If present, the attitude is recognized with cue
phrases. The Disposition2If function is
given in annex 3.
Then, according to the matched arguments, the
main predication and predication participant are
calculated.
Finally the IF is built by concatenating the
speaker (a: or c:), the dialogue act and the
arguments values.
3 Evaluation
The analyzer has not been evaluated on its own
yet (grading the IFs produced). We have
performed a set of end-to-end evaluations on the
translation chain (analysis and generation) that
give some information on the performances of
the analyzer itself. The generator, being
developed in parallel with the analyzer, covers
the IFs produced by the analyzer.
3.1 Evaluation suites
We performed both mono-lingual evaluation, as
well as cross-lingual evaluation We evaluated on
both textual manually transcribed input as well
as on input from actual speech-recognition of the
original audio.
We graded the word accuracy rate obtained by
the recognition engine. We also graded the
speech recognized output as a "paraphrase" of
the transcriptions, to measure the semantic loss
of information due to recognition errors.
In the following we will give the results for
mono-lingual French-to-French evaluation. The
whole set of results is given in [Lavie, A. & al.
2002]
3.2 Data and Grading
3.2.1 Data set
The French data set was made of four dialogs
extracted from the NESPOLE database [Burger
& al. 2000]. Two of them were related to a client
/ agent discussion for organizing winter holidays
in Val di Fiemme in Italy; the two others were
related to summer vacations in the same region.
Speech signals were then re-recorded from client
turn transcriptions of these 4 dialogs (8kHz
sampling rate). This data represents 235 signals
related to 235 speaker turns of two different
speakers (1 male, 1 female). Finally, these 235
speaker turns were segmented manually into 427
SDUs for translation evaluation. These turns
were also segmented automatically into 407
SDUs by the SDU segmentation step of the
analyzer. We had thus 2 sets of SDUs.
3.2.2 Grading
All graders then used these segmentations in
order to assign scores for each SDU present in
the utterance. We followed the three-point
grading scheme previously developed for the C-
STAR consortium, as described in [Levin, L. &
al. 2000]. Each SDU is graded as either Perfect
(meaning translated correctly and output is
fluent), OK (meaning is translated reasonably
correct but output may be disfluent), or Bad
(meaning not properly translated).  We calculate
the percent of SDUs that are graded with each of
the above categories. Pe r f e c t  and O K
percentages are also summed together into a
category of Acceptable translations. Average
percentages are calculated for each dialogue,
each grader, and separately for client and agent
utterances. We then calculated combined
averages for all graders and for all dialogues for
each language pair.
3.3 Results
In the following tables the result are given for
acceptable paraphrase (for the ASR) and
acceptable translation (for monolingual and
cross lingual translation).
3.3.1 Results on automatic SDUs
Monolingual Translation
Language Transcribed Speech Rec.
French-to-French 62% 48%
Table 1: French Monolingual End-to-End Translation
Results (Percent Acceptable) on Transcribed and
Speech Recognized Input on Analyzer's SDUs
3.3.2 Results on manual SDUs
Speech Recognition
Language WARs Acceptable Paraphrase
French 71.2% 65.0%
Table 2: Speech Recognition Word Accuracy Rates
and Results of Human Grading (Percent Acceptable)
of recognition Output as a Paraphrase
Monolingual Translation
Language Transcribed Speech Rec.
French-to-French 54% 41%
Table 3: Monolingual End-to-End Translation
Results (Percent Acceptable) on Transcribed and
Speech Recognized Input
3.4 Comments
On Speech Recognition
About 65% of the SDUs were judged correctly
paraphrased. This score is more informative than
the WAR% since it means that 35% of the SDUs
will not be correctly translated. This evaluation
is also a good way to check that the graders give
more or less the same scores (it is the case here).
On French-to-French Monolingual Translation
About 54% of the SDUs were judged acceptably
translated on the transcribed data. Thus, we
know for sure that 46% of the SDUs will not be
correctly translated anyway. It is thus important
to know if this percentage includes the SDUs
badly recognized by the ASR system or not.
That is shown in the next paragraph.
About 41% of the SDUs were judged acceptably
translated on the speech recognized data. This
result alone would have been very difficult to
interpret, but the previous results show the
respective contribution of ASR and Translation
to this performance. It is an important
information that will be used to further improve
the system.
On the Results on Automatic SDUs
The results we are producing here are isolated as
the same experiment has not been done for the
other languages. The results for French-to-
French are better, +7%. The number of
automatically produced SDUs (407) is lower
than the number of manually produced SDUs
(427). A careful study of the results shows that
the perfect scores are quite the same, but the
number of OK  scores is higher with the
automatically produced SDUs. However, the
data have to be checked carefully to give a
grounded conclusion.
This phenomenon, if it is confirmed by our
partners, may explain part of the fact that user
studies and system demonstrations indicate that
while the current level of translation accuracy
cannot be considered impressive, it is already
sufficient for achieving effective communication
with real users.
Conclusion
In this paper we have described our first
evaluated prototype of a pattern-based analyzer
from spoken French into IF.
We have tried to show that this approach seems
very promising. The "phrase spotting"
mechanism we implemented handle quite well
the output of a speech recognizer whose
language models allows for the construction of
isolated correct segments.
Having a unique construction method for each
argument and a common DA construction
pattern shared between the topics allows for
reusability and easy updating of the code to
follow the regular IF specification changes.
The first results we have shown are encouraging
and pave the way for better results in the next
NESPOLE! evaluation studies.
Acknowledgements
Our thanks go to the graders (L. Besacier, D.
Vaufreydaz, S. Mazenot, S. Rossato, A.C.
Descalle), and to the CLIPS and IRST teams for
their contribution to this paper.
References
Besacier, L., Blanchon, H., Fouquet, Y., Guilbaud, J.-
P., Helme, S., Mazenot, S., Moraru, D. and
Vaufreydaz, D. (2001) Speech Translation for
French in the NESPOLE! European Project. Proc.
Eurospeech. Aalborg, Denmark. September 3-7,
2001. vol. 2/4: pp. 1291-1294.
Blanchon, H. and Boitet, C. (2000) Spe e ch
Translation for French within the C-STAR II
Consortium and Future Perspectives. Proc. ICSLP
2000. Beijing, China. Oct. 16-20, 2000. vol. 4/4:
pp. 412-417.
Boitet, C. (1997) GETA's metodology and its current
development towards networking communication
and speech translation in the context of the UNL
and C-STAR projets. Proc. PACLING-97. Ome,
Japan. 2-5 September, 1997. vol. 1/1: pp. 23-57.
Boitet, C. and Guilbaud, J.-P. (2000) Analysis into a
Formal Task-Oriented Pivot without Clear
Abstract Semantics is Best Handled as "Usual"
Translation. Proc. ICSLP 2000. Beijing, China.
Oct. 16-20, 2000. vol. 4/4: pp. 436-439.
Burger, S., Besacier, L., Metze, F., Morel, C. and
Coletti, P. (2001) The NESPOLE! VoIP Dialog
Database. Proc. Eurospeech. Aalborg, Denmark.
September 3-7, 2001.
Lazzari, G. (2000) Spoken Translation: Challenges
and Opportunities. Proc. ICSLP 2000. Beijing,
China. Oct. 16-20, 2000. vol. 4/4: pp. 430-435.
Levin, L., Gates, D., Lavie, A., Pianesi, F., Wallace,
D., Watanabe, T. and Woszczyna, M. (2000)
Evaluation of a Practical Interlingua for task-
Oriented Dialogue. Proc. Workshop of the SIG-IL,
NAACL 2000. Seattle, Washington. April 30,
2000. 6 p.
Levin, L., Gates, D., Lavie, A. and Waibel, A. (1998)
An Interlingua Based on Domaine Actions for
Machine Translation of Task-Oriented Dialogues.
Proc. ICSLP'98. Syndney, Australia. 30th
November - 4th December 1998. vol. 4/7:
pp. 1155-1158.
Lavie A., Metze F., Pianesi F., Burger S., Gates D.,
Levin L., Langley C., Peterson K., Schultz T.,
Waibel A., Wallace D., McDonough J., Soltau H.,
Laskowski K., Cattoni R., Lazzari G., Mana N.,
Pianta E., Costantini E., Besacier L., Blanchon H.,
Vaufreydaz D., Taddei L. (2002) Enhancing the
Usability and Performance of Nespole! ? a Real-
World Speech-to-Speech Translation System. Proc.
HLT 2002. San Diego, California (USA). March
22-27, 2002. 6p.
Zong, C., Huang, T. and Xu, B., (2000). An Improved
Template-Based Approach to Spoken Language
Translation. Proc. ICSLP 2000. Beijing, China.
Oct. 16-20, 2000. vol. 4/4: pp. 440-443.
Annexes
Annex 1: Regular expression for splitting into SDUs
In the following extract of the SplitHypo procedure, a sequence made of a simple sentence (leading to an if), a
non-empty string, another simple sentence, and a possibly empty string is searched.
If such a sequence is found, the first simple sentence is an SDU (theSDU1), the non-empty string is split into
SDUs, the second simple sentence is an SDU (theSDU2), and the possibly empty string is split into SDUs.
proc SplitHypo {inWho inString} {
  ...
  #simple sentence
  } elseif {[regexp "^ ($simplesentences) (.+?) ($simplesentences) (.*)"
                    $inString lMatch lFirst lSecond lThird lFourth]!=0} {
        append theSDU1 "{<" $inWho "> " $lFirst " }"
        append the_rest1 " " $lSecond " "
        append theSDU2 "{<" $inWho "> " $lThird " }"
        append the_rest2 " " $lFourth
        concat $theSDU1 [SplitHypo $inWho $the_rest1] $theSDU2 [SplitHypo $inWho $the_rest2]
    }
  ...
}
Annex 2: room-spec argument value construction
In the following extract of the RoomSpec2If  procedure, a sequence made of a number and a room
specification expressed in French eventually preceded by the French plural definite article les is searched.
If such sequence is found then the IF argument
room-spec=(identifiability=yes/no, quantity=quantity, room_specification) is constructed
whether the article is found or not.
proc RoomSpec2If {inString} {
  ...
  } elseif {[regexp "(?:les) (\[0\-9\]+) ($fifservdico::frenchroomspec)(?:x|s)?"
                    $inString lMatch lQuantity lRoomSpec]!=0} {
        append the_result "room-spec=(identifiability=yes, quantity=" $lQuantity ", "
                          [fifservdico::RoomSpec2If $lRoomSpec] ")"
  } elseif {[regexp "(\[0\-9\]+) ($frenchroomspec)(?:x|s)?"
                  $inString lMatch lQuantity lRoomSpec]!=0} {
        append the_result "room-spec=(identifiability=no, quantity=" $lQuantity ", "
                          [fifservdico::RoomSpec2If $lRoomSpec] ")"
  }
  ...
}
Annex 3: disposition argument value construction
In the following extract of the Disposition2If procedure, a sequence made of a French pronoun and a
disposition verb eventually surrounded by negation markers is searched.
If such sequence is found then the IF argument disposition=(who=pronoun, disposition_verb) is
constructed whether the verb is negated of not.
proc Disposition2If {inString} {
  if {[regexp "($frenchpronoun) ?(ne |n')?($frenchdispositionverb) (pas)?"
               $inString lMatch lPron lNe lVerb lPas]!=0} {
    if {$lNe!="" || $lPas!="" } {
          append the_result "disposition=(who=" [fifservdico::NormalizePronoun2If $lPron] ", "
                            [fifservdico::NegativDisposition2If $lVerb] ")"
    } else {
          append the_result "disposition=(who=" [fifservdico::NormalizePronoun2If $lPron] ", "
                            [fifservdico::PositivDisposition2If $lVerb] ")"
    }
    }
  ...
  } else {
          return ""
    }
}
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 232?240, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
GETALP: Propagation of a Lesk Measure through an Ant Colony Algorithm
Didier Schwab, Andon Tchechmedjiev, J?r?me Goulian,
Mohammad Nasiruddin, Gilles S?rasset, Herv? Blanchon
LIG-GETALP
Univ. Grenoble Alpes
http://getalp.imag.fr/WSD
firstname.lastname@imag.fr
Abstract
This article presents the GETALP system for
the participation to SemEval-2013 Task 12,
based on an adaptation of the Lesk measure
propagated through an Ant Colony Algorithm,
that yielded good results on the corpus of Se-
meval 2007 Task 7 (WordNet 2.1) as well as
the trial data for Task 12 SemEval 2013 (Ba-
belNet 1.0). We approach the parameter es-
timation to our algorithm from two perspec-
tives: edogenous estimation where we max-
imised the sum the local Lesk scores; exoge-
nous estimation where we maximised the F1
score on trial data. We proposed three runs
of out system, exogenous estimation with Ba-
belNet 1.1.1 synset id annotations, endoge-
nous estimation with BabelNet 1.1.1 synset id
annotations and endogenous estimation with
WordNet 3.1 sense keys. A bug in our imple-
mentation led to incorrect results and here, we
present an amended version thereof. Our sys-
tem arrived third on this task and a more fine
grained analysis of our results reveals that the
algorithms performs best on general domain
texts with as little named entities as possible.
The presence of many named entities leads the
performance of the system to plummet greatly.
1 Introduction
Out team is mainly interested in Word Sense Disam-
biguation (WSD) based on semantic similarity mea-
sures. This approach to WSD is based on a local
algorithm and a global algorithm. The local algo-
rithm corresponds to a semantic similarity measure
(for example (Wu and Palmer, 1994), (Resnik, 1995)
or (Lesk, 1986)), while the global algorithm propa-
gates the values resulting from these measures at the
level of a text, in order to disambiguate the words
that compose it. For two years, now, our team has
focussed on researching global algorithms. The lo-
cal algorithm we use, a variant of the Lesk algo-
rithm that we have evaluated with several global al-
gorithms (Simulated Annealing (SA), Genetic Al-
gorithms (GA) and Ant Colony Algorithms (ACA))
(Schwab et al, 2012; Schwab et al, 2013), has
shown its robustness with WordNet 3.0. For the
present campaign, we chose to work with an ant
colony based global algorithms that has proven its
efficiency (Schwab et al, 2012; Tchechmedjiev et
al., 2012).
Presently, for this SemEval 2013 Task 12 (Nav-
igli et al, 2013), the objective is to disambiguate a
set of target words (nouns) in a corpus of 13 texts
in 5 Languages (English, French, German, Italian,
Spanish) by providing, for each sense the appropri-
ate sense labels. The evaluation of the answers is
performed by comparing them to a gold standard
annotation of the corpus in all 5 languages using
three possible sense inventories and thus sense tags:
BabelNet 1.1.1 Synset ids (Navigli and Pozetto,
2012), Wikipedia page names and Wordnet sense
keys (Miller, 1995).
Our ant colony algorithm is a stochastic algorithm
that has several parameters that need to be selected
and tuned. Choosing the values of the parameters
based on linguistic criteria remains an open and dif-
ficult problem, which is why we wanted to autom-
atize the parameter search process. There are two
ways to go about this process: exogenous estima-
232
tion, when the parameter values are selected so as
to maximise the F-score on a small training anno-
tated corpus and then used to disambiguate another
corpus (weakly supervised); endogenous estimation,
when the parameters are chosen so as to maximise
the global similarity score on a text or corpus (unsu-
pervised). Our first experiment and system run con-
sists in tuning the parameters on the trial corpus of
the campaign and running the system with the Ba-
belNet sense inventory. Our second and third exper-
iments consist in endogenous parameter estimation,
the first using BabelNet as a sense inventory and the
second using WordNet. Unfortunately, the presence
of an implementation issue prevented us from ob-
taining scores up to par with the potential of our sys-
tem and thus we will present indicative results of the
performance of the system after the implementation
issue was fixed.
2 The GETALP System: Propagation of a
Lesk Measure through an Ant Colony
Algorithm
In this section we will first describe the local al-
gorithm we used, followed by a quick overview of
global algorithms and our own Ant Colony Algo-
rithm.
2.1 The Local Algorithm: a Lesk Measure
Our local algorithm is a variant of the Lesk Algo-
rithm (Lesk, 1986). Proposed more than 25 years
ago, it is simple, only requires a dictionary and no
training. The score given to a sense pair is the num-
ber of common words (space separated strings) in
the definition of the senses, without taking into ac-
count neither the word order in the definitions (bag-
of-words approach), nor any syntactic or morpho-
logical information. Variants of this algorithm are
still today among the best on English-language texts
(Ponzetto and Navigli, 2010).
Our local algorithm exploits the links provided by
WordNet: it considers not only the definition of a
sense but also the definitions of the linked senses
(using all the semantic relations for WordNet, most
of them for BabelNet) following (Banerjee and Ped-
ersen, 2002), henceforth referred asExtLesk1 Con-
1All dictionaries and Java implementations of all algorithms
of our team can be found on our WSD page
trarily to Banerjee, however, we do not consider
the sum of squared sub-string overlaps, but merely
a bag-of-words overlap that allows us to generate
a dictionary from WordNet, where each word con-
tained in any of the word sense definitions is indexed
by a unique integer and where each resulting defini-
tion is sorted. Thus we are able to lower the compu-
tational complexity fromO(mn) toO(m), wherem
and n are the respective length of two definitions and
m ? n. For example for the definition: "Some kind
of evergreen tree", if we say that Some is indexed by
123, kind by 14, evergreen by 34, and tree by 90,
then the indexed representation is {14, 34, 90, 123}.
2.2 Global Algorithm : Ant Colony Algorithm
We will first review the principles pertaining to
global algorithms and then a more detailed account
of our Ant Colony algorithm.
2.2.1 Global algorithms, Global scores and
Configurations
A global algorithm is a method that allows to
propagate a local measure to a whole text in or-
der to assign a sense label to each word. In the
similarity-based WSD perspective, the algorithms
require some fitness measure to evaluate how good
a configuration is. With this in mind, the score
of the selected sense of a word can be expressed
as the sum of the local scores between that sense
and the selected senses of all the other words of a
context. Hence, in order to obtain a fitness value
(global score) for the whole configuration, it is
possible to simply sum the scores for all selected
senses of the words of the context: Score(C) =
?m
i=1
?m
j=iExtLesk(wi,C[i], wj,C[j]).
For a given text, the chosen configuration is
the one which maximizes the global score among
the evaluated ones. The simplest approach is the
exhaustive evaluation of sense combinations (BF),
used for example in (Banerjee and Pedersen, 2002),
that assigns a score to each word sense combination
in a given context (window or whole text) and se-
lects the one with the highest score. The main is-
sue with this approach is that it leads to a combi-
http://getalp.imag.fr/WSD and more specifically for
SemEval 2013 Task 12 on the following page
http://getalp.imag.fr/static/wsd/
GETALP-WSD-ACA/
233
natorial explosion in the length of the context win-
dow or text. The number of combinations is indeed
?|T |
i=1(|s(wi)|), where s(wi) is the set of possible
senses of word i of a text T . For this reason it is
very difficult to use the BF approach on an analy-
sis window larger than a few words. In our work,
we consider the whole text as context. In this per-
spective, we studied several methods to overcome
the combinatorial explosion problem.
2.2.2 Complete and Incomplete Approaches
Several approximation methods can be used in or-
der to overcome the combinatorial explosion issue.
On the one hand, complete approaches try to reduce
dimensionality using pruning techniques and sense
selection heuristics. Some examples include: (Hirst
and St-Onge, 1998), based on lexical chains that re-
strict the possible sense combinations by imposing
constraints on the succession of relations in a taxon-
omy (e.g. WordNet); or (Gelbukh et al, 2005) that
review general pruning techniques for Lesk-based
algorithms; or yet (Brody and Lapata, 2008) who
exploit distributional similarity measures extracted
from corpora (information content).
On the other hand, incomplete approaches gen-
erally use stochastic sampling techniques to reach a
local maximum by exploring as little as necessary
of the search space. Our present work focuses on
such approaches. Furthermore, we can distinguish
two possible variants:
? local neighbourhood-based approaches (new
configurations are created from existing con-
figurations) among which are some approaches
from artificial intelligence such as genetic al-
gorithms or optimization methods such as sim-
ulated annealing;
? constructive approaches (new configurations
are generated by iteratively adding new ele-
ments of solutions to the configuration under
construction), among which are for example
ant colony algorithms.
2.2.3 Principle of our Ant Colony Algorithm
In this section, we briefly describe out Ant Colony
Algorithm so as to give a general idea of how it op-
erates. However, readers are strongly encouraged
to read the detailed papers (Schwab et al, 2012;
Schwab et al, 2013) for a more detailed description
of the system, including examples of how the graph
is built, of how the algorithm operates step by step
as well all pseudo code listing.
Ant colony algorithms (ACA) are inspired from
nature through observations of ant social behavior.
Indeed, these insects have the ability to collectively
find the shortest path between their nest and a source
of food (energy). It has been demonstrated that
cooperation inside an ant colony is self-organised
and allows the colony to solve complex problems.
The environment is usually represented by a graph,
in which virtual ants exploit pheromone trails de-
posited by others, or pseudo-randomly explore the
graph. ACAs are a good alternative for the resolu-
tion of optimization problems that can be encoded
as graphs and allow for a fast and efficient explo-
ration on par with other search heuristics. The main
advantage of ACAs lies in their high adaptivity to
dynamically changing environments. Readers can
refer to (Dorigo and St?tzle, 2004) or (Monmarch?,
2010) for a state of the art.
In this article we use a simple hierarchical graph
(text, sentence, word) that matches the structure of
the text and that exploits no external linguistic infor-
mation. In this graph we distinguish two types of
nodes: nests and plain nodes. Following (Schwab et
al., 2012), each possible word sense is associated to
a nest. Nests produce ants that move in the graph in
order to find energy and bring it back to their mother
nest: the more energy is brought back by ants, the
more ants can be produced by the nest in turn. Ants
carry an odour (a vector) that contains the words of
the definition of the sense of its mother nest. From
the point of view of an ant, a node can be: (1) its
mother nest, where it was born; (2) an enemy nest
that corresponds to another sense of the same word;
(3) a potential friend nest: any other nest; (4) a plain
node: any node that is not a nest. Furthermore, to
each plain node is also associated an odour vector of
a fixed length that is initially empty.
Ant movement is function of the scores given by
the local algorithm, of the presence of energy, of the
passage of other ants (when passing on an edge ants
leave a pheromone trail that evaporates over time)
and of the nodes? odour vectors (ants deposit a part
of their odour on the nodes they go through). When
an ant arrives onto the nest of another word (that cor-
responds to a sense thereof), it can either continue its
234
exploration or, depending on the score between this
nest and its mother nest, decide to build a bridge be-
tween them and to follow it home. Bridges behave
like normal edges except that if at any given time the
concentration of pheromone reaches 0, the bridge
collapses. Depending on the lexical information
present and the structure of the graph, ants will fa-
vor following bridges between more closely related
senses. Thus, the more closely related the senses of
the nests are, the more bridges between them will
contribute to their mutual reinforcement and to the
sharing of resources between them (thus forming
meta-nests); while the bridges between more dis-
tant senses will tend to fade away. We are thus able
to build interpretative paths (possible interpretations
of the text) through emergent behaviour and to sup-
press the need to use a complete graph that includes
all the links between the senses from the start (as is
usually the case with classical graph-based optimi-
sation approaches).
Through the emergence of interpretative paths,
sense pairs that are closer semantically benefit from
an increased ant traffic and thus tend to capture most
of the energy of the system at a faster pace, thus
favouring a faster convergence over an algorithm
that uses a local neighbourhood graph (nodes are
senses interconnected so as to represent all sense
combinations in a context window) without sacrific-
ing the quality of the results.
The selected answers correspond, for each word
to the nest node with the highest energy value. The
reason for this choice over using the pheromone con-
centration is that empirically, the energy level bet-
ter correlates with the actual F1 scores. In turn, the
global Lesk score of a selected sense combination
correlates even better with the F1 score, which is
why, we keep the sense combinations resulting from
each iteration of the algorithm (highest energy nests
at each iteration) and select the one with the highest
global Lesk score as the final solution.
2.3 Parameters
This version of our ant algorithm has seven param-
eters (?, Ea, Emax, E0, ?v, ?, LV ) which have an
influence on the emergent phenomena in the system:
? The maximum amount of energy an ant can
carry, Emax and Ea the amount of energy an
ant can take on a node, influences how much
an ant explores the environment. Ants cannot
go back through an edge they just crossed and
have to make circuits to come back to their nest
(if the ant does not die before that). The size
of the circuits depend on the moment the ants
switch to return mode, hence on Emax.
? The evaporation rate of the pheromone between
cycles (?) is one of the memories of the sys-
tem. The higher the rate is, the least the trails
from previous ants are given importance and
the faster interpretative paths have to be con-
firmed (passed on) by new ants in order not to
be forgotten by the system.
? The initial amount of energy per node (E0)
and the ant life-span (?) influence the number
of ants that can be produced and therefore the
probability of reinforcing less likely paths.
? The odour vector length (Lv) and the propor-
tion of odour components deposited by an ant
on a plain node (?V ) are two dependent param-
eters that influence the global system memory.
The higher the length of the vector, the longer
the memory of the passage of an ant is kept. On
the other hand, the proportion of odour compo-
nents deposited has the opposite effect.
Given the lack of an analytical way of determin-
ing the optimal parameters of the ant colony al-
gorithm, they have to be estimated experimentally,
which is detailed in the following section.
3 Acquisition of Parameter Values
The algorithms we are interested in have a certain
number of parameters that need tuning in order to
obtain the best possible score on the evaluation cor-
pus. There are three possible approaches:
? Make an educated guess about the value ranges
based on a priori knowledge about the dynam-
ics of the algorithm;
? Test manually (or semi-manually) several com-
binations of parameters that appear promising
and determine the influence of making small
adjustments to the values ;
? Use a learning algorithm to automate acquisi-
tion of parameters values. We present that ap-
proach in the following part.
235
3.1 Automated Parameter Estimation
Two methods can be used to automatically acquire
parameters. The first one consists in maximizing
the F-score on an sense-annotated corpus (weak ap-
proach) while the second one consist in maximizing
the global Lesk score (unsupervised approach).
3.1.1 Generalities
Both approaches are based on the same principle
(Tchechmedjiev et al, 2012). We use a simulated
annealing algorithm (Laarhoven and Aarts, 1987)
combined with a non-parametric statistical (Mann-
Whitney-U test (Mann and Whitney, 1947)) test with
a p-value adapted for multiple comparisons through
False Discovery Rate control (FDR) (Benjamini and
Hochberg, 1995). The estimation algorithm oper-
ates on all the parameters of the ant colony algo-
rithm described above and attempts to maximise the
objective function (Global score, F1). The reason
why we need to use a statistical test and FDR rather
than using the standard SA algorithm, is that the
Ant Colony Algorithm is stochastic in nature and
requires tuning to be performed over the distribu-
tion of possible answers for a given set of param-
eter values. Indeed, there is no guarantee that the
value resulting from one execution is representative
at all of the distribution. The exact nature of the dis-
tribution of answers is unknown and thus we take
a sampling of the distribution as precise as can be
afforded. Thus, we require the statistical test to as-
certain the significance between the scores for two
parameter configurations.
3.1.2 Exogenous parameter tuning
If we have a sense-annotated corpus at our dis-
posal, it is possible to directly use the F1 value ob-
tained by the system on this reference to tune the
parameters of the systems so as to maximise said F1
score. The main issues that arise from such meth-
ods are the fact that gold standards are expensive to
produce and that there is no guarantee on the gen-
erality of the contents of the gold standard. Thus,
in languages with little resources we may be un-
able to obtain a gold standard and in the case one
is available, there is a potentially strong risk of over
fitting. Furthermore due to the nature of the train-
ing, taking training samples in a random order for
cross-validation becomes tricky. This is why we also
want to test another method that can tune the pa-
rameters without using labelled examples. For the
evaluation, we estimated parameters on the F1 score
on the test corpus for English and French (the only
ones available). We used the parameters estimated
for English for our English results for our first sys-
tem run GETALP-BN1 and the French parameters
for the results on French, German, Italian, Spanish.
For English we found: ? = 26, Ea =
14, Emax = 3, E0 = 34, ?v = 0.9775, ? =
0.3577, LV = 25.
For French: ? = 19, Ea = 9, Emax = 3, E0 =
32, ?v = 0.9775, ? = 0.3577, LV = 25.
3.1.3 Endogenous parameter tuning
In the context of the evaluation campaign, the ab-
sence of an example gold standard on the same ver-
sion of the resource (synset id mismatch between
BabelNet 1.0 and 1.1.1 2) made dubious the prospect
of using parameters estimated from a gold standard.
Consequently, we set out to investigate the relation
between the F1 score of the gold standard and the
Global Lesk Score of successive solutions through-
out the execution of the algorithm.
We observed that the Lesk score is highly corre-
lated to the F1 score and can be used as an estimator
thereof. The main quality criterion being the dis-
criminativeness of the Lesk score compared to the
F1 score (average ratio between the number of pos-
sible F1 score values for a single Lesk score value),
for which the correlation is a possible indicator. We
make the hypothesis based on the correlation that for
a given specific local measure, the global score will
be an adequate estimator of the F1 score. Our sec-
ond system run GETALP-WSD-BN2 is based on the
endogenous parameter estimation. We will not list
all the parameters here, as there is a different set of
parameters for each text and each language.
3.2 Voting
In previous experiment, as can be expected, we have
observed a consistent rise the F1 score when apply-
ing a majority vote method on the output of several
executions (Schwab et al, 2012). Consequently we
followed the same process here, and for all the runs
of our system we performed 100 executions and ap-
plied a majority vote (For each word, our of all se-
2http://lcl.uniroma1.it/babelnet/
236
lected senses, take the one that has been selected the
most over all the executions) on all 100 answer files.
The result of this process is a single answer file and
comes with the advantage of greatly reducing the
variability of the answers. Say this voting process
is repeated over and over again 100 times, then the
standard deviation of F1 scores around the mean is
much smaller. Thus, we also have a good solutions
to the problem of selecting the answer that yields the
highest score, without actually having access to the
gold standard.
4 Runs for SemEval 2013 task 12
In this section we will describe the various runs we
performed in the context of Task 12. We will first
present our methodologies relating to the BabelNet
tagged gold standard followed by the methodologies
relating to the WordNet tagged gold standard.
4.1 BabelNet Gold Standard Evaluation
In the context of the BabelNet gold standard evalu-
ation, we need to tag the words of the corpus with
BabelNet synset ids. Due to the slow speed of re-
trieving Babel synsets and extracting glosses, espe-
cially in the context of our extended Lesk Approach,
we pre-generate a dictionary for each language that
contains entries for each word of the corpus and then
for each possible sense (as per BabelNet). In the
short time allotted for the competition, we restrict
ourselves to building dictionaries only for the words
of the corpus, but the process described can be ap-
plied to pre-generate a dictionary for the whole of
BabelNet.
Each BabelNet synset for a word is considered as
a possible sense in the dictionary. For each synset
we retrieve the Babel senses and retain the ones that
are in the appropriate language. Then, we retrieve
the Glosses corresponding to each selected sense
and combine them in as the definition correspond-
ing to that particular BabelNet synset. Furthermore,
we also retrieve certain of the related synsets and
repeat the same process so as to add the related def-
initions to the BabelNet synset being considered. In
our experiments on the test corpus, we determined
that what worked best (i.e. English and French)
was to use only relations coming from WordNet, all
the while excluding the r, gdis, gmono relation
added by BabelNet. We observed a similar increase
in disambiguation quality with the Degree (Navigli
and Lapata, 2010) algorithm implementation that
comes with BabelNet. The r relation correspond to
the relations in BabelNet extracted from Wikipedia,
whereas gdis and gmono corresponds to relation
created using a disambiguation algorithm (respec-
tively for monosemous and polysemous words).
4.2 WordNet Gold Standard Evaluation
In the context of the WordNet gold standard evalua-
tion, we initially thought the purpose would be to an-
notate the corpus in all five languages with WordNet
sense keys through alignments extracted from Ba-
belNet. As a consequence, we exploited BabelNet
as a resource, merely obtaining WordNet sense keys
through the main senses expressed in BabelNet, that
correspond to WordNet synsets. Although we were
able to produce annotations for all languages, as it
turns out, the WordNet evaluation was merely aimed
at evaluating monolingual systems that do not sup-
port BabelNet at all. For reference, we subsequently
generated a dictionary from WordNet only, to gauge
the performance of our system on the evaluation as
intended by the organisers.
5 Results
We will first present the general results pertaining
to Task 12, followed by a more detailed analysis on
a text by text basis, as well as the comparison with
results obtained on the Semeval 2007 WSD task in
terms of specific parts of speech.
5.1 General Results for Semeval-2013 Task 12
Important: implementation issue during the
evaluation period During the evaluation period,
we had an implementation issue, where a parameter
that limited the size of definition was not disabled
properly. As a consequence, when we experimented
to determine the appropriate relations to consider
for the context expansion of the glosses, we arrived
at the experimental conclusion that using all rela-
tions worked best. However, since it was already the
case with WordNet (Schwab et al, 2011), we read-
ily accepted that our experimental conclusion was
indeed correct. The issue was indirectly resolved
as an unforeseen side effect of another hot-fix ap-
plied shortly before the start of the evaluation period.
237
Given that we were not aware of the presence of a
limitation on the definition length before the hot-fix,
we performed all the experiments under an incorrect
hypothesis which led us to an incorrect conclusion,
that itself led to the results we obtained for the cam-
paign. Indeed, with no restrictions on the size of the
definition, our official results for this task were con-
sistently inferior to the random baseline across the
board. After a thorough analysis of our runs we ob-
served that the sum of local measures (global lesk
score) that correlated inversely with the gold stan-
dard F1 score, the opposite of what it should have
been. We immediately located and corrected this
bug when we realized what had caused these bad
results that did not correspond at all with what we
obtained on the test corpus. After the fix, we strictly
ran the same experiment without exploiting the gold
standard, so as to obtain the results we would have
obtained had the bug not been present in the first
place.
Run Lang. P R F1 MFS
BN1 EN 58.3 58.3 58.3 65.6
FR 48.3 48.2 48.3 50.1
DE 52.3 52.3 52.3 68.6
ES 57.6 57.6 57.6 64.4
IT 52.6 52.5 52.6 57.2
BN2 EN 56.8 56.8 56.8 65.6
FR 48.3 48.2 48.3 50.1
DE 51.9 51.9 51.9 68.6
ES 57.8 57.8 57.8 64.4
IT 52.8 52.8 52.8 57.2
WN1 EN 51.4 51.4 51.4 63.0
Table 1: Results after fixing the implementation is-
sue for all three of our runs, compared to the Most
Frequent Sense baseline (MFS).
We can see in Table 1, that after the removal of
the implementation issues, the scores become more
competitive and meaningful compared to the other
system, although we remain third of the evalua-
tion campaign. We can observe that there is no
large difference between the exogenous results (us-
ing a small annotated corpus) and endogenous re-
sults. Except for the English corpus where there is
a 2% increase. The endogenous estimation, since it
is performed on a text by text basis is much slower
and resource consuming. Given that the exogenous
estimation offers slightly better results and that it re-
quires very little annotated data, we can conclude
that in most cases the exogenous estimation will be
much faster to obtain.
5.2 A more detailed analysis
In this section we will first make a more detailed
analysis for each text on the English corpus, by look-
ing where our algorithm performed best. We restrict
ourselves on one language for this analysis for the
sake of brevity. As we can see in Table 2, the re-
sults can vary greatly depending on the text (within
a twofold range). The system consistently performs
better on texts from the general domain (T 4, 6, 10),
often beating the first sense baseline. For more spe-
cialized texts, however, (T 2, 7, 8, 11, 12, 13) the
algorithm performs notably lower than the baseline.
The one instance where the algorithm truly fails, is
when the text in question contains many ambigu-
ous entities. Indeed for text 7, which is about foot-
ball, many of the instance words to disambiguate are
the names of players and of clubs. Intuitively, this
behaviour is understandable and can be mainly at-
tributed to the local Lesk algorithm. Since we use
glosses from the resource, that mostly remain in the
general domain, a better performance in matching
texts is likely. As for named entities, the Lesk algo-
rithm is mainly meant to capture the similarity be-
tween concepts and it is much more difficult to dif-
ferentiate two football players from a definition over
concepts (often more general).
To further outline the strength of our approach, we
need to look back further at a setting with all parts
of speech being considered, namely Task 7 from Se-
mEval 2007. As can be seen in Table 3, even though
for adjectives and adverbs the system is slightly be-
low the MFS (respectively), it has a good perfor-
mance compared to graph based WSD approaches
that would be hindered by the lack of taxonomical
relations. For verbs the performance is lower as is
consistently observed with WSD algorithms due to
the high degree of polysemy of verbs. For example,
in the case of Degree (Navigli and Pozetto, 2012),
nouns are the part of speech for which the system
performs the best, while the scores for other parts of
speech are somewhat lower. Thus, we can hypoth-
238
Text Descr. Len. F1 MFS Diff.
1 Gen. Env. 228 61.4 68.9 -7.5
2 T. Polit. 84 51.2 66.7 -15.5
3 T. Econ. 84 52.4 56.0 - 3.6
4 News. Gen. 119 58.8 58.0 0.8
5 T. Econ. 74 39.2 36.5 2.7
6 Web Gen. 210 67.1 64.3 2.8
7 T. Sport. 190 34.2 60.5 -26.3
8 Sci. 153 63.4 67.3 -3.9
9 Geo. Econ. 190 63.2 74.2 -11
10 Gen. Law. 160 61.9 61.9 0
11 T. Sport. 125 56.8 64.0 -7.2
12 T. Polit. 185 64.3 73.0 -8.7
13 T. Econ. 130 68.5 72.6 -4.1
Table 2: Text by text F1 scores compared to the
MFS baseline for the English corpus (T.= Trans-
lated, Gen.= General, Env.= Environment, Polit.=
Politics, Econ.= Economics, Web= Internet, Sport.=
Sports, Geo.= Geopolitics, Sci.= Science).
A P.O.S. F1 MFS F1 Diff
1108 Noun 79.42 77.4 +1.99
591 Verb 74.78 75.3 -0.51
362 Adj. 82.66 84.3 -1.59
208 Adv. 86.95 87.5 -0.55
2269 All 79.42 78.9 +0.53
Table 3: Detailed breakdown of F1 score per part
of speech category for Semeval-2007 Task 7, over
results resulting from a vote over 100 executions
esise that using a different local measure depending
on the part of speech may constitute an interesting
development while allowing a return to a more gen-
eral all-words WSD task where all parts of speech
are considered, even when the resource does not of-
fer taxonomical relation for the said parts of speech.
6 Conclusions & Perspectives
In this paper, we present a method based on a
Lesk inspired local algorithm and a global algorithm
based on ant colony optimisation. An endogenous
version (parameter estimation based on the maximi-
sation of the F-score on an annotated corpus) and
an exogenous version (parameter estimation based
on the maximisation of the global Lesk score on
the corpus) of the latter algorithm do not exhibit a
significant difference in terms of the F-score of the
result. After a more detailed analysis on a text by
text basis, we found that the algorithm performs best
on general domain texts with as little named enti-
ties as possible (around or above the MFS baseline).
For texts of more specialized domain the algorithm
consistently performs below the MFS baseline, and
for texts with many named entities, the performance
plummets greatly slightly above the level of a ran-
dom selection. We also show that with our Lesk
measure the system is best suited for WSD in a more
general setting with all parts of speech, however in
the context of just nouns, it is not the most suitable
local measure. As we have seen from the other sys-
tems, graph based local measures may be the appro-
priate answer to reach the level of the best systems
on this task, however it is important not to dismiss
the potential of other approaches. The quality of the
results depend on the global algorithm, however they
are also strongly bounded by the local measure con-
sidered. Our team, is headed towards investigating
local semantic similarity measures and towards ex-
ploiting multilingual features so as to improve the
disambiguation quality.
7 Acknowledgements
The work presented in this paper was conducted in
the context of the Formicae project funded by the
University Grenoble 2 (Universit? Pierre Mend?s
France) and the Videosense project, funded by the
French National Research Agency (ANR) under
its CONTINT 2009 programme (grant ANR-09-
CORD-026).
References
[Banerjee and Pedersen2002] Satanjee Banerjee and Ted
Pedersen. 2002. An adapted lesk algorithm for word
sense disambiguation using wordnet. In CICLing
2002, Mexico City, February.
[Benjamini and Hochberg1995] Yoav Benjamini and
Yosef Hochberg. 1995. Controlling the False Dis-
covery Rate: A Practical and Powerful Approach to
Multiple Testing. Journal of the Royal Statistical
Society. Series B (Methodological), 57(1):289?300.
[Brody and Lapata2008] Samuel Brody and Mirella La-
pata. 2008. Good neighbors make good senses:
Exploiting distributional similarity for unsupervised
239
WSD. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 65?72, Manchester, UK.
[Dorigo and St?tzle2004] Dorigo and St?tzle. 2004. Ant
Colony Optimization. MIT-Press.
[Gelbukh et al2005] Alexander Gelbukh, Grigori
Sidorov, and Sang-Yong Han. 2005. On some opti-
mization heuristics for Lesk-like WSD algorithms. In
International Conference on Applications of Natural
Language to Information Systems ? NLDB?05, pages
402?405, Alicante, Spain.
[Hirst and St-Onge1998] G. Hirst and David D. St-Onge.
1998. Lexical chains as representations of context for
the detection and correction of malapropisms. Word-
Net: An electronic Lexical Database. C. Fellbaum. Ed.
MIT Press. Cambridge. MA, pages 305?332. Ed. MIT
Press.
[Laarhoven and Aarts1987] P.J.M. Laarhoven and E.H.L.
Aarts. 1987. Simulated annealing: theory and appli-
cations. Mathematics and its applications. D. Reidel.
[Lesk1986] Michael Lesk. 1986. Automatic sense dis-
ambiguation using mrd: how to tell a pine cone from
an ice cream cone. In Proceedings of SIGDOC ?86,
pages 24?26, New York, NY, USA. ACM.
[Mann and Whitney1947] H. B. Mann and D. R. Whitney.
1947. On a Test of Whether one of Two Random Vari-
ables is Stochastically Larger than the Other. The An-
nals of Mathematical Statistics, 18(1):50?60.
[Miller1995] George A. Miller. 1995. Wordnet: A lexical
database. ACM, Vol. 38(No. 11):p. 1?41.
[Monmarch?2010] N. Monmarch?. 2010. Artificial Ants.
Iste Series. John Wiley & Sons.
[Navigli and Lapata2010] Roberto Navigli and Mirella
Lapata. 2010. An experimental study of graph con-
nectivity for unsupervised word sense disambiguation.
IEEE Trans. Pattern Anal. Mach. Intell., 32:678?692,
April.
[Navigli and Pozetto2012] Roberto Navigli and Si-
mone Paolo Pozetto. 2012. Babelnet: The
automatic construction, evaluation and applica-
tion of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
http://dx.doi.org/10.1016/j.artint.2012.07.004.
[Navigli et al2013] Roberto Navigli, David Jurgens, and
Daniele Vannella. 2013. Semeval-2013 task 12: Mul-
tilingual word sense disambiguation. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM 2013), Atlanta, Georgia, 14-15 June.
[Ponzetto and Navigli2010] Simone Paolo Ponzetto and
Roberto Navigli. 2010. Knowledge-rich word sense
disambiguation rivaling supervised systems. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1522?1531.
[Resnik1995] Philip Resnik. 1995. Using information
content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
[Schwab et al2011] Didier Schwab, J?r?me Goulian, and
Nathan Guillaume. 2011. D?sambigu?sation lexicale
par propagation de mesures semantiques locales par al-
gorithmes a colonies de fourmis. In TALN, Montpel-
lier (France), Juillet.
[Schwab et al2012] Didier Schwab, J?r?me Goulian, An-
don Tchechmedjiev, and Herv? Blanchon. 2012. Ant
colony algorithm for the unsupervised word sense dis-
ambiguation of texts: Comparison and evaluation. In
Proceedings of COLING?2012, Mumbai (India), De-
cember. To be published.
[Schwab et al2013] Didier Schwab, Jer?me Goulian, and
Andon Tchechmedjiev. 2013. Theoretical and empir-
ical comparison of artificial intelligence methods for
unsupervised word sense disambiguation. Int. J. of
Web Engineering and Technology. In Press.
[Tchechmedjiev et al2012] Andon Tchechmedjiev,
J?r?me Goulian, Didier Schwab, and Gilles S?rasset.
2012. Parameter estimation under uncertainty with
simulated annealing applied to an ant colony based
probabilistic wsd algorithm. In Proceedings of
the First International Workshop on Optimization
Techniques for Human Language Technology, pages
109?124, Mumbai, India, December. The COLING
2012 Organizing Committee.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.
1994. Verbs semantics and lexical selection. In Pro-
ceedings of the 32nd annual meeting of Association for
Computational Linguistics, ACL ?94, pages 133?138,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
240
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 161?166,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The LIG machine translation system for WMT 2010
Marion Potet, Laurent Besacier and Herve? Blanchon
LIG Laboratory, GETALP Team
University Joseph Fourier, Grenoble, France.
Marion.Potet@imag.fr
Laurent.Besacier@imag.fr
Herve.Blanchon@imag.fr
Abstract
This paper describes the system submit-
ted by the Laboratory of Informatics of
Grenoble (LIG) for the fifth Workshop
on Statistical Machine Translation. We
participated to the news shared transla-
tion task for the French-English language
pair. We investigated differents techniques
to simply deal with Out-Of-Vocabulary
words in a statistical phrase-based ma-
chine translation system and analyze their
impact on translation quality. The final
submission is a combination between a
standard phrase-based system using the
Moses decoder, with appropriate setups
and pre-processing, and a lemmatized sys-
tem to deal with Out-Of-Vocabulary con-
jugated verbs.
1 Introduction
We participated, for the first time, to the shared
news translation task of the fifth Workshop on Ma-
chine Translation (WMT 2010) for the French-
English language pair. The submission was
performed using a standard phrase-based trans-
lation system with appropriate setups and pre-
processings in order to deal with system?s un-
known words. Indeed, as shown in (Carpuat,
2009), (Habash, 2008) and (Niessen, 2004), han-
dling Ou-of-Vocabulary words with techniques
like lemmatization, phrase table extension or mor-
phological pre-processing is a way to improve
translation quality. After a short presentation of
our baseline system setups we discuss the effect
of Out-Of-Vocabulary words in the system and in-
troduce some ideas we chose to implement. In the
last part, we evaluate their impact on translation
quality using automatic and human evaluations.
2 Baseline System Setup
2.1 Used Resources
We used the provided Europarl and News par-
allel corpora (total 1,638,440 sentences) to train
the translation model and the News monolin-
gual corpora (48,653,884 sentences) to train the
language model. The 2008 News test corpora
(news-test2008; 2,028 sentences) was used to tune
the produced system and last year?s test corpora
(news-test2009; 3,027 sentences) was used for
evaluation purposes. These corpora will be ref-
ered to as Dev and Test later in the paper. As pre-
processing steps, we applied the PERL scripts pro-
vided with the corpora to lowercase and tokenise
the data.
2.2 Language modeling
The target language model is a standard n-gram
language model trained using the SRI language
modeling toolkit (Stocke, 2002) on the news
monolingual corpus. The smoothing technique we
applied is the modified Kneser-Ney discounting
with interpolation.
2.3 Translation modeling
The translation model was trained using the par-
allel corpus described earlier (Europarl+News).
First, the corpus was word aligned and then, the
pairs of source and corresponding target phrases
were extracted from the word-aligned bilingual
training corpus using the scripts provided with
the Moses decoder (Koehn et al, 2007). The re-
sult is a phrase-table containing all the aligned
phrases. This phrase-table, produced by the trans-
lation modeling, is used to extract several transla-
tions models. In our experiment we used thirteen
standard translation models: six distortion models,
a lexicon word-based and a phrase-based transla-
tion model for both direction, and a phrase, word
and distortion penalty.
161
2.4 Tuning and decoding
For the decoding (i.e. translation of the test
set), the system uses a log-linear combination of
the previous target language model and the thir-
teen translation models extracted from the phrase-
table. As the system can be beforehand tuned by
adjusting log-linear combination weights on a de-
velopement corpus, we used the Minimum Error
Rate Training (MERT) method, by (Och, 2003).
3 Ways of Improvements
3.1 Discussion about Out-Of-Vocabulary
words in PBMT systems
Phrase-based statistical machine translation
(PBMT) use phrases as units in the translation
process. A phrase is a sequence of n consecutive
words known by the system. During the training,
these phrases are automaticaly learned and each
source phrase is mapped with its corresponding
target phrase. Throughout test set decoding, a
word not being part of this vocabulary list is
labeled as ?Out-Of-Vocabulary? (OOV) and, as it
doesn?t appear in the translation table, the system
is unable to translate it. During the decoding,
Out-Of-Vocabulary words lead to ?broken?
phrases and degrade translation quality. For these
reasons, we present some techniques to handle
Out-Of-Vocabulary words in a PBMT system and
combine these techniques before evaluating them.
In a preliminary study, we automatically ex-
tracted and manually analyzed OOVs of a 1000
sentences sample extracted from the test cor-
pus (news-test2009). There were altogether 487
OOVs tokens wich include 64.34% proper nouns
and words in foreign languages, 17.62% common
nouns, 15.16% conjugated verbs, 1.84% errors in
source corpus and 1.02% numbers. Note that, as
our system is configured to copy systematically
the OOVs in the produced translated sentence, the
rewriting of proper nouns and words in foreign
language is straightforward in that case. However,
we still have to deal with common nouns and con-
jugated verbs.
Initial sentence:
?Cela ne marchera pas? souligna-t-il par la suite.
Normalised sentence:
?Cela ne marchera pas? il souligna par la suite
Figure 1: Normalisation of the euphonious ?t?
3.2 Term expansion with dictionary
The first idea is to expand the vocabulary size,
more specifically minimizing Out-Of-Vocabulary
common nouns adding a French-English dictio-
nary during the training process. In our experi-
ment, we used a free dictionnary made available
by the Wiktionary1 collaborative project (wich
aims to produce free-content multilingual dictio-
naries). The provided dictionnary, containing
15,200 entries, is added to the bilingual training
corpus before phrase-table extraction.
3.3 Lemmatization of the French source
verbs
To avoid Out-Of-Vocabulary conjugated verbs one
idea is to lemmatize verbs in the source train-
ing and test corpus to train a so-called lemma-
tized system. We used the freely available French
lemmatiser LIA TAGG (Be?chet, 2001). But, ap-
plying lemmatization leads to a loss of informa-
tion (tense, person, number) which may affect
deeply the translation quality. Thus, we decided
to use the lematized system only when OOV verbs
are present in the source sentence to be trans-
lated. Consequently, we differentiate two kinds
of sentences: -sentences containing at least one
OOV conjugated verb, and - sentences which do
not have any conjugated verb (these latter sen-
tences obviously don?t need any lemmatization!).
Thereby, we decided to build a combined trans-
lation system which call the lemmatized system
only when the source sentence contains at least
one Out-Of-Vocabulary conjugated verb (other-
wise, the sentence will be translated by the stan-
dard system). To detect sentences with Out-Of-
Vocabulary conjugated verb we translate each sen-
tence with both systems (lemmatized and stan-
dard), count OOV and use the lemmatized transla-
tion only if it contains less OOV than the standard
translation. For example, a translation containing
k Out-Of-Vocabulary conjugated verbs and n oth-
ers Out-Of-Vocabulary words (in total k+n OOV)
with the standard system, contains, most probably,
only n Out-Of-Vocabulary words with the lemma-
tised system because the conjugated verbs will be
lemmatized, recognized and translated by the sys-
tem.
1http://wiki.webz.cz/dict/
162
3.4 Normalization of a special French form
We observed, in the French source corpra, a spe-
cial French form which generates almost always
Out-Of-Vocabulary words in the English transla-
tion. The special French form, named euphonious
?t?, consists of adding the letter ?t? between a verb
(ended by ?a?, ?e? or ?c?) and a personal pronoun
and, then, inverse them in order to facilitate the
prononciation. The sequence is represented by:
verb-t-pronoun like annonca-t-elle, arrive-t-il, a-
t-on, etc. This form concerns 1.75% of the French
sentences in the test corpus whereas these account
for 0.66% and 0.78% respetively in the training
and the developement corpora. The normalized
proposed form, illustrated below in figure 1, con-
tains the subject pronoun (in first posistion) and
the verb (in the second position). This change has
no influence on the French source sentence and ac-
cordingly on the correctness and fluency of the En-
glish translation.
3.5 Adaptation of the language model
Finally, for each system, we decided to apply dif-
ferent language models and to look at those who
perfom well. In addition to the 5-gram language
model, we trained and tested 3-gram and 4-gram
language models with two different kinds of vo-
cabularies : - the first one (conventional, refered to
as n-gram in table 3) contains an open-vocabulary
extracted from the monolingual English training
data, and - the second one (refered to as n-gram-
vocab in table 3) contains a closed-vocabulary ex-
tracted from the English part of the bilingual train-
ing data. In both cases, language model probabil-
ities are trained from the monolingual LM train-
ing data but, in the second case, the lexicon is re-
stricted to the one of the phrase-table.
4 Experimental results
In the automatic evaluation, the reported evalu-
ation metric is the BLEU score (Papineni et al,
2002) computed by MTEval version 13a. The re-
sults are reported in table 1. Note that in our ex-
periments, according to the resampling method of
(Koehn, 2004), there are significative variations
(improvement or deterioration), with 95% cer-
tainty, only if the difference between two BLEU
scores represent, at least, 0.33 points. To complete
this automatic evaluation, we performed a human
analysis of the systems outputs.
4.1 Standard systems
4.1.1 Term expansion with dictionary
Regarding the results of automatic evaluation (ta-
ble 1, system (2)), adding the dictionary do not
leads to a significant improvement. The OOV
rate and system perplexity are reduced but, ignor-
ing the tuned system which presents lower per-
formance, the BLEU score decreases significatly
on the test set. The BLEU score of the system
augmented with the dictionary is 24.50 whereas
the baseline one is 24.94. So we can conclude
that there is not a meaningfull positive contribu-
tion, probably because the size of the dictionary
is very small regarding the bilingual training cor-
pus. We found out very few Out-Of-Vocabulary
words of the standard system recognized by the
system with the dictionary, see figure 2 for exam-
ple (among them : coupon, cafard, blonde, retar-
dataire, me?dicaments, pamplemousse, etc.). But,
as the dictionnary is very small, most OOV com-
mon words like ho?tesse and clignotant are still un-
known. Regarding the output sentences, we note
that there are very few differences and the quality
is equivalent. The dictionary used is to small to
extend the system?s vocabulary and most of words
still Out-Of-Vocabulary are conjugated verbs and
unrecognized forms.
Baseline system:
A cafard fled before the danger, but if he felt fear?
System with dictionary:
A blues fled before the danger, but if he felt fear?
Figure 2: Example of sentence with an OOV com-
mon noun
4.1.2 Normalisation of special French form
Considering the BLEU score, the normalization of
French euphonious ?t? have, apparently, very few
repercussion on the translation result (table 1, sys-
tem (3)) but the human analysis indicates that, in
our context, the normalisation of euphonious ?t?
brings a clear improvement as seen in example 3.
Consequently, this preprocessing is kept in the fi-
nal system.
4.1.3 Tuning
We can see in table 1 that the usual tuning with
Minimum Error Rate Training algorithm deterio-
rates systematically performance scores on the test
set, for all systems. This can be explained by the
163
System OOVs ppl Dev score Test score
(1) Baseline 2.32% 207 29.72 (19.93) 23.77 (24.94)
(2) + dictionary 2.30% 204 30.01 (23.92) 24.32 (24.50)
(3) + normalization 2.31% 204 30.07 (19.90) 23.99 (24.98)
(4) + normalization + Dev data 2.30% 204 / (/) / (25,05)
Table 1: Standard systems BLEU scores with tuning (without tuning)/ LM 5-gram
Baseline system:
?It will not work? souligna-t-il afterwards.
System with normalisation:
?It will not work? he stressed afterwards.
Figure 3: Example of sentence with a ?verb-t-
pronoun? form
gap between the developement and test corpora (ie
the Dev set may be not representative of the Test
set). So, even if it is recommanded in the standard
process, we do not tune our system (we use the de-
fault weights proposed by the Moses decoder) and
add the developement corpus to train it. In this
case, the training set contains 1,640,468 sentences
(the initial 1,638,440 sentences and the 2,028 sen-
tences of the developement set). This slightly im-
proves the system (from 24.98, the BLEU score
raise to 25,05 after adding the developpement set
to the training).
4.2 Lemmatised systems
Results of lemmatised systems are reported on ta-
ble 2. First, we can notice that, in this particular
case, the tuning (with MERT method) is manda-
tory to adapt the weights of the log linear model.
Our analysis of the tuned weight of the lemma-
tised system shows that, in particular, the word
penalty model has a very low weight (this favours
short sentences) and the lexical word-based trans-
lation models have a very low weight (no use of
the lexical translation probability). We also no-
tice that the lemmatization leads to a real drop-off
of OOV rate (fall from 2.32% for the baseline, to
2.23% for the lemmatized system) and perplexity
(fall from 207 for the baseline, to 178 for the lem-
matized system). We can observe a clear decrease
of the performance with the lemmatized system
(BLEU score of 20.50) compared with a non-
lemmatized one (BLEU score of 24.94). This can
be significatively improved applying euphonious
?t? normalization to the source data (BLEU score
of 22.14). Almost all French OOV conjugated
verbs with the standard system were recognized
by the lemmatized one (trierait, joues, testaient,
immerge?e, e?conomiseraient, baisserait, pre?pares,
etc.) but the small decrease of the translation qual-
ity can be explained, among other things, by sev-
eral tense errors. See illustration in figure 4. So,
we conclude that the systematic normalization of
French verbs, as a pre-process, reduce the Out-Of-
Vocabulary conjugated verbs but decrease slighly
the final translation quality. The use of such a sys-
tem is helpfull especially when the sentence con-
tains conjugated verbs (see example 5).
4.3 Adaptation of the language model
We applied five differents language models (3-
gram and 4-gram language models with selected
vocabulary or not and a 5-gram language model)
to the four standard systems and the two lemma-
tised one. The results, reported in table 3, show
that BLEU score can be significantly different de-
pending on the language model used. For exam-
ple, the fifth system (5) obtained a BLEU score of
21.48 with a 3-gram language model and a BLEU
score of 22.84 with a 4-gram language model. We
can also notice that five out of our six systems out-
perform using a language model with selected vo-
cabulary (n-gram-vocab). One possible explana-
tion is that with LM using selected vocabulary (n-
gram-vocab), there is no loss of probability mass
for english words not present in the translation ta-
ble.
4.4 Final combined system
Considering the previous observations, we believe
that the best choice is to apply the lemmatized
system only if necessary i.e. only if the sentence
contains OOV conjugated verbs, otherwise, a stan-
dard system should be used. We consider system
(4), with 4-gram-vocab language model (selected
vocabulary) without tuning, as the best standard
system and system (6), with 3-gram-vocab lan-
guage model (selected vocabulary) not tuned ei-
ther, as the best lemmatized system. The final
164
System OOVs ppl Dev score Test score
(5) lemmatization 2.23% 178 20.97 (8.57) 20.50 (8.56)
(6) lemmatization + normalization 2.18% 175 27.81 (9.20) 22.14 (10.82)
Table 2: Lemmatised systems BLEU scores with tuning (without tuning)/ LM 5-gram
Baseline system: You will be limited by the absence of exit for headphones.
Lemmatised system: You are limited by the lack of exit for ordinary headphones.
reference: You will be limited by the absence of output on ordinary headphones.
Figure 4: Example of sentences without OOV verbs
system translations are those of the lemmatized
system (6) when we translate sentences with one
or more Out-Of-Vocabulary conjugated verbs and
those of the un-lemmatized system (4) otherwise.
Around 6% of test set sentences were translated
by the lemmatized system. Considering the results
reported in table 4, the combined system?s BLEU
score is comparable to the standard one (25.11
against 25.17).
System Test score sentences
(4) Standard sys. 25.17 94 %
(6) Lemmatised sys. 22.89 6%
(7) Combined 25.11 100 %
Table 4: Combined system?s results and % trans-
lated sentences by each system
5 Human evaluation
We compared two data set. The first set (selected
sent.) contains 301 sentences selected from test
data by the combined system (7) to be translated
by the lemmatized system (6) whereas the second
set (random sent.) contains 301 sentences ran-
domly picked up. The latter is our control data set.
We compared for both groups the translation hy-
pothesis given by the lemmatized system and the
standard one.
We performed a subjective evaluation with the
NIST five points scales to measure fluency and ad-
equacy of each sentences through SECtra w inter-
face (Huynh et al, 2009).We involved a total of 6
volunteers judges (3 for each set). We evaluated
the inter-annotator agreement using a generalized
version of Kappa. The results show a slight to fair
agreement according (Landis, 1977).
The evaluation results, detailled in table 5 and 6,
showed that both fluency and adequacy were im-
proved using our combined system. Indeed, for a
random input (random sent.), the lemmatized sys-
tem lowers the translations quality (fluency and
adequacy are degraded for, respectively, 35.8%
and 37.5% of the sentences), while it improves
the quality for sentences selected by the combined
system (for ?selected sent.?, fluency and adequacy
are improved or stable for 81% of the sentences).
Adequacy selected sent. random sent.
(6) ? (4) 81% 62.4%
(6) < (4) 18.9% 37.5%
Table 5: Subjective evaluation of sentences ade-
quacy ((6) lemmatized system - (4) standard sys-
tem)
Fluency selected sent. random sent.
(6) ? (4) 81% 64.1%
(6)<(4) 18.9% 35.8%
Table 6: Subjective evaluation of sentences flu-
ency ((6) lemmatized system - (4) standard sys-
tem)
6 Conclusion and Discussion
We have described the system used for our sub-
mission to the WMT?10 shared translation task for
the French-English language pair.
We propose dsome very simple techniques to
improve rapidely a statistical machine translation.
Those techniques particularly aim at handling
Out-Of-Vocabulary words in statistical phrase-
based machine translation and lead an improved
fluency in translation results. The submited sys-
tem (see section 4.4) is a combination between a
standard system and a lemmatized system with ap-
propriate setup.
165
Baseline system: At the end of trade, the stock market in the negative bascula.
Lemmatised system: At the end of trade, the stock market exchange stumbled into the negative.
Baseline system: You can choose conseillera.
Lemmatised system: We would advise you, how to choose.
Figure 5: Example of sentences with OOV conjugated verbs
System 3-gram 3-gram-vocab 4-gram 4-gram-vocab 5-gram
(1) 24.60 24.95 24.94 25.11 24.94
(2) 25.14 25.17 24.50 23.49 24.50
(3) 24.88 25.00 24.98 25.15 24.98
(4) 24.92 24.99 25.05 25.17 25.05
(5) 21.48 19.48 22.84 20.18 20.50
(6) 22.60 22.89 22.14 22.24 22.14
Table 3: Systems?s results on test set with differents language models
This system evaluation showed a positive influ-
ence on translation quality, indeed, while the im-
provements on automatic metrics are small, man-
ual inspection suggests a significant improvements
of translation fluency and adequacy.
In future work, we plan to investigate and de-
velop more sophisticated methods to deal with
Out-Of-Vocabulary words, still relying on the an-
alyze of our system output. We believe, for ex-
ample, that an appropriate way to use the dictio-
nary, a sensible pre-processings of French source
texts (in particular normalization of some specific
French forms) and a factorial lemmatization with
the tense information can highly reduce OOV rate
and improve translation quality.
References
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
Annual Meeting of the Association for Computa-
tional Linguistics (ACL). Prague, Czech Republic.
Papineni K., Roukos S., Ward T., and Zhu W.J. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. ACL-2002: 40th Annual meeting
of the Association for Computational Linguistics,
pp. 311?318. Philadelphia, Pennsylvania, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. International Conference
on Spoken Language Processing, Vol. 2, pp 901?
904. Denver, Colorado, USA.
Frederic Be?chet. 2001. LIA TAGG. http://old.lia.univ
-avignon.fr/chercheurs/bechet/download fred.html.
Franz Josef Och. 2003. Minimum error rate train-
ing for statistical machine translation. Annual Meet-
ing of the Association for Computational Linguistics
(ACL). Sapporo, July.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. conference on
Empirical Methods in Natural Language Processing
(EMNLP), pp 388?395. Barcelona, Spain.
Marine Carpuat. 2009. Toward Using Morphology in
French-English Phrase-based SMT. Workshop on
Machine Translation in European Association for
Computational Linguistics (EACL-WMT), pp 150?
154. Athens, Greece.
Sonja Niessen and Hermann Ney. 2004. Statistical
Machine Translation with Scarce Resources Using
Morpho-syntactic Information. Computational Lin-
guistics, vol. 30, pp 181?204.
Nizar Habash. 2008. Four techniques for Online
Handling of Out-Of-Vocabulary Words in Arabic-
English Statistical Machine Translation. Human
Language Technology Workshop in Association for
Computational Linguistics, (ACL-HTL), pp 57?60.
Columbus, Ohio, USA.
Landis J. R. and Koch G. G.. 1977. The Measurement
of Observer Agreement for Categorical Data. Bio-
metrics, vol. 33, pp. 159?174.
Herve? Blanchon, Christian Boitet and Cong-Phap
Huynh. 2009. A Web Service Enabling Grad-
able Post-edition of Pre-translations Produced by
Existing Translation Tools: Practical Use to Provide
High-quality Translation of an Online Encyclopedia.
MT Summit XII, Beyond Translation Memories: New
Tools for Translators Workshop, pp 20?27. Ottawa,
Canada.
166
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 52?60,
Beijing, August 2010
Ontology driven content extraction using interlingual annotation of
texts in the OMNIA project
Achille Falaise, David Rouquet, Didier Schwab, Herve? Blanchon, Christian Boitet
LIG-GETALP, University of Grenoble
{Firstname}.{Lastname}@imag.fr
Abstract
OMNIA is an on-going project that aims
to retrieve images accompanied with
multilingual texts. In this paper, we pro-
pose a generic method (language and do-
main independent) to extract conceptual
information from such texts and sponta-
neous user requests. First, texts are la-
belled with interlingual annotation, then
a generic extractor taking a domain on-
tology as a parameter extract relevant
conceptual information. Implementation
is also presented with a first experiment
and preliminary results.
1 Introduction
The OMNIA project (Luca Marchesotti et al,
2010) aims to retrieve images that are described
with multilingual free companion texts (cap-
tions, comments, etc.) in large Web datasets.
Images are first classified with formal descrip-
tors in a lightweight ontology using automatic
textual and visual analysis. Then, users may ex-
press spontaneous queries in their mother tongue
to retrieve images. In order to build both formal
descriptors and queries for the ontology, a con-
tent extraction in multilingual texts is required.
Multilingual content extraction does not im-
ply translation. It has been shown in (Daoud,
2006) that annotating words or chunks with in-
terlingual lexemes is a valid approach to initiate
a content extraction. We thus skip syntactical
analysis, an expensive and low quality process,
and get language-independent data early in our
flow, allowing further treatments to be language-
independent. We use the lightweight ontology
for image classifications as the formal knowl-
edge representation tha determines relevant in-
formation to extract. This ontology is considered
as a domain parameter for the content extractor.
We are testing this method on a database pro-
vided for the image retrieval challenge CLEF09
by the Belgium press agency Belga. The
database contains 500K images with free com-
panion texts of about 50 words (about 25M
words in total). The texts in the database are in
English only, and we ?simulate? multilinguism
with partially post-edited machine translation.
The rest of the paper is organized as fol-
low. We first depict our general architecture de-
ployed for CLIA and then detail the various pro-
cesses involved : interlingual annotation, con-
ceptual vector based disambiguation and ontol-
ogy driven content extraction. We conclude
with the first results of experimentations on the
CLEF09 data.
2 General architecture
2.1 General process
In our scenario, there are two types of tex-
tual data to deal with : companion texts in the
database (captions), but also user requests. The
two are processed in a very similar way.
The general architecture is depicted in figure
1. The main components, that will be described
in detail, may be summarized as follows:
? Texts (both companions and requests) are
first lemmatised with a language-dependent
piece of software. Ambiguities are pre-
served in a Q-graph structure presented in
section 3.1.2.
52
ConceptsQ-Graph Conceptextraction Lemmatisation Disamb
Companiontexts
NL Requests
NL-UWdictionnary UW-ConceptMap Ontology
Interlingualannotation
Figure 1: General architecture of CLIA in the OMNIA project
? Then, the lemmatised texts are annotated
with interlingual (ideally unambiguous)
lexemes, namely Universal Words (UW)
presented in section 3.1.1. This adds a lot
of ambiguities to the structure, as an ac-
tual lemma may refer to several semanti-
cally different lexemes.
? The possible meanings for lemmas are then
weighted in the Q-graph through a disam-
biguation process.
? Finally, relevant conceptual information is
extracted using an alignment between a do-
main ontology and the interlingual lexemes.
The conceptual information in the output may
adopt different shapes, such as a weighted con-
ceptual vector, statements in the A-Box of the
ontology or annotations in the original text, etc.
In the case of OMNIA, conceptual informa-
tion extracted from companion texts is stored
in a database, while conceptual information ex-
tracted from users requests are transformed into
formal requests for the database (such as SQL,
SPARQL, etc.).
2.2 Implementation
The general process is implemented following a
Service Oriented Architecture (SOA). Each part
of the process corresponds to a service.
This allowed us to reuse part of existing re-
sources developed on heterogeneous platforms
using web interfaces (in the best case REST in-
terfaces (Fielding, 2000), but frequently only
HTML form-based interfaces). A service su-
pervisor has been built to deal with such an
heterogeneity and address normalization issues
(e.g. line-breaks, encoding, identification, cook-
ies, page forwarding, etc.).
This architecture is able to process multiple
tasks concurrently, allowing to deal with users
requests in real time while processing compan-
ion texts in the background.
3 Interlingual annotation
We present in this section the preliminary treat-
ments of multilingual texts (image companion
texts or user requests) that are required for
our content extraction process (Rouquet and
Nguyen, 2009a).
In order to allow a content extraction in multi-
lingual texts, we propose to represent texts with
the internal formalism of the Q-Systems and
to annotate chunks with UNL interlingual lex-
emes (UW) . Roughly, we are making an inter-
lingual lemmatisation, containing more informa-
tion than simple tagging, that is not currently
proposed by any lemmatisation software.
3.1 Resources and data structures
3.1.1 The Universal Network Language
UNL (Boitet et al, 2009; Uchida Hiroshi et
al., 2009) is a pivot language that represents the
meaning of a sentence with a semantic abstract
structure (an hyper-graph) of an equivalent En-
glish sentence.
The vocabulary of UNL consists in a set of
Universal Words (UW). An UW consists of:
1. a headword, if possible derived from En-
glish, that can be a word, initials, an expres-
sion or even an entire sentence. It is a label
for the concepts it represents in its original
language ;
2. a list of restrictions that aims to precisely
specify the concept the UW refers to. Re-
strictions are semantic relations with other
53
UW. The most used is the ?icl? relation that
points to a more general UW.
Examples :
? book(icl>do, agt>human, obj>thing)
and book(icl>thing).
Here, the sense of the headword is focused
by the attributes.
? ikebana(icl>flower arrangement).
Here, the headword comes from Japanese.
? go down.
Here, the headword does not need any re-
finement.
Ideally, an UW refers unambiguously to a con-
cept, shared among several languages. However,
UW are designed to represent acceptions in a
language ; we therefore find distinct UW that
refer to the same concept as for ?affection? and
?disease?.
We are mainly using the 207k UW built by the
U++ Consortium (Jesus Carden?osa et al, 2009)
from the synsets of the Princeton WordNet, that
are linked to natural languages via bilingual dic-
tionaries. The storage of these dictionaries can
be supported by a suitable platform like PIVAX
(Nguyen et al, 2007) or a dedicated database.
The gain of a pivot language is illustrated in fig-
ure 2. If we want to add a new language in the
multilingual system, we just need to create the
links with the pivot but not with all the other lan-
guages.
3.1.2 The Q-Systems
We can think of inserting the UW annotations
with tags (e.g. XML) directly along the source
text as in table 1. However, this naive approach is
not adequate to represent the segmentation am-
biguities that can occur in the text interpretation
(in the example of table 1, we list the different
possible meanings for ?in?, but cannot represent
?waiting?, ?room? and ?waiting room? as three
possible lexical units).
In order to allow the representation of segmen-
tation and other ambiguities, that can occur in
a text interpretation, we propose to use the Q-
Systems. They represent texts in an adequate
Interlingual
UW volume
French 
volume
English 
volume
Chinese
 volume
Figure 2: Multilingual architecture with a pivot
in a waiting room
<tag uw=?in(icl-sup-how),
in(icl-sup-adj),
in(icl-sup-linear unit,
equ-sup-inch)?>in</tag>
<tag uw=?unk?>a</tag> <tag
uw=?waiting room(icl-sup-room,
equ-sup-lounge)?>waiting
room</tag>
Table 1: Naive annotation of a text fragment
graph structure decorated with bracketed expres-
sions (trees) and, moreover, allow processing on
this structure via graph rewriting rules (a set of
such rewriting rules is a so called Q-System).
An example of the Q-System formalism is
given in figure 3 of section 3.2.3. It presents
successively : the textual input representing a Q-
graph, a rewriting rule and a graphical view of
the Q-graph obtained after the application of the
rule (and others).
The Q-Systems were proposed by Alain
Colmeraurer at Montreal University (Colmer-
auer, 1970). For our goal, they have three main
advantages :
? they provide the formalized internal struc-
ture for linguistic portability that we men-
tioned in the introduction (Hajlaoui and
Boitet, 2007) ;
? they unify text processing with powerful
graph rewriting systems ;
54
? they allow the creation or the edition of
a process by non-programmers (e.g. lin-
guists) using SLLP (Specialized Language
for Linguistic Programming).
We are actually using a reimplementation of
the Q-Systems made in 2007 by Hong-Thai
Nguyen during his PhD in the LIG-GETALP
team (Nguyen, 2009).
3.2 Framework of the annotation process
3.2.1 Overview
The annotation process is composed by the
following steps :
1. splitting the text in fragments if too long ;
2. lemmatisation with a specialized software ;
3. transcription to the Q-Systems format ;
4. creation of local bilingual dictionaries
(source language - UW) for each fragment
with PIVAX ;
5. execution of those dictionaries on the frag-
ments ;
3.2.2 Lemmatisation
As we want to use dictionaries where entries
are lemmas, the first step is to lemmatise the in-
put text (i.e. to annotate occurrences with possi-
ble lemmas). This step is very important because
it although gives the possible segmentations of
the text in lexical units. It brings two kinds of
ambiguities into play : on one hand, an occur-
rence can be interpreted as different lemmas, on
the other, there can be several possible segmen-
tations (eventually overlapping) to determine the
lexical units.
For content extraction or information retrieval
purpose, it is better to preserve an ambiguity than
to badly resolve it. Therefore we expect from a
lemmatiser to keep all ambiguities and to repre-
sent them in a confusion network (a simple tag-
ger is not suitable). Several lemmatiser can be
used to cover different languages. For each of
them, we propose to use a dedicated ANTLR
grammar (Terence Parr et al, 2009) in order to
soundly transform the output in a Q-graph.
To process the Belga corpus, we developed a
lemmatiser that produce natively Q-graphs. It
is based on the morphologic dictionary DELA1
available under LGPL licence.
3.2.3 Local dictionaries as Q-Systems
Having the input text annotated with lemmas,
with the Q-System formalism, we want to use the
graph rewriting possibilities to annotate it with
UW. To do so, we use PIVAX export features to
produce rules that rewrite a lemma in an UW (see
figure 3). Each rule correspond to an entry in
the bilingual dictionary. To obtain a tractable Q-
Systems (sets of rules), we built local dictionar-
ies that contain the entries for fragments of the
text (about 250 words in the first experiment).
Figure 3: Creation and execution of a Q-System
Considering the significant quantity of ambi-
guities generated by this approach (up to a dozen
UW for a single word), we need to include a
disambiguation process. This process, based on
conceptual vectors, is presented in the next sec-
tion.
4 Conceptual vector based
disambiguation
Vectors have been used in NLP for over 40 years.
For information retrieval, the standard vector
model (SVM) was invented by Salton (Salton,
1991) during the late 60?s, while for meaning
representation, latent semantic analysis (LSA)
1http://infolingu.univ-mlv.fr/DonneesLinguistiques/
Dictionnaires/telechargement.html
55
was developed during the late 80?s (Deerwester
et al, 1990). These approaches are inspired
by distributional semantics (Harris et al, 1989)
which hypothesises that a word meaning can be
defined by its co-text. For example, the mean-
ing of ?milk? could be described by {?cow?, ?cat?,
?white?, ?cheese?, ?mammal?, . . . }. Hence, distribu-
tional vector elements correspond directly (for
SVM) or indirectly (for LSA) to lexical items
from utterances.
The conceptual vector model is different as it
is inspired by componential linguistics (Hjelm-
lev, 1968) which holds that the meaning of words
can be described with semantic components.
These can be considered as atoms of meaning
(known as primitives (Wierzbicka, 1996)), or
also only as constituents of the meaning (known
as semes, features (Greimas, 1984), concepts,
ideas). For example, the meaning of ?milk?
could be described by {LIQUID, DAIRY PRODUCT, WHITE,
FOOD, . . .}. Conceptual vectors model a formal-
ism for the projection of this notion in a vectorial
space. Hence, conceptual vector elements corre-
spond to concepts indirectly, as we will see later.
For textual purposes2, conceptual vectors can
be associated to all levels of a text (word, phrase,
sentence, paragraph, whole texts, etc.). As they
represent ideas, they correspond to the notion of
semantic field3 at the lexical level, and to the
overall thematic aspects at the level of the entire
text.
Conceptual vectors can also be applied to
lexical meanings. They have been studied in
word sense disambiguation (WSD) using iso-
topic properties in a text, i.e. redundancy of ideas
(Greimas, 1984). The basic idea is to maximise
the overlap of shared ideas between senses of
lexical items. This can be done by computing the
angular distance between two conceptual vectors
(Schwab and Lafourcade, 2007).
In our case, conceptual vectors are used for
automatic disambiguation of texts. Using this
method, we calculate confidence score for each
UW hypothesis appearing in the Q-Graph.
2Conceptual vectors can be associated with any content,
not only text: images, videos, multimedia, Web pages, etc.
3The semantic field is the set of ideas conveyed by a
term.
5 Ontology driven content extraction
The content extraction has to be leaded by a
?knowledge base? containing the informations
we want to retrieve.
5.1 Previous works in content extraction
This approach has its roots in machine trans-
lation projects such as C-Star II (1993-1999)
(Blanchon and Boitet, 2000) and Nespole!
(2000-2002) (Metze et al, 2002), for on the fly
translation of oral speech acts in the domain of
tourism. In these projects, semantic transfer was
achieved through an IF (Inter-exchange Format),
that is a semantic pivot dedicated to the domain.
This IF allows to store information extracted
from texts but is although used to lead the con-
tent extraction process by giving a formal repre-
sentation of the relevant informations to extract,
according to the domain.
The Nespole! IF consists of 123 concepts
from the tourism domain, associated with sev-
eral arguments and associable with speech acts
markers. The extraction process is based on pat-
terns. As an example, the statement ?I wish a
single room from September 10th to 15th? may
be represented as follows:
{ c:give-information+disposition+room
( disposition=(desire, who=i),
room-spec=
( identifiability=no,single_room ),
time=
( start-time=(md=10),
end-time(md=15, month=9)
)
)
}
5.2 Ontologies as parameter for the domain
In the project OMNIA, the knowledge base has
the form of a lightweight ontology for image
classification 4. This ontology contains 732 con-
cepts in the following domains : animals, pol-
itics, religion, army, sports, monuments, trans-
ports, games, entertainment, emotions, etc. To
us, using an ontology has the following advan-
tages :
? Ontologies give an axiomatic description
of a domain, based on formal logics (usu-
4http://kaiko.getalp.org/kaiko/ontology/OMNIA/OMNIA current.owl
56
ally description logics (Baader et al, 2003))
with an explicit semantic. Thus, the knowl-
edge stored in them can be used soundly by
software agents;
? Ontological structures are close to the or-
ganisation of ideas as semantic networks in
human mind (Aitchenson, 2003) and are la-
beled with strings derived from natural lan-
guages. Thus humans can use them (brows-
ing or contributing) in a pretty natural way;
? Finally, with the advent of the Semantic
Web and normative initiatives such as the
W3C5, ontologies come with a lot of shared
tools for editing, querying, merging, etc.
As the content extractor might only process
UW annotations, it is necessary that the knowl-
edge base is whether expressed using UW or
linked to UW. The ontology is here considered
as a domain parameter of content extraction
and can be changed to improve preformances
on specific data collections. Therefore, given
any OWL ontology6, we must be able to link it
with a volume of UW considering the following
constraints :
Creating manually such correspondences
is costly due to the size of resources so an
automatic process is requiered.
Ontologies and lexicons evolve over the time
so an alignment must be adaptable to incremen-
tal evolutions of resources.
The correspondences must be easily manip-
ulated by users so they can manually improve
the quality of automatically created alignments
with post-edition.
Constructing and maintaining an alignment
between an ontology and an UW lexicon is a
challenging task (Rouquet and Nguyen, 2009b).
Basically, any lexical resource can be repre-
sented in an ontology language as a graph. We
propose to use an OWL version of the UW vol-
ume available on Kaiko website 7. It allows us
5http://www.w3.org/
6http://www.w3.org/2004/OWL/
7http://kaiko.getalp.org
to benefit of classical ontology matching tech-
niques and tools (Euzenat and Shvaiko, 2007)
to represent, compute and manipulate the align-
ment. We implemented two string based match-
ing techniques on top of the alignment API (Eu-
zenat, 2004). Specific disambiguation methods
are in development to improve the alignment
precision. Some of them are based on conceptual
vectors presented in section 4, others will adapt
structural ontology matching techniques. This
approach to match an ontology with a lexical re-
source is detailled in (Rouquet et al, 2010).
5.3 The generic extractor
In the case of the OMNIA project, the system
output format is constraint by the goal of an in-
tegration with visual analysis results, in a larger
multimodal system. The visual analysis systems
are also based on concept extraction, but does not
need an ontology to organise concepts. There-
fore, our results has to remain autonaumous,
which means without references to the ontology
used to extract concepts. So, we use a simple
concept vector as output, with intensity weights;
practically, a simple data-value pairs sequence
formatted in XML.
Concept extraction is achieved through a 3
steps process, has shown in figure 4.
1. Concept matching: each UW in the Q-
Graph, that matches a concept according to
the UW-concept map, is labelled with this
concept.
2. Confidence calculation: each concept la-
bel is given a confidence score, in accor-
dance with the score of the UW carrying the
concept, obtained after disambiguation, and
pondered according to the number of UWs
in the Q-Graph. It is planed to take into ac-
count a few linguistics hints here, such as
negations, and intensity adverbs.
3. Score propagation: because we need au-
tonomous results, we have to perform all
ontology-based calculation before releasing
them. The confidence scores are propagated
in the ontology concept hierarchy: for each
57
labelled concept, its score is added to the
super-concept, and so on.
The ontology and the derivated UW-concept
map are considered as parameters for the treat-
ments, and may be replaced in accordance with
the domain, and the relevance of the concepts
and their hierarchy, according to the task.
ConceptnstsQ-sG-Qreon
ConrtnrtarhQsreons xiQrs enL
msohtxhoxQLQreon
DbNhQx 
ConstxrR
qubConstxrUQx
Wnro-oLd
Figure 4: Detail of concept extraction.
6 Experiments
For a first experiment, we used a small dataset,
containing:
? a sub-corpus of 1046 English companion
texts from CLEF09 corpus (press pictures
and captions of about 50 words),
? a 159 concepts ontology, designed for pic-
ture and emotions depiction,
? a UW-concept map comprising 3099 UW.
It appeared that, with this parameters, con-
cepts where extracted for only 25% of the texts.
This preliminary result stressed the importance
of recall for such short texts. However, there
were many ways to improve recall in the system:
? improve the ontology, in order to better
cover the press domain;
? significantly increase the quantity of UW
linked to concepts (only 3099 obtained for
this experiment), by considering synonyms
during the linking process;
? using UW restrictions during concept
matching for UW that are not directly
linked to a concept, as these restrictions are
a rich source of refined semantic informa-
tion.
A second experiment with an improved on-
tology, including 732 concepts, and the use of
UW restrictions, showed very promising results.
Concepts were retrieved from 77% of texts. The
remaining texts were very short (less than 10
words, sometime just date or name).
For example, we extracted the following con-
cepts from the picture and companion text repro-
duced in figure 5.
CoCncepetnntnstQepe-CGraCahexiC eLexmDbNeRquUWedNyeMWOUmeDelqmymDNyeqgexmDbNeImUdNOUWye?OODuerMddUNWeNWeDeRDNyNW?mqqueNWe-D?ODO?deD??WdMme?dlNyD?tQe?lyUu?metnnt??OODuerMddUNWeNdeOq?UO?elMmdMNW?ey?UeOU??qluUWyeqgeRUDlqWdeqgeuDddeOUdymM?NqWeDWOeRN?eOqe?de?Udyeyqe?NOUey?uegmque?eNWdlU?qmdhey?e-mNyNd???mWuUWye?DNuUOeNWeDeccplD?eOqddNUmeuDOUelM????dye?qMmde?UgqmUeDedlU?D?erqMdUeqge?quuqWdeOU?DyUeqWexmDb?xmDbNe?M?MmUe?NWNdyUmerDuDOe?MddUgerDuuDONe???Oey?e-mNyNd?D?U?yNqWde??DdU?dd??eeeee?CeIr??C?xeCoCaeCoCa
Figure 5: Picture document and companion text
example.
CONCEPT WEIGHT
BUILDING 0.098
HOSPITAL 0.005
HOUSE 0.043
MINISTER 0.016
OTHER BUILDING 0.005
PEOPLE 0.142
PERSON 0.038
POLITICS 0.032
PRESIDENT 0.016
RESIDENTIAL BUILDING 0.043
WOMAN 0.005
As this results were more consistent, we could
have a preliminary survey about precision, on a
30 texts sample. While disambiguation imple-
mentation is still at an early stage, weights were
not yet taken into account. A concept match can
be considered correct following two criterons :
1. Visual relevance considers a concept as
correct if carried by an element of the pic-
ture; for instance, the match of concept
58
?SPORT? is regarded as correct for a pic-
ture containing a minister of sports, even if
not actually performing any sport.
2. Textual relevance considers a concept as
correct if carried by a word of the text,
as parts of texts may involve concepts that
are not actually present in the picture, such
as contextual information, previous events,
etc.
124 concepts were found in 23 texts (7 texts had
no concept match):
1. 99 concepts were correct according to the
visual relevance,
2. 110 were correct according to the textual
relevance,
3. 14 were totally incorrect.
We thus have an overall precision score of 0.798
according to the visual relevance and 0.895 ac-
cording to the textual relevance. Most of the er-
rors where caused by ambiguity problems, and
may be addressed with disambiguation process
that are not fully implemented yet.
7 Conclusion and perspectives
We exposed a generic system designed to extract
content (in the form of concepts) from multi-
lingual texts. Our content extraction process is
generic regarding to two aspects :
? it is language independent, as it process an
interlingual representation of the texts
? the content to be extracted can be specified
using a domain ontology as a parameter
This is an ongoing work, and disambiguation
through conceptual vectors is expected to im-
prove accuracy, giving significant weights to the
hypothetical meanings of words.
In the long run, we will focus on integration
with visual content extractors, speed optimiza-
tion to achieve a real-time demonstrator and de-
tailled evaluation of the method.
References
Aitchenson, J. 2003. Words in the Mind. An Intro-
duction to the Mental Lexicon. Blackwell Publish-
ers.
Baader, De Franz, Diego Calvanese, Deborah
McGuinness, Peter Patel-Schneider, and Daniele
Nardi. 2003. The Description Logic Handbook.
Cambridge University Press.
Blanchon, H. and C. Boitet. 2000. Speech translation
for french within the C-STAR II consortium and
future perspectives. In Proc. ICSLP 2000, pages
412?417, Beijing, China.
Boitet, Christian, Igor Boguslavskij, and Jesus
Carden?osa. 2009. An evaluation of UNL usabil-
ity for high quality multilingualization and projec-
tions for a future UNL++ language. In Computa-
tional Linguistics and Intelligent Text Processing,
pages 361?373.
Colmerauer, A. 1970. Les syste`mes-q ou un for-
malisme pour analyser et synthe?tiser des phrases
sur ordinateur. de?partement d?informatique de
l?Universite? de Montre?al, publication interne, 43,
September.
Daoud, Daoud. 2006. Il faut et on peut constru-
ire des syste`mes de commerce e?lectronique a` inter-
face en langue naturelle restreints (et multilingues)
en utilisant des me?thodes oriente?es vers les sous-
langages et le contenu. Ph.D. thesis, UJF, Septem-
ber.
Deerwester, Scott C., Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A.
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society of In-
formation Science, 41(6).
Euzenat, Je?ro?me and Pavel Shvaiko. 2007. Ontology
matching. Springer, Heidelberg (DE).
Euzenat, Je?ro?me. 2004. An API for ontology align-
ment. In Proceedings of the 3rd International
Semantic Web Conference, pages 698?7112, Hi-
roshima, Japan.
Fielding, Roy T. 2000. Architectural styles and the
design of network-based software architectures.
Ph.D. thesis, University of California.
Greimas, Algirdas Julien. 1984. Structural Seman-
tics: An Attempt at a Method. University of Ne-
braska Press.
Hajlaoui, Najeh and Christian Boitet. 2007. Portage
linguistique d?applications de gestion de contenu.
In TOTh07, Annecy.
59
Harris, Zellig S., Michael Gottfried, Thomas Ryck-
man, Paul Mattick Jr., Anne Daladier, T.N. Har-
ris, and S. Harris. 1989. The form of Information
in Science, Analysis of Immunology Sublanguage,
volume 104 of Boston Studies in the Philosophy of
Science. Kluwer Academic Publisher, Dordrecht.
Hjelmlev, Louis. 1968. Prole?gole`me a` une the?orie
du langage. e?ditions de minuit.
Jesus Carden?osa et al 2009. The U++ con-
sortium (accessed on september 2009).
http://www.unl.fi.upm.es/consorcio/index.php,
September.
Luca Marchesotti et al 2010. The Omnia project
(accessed on may 2010). http://www.omnia-
project.org, May.
Max Silberztein. 2009. NooJ linguistic
software (accessed on september 2009).
http://www.nooj4nlp.net/pages/nooj.html,
September.
Metze, F., J. McDonough, H. Soltau, A. Waibel,
A. Lavie, S. Burger, C. Langley, L. Levin,
T. Schultz, F. Pianesi, R. Cattoni, G. Lazzari,
N. Mana, and E. Pianta. 2002. The Nespole!
speech-to-speech translation system. In Proceed-
ings of HLT-2002 Human Language Technology
Conference, San Diego, USA, march.
Nguyen, H.T., C. Boitet, and G. Se?rasset. 2007. PI-
VAX, an online contributive lexical data base for
heterogeneous MT systems using a lexical pivot.
In SNLP, Bangkok, Thailand.
Nguyen, Hong-Thai. 2009. EMEU w,
a simple interface to test the Q-
Systems (accessed on september 2009).
http://sway.imag.fr/unldeco/SystemsQ.po?localhost=
/home/nguyenht/SYS-Q/MONITEUR/, Septem-
ber.
Rouquet, David and Hong-Thai Nguyen. 2009a.
Interlingual annotation of texts in the OMNIA
project. Poznan, Poland.
Rouquet, David and Hong-Thai Nguyen. 2009b.
Multilingu??sation d?une ontologie par des core-
spondances avec un lexique pivot. In TOTh09, An-
necy, France, May.
Rouquet, David, Cassia Trojahn, Didier Scwab, and
Gilles Se?rasset. 2010. Building correspondences
between ontologies and lexical resources. In to be
published.
Salton, Gerard. 1991. The Smart document re-
trieval project. In Proc. of the 14th Annual Int?l
ACM/SIGIR Conf. on Research and Development
in Information Retrieval, Chicago.
Schwab, Didier and Mathieu Lafourcade. 2007. Lex-
ical functions for ants based semantic analysis. In
ICAI?07- The 2007 International Conference on
Artificial Intelligence, Las Vegas, Nevada, USA,
juin.
Terence Parr et al 2009. ANTLR parser
generator (accessed on september 2009).
http://www.antlr.org/, September.
Uchida Hiroshi et al 2009. The UNDL
foundation (accessed on september 2009).
http://www.undl.org/, September.
Wierzbicka, Anna. 1996. Semantics: Primes and
Universals. Oxford University Press.
60
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 440?446,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The LIGA (LIG/LIA) Machine Translation System for WMT 2011
Marion Potet1, Raphae?l Rubino2, Benjamin Lecouteux1, Ste?phane Huet2,
Herve? Blanchon1, Laurent Besacier1 and Fabrice Lefe`vre2
1UJF-Grenoble1, UPMF-Grenoble2
LIG UMR 5217
Grenoble, F-38041, France
FirstName.LastName@imag.fr
2Universite? d?Avignon
LIA-CERI
Avignon, F-84911, France
FirstName.LastName@univ-avignon.fr
Abstract
We describe our system for the news com-
mentary translation task of WMT 2011. The
submitted run for the French-English direction
is a combination of two MOSES-based sys-
tems developed at LIG and LIA laboratories.
We report experiments to improve over the
standard phrase-based model using statistical
post-edition, information retrieval methods to
subsample out-of-domain parallel corpora and
ROVER to combine n-best list of hypotheses
output by different systems.
1 Introduction
This year, LIG and LIA have combined their efforts
to produce a joint submission to WMT 2011 for the
French-English translation task. Each group started
by developing its own solution whilst sharing re-
sources (corpora as provided by the organizers but
also aligned data etc) and acquired knowledge (cur-
rent parameters, effect of the size of n-grams, etc.)
with the other. Both LIG and LIA systems are stan-
dard phrase-based translation systems based on the
MOSES toolkit with appropriate carefully-tuned se-
tups. The final LIGA submission is a combination
of the two systems.
We summarize in Section 2 the resources used
and the main characteristics of the systems. Sec-
tions 3 and 4 describe the specificities and report
experiments of resp. the LIG and the LIA system.
Section 5 presents the combination of n-best lists
hypotheses generated by both systems. Finally, we
conclude in Section 6.
2 System overview
2.1 Used data
Globally, our system1 was built using all the French
and English data supplied for the workshop?s shared
translation task, apart from the Gigaword monolin-
gual corpora released by the LDC. Table 1 sums up
the used data and introduces designations that we
follow in the remainder of this paper to refer to cor-
pora. Four corpora were used to build translation
models: news-c, euro, UN and giga, while three
others are employed to train monolingual language
models (LMs). Three bilingual corpora were de-
voted to model tuning: test09 was used for the de-
velopment of the two seed systems (LIG and LIA),
whereas test08 and testcomb08 were used to tune the
weights for system combination. test10 was finally
put aside to compare internally our methods.
2.2 LIG and LIA system characteristics
Both LIG and LIA systems are phrase-based trans-
lation models. All the data were first tokenized with
the tokenizer provided for the workshop. Kneser-
Ney discounted LMs were built from monolingual
corpora using the SRILM toolkit (Stolcke, 2002),
while bilingual corpora were aligned at the word-
level using GIZA++ (Och and Ney, 2003) or its
multi-threaded version MGIZA++ (Gao and Vogel,
2008) for the large corpora UN and giga. Phrase
table and lexicalized reordering models were built
with MOSES (Koehn et al, 2007). Finally, 14 fea-
tures were used in the phrase-based models:
1When not specified otherwise ?our? system refers to the
LIGA system.
440
CORPORA DESIGNATION SIZE (SENTENCES)
English-French Bilingual training
News Commentary v6 news-c 116 k
Europarl v6 euro 1.8 M
United Nation corpus UN 12 M
109 corpus giga 23 M
English Monolingual training
News Commentary v6 mono-news-c 181 k
Shuffled News Crawl corpus (from 2007 to 2011) news-s 25 M
Europarl v6 mono-euro 1.8 M
Development
newstest2008 test08 2,051
newssyscomb2009 testcomb09 502
newstest2009 test09 2,525
Test
newstest2010 test10 2,489
Table 1: Used corpora
? 5 translation model scores,
? 1 distance-based reordering score,
? 6 lexicalized reordering score,
? 1 LM score and
? 1 word penalty score.
The score weights were optimized on the test09 cor-
pus according to the BLEU score with the MERT
method (Och, 2003). The experiments led specifi-
cally with either LIG or LIA system are respectively
described in Sections 3 and 4. Unless otherwise
indicated, all the evaluations were performed using
case-insensitive BLEU and were computed with the
mteval-v13a.pl script provided by NIST. Ta-
ble 2 summarizes the differences between the final
configuration of the systems.
3 The LIG machine translation system
LIG participated for the second time to the WMT
shared news translation task for the French-English
language pair.
3.1 Pre-processing
Training data were first lowercased with the PERL
script provided for the campaign. They were also
processed in order to normalize a special French
form (named euphonious ?t?) as described in (Potet
et al, 2010).
The baseline system was built using a 4-gram LM
trained on the monolingual corpora provided last
year and translation models trained on news-c and
euro (Table 3, System 1). A significant improve-
ment in terms of BLEU is obtained when taking into
account a third corpus, UN, to build translation mod-
els (System 2). The next section describes the LMs
that were trained using the monolingual data pro-
vided this year.
3.2 Language model training
Target LMs are standard 4-gram models trained
on the provided monolingual corpus (mono-news-c,
mono-euro and news-s). We decided to test two dif-
ferent n-gram cut-off settings. The fist set has low
cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram,
3-gram and 4-gram counts), whereas the second one
(LM2) is more aggressive: 1-5-7-7. Experiment re-
sults (Table 3, Systems 3 and 4) show that resorting
to LM2 leads to an improvement of BLEU with re-
spect to LM1. LM2 was therefore used in the sub-
sequent experiments.
441
FEATURES LIG SYSTEM LIA SYSTEM
Pre-processing Text lowercased Text truecasedNormalization of French euphonious
?t?
Reaccentuation of French words start-
ing with a capital letter
LM Training on mono-news-c, news-s and
mono-euro
Training on mono-news-c and news-s
4-gram models 5-gram models
Translation model
Training on news-c, euro and UN Training on 10 M sentence pairs se-
lected in news-c, euro, UN and giga
Phrase table filtering
Use of -monotone-at-punctuation op-
tion
Table 2: Distinct features between final configurations retained for the LIG and LIA systems
3.3 Translation model training
Translation models were trained from the parallel
corpora news-c, euro and UN. Data were aligned
at the word-level and then used to build standard
phrase-based translation models. We filtered the ob-
tained phrase table using the method described in
(Johnson et al, 2007). Since this technique drasti-
cally reduces the size of the phrase table, while not
degrading (and even slightly improving) the results
on the development and test corpora (System 6), we
decided to employ filtered phrase tables in the final
configuration of the LIG system.
3.4 Tuning
For decoding, the system uses a log-linear com-
bination of translation model scores with the LM
log-probability. We prevent phrase reordering over
punctuation using the MOSES option -monotone-at-
punctuation. As the system can be beforehand tuned
by adjusting the log-linear combination weights on
a development corpus, we used the MERT method
(System 5). Optimizing weights according to BLEU
leads to an improvement with respect to the sys-
tem with MOSES default value weights (System 5
vs System 4).
3.5 Post-processing
We also investigated the interest of a statistical
post-editor (SPE) to improve translation hypotheses.
About 9,000 sentences extracted from the news do-
main test corpora of the 2007?2009 WMT transla-
tion tasks were automatically translated by a sys-
tem very similar to that described in (Potet et al,
2010), then manually post-edited. Manual correc-
tions of translations were performed by means of the
crowd-sourcing platform AMAZON MECHANICAL
TURK2 ($0.15/sent.). These collected data make
a parallel corpus whose source part is MT output
and target part is the human post-edited version of
MT output. This are used to train a phrase-based
SMT (with Moses without the tuning step) that au-
tomatically post-edit the MT output. That aims at
learning how to correct translation hypotheses. Sys-
tem 7 obtained when post-processing MT 1-best out-
put shows a slight improvement. However, SPE was
not used in the final LIG system since we lacked
time to apply SPE on the N-best hypotheses for the
development and test corpora (the N-best being nec-
essary for combination of LIG and LIA systems).
Ths LIGA submission is thus a constrained one.
3.6 Recasing
We trained a phrase-based recaser model on the
news-s corpus using the provided MOSES scripts
and applied it to uppercase translation outputs. A
common and expected loss of around 1.5 case-
sensitive BLEU points was observed on the test cor-
pus (news10) after applying this recaser (System 7)
with respect to the score case-insensitive BLEU pre-
viously measured.
2http://www.mturk.com/mturk/welcome
442
? SYSTEM DESCRIPTION BLEU SCORE
test09 test10
1 Training: euro+news-c 24.89 26.01
2 Training: euro+news-c+UN 25.44 26.43
3 2 + LM1 24.81 27.19
4 2 + LM2 25.37 27.25
5 4 + MERT on test09 26.83 27.53
6 5 + phrase-table filtering 27.09 27.64
7 6 + SPE 27.53 27.74
8 6 + recaser 24.95 26.07
Table 3: Incremental improvement of the LIG system in
terms of case-insensitive BLEU (%), except for line 8
where case-sensitive BLEU (%) are reported
4 The LIA machine translation system
This section describes the particularities of the MT
system which was built at the LIA for its first partic-
ipation to WMT.
4.1 System description
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated French
words starting with a capital letter. We significantly
cleaned up the crawled parallel giga corpus, keeping
19.3 M of the original 22.5 M sentence pairs. For ex-
ample, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with cap-
ital letters were removed. The whole training ma-
terial is truecased, meaning that the words occur-
ing after a strong punctuation mark were lowercased
when they belonged to a dictionary of common all-
lowercased forms; the others were left unchanged.
The training of a 5-gram English LM was re-
strained to the news corpora mono-news-c and news-
s that we consider large enough to ignore other data.
In order to reduce the size of the LM, we first limited
the vocabulary of our model to a 1 M word vocabu-
lary taking the most frequent words in the news cor-
pora. We also resorted to cut-offs to discard infre-
quent n-grams (2-2-3-5 thresholds on 2- to 5-gram
counts) and uses the SRILM option prune, which
allowed us to train the LM on large data with 32 Gb
RAM.
Our translation models are phrase-based models
(PBMs) built with MOSES with the following non-
default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized re-
ordering model scores were optimized on the devel-
opment corpus thanks to the MERT algorithm.
Besides the size of used data, we experimented
with two advanced features made available for
MOSES. Firstly, we filtered phrase tables using the
default setting -l a+e -n 30. This dramatically
reduced phrase tables by dividing their size by a
factor of 5 but did not improve our best configu-
ration from the BLEU score perspective (Table 4,
line 1); the method was therefore not kept in the
LIA system. Secondly, we introduced reordering
constraints in order to consider quoted material as
a block. This method is particularly useful when ci-
tations included in sentences have to be translated.
Two configurations were tested: zone markups in-
clusion around quotes and wall markups inclusion
within zone markups. However, the measured gains
were finally too marginal to include the method in
the final system.
4.2 Parallel corpus subsampling
As the only news parallel corpus provided for the
workshop contains 116 k sentence pairs, we must
resort to parallel out-of-domain corpora in order to
build reliable translation models. Information re-
trieval (IR) methods have been used in the past to
subsample parallel corpora. For example, Hilde-
brand et al (2005) used sentences belonging to the
development and test corpora as queries to select the
k most similar source sentences in an indexed paral-
lel corpus. The retrieved sentence pairs constituted
a training corpus for the translation models.
The RALI submission for WMT10 proposed a
similar approach that builds queries from the mono-
lingual news corpus in order to select sentence pairs
stylistically close to the news domain (Huet et al,
2010). This method has the major interest that it
does not require to build a new training parallel
corpus for each news data set to translate. Fol-
lowing the best configuration tested in (Huet et al,
443
2010), we index the three out-of-domain corpora us-
ing LEMUR3, and build queries from English news-s
sentences where stop words are removed. The 10 top
sentence pairs retrieved per query are selected and
added to the new training corpus if they are not re-
dundant with a sentence pair already collected. The
process is repeated until the training parallel cor-
pus reaches a threshold over the number of retrieved
pairs.
Table 4 reports BLEU scores obtained with the
LIA system using the in-domain corpus news-c and
various amounts of out-of-domain data. MERT was
re-run for each set of training data. The first four
lines display results obtained with the same num-
ber of sentence pairs, which corresponds to the
size of news-c appended to euro. The experiments
show that using euro instead of the first sentences of
UN and giga significantly improves BLEU scores,
which indicates the better adequacy of euro with re-
spect to the test10 corpus. The use of the IR method
to select sentences from euro, UN and giga leads to
a similar BLEU score to the one obtained with euro.
The increase of the collected pairs up to 3 M pairs
generates a significant improvement of 0.9 BLEU
point. A further rise of the amount of collected
pairs does not introduce a major gain since retriev-
ing 10 M sentence pairs only augments BLEU from
29.1 to 29.3. This last configuration which leads to
the best BLEU was used to build the final LIA sys-
tem. Let us note that 2 M, 3 M and 15 M queries
were required to respectively obtain 3 M, 5 M and
10 M sentence pairs because of the removal of re-
dundant sentences in the increased corpus.
For a matter of comparison, a system was also
built taking into account all the training material,
i.e. 37 M sentence pairs4. This last system is out-
performed by our best system built with IR and has
finally close performance to the one obtained with
news-c+euro relatively to the quantity of used data.
5 The system combination
System combination is based on the 500-best out-
puts generated by the LIA and the LIG systems.
3www.lemurproject.org
4For this experiment, the data were split into three parts
to build independent alignment models: news-c+euro, UN and
giga, and they were joined afterwards to build translation mod-
els.
USED PARALLEL CORPORA FILTERING
without with
news-c + euro (1.77 M) 28.1 28.0
news-c + 1.77 M of UN 27.2 -
news-c + 1.77 M of giga 27.1 -
news-c + 1.77 M with IR 28.2 -
news-c + 3 M with IR 29.1 29.0
news-c + 5 M with IR 28.8 -
news-c + 10 M with IR 29.3 29.2
All data 28.9 29.0
Table 4: BLEU (%) on test10 measured with the LIA
system using different training parallel corpora
They both used the MOSES option distinct, en-
suring that the hypotheses produced for a given sen-
tence are different inside an N-best list. Each N-best
list is associated with a set of 14 scores and com-
bined in several steps.
The first step takes as input lowercased 500-best
lists, since preliminary experiments have shown a
better behavior using only lowercased output (with
cased output, combination presents some degrada-
tions). The score combination weights are opti-
mized on the development corpus, in order to max-
imize the BLEU score at the sentence level when
N-best lists are reordered according to the 14 avail-
able scores. To this end, we resorted to the SRILM
nbest-optimize tool to do a simplex-based
Amoeba search (Press et al, 1988) on the error func-
tion with multiple restarts to avoid local minima.
Once the optimized feature weights are com-
puted independently for each system, N-best lists
are turned into confusion networks (Mangu et al,
2000). The 14 features are used to compute poste-
riors relatively to all the hypotheses in the N-best
list. Confusion networks are computed for each sen-
tence and for each system. In Table 5 we present
the ROVER (Fiscus, 1997) results for the LIA and
LIG confusion networks (LIA CNC and LIG CNC).
Then, both confusion networks computed for each
sentence are merged into a single one. A ROVER
is applied on the combined confusion network and
generates a lowercased 1-best.
The final step aims at producing cased hypothe-
ses. The LIA system built from truecased corpora
achieved significantly higher performance than the
444
LIG LIA LIG CNC LIA CNC LIG+LIA
case-insensitive test10 27.6 29.3 28.1 29.4 29.7
BLEU test11 28.5 29.4 28.5 29.3 29.9
case-sensitive test10 26.1 28.4 27.0 28.4 28.7
BLEU test11 26.9 28.4 27.5 28.4 28.8
Table 5: Performance measured before and after combining systems
LIG system trained on lowercased corpora (Table 5,
two last lines). In order to get an improvement when
combining the outputs, we had to adopt the follow-
ing strategy. The 500-best truecased outputs of the
LIA system are first merged in a word graph (and
not a mesh lattice). Then, the lowercased 1-best
previously obtained with ROVER is aligned with the
graph in order to find the closest existing path, which
is equivalent to matching an oracle with the graph.
This method allows for several benefits. The new
hypothesis is based on a ?true? decoding pass gener-
ated by a truecased system and discarded marginal
hypotheses. Moreover, the selected path offers a
better BLEU score than the initial hypothesis with
and without case. This method is better than the one
which consists of applying the LIG recaser (section
3.6) on the combined (un-cased) hypothesis.
The new recased one-best hypothesis is then used
as the final submission for WMT. Our combination
approach improves on test11 the best single sys-
tem by 0.5 case-insensitive BLEU point and by 0.4
case-sensitive BLEU (Table 5). However, it also in-
troduces some mistakes by duplicating in particular
some segments. We plan to apply rules at the seg-
ment level in order to reduce these artifacts.
6 Conclusion
This paper presented two statistical machine trans-
lation systems developed at different sites using
MOSES and the combination of these systems. The
LIGA submission presented this year was ranked
among the best MT system for the French-English
direction. This campaign was the first shot for LIA
and the second for LIG. Beside following the tradi-
tional pipeline for building a phrase-based transla-
tion system, each individual system led to specific
works: LIG worked on using SPE as post-treatment,
LIA focused on extracting useful data from large-
sized corpora. And their combination implied to ad-
dress the interesting issue of matching results from
systems with different casing approaches.
WMT is a great opportunity to chase after perfor-
mance and joining our efforts has allowed to save
considerable amount of time for data preparation
and tuning choices (even when final decisions were
different among systems), yet obtaining very com-
petitive results. This year, our goal was to develop
state-of-the-art systems so as to investigate new ap-
proaches for related topics such as translation with
human-in-the-loop or multilingual interaction sys-
tems (e.g. vocal telephone information-query di-
alogue systems in multiple languages or language
portability of such systems).
References
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates:recognizer output vot-
ing error reduction (ROVER). In Proceedings of the
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?354, Santa Barbara, CA,
USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of the
ACL Workshop: Software Engineering, Testing, and
Quality Assurance for Natural Language Processing,
pages 49?57, Columbus, OH, USA.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th confer-
ence of the European Association for Machine Trans-
lation (EAMT), Budapest, Hungary.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry, and
Philippe Langlais. 2010. The RALI machine trans-
lation system for WMT 2010. In Proceedings of the
ACL Joint 5th Workshop on Statistical Machine Trans-
lation and Metrics (WMT), Uppsala, Sweden.
Howard Johnson, Joel Martin, George Foster, and Roland
445
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, jun.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), Companion Vol-
ume, pages 177?180, Prague, Czech Republic, June.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The LIG machine translation for WMT 2010.
In Proceedings of the ACL Joint 5th Workshop on Sta-
tistical Machine Translation and Metrics (WMT), Up-
psala, Sweden.
William H. Press, Brian P. Flannery, Saul A. Teukolsky,
and William T. Vetterling. 1988. Numerical Recipes
in C: The Art of Scientific Computing. Cambridge
University Press.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Denver, CO, USA.
446
