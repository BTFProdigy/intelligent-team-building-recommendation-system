Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61?64,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Combination of Active Learning and Semi-supervised Learning 
Starting with Positive and Unlabeled Examples for Word Sense   
Disambiguation: An Empirical Study on Japanese Web Search Query 
Makoto Imamura 
and Yasuhiro Takayama 
Information Technology R&D Center,  
Mitsubishi Electric Corporation 
5-1-1 Ofuna, Kamakura, Kanagawa, Japan 
{Imamura.Makoto@bx,Takayama.Yasu 
hiro@ea}.MitsubishiElectric.co.jp 
Nobuhiro Kaji, Masashi Toyoda  
and Masaru Kitsuregawa 
Institute of Industrial Science, 
The University of Tokyo 
4-6-1 Komaba, Meguro-ku Tokyo, Japan 
{kaji,toyoda,kitsure} 
@tkl.iis.u-tokyo.ac.jp 
 
 
Abstract 
This paper proposes to solve the bottle-
neck of finding training data for word 
sense disambiguation (WSD) in the do-
main of web queries, where a complete set 
of ambiguous word senses are unknown. 
In this paper, we present a combination of 
active learning and semi-supervised learn-
ing method to treat the case when positive 
examples, which have an expected word 
sense in web search result, are only given. 
The novelty of our approach is to use 
?pseudo negative examples? with reliable 
confidence score estimated by a classifier 
trained with positive and unlabeled exam-
ples. We show experimentally that our 
proposed method achieves close enough 
WSD accuracy to the method with the 
manually prepared negative examples in 
several Japanese Web search data. 
1 Introduction 
In Web mining for sentiment or reputation 
analysis, it is important for reliable analysis to 
extract large amount of texts about certain prod-
ucts, shops, or persons with high accuracy. When 
retrieving texts from Web archive, we often suf-
fer from word sense ambiguity and WSD system 
is indispensable. For instance, when we try to 
analyze reputation of "Loft", a name of variety 
store chain in Japan, we found that simple text 
search retrieved many unrelated texts which con-
tain "Loft" with different senses such as an attic 
room, an angle of golf club face, a movie title, a 
name of a club with live music and so on. The 
words in Web search queries are often proper 
nouns. Then it is not trivial to discriminate these 
senses especially for the language like Japanese 
whose proper nouns are not capitalized. 
To train WSD systems we need a large 
amount of positive and negative examples. In the 
real Web mining application, how to acquire 
training data for a various target of analysis has 
become a major hurdle to use supervised WSD.  
Fortunately, it is not so difficult to create posi-
tive examples. We can retrieve positive examples 
from Web archive with high precision (but low 
recall) by manually augmenting queries with hy-
pernyms or semantically related words (e.g., 
"Loft AND shop" or "Loft AND stationary").  
On the other hand, it is often costly to create 
negative examples. In principle, we can create 
negative examples in the same way as we did to 
create positive ones. The problem is, however, 
that we are not sure of most of the senses of a 
target word. Because target words are often 
proper nouns, their word senses are rarely listed 
in hand-crafted lexicon. In addition, since the 
Web is huge and contains heterogeneous do-
mains, we often find a large number of unex-
pected senses. For example, all the authors did 
not know the music club meaning of Loft. As the 
result, we often had to spend much time to find 
such unexpected meaning of target words. 
This situation motivated us to study active 
learning for WSD starting with only positive ex-
amples. The previous techniques (Chan and Ng, 
2007; Chen et al 2006) require balanced positive 
and negative examples to estimate the score. In 
our problem setting, however, we have no nega-
tive examples at the initial stage. To tackle this 
problem, we propose a method of active learning 
for WSD with pseudo negative examples, which 
are selected from unlabeled data by a classifier 
trained with positive and unlabeled examples. 
McCallum and Nigam (1998) combined active 
learning and semi-supervised learning technique 
61
by using EM with unlabeled data integrated into 
active learning, but it did not treat our problem 
setting where only positive examples are given. 
The construction of this paper is as follows; 
Section 2 describes a proposed learning algo-
rithm. Section 3 shows the experimental results.  
2 Learning Starting with Positive and 
Unlabeled Examples for WSD 
We treat WSD problem as binary classification 
where desired texts are positive examples and 
other texts are negative examples. This setting is 
practical, because ambiguous senses other than 
the expected sense are difficult to know and are 
no concern in  most Web mining applications. 
2.1 Classifier 
For our experiment, we use naive Bayes classifi-
ers as learning algorithm. In performing WSD, 
the sense ?s? is assigned to an example charac-
terized with the probability of linguistic features 
f1,...,fn so as to maximize: 
?
=
n
j
pp
1
)|(f)( ss j               (1) 
The sense s is positive when it is the target 
meaning in Web mining application, otherwise s 
is negative. We use the following typical linguis-
tic features for Japanese sentence analysis, (a) 
Word feature within sentences, (b) Preceding 
word feature within bunsetsu (Japanese base 
phrase), (c) Backward word feature within bun-
setsu, (d) Modifier bunsetsu feature and (e) 
Modifiee bunsetsu feature. 
Using naive Bayes classifier, we can estimate 
the confidence score c(d, s) that the sense of a 
data instance ?d?, whose features are f1, f2, ..., fn, 
is predicted sense ?s?.  
?
=
+=
n
j
pp
1
)|(f log)( logs)c(d, ss j      (2) 
2.2 Proposed Algorithm 
At the beginning of our algorithm, the system is 
provided with positive examples and unlabeled 
examples. The positive examples are collected 
by full text queries with hypernyms or semanti-
cally related words. 
First we select positive dataset P from initial 
dataset by manually augmenting full text query.      
At each iteration of active learning, we select 
pseudo negative dataset Np (Figure 1 line 15). In 
selecting pseudo negative dataset, we predict 
word sense of each unlabeled example using the 
naive Bayes classifier with all the unlabeled ex-
amples as negative examples (Figure 2). In detail, 
if the prediction score (equation(3)) is more than 
?, which means the example is very likely to be 
negative, it is considered as the pseudo negative 
example (Figure 2 line 10-12). 
pos)c(d,neg)c(d,psdNeg)c(d, ?=          (3) 
 
01    # Definition 
02   ?(P, N): WSD system trained on P as Positive  
03                   examples, N as Negative examples.  
04   ?EM(P, N, U): WSD system trained on P as  
05   Positive examples, N as Negative examples, 
06   U as Unlabeled examples by using EM  
07   (Nigam et. all 2000) 
08    # Input 
09    T ? Initial unlabeled dataset which contain  
10            ambiguous words 
11    # Initialization 
12    P ?  positive training dataset by full text search on T 
13    N ? ? (initial negative training dataset) 
14    repeat 
15      # selecting pseudo negative examples Np  
16          by   the score of  ?(P, T-P)  (see figure 2) 
17      # building a classifier with  Np 
18      ?new ? ?EM (P,  N+Np, T-N-P)   
19      #  sampling data by using the score of ?new 
20      cmin   ? ? 
21      foreach d ? (T ? P ? N )  
22         classify d by WSD system?new 
23         s(d) ? word sense prediction for d using?new 
24         c(d, s(d)) ? the confidence of  prediction of d 
25         if c(d, s(d))  ? cmin   then  
26             cmin  ? c(d),   d min ? d 
27      end 
28    end 
29     provide correct sense s for d min  by human 
30     if s is positive then add d min   to P 
31                             else  add d min   to N 
32   until Training dataset reaches desirable size 
33   ?new  is the output classifier 
 Figure 1: A combination of active learning and 
semi-supervised learning starting with positive 
and unlabeled examples 
Next we use Nigam?s semi-supervised learning 
method using EM and a naive Bayes classifier 
(Nigam et. all, 2000) with pseudo negative data-
set Np  as negative training dataset to build the 
refined classifier ?EM (Figure 1 line 17).  
In building training dataset by active learning, 
we use uncertainty sampling like (Chan and Ng, 
2007) (Figure 1 line 30-31). This step selects the 
most uncertain example that is predicted with the 
lowest confidence in the refined classifier ?EM. 
Then, the correct sense for the most uncertain 
62
example is provided by human and added to the 
positive dataset P or the negative dataset N ac-
cording to the sense of d. 
The above steps are repeated until dataset 
reaches the predefined desirable size. 
 
01    foreach d ? ( T ? P ? N ) 
02       classify d by WSD system?(P, T-P) 
03       c(d, pos) ? the confidence score that d is  
04           predicted as positive defined in equation (2) 
05       c(d, neg) ? the confidence score that d is  
06           predicted as negative defined in equation (2) 
07       c(d, psdNeg) =  c(d, neg)  - c(d, pos)    
08                       (the confidence score that d is  
09                         predicted as pseudo negative)               
10        PN ? d ? ( T ? P ? N ) |  s(d) = neg ?  
11                                                  c(d, psdNeg)  ??} 
12                        (PN is pseudo negative dataset ) 
13     end 
Figure 2: Selection of pseudo negative examples 
3 Experimental Results 
3.1 Data and Condition of Experiments 
We select several example data sets from Japa-
nese blog data crawled from Web. Table 1 shows 
the ambiguous words and each ambiguous senses. 
Word Positive sense Other ambiguous senses 
Wega product name 
(TV) 
Las Vegas, football team 
name, nickname, star, horse 
race, Baccarat glass, atelier, 
wine, game, music 
Loft store name attic room, angle of golf 
club face, club with live 
music,  movie 
Honda personal name 
(football player) 
Personal names (actress, 
artists, other football play-
ers, etc.) hardware store, car 
company name 
Tsubaki product name 
(shampoo) 
flower name, kimono, horse 
race, camellia ingredient, 
shop name 
 Table 1: Selected examples for evaluation 
Table 2 shows the ambiguous words, the num-
ber of its senses, the number of its data instances, 
the number of feature, and the percentage of 
positive sense instances for each data set. 
Assigning the correct labels of data instances is 
done by one person and 48.5% of all the labels 
are checked by another person. The percentage 
of agreement between 2 persons for the assigned 
labels is 99.0%. The average time of assigning 
labels is 35 minutes per 100 instances. 
Selected instances for evaluation are randomly 
divided 10% test set and 90% training set. Table 
3 shows the each full text search query and the 
number of initial positive examples and the per-
centage of it in the training data set. 
word No. of 
senses
No. of  
instances
No. of  
features 
Percentage of  
positive sense
Wega 11 5,372 164,617 31.1%
Loft 5 1,582   38,491 39.4%
Honda 25 2,100   65,687 21.2%
Tsubaki 6 2,022   47,629 40.2%
Table 2: Selected examples for evaluation 
word Full text query for initial 
positive examples 
No. of positive 
examples (percent-
age in trainig set)  
Wega Wega  AND TV 316  (6.5%) 
Loft Loft AND (Grocery OR-
Stationery) 
64  (4.5%) 
Honda Honda AND Keisuke 86 (4.6%) 
Tsubaki Tsubaki AND Shiseido 380 (20.9%) 
Table 3: Initial positive examples 
The threshold value?in figure 2 is set to em-
pirically optimized value 50. Dependency on 
threshold value ? will be discussed in 3.3. 
3.2 Comparison Results 
Figure 3 shows the average WSD accuracy of 
the following 6 approaches. 
 
Figure 3: Average active learning process  
B-clustering is a standard unsupervised WSD, a 
clustering using naive Bayes classifier learned 
with two cluster numbers via EM algorithm. The 
given number of the clusters are two, negative 
and positive datasets.  
  M-clustering is a variant of b-clustering where 
the given number of clusters are each number of 
ambiguous word senses in table 2. 
Human labeling, abbreviated as human, is an 
active learning approach starting with human 
labeled negative examples. The number of hu-
56
58
60
62
64
66
68
70
72
0 10 20 30 40 50 60 70 80 90 100
75
77
79
81
83
85
87
89
91
human
with-EM
without-EM
random
m-clustering
b-clustering
63
man labeled negative examples in initial training 
data is the same as that of positive examples in 
figure 3. Human labeling is considered to be the 
upper accuracy in the variants of selecting 
pseudo negative examples.  
Random sampling with EM, abbreviated as 
with-EM, is the variant approach where dmin  in 
line 26 of figure 1 is randomly selected without 
using confidence score.  
Uncertainty sampling without EM (Takayama 
et al 2009), abbreviated as without-EM, is a vari-
ant approach where ?EM (P,  N+Np, T-N-P) in 
line 18 of figure 1 is replaced by ?(P, N+Np).  
Uncertainty Sampling with EM, abbreviated as un-
certain, is a proposed method described in figure 1. 
The accuracy of the proposed approach with-
EM is gradually increasing according to the per-
centage of added hand labeled examples. 
The initial accuracy of with-EM, which means 
the accuracy with no hand labeled negative ex-
amples, is the best score 81.4% except for that of 
human. The initial WSD accuracy of with-EM is 
23.4 and 4.2 percentage points higher than those 
of b-clustering (58.0%) and m-clustering 
(77.2%), respectively. This result shows that the 
proposed selecting method of pseudo negative 
examples is effective.  
The initial WSD accuracy of with-EM is 1.3 
percentage points higher than that of without-EM 
(80.1%). This result suggests semi-supervised 
learning using unlabeled examples is effective.  
The accuracies of with-EM, random and with-
out-EM are gradually increasing according to the 
percentage of added hand labeled examples and 
catch up that of human and converge at 30 per-
centage added points. This result suggests that 
our proposed approach can reduce the labor cost 
of assigning correct labels.  
The curve with-EM are slightly upper than the 
curve random at the initial stage of active learn-
ing. At 20 percentage added point, the accuracy 
with-EM is 87.0 %, 1.1 percentage points higher 
than that of random (85.9%). This result suggests 
that the effectiveness of proposed uncertainty 
sampling method is not remarkable depending on 
the word distribution of target data.  
There is really not much difference between the 
curve with-EM and without-EM. As a classifies 
to use the score for sampling examples in adapta-
tion iterations, it is indifferent whether with-EM 
or without-EM.  
Larger evaluation is the future issue to confirm 
if the above results could be generalized beyond 
the above four examples used as proper nouns. 
3.3 Dependency on Threshold Value ? 
Figure 4 shows the average WSD accuracies of 
with-EM at 0, 25, 50 and 75 as the values of ?.  
The each curve represents our proposed algorithm 
with threshold value ? in the parenthesis.  The 
accuracy in the case of ? = 75 is higher than that 
of? = 50 over 20 percentage data added point. 
This result suggests that as the number of hand 
labeled negative examples increasing, ? should 
be gradually decreasing, that is, the number of 
pseudo negative examples should be decreasing. 
Because, if sufficient number of hand labeled 
negative examples exist, a classifier does not need 
pseudo negative examples. The control of?
depending on the number of hand labeled examples 
during active learning iterations is a future issue. 
76
78
80
82
84
86
88
90
92
0 10 20 30 40 50 60 70 80 90 100
?=   0.0 
?= 25.0
?= 50.0 
?= 75.0 
 
Figure 4: Dependency of threshold value ? 
References  
Chan, Y. S. and Ng, H. T. 2007. Domain Adaptation 
with Active Learning for Word Sense Disambigua-
tion. Proc. of ACL 2007, 49-56. 
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. 
An Empirical Study of the Behavior of Active 
Learning for Word Sense Disambiguation, Proc. of 
the main conference on Human Language Tech-
nology Conference of the North American Chapter 
of ACL, pp. 120-127. 
McCallum, A. and Nigam, K. 1998. Employing EM 
and Pool-Based Active Learning for Text Classifi-
cation. Proceedings of the Fifteenth international 
Conference on Machine Learning, 350-358. 
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T. 
2000. Text Classification from Labeled and Unla-
beled Documents using EM, Machine Learning, 39, 
103-134.  
Takayama, Y., Imamura, M., Kaji N., Toyoda, M. and 
Kitsuregawa, M. 2009. Active Learning with 
Pseudo Negative Examples for Word Sense Dis-
ambiguation in Web Mining (in Japanese), Journal 
of IPSJ (in printing). 
64
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 883?892, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Identifying Constant and Unique Relations by using Time-Series Text
Yohei Takaku?
Toyo Keizai Inc.
Chuo-ku, Tokyo 103-8345, Japan
takaku.yohei@gmail.com
Nobuhiro Kaji and Naoki Yoshinaga
Institute of Industrial Science,
University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
{kaji,ynaga}@tkl.iis.u-tokyo.ac.jp
Masashi Toyoda
Institute of Industrial Science,
University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
toyoda@tkl.iis.u-tokyo.ac.jp
Abstract
Because the real world evolves over time, nu-
merous relations between entities written in
presently available texts are already obsolete
or will potentially evolve in the future. This
study aims at resolving the intricacy in con-
sistently compiling relations extracted from
text, and presents a method for identifying
constancy and uniqueness of the relations in
the context of supervised learning. We ex-
ploit massive time-series web texts to induce
features on the basis of time-series frequency
and linguistic cues. Experimental results con-
firmed that the time-series frequency distribu-
tions contributed much to the recall of con-
stancy identification and the precision of the
uniqueness identification.
1 Introduction
We have witnessed a number of success stories in
acquiring semantic relations between entities from
ever-increasing text on the web (Pantel and Pennac-
chiotti, 2006; Banko et al2007; Suchanek et al
2007; Wu et al2008; Zhu et al2009; Mintz et al
2009; Wu and Weld, 2010). These studies have suc-
cessfully revealed to us millions of relations between
real-world entities, which have been proven to be
beneficial in solving knowledge-rich problems such
as question answering and textual entailment (Fer-
rucci et al2010).
?This work was conducted while the first author was a grad-
uate student at University of Tokyo.
There exists, however, a great challenge to com-
pile consistently relations extracted from text by
these methods, because they assume a simplifying
assumption that relations are time-invariant. In other
words, they implicitly disregard the fact that state-
ments in texts actually reflect the state of the world
at the time when they were written, which follows
that relations extracted from such texts eventually
become outdated as the real world evolves over time.
Let us consider that relations are extracted from
the following sentences:1
(1) a. 1Q84 is written by Haruki Murakami.
b. Moselle river flows through Germany.
c. U.S.?s president is George Bush.
d. Pentax sells K-5, a digital SLR.
Here, italicized predicates represent the relations,
while underlined entities are their arguments. The
relations in statements 1a and 1b are true across
time, so we can simply accumulate all the relation
instances. The relations in 1c and 1d in contrast
evolve over time. The relation written in 1c be-
comes outdated when the other person takes the
position, so we need to supersede it when a new
relation is extracted from text (e.g., U.S?s president
is Barack Obama). For the relation in 1d, we do not
always need to supersede it with a new relation.
This study is motivated from the above consider-
1Since our task settings are language-independent, we here-
after employ English examples as much as possible to widen
the potential readership of the paper, although we conducted
experiments with relations between entities in Japanese.
883
ations and proposes a method for identifying con-
stancy and uniqueness of relations in order to se-
lect an appropriate strategy to maintain relation in-
stances extracted from text. For example, the rela-
tions written in statements 1a and 1b are constant,
while those in 1c and 1d are non-constant; the re-
lation in 1c is unique,2 whereas the relation in 1d
is non-unique. With these properties of relations in
mind, we can accumulate constant relations while
appropriately superseding non-constant, unique re-
lations with newly acquired relations.
We locate each identification task in the context
of supervised classification. The key challenge in
solving these classification tasks is how to induce an
effective feature that identifies unique, non-constant
relations (statement 1c) that seemingly appear as
non-unique relations on text (statement 1b). We ex-
ploit massive time-series web text to observe actual
evolutions of relation instances and induce features
from the relation instances taken from a time sliding
window and linguistic cues modifying the predicate
and arguments of the target relation.
We evaluated our method on 1000 relations ex-
tracted from 6-year?s worth of Japanese blog posts
with 2.3-billion sentences. We have thereby con-
firmed that the features induced from this time-series
text contributed much to improve the classification
accuracy.
The main contributions of this paper are twofold:
? We have introduced a novel task for identify-
ing constancy relations. Since most of the ex-
isting studies assume that relations are time-
invariant as discussed by Weikum et al2011),
non-constant relations prevalent in their out-
come incur a serious problem in maintaining
the acquired relations. The notion of constancy
is meant to resolve this stalemate.
? We have for the first time demonstrated the
usefulness of a time-series text in relation ac-
quisition and confirmed its impact in the two
relation classification tasks. The features in-
duced from the time-series text have greatly
contributed to the accuracy of the classification
based on uniqueness as well as the recall of the
classification based on constancy.
2This kind of relation is referred to as functional relation in
the literature (Ritter et al2008; Lin et al2010).
Constant Non-constant
arg1 was born in arg2 arg1?s president is arg2
arg1 is a father of arg2 arg1 belongs to arg2
arg1 is written by arg2 arg1 lives in arg2
Table 1: Examples of constant, non-constant relations.
The reminder of this paper is structured as fol-
lows. Section 2 introduces the two properties of
relations (constancy and uniqueness) and then de-
fines the task setting of this study. Sections 3 and 4
describe the features induced from time-series text
for constancy and uniqueness classification, respec-
tively. Section 5 reports experimental results. Sec-
tion 6 addresses work related to this study. Section 7
concludes this study and mentions future work.
2 Classification of Relations based on
Constancy and Uniqueness
2.1 Constancy and uniqueness
We introduce two properties of relations: constancy
and uniqueness.
A relation is constant if, for most values of arg1,
the value of arg2 is independent of time (Table 1).
For example, ?arg1 was born in arg2? is a constant
relation since one?s birthplace never changes. On the
other hand, ?arg1 ?s president is arg2? is an example
of non-constant relations. This can be checked by
noting that, for example, the president of the United
States was Barack Obama in 2011 but was previ-
ously George Bush and Bill Clinton before him.
A relation is unique if, for most values of arg1,
there exists, at any given point in time, only one
value of arg2 that satisfies the relation (Table 2). For
example, ?arg1 was born in arg2? is obviously a
unique relation. The relation ?arg1 is headquartered
in arg2? is also unique, while it is non-constant. No-
tice that there is usually only one headquarters at any
point in time, although the location of a headquarters
can change. In contrast, the relation ?arg1 is funded
by arg2? is a non-unique relation since it is likely
that there exist more than one funder.
2.2 Discussion
Both constancy and uniqueness are properties that
usually, not always, hold for most, not all, of the
arg1?s values. To see this, let us examine the relation
?arg1 ?s president is arg2?. Although this relation is
884
Unique Non-unique
arg1 was born in arg2 arg1 is funded by arg2
arg1 is headquartered in arg2 arg1 consists of arg2
arg1?s president is arg2 arg1 borders on arg2
Table 2: Examples of unique and non-unique relations.
non-constant and unique (Table 1 and 2), it is still
possible to find exceptional cases. For example, a
country might exist in which the president has never
changed; a country might have more than one pres-
ident at the same time during civil war. However,
since such situations are rare, the relation ?arg1 ?s
president is arg2? is considered as neither constant
nor non-unique.
The above discussion implies that the constancy
and uniqueness of relations can not be determined
completely objectively. We, nevertheless, claim that
these properties of relations are intuitively accept-
able and thus they can be identified with moderate
agreement by different people (see section 5).
2.3 Task and our approach
This paper explores classifying given relations on
the basis of constancy and uniqueness. We treat
the problem as two independent binary classification
tasks, and train supervised classifiers.
The technical challenge we address in this paper
is how to design features for the two tasks. Section
3 presents features based on time-series frequency
and linguistic cues for classifying constant and non-
constant relations. Similarly, section 4 presents
analogous features for classifying unique and non-
unique relations.
3 Features for Constancy Classification
3.1 Time-series frequency
It is intuitive to identify constant relations by com-
paring frequency distributions over arg2 in different
time periods. This idea leads us to use frequency
estimates from time-series text as features.
Time-series text For a time-series text, we used
Japanese blog posts that had been gathered from
Feb. 2006 to Sep. 2011 (68 months). These data in-
clude 2.3 billions of sentences. These posts were ag-
gregated on a monthly basis by using time stamps at-
tached with them, i.e., the unit of time is one month
0
2
4
6
8
10
12
Mar-08 Sep-08 Mar-09 Sep-09 Mar-10 Sep-10 Mar-11 Sep-11
Freq
uenc
y
PAO ChelseaChairman Haiberuden Luzhniki StadiumDutch league ItalyVVV VVV VenloCSKA Moscow
Figure 1: Time-series frequency distribution of ?arg1 be-
longs to arg2? when arg1 takes Keisuke Honda.
in our corpus.
Basic idea For constant relations (e.g., ?arg1 was
born in arg2?), we can expect that the frequency dis-
tributions over arg2 for a given arg1 (e.g., Mozart)
are similar to each other irrespective of the time win-
dows that are used to estimate frequency.
In the case of non-constant relations (e.g., ?arg1
belongs to arg2?), on the other hand, the frequency
distributions over arg2 for a given arg1 significantly
differ depending on the time window. For exam-
ple, Figure 1 illustrates the frequency distributions
of arg2s for ?arg1 belongs to arg2? in which arg1
takes Keisuke Honda, a famous football player. We
can clearly observe that due to Keisuke Honda being
sold from VVV Venlo to CSKA Moscow, the distri-
butions differ greatly between 2008 and 2010.
As is evident from the above discussions, the sta-
bility/change in the distribution over arg2 is a good
indicator of constant/non-constant relations. The
following subsection addresses how to encode such
information as features.
Feature computation Let us examine using as
features the cosine similarity between frequency dis-
tributions over arg2. Averaging such similarities
over representative values of arg1, we have
1
N
?
e?EN (r)
cos(Fw1(r, e), Fw2(r, e)),
where r is a relation (e.g., ?arg1 ?s president is
arg2?), e is a named entity (e.g., United States) ap-
pearing in arg1, and Fw(r, e) is the frequency distri-
bution over arg2 when arg1 takes e. The subscripts
885
w1 and w2 denote the time window (e.g., from Jan.
2011 to Feb. 2011) used to estimate the frequency
distribution. EN (r) denotes a set of top N frequent
entities appearing in arg1. We use the entire time-
series text to obtain EN (r).
Unfortunately, this idea is not suitable for our pur-
pose. The problem is that it is not clear how to deter-
mine the two time windows, w1 and w2. To identify
non-constant relations, arg2 must have different val-
ues in the two time periods. Such time windows are,
however, impossible to know of in advance.
We propose avoiding this difficulty by using av-
erage, maximum and minimum similarity over all
possible time windows:
1
N
?
e?EN (r)
ave
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
1
N
?
e?EN (r)
max
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
1
N
?
e?EN (r)
min
w1,w2?WT
cos(Fw1(r, e), Fw2(r, e)),
where WT is a set of all time windows of the size
T . For example, if we set T to 3 (months) in the
68-month?s worth of blog posts, WT consists of 66
(= 68?3+1) time windows. Although we still have
to specify the number of entities N and the window
size T , this is not a serious problem in practice. We
set N to 100. We use four window sizes (1, 3, 6, and
12 months) and induce different features for each
window size. As a result, we have 12 real-valued
features.
3.2 Linguistic cues
This subsection presents two types of linguistically-
motivated features for discriminating between con-
stant and non-constant relations.
Nominal modifiers We observe that non-constant
relations could be indicated by some nominal modi-
fiers:
(2) a. George Bush, ex-president of USA.
b. Lincoln is the first president of the USA.
The use of the prefix ex- and the adjective first im-
plies that the president changes, and hence the rela-
tion ?arg1 ?s president is arg2? is not constant.
? (ex-),? (present),?? (next),? (former),? (new),
? (old),?? (successive),?? (first),? (first)
Table 3: Japanese prefixes and adjectives indicating non-
constant relations. The translations are provided in the
parentheses.
We propose making use of such modifiers as fea-
tures. Although the above examples are in English,
we think modifiers also exist that have similar mean-
ings in other languages including Japanese, our tar-
get language.
Our new features are induced as follows:
? First, we manually list eight nominal modifiers
that indicate the non-constancy (Table 3).
? Next, we extract nouns from a relation to
be classified (e.g., president), and count the
frequency with which each modifier modifies
those nouns. We use the same blog posts as in
section 3.1 for counting the frequency. Since
time information is not important in this case,
the frequency is simply accumulated over the
entire time span.
? We then generate eight features, one for each of
the eight modifiers. The value of the features
is one if the frequency exceeds threshold ?1,3
otherwise it is zero. Note that the value of this
feature is always zero if the relation includes no
nouns.
Tense and aspect Tense and aspect of verbs are
also important indicators of the non-constancy:
(3) The U.S. president was George Bush.
If a relation, such as ?arg1 ?s president is arg2?, can
often be rephrased in the past tense as in (3), it is
likely to be, if not always, a non-constant relation.
It is, fortunately, straightforward to recognize
tense and aspect in Japanese, because they are ex-
pressed by attaching suffixes to verbs. In this study,
we use three common suffixes: ???, ?????, and
????. The first suffix expresses past tense, while
the other two express present continuous or progres-
sive aspects depending on context.
3?1 = 10 in our experiment.
886
A given relation is transformed into different
forms by attaching the suffixes to a verb in the rela-
tion, and their frequencies are counted. By using the
frequency estimates, we generate three new features,
each of which corresponds to one of the three suf-
fixes. The value of the new features is one if the fre-
quency exceeds threshold ?2,4 otherwise it is zero.
The frequency is counted in the same way as in
the case of the nominal modifiers. The value of
this feature is always zero if the relation includes no
verbs.
4 Features for Uniqueness Classification
This section provides features for identifying unique
relations. These features are also based on the time-
series text and linguistic cues, as in the case of con-
stancy classification.
4.1 Time-series frequency
Number of entity types A straightforward ap-
proach to identifying unique relations is, for a given
arg1, to count the number of entity types appear-
ing in arg2 (Lin et al2010). For unique relations,
the number of entity types should be one in an ideal
noiseless situation. Even if the estimate is contam-
inated by noise, a small number of entity types can
still be considered to indicate the uniqueness of the
relation.
A shortcoming of such a simple approach is that
it never considers the (non-)constancy of relations.
Presume counting the number of entity types in arg2
of the relation ?arg1 is headquartered in arg2?,
which is non-constant and unique. If we use large
size of time window to obtain counts, we will ob-
serve multiple types of entities in arg2, not because
the relation is non-unique, but because it is non-
constant. This problem cannot be resolved by triv-
ially using very small windows, since a time win-
dow that is too small in turn causes a data sparseness
problem.
This problem is attributed to the difficulty in de-
termining the appropriate size of the time window.
We tackle this problem by using the same technique
presented in section 3.1. Specifically, we use the fol-
4?2 = 3000 in our experiment.
lowing three measures as features:
1
N
?
e?EN (r)
ave
w?WT
#type(Fw(r, e)),
1
N
?
e?EN (r)
max
w?WT
#type(Fw(r, e)),
1
N
?
e?EN (r)
min
w?WT
#type(Fw(r, e)),
where the function #type(?) denotes the number of
entity types appearing in arg2.
Ratio of entity frequency Since it is not reliable
enough to use only the number of entity types, we
also exploit the frequency of the entity. Let e1st and
e2nd be the most and the second most frequent enti-
ties found in arg2. If the frequency of e1st is much
larger than that of e2nd, the relation is likely to be
constant.
To encode this intuition, the following measures
are used as features:
1
N
?
e?EN (r)
ave
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
1
N
?
e?EN (r)
max
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
1
N
?
e?EN (r)
min
w?WT
fw(e, r, e1st)
fw(e, r, e2nd)
where the fw(e, r, e?) is the frequency of the relation
r in which arg1 and arg2 take e and e?, respectively.
The subscript w denotes the time window.
4.2 Linguistic cues
Coordination structures and some keywords indicate
non-unique relations:
(4) a. France borders on Italy and Spain.
b. France borders on Italy etc.
The coordination structure in the first example im-
plies an entity can border on more than one entity,
and hence the relation ?arg1 borders on arg2? is not
unique. The keyword etc. in the second example also
indicates the non-uniqueness.
887
?,??,?,??,??,??,?
Table 4: List of Japanese particles that are used to form
coordination structures.
To capture this intuition, we introduce two types
of linguistic features for classifying unique and non-
unique relations. The first feature checks whether
entities in arg2 form coordination structures. The
feature is fired if the number of times that coordina-
tion structures are found in arg2 exceeds threshold
?3.5 Coordination structures are identified by a list
of Japanese particles, which roughly correspond to
and or or in English (Table 4). If two entities are
connected by one of those particles, they are seen as
forming a coordination structure.
The second feature exploits such keywords as etc.
for identifying non-unique relations. We list four
Japanese keywords that have similar meaning to the
English word etc., and induce another binary fea-
ture6. The feature is fired if the number of times that
an entity in arg2 is followed by one of the four key-
words exceeds threshold ?3.
5 Experiments and discussions
We built labeled data and examine the classification
performance of the proposed method. We also an-
alyzed the influence of window size T on the per-
formance, as well as major errors caused by our
method.
5.1 Data
We built a dataset for evaluation by extracting rela-
tions from the time-series text (section 3.1) and then
manually annotating 1000 relations. The detailed
procedure is as follows.
First, we parsed the time-series text and extracted
as relation dependency paths connecting two named
entities. We used J.DepP,7 an efficient shift-reduce
parser with feature sequence trie (Yoshinaga and
Kitsuregawa, 2009; Yoshinaga and Kitsuregawa,
2010), for parsing. All Japanese words that conju-
gate were normalized into standard forms.
5?3 = 10 in our experiment.
6The keywords we used are?,?,??, and?.
7http://www.tkl.iis.u-tokyo.ac.jp/
?ynaga/jdepp/
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
Proposed
Baseline
Figure 2: Recall-precision curve (constancy classifica-
tion).
Then, annotators were asked to label 1000 rela-
tions as not only constant or non-constant but also
unique or non-unique. Three annotators were as-
signed to each relation, and the goldstandard label
is determined by majority vote. The Fleiss kappa
(Fleiss, 1971) was 0.346 for constancy classification
and was 0.428 for uniqueness classification. They
indicate fair and moderate agreement, respectively
(Landis and Koch, 1977).
We have briefly investigated the relations whose
labels assigned by the annotators conflicted. The
major cause was that the annotators sometimes as-
sumed different types of named entities as values
of arguments. A typical case in which this problem
arises is that the relation has polysemous meanings,
e.g., ?arg1 was born in arg2?, or a vague meaning,
e.g., ?arg1makes arg2?. For example, arg2 of ?arg1
was born in arg2? can be filled with different types
of entities such as date and place. We can address
this problem by typing arguments (Lin et al2010).
5.2 Result
Using the dataset, we performed 5-fold cross-
validation for both classification tasks. We used
the passive-aggressive algorithm for our classifier
(Crammer et al2006).
Constancy classification Figure 2 illustrates the
recall-precision curve in constancy classification.
Because we are unaware of any previous methods
for classifying constant and non-constant relations,
a simple method based on the cosine similarity was
888
00.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
Proposed
Baseline
Figure 3: Recall-precision curve (uniqueness classifica-
tion).
used as a baseline:
1
N
?
e?EN (r)
cos(Fw1(r, e), Fw2(r, e)),
where the time windows w1 and w2 are determined
as the first and last month in which the relation r
is observed. A given relation is classified as non-
constant if the above similarity exceeds a threshold.
The recall-precision curve was drawn by changing
the threshold.
The results demonstrated that our method outper-
forms the baseline. This indicates the effectiveness
of using time-series frequency and linguistic cues as
features.
The poor performance of the baseline was mainly
due to data sparseness. Since the baseline method is
dependent on the frequency estimates obtained from
only two months of texts, it is less reliable than the
proposed method.
Uniqueness classification Figure 3 illustrates the
recall-precision curve in uniqueness classification.
As a baseline we implemented the method proposed
by Lin et al2010). While they have presented
three methods (KLFUNC, KLDIFF, and their aver-
age), we report the results of the last one because it
performed the best among the three in our experi-
ment.
From the figure, we can again see that the pro-
posed method outperforms the baseline method.
Lin?s method is similar to ours, but differs in that
they do not exploit time-series information at all.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
N = 2N = 10N = 20N = 100
Figure 4: Comparison with the methods varying a value
of N for constancy classification.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
N = 2N = 10N = 20N = 100
Figure 5: Comparison with the methods varying a value
of N for uniqueness classification.
We hence conclude time-series information is use-
ful for classifying not only constant but also unique
relations.
5.3 Investigation into the number of entities, N
We ranged the value of N in {2, 10, 20, 100}. Set-
ting N to a larger value yields the better recall for
constancy classification and the better precision for
uniqueness classification (Figures 4 and 5). These
results meet our expectations, since features derived
from frequency distributions of arg2 over various
arg1s capture the generic nature of the target rela-
tion.
889
00.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
T = 1, 3, 6, 12T = 1T = 3T = 6T = 12
Figure 6: Comparison with the methods using only a sin-
gle value of T for constancy classification.
0
0.2
0.4
0.6
0.8
1.0
0 0.2 0.4 0.6 0.8 1.0
Pr
ec
isi
on
Recall
T = 1, 3, 6, 12T = 1T = 3T = 6T = 12
Figure 7: Comparison with the methods using only a sin-
gle value of T for uniqueness classification.
5.4 Investigation into the window size, T
Our method uses multiple time windows of different
sizes (i.e., different values of T ) to induce features,
as detailed in sections 3.1 and 4.1. To confirm the
effect of this technique, we investigated the perfor-
mance when we use only a single value of T (Fig-
ures 6 and 7).
The results in the uniqueness classification task
demonstrated that our method achieves better over-
all results than the methods using a single value of
T . We can therefore consider that using multiple
values of T as features is a reasonable strategy. On
the other hand, we could not confirm the effect of
using multiple time windows of different sizes in the
constancy classification task.
5.5 Error analysis
We randomly selected and analyzed 200 misclassi-
fied relations for both tasks. The analysis revealed
four types of errors.
Paraphrases We observed that constant relations
are prone to be miss-classified as non-constant when
more than one paraphrase appear in arg2 and thus
the value of arg2 is pretended to change. For exam-
ple, America was also referred to as USA or United
States of America. A similar problem was observed
for unique relations as well.
Topical bias Topics mentioned in the blog posts
are sometimes biased, and such bias can have a neg-
ative effect on classification, especially when a rela-
tion takes a small number of entity types in arg2 for
given arg1. For example, Jaden Smith, who is one
of Will Smith?s sons, is frequently mentioned in our
time-series text because he co-starred with his father
in a movie, while Will Smith?s other sons never ap-
peared in our text. We consider this a possible rea-
son for our method wrongly identifying ?arg1 ?s son
is arg2? as a unique relation.
Short-/Long-term evolution Since we have ag-
gregated on a monthly basis the 6-year?s worth of
blog posts, the induced features cannot capture evo-
lutions that occur in shorter or longer intervals. For
example, consider relation ?arg1 beats arg2? tak-
ing Real Madrid as arg1. Since Real Madrid usually
have more than one football match in a month, they
can beat several teams in a month, which misleads
the classifier to recognize the relation as non-unique.
Similarly when a relation takes more than 6 years to
evolve, it will be regarded as constant.
Reference to past, future, or speculative facts
The blog authors sometimes refer to relations that do
not occur around when they write their posts; such
relations actually occurred in the past, will occur in
the future, or even speculative. Since our method
exploits the time stamps attached to the posts to as-
sociate the relations with time, those relations in-
troduce noises in the frequency distributions. Al-
though our robust feature induction could in most
cases avoid an adverse effect caused by these noises,
they sometimes leaded to misclassification.
890
6 Related Work
In recent years, much attention has been given to
extracting relations from a massive amount of tex-
tual data, especially the web (cf. section 1). Most of
those studies, however, explored just extracting re-
lations from text. Only a few studies, as described
below, have discussed classifying those relations.
There has been no previous work on identify-
ing the constancy of relations. The most relevant
research topic is the temporal information extrac-
tion (Verhagen et al2007; Verhagen et al2010;
Ling and Weld, 2010; Wang et al2010; Hovy et
al., 2012). This is the task of extracting from textual
data an event and the time it happened, e.g., Othello
was written by Shakespeare in 1602. Such tempo-
ral information alone is not sufficient for identifying
the constancy of relations, while we think it would
be helpful.
On the other hand, the uniqueness of relations has
so far been discussed in some studies. Ritter et al
(2008) have pointed out the importance of identi-
fying unique relations for various NLP tasks such
as contradiction detection, quantifier scope disam-
biguation, and synonym resolution. They proposed
an EM-style algorithm for scoring the uniqueness
of relations. Lin et al2010) also proposed three
algorithms for identifying unique relations. While
those studies discussed the same problem as this pa-
per, they did not point out the importance of the
constancy in identifying unique relations (cf. sec-
tion 4.1).
7 Conclusion
This paper discussed that the notion of constancy
is essential in compiling relations between enti-
ties extracted from real-world text and proposed a
method for classifying relations on the basis of con-
stancy and uniqueness. The time-series web text
was fully exploited to induce frequency-based fea-
tures from time-series frequency distribution on re-
lation instances as well as language-based features
tailored for individual classification tasks. Exper-
imental results confirmed that the frequency-based
features contributed much to the precision and recall
in both identification tasks.
We will utilize the identified properties of the re-
lations to adopt an appropriate strategy to compile
their instances. We also plan to start a spin-off re-
search that acquires paraphrases by grouping values
of arg2s for each value of arg1 in a constant, unique
relation.
We consider that the notion of constancy will even
be beneficial in acquiring world knowledge, other
than relations between entities, from text; we aim
at extending the notion of constancy to other types
of knowledge involving real-world entities, such as
concept-instance relations.
Acknowledgments
This work was supported by the Multimedia Web
Analysis Framework towards Development of So-
cial Analysis Software program of the Ministry of
Education, Culture, Sports, Science and Technol-
ogy, Japan. The authors thank the annotators for
their hard work. The authors are also indebted to the
three anonymous reviewers for their valuable com-
ments.
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI, pages 2670?2676.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shawartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?583.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John Prager,
Nico Schlaefer, and Chris Welty. 2010. Building Wat-
son: An overview of the DeepQA project. AI Maga-
zine, 31(3):59?79.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patward-
han, and Christopher Welty. 2012. When did that hap-
pen? ? linking events and relations to timestamps. In
Proceedings of EACL, pages 185?193.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 1(33):159?174.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Identify-
ing functional relation in web text. In Proceedings of
EMNLP, pages 1266?1276.
891
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of AAAI, pages 1385?
1390.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP,
pages 1003?1011.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of ACL, pages
113?120.
Alan Ritter, Doug Downey, Stephen Soderland, and Oren
Etzioni. 2008. It?s a contradiction?no, it?s not: A
case study using functional relations. In Proceedings
of EMNLP, pages 11?20.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying WordNet and Wikipedia. In Proceed-
ings of WWW, pages 697?706.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal
relation identification. In Proceedings of SemEval,
pages 75?80.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of SemEval, pages 57?
62.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely YAGO: har-
vesting, querying, and visualizing temporal knowledge
from Wikipedia. In Proceedings of EDBT, pages 697?
700.
Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.
2011. Temporal knowledge for timely intelligence. In
Proceedings of BIRTE, pages 1?6.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of ACL,
pages 118?127.
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008.
Information extraction from Wikipedia: moving down
the long tail. In Proceedings of KDD, pages 731?739.
Naoki Yoshinaga and Masaru Kitsuregawa. 2009. Poly-
nomial to linear: Efficient classification with conjunc-
tive features. In Proceedings of EMNLP, pages 1542?
1551.
Naoki Yoshinaga andMasaru Kitsuregawa. 2010. Kernel
slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of COLING, pages 1245?1253.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW, pages 101?110.
892
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 964?972,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Predicting and Eliciting Addressee?s Emotion in Online Dialogue
Takayuki Hasegawa?
GREE Inc.
Minato-ku, Tokyo 106-6101, Japan
takayuki.hasegawa@gree.net
Nobuhiro Kaji and Naoki Yoshinaga
Institute of Industrial Science,
the University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
{kaji,ynaga}@tkl.iis.u-tokyo.ac.jp
Masashi Toyoda
Institute of Industrial Science,
the University of Tokyo
Meguro-ku, Tokyo 153-8505, Japan
toyoda@tkl.iis.u-tokyo.ac.jp
Abstract
While there have been many attempts to
estimate the emotion of an addresser from
her/his utterance, few studies have ex-
plored how her/his utterance affects the
emotion of the addressee. This has mo-
tivated us to investigate two novel tasks:
predicting the emotion of the addressee
and generating a response that elicits a
specific emotion in the addressee?s mind.
We target Japanese Twitter posts as a
source of dialogue data and automatically
build training data for learning the pre-
dictors and generators. The feasibility of
our approaches is assessed by using 1099
utterance-response pairs that are built by
five human workers.
1 Introduction
When we have a conversation, we usually care
about the emotion of the person to whom we
speak. For example, we try to cheer her/him up
if we find out s/he feels down, or we avoid saying
things that would trouble her/him.
To date, the modeling of emotion in a dialogue
has extensively been studied in NLP as well as re-
lated areas (Forbes-Riley and Litman, 2004; Ayadi
et al, 2011). However, the past attempts are vir-
tually restricted to estimating the emotion of an
addresser1 from her/his utterance. In contrast, few
studies have explored how the emotion of the ad-
dressee is affected by the utterance. We consider
the insufficiency of such research to be fatal for
?This work was conducted while the first author was a
graduate student at the University of Tokyo.
1We use the terms addresser/addressee rather than a
speaker/listener, because we target not spoken but online di-
alogue.
I have had a high fever for 3 days.
JOY
I hope you feel better soon.
I have had a high fever for 3 days.
SADNESS
Sorry, but you can?t join us today.
Figure 1: Two example pairs of utterances and re-
sponses. Those responses elicit certain emotions,
JOY or SADNESS, in the addressee?s mind. The ad-
dressee in this example refers to the left-hand user,
who receives the response.
computers to support human-human communica-
tions or to provide a communicative man-machine
interface.
With this motivation in mind, the paper inves-
tigates two novel tasks: (1) prediction of the ad-
dressee?s emotion and (2) generation of the re-
sponse that elicits a prespecified emotion in the ad-
dressee?s mind.2 In the prediction task, the system
is provided with a dialogue history. For simplic-
ity, we consider, as a history, an utterance and a
response to it (Figure 1). Given the history, the
system predicts the addressee?s emotion that will
be caused by the response. For example, the sys-
tem outputs JOY when the response is I hope you
feel better soon, while it outputs SADNESS when
the response is Sorry, but you can?t join us today
2We adopt Plutchik (1980)?s eight emotional categories in
both tasks.
964
(Figure 1).
In the generation task, on the other hand, the
system is provided with an utterance and an emo-
tional category such as JOY or SADNESS, which is
referred to as goal emotion. Then the system gen-
erates the response that elicits the goal emotion in
the addressee?s mind. For example, I hope you feel
better soon is generated as a response to I have had
a high fever for 3 days when the goal emotion is
specified as JOY, while Sorry, but you can?t join us
today is generated for SADNESS (Figure 1).
Systems that can perform the two tasks not only
serve as crucial components of dialogue systems
but also have interesting applications of their own.
Predicting the emotion of an addressee is use-
ful for filtering flames or infelicitous expressions
from online messages (Spertus, 1997). The re-
sponse generator that is aware of the emotion of
an addressee is also useful for text completion in
online conversation (Hasselgren et al, 2003; Pang
and Ravi, 2012).
This paper explores a data-driven approach to
performing the two tasks. With the recent emer-
gence of social media, especially microblogs, the
amount of dialogue data available is rapidly in-
creasing. Therefore, we are taking this opportu-
nity to building large-scale training data from mi-
croblog posts automatically. This approach allows
us to perform the two tasks in a large-scale with
little human effort.
We employ standard classifiers for predicting
the emotion of an addressee. Our contribution here
is to investigate the effectiveness of new features
that cannot be used in ordinary emotion recog-
nition, the task of estimating the emotion of a
speaker (or writer) from her/his utterance (or writ-
ing) (Ayadi et al, 2011; Bandyopadhyay and Oku-
mura, 2011; Balahur et al, 2011; Balahur et al,
2012). We specifically extract features from the
addressee?s last utterance (e.g., I have had a high
fever for 3 days in Figure 1) and explore the effec-
tiveness of using such features. Such information
is characteristic of a dialogue situation.
To perform the generation task, we build a sta-
tistical response generator by following (Ritter et
al., 2011). To improve on the previous study, we
investigate a method for controlling the contents
of the response for, in our case, eliciting the goal
emotion. We achieve this by using a technique in-
spired by domain adaptation. We learn multiple
models, each of which is adapted for eliciting one
specific emotion. Also, we perform model inter-
polation for addressing data sparseness.
In our experiment, we automatically build train-
ing data consisting of over 640 million dialogues
from Japanese Twitter posts. Using this data set,
we train the classifiers that predict the emotion
of an addressee, and the response generators that
elicit the goal emotion. We evaluate our methods
on the test data that are built by five human work-
ers, and confirm the feasibility of the proposed ap-
proaches.
2 Emotion-tagged Dialogue Corpus
The key in making a supervised approach to pre-
dicting and eliciting addressee?s emotion success-
ful is to obtain large-scale, reliable training data
effectually. We thus automatically build a large-
scale emotion-tagged dialogue corpus from mi-
croblog posts, and use it as the training data in the
prediction and generation tasks.
This section describes a method for construct-
ing the emotion-tagged dialogue corpus. We first
describe how to extract dialogues from posts in
Twitter, a popular microblogging service. We then
explain how to automatically annotate utterances
in the extracted dialogues with the addressers?
emotions by using emotional expressions as clues.
2.1 Mining dialogues from Twitter
We have first crawled utterances (posts) from
Twitter by using the Twitter REST API.3 The
crawled data consist of 5.5 billion utterances in
Japanese tweeted by 770 thousand users from
March 2011 to December 2012. We next cleaned
up the crawled utterances by handling Twitter-
specific expressions; we replaced all URL strings
to ?URL?, excluded utterances with the symbols
that indicate the re-posting (RT) or quoting (QT)
of others? tweets, and erased @user name ap-
pearing at the head and tail of the utterances, since
they are usually added to make a reply. We ex-
cluded utterances given by any user whose name
included ?bot.?
We then extracted dialogues from the resulting
utterances, assuming that a series of utterances
interchangeably made by two users form a dia-
logue. We here exploited ?in reply to status id?
field of each utterance provided by Twitter REST
API to link to the other, if any, utterance to which
it replied.
3https://dev.twitter.com/docs/api/
965
# users 672,937
# dialogues 311,541,839
# unique utterances 1,007,403,858
ave. # dialogues / user 463.0
ave. # utterances / user 1497.0
ave. # utterances / dialogue 3.2
Table 1: Statistics of dialogues extracted from
Twitter.
2,000,000
40,000,000
60,000,000
80,000,000
100,000,000
120,000,000
140,000,000
160,000,000
180,000,000
0 2 3 4 5 6 7 8 9 10 11+
#
Di
alo
gu
es
Dialogue length (# utterances in dialogue)
Figure 2: The number of dialogues plotted against
the dialogue length.
Utterance Emotion
A: Would you like to go for dinner with me?
B: Sorry, I can?t. I have a fever of 38 degrees.
A: Oh dear. I hope you feel better soon. SURPRISE
B: Thanks. I?m happy to hear you say that. JOY
Table 2: An illustration of an emotion-tagged dia-
logue: The first column shows a dialogue (a series
of utterances interchangeably made by two users),
while the second column shows the addresser?s
emotion estimated from the utterance.
Table 1 lists the statistics of the extracted di-
alogues, while Figure 2 plots the number of di-
alogues plotted against the dialogue length (the
number of utterances in dialogue). Most dialogues
(98.2%) consist of at most 10 utterances, although
the longest dialogue includes 1745 utterances and
spans more than six weeks.
2.2 Tagging utterances with addressers?
emotions
We then automatically labeled utterances in the
obtained dialogues with the addressers? emotions
by using emotional expressions as clues (Table 2).
In this study, we have adopted Plutchik (1980)?s
eight emotional categories (ANGER, ANTICIPA-
TION, DISGUST, FEAR, JOY, SADNESS, SUR-
PRISE, and TRUST) as the targets to label, and
manually tailored around ten emotional expres-
sions for each emotional category. Table 3 lists
examples of the emotional expressions, while the
Emotion Emotional expressions
ANGER frustrating, irritating, nonsense
ANTICIPATION exciting, expecting, looking forward
DISGUST disgusting, unpleasant, hate
FEAR afraid, anxious, scary
JOY glad, happy, delighted
SADNESS sad, lonely, unhappy
SURPRISE surprised, oh dear, wow
TRUST relieved, reliable, solid
Table 3: Example of clue emotional expressions.
Emotion # utterances Precision
Worker A Worker B
ANGER 190,555 0.95 0.95
ANTICIPATION 2,548,706 0.99 0.99
DISGUST 475,711 0.93 0.93
FEAR 2,671,222 0.96 0.96
JOY 2,725,235 0.94 0.96
SADNESS 712,273 0.97 0.97
SURPRISE 975,433 0.97 0.97
TRUST 359,482 0.97 0.98
Table 4: Size and precision of utterances labeled
with the addressers? emotions.
rest are mostly their spelling variations.4
Because precise annotation is critical in the su-
pervised learning scenario, we annotate utterances
with the addressers? emotions only when the emo-
tional expressions do not:
1. modify content words.
2. accompany an expression of negation, condi-
tional, imperative, interrogative, concession,
or indirect speech in the same sentence.
For example, I saw a frustrated teacher is re-
jected by the first condition, while I?ll be happy
if it rains is rejected by the second condition. The
second condition was judged by checking whether
the sentence includes trigger expressions such as
??? (not/never)?, ??? (if-clause)?, ???, ???
((al)though)?, and ?? (that-clause)?.
Table 4 lists the size and precision of the utter-
ances labeled with the addressers? emotions. Two
human workers measured the precision of the an-
notation by examining 100 labeled utterances ran-
domly sampled for each emotional category. The
inter-rater agreement was ? = 0.85, indicating al-
most perfect agreement. The precision of the an-
notation exceeded 0.95 for most of the emotional
categories.
4Note that the clue emotional expressions are language-
specific but can be easily tailored for other languages. Here,
Japanese emotional expressions are translated into English to
widen the potential readership of the paper.
966
3 Predicting Addressee?s Emotion
This section describes a method for predicting
emotion elicited in an addressee when s/he re-
ceives a response to her/his utterance. The input
to this task is a pair of an utterance and a response
to it, e.g., the two utterances in Figure 1, while
the output is the addressee?s emotion among the
emotional categories of Plutchik (1980) (JOY and
SADNESS for the top and bottom dialogues in Fig-
ure 1, respectively).
Although a response could elicit multiple emo-
tions in the addressee, in this paper we focus on
predicting the most salient emotion elicited in the
addressee and cast the prediction as a single-label
multi-class classification problem.5 We then con-
struct a one-versus-the-rest classifier6 by combin-
ing eight binary classifiers, each of which predicts
whether the response elicits each emotional cate-
gory. We use online passive-aggressive algorithm
to train the eight binary classifiers.
We exploit the emotion-tagged dialogue corpus
constructed in Section 2 to collect training exam-
ples for the prediction task. For each emotion-
tagged utterance in the corpus, we assume that the
tagged emotion is elicited by the (last) response.
We thereby extract the pair of utterances preced-
ing the emotion-tagged utterance and the tagged
emotion as one training example. Taking the di-
alogue in Table 2 as an example, we obtain one
training example from the first two utterances and
SURPRISE as the emotion elicited in user A.
We extract all the n-grams (n ? 3) in the re-
sponse to induce (binary) n-gram features. The
extracted n-grams could indicate a certain action
that elicits a specific emotion (e.g., ?have a fever?
in Table 2), or a style or tone of speaking (e.g.,
?Sorry?). Likewise, we extract word n-grams from
the addressee?s utterance. The extracted n-grams
activate another set of binary n-gram features.
Because word n-grams themselves are likely to
be sparse, we estimate the addressers? emotions
from their utterances and exploit them to induce
emotion features. The addresser?s emotion has
been reported to influence the addressee?s emotion
5Because microblog posts are short, we expect emotions
elicited by a response post not to be very diverse and a multi-
class classification to be able to capture the essential crux of
the prediction task.
6We should note that a one-versus-the-rest classifier can
be used in the multi-label classification scenario, just by al-
lowing the classifier to output more than one emotional cate-
gory (Ghamrawi and McCallum, 2005).
strongly (Kim et al, 2012), while the addressee?s
emotion just before receiving a response can be a
reference to predict her/his emotion in question af-
ter receiving the response.
To induce emotion features, we exploit the rule-
based approach used in Section 2.2 to estimate
the addresser?s emotion. Since the rule-based ap-
proach annotates utterances with emotions only
when they contain emotional expressions, we in-
dependently train for each emotional category
a binary classifier that estimates the addresser?s
emotion from her/his utterance and apply it to the
unlabeled utterances. The training data for these
classifiers are the emotion-tagged utterances ob-
tained in Section 2, while the features are n-grams
(n ? 3)7 in the utterance.
We should emphasize that the features induced
from the addressee?s utterance are unique to this
task and are hardly available in the related tasks
that predicted the emotion of a reader of news ar-
ticles (Lin and Hsin-Yihn, 2008) or personal sto-
ries (Socher et al, 2011). We will later confirm the
impact of these features on the prediction accuracy
in the experiments.
4 Eliciting Addressee?s Emotion
This section presents a method for generating a re-
sponse that elicits the goal emotion, which is one
of the emotional categories of Plutchik (1980), in
the addressee. In section 4.1, we describe a statis-
tical framework for response generation proposed
by (Ritter et al, 2011). In section 4.2, we present
how to adapt the model in order to generate a
response that elicits the goal emotion in the ad-
dressee.
4.1 Statistical response generation
Following (Ritter et al, 2011), we apply the sta-
tistical machine translation model for generating a
response to a given utterance. In this framework,
a response is viewed as a translation of the input
utterance. Similar to ordinary machine translation
systems, the model is learned from pairs of an ut-
terance and a response by using off-the-shelf tools
for machine translation.
We use GIZA++8 and SRILM9 for learning
translation model and 5-gram language model, re-
7We have excluded n-grams that matched the emotional
expressions used in Section 2 to avoid overfitting.
8http://code.google.com/p/giza-pp/
9http://www.speech.sri.com/projects/
srilm/
967
spectively. As post-processing, some phrase pairs
are filtered out from the translation table as fol-
lows. When GIZA++ is directly applied to di-
alogue data, it frequently finds paraphrase pairs,
learning to parrot back the input (Ritter et al,
2011). To avoid using such pairs for response gen-
eration, a phrase pair is removed if one phrase is
the substring of the other.
We use Moses decoder10 to search for the best
response to a given utterance. Unlike machine
translation, we do not use reordering models, be-
cause the positions of phrases are not considered
to correlate strongly with the appropriateness of
responses (Ritter et al, 2011). In addition, we do
not use any discriminative training methods such
as MERT for optimizing the feature weights (Och,
2003). They are set as default values provided by
Moses (Ritter et al, 2011).
4.2 Model adaptation
The above framework allows us to generate appro-
priate responses to arbitrary input utterances. On
top of this framework, we have developed a re-
sponse generator that elicits a specific emotion.
We use the emotion-tagged dialogue corpus to
learn eight translation models and language mod-
els, each of which is specialized in generating
the response that elicits one of the eight emo-
tions (Plutchik, 1980). Specifically, the models
are learned from utterances preceding ones that are
tagged with emotional category. As an example,
let us examine to learn models for eliciting SUR-
PRISE from the dialogue in Table 2. In this case,
the first two utterances are used to learn the trans-
lation model, while only the second utterance is
used to learn the language model.
However, this simple approach is prone to suf-
fer from the data sparseness problem. Because
not all the utterances are tagged with the emotion
in emotion-tagged dialogue corpus, only a small
fraction of utterances can be used for learning the
adapted models.
We perform model interpolation for addressing
this problem. In addition to the adapted mod-
els described above, we also use a general model,
which is learned from the entire corpus. The two
models are then merged as the weighted linear in-
terpolation.
Specifically, we use tmcombine.py script
provided by Moses for the interpolation of trans-
10http://www.statmt.org/moses/
lation models (Sennrich, 2012). For all the four
features (i.e., two phrase translation probabilities
and two lexical weights) derived from transla-
tion model, the weights of the adapted model are
equally set as ? (0 ? ? ? 1.0). On the other
hand, we use SRILM for the interpolation of lan-
guage models. The weight of the adapted model is
set as ? (0 ? ? ? 1.0).
The parameters ? and ? control the strength of
the adapted models. Only adapted models are used
when ? (or ?)= 1.0, while the adapted models are
not at all used when ? (or ?) = 0. When both ?
and ? are specified as 0, the model becomes equiv-
alent to the original one described in section 4.1.
5 Experiments
5.1 Test data
To evaluate the proposed method, we built, as test
data, sets of an utterance paired with responses
that elicit a certain goal emotion (Table 5). Note
that they were used for evaluation in both of the
two tasks. Each utterance in the test data has
more than one responses that elicit the same goal
emotion, because they are used to compute BLEU
score (see section 5.3).
The data set was built in the following manner.
We first asked five human worker to produce re-
sponses to 80 utterances (10 utterances for each
goal emotion). Note that the 80 utterances do not
have overlap between workers and that the worker
produced only one response to each utterance.
To alleviate the burden on the workers, we ac-
tually provided each worker with the utterances
in the emotion-tagged corpus. Then we asked
each worker to select 80 utterances to which s/he
thought s/he could easily respond. The selected
utterances were removed from the corpus during
training.
As a result, we obtained 400 utterance-response
pairs (= 80 utterance-response pairs ? 5 work-
ers). For each of those 400 utterances, two ad-
ditional responses are produced. We did not al-
low the same worker to produce more than one
response to the same utterance. In this way, we
obtained 1200 responses for the 400 utterances in
total.
Finally, we assessed the data quality to remove
responses that were unlikely to elicit the goal emo-
tion. For each utterance-response pair, we asked
two workers to judge whether the response elicited
the goal emotion. If both workers regarded the
968
Goal emotion: JOY
U: 16???????????????????
?????
(I?m turning 16. Hope to get alng with you as
well as ever!)
R1:??????????????
(Happy birthday!)
R2:?????????????????????
(Congratulations! I?ll give you a birthday present.)
R3:???????????????
(Congratulations! I hope you have a happy year!)
Table 5: Example of the test data. English transla-
tions are attached in the parenthesis.
Emotion # utterance pairs
ANGER 119,881
ANTICIPATION 1,416,847
DISGUST 333,972
FEAR 1,662,998
JOY 1,724,198
SADNESS 436,668
SURPRISE 589,790
TRUST 228,974
GENERAL 646,429,405
Table 6: The number of utterance pairs used
for training classifiers in emotion prediction and
learning the translation models and language mod-
els in response generation.
response as inappropriate, it was removed from
the data. The resulting test data consist of 1099
utterance-response pairs for 396 utterances.
This data set is submitted as supplementary ma-
terial to support the reproducibility of our experi-
mental results.
5.2 Prediction task
We first report experimental results on predicting
the addressee?s emotion within a dialogue. Table 6
lists the number of utterance-response pairs used
to train eight binary classifiers for individual emo-
tional categories, which form a one-versus-the rest
classifier for the prediction task. We used opal11
as an implementation of online passive-aggressive
algorithm to train the individual classifiers.
To investigate the impact of the features that are
uniquely available in a dialogue data, we com-
pared classifiers trained with the following two
sets of features in terms of precision, recall, and
F1 for each emotional category.
RESPONSE The n-gram and emotion features in-
duced from the response.
11http://www.tkl.iis.u-tokyo.ac.jp/
?ynaga/opal/.
Emotion RESPONSE RESPONSE/UTTER.
PREC REC F1 PREC REC F1
ANGER 0.455 0.476 0.465 0.600 0.548 0.573
ANTICIPA. 0.518 0.526 0.522 0.614 0.637 0.625
DISGUST 0.275 0.519 0.359 0.378 0.511 0.435
FEAR 0.484 0.727 0.581 0.459 0.706 0.556
JOY 0.690 0.417 0.519 0.720 0.590 0.649
SADNESS 0.711 0.467 0.564 0.670 0.562 0.611
SURPRISE 0.511 0.348 0.414 0.584 0.437 0.500
TRUST 0.695 0.452 0.548 0.682 0.514 0.586
average 0.542 0.492 0.497 0.588 0.563 0.567
Table 7: Predicting addressee?s emotion: Results.
PREDICTED EMOTION
AN
GE
R
AN
TI
CI
PA
.
DI
SG
US
T
FE
AR
JO
Y
SA
DN
ES
S
SU
RP
RI
SE
TR
US
T
tot
al
ANGER 69 0 26 20 0 8 2 1 126
ANTICIPA. 1 86 11 7 13 0 6 11 135
DISGUST 25 1 68 18 2 8 7 4 133
FEAR 3 0 22 101 1 5 9 2 143
JOY 1 28 9 4 85 1 7 9 144
SADNESS 6 3 25 14 5 77 5 2 137
SURPRISE 7 10 9 32 5 7 59 6 135
TRUST 3 12 10 24 7 9 6 75 146CO
RR
EC
T
EM
OT
IO
N
total 115 140 180 220 118 115 101 110 1099
Table 8: Confusion matrix of predicting ad-
dressee?s emotion, with mostly predicted emo-
tions bold-faced and mostly confused emotions
underlined for each emotional category.
RESPONSE/UTTER. The n-gram and emotion
features induced from the response and the
addressee?s utterance.
Table 7 lists prediction results. We can see that
the features induced from the addressee?s utter-
ance significantly improved the prediction perfor-
mance, F1, for emotions other than FEAR. FEAR is
elicited instantly by the response, and the features
induced from the addressee?s utterance thereby
confused the classifier.
Table 8 shows a confusion matrix of the classi-
fier using all the features, with mostly predicted
emotions bold-faced and mostly confused emo-
tions underlined for each emotional category. We
can find some typical confusing pairs of emotions
from this matrix. The classifier confuses DISGUST
with ANGER and vice versa, while it confuses JOY
with ANTICIPATION. These confusions conform
to our expectation, since they are actually similar
emotions. The classifier was less likely to confuse
positive emotions (JOY and ANTICIPATION) with
negative emotion (ANGER, DISGUST, FEAR, and
SADNESS) vice versa.
969
Goal emotion: ANGER (predicted as SADNESS)
U:????????????????
(You have phone calls every day, I envy you.)
R:????????????????????????
(I envy you have a lot of time ?cause no one calls you.)
Goal emotion: SURPRISE (predicted as FEAR)
U:????????????
(Is it true that dark-haired girls are popular with boys?)
R:???????????????????
(About 80% of boys seem to prefer dark-haired girls.)
Table 9: Examples of utterance-response pairs to
which the system predicted wrong emotions.
We have briefly examined the confusions and
found the two major types of errors, each of which
is exemplified in Table 9. The first (top) one is sar-
casm or irony, which has been reported to be diffi-
cult to capture by lexical features alone (Gonza?lez-
Iba?n?ez et al, 2011). The other (bottom) one is due
to lack of information. In this example, only if the
addressee does not know the fact provided by the
response, s/he will surprise at it.
5.3 Generation task
We next demonstrate the experimental results for
eliciting the emotion of the addressee.
We use the utterance pairs summarized in Ta-
ble 6 to learn the translation models and language
models for eliciting each emotional category. We
also use the 640 million utterances pairs in the
entire emotion-tagged corpus for learning general
models. However, for learning the general transla-
tion models, we currently use 4 millions of utter-
ance pairs sampled from the 640 millions of pairs
due to the computational limitation.
Automatic evaluation
We first use BLEU score (Papineni et al, 2002)
to perform automatic evaluation (Ritter et al,
2011). In this evaluation, the system is pro-
vided with the utterance and the goal emotion
in the test data and the generated responses are
evaluated through BLEU score. Specifically, we
conducted two-fold cross-validation to optimize
the weights of our method. We tried ? and
? in {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} and selected the
weights that achieved the best BLEU score. Note
that we adopted different values of the weights for
different emotional categories.
Table 10 compares BLEU scores of three meth-
ods including the proposed one. The first row
represents a method that does not perform model
adaptation at all. It corresponds to the special case
System BLEU
NO ADAPTATION 0.64
PROPOSED 1.05
OPTIMAL 1.57
Table 10: Comparison of BLEU scores.
(i.e., ? = ? = 0.0) of the proposed method. The
second row represents our method, while the last
row represents the result of our method when the
weights are set as optimal, i.e., those achieving the
best BLEU on the test data. This result can be con-
sidered as an upper bound on BLEU score.
The results demonstrate that model adaptation
is useful for generating the responses that elicit
the goal emotion. We can clearly observe the im-
provement in the BLEU from 0.64 to 1.05.
On the other hand, there still remains a gap be-
tween the last two rows (i.e., proposed and opti-
mal). We think this is partly because the current
test data is too small to reliably tune parameters.
Human evaluation
We next asked two human workers to manually
evaluate the generation results.
In this evaluation, the baseline (no adaptation
in Table 10) and proposed method generated a re-
sponse for each of the 396 utterances in the test
data. For the resulting 792 utterance-response
pairs, the two workers manually assessed the ap-
propriateness of the response. Each response was
judged whether it is grammatical and meaningful.
If the response was regarded as so by either of the
workers, it was further judged whether it elicits the
goal emotion or not. To make the comparison fair,
we did not expose to the workers which system
generated the response. In addition, the responses
generated by the two systems were presented in a
random order.
As the result, 147 and 157 responses of the
baseline and proposed method were regarded as
appropriate, i.e., ecliting the goal emotion, by ei-
ther of the workers; 74 and 92 responses were
regarded as appropirate by both of the workers.
These results suggest the effectiveness of the pro-
posed method. Especially, we can confirm that
the proposed method can generate responses that
elicit addresee?s emotion more clearly. We inves-
tigated the agreement between the two workers in
this evaluation. We found that the ? coefficient is
0.59, which indicates moderate agreement. This
supports the reliability of our evaluation.
970
Goal emotion: JOY
Input: ???????????????????2? 7?????
(I wooooon the outstanding award at the photo competition! The ceremony is on Feb. 7!)
NO ADAPTATION: ?????????????? (Sorry to say, only the first day.)
PROPOSED: ????????????????????
(Congratulations on winning the gold prize!!! Congrats.)
Goal emotion: TRUST
Input: ???????????? (Do you get desperate? )
NO ADAPTATION: ???? (I?m looking forward to it!)
PROPOSED: ???????? (Maybe still OK.)
Goal emotion: ANTICIPATION
Input: ?????????????ww
(Huh! It?s gonna be all right! lol)
?????????????????????????????????? (???)
(I gotta buy the goods, so I?ll be glad if you can take the time :-))
NO ADAPTATION: ????????????????? (Since I?ve not bought it, I feel worried.)
PROPOSED: ???????????????? (Good! I?ll buy it too!!!)
Table 11: Examples of the responses generated by the two systems, NO ADAPTATION and PROPOSED.
Examples
Table 11 illustrates examples of the responses gen-
erated by the no adaptation baseline and proposed
method. In the first two examples, the proposed
method successfully generates responses that elicit
the goal emotions: JOY and TRUST. From these
examples, we can consider that the adapted model
assigns large probability to phrases such as con-
gratulations or OK. In the last example, the sys-
tem also succeeded in eliciting the goal emotion:
ANTICIPATION. For this example, we can interpret
that the speaker of the response (i.e., the system)
feels anticipation, and consequently the emotion
of the addressee is affected by the emotion of the
speaker (i.e., the system). Interestingly, a similar
phenomenon is also observed in real conversation
(Kim et al, 2012).
6 Related Work
There have been a tremendous amount of stud-
ies on predicting the emotion from text or speech
data (Ayadi et al, 2011; Bandyopadhyay and Oku-
mura, 2011; Balahur et al, 2011; Balahur et al,
2012). Unlike our prediction task, most of them
have exclusively focused on estimating the emo-
tion of a speaker (or writer) from her/his utterance
(or writing).
Analogous to our prediction task, Lin and Hsin-
Yihn (2008) and Socher et al (2011) investigated
predicting the emotion of a reader from the text
that s/he reads. Our work differs from them in that
we focus on dialogue data, and we exploit fea-
tures that are not available within their task set-
tings, e.g., the addressee?s previous utterance.
Tokuhisa et al (2008) proposed a method for
extracting pairs of an event (e.g., It rained sud-
denly when I went to see the cherry blossoms) and
an emotion elicited by it (e.g., SADNESS) from the
Web text. The extracted data are used for emotion
classification. A similar technique would be use-
ful for prediction the emotion of an addressee as
well.
Response generation has a long research history
(Weizenbaum, 1966), although it is only very re-
cently that a fully statistical approach was intro-
duced in this field (Ritter et al, 2011). At this mo-
ment, we are unaware of any statistical response
generators that model the emotion of the user.
Some researchers have explored generating
jokes or humorous text (Dybala et al, 2010;
Labtov and Lipson, 2012). Those attempts are
similar to our work in that they also aim at elic-
iting a certain emotion in the addressee. They are,
however, restricted to elicit a specific emotion.
The linear interpolation of translation and/or
language models is a widely-used technique for
adapting machine translation systems to new do-
mains (Sennrich, 2012). However, it has not been
touched in the context of response generation.
7 Conclusion and Future Work
In this paper, we have explored predicting and
eliciting the emotion of an addressee by using a
large amount of dialogue data obtained from mi-
croblog posts. In the first attempt to model the
emotion of an addressee in the field of NLP, we
demonstrated that the response of the dialogue
partner and the previous utterance of the addressee
are useful for predicting the emotion. In the gen-
eration task, on the other hand, we showed that the
971
model adaptation approach successfully generates
the responses that elicit the goal emotion.
For future work, we want to use longer dialogue
history in both tasks. While we considered only
two utterances as a history, a longer history would
be helpful. We also plan to personalize the pro-
posed methods, exploiting microblog posts made
by users of a certain age, gender, occupation, or
even character to perform model adaptation.
Acknowledgment
This work was supported by the FIRST program of
JSPS. The authors thank the anonymous review-
ers for their valuable comments. The authors also
thank the student annotators for their hard work.
References
Moataz El Ayadi, Mohamed S. Kamel, and Fakhri Kar-
ray. 2011. Survey on speech emotion recognition:
Features, classification schemes, and databases.
Pattern Recognition, 44:572?587.
Alexandra Balahur, Ester Boldrini, Andres Montoyo,
and Patricio Martinez-Barco, editors. 2011. Pro-
ceedings of the 2nd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis.
Association for Computational Linguistics.
Alexandra Balahur, Andres Montoyo, Patricio Mar-
tinez Barco, and Ester Boldrini, editors. 2012. Pro-
ceedings of the 3rd Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis.
Association for Computational Linguistics.
Sivaji Bandyopadhyay and Manabu Okumura, editors.
2011. Proceedings of the Workshop on Sentiment
Analysis where AI meets Psychology. Asian Federa-
tion of Natural Language Processing.
Pawel Dybala, Michal Ptaszynski, Jacek Maciejewski,
Mizuki Takahashi, Rafal Rzepka, and Kenji Araki.
2010. Multiagent system for joke generation: Hu-
mor and emotions combined in human-agent conver-
sation. Journal of Ambient Intelligence and Smart
Environments, 2(1):31?48.
Kate Forbes-Riley and Diane J. Litman. 2004. Pre-
dicting emotion in spoken dialogue from multiple
knowledge sources. In Proceedings of NAACL,
pages 201?208.
Nadia Ghamrawi and Andrew McCallum. 2005. Col-
lective multi-label classification. In Proceedings of
CIKM, pages 195?200.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of ACL, pages
581?586.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. HMS: A predictive text
entry method using bigrams. In Proceedings of
EACL Workshop on Language Modeling for Text En-
try Methods, pages 43?50.
Suin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012.
Do you feel what I feel? social aspects of emotions
in Twitter conversations. In Proceedings of ICWSM,
pages 495?498.
Igor Labtov and Hod Lipson. 2012. Humor as circuits
in semantic networks. In Proceedings of ACL (Short
Papers), pages 150?155.
Kevin Lin and Hsin-Hsi Hsin-Yihn. 2008. Ranking
reader emotions using pairwise loss minimization
and emotional distribution regression. In Proceed-
ings of EMNLP, pages 136?144.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Bo Pang and Sujith Ravi. 2012. Revisiting the pre-
dictability of language: Response completion in so-
cial media. In Proceedings of EMNLP, pages 1489?
1499.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. In Emotion: Theory, research,
and experience: Vol. 1. Theories of emotion, pages
3?33. New York: Academic.
Alan Ritter, Colin Cherry, andWilliam B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of EMNLP, pages 583?593.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of EACL, pages
539?549.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151?161.
Ellen Spertus. 1997. Smokey: Automatic recognition
of hostile messages. In Proceedings of IAAI, pages
1058?1065.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive exam-
ples extracted from the Web. In Proceedings of
COLING, pages 881?888.
JosephWeizenbaum. 1966. ELIZA? a computer pro-
gram for the study of natural language communica-
tion between man and machine. Communications of
the ACM, 9(1):36?45.
972
