Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 22?32, Prague, June 2007. c?2007 Association for Computational Linguistics
What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA
Mengqiu Wang and Noah A. Smith and Teruko Mitamura
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mengqiu,nasmith,teruko}@cs.cmu.edu
Abstract
This paper presents a syntax-driven ap-
proach to question answering, specifically
the answer-sentence selection problem for
short-answer questions. Rather than us-
ing syntactic features to augment exist-
ing statistical classifiers (as in previous
work), we build on the idea that ques-
tions and their (correct) answers relate to
each other via loose but predictable syntac-
tic transformations. We propose a prob-
abilistic quasi-synchronous grammar, in-
spired by one proposed for machine trans-
lation (D. Smith and Eisner, 2006), and pa-
rameterized by mixtures of a robust non-
lexical syntax/alignment model with a(n
optional) lexical-semantics-driven log-linear
model. Our model learns soft alignments as
a hidden variable in discriminative training.
Experimental results using the TREC dataset
are shown to significantly outperform strong
state-of-the-art baselines.
1 Introduction and Motivation
Open-domain question answering (QA) is a widely-
studied and fast-growing research problem. State-
of-the-art QA systems are extremely complex. They
usually take the form of a pipeline architecture,
chaining together modules that perform tasks such
as answer type analysis (identifying whether the
correct answer will be a person, location, date,
etc.), document retrieval, answer candidate extrac-
tion, and answer reranking. This architecture is so
predominant that each task listed above has evolved
into its own sub-field and is often studied and evalu-
ated independently (Shima et al, 2006).
At a high level, the QA task boils down to only
two essential steps (Echihabi andMarcu, 2003). The
first step, retrieval, narrows down the search space
from a corpus of millions of documents to a fo-
cused set of maybe a few hundred using an IR en-
gine, where efficiency and recall are the main fo-
cus. The second step, selection, assesses each can-
didate answer string proposed by the first step, and
finds the one that is most likely to be an answer
to the given question. The granularity of the tar-
get answer string varies depending on the type of
the question. For example, answers to factoid ques-
tions (e.g., Who, When, Where) are usually single
words or short phrases, while definitional questions
and other more complex question types (e.g., How,
Why) look for sentences or short passages. In this
work, we fix the granularity of an answer to a single
sentence.
Earlier work on answer selection relies only on
the surface-level text information. Two approaches
are most common: surface pattern matching, and
similarity measures on the question and answer, rep-
resented as bags of words. In the former, pat-
terns for a certain answer type are either crafted
manually (Soubbotin and Soubbotin, 2001) or ac-
quired from training examples automatically (Itty-
cheriah et al, 2001; Ravichandran et al, 2003;
Licuanan and Weischedel, 2003). In the latter,
measures like cosine-similarity are applied to (usu-
ally) bag-of-words representations of the question
and answer. Although many of these systems have
achieved very good results in TREC-style evalua-
tions, shallow methods using the bag-of-word repre-
sentation clearly have their limitations. Examples of
22
cases where the bag-of-words approach fails abound
in QA literature; here we borrow an example used by
Echihabi and Marcu (2003). The question is ?Who
is the leader of France??, and the sentence ?Henri
Hadjenberg, who is the leader of France ?s Jewish
community, endorsed ...? (note tokenization), which
is not the correct answer, matches all keywords in
the question in exactly the same order. (The cor-
rect answer is found in ?Bush later met with French
President Jacques Chirac.?)
This example illustrates two types of variation
that need to be recognized in order to connect this
question-answer pair. The first variation is the
change of the word ?leader? to its semantically re-
lated term ?president?. The second variation is the
syntactic shift from ?leader of France? to ?French
president.? It is also important to recognize that
?France? in the first sentence is modifying ?com-
munity?, and therefore ?Henri Hadjenberg? is the
?leader of ... community? rather than the ?leader of
France.? These syntactic and semantic variations oc-
cur in almost every question-answer pair, and typi-
cally they cannot be easily captured using shallow
representations. It is also worth noting that such
syntactic and semantic variations are not unique to
QA; they can be found in many other closely related
NLP tasks, motivating extensive community efforts
in syntactic and semantic processing.
Indeed, in this work, we imagine a generative
story for QA in which the question is generated
from the answer sentence through a series of syn-
tactic and semantic transformations. The same story
has been told for machine translation (Yamada and
Knight, 2001, inter alia), in which a target language
sentence (the desired output) has undergone seman-
tic transformation (word to word translation) and
syntactic transformation (syntax divergence across
languages) to generate the source language sen-
tence (noisy-channel model). Similar stories can
also be found in paraphrasing (Quirk et al, 2004;
Wu, 2005) and textual entailment (Harabagiu and
Hickl, 2006; Wu, 2005).
Our story makes use of a weighted formalism
known as quasi-synchronous grammar (hereafter,
QG), originally developed by D. Smith and Eisner
(2006) for machine translation. Unlike most syn-
chronous formalisms, QG does not posit a strict iso-
morphism between the two trees, and it provides
an elegant description for the set of local configura-
tions. In Section 2 we situate our contribution in the
context of earlier work, and we give a brief discus-
sion of quasi-synchronous grammars in Section 3.
Our version of QG, called the Jeopardy model, and
our parameter estimation method are described in
Section 4. Experimental results comparing our ap-
proach to two state-of-the-art baselines are presented
in Section 5. We discuss portability to cross-lingual
QA and other applied semantic processing tasks in
Section 6.
2 Related Work
To model the syntactic transformation process, re-
searchers in these fields?especially in machine
translation?have developed powerful grammatical
formalisms and statistical models for representing
and learning these tree-to-tree relations (Wu and
Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed,
2004; Ding and Palmer, 2005; Quirk et al, 2005;
Galley et al, 2006; Smith and Eisner, 2006, in-
ter alia). We can also observe a trend in recent work
in textual entailment that more emphasis is put on
explicit learning of the syntactic graph mapping be-
tween the entailed and entailed-by sentences (Mac-
Cartney et al, 2006).
However, relatively fewer attempts have been
made in the QA community. As pointed out by
Katz and Lin (2003), most early experiments in
QA that tried to bring in syntactic or semantic
features showed little or no improvement, and it
was often the case that performance actually de-
graded (Litkowski, 1999; Attardi et al, 2001). More
recent attempts have tried to augment the bag-of-
words representation?which, after all, is simply a
real-valued feature vector?with syntactic features.
The usual similarity measures can then be used on
the new feature representation. For example, Pun-
yakanok et al (2004) used approximate tree match-
ing and tree-edit-distance to compute a similarity
score between the question and answer parse trees.
Similarly, Shen et al (2005) experimented with de-
pendency tree kernels to compute similarity between
parse trees. Cui et al (2005) measured sentence
similarity based on similarity measures between de-
pendency paths among aligned words. They used
heuristic functions similar to mutual information to
23
assign scores to matched pairs of dependency links.
Shen and Klakow (2006) extend the idea further
through the use of log-linear models to learn a scor-
ing function for relation pairs.
Echihabi and Marcu (2003) presented a noisy-
channel approach in which they adapted the IBM
model 4 from statistical machine translation (Brown
et al, 1990; Brown et al, 1993) and applied it to QA.
Similarly, Murdock and Croft (2005) adopted a sim-
ple translation model from IBM model 1 (Brown et
al., 1990; Brown et al, 1993) and applied it to QA.
Porting the translation model to QA is not straight-
forward; it involves parse-tree pruning heuristics
(the first two deterministic steps in Echihabi and
Marcu, 2003) and also replacing the lexical trans-
lation table with a monolingual ?dictionary? which
simply encodes the identity relation. This brings us
to the question that drives this work: is there a statis-
tical translation-like model that is natural and accu-
rate for question answering? We propose Smith and
Eisner?s (2006) quasi-synchronous grammar (Sec-
tion 3) as a general solution and the Jeopardy model
(Section 4) as a specific instance.
3 Quasi-Synchronous Grammar
For a formal description of QG, we recommend
Smith and Eisner (2006). We briefly review the cen-
tral idea here. QG arose out of the empirical obser-
vation that translated sentences often have some iso-
morphic syntactic structure, but not usually in en-
tirety, and the strictness of the isomorphism may
vary across words or syntactic rules. The idea is that,
rather than a synchronous structure over the source
and target sentences, a tree over the target sentence
is modeled by a source-sentence-specific grammar
that is inspired by the source sentence?s tree.1 This
is implemented by a ?sense??really just a subset
of nodes in the source tree?attached to each gram-
mar node in the target tree. The senses define an
alignment between the trees. Because it only loosely
links the two sentences? syntactic structure, QG is
particularly well-suited for QA insofar as QA is like
?free? translation.
A concrete example that is easy to understand
is a binary quasi-synchronous context-free grammar
1Smith and Eisner also show how QG formalisms generalize
synchronous grammar formalisms.
(denoted QCFG). Let VS be the set of constituent to-
kens in the source tree. QCFG rules would take the
augmented form
?X, S1? ? ?Y, S2??Z, S3?
?X, S1? ? w
where X,Y, and Z are ordinary CFG nonterminals,
each Si ? 2VS (subsets of nodes in the source tree
to which the nonterminals align), and w is a target-
language word. QG can be made more or less ?lib-
eral? by constraining the cardinality of the Si (we
force all |Si| = 1), and by constraining the relation-
ships among the Si mentioned in a single rule. These
are called permissible ?configurations.? An example
of a strict configuration is that a target parent-child
pair must align (respectively) to a source parent-
child pair. Configurations are shown in Table 1.
Here, following Smith and Eisner (2006), we use
a weighted, quasi-synchronous dependency gram-
mar. Apart from the obvious difference in appli-
cation task, there are a few important differences
with their model. First, we are not interested in the
alignments per se; we will sum them out as a hid-
den variable when scoring a question-answer pair.
Second, our probability model includes an optional
mixture component that permits arbitrary features?
we experiment with a small set of WordNet lexical-
semantics features (see Section 4.4). Third, we ap-
ply a more discriminative training method (condi-
tional maximum likelihood estimation, Section 4.5).
4 The Jeopardy Model
Our model, informally speaking, aims to follow the
process a player of the television game show Jeop-
ardy! might follow. The player knows the answer
(or at least thinks he knows the answer) and must
quickly turn it into a question.2 The question-answer
pairs used on Jeopardy! are not precisely what we
have in mind for the real task (the questions are not
specific enough), but the syntactic transformation in-
spires our model. In this section we formally define
2A round of Jeopardy! involves a somewhat involved and
specific ?answer? presented to the competitors, and the first
competitor to hit a buzzer proposes the ?question? that leads to
the answer. For example, an answer might be, This Eastern Eu-
ropean capital is famous for defenestrations. In Jeopardy! the
players must respond with a queston: What is Prague?
24
this probability model and present the necessary al-
gorithms for parameter estimation.
4.1 Probabilistic Model
The Jeopardy model is a QG designed for QA. Let
q = ?q1, ..., qn? be a question sentence (each qi is a
word), and let a = ?a1, ..., am? be a candidate an-
swer sentence. (We will use w to denote an abstract
sequence that could be a question or an answer.) In
practice, these sequences may include other infor-
mation, such as POS, but for clarity we assume just
words in the exposition. Let A be the set of can-
didate answers under consideration. Our aim is to
choose:
a? = argmax
a?A
p(a | q) (1)
At a high level, we make three adjustments. The
first is to apply Bayes? rule, p(a | q) ? p(q |
a) ? p(a). Because A is known and is assumed to
be generated by an external extraction system, we
could use that extraction system to assign scores
(and hence, probabilities p(a)) to the candidate an-
swers. Other scores could also be used, such as
reputability of the document the answer came from,
grammaticality, etc. Here, aiming for simplicity, we
do not aim to use such information. Hence we treat
p(a) as uniform over A.3
The second adjustment adds a labeled, directed
dependency tree to the question and the answer.
The tree is produced by a state-of-the-art depen-
dency parser (McDonald et al, 2005) trained on
the Wall Street Journal Penn Treebank (Marcus et
al., 1993). A dependency tree on a sequence w =
?w1, ..., wk? is a mapping of indices of words to in-
dices of their syntactic parents and a label for the
syntactic relation, ? : {1, ..., k} ? {0, ..., k} ? L.
Each word wi has a single parent, denoted w?(i).par .
Cycles are not permitted. w0 is taken to be the invis-
ible ?wall? symbol at the left edge of the sentence; it
has a single child (|{i : ?(i) = 0}| = 1). The label
for wi is denoted ?(i).lab.
The third adjustment involves a hidden variable
X , the alignment between question and answer
3The main motivation for modeling p(q | a) is that it is eas-
ier to model deletion of information (such as the part of the sen-
tence that answers the question) than insertion. Our QG does
not model the real-world knowledge required to fill in an an-
swer; its job is to know what answers are likely to look like,
syntactically.
words. In our model, each question-word maps to
exactly one answer-word. Let x : {1, ..., n} ?
{1, ...,m} be a mapping from indices of words in q
to indices of words in a. (It is for computational rea-
sons that we assume |x(i)| = 1; in general x could
range over subsets of {1, ...,m}.) Because we de-
fine the correspondence in this direction, note that it
is possible for multple question words to map to the
same answer word.
Why do we treat the alignmentX as a hidden vari-
able? In prior work, the alignment is assumed to be
known given the sentences, but we aim to discover
it from data. Our guide in this learning is the struc-
ture inherent in the QG: the configurations between
parent-child pairs in the question and their corre-
sponding, aligned words in the answer. The hidden
variable treatment lets us avoid commitment to any
one x mapping, making the method more robust to
noisy parses (after all, the parser is not 100% ac-
curate) and any wrong assumptions imposed by the
model (that |x(i)| = 1, for example, or that syntactic
transformations can explain the connection between
q and a at all).4
Our model, then, defines
p(q, ?q | a, ?a) =
?
x
p(q, ?q, x | a, ?a) (2)
where ?q and ?a are the question tree and answer
tree, respectively. The stochastic process defined by
our model factors cleanly into recursive steps that
derive the question from the top down. The QG de-
fines a grammar for this derivation; the grammar de-
pends on the specific answer.
Let ? iw refer to the subtree of ?w rooted at wi. The
model is defined by:
p(? iq | qi, ?q(i), x(i), ?a) = (3)
p#kids(|{j : ?q(j) = i, j < i}| | qi, left)
?p#kids(|{j : ?q(j) = i, j > i}| | qi, right)
?
?
j:?q(j)=i
m?
x(j)=0
pkid (qj , ?q(j).lab | qi, ?q(i), x(i), x(j), ?a)
?p(? jq | qj , ?q(j), x(j), ?a)
4If parsing performance is a concern, we might also treat the
question and/or answer parse trees as hidden variables, though
that makes training and testing more computationally expen-
sive.
25
Note the recursion in the last line. While the above
may be daunting, in practice it boils down only to
defining the conditional distribution pkid , since the
number of left and right children of each node need
not be modeled (the trees are assumed known)?
p#kids is included above for completeness, but in the
model applied here we do not condition it on qi and
therefore do not need to estimate it (since the trees
are fixed).
pkid defines a distribution over syntactic children
of qi and their labels, given (1) the word qi, (2) the
parent of qi, (3) the dependency relation between
qi and its parent, (4) the answer-word qi is aligned
to, (5) the answer-word the child being predicted is
aligned to, and (6) the remainder of the answer tree.
4.2 Dynamic Programming
Given q, the score for an answer is simply p(q, ?q |
a, ?a). Computing the score requires summing over
alignments and can be done efficiently by bottom-up
dynamic programming. Let S(j, `) refer to the score
of ? jq, assuming that the parent of qj , ?q(j).par , is
aligned to a`. The base case, for leaves of ?q, is:
S(j, `) = (4)
p#kids(0 | qj , left) ? p#kids(0 | qj , right)
?
m?
k=0
pkid (qj , ?q(j).lab | q?q(j) , `, k, ?a)
Note that k ranges over indices of answer-words to
be aligned to qj . The recursive case is
S(i, `) = (5)
p#kids(|{j : ?q(j) = i, j < i}| | qj , left)
?p#kids(|{j : ?q(j) = i, j > i}| | qj , right)
?
m?
k=0
pkid (qi, ?q(i).lab | q?q(i), `, k, ?a)
?
?
j:?q(j)=i
S(j, k)
Solving these equations bottom-up can be done
in O(nm2) time and O(nm) space; in practice this
is very efficient. In our experiments, computing the
value of a question-answer pair took two seconds on
average.5 We turn next to the details of pkid , the core
of the model.
4.3 Base Model
Our base model factors pkid into three conditional
multinomial distributions.
pbasekid (qi, ?q(i).lab | q?q(i), `, k, ?a) =
p(qi.pos | ak.pos) ? p(qi.ne | ak.ne)
?p(?q(i).lab | config(?q, ?a, i)) (6)
where qi.pos is question-word i?s POS label and
qi.ne is its named-entity label. config maps
question-word i, its parent, and their alignees to
a QG configuration as described in Table 1; note
that some configurations are extended with addi-
tional tree information. The base model does not
directly predict the specific words in the question?
only their parts-of-speech, named-entity labels, and
dependency relation labels. This model is very sim-
ilar to Smith and Eisner (2006).
Because we are interested in augmenting the QG
with additional lexical-semantic knowledge, we also
estimate pkid by mixing the base model with a
model that exploits WordNet (Miller et al, 1990)
lexical-semantic relations. The mixture is given by:
pkid (? | ?) = ?p
base
kid (? | ?)+(1??)p
ls
kid (? | ?) (7)
4.4 Lexical-Semantics Log-Linear Model
The lexical-semantics model plskid is defined by pre-
dicting a (nonempty) subset of the thirteen classes
for the question-side word given the identity of
its aligned answer-side word. These classes in-
clude WordNet relations: identical-word, synonym,
antonym (also extended and indirect antonym), hy-
pernym, hyponym, derived form, morphological
variation (e.g., plural form), verb group, entailment,
entailed-by, see-also, and causal relation. In ad-
dition, to capture the special importance of Wh-
words in questions, we add a special semantic re-
lation called ?q-word? between any word and any
Wh-word. This is done through a log-linear model
with one feature per relation. Multiple relations may
fire, motivating the log-linear model, which permits
?overlapping? features, and, therefore prediction of
5Experiments were run on a 64-bit machine with 2? 2.2GHz
dual-core CPUs and 4GB of memory.
26
any of the possible 213 ? 1 nonempty subsets. It
is important to note that this model assigns zero
probability to alignment of an answer-word with
any question-word that is not directly related to it
through any relation. Such words may be linked in
the mixture model, however, via pbasekid .
6
(It is worth pointing out that log-linear models
provide great flexibility in defining new features. It
is straightforward to extend the feature set to include
more domain-specific knowledge or other kinds of
morphological, syntactic, or semantic information.
Indeed, we explored some additional syntactic fea-
tures, fleshing out the configurations in Table 1 in
more detail, but did not see any interesting improve-
ments.)
parent-child Question parent-child pair align respec-
tively to answer parent-child pair. Aug-
mented with the q.-side dependency la-
bel.
child-parent Question parent-child pair align respec-
tively to answer child-parent pair. Aug-
mented with the q.-side dependency la-
bel.
grandparent-child Question parent-child pair align respec-
tively to answer grandparent-child pair.
Augmented with the q.-side dependency
label.
same node Question parent-child pair align to the
same answer-word.
siblings Question parent-child pair align to sib-
lings in the answer. Augmented with
the tree-distance between the a.-side sib-
lings.
c-command The parent of one answer-side word is
an ancestor of the other answer-side
word.
other A catch-all for all other types of config-
urations, which are permitted.
Table 1: Syntactic alignment configurations are par-
titioned into these sets for prediction under the Jeop-
ardy model.
4.5 Parameter Estimation
The parameters to be estimated for the Jeopardy
model boil down to the conditional multinomial
distributions in pbasekid , the log-linear weights in-
side of plskid , and the mixture coefficient ?.
7 Stan-
6It is to preserve that robustness property that the models are
mixed, and not combined some other way.
7In our experiments, all log-linear weights are initialized to
be 1; all multinomial distributions are initialized as uniform dis-
dard applications of log-linear models apply con-
ditional maximum likelihood estimation, which for
our case involves using an empirical distribution p?
over question-answer pairs (and their trees) to opti-
mize as follows:
max
?
?
q,?q,a,?a
p?(q, ?q,a, ?a) log p?(q, ?q | a, ?a)
? ?? ?
P
x p?(q,?q,x|a,?a)
(8)
Note the hidden variable x being summed out; that
makes the optimization problem non-convex. This
sort of problem can be solved in principle by condi-
tional variants of the Expectation-Maximization al-
gorithm (Baum et al, 1970; Dempster et al, 1977;
Meng and Rubin, 1993; Jebara and Pentland, 1999).
We use a quasi-Newton method known as L-BFGS
(Liu and Nocedal, 1989) that makes use of the gra-
dient of the above function (straightforward to com-
pute, but omitted for space).
5 Experiments
To evaluate our model, we conducted experiments
using Text REtrieval Conference (TREC) 8?13 QA
dataset.8
5.1 Experimental Setup
The TREC dataset contains questions and answer
patterns, as well as a pool of documents returned by
participating teams. Our task is the same as Pun-
yakanok et al (2004) and Cui et al (2005), where
we search for single-sentence answers to factoid
questions. We follow a similar setup to Shen and
Klakow (2006) by automatically selecting answer
candidate sentences and then comparing against a
human-judged gold standard.
We used the questions in TREC 8?12 for training
and set aside TREC 13 questions for development
(84 questions) and testing (100 questions). To gen-
erate the candidate answer set for development and
testing, we automatically selected sentences from
each question?s document pool that contains one or
more non-stopwords from the question. For gen-
erating the training candidate set, in addtion to the
sentences that contain non-stopwords from the ques-
tion, we also added sentences that contain correct
tributions; ? is initialized to be 0.1.
8We thank the organizers and NIST for making the dataset
publicly available.
27
answer pattern. Manual judgement was produced
for the entire TREC 13 set, and also for the first 100
questions from the training set TREC 8?12.9 On av-
erage, each question in the development set has 3.1
positive and 17.1 negative answers. There are 3.6
positive and 20.0 negative answers per question in
the test set.
We tokenized sentences using the standard tree-
bank tokenization script, and then we performed
part-of-speech tagging using MXPOST tagger (Rat-
naparkhi, 1996). The resulting POS-tagged sen-
tences were then parsed using MSTParser (McDon-
ald et al, 2005), trained on the entire Penn Treebank
to produce labeled dependency parse trees (we used
a coarse dependency label set that includes twelve
label types). We used BBN Identifinder (Bikel et al,
1999) for named-entity tagging.
As answers in our task are considered to be sin-
gle sentences, our evaluation differs slightly from
TREC, where an answer string (a word or phrase
like 1977 or George Bush) has to be accompanied
by a supporting document ID. As discussed by Pun-
yakanok et al (2004), the single-sentence assump-
tion does not simplify the task, since the hardest part
of answer finding is to locate the correct sentence.
From an end-user?s point of view, presenting the
sentence that contains the answer is often more in-
formative and evidential. Furthermore, although the
judgement data in our case are more labor-intensive
to obtain, we believe our evaluation method is a bet-
ter indicator than the TREC evaluation for the qual-
ity of an answer selection algorithm.
To illustrate the point, consider the example ques-
tion, ?When did James Dean die?? The correct an-
9More human-judged data are desirable, though we will ad-
dress training from noisy, automatically judged data in Sec-
tion 5.4. It is important to note that human judgement of an-
swer sentence correctness was carried out prior to any experi-
ments, and therefore is unbiased. The total number of questions
in TREC 13 is 230. We exclude from the TREC 13 set questions
that either have no correct answer candidates (27 questions), or
no incorrect answer candidates (19 questions). Any algorithm
will get the same performance on these questions, and therefore
obscures the evaluation results. 6 such questions were also ex-
cluded from the 100 manually-judged training questions, result-
ing in 94 questions for training. For computational reasons (the
cost of parsing), we also eliminated answer candidate sentences
that are longer than 40 words from the training and evaluation
set. After these data preparation steps, we have 348 positive
Q-A pairs for training, 1,415 Q-A pairs in the development set,
and 1,703 Q-A pairs in the test set.
swer as appeared in the sentence ?In 1955, actor
James Dean was killed in a two-car collision near
Cholame, Calif.? is 1955. But from the same docu-
ment, there is another sentence which also contains
1955: ?In 1955, the studio asked him to become a
technical adviser on Elia Kazan?s ?East of Eden,?
starring James Dean.? If a system missed the first
sentence but happened to have extracted 1955 from
the second one, the TREC evaluation grants it a ?cor-
rect and well-supported? point, since the document
ID matches the correct document ID?even though
the latter answer does not entail the true answer. Our
evaluation does not suffer from this problem.
We report two standard evaluation measures com-
monly used in IR and QA research: mean av-
erage precision (MAP) and mean reciprocal rank
(MRR). All results are produced using the standard
trec eval program.
5.2 Baseline Systems
We implemented two state-of-the-art answer-finding
algorithms (Cui et al, 2005; Punyakanok et al,
2004) as strong baselines for comparison. Cui et
al. (2005) is the answer-finding algorithm behind
one of the best performing systems in TREC eval-
uations. It uses a mutual information-inspired score
computed over dependency trees and a single align-
ment between them. We found the method to be brit-
tle, often not finding a score for a testing instance
because alignment was not possible. We extended
the original algorithm, allowing fuzzy word align-
ments through WordNet expansion; both results are
reported.
The second baseline is the approximate tree-
matching work by Punyakanok et al (2004). Their
algorithm measures the similarity between ?q and ?a
by computing tree edit distance. Our replication is
close to the algorithm they describe, with one subtle
difference. Punyakanok et al used answer-typing in
computing edit distance; this is not available in our
dataset (and our method does not explicitly carry out
answer-typing). Their heuristics for reformulating
questions into statements were not replicated. We
did, however, apply WordNet type-checking and ap-
proximate, penalized lexical matching. Both results
are reported.
28
development set test set
training dataset model MAP MRR MAP MRR
100 manually-judged TreeMatch 0.4074 0.4458 0.3814 0.4462
+WN 0.4328 0.4961 0.4189 0.4939
Cui et al 0.4715 0.6059 0.4350 0.5569
+WN 0.5311 0.6162 0.4271 0.5259
Jeopardy (base only) 0.5189 0.5788 0.4828 0.5571
Jeopardy 0.6812 0.7636 0.6029 0.6852
+2,293 noisy Cui et al 0.2165 0.3690 0.2833 0.4248
+WN 0.4333 0.5363 0.3811 0.4964
Jeopardy (base only) 0.5174 0.5570 0.4922 0.5732
Jeopardy 0.6683 0.7443 0.5655 0.6687
Table 2: Results on development and test sets. TreeMatch is our implementation of Punyakanok et al
(2004); +WN modifies their edit distance function using WordNet. We also report our implementation of
Cui et al (2005), along with our WordNet expansion (+WN). The Jeopardy base model and mixture with
the lexical-semantics log-linear model perform best; both are trained using conditional maximum likelihood
estimation. The top part of the table shows performance using 100 manually-annotated question examples
(questions 1?100 in TREC 8?12), and the bottom part adds noisily, automatically annotated questions 101?
2,393. Boldface marks the best score in a column and any scores in that column not significantly worse
under a a two-tailed paired t-test (p < 0.03).
5.3 Results
Evaluation results on the development and test sets
of our model in comparison with the baseline algo-
rithms are shown in Table 2. Both our model and
the model in Cui et al (2005) are trained on the
manually-judged training set (questions 1-100 from
TREC 8?12). The approximate tree matching algo-
rithm in Punyakanok et al (2004) uses fixed edit dis-
tance functions and therefore does not require train-
ing. From the table we can see that our model signif-
icantly outperforms the two baseline algorithms?
even when they are given the benefit of WordNet?
on both development and test set, and on both MRR
and MAP.
5.4 Experiments with Noisy Training Data
Although manual annotation of the remaining 2,293
training sentences? answers in TREC 8?12 was too
labor-intensive, we did experiment with a simple,
noisy automatic labeling technique. Any answer
that had at least three non-stop word types seen in
the question and contains the answer pattern defined
in the dataset was labeled as ?correct? and used in
training. The bottom part of Table 2 shows the re-
sults. Adding the noisy data hurts all methods, but
the Jeopardy model maintains its lead and consis-
tently suffers less damage than Cui et al (2005).
(The TreeMatch method of Punyakanok et al (2004)
does not use training examples.)
5.5 Summing vs. Maximizing
Unlike most previous work, our model does not try
to find a single correspondence between words in the
question and words in the answer, during training or
during testing. An alternative method might choose
the best (most probable) alignment, rather than the
sum of all alignment scores. This involves a slight
change to Equation 3, replacing the summation with
a maximization. The change could be made during
training, during testing, or both. Table 3 shows that
summing is preferable, especially during training.
6 Discussion
The key experimental result of this work is that
loose syntactic transformations are an effective way
to carry out statistical question answering.
One unique advantage of our model is the mix-
ture of a factored, multinomial-based base model
and a potentially very rich log-linear model. The
base model gives our model robustness, and the log-
29
test set
training decoding MAP MRR
? ? 0.6029 0.6852
? max 0.5822 0.6489
max ? 0.5559 0.6250
max max 0.5571 0.6365
Table 3: Experimental results on comparing sum-
ming over alignments (?) with maximizing (max)
over alignments on the test set. Boldface marks the
best score in a column and any scores in that column
not significantly worse under a a two-tailed paired t-
test (p < 0.03).
linear model allows us to throw in task- or domain-
specific features. Using a mixture gives the advan-
tage of smoothing (in the base model) without hav-
ing to normalize the log-linear model by summing
over large sets. This powerful combination leads
us to believe that our model can be easily ported
to other semantic processing tasks where modeling
syntactic and semantic transformations is the key,
such as textual entailment, paraphrasing, and cross-
lingual QA.
The traditional approach to cross-lingual QA is
that translation is either a pre-processing or post-
processing step done independently from the main
QA task. Notice that the QG formalism that we have
employed in this work was originally proposed for
machine translation. We might envision transfor-
mations that are performed together to form ques-
tions from answers (or vice versa) and to translate?
a Jeopardy! game in which bilingual players must
ask a question in a different language than that in
which the answer is posed.
7 Conclusion
We described a statistical syntax-based model that
softly aligns a question sentence with a candidate
answer sentence and returns a score. Discrimina-
tive training and a relatively straightforward, barely-
engineered feature set were used in the implementa-
tion. Our scoring model was found to greatly out-
perform two state-of-the-art baselines on an answer
selection task using the TREC dataset.
Acknowledgments
The authors acknowledge helpful input from three
anonymous reviewers, Kevin Gimpel, and David
Smith. This work is supported in part by
ARDA/DTO Advanced Question Answering for
Intelligence (AQUAINT) program award number
NBCHC040164.
References
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, Alessandro Tommasi, Ellen M.
Voorhees, and D. K. Harman. 2001. Selectively using
relations to improve precision in question answering.
In Proceedings of the 10th Text REtrieval Conference
(TREC-10), Gaithersburg, MD, USA.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic functions
of Markov chains. The Annals of Mathematical Statis-
tics, 41(1):164?171.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns whats? in
a name. Machine Learning, 34(1-3):211?231.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings of
the 28th ACM-SIGIR International Conference on Re-
search and Development in Information Retrieval, Sal-
vador, Brazil.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1?38.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43st Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Ann Arbor, MI, USA.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Sapporo,
Japan.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
30
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and the 44st Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL), Sydney, Australia.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics (ACL), Sapporo, Japan.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL), Sydney, Australia.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system?
TREC-10. In Proceedings of the 10th Text REtrieval
Conference (TREC-10), Gaithersburg, MD, USA.
Tony Jebara and Alex Pentland. 1999. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Proceedings of the 1998 Confer-
ence on Advances in Neural Information Processing
Systems II (NIPS), pages 494?500, Denver, CO, USA.
Boris Katz and Jimmy Lin. 2003. Selectively using
relations to improve precision in question answering.
In Proceedings of the EACL-2003 Workshop on Nat-
ural Language Processing for Question Answering,
Gaithersburg, MD, USA.
Jinxi Xu Ana Licuanan and Ralph Weischedel. 2003.
Trec2003 qa at bbn: Answering definitional questions.
In Proceedings of the 12th Text REtrieval Conference
(TREC-12), Gaithersburg, MD, USA.
Kenneth C. Litkowski. 1999. Question-answering us-
ing semantic relation triples. In Proceedings of the
8th Text REtrieval Conference (TREC-8), Gaithers-
burg, MD, USA.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45:503?528.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), New York, NY, USA.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernado Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43st Annual Meeting of
the Association for Computational Linguistics (ACL),
Ann Arbor, MI, USA.
I. Dan Melamed. 2004. Algorithms for syntax-aware
statistical machine translation. In Proceedings of the
Conference on Theoretical and Methodological Issues
in Machine Translation (TMI), Baltimore, MD, USA.
Xiao-Li Meng and Donald B. Rubin. 1993. Maximum
likelihood estimation via the ECM algorithm: A gen-
eral framework. Biometrika, 80:267?278.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
WordNet: an on-line lexical database. International
Journal of Lexicography, 3(4).
Vanessa Murdock and W. Bruce Croft. 2005. A trans-
lation model for sentence retrieval. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP), Vancouver, BC, USA.
Vasin Punyakanok, Dan Roth, and Wen-Tau Yih. 2004.
Mapping dependencies trees: An application to ques-
tion answering. In Proceedings of the 8th Interna-
tional Symposium on Artificial Intelligence and Math-
ematics, Fort Lauderdale, FL, USA.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistics
(ACL), Ann Arbor, MI, USA.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Philadelphia, PA, USA.
Deepak Ravichandran, Abharam Ittycheriah, and Salim
Roukos. 2003. Automatic derivation of surface text
patterns for a maximum entropy based question an-
swering system. In Proceedings of the Human Lan-
guage Technology Conference and North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL), Edmonton, Canada.
Dan Shen and Dietrich Klakow. 2006. Exploring corre-
lation of dependency relation paths for answer extrac-
tion. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics (COLING-ACL), Sydney, Australia.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for ques-
tion answering. In Proceedings of the Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP), Jeju Island, Republic of Korea.
Hideki Shima, Mengqiu Wang, Frank Lin, and Teruko
Mitamura. 2006. Modular approach to error analysis
and evaluation for multilingual question answering. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation (LREC), Genoa,
Italy.
31
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In Proceedings of the HLT-
NAACL Workshop on Statistical Machine Translation,
New York, NY, USA.
Martin M. Soubbotin and Sergei M. Soubbotin. 2001.
Patterns for potential answer expressions as clues to
the right answers. In Proceedings of the 10th Text
REtrieval Conference (TREC-10), Gaithersburg, MD,
USA.
Dekai Wu and Hongsing Wong. 1998. Machine
translation with a stochastic grammatical channel.
In Proceedings of the 17th International Conference
on Computational Linguistics (COLING), Montreal,
Canada.
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars. In
Proceedings of the ACL Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, MI, USA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Toulouse, France.
32
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 132?140, Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Models of Nonprojective Dependency Trees
David A. Smith
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218 USA
dasmith@cs.jhu.edu
Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
nasmith@cs.cmu.edu
Abstract
A notable gap in research on statistical de-
pendency parsing is a proper conditional
probability distribution over nonprojective
dependency trees for a given sentence. We
exploit the Matrix Tree Theorem (Tutte,
1984) to derive an algorithm that efficiently
sums the scores of all nonprojective trees
in a sentence, permitting the definition of
a conditional log-linear model over trees.
While discriminative methods, such as those
presented in McDonald et al (2005b), ob-
tain very high accuracy on standard de-
pendency parsing tasks and can be trained
and applied without marginalization, ?sum-
ming trees? permits some alternative tech-
niques of interest. Using the summing al-
gorithm, we present competitive experimen-
tal results on four nonprojective languages,
for maximum conditional likelihood estima-
tion, minimum Bayes-risk parsing, and hid-
den variable training.
1 Introduction
Recently dependency parsing has received renewed
interest, both in the parsing literature (Buchholz
and Marsi, 2006) and in applications like translation
(Quirk et al, 2005) and information extraction (Cu-
lotta and Sorensen, 2004). Dependency parsing can
be used to provide a ?bare bones? syntactic struc-
ture that approximates semantics, and it has the addi-
tional advantage of admitting fast parsing algorithms
(Eisner, 1996; McDonald et al, 2005b) with a neg-
ligible grammar constant in many cases.
The latest state-of-the-art statistical dependency
parsers are discriminative, meaning that they are
based on classifiers trained to score trees, given a
sentence, either via factored whole-structure scores
(McDonald et al, 2005a) or local parsing decision
scores (Hall et al, 2006). In the works cited, these
scores are not intended to be interpreted as proba-
bilistic quantities.
Here we consider weighted dependency parsing
models that can be used to define well-formed con-
ditional distributions p(y | x), for dependency
trees y and a sentence x. Conditional distribu-
tions over outputs (here, trees) given inputs (here,
sentences) have certain advantages. They per-
mit marginalization over trees to compute poste-
riors of interesting sub-events (e.g., the probabil-
ity that two noun tokens bear a relation, regard-
less of which tree is correct). A probability model
permits alternative decoding procedures (Goodman,
1996). Well-motivated hidden variable training
procedures (such as EM and conditional EM) are
also readily available for probabilistic models. Fi-
nally, probability models can be chained together (as
in a noisy channel model), mixed, or combined in a
product-of-experts.
Sequence models, context-free models, and de-
pendency models have appeared in several guises;
a cross-model comparison clarifies the contribution
of this paper. First, there were generative, stochas-
tic models like HMMs, PCFGs, and Eisner?s (1996)
models. Local discriminative classifiers were pro-
posed by McCallum et al (2000) for sequence mod-
eling, by Ratnaparkhi et al (1994) for constituent
parsing, and by Hall et al (2006) (among others) for
132
dependencies. Large-margin whole-structure mod-
els were proposed for sequence labeling by Al-
tun et al (2003), for constituents by Taskar et al
(2004), and for dependency trees by McDonald et
al. (2005a). In this paper, we propose a model
most similar to the conditional random fields?
interpretable as log-linear models?of Lafferty et al
(2001), which are now widely used for sequence la-
beling. Log-linear models have been used in pars-
ing by Riezler et al (2000) (for constraint-based
grammars) and Johnson (2001) and Miyao and Tsu-
jii (2002) (for CFGs). Like McDonald et al, we use
an edge-factored model that permits nonprojective
trees; like Lafferty et al, we argue for an alternative
interpretation as a log-linear model over structures,
conditioned on the observed sentence.
In Section 2 we point out what would be required,
computationally, for conditional training of nonpro-
jective dependency models. The solution to the con-
ditionalization problem is given in Section 3, using a
widely-known but newly-applied Matrix Tree Theo-
rem due to Tutte (1984), and experimental results are
presented with a comparison to the MIRA learning
algorithm used by McDonald et al (2005a). We go
on to describe and experiment with two useful appli-
cations of conditional modeling: minimum Bayes-
risk decoding (Section 4) and hidden-variable train-
ing by conditional maximum likelihood estimation
(Section 5). Discussion in Section 6 considers the
implications of our experimental results.
Two indepedent papers, published concurrently
with this one, report closely related results to ours.
Koo et al (2007) and McDonald and Satta (2007)
both describe how the Matrix Tree Theorem can be
applied to computing the sum of scores of edge-
factored dependency trees and the edge marginals.
Koo et al compare conditional likelihood training
(as here) to the averaged perceptron and a max-
imum margin model trained using exponentiated-
gradient (Bartlett et al, 2004); the latter requires
the same marginalization calculations as conditional
log-linear estimation. McDonald and Satta discuss a
variety of applications (including minimum Bayes-
risk decoding) and give complexity results for non-
edge-factored models. Interested readers are re-
ferred to those papers for further discussion.
2 Conditional Training for Nonprojective
Dependency Models
Let x = ?x1, ..., xn? be a sequence of words (possi-
bly with POS tags, lemmas, and morphological in-
formation) that are the input to a parser. y will refer
to a directed, unlabeled dependency tree, which is a
map y : {1, ..., n} ? {0, ..., n} from child indices
to parent indices; x0 is the invisible ?wall? symbol.
Let Yx be the set of valid dependency trees for x. In
this paper, Yx is equivalent to the set of all directed
spanning trees over x.1
A conditional model defines a family of probabil-
ity distributions p(y | x), for all x and y ? Yx. We
propose that this model take a log-linear form:
p~?(y | x) =
e~??~f(x,y)
?
y??Yx
e
~??~f(x,y?)
=
e~??~f(x,y)
Z~?(x)
(1)
where ~f is a feature vector function on parsed sen-
tences and ~? ? Rm parameterizes the model. Fol-
lowing McDonald et al (2005a), we assume that the
features are edge-factored:
~f(x,y) =
n?
i=1
~f(x, xi, xy(i)) (2)
In other words, the dependencies between words in
the tree are all conditionally independent of each
other, given the sequence x and the fact that the
parse is a spanning tree. Despite the constraints they
impose on features, edge-factored models have the
advantage of tractable O(n3) inference algorithms
or, with some trickery, O(n2) maximum a posteriori
(?best parse tree?) inference algorithms in the non-
projective case. Exact nonprojective inference and
estimation become intractable if we break edge fac-
toring (McDonald and Pereira, 2006).
We wish to estimate the parameters ~? by maxi-
mizing the conditional likelihood (like a CRF) rather
1To be precise, every word has in-degree 1, with the sole
edge pointing from the word?s parent, xy(i) ? xi. x0 has in-
degree 0. By definition, trees are acyclic. The edges need not
be planar and may ?cross? in the plane, since we do not have a
projectivity constraint. In some formulations, exactly one node
in x can attach to x0; here we allow multiple nodes to attach
to x0, since this occurs with some frequency in many existing
datasets. Summation over trees where x0 has exactly one child
is addressed directly by Koo et al (2007).
133
than the margin (McDonald et al, 2005a). For an
empirical distribution p? given by a set of training ex-
amples, this means:
max
~?
?
x,y
p?(x,y)
(
~? ? ~f(x,y)
)
?
?
x
p?(x) logZ~?(x)
(3)
This optimization problem is typically solved us-
ing a quasi-Newton numerical optimization method
such as L-BFGS (Liu and Nocedal, 1989). Such a
method requires the gradient of the objective func-
tion, which for ?k is given by the following differ-
ence in expectations of the value of feature fk:
?
??k
= (4)
Ep?(X,Y) [fk(X,Y)] ?Ep?(X)p~?(Y|X) [fk(X,Y)]
The computation of Z~?(x) and the sufficient
statistics (second expectation in Equation 4) are typ-
ically the difficult parts. They require summing the
scores of all the spanning trees for a given sentence.
Note that, in large-margin training, and in standard
maximum a posteriori decoding, only a maximum
over spanning trees is called for?it is conditional
training that requires Z~?(x). In Section 3, we will
show how this can be done exactly in O(n3) time.
3 Exploiting the Matrix Tree Theorem for
Z~?(x)
We wish to apply conditional training to estimate
conditional models of nonprojective trees. This re-
quires computing Z~?(x) for each training example
(as an inner loop to training). In this section we show
how the summation can be computed and how con-
ditional training performs.
3.1 Kirchoff Matrix
Recall that we defined the unnormalized probability
(henceforth, score) of a dependency tree as a combi-
nation of edge-factored scores for the edges present
in the tree (Eq. 2):
exp ~??~f(x,y) =
n?
i=1
e
~??~f(x,xi,xy(i)) =
n?
i=1
sx,~?(i,y(i))
(5)
where y(i) denotes the parent of xi in y. sx,~?(i, j),
then, denotes the (multiplicative) contribution of the
edge from child i to parent j to the total score of
the tree, if the edge is present. Define the Kirchoff
matrixKx,~? ? R
n?n by
[
Kx,~?
]
mom,kid
= (6)
?
?
?
?sx,~?(kid ,mom) if mom 6= kid?
j?{0,...n}:j 6=mom
sx,~?(kid , j) if mom = kid .
where mom indexes a parent node and kid a child
node.
Kx ~? can be regarded as a special weighted adja-
cency matrix in which the ith diagonal entry is the
sum of edge-scores directed into vertex i (i.e., xi is
the child)?note that the sum includes the score of
attaching xi to the wall x0.
In our notation and in one specific form, the Ma-
trix Tree Theorem (Tutte, 1984) states:2
Theorem 1 The determinant of the Kirchoff matrix
Kx,~? is equal to the sum of scores of all directed
spanning trees in Yx rooted at x0. Formally:
?
?
?Kx,~?
?
?
? = Z~?(x).
A proof is omitted; see Tutte (1984).
To compute Z~?(x), we need only take the deter-
minant of Kx,~?, which can be done in O(n
3) time
using the standard LU factorization to compute the
matrix inverse. Since all of the edge weights used
to construct the Kirchoff matrix are positive, it is di-
agonally dominant and therefore non-singular (i.e.,
invertible).
3.2 Gradient
The gradient of Z~?(x) (required for numerical opti-
mization; see Eqs. 3?4) can be efficiently computed
from the same matrix inverse. While ? logZ~?(x)
equates to a vector of feature expectations (Eq. 4),
we exploit instead some facts from linear algebra
2There are proven generalizations of this theorem (Chen,
1965; Chaiken, 1982; Minoux, 1999); we give the most specific
form that applies to our case, originally proved by Tutte in 1948.
Strictly speaking, ourKx,~? is not the Kirchoff matrix, but rather
a submatrix of the Kirchoff matrix with a leftmost column of
zeroes and a topmost row [0,?sx,~?(1, 0), ...,?sx,~?(n, 0)] re-
moved. Farther afield, Jaakkola et al (1999) used an undirected
matrix tree theorem for learning tree structures for graphical
models.
134
Kx,~? =
?
?
?
?
?
?
?
?
?
?
?
?
?
j?{0,...,n}:j 6=1
sx,~?(1, j) ?sx,~?(2, 1) ? ? ? ?sx,~?(n, 1)
?sx,~?(1, 2)
?
j?{0,...,n}:j 6=2
sx,~?(2, j) ? ? ? ?sx,~?(n, 2)
...
...
. . .
...
?sx,~?(1, n) ?sx,~?(2, n) ? ? ?
?
j?{0,...,n}:j 6=n
sx,~?(n, j)
?
?
?
?
?
?
?
?
?
?
?
?
and the chain rule. First, note that, for any weight
?k,
? logZ~?(x)
??k
=
? log |Kx,~?|
??k
=
1
|Kx,~?|
?|Kx,~?|
??k
=
1
|Kx,~?|
n?
i=1
n?
j=0
?|Kx,~?|
?sx,~?(i, j)
?sx,~?(i, j)
??k
=
1
|Kx,~?|
n?
i=1
n?
j=0
sx,~?(i, j)fk(x, xi, xj)
?
?|Kx,~?|
?sx,~?(i, j)
(7)
(We assume sx,~?(i, i) = 0, for simplicity of nota-
tion.) The last line follows from the definition of
sx,~?(i, j) as exp
~?? ~f(x, xi, xj). Now, since sx,~?(i, j)
affects the Kirchoff matrix in at most two cells?
(i, i) and (j, i), the latter only when j > 0?we
know that
?|Kx,~?|
?sx,~?(i, j)
=
?|Kx,~?|
?[Kx,~?]i,i
?[Kx,~?]i,i
?sx,~?(i, i)
?
?|Kx,~?|
?[Kx,~?]j,i
?[Kx,~?]j,i
?sx,~?(i, j)
=
?|Kx,~?|
?[Kx,~?]i,i
?
?|Kx,~?|
?[Kx,~?]j,i
(8)
We have now reduced the problem of the gradient
to a linear function of ?|Kx,~?| with respect to the
cells of the matrix itself. At this point, we simplify
notation and consider an arbitrary matrixA.
The minor mj,i of a matrix A is the determi-
nant of the submatrix obtained by striking out row
j and column i of A; the cofactor cj,i of A is then
(?1)i+jmj,i. Laplace?s formula defines the deter-
minant as a linear combination of matrix cofactors
of an arbitrary row j:
|A| =
n?
i=1
[A]j,icj,i (9)
It should be clear that any cj,k is constant with re-
spect to the cell [A]j,i (since it is formed by remov-
ing row j of A) and that other entries of A are con-
stant with respect to the cell [A]j,i. Therefore:
?|A|
?[A]j,i
= cj,i (10)
The inverse matrixA?1 can also be defined in terms
of cofactors:
[A?1]i,j =
cj,i
|A|
(11)
Combining Eqs. 10 and 11, we have:
?|A|
?[A]j,i
= |A|[A?1]i,j (12)
Plugging back in through Eq. 8 to Eq. 7, we have:
? logZ~?(x)
??k
=
n?
i=1
n?
j=0
sx,~?(i, j)fk(x, xi, xj)
?
([
K?1
x,~?
]
i,i
?
[
K?1
x,~?
]
i,j
)
(13)
where [K?1]i,0 is taken to be 0. Note that the cofac-
tors do not need to be computed directly. We pro-
posed in Section 3.1 to get Z~?(x) by computing the
inverse of the Kirchoff matrix (which is known to
exist). Under that procedure, the marginalization is
a by-product of the gradient.
135
decode train Arabic Czech Danish Dutch
map MIRA 79.9 81.4 86.6 90.0
CE 80.4 80.2 87.5 90.0 (Section 3)
mBr MIRA 79.4 80.3 85.0 87.2 (Section 4)
CE 80.5 80.4 87.5 90.0 (Sections 3 & 4)
Table 1: Unlabeled dependency parsing accuracy (on test data) for two training methods (MIRA, as in
McDonald et al (2005b), and conditional estimation) and with maximum a posteriori (map) and minimum
Bayes-risk (mBr) decoding. Boldface scores are best in their column on a permutation test at the .05 level.
3.3 Experiment
We compare conditional training of a nonprojective
edge-factored parsing model to the online MIRA
training used by McDonald et al (2005b). Four lan-
guages with relatively common nonprojective phe-
nomena were tested: Arabic (Hajic? et al, 2004),
Czech (Bo?hmova? et al, 2003), Danish (Kromann,
2003), and Dutch (van der Beek et al, 2002). The
Danish and Dutch datasets were prepared for the
CoNLL 2006 shared task (Buchholz and Marsi,
2006); Arabic and Czech are from the 2007 shared
task. We used the same features, extracted by Mc-
Donald?s code, in both MIRA and conditional train-
ing. In this paper, we consider only unlabeled de-
pendency parsing.
Our conditional training used an online gradient-
based method known as stochastic gradient descent
(see, e.g., Bottou, 2003). Training with MIRA and
conditional estimation take about the same amount
of time: approximately 50 sentences per second.
Training proceeded as long as an improvement on
held-out data was evident. The accuracy of the hy-
pothesized parses for the two models, on each lan-
guage, are shown in the top two rows of Tab. 1 (la-
beled ?map? for maximum a posteriori, meaning
that the highest-weighted tree is hypothesized).
The two methods are, not surprisingly, close in
performance; conditional likelihood outperformed
MIRA on Arabic and Danish, underperformed
MIRA on Czech, and the two tied on Dutch. Results
are significant at the .05 level on a permutation test.
Conditional estimation is in practice more prone to
over-fitting than maximum margin methods, though
we did not see any improvement using zero-mean
Gaussian priors (variance 1 or 10).
These experiments serve to validate conditional
estimation as a competitive learning algorithm for
parsing models, and the key contribution of the sum-
ming algorithm that permits conditional estimation.
4 Minimum Bayes-Risk Decoding
A second application of probability distributions
over trees is the alternative decoding algorithm
known as minimum Bayes-risk (mBr) decoding.
The more commonly used maximum a posteriori
decoding (also known as ?Viterbi? decoding) that
we applied in Section 3.3 sought to minimize the ex-
pected whole-tree loss:
y? = argmax
y
p~?(y | x) = argmin
y
Ep~?(Y|x) [??(y,Y)]
(14)
Minimum Bayes-risk decoding generalizes this idea
to an arbitrary loss function ` on the proposed tree:
y? = argmin
y
Ep~?(Y|x) [`(y,Y)] (15)
This technique was originally applied in speech
recognition (Goel and Byrne, 2000) and translation
(Kumar and Byrne, 2004); Goodman (1996) pro-
posed a similar idea in probabilistic context-free
parsing, seeking to maximize expected recall. For
more applications in parsing, see Petrov and Klein
(2007).
The most common loss function used to evaluate
dependency parsers is the number of attachment er-
rors, so we seek to decode using:
y? = argmin
y
Ep~?(Y|x)
[
n?
i=1
??(y(i),Y(i))
]
= argmax
y
n?
i=1
p~?(Y(i) = y(i) | x) (16)
To apply this decoding method, we make use of
Eq. 13, which gives us the posterior probabilities
136
of edges under the model, and the same Chiu-
Liu-Edmonds maximum directed spanning tree al-
gorithm used for maximum a posteriori decoding.
Note that this decoding method can be applied re-
gardless of how the model is trained. It merely re-
quires assuming that the tree scores under the trained
model (probabilistic or not) can be treated as unnor-
malized log-probabilities over trees given the sen-
tence x.
We applied minimum Bayes-risk decoding to the
models trained using MIRA and using conditional
estimation (see Section 3.3). Table 1 shows that,
across languages, minimum Bayes-risk decoding
hurts slightly the performance of a MIRA-trained
model, but helps slightly or does not affect the per-
formance of a conditionally-trained model. Since
MIRA does not attempt to model the distribution
over trees, this result is not surprising; interpreting
weights as defining a conditional log-linear distribu-
tion is questionable under MIRA?s training criterion.
One option, which we do not test here, is to
use minimum Bayes-risk decoding inside of MIRA
training, to propose a hypothesis tree (or k-best
trees) at each training step. Doing this would more
closely match the training conditions with the test-
ing conditions; however, it is unclear whether there
is a formal interpretation of such a combination, for
example its relationship to McDonald et al?s ?fac-
tored MIRA.?
Minimum Bayes-risk decoding, we believe, will
become important in nonprojective parsing with
non-edge-factored models. Note that minimium
Bayes-risk decoding reduces any parsing problem to
the maximum directed spanning tree problem, even
if the original model is not edge-factored. All that
is required are the marginals p~?(Y(i) = y(i) | x),
which may be intractable to compute exactly, though
it may be possible to develop efficient approxima-
tions.
5 Hidden Variables
A third application of probability distributions over
trees is hidden-variable learning. The Expectation-
Maximization (EM) algorithm (Baum and Petrie,
1966; Dempster et al, 1977; Baker, 1979), for
example, is a way to maximum the likelihood of
training data, marginalizing out hidden variables.
This has been applied widely in unsupervised pars-
ing (Carroll and Charniak, 1992; Klein and Man-
ning, 2002). More recently, EM has been used to
learn hidden variables in parse trees; these can be
head-child annotations (Chiang and Bikel, 2002), la-
tent head features (Matsuzaki et al, 2005; Prescher,
2005; Dreyer and Eisner, 2006), or hierarchically-
split nonterminal states (Petrov et al, 2006).
To date, we know of no attempts to apply hid-
den variables to supervised dependency tree mod-
els. If the trees are constrained to be projective, EM
is easily applied using the inside-outside variant of
the parsing algorithm described by Eisner (1996) to
compute the marginal probability. Moving to the
nonprojective case, there are two difficulties: (a) we
must marginalize over nonprojective trees and (b)
we must define a generative model over (X,Y).
We have already shown in Section 3 how to solve
(a); here we avoid (b) by maximizing conditional
likelihood, marginalizing out the hidden variable,
denoted z:
max
~?
?
x,y
p?(x,y) log
?
z
p~?(y, z | x) (17)
This sort of conditional training with hidden vari-
ables was carried out by Koo and Collins (2005),
for example, in reranking; it is related to the infor-
mation bottleneck method (Tishby et al, 1999) and
contrastive estimation (Smith and Eisner, 2005).
5.1 Latent Dependency Labels
Noting that our model is edge-factored (Eq. 2), we
define our hidden variables to be edge-factored as
well. We can think of the hidden variables as clusters
on dependency tokens, and redefine the score of an
edge to be:
sx,~?(i, j) =
?
z?Z
e
~??~f(x,xi,xj ,z) (18)
where Z is a set of dependency clusters.
Note that keeping the model edge-factored means
that the cluster of each dependency in a tree is con-
ditionally independent of all the others, given the
words. This is computationally advantageous (we
can factor out the marginalization of the hidden vari-
able by edge), and it permits the use of any cluster-
ing method at all. For example, if an auxiliary clus-
tering model q(z | x,y)?perhaps one that did not
137
make such independence assumptions?were used,
the posterior probability q(Zi = z | x,y) could
be a feature in the proposed model. On the other
hand, we must consider carefully the role of the
dependency clusters in the model; if clusters are
learned extrinsic to estimation of the parsing model,
we should not expect them to be directly advanta-
geous to parsing accuracy.
5.2 Experiments
We tried two sets of experiments with clustering. In
one case, we simply augmented all of McDonald
et al?s edge features with a cluster label in hopes
of improved accuracy. Models were initialized near
zero, with Gaussian noise added to break symmetry
among clusters.
Under these conditions, performance stayed the
same or changed slightly (see Table 2); none of the
improvements are significant. Note that three de-
coders were applied: maximum a posteriori (map)
and minimum Bayes-risk (mBr) as described in Sec-
tion 4, and ?max-z,? in which each possible edge
was labeled and weighted only with its most likely
cluster (rather than the sum over all clusters), before
finding the most probable tree.3 For each of the three
languages tested, some number of clusters and some
decoding method gave small improvements over the
baseline.
More ambitiously, we hypothesized that many
lexicalized features on edges could be ?squeezed?
through clusters to reduce the size of the feature set.
We thus removed all word-word and lemma-lemma
features and all tag fourgrams. Although this re-
duced our feature set by a factor of 60 or more (prior
to taking a cross-product with the clusters), the dam-
age of breaking the features was tremendous, and
performance even with a thousand clusters barely
broke 25% accuracy.
6 Discussion
Noting that adding latent features to nonterminals
in unlexicalized context-free parsing has been very
successful (Chiang and Bikel, 2002; Matsuzaki et
al., 2005; Prescher, 2005; Dreyer and Eisner, 2006;
Petrov et al, 2006), we were surprised not to see a
3Czech experiments were not done, since the number of fea-
tures (more than 14 million) was too high to multiply out by
clusters.
# cl. decoding Arabic Danish Dutch
none map=max-z 80.4 87.5 90.0
mBr 80.5 87.5 90.0
2 map 80.4 87.5 89.5
mBr 80.6 87.3 89.7
max-z 80.4 86.3 89.4
16 map 80.4 87.6 90.1
mBr 80.4 87.6 90.1
max-z 80.4 87.6 90.2
32 map 80.0 87.6 ?
mBr 80.4 87.5 ?
max-z 80.0 87.5 ?
Table 2: Augmenting edge features with clusters re-
sults in similar performance to conditional training
with no clusters (top two lines). Scores are unla-
beled dependency accuracy on test data.
more substantial performance improvement through
latent features. We propose several interpretations.
First, it may simply be that many more clusters may
be required. Note that the label-set sizes for the la-
beled versions of these datasets are larger than 32
(e.g., 50 for Danish). This has the unfortunate effect
of blowing up the feature space beyond the mem-
ory capacity of our machines (hence our attempts
at squeezing high-dimensional features through the
clusters).
Of course, improved clustering methods may
also improve performance. In particular, a cluster-
learning algorithm that permits clusters to split
and/or merge, as in Petrov et al (2006) or in Pereira
et al (1993), may be appropriate.
Given the relative simplicity of clustering meth-
ods for context-free parsing to date (gains were
found just by using Expectation-Maximization), we
believe the fundamental reason clustering was not
particularly helpful here is a structural one. In
context-free parsing, the latent features are (in pub-
lished work to date) on nonterminal states, which are
the stuctural ?bridge? between context-free rules.
Adding features to those states is a way of pushing
information?encoded indirectly, perhaps?farther
around the tree, and therefore circumventing the
strict independence assumptions of probabilistic
CFGs.
In an edge-factored dependency model, on the
138
other hand, latent features on the edges seem to have
little effect. Given that they are locally ?summed
out? when we compute the scores of possible at-
tachments, it should be clear that the edge clusters
do not circumvent any independence assumptions.
Three options appear to present themselves. First,
we might attempt to learn clusters in tandem with
estimating a richer, non-edge-factored model which
would require approximations to Z~?(x), if condi-
tional training were to be used. Note that the approx-
imations to maximizing over spanning trees with
second-order features, proposed by McDonald and
Pereira (2006), do not permit estimating the clusters
as part of the same process as weight estimation (at
least not without modification). In the conditional
estimation case, a variational approach might be ap-
propriate. The second option is to learn clusters of-
fline, before estimating the parser. (We suggested
how to incorporate soft clusters into our model in
Section 5.1.) This option is computationally ad-
vantageous but loses sight of the aim of learning
the clusters specifically to improve parsing accuracy.
Third, noting that the structural ?bridge? between
two coincident edges is the shared vertex (word), we
might consider word token clustering.
We also believe this structural locality issue helps
explain the modesty of the gains using minimum
Bayes-risk decoding with conditional training (Sec-
tion 4). In other dependency parsing scenarios, min-
imum Bayes-risk decoding has been found to offer
significant advantages?why not here? Minimum
Bayes-risk makes use of global statistical dependen-
cies in the posterior when making local decisions.
But in an edge-factored model, the edges are all con-
ditionally independent, given that y is a spanning
tree.
As a post hoc experiment, we compared
purely greedy attachment (attach each word to its
maximum-weighted parent, without any tree con-
straints). Edge scores as defined in the model were
compared to minimum Bayes-risk posterior scores,
and the latter were consistently better (though this
always under-performed optimal spanning-tree de-
coding, unsurprisingly). This comparison serves
only to confirm that minimum Bayes-risk decoding
is a way to circumvent independence assumptions
(here made by a decoder), but only when the trained
model does not make those particular assumptions.
7 Conclusion
We have shown how to carry out exact marginaliza-
tion under an edge-factored, conditional log-linear
model over nonprojective dependency trees. The
method has cubic runtime in the length of the se-
quence, but is very fast in practice. It can be used
in conditional training of such a model, in minimum
Bayes-risk decoding (regardless of how the model is
trained), and in training with hidden variables. We
demonstrated how each of these techniques gives re-
sults competitive with state-of-the-art existing de-
pendency parsers.
Acknowledgments
The authors thank the anonymous reviewers, Jason
Eisner, Keith Hall, and Sanjeev Khudanpur for help-
ful comments, and Michael Collins and Ryan Mc-
Donald for sharing drafts of their related, concurrent
papers. This work was supported in part by NSF
ITR grant IIS-0313193.
References
Y. Altun, M. Johnson, and T. Hofmann. 2003. Inves-
tigating loss functions and optimization methods for
discriminative learning of label sequences. In Proc. of
EMNLP.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Proc. of the Acoustical Society of America,
pages 547?550.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester.
2004. Exponentiated gradient algorithms for large-
margin structured classification. In Advances in NIPS
17.
L. E. Baum and T. Petrie. 1966. Statistical inference for
probabilistic functions of finite state Markov chains.
Annals of Mathematical Statistics, 37:1554?1563.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The PDT: a 3-level annotation scenario.
In A. Abeille, editor, Building and Exploiting
Syntactically-Annotated Corpora. Kluwer.
L. Bottou. 2003. Stochastic learning. In Advanced Lec-
tures in Machine Learning, pages 146?168. Springer.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
S. Chaiken. 1982. A combinatorial proof of the all mi-
nors matrix tree theorem. SIAM Journal on Algebraic
and Discrete Methods, 3(3):319?329.
139
W.-K. Chen. 1965. Topological analysis for active
networks. IEEE Transactions on Circuit Theory,
12(1):85?91.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In Proc. of COLING.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. of ACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum
likelihood estimation from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society B,
39:1?38.
M. Dreyer and J. Eisner. 2006. Better informed training
of latent syntactic features. In Proc. of EMNLP.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
V. Goel and W. Byrne. 2000. Minimum Bayes risk auto-
matic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
J. Hajic?, O. Smrz?, P. Zema?nek J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic Dependency Treebank: Devel-
opment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
learning for data-driven dependency parsing. In Proc.
of COLING-ACL.
T. Jaakkola, M. Meila, and T. Jebara. 1999. Maximum
entropy discrimination. In Advances in NIPS 12.
M. Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. of ACL.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of ACL.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. of EMNLP.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the Matrix-Tree The-
orem. In Proc. of EMNLP-CoNLL.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of TLT.
S. Kumar and W. Byrne. 2004. Minimum Bayes risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proc. of ACL.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proc. of IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of HLT-EMNLP.
M. Minoux. 1999. A generalization of the all minors ma-
trix tree theorem to semirings. Discrete Mathematics,
199:139?150.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estima-
tion for feature forests. In Proc. of HLT.
F. C. N. Pereira, N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of English words. In Proc. of the 31st
ACL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of COLING-ACL.
D. Prescher. 2005. Head-driven PCFGs with latent-head
statistics. In Proc. of IWPT.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A
maximum entropy model for parsing. In Proc. of IC-
SLP.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000.
Lexicalized stochastic modeling of constraint-based
grammars using log-linear measures and EM training.
In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
N. Tishby, F. C. N. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the 37th
Allerton Conference on Communication, Control and
Computing, pages 368?377.
W. T. Tutte. 1984. Graph Theory. Addison-Wesley.
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In
CLIN.
140
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 208?217, Prague, June 2007. c?2007 Association for Computational Linguistics
Joint Morphological and Syntactic Disambiguation?
Shay B. Cohen and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{scohen,nasmith}@cs.cmu.edu
Abstract
In morphologically rich languages, should morphological and
syntactic disambiguation be treated sequentially or as a sin-
gle problem? We describe several efficient, probabilistically-
interpretable ways to apply joint inference to morphological
and syntactic disambiguation using lattice parsing. Joint infer-
ence is shown to compare favorably to pipeline parsing methods
across a variety of component models. State-of-the-art perfor-
mance on Hebrew Treebank parsing is demonstrated using the
new method. The benefits of joint inference are modest with
the current component models, but appear to increase as com-
ponents themselves improve.
1 Introduction
As the field of statistical NLP expands to handle
more languages and domains, models appropriate
for standard benchmark tasks do not always work
well in new situations. Take, for example, pars-
ing the Wall Street Journal Penn Treebank, a long-
standing task for which highly accurate context-free
models stabilized by the year 2000 (Collins, 1999;
Charniak, 2000). On this task, the Collins model
achieves 90% F1-accuracy. Extended for new lan-
guages by Bikel (2004), it achieves only 75% on
Arabic and 72% on Hebrew.1
It should come as no surprise that Semitic parsing
lags behind English. The Collins model was care-
fully designed and tuned for WSJ English. Many of
the features in the model depend on English syntax
or Penn Treebank annotation conventions. Inherent
in its crafting is the assumption that a million words
of training text are available. Finally, for English, it
need not handle morphological ambiguity.
Indeed, the figures cited above for Arabic and
Hebrew are achieved using gold-standard morpho-
logical disambiguation and part-of-speech tagging.
?The authors acknowledge helpful feedback from the
anonymous reviewers, Sharon Goldwater, Rebecca Hwa, Alon
Lavie, and Shuly Wintner.
1Compared to the Penn Treebank, the Arabic Treebank
(Maamouri et al, 2004) has 60% as many word tokens, and
the Hebrew Treebank (Sima?an et al, 2001) has 6%.
Given only surface words, Arabic performance
drops by 1.5 F1 points. The Hebrew Treebank (un-
like Arabic) is built over morphemes, a convention
we view as sensible, though it complicates parsing.
This paper considers parsing for morphologically
rich languages, with Hebrew as a test case. Mor-
phology and syntax are two levels of linguistic de-
scription that interact. This interaction, we argue,
can affect disambiguation, so we explore here the
matter of joint disambiguation. This involves the
comparison of a pipeline (where morphology is in-
ferred first and syntactic parsing follows) with joint
inference. We present a generalization of the two,
and show new ways to do joint inference for this task
that does not involve a computational blow-up.
The paper is organized as follows. ?2 describes
the state of the art in NLP for Hebrew and some
phenomena it exhibits that motivate joint inference
for morphology and syntax. ?3 describes our ap-
proach to joint inference using lattice parsing, and
gives three variants of weighted lattice parsing with
their probabilistic interpretations. The different fac-
tor models and their stand-alone performance are
given in ?4. ?5 presents experiments on Hebrew
parsing and explores the benefits of joint inference.
2 Background
In this section we discuss prior work on statistical
morphological and syntactic processing of Hebrew
and motivate the joint approach.
2.1 NLP for Modern Hebrew
Wintner (2004) reviews work in Hebrew NLP, em-
phasizing that the challenges stem from the writ-
ing system, rich morphology, unique word forma-
tion process of roots and patterns, and relative lack
of annotated corpora.
We know of no publicly available statistical parser
designed specifically for Hebrew. Sima?an et al
208
h.
Figure 1: (a.) A sentence in Hebrew (to be read right to left), with (b.) one morphological analysis, (c.) English glosses, and (d.)
natural translation; and (e.) a different morphological analysis, (f.) English glosses, and (g.) less natural translation. (h.) shows a
morphological ?sausage? lattice that encodes the morpheme-sequence analyses L(~x) possible for a shortened sentence (unmodified
?meadow?). Shaded states are word boundaries, white states are intra-word morpheme boundaries; in practice we add POS tags to
the arcs, to permit disambiguation. According to both native speakers we polled, both interpretations are grammatical?note the
long-distance agreement required for grammaticality.
(2001) built a Hebrew Treebank of 88,747 words
(4,783 sentences) and parsed it using a probabilis-
tic model. However, they assumed that the input to
the parser was already (perfectly) morphologically
disambiguated. This assumption is very common in
multilingual parsing (see, for example, Cowan et al,
2005, and Buchholz et al, 2006).
Until recently, the NLP literature on morpho-
logical processing was dominated by the largely
non-probabilistic application of finite-state trans-
ducers (Kaplan and Kay, 1981; Koskenniemi, 1983;
Beesley and Karttunen, 2003) and the largely unsu-
pervised discovery of morphological patterns in text
(Goldsmith, 2001; Wicentowski, 2002); Hebrew
morphology receives special attention in Levinger
et al (1995), Daya et al (2004), and Adler and El-
hadad (2006). Lately a few supervised disambigua-
tion methods have come about, including hidden
Markov models (Hakkani-Tu?r et al, 2000; Hajic? et
al., 2001), conditional random fields (Kudo et al,
2004; Smith et al, 2005b), and local support vector
machines (Habash and Rambow, 2005). There are
also morphological disambiguators designed specif-
ically for Hebrew (Segal, 2000; Bar-Haim et al,
2005).
2.2 Why Joint Inference?
In NLP, the separation of syntax and morphology is
understandable when the latter is impoverished, as
in English. When both involve high levels of am-
biguity, this separation becomes harder to justify,
as argued by Tsarfaty (2006). To our knowledge,
that is the only study to move toward joint inference
of syntax and morphology, presenting joint models
and testing approximation of these models with two
parsers: one a pipeline (segmentation ? tagging ?
parsing), the other involved joint inference of seg-
mentation and tagging, with the result piped to the
parser. The latter was slightly more accurate. Tsar-
faty discussed but did not carry out joint inference.
In a morphologically rich language, the different
morphemes that make up a word can play a variety
of different syntactic roles. A reasonable linguistic
analysis might not make such morphemes immedi-
ate sisters in the tree. Indeed, the convention of the
Hebrew Treebank is to placemorphemes (rather than
words) at the leaves of the parse tree, allowing mor-
phemes of a word to attach to different nonterminal
parents.2
Generating parse trees over morphemes requires
the availability of morphological information when
parsing. Because this analysis is not in general re-
ducible to sequence labeling (tagging), the problem
is different from POS tagging. Figure 1 gives an
2The Arabic Treebank, by contrast, annotates words mor-
phologically but keeps the morphemes together as a single node
tagged with a POS sequence. In Bikel?s Arabic parser, complex
POS tags are projected to a small atomic set; it is unclear how
much information is lost.
209
example from Hebrew that illustrates the interaction
between morphology and syntax. In this example,
we show two interpretations of the surface text, with
the first being a more common natural analysis for
the sentence. The first and third-to-last words? anal-
yses depend on each other if the resulting analysis
is to be the more natural one: for this analysis the
first seven words have to be a noun phrase, while for
the less common analysis (?lying there nicely?) only
the first six words compose a noun phrase, with the
last two words composing a verb phrase. Consis-
tency depends on a long-distance dependency that a
finite-state morphology model cannot capture, but a
model that involves syntactic information can. Dis-
ambiguating the syntax aids in disambiguating the
morphology, suggesting that a joint model will per-
form both more accurately.
In sum, joint inference of morphology and syntax
is expected to allow decisions of both kinds to influ-
ence each other, enforce adherence to constraints at
both levels, and to diminish the propagation of errors
inherent in pipelines.
3 Joint Inference of Morphology and
Syntax
We now formalize the problem and supply the nec-
essary framework for performing joint morphologi-
cal disambiguation and syntactic parsing.
3.1 Notation and Morphological Sausages
Let X be the language?s word vocabulary and M be
its morpheme inventory. The set of valid analyses
for a surface word is defined using a morphologi-
cal lexicon L, which defines L(x) ? M+. L(~x) ?
(M+)+ (sequence of sequences) is the set of whole-
sentence analyses for sentence ~x = ?x1, x2, ..., xn?,
produced by concatenating elements of L(xi) in or-
der. L(~x) can be represented as an acyclic lattice
with a ?sausage? shape familiar from speech recog-
nition (Mangu et al, 1999) and machine translation
(Lavie et al, 2004). Fig. 1h shows a sausage lat-
tice for a sentence in Hebrew. We use ~m to denote
an element of L(~x) and ~mi to denote an element of
L(xi); in general, ~m = ?~m1, ~m2, ..., ~mn?.
We are interested in a function f :X+ ?
(M+)+ ? T, where T is the set of syntactic trees
for the language. f can be viewed as a structured
classifier. We use DG(~m) ? T to denote the set of
valid trees under a grammar G (here, a PCFG with
terminal alphabetM) for morpheme sequence ~m. To
be precise, f(~x) selects a mutually consistent mor-
phological and syntactic analysis from
GEN(~x) = {?~m, ?? | ~m ? L(~x), ? ? DG(~m)}
3.2 Product of Experts
Our mapping f(~x) is based on a joint probability
model p(?, ~m | ~x) which combines two probabil-
ity models pG(?, ~m) (a PCFG built on the gram-
mar G) and pL(~m | ~x) (a morphological disam-
biguation model built on the lexicon L). Factoring
the joint model into sub-models simplifies training,
since we can train each model separately, and in-
ference (parsing), as we will see later in this sec-
tion. Factored estimation has been quite popular in
NLP of late (Klein and Manning, 2003b; Smith and
Smith, 2004; Smith et al, 2005a, inter alia).
The most obvious joint parser uses pG as a condi-
tional model over trees given morphemes and maxi-
mizes the joint likelihood:
flik(~x)
= argmax
?~m,???GEN(~x)
pG(? | ~m) ? pL(~m | ~x) (1)
= argmax
?~m,???GEN(~x)
pG(?, ~m)
?
? ?
pG(?
?, ~m)
?
pL(~m, ~x)
?
~m?
pL(~m
?, ~x)
This is not straightforward, because it involves sum-
ming up the trees for each ~m to compute pG(~m),
which calls for the O(|~m|3)-Inside algorithm to
be called on each ~m. Instead, we use the joint,
pG(?, ~m), which, strictly speaking, makes the model
deficient (?leaky?), but permits a dynamic program-
ming solution.
Our models will be parametrized using either un-
normalized weights (a log-linear model) or multino-
mial distributions. Either way, both models define
scores over parts of analyses, and it may be advanta-
geous to give one model relatively greater strength,
especially since we often ignore constant normal-
izing factors. This is known as a product of ex-
perts (Hinton, 1999), where a new combined distri-
bution over events is defined by multiplying compo-
nent distributions together and renormalizing. In the
210
present setting, for some value ? ? 0,
fpoe,?(~x) = argmax
?~m,???GEN(~x)
pG(?, ~m) ? pL(~m | ~x)?
Z(~x, ?)
(2)
where Z(~x, ?) need not be computed (since it is a
constant in ~m and ? ). ? tunes the relative weight
of the morphology model with respect to the pars-
ing model. The higher ? is, the more we trust the
morphology model over the parser to correctly dis-
ambiguate the sentence. We might trust one model
more than the other for a variety of reasons: it could
be more robustly or discriminatively estimated, or it
could be known to come from a more appropriate
family.
This formulation also generalizes two more na??ve
parsing methods. If ? = 0, the morphology is mod-
eled only through the PCFG and pL is ignored ex-
cept as a constraint on which analyses L(~x) are al-
lowed (i.e., on the definition of the set GEN(~x)). At
the other extreme, as ? ? +?, pL becomes more
important. Because pL does not predict trees, pG
still ?gets to choose? the syntax tree, but in the limit
it must find a tree for argmax~m?L(~x) pL(~m | ~x).
This is effectively the morphology-first pipeline.3
3.3 Parsing Algorithms
To parse, we apply a dynamic programming algo-
rithm in the ?max, +? semiring to solve the fpoe,?
problem shown in Eq. 4. If pL is a unigram-factored
model, such that for some single-word morphologi-
cal model ? we have
pL(~m | ~x) =
?n
i=1 ?(~mi | xi) (3)
then we can implement morpho-syntactic parsing by
weighting the sausage lattice. Let the weight of each
arc that starts an analysis ~mi ? L(xi) be equal to
log ?(~mi | xi), and let other arcs have weight 0.
In the parsing algorithm, the weight on an arc is
summed in when the arc is first used to build a con-
stituent.
In general, we would like to define a joint model
that assigns (unnormalized) probabilities to ele-
ments of GEN(~x). If pG is a PCFG and pL can
3There is a slight difference. If no parse tree exists for the
pL-best morphological analysis, then a less probable ~m may be
chosen. So as ? ? +?, we can view flik,? as finding the best
grammatical ~m and its best tree?not exactly a pipeline.
be described as a weighted finite-state transducer,
then this joint model is their weighted composition,
which is a weighted CFG; call the composed gram-
mar I and its (unnormalized) distribution pI . Com-
pared to G, I will have many more nonterminals if
pL has a Markov order greater than 0 (unigram, as
above). Because parsing runtime depends heavily on
the grammar constant (at best, quadratic in the num-
ber of nonterminals), parsing with pI is not compu-
tationally attractive.4 fpoe,? is not, then, a scalable
solution when we wish to use a morphology model
pL that can make interdependent decisions about dif-
ferent words in ~x in context. We propose two new,
efficient dynamic programming solutions for joint
parsing.
In the first, we approximate the distribution
pL( ~M | ~x) using a unigram-factored model of the
form in Eq. 3:
p?L(~m | ~x) =
?n
i=1 pL( ~Mi = ~mi | ~x)? ?? ?
posterior, depends on all of ~x
(7)
Similar methods were applied by Matsuzaki et al
(2005) and Petrov and Klein (2007) for parsing un-
der a PCFG with nonterminals with latent anno-
tations. Their approach was variational, approxi-
mating the true posterior over coarse parses using
a sentence-specific PCFG on the coarse nontermi-
nals, created directly out of the true fine-grained
PCFG. In our case, we approximate the full distri-
bution over morphological analyses for the sentence
by a simpler, sentence-specific unigram model that
assumes each word?s analysis is to be chosen inde-
pendently of the others. Note that our model (pL)
does not make such an assumption, only the ap-
proximate model p?L does, and the approximation is
per-sentence. The idea resembles a mean-field vari-
ational approximation for graphical models. Turn-
ing to implementation, we can solve for pL(~mi | ~x)
exactly using the forward-backward algorithm. We
will call this method fvari,? (see Eq. 5).
A closely related method, applied by Goodman
(1996) is called minimum-risk decoding. Good-
man called it ?maximum expected recall? when ap-
plying it to parsing. In the HMM community it
4In prior work involving factored syntax models?
lexicalized (Klein and Manning, 2003b) and bilingual (Smith
and Smith, 2004)?fpoe,1 was applied, and the asymptotic run-
time went to O(n5) and O(n7).
211
fpoe,?(~x) = argmax
?~m,???GEN(~x)
log pG(?, ~m) + ? log pL(~m | ~x) (4)
fvari,?(~x) = argmax
?~m,???GEN(~x)
log pG(?, ~m) + ?
?n
i=1 log pL(~mi | ~x) (5)
frisk,?(~x) = argmax
?~m,???GEN(~x)
log pG(?, ~m) + ?
?n
i=1 pL(~mi | ~x) (6)
is sometimes called ?posterior decoding.? Mini-
mum risk decoding is attributable to Goel and Byrne
(2000). Applied to a single model, it factors the
parsing decision by penalizable errors, and chooses
the solution that minimizes the risk (expected num-
ber of errors under the model). This factors into a
sum of expectations, one per potential mistake. This
method is expensive for parsing models (since it re-
quires the Inside algorithm to compute expected re-
call mistakes), but entirely reasonable for sequence
labeling models. The idea is to score each word-
analysis ~mi in the morphological lattice by the ex-
pected value (under pL) that ~mi is present in the fi-
nal analysis ~m. This is, of course pL( ~Mi = ~mi | ~x),
the same quantity computed for fvari,?, except the
score of a path in the lattice is now a sum of pos-
teriors rather than a product. Our second approxi-
mate joint parser tries to maximize the probability
of the parse (as before) and at the same time to min-
imize the risk of the morphological analysis. See
frisk,? in Eq. 6; the only difference between frisk,?
and fvari,? is whether posteriors are added (frisk,?)
or multiplied (fvari,?).
To summarize this section, fvari,? and frisk,?
are two approximations to the expensive-in-general
fpoe,? that boil down to parsing over weighted lat-
tices. The only difference between them is how
the lattice is weighted: using ? log pL(~mi | ~x) for
fvari,? or using ?pL(~mi | ~x) for frisk,?.5 In case of
a unigram pL, fpoe,? is equivalent to fvari,?; other-
wise fpoe,? is likely to be too expensive.
3.4 Lattice Parsing
To parse the weighted lattices using fvari,? and
frisk,? in the previous section, we use lattice parsing.
Lattice parsing is a straightforward generalization of
5Until now, we have talked about weighting word analyses,
which may cover several arcs, rather than arcs. In practice we
apply the weight to the first arc of a word analysis, and weight
the remaining arcs of that analysis with 0 (no cost or benefit),
giving the desired effect.
string parsing that indexes constituents by states in
the lattice rather than word interstices. At parsing
time, a ?max, +? lattice parser finds the best com-
bined parse tree and path through the lattice. Im-
portantly, the data structures that are used in chart
parsing need not change in order to accommodate
lattices. The generalization over classic Earley or
CKY parsing is simple: keep in the parsing chart
constituents created over a pair of start state and end
state (instead of start position and end position), and
(if desired) factor in weights on lattice arcs; see Hall
(2005).
4 Factored Models
A fair comparison of joint and pipeline parsing must
make some attempt to control for the component
models. We describe here two PCFGs we used for
pG(?, ~m) and two finite-state morphological models
we used for pL(~m | ~x). We show how these mod-
els perform in stand-alone evaluations. For all ex-
periments, we used the Hebrew Treebank (Sima?an
et al, 2001). After removing traces and removing
functional information from the nonterminals, we
had 3,770 sentences in the training set, 371 sen-
tences in the development set (used primarily to se-
lect the value of ?) and 370 sentences in the test set.
4.1 Syntax Model
Our first syntax model is an unbinarized PCFG
trained using relative frequencies. Preterminal (POS
tag ? morpheme) rules are smoothed using back-
off to a model that predicts the morpheme length
and letter sequence. The PCFG is not binarized.
This grammar is remarkably good, given the lim-
ited effort that went into it. The rules in the train-
ing set had high coverage with respect to the de-
velopment set: an oracle experiment in which we
maximized the number of recovered gold-standard
constituents (on the development set) gave F1 ac-
curacy of 93.7%. In fact, its accuracy supersedes
212
more complex, lexicalized, models: given gold-
standard morphology, it achieves 81.2% (compared
to 72.0% by Bikel?s parser, with head rules specified
by a native speaker). This is probably attributable
to the dataset?s size, which makes training with
highly-parameterized lexicalized models precarious
and prone to overfitting. With first-order vertical
markovization (i.e., annotating each nonterminal
with its parent as in Johnson, 1998), accuracy is also
at 81.2%. Tuning the horizontal markovization of
the grammar rules (Klein and Manning, 2003a) had
a small, adverse effect on this dataset.
Since the PCFG model was relatively successful
compared to lexicalized models, and is faster to run,
we decided to use a vanilla PCFG, denoted Gvan,
and a parent-annotated version of that PCFG (John-
son, 1998), denoted Gv=2.
4.2 Morphology Model
Both of our morphology models use the same mor-
phological lexicon L, which we describe first.
4.2.1 Morphological Lexicon
In this work, a morphological analysis of a word
is a sequence of morphemes, possibly with a tag for
each morpheme. There are several available analyz-
ers for Hebrew, including Yona and Wintner (2005)
and Segal (2000). We use instead an empirically-
constructed generative lexicon that has the advan-
tage of matching the Treebank data and conventions.
If the Treebank is enriched, this would then directly
benefit the lexicon and our models.
Starting with the training data from the Hebrew
Treebank, we first create a set of prefixesMp ? M;
this set includes any morpheme seen in a non-final
position within any word. We also create a set of
stems Ms ? M that includes any morpheme seen
in a final position in a word. This effectively cap-
tures the morphological analysis convention in the
Hebrew Treebank, where a stem is prefixed by a rel-
atively dominant low-entropy sequence of 0?5 prefix
morphemes. For example,MHKLB (?from the dog?)
is analyzed as M+H+KLB with prefixes M (?from?)
and H (?the?) and KLB (?dog?) is the stem. In prac-
tice, |Mp| = 124 (including some conventions for
numerals) and |Ms| = 13,588. The morphological
lexicon is then defined as any analysis givenMp and
Ms:
L(x) = {mk1 ? M
?
p ?Ms | concat(m
k
1) = x)}
?{mk1 | count(m
k
1, x) ? 1} (9)
where mk1 denotes ?m1, ...,mk? and count(m
k
1, x)
denotes the number of occurrences of x disam-
biguated as mk1 in the training set. Note that L(x)
also includes any analysis of x observed in the train-
ing data. This permits the memorization of any
observed analysis that is more involved than sim-
ple segmentation (4% of word tokens in the train-
ing set; e.g., LXDR (?to the room?) is analyzed as
L+H+XDR). This will have an effect on evaluation
(see ?5.1). On the development data, L has 98.6%
coverage.
4.2.2 Unigram Baseline
The baseline morphology model, puniL , first de-
fines a joint distribution following Eq. 8. The word
model factors out when we conditionalize to form
puniL (?m1, ...,mk? | x). The prefix sequence model
is multinomial estimated by MLE. The stem model
(conditioned on the prefix sequence) is smoothed to
permit any stem that is a sequence of Hebrew char-
acters. On the development data, puniL is 88.8% ac-
curate (by word).
4.2.3 Conditional Random Field
The second morphology model, pcrfL , which is
based on the same morphological lexicon L, uses
a second-order conditional random field (Lafferty et
al., 2001) to disambiguate the full sentence by mod-
eling local contexts (Kudo et al, 2004; Smith et al,
2005b). Space does not permit a full description; the
model uses all the features of Smith et al (2005b)
except the ?lemma? portion of the model, since the
Hebrew Treebank does not provide lemmas. The
weights are trained to maximize the probability of
the correct path through the morphological lattice,
conditioned on the lattice. This is therefore a dis-
criminative model that defines pL(~m | ~x) directly,
though we ignore the normalization factor in pars-
ing.
Until now we have described pL as a model of
morphemes, but this CRF is trained to predict POS
tags as well?we can either use the tags (i.e., label
the morphological lattice with tag/morpheme pairs,
213
puniL (?m1,m2, ...,mk?, x) = p(x | ?m1,m2, ...,mk?)? ?? ?
word
? p(mk | ?m1, ...,mk?1?)
? ?? ?
stem
? p(?m1, ...,mk?1?)
? ?? ?
prefix sequence
(8)
so that the lattice parser finds a parse that is con-
sistent under both models), or sum the tags out and
let the parser do the tagging. One subtlety is the
tagging of words not seen in the training data; for
such words an unsegmented hypothesis with tag UN-
KNOWN is included in the lattice and may therefore
be selected by the CRF. On the development data,
pcrfL is 89.8% accurate on morphology, with 74.9%
fine-grained POS-tagging F1-accuracy (see ?5.1).
Note on generative and discriminative models.
The reader may be skeptical of our choice to com-
bine a generative PCFG with a discrimative CRF.
We point out that both are used to define conditional
distributions over desired ?output? structures given
?input? sequences. Notwithstanding the fact that the
factors can be estimated in very different ways, our
combination in an exact or approximate product-of-
experts is a reasonable and principled approach.
5 Experiments
In this section we evaluate parsing performance, but
an evaluation issue is resolved first.
5.1 Evaluation Measures
The ?Parseval? measures (Black et al, 1991) are
used to evaluate a parser?s phrase-structure trees
against a gold standard. They compute precision and
recall of constituents, each indexed by a label and
two endpoints. As pointed out by Tsarfaty (2006),
joint parsing of morphology and syntax renders this
indexing inappropriate, since it assumes the yields
of the trees are identical?that assumption is vio-
lated if there are any errors in the hypothesized ~m.
Tsarfaty (2006) instead indexed by non-whitespace
character positions, to deal with segmentation mis-
matches. In general (and in this work) that is still
insufficient, since L(~x) may include ~m that are not
simply segmentations of ~x (see ?4.2.1).
Roark et al (2006) propose an evaluation met-
ric for comparing a parse tree over a sentence gen-
erated by a speech recognizer to a gold-standard
parse. As in our case, the hypothesized tree could
have a different yield than the original gold-standard
parse tree, because of errors made by the speech
recognizer. The metric is based on an alignment
between the hypothesized sentence and the gold-
standard sentence. We used a similar evaluation
metric, which takes into account the information
about parallel word boundaries as well, a piece of
information that does not appear naturally in speech
recognition. Given the correct ~m? and the hypothe-
sis ~?m, we use dynamic programming to find an op-
timal many-to-many monotonic alignment between
the atomic morphemes in the two sequences. The
algorithm penalizes each violation (by a morpheme)
of a one-to-one correspondence,6 and each character
edit required to transform one side of a correspon-
dence into the other (without whitespace). Word
boundaries are (here) known and included as index
positions. In the case where ~?m = ~m? (or equal up to
whitespace) the method is identical to Parseval (and
also to Tsarfaty, 2006). POS tag accuracy is evalu-
ated the same way, for the same reasons; we report
F1-accuracy for tagging and parsing.
5.2 Experimental Comparison
In our experiment we vary four settings:
? Decoding algorithm: fpoe,?, frisk,?, or fvari,?
(?3.3).
? Syntax model: Gvan or Gv=2 (?4.1).
? Morphology model: puniL or p
crf
L (?4.2). In the lat-
ter case, we can use the scores over morpheme
sequences only (summing out tags before lattice
parsing; denoted m.-pcrfL ) or the full model over
morphemes and tags, denoted t.-pcrfL .
7
? ?, the relative strength given to the morphol-
ogy model (see ?3). We tested values of ? in
{0,+?} ? {10q | q ? {0, 1, ..., 16}}. Recall
that ? = 0 ignores the morphology model prob-
abilities altogether (using an unweighted lattice),
6That is, in a correspondence of a morphemes in one string
with b in the other, the penalty is a+ b?2, since the morpheme
on each side is not in violation.
7One subtlety is that any arc with the UNKNOWN POS
tag can be relabeled?to any other tag?by the syntax model,
whose preterminal rules are smoothed. This was crucial for
? = +? (pipeline) parsing with t.-pcrfL as the morphology
model, since the parser does not recognize UNKNOWN as a tag.
214
tuned ? pipeline (? ? +?)
pa
rs
er
m
or
ph
. m
od
el
sy
nt
ax
m
od
el
se
g.
ac
c
fin
e
PO
S
F 1
co
ar
se
PO
S
F 1
pa
rs
e
F 1
se
g.
ac
c
fin
e
PO
S
F 1
co
ar
se
PO
S
F 1
pa
rs
e
F 1
puniL pGvan 88.0 70.6 75.5 59.5 88.5 71.5 76.1 59.8
pGv=2 88.0 70.7 75.8 60.4 88.6 70.8 75.7 59.9
m.-pcrfL pGvan ? ? ? ? 90.9 75.6 80.2 63.7
f p
o
e,
?
pGv=2 ? ? ? ? 90.9 75.3 80.2 64.2
t.-pcrfL pGvan ? ? ? ? 90.9 77.2
?81.5 63.0
pGv=2 ? ? ? ? 90.9 77.2
?81.5 64.0
puniL pGvan 87.9 70.9 75.3 58.9 88.5 71.5 76.1 59.8
pGv=2 87.8 70.9 75.6 59.5 88.6 70.8 75.6 59.9
m.-pcrfL pGvan 89.8 74.5 78.9 62.5 89.8 74.5 78.9 62.4
f r
is
k
,?
pGv=2 89.8 74.3 79.1 63.0 89.8 74.3 79.1 63.0
t.-pcrfL pGvan 90.2 76.6 80.5 62.4 89.9 76.4 80.4 61.6
pGv=2 90.2 76.6 80.5 63.1 89.9 76.4 80.4 62.2
puniL pGvan 88.0 70.6 75.5 59.5 88.5 71.5 76.1 59.8
pGv=2 88.0 70.7 75.8 60.4 88.6 70.8 75.7 59.9
m.-pcrfL pGvan
?91.1 75.6 80.4 64.0 90.9 74.8 79.3 62.9
pGv=2 90.9 75.4 80.5
?64.4 90.1 74.6 79.5 63.2
f v
a
ri
,?
t.-pcrfL pGvan
?91.3 ?77.7 ?81.7 63.0 90.9 77.0 ?81.3 62.6
pGv=2
?91.3 ?77.6 ?81.6 63.6 90.9 77.0 ?81.3 63.6
Table 1: Results of experi-
ments on Hebrew (test data,
max. length 40). This table
shows the performance of
joint parsing (finite ?; left)
and a pipeline (? ? +?;
right). Joint parsing with a non-
unigram morphology model
is too expensive (marked ?).
Morphological analysis accu-
racy (by word), fine-grained
(full tags) and coarse-grained
(only parts of speech) POS
tagging accuracy (F1), and gen-
eralized constituent accuracy
(F1) are reported; ? was tuned
for each of these separately.
Boldface denotes that figures
were significantly better than
their counterparts in the same
row, under a binomial sign test
(p < 0.05). ? marks the best
overall accuracy and figures
that are not significantly worse
(binomial sign test, p < 0.05).
and as ? ? +? a morphology-first pipeline is
approached.
We measured four outcome values: segmentation
accuracy (fraction of word tokens segmented cor-
rectly), fine- and coarse-grained tagging accuracy,8
and parsing accuracy. For tagging and parsing, F1-
measures are given, according to the generalized
evaluation measure described in ?5.1.
5.3 Results
Tab. 1 compares parsing with tuned ? values to the
pipeline.
The best results were achieved using fvari,?, us-
ing the CRF and joint disambiguation. Without the
CRF (using puniL ), the difference between the decod-
ing algorithms is less apparent, suggesting an inter-
action between the sophistication of the components
and the best way to decode with them. These re-
sults suggest that fvari,?, which permits pL to ?veto?
any structure involving a morphological analysis for
any word that is a posteriori unlikely (note that
8Although the Hebrew Treebank is small, the size of its POS
tagset is large (four times larger than the Penn Treebank), be-
cause the tags encode morphological features (gender, person,
and number). These features have either been ignored in prior
work or encoded differently. In order for our POS-tagging fig-
ures to be reasonably comparable to previous work, we include
accuracy for coarse-grained tags (only the core part of speech)
tags as well as the detailed Hebrew Treebank tags.
log pL(~mi | ~x) can be an arbitrarily large negative
number), is beneficial as a ?filter? on parses.9 frisk,?,
on the other hand, is only allowed to give ?bonuses?
of up to ? to each morphological analysis that pL
believes in; its influence is therefore weaker. This
result is consistent with the findings of Petrov et al
(2007) for another approximate parsing task.
The advantage of the parent-annotated PCFG is
also more apparent when the CRF is used for mor-
phology, and when ? is tuned. All other things
equal, then, pcrfL led to higher accuracy all around.
Letting the CRF help predict the POS tags helped
tagging accuracy but not parsing accuracy.
While the gains over the pipeline are modest,
the segmentation, fine POS, and parsing accuracy
scores achieved by joint disambiguation with fvari,?
with the CRF are significantly better than any of the
pipeline conditions.
Interestingly, if we had not tested with the CRF,
we might have reached a very different conclusion
about the usefulness of tuning ? as opposed to a
pipeline. With the unigram morphology model,
joint parsing frequently underperforms the pipeline,
sometimes even signficantly. The explanation, we
9Another way to describe this combination is to call it a
product of |~x|+1 experts: one for the morphological analysis of
each word, plus the grammar. The morphology experts (softly)
veto any analysis that is dubious based on surface criteria, and
the grammar (softly) vetoes less-grammatical parses.
215
pa
rs
er
m
or
ph
. m
od
el
sy
nt
ax
m
od
el
se
g.
ac
c
fin
e
PO
S
F 1
co
ar
se
PO
S
F 1
pa
rs
e
F 1
puniL pGvan 90.7 73.4 78.5 64.3
pGv=2 90.2 73.0 78.5 64.9
m.-pcrfL pGvan 90.7 75.4 80.0 65.2
f r
is
k
,?
pGv=2 90.8 75.1 80.2 65.4
t.-pcrfL pGvan 91.2 78.1 82.4 65.7
pGv=2 91.1 78.0 82.2 66.2
puniL pGvan 90.6 73.2 78.3 63.5
pGv=2 90.2 72.8 78.4 64.4
m.-pcrfL pGvan 92.0 76.6 81.5 66.9
pGv=2 91.9 76.2 81.6 66.9
f v
a
ri
,?
t.-pcrfL pGvan 91.8 79.1 83.2 66.5
pGv=2 91.7 78.7 83.0 67.4
Table 2: Oracle results of experiments on Hebrew (test data,
max. length 40). This table shows the performance of mor-
phological segmentation, part-of-speech tagging, coarse part-
of-speech tagging and parsing when using an oracle to select
the best ? for each sentence. The notation and interpretation of
the numbers are the same as in Tab. 1.
believe, has to do with the ability of the unigram
model to estimate a good distribution over analy-
ses. While the unigram model is nearly as good
as the CRF at picking the right segmentation for a
word, joint parsing demands much more. In case
the best segmentation does not lead to a grammat-
ical morpheme sequence (under the syntax model),
the morphology model needs to be able to give rela-
tive strengths to the alternatives. The unigrammodel
is less able to do this, because it ignores the context
of the word, and so the benefit of joint parsing is lost.
Most commonly the tuned value of ? is around
10 (not shown, to preserve clarity). Because of ig-
nored normalization constants, this does not mean
that morphology is ?10? more important than syn-
tax,? but it does mean that, for a particular pL and
pG, tuning their relative importance in decoding can
improve accuracy. In Tab. 2 we show how perfor-
mance would improve if the oracle value of ? was
selected for each test-set sentence; this further high-
lights the potential impact of perfecting the tradeoff
between models. Of course, selecting ? automati-
cally at test-time, per sentence, is an open problem.
To our knowledge, the parsers we have described
represent the state-of-the-art in Modern Hebrew
parsing. The closest result is Tsarfaty (2006), which
we have not directly replicated. Tsarfaty?s model is
essentially a pipeline application of fpoe,? with a
grammar like pGvan . Her work focused more on the
interplay between the segmentation and POS tag-
ging models and the amount of information passed
to the parser. Some key differences preclude direct
comparison: we modeled fine-grained tags (though
we report both kinds of tagging accurcy), we em-
ployed a richer morphological lexicon (permitting
analyses that are not just segmentation), and a dif-
ferent training/test split and length filter (we used
longer sentences). Nonetheless, our conclusions
support the argument in Tsarfaty (2006) for more in-
tegrated parsing methods.
We conclude that tuning the relative importance
of the two models?rather than pipelining to give
one infinitely more importance?can provide an im-
provement on segmentation, tagging, and parsing
accuracy. This suggests that future parsing efforts
for languages with rich morphology might con-
tinue to assume separately-trained (and separately-
improved) morphology and syntax components,
which would stand to gain from joint decoding. In
our experiments, better morphological disambigua-
tion was crucial to getting any benefit from joint
decoding. Our result also suggests that exploring
new, fully-integrated models (and training methods
for them) may be advantageous.
6 Conclusion
We showed that joint morpho-syntactic parsing can
improve the accuracy of both kinds of disambigua-
tion. Several efficient parsing methods were pre-
sented, using factored state-of-the-art morphology
and syntax models for the language under considera-
tion. We demonstrated state-of-the-art performance
on and consistent improvements across many set-
tings for Modern Hebrew, a morphologically-rich
language with a relatively small treebank.
References
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological
disambiguation. In Proc. of COLING-ACL.
R. Bar-Haim, K. Sima?an, and Y. Winter. 2005. Choos-
ing an optimal architecture for segmentation and POS-
tagging of Modern Hebrew. In Proc. of ACLWorkshop
on Computational Approaches to Semitic Languages.
K. R. Beesley and L. Karttunen. 2003. Finite State Mor-
phology. CSLI.
216
D. Bikel. 2004. Multilingual statistical pars-
ing engine. http://www.cis.upenn.edu/
?dbikel/software.html#stat-parser.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proc. of DARPA Workshop on
Speech and Natural Language.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, U. Penn.
B. Cowan and M. Collins. 2005. Morphology and
reranking for the statistical parsing of Spanish. In
Proc. of HLT-EMNLP.
E. Daya, D. Roth, and S. Wintner. 2004. Learning
Hebrew roots: Machine learning with linguistic con-
straints. In Proc. of EMNLP.
V. Goel and W. Byrne. 2000. Minimum Bayes risk auto-
matic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of natural language. Comp. Ling., 27(2):153?
198.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging, and morphological dis-
ambiguation in one fell swoop. In Proc. of ACL.
J. Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petkevic?.
2001. Serial combination of rules and statistics: A
case study in Czech tagging. In Proc. of ACL.
D. Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2000. Statis-
tical morphological disambiguation for agglutinative
languages. In Proc. of COLING.
K. Hall. 2005. Best-first Word-lattice Parsing: Tech-
niques for Integrated Syntactic Language Modeling.
Ph.D. thesis, Brown University.
G. E. Hinton. 1999. Products of experts. In Proc. of
ICANN.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Comp. Ling., 24(4):613?632.
R. M. Kaplan and M. Kay. 1981. Phonological rules and
finite-state transducers. Presented at LSA.
D. Klein and C. D. Manning. 2003a. Accurate unlexical-
ized parsing. In Proc. of ACL, pages 423?430.
D. Klein and C. D. Manning. 2003b. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
K. Koskenniemi. 1983. A general computational model
of word-form recognition and production. Technical
Report 11, University of Helsinki.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
plying conditional random fields to Japanese morpho-
logical analysis. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. Lavie, S. Wintner, Y. Eytani, E. Peterson, and
K. Probst. 2004. Rapid prototyping of a transfer-
based Hebrew-to-English machine translation system.
In Proc. of TMI.
M. Levinger, U. Ornan, and A. Itai. 1995. Learning mor-
pholexical probabilities from an untagged corpus with
an application to Hebrew. Comp. Ling., 21:383?404.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proc. of NEMLAR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words: Lattice-based word error mini-
mization. In Proc. of ECSCT.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proc. of ACL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
B. Roark, M. Harper, E. Charniak, B. Dorr, M. Johnson,
J. Kahn, Y. Liu, M. Ostendorf, J. Hale, A. Krasnyan-
skaya, M. Lease, I. Shafran, M. Snover, R. Stewart,
and Lisa Yung. 2006. Sparseval: Evaluation metrics
for parsing speech. In Proc. of LREC.
E. Segal. 2000. A probabilistic morphological analyzer
for Hebrew undotted texts. Master?s thesis, Technion.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a treebank of modern Hebrew text.
Journal Traitement Automatique des Langues. Avail-
able at http://mila.cs.technion.ac.il.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proc. of EMNLP, pages 49?56.
A. Smith, T. Cohn, and M. Osborne. 2005a. Logarithmic
opinion pools for conditional random fields. In Proc.
of ACL.
N. A. Smith, D. A. Smith, and R. W. Tromble.
2005b. Context-based morphological disambiguation
with random fields. In Proc. of HLT-EMNLP.
R. Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for Modern Hebrew. In Proc. of
COLING-ACL Student Research Workshop.
R. Wicentowski. 2002. Modeling and Learning Mul-
tilingual Inflectional Morphology in a Minimally Su-
pervised Framework. Ph.D. thesis, Johns Hopkins U.
S. Wintner. 2004. Hebrew computational linguistics:
Past and future. Art. Int. Rev., 21(2):113?138.
S. Yona and S. Wintner. 2005. A finite-state morpholog-
ical grammar of Hebrew. In Proc. of ACL Workshop
on Computational Approaches to Semitic Languages.
217
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157?166,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Stacking Dependency Parsers
Andre? F. T. Martins?? Dipanjan Das? Noah A. Smith? Eric P. Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,dipanjan,nasmith,epxing}@cs.cmu.edu
Abstract
We explore a stacked framework for learn-
ing to predict dependency structures for natu-
ral language sentences. A typical approach in
graph-based dependency parsing has been to
assume a factorized model, where local fea-
tures are used but a global function is opti-
mized (McDonald et al, 2005b). Recently
Nivre and McDonald (2008) used the output
of one dependency parser to provide features
for another. We show that this is an example
of stacked learning, in which a second pre-
dictor is trained to improve the performance
of the first. Further, we argue that this tech-
nique is a novel way of approximating rich
non-local features in the second parser, with-
out sacrificing efficient, model-optimal pre-
diction. Experiments on twelve languages
show that stacking transition-based and graph-
based parsers improves performance over ex-
isting state-of-the-art dependency parsers.
1 Introduction
In this paper we address a representation-efficiency
tradeoff in statistical natural language processing
through the use of stacked learning (Wolpert,
1992). This tradeoff is exemplified in dependency
parsing, illustrated in Fig. 1, on which we focus in
this paper:
? Exact algorithms for dependency parsing (Eis-
ner and Satta, 1999; McDonald et al, 2005b)
are tractable only when the model makes very
strong, linguistically unsupportable independence
assumptions, such as ?arc factorization? for non-
projective dependency parsing (McDonald and
Satta, 2007).
? Feature-rich parsers must resort to search or
greediness, (Ratnaparkhi et al, 1994; Sagae and
Lavie, 2005; Hall et al, 2006), so that parsing
solutions are inexact and learned models may be
subject to certain kinds of bias (Lafferty et al,
2001).
A solution that leverages the complementary
strengths of these two approaches?described in de-
tail by McDonald and Nivre (2007)?was recently
and successfully explored by Nivre and McDonald
(2008). Our contribution begins by reinterpreting
and generalizing their parser combination scheme as
a stacking of parsers.
We give a new theoretical motivation for stacking
parsers, in terms of extending a parsing model?s fea-
ture space. Specifically, we view stacked learning as
a way of approximating non-local features in a lin-
ear model, rather than making empirically dubious
independence (McDonald et al, 2005b) or structural
assumptions (e.g., projectivity, Eisner, 1996), using
search approximations (Sagae and Lavie, 2005; Hall
et al, 2006; McDonald and Pereira, 2006), solving a
(generally NP-hard) integer linear program (Riedel
and Clarke, 2006), or adding latent variables (Titov
and Henderson, 2007). Notably, we introduce the
use of very rich non-local approximate features in
one parser, through the output of another parser.
Related approaches are the belief propagation algo-
rithm of Smith and Eisner (2008), and the ?trading
of structure for features? explored by Liang et al
157
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$ Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and eighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and P reira, 2006). How-
ever, i the data-dr ven parsing etting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our curr nt
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known t have exact n n-projective
implementations.
We th switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related W rk
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorit ms (Yamad and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). I the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, a
is the case for edge-factored mod ls (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A projective dependency parse (top), and a non-
projective dependency parse (bottom) for two English
sentences; examples from McDonald and Satta (2007).
(2008).
This paper focuses on dependency parsing, which
has become widely used in relation extraction (Cu-
lotta and Sorensen, 2004), machine translation
(Ding and Palmer, 2005), question answering (Wang
et al, 2007), and many other NLP applications.
We show that stacking methods outperform the ap-
proximate ?second-order? parser of McDonald and
Pereira (2006) on twelve languages and can be used
within that approximatio to chieve even better re-
sults. These results are similar in sp rit t (Nivre and
McDonald, 008), but with the following novel con-
tributio s:
? a stacking interpretation,
? a ric r fe tu e set that includes non-l c l f atures
(shown here to improve erform nc ), and
? a variety of stacking architectures.
Using stacking with rich features, we obtain results
comp titive ith Nivre an McDonald (2008) while
preserving the fast qua ratic parsing ime of arc-
factored spanning tree algorithms.
The paper is organiz d as follows. We discuss re-
lated prior work on dependency parsing and stacking
in ?2. Our model is given in ?3. A novel analysis of
stacking in linear models is given in ?4. Experiments
are presented in ?5.
2 Background and Related Work
We briefly review work on the NLP task of depen-
dency parsing and the machine learning framework
known as stacked learning.
2.1 Dep dency Parsing
Dependency syntax is a lightweight syntactic rep-
resentation that models a sentence as a graph where
the words are vertices and syntactic relationships are
directed edges (arcs) connecting heads to their argu-
ments and modifiers.
Dependency parsing is often viewed computa-
tionally as a structured prediction problem: for each
input sentence x, with n words, exponentially many
candidate depend ncy trees y ? Y(x) are possible in
principle. We den te each tree by its set of vertices
and directed arcs, y = (Vy, Ay). A legal depen-
dency tree has n+ 1 verti es, each corresponding to
one word plus a ?wall? symbol, $, assumed to be the
hidden root of the sentence. In a valid dependency
tree, each vertex except the root has exactly one par-
ent. In the projective case, arcs cannot cross when
depicted on one side of the sentence; in the non-
projective cas , this constraint is not impos d (see
Fig. 1).
2.1.1 Graph-based vs. transition-based models
Most recent work on dependency parsing can be
categorized as graph-based or transition-based. In
graph-based parsing, dependency trees are scored
by factoring the tree into its arcs, and parsing is
perfo med by searching for the highest scoring tree
(Eis er 1996; McD nald et al, 2005b). Transition-
based parsers model the sequ nce of ecisions of
shift-reduce parser, given previous deci ions and
current state, and par ing is perfor ed by greedily
choosing the highest scori g transition out of each
successive parsing state or by searching for the best
sequence of transitions (Ratnaparkhi et al, 1994;
Yamada and Matsumoto, 2003; Nivre et al, 2004;
Sagae and Lavie, 2005; Hall et al, 2006).
Both approaches most commonly use linear mod-
els to assign scores to arcs or decisions, so that a
score is a dot-product of a feature vector f and a
learned weight vector w.
In sum, these two lines of researc use different
approximations to achieve trac ability. Transition-
based approaches solve sequence of local prob-
lems in sequ nce, sacrificing global opti ality guar-
antees and possibly expressive power (Abney et al,
1999). Graph-based methods perform global in-
ference using score factorizations that correspond
to strong independence assumptions (discussed in
158
?2.1.2). Recently, Nivre and McDonald (2008) pro-
posed combining a graph-based and a transition-
based parser and have shown a significant improve-
ment for several languages by letting one of the
parsers ?guide? the other. Our stacked formalism
(to be described in ?3) generalizes this approach.
2.1.2 Arc factorization
In the successful graph-based method of McDon-
ald et al (2005b), an arc factorization independence
assumption is used to ensure tractability. This as-
sumption forbids any feature that depends on two
or more arcs, permitting only ?arc-factored? features
(i.e. features that depend only on a single candidate
arc a ? Ay and on the input sequence x). This in-
duces a decomposition of the feature vector f(x, y)
as:
f(x, y) =
?
a?Ay
fa(x).
Parsing amounts to solving arg maxy?Y(x)
w>f(x, y), where w is a weight vector. With
a projectivity constraint and arc factorization, the
parsing problem can be solved in cubic time by
dynamic programming (Eisner, 1996), and with a
weaker ?tree? constraint (permitting nonprojective
parses) and arc factorization, a quadratic-time
algorithm exists (Chu and Liu, 1965; Edmonds,
1967), as shown by McDonald et al (2005b). In
the projective case, the arc-factored assumption can
be weakened in certain ways while maintaining
polynomial parser runtime (Eisner and Satta, 1999),
but not in the nonprojective case (McDonald and
Satta, 2007), where finding the highest-scoring tree
becomes NP-hard.
McDonald and Pereira (2006) adopted an approx-
imation based on O(n3) projective parsing followed
by rearrangement to permit crossing arcs, achieving
higher performance. In ?3 we adopt a framework
that maintains O(n2) runtime (still exploiting the
Chu-Liu-Edmonds algorithm) while approximating
non arc-factored features.
2.2 Stacked Learning
Stacked generalization was first proposed by
Wolpert (1992) and Breiman (1996) for regression.
The idea is to include two ?levels? of predictors. The
first level, ?level 0,? includes one or more predictors
g1, . . . , gK : Rd ? R; each receives input x ? Rd
and outputs a prediction gk(x). The second level,
?level 1,? consists of a single function h : Rd+K ?
R that takes as input ?x, g1(x), . . . gK(x)? and out-
puts a final prediction y? = h(x, g1(x), . . . gK(x)).
The predictor, then, combines an ensemble (the gk)
with a meta-predictor (h).
Training is done as follows: the training data are
split into L partitions, and L instances of the level
0 predictor are trained in a ?leave-one-out? basis.
Then, an augmented dataset is formed by letting
each instance output predictions for the partition that
was left out. Finally, each level 0 predictor is trained
using the original dataset, and the level 1 predictor
is trained on the augmented dataset, simulating the
test-time setting when h is applied to a new instance
x concatenated with ?gk(x)?k.
This framework has also been applied to classifi-
cation, for example with structured data. Some ap-
plications (including here) use only one classifier at
level 0; recent work includes sequence labeling (Co-
hen and de Carvalho, 2005) and inference in condi-
tional random fields (Kou and Cohen, 2007). Stack-
ing is also intuitively related to transformation-based
learning (Brill, 1993).
3 Stacked Dependency Parsing
We next describe how to use stacked learning for
efficient, rich-featured dependency parsing.
3.1 Architecture
The architecture consists of two levels. At level 0
we include a single dependency parser. At runtime,
this ?level 0 parser? g processes an input sentence x
and outputs the set of predicted edges that make up
its estimation of the dependency tree, y?0 = g(x). At
level 1, we apply a dependency parser?in this work,
always a graph-based dependency parser?that uses
basic factored features plus new ones from the edges
predicted by the level 0 parser. The final parser pre-
dicts parse trees as h(x, g(x)), so that the total run-
time is additive in calculating h(?) and g(?).
The stacking framework is agnostic about the
form of g and h and the methods used to learn them
from data. In this work we use two well-known,
publicly available dependency parsers, MSTParser
(McDonald et al, 2005b),1 which implements ex-
1http://sourceforge.net/projects/mstparser
159
act first-order arc-factored nonprojective parsing
(?2.1.2) and approximate second-order nonprojec-
tive parsing, and MaltParser (Nivre et al, 2006),
which is a state-of-the-art transition-based parser.2
We do not alter the training algorithms used in prior
work for learning these two parsers from data. Us-
ing the existing parsers as starting points, we will
combine them in a variety of ways.
3.2 Training
Regardless of our choices for the specific parsers and
learning algorithms at level 0 and level 1, training is
done as sketched in ?2.2. Let D be a set of training
examples {?xi, yi?}i.
1. Split training data D into L partitions
D1, . . . ,DL.
2. Train L instances of the level 0 parser in the fol-
lowing way: the l-th instance, gl, is trained on
D?l = D \ Dl. Then use gl to output predic-
tions for the (unseen) partition Dl. At the end,
an augmented dataset D? =
?L
l=1 D?
l is built, so
that D? = {?xi, g(xi), yi?}i.
3. Train the level 0 parser g on the original training
data D.
4. Train the level 1 parser h on the augmented train-
ing data D?.
The runtime of this algorithm is O(LT0+T1), where
T0 and T1 are the individual runtimes required for
training level 0 and level 1 alone, respectively.
4 Two Views of Stacked Parsing
We next describe two motivations for stacking
parsers: as a way of augmenting the features of a
graph-based dependency parser or as a way to ap-
proximate higher-order models.
4.1 Adding Input Features
Suppose that the level 1 classifier is an arc-factored
graph-based parser. The feature vectors will take the
form3
f(x, y) = f1(x, y) ^ f2(x, y?0, y)
=
?
a?Ay
f1,a(x) ^ f2,a(x, g(x)),
2http://w3.msi.vxu.se/?jha/maltparser
3We use^ to denote vector concatenation.
where f1(x, y) =
?
a?Ay f1,a(x) are regu-
lar arc-factored features, and f2(x, y?0, y) =?
a?Ay f2,a(x, g(x)) are the stacked features. An
example of a stacked feature is a binary feature
f2,a(x, g(x)) that fires if and only if the arc a was
predicted by g, i.e., if a ? Ag(x); such a feature was
used by Nivre and McDonald (2008).
It is difficult in general to decide whether the in-
clusion of such a feature yields a better parser, since
features strongly correlate with each other. How-
ever, a popular heuristic for feature selection con-
sists of measuring the information gain provided by
each individual feature. In this case, we may obtain
a closed-form expression for the information gain
that f2,a(x, g(x)) provides about the existence or not
of the arc a in the actual dependency tree y. Let A
and A? be binary random variables associated with
the events a ? Ay and a? ? Ag(x), respectively. We
have:
I(A;A?) =
?
a,a??{0,1}
p(a, a?) log2
p(a, a?)
p(a)p(a?)
= H(A?)?
?
a?{0,1}
p(a)H(A?|A = a).
Assuming, for simplicity, that at level 0 the prob-
ability of false positives equals the probability of
false negatives (i.e., Perr , p(a? = 0|a = 1) =
p(a? = 1|a = 0)), and that the probability of
true positives equals the probability of true negatives
(1 ? Perr = p(a? = 0|a = 0) = p(a? = 1|a = 1)),
the expression above reduces to:
I(A;A?) = H(A?) + Perr log2 Perr
+ (1? Perr) log2(1? Perr)
= H(A?)?Herr,
where Herr denotes the entropy of the probability of
error on each arc?s prediction by the level 0 classi-
fier. If Perr ? 0.5 (i.e. if the level 0 classifier is
better than random), then the information gain pro-
vided by this simple stacked feature increases with
(a) the accuracy of the level 0 classifier, and (b) the
entropy H(A?) of the distribution associated with its
arc predictions.
4.2 Approximating Non-factored Features
Another way of interpreting the stacking framework
is as a means to approximate a higher order model,
160
such as one that is not arc-factored, by using stacked
features that make use of the predicted structure
around a candidate arc. Consider a second-order
model where the features decompose by arc and by
arc pair:
f(x, y) =
?
a1?Ay
?
?fa1(x) ^
?
a2?Ay
fa1,a2(x)
?
? .
Exact parsing under such model, with arbitrary
second-order features, is intractable (McDonald and
Satta, 2007). Let us now consider a stacked model
in which the level 0 predictor outputs a parse y?. At
level 1, we use arc-factored features that may be
written as
f?(x, y) =
?
a1?Ay
?
?fa1(x) ^
?
a2?Ay?
fa1,a2(x)
?
? ;
this model differs from the previous one only by re-
placing Ay by Ay? in the index set of the second sum-
mation. Since y? is given, this makes the latter model
arc-factored, and therefore, tractable. We can now
view f?(x, y) as an approximation of f(x, y); indeed,
we can bound the score approximation error,
?s(x, y) =
?
?
?w?>f?(x, y)?w>f(x, y)
?
?
? ,
where w? and w stand respectively for the parameters
learned for the stacked model and those that would
be learned for the (intractable) exact second order
model. We can bound ?s(x, y) by spliting it into
two terms: ?s(x, y) =
?
?
?(w? ?w)>f?(x, y) + w>(f?(x, y)? f(x, y))
?
?
?
?
?
?
?(w? ?w)>f?(x, y)
?
?
?
? ?? ?
,?str(x,y)
+
?
?
?w>(f?(x, y)? f(x, y))
?
?
?
? ?? ?
,?sdec(x,y)
;
where we introduced the terms ?str and ?sdec that
reflect the portion of the score approximation error
that are due to training error (i.e., different parame-
terizations of the exact and approximate models) and
decoding error (same parameterizations, but differ-
ent feature vectors). Using Ho?lder?s inequality, the
former term can be bounded as:
?str(x, y) =
?
?
?(w? ?w)>f?(x, y)
?
?
?
? ?w? ?w?1 ? ?f?(x, y)??
? ?w? ?w?1 ;
where ?.?1 and ?.?? denote the `1-norm and sup-
norm, respectively, and the last inequality holds
when the features are binary (so that ?f?(x, y)?? ?
1). The proper way to bound the term ?w? ?w?1
depends on the training algorithm. As for the de-
coding error term, it can bounded for a given weight
vector w, sentence x, candidate tree y, and level 0
prediction y?. Decomposing the weighted vector as
w = w1 ^ w2, w2 being the sub-vector associ-
ated with the second-order features, we have respec-
tively: ?sdec(x, y) =
?
?
?w>(f?(x, y)? f(x, y))
?
?
?
=
?
?
?
?
?
?
?
a1?Ay
w>2
?
?
?
a2?Ay?
fa1,a2(x)?
?
a2?Ay
fa1,a2(x)
?
?
?
?
?
?
?
?
?
?
a1?Ay
?
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
?
?
?
a1?Ay
|Ay??Ay| ? max
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
?
=
?
a1?Ay
2L(y, y?) ? max
a2?Ay??Ay
?
?
?w>2 fa1,a2(x)
?
?
? ,
where Ay??Ay , (Ay? ?Ay) ? (Ay ?Ay?) denotes
the symmetric difference of the sets Ay? and Ay,
which has cardinality 2L(y, y?), i.e., twice the Ham-
ming distance between the sequences of heads that
characterize y and the predicted parse y?. Using
Ho?lder?s inequality, we have both
?
?
?w>2 fa1,a2(x)
?
?
? ? ?w2?1 ? ?fa1,a2(x)??
and
?
?
?w>2 fa1,a2(x)
?
?
? ? ?w2?? ? ?fa1,a2(x)?1.
Assuming that all features are binary valued, we
have that ?fa1,a2(x)?? ? 1 and that ?fa1,a2(x)?1 ?
Nf,2, where Nf,2 denotes the maximum number of
active second order features for any possible pair of
arcs (a1, a2). Therefore:
?sdec(x, y) ? 2nL(y, y?) min{?w2?1, Nf,2??w2??},
where n is the sentence length. Although this bound
can be loose, it suggests (intuitively) that the score
approximation degrades as the predicted tree y? gets
farther away from the true tree y (in Hamming dis-
tance). It also degrades with the magnitude of
weights associated with the second-order features,
161
Name Description
PredEdge Indicates whether the candidate edge
was present, and what was its label.
Sibling Lemma, POS, link label, distance and
direction of attachment of the previous
and and next predicted siblings
GrandParents Lemma, POS, link label, distance and
direction of attachment of the grandpar-
ent of the current modifier
PredHead Predicted head of the candidate modifier
(if PredEdge=0)
AllChildren Sequence of POS and link labels of all
the predicted children of the candidate
head
Table 1: Feature sets derived from the level 0 parser.
Subset Description
A PredEdge
B PredEdge+Sibling
C PredEdge+Sibling+GrandParents
D PredEdge+Sibling+GrandParents+PredHead
E PredEdge+Sibling+GrandParents+PredHead+
AllChildren
Table 2: Combinations of features enumerated in Table 1
used for stacking. A is a replication of (Nivre and Mc-
Donald, 2008), except for the modifications described in
footnote 4.
which suggests that a separate regularization of the
first-order and stacked features might be beneficial
in a stacking framework.
As a side note, if we set each component of
the weight vector to one, we obtain a bound
on the `1-norm of the feature vector difference,?
?
?f?(x, y)? f(x, y)
?
?
?
1
? 2nL(y, y?)Nf,2.
5 Experiments
In the following experiments we demonstrate the ef-
fectiveness of stacking parsers. As noted in ?3.1, we
make use of two component parsers, the graph-based
MSTParser and the transition-based MaltParser.
5.1 Implementation and Experimental Details
The publicly available version of MSTParser per-
forms parsing and labeling jointly. We adapted this
system to first perform unlabeled parsing, then la-
bel the arcs using a log-linear classifier with access
to the full unlabeled parse (McDonald et al, 2005a;
McDonald et al, 2005b; McDonald and Pereira,
2006). In stacking experiments, the arc labels from
the level 0 parser are also used as a feature.4
In the following subsections, we refer to our mod-
ification of the MSTParser as MST 1O (the arc-
factored version) and MST 2O (the second-order
arc-pair-factored version). All our experiments use
the non-projective version of this parser. We refer to
the MaltParser as Malt .
We report experiments on twelve languages from
the CoNLL-X shared task (Buchholz and Marsi,
2006).5 All experiments are evaluated using the
labeled attachment score (LAS), using the default
settings.6 Statistical significance is measured us-
ing Dan Bikel?s randomized parsing evaluation com-
parator with 10,000 iterations.7 The additional fea-
tures used in the level 1 parser are enumerated in
Table 1 and their various subsets are depicted in Ta-
ble 2. The PredEdge features are exactly the six fea-
tures used by Nivre and McDonald (2008) in their
MSTMalt parser; therefore, feature set A is a repli-
cation of this parser except for modifications noted
in footnote 4. In all our experiments, the number of
partitions used to create D? is L = 2.
5.2 Experiment: MST 2O + MST 2O
Our first experiment stacks the highly accurate
MST 2O parser with itself. At level 0, the parser
uses only the standard features (?5.1), and at level 1,
these are augmented by various subsets of features
of x along with the output of the level 0 parser, g(x)
(Table 2). The results are shown in Table 3. While
we see improvements over the single-parser baseline
4We made other modifications to MSTParser, implement-
ing many of the successes described by (McDonald et al,
2006). Our version of the code is publicly available at http:
//www.ark.cs.cmu.edu/MSTParserStacked. The
modifications included an approximation to lemmas for datasets
without lemmas (three-character prefixes), and replacing mor-
phology/word and morphology/lemma features with morphol-
ogy/POS features.
5The CoNLL-X shared task actually involves thirteen lan-
guages; our experiments do not include Czech (the largest
dataset), due to time constraints. Therefore, the average results
plotted in the last rows of Tables 3, 4, and 5 are not directly
comparable with previously published averages over thirteen
languages.
6http://nextens.uvt.nl/?conll/software.html
7http://www.cis.upenn.edu/?dbikel/software.
html
162
MST
2O
+MS
T 2O
, A
+MS
T 2O
, B
+MS
T 2O
, C
+MS
T 2O
, D
+MS
T 2O
, E
Arabic 67.88 66.91 67.41 67.68 67.37 68.02
Bulgarian 87.31 87.39 87.03 87.61 87.57 87.55
Chinese 87.57 87.16 87.24 87.48 87.42 87.48
Danish 85.27 85.39 85.61 85.57 85.43 85.57
Dutch 79.99 79.79 79.79 79.83 80.17 80.13
German 87.44 86.92 87.32 87.32 87.26 87.04
Japanese 90.93 91.41 91.21 91.35 91.11 91.19
Portuguese 87.12 87.26 86.88 87.02 87.04 86.98
Slovene 74.02 74.30 74.30 74.00 74.14 73.94
Spanish 82.43 82.17 82.35 82.81 82.53 82.75
Swedish 82.87 82.99 82.95 82.51 83.01 82.69
Turkish 60.11 59.47 59.25 59.47 59.45 59.31
Average 81.08 80.93 80.94 81.05 81.04 81.05
Table 3: Results of stacking MST 2O with itself at both level 0 and level 1. Column 2 enumerates LAS for MST 2O.
Columns 3?6 enumerate results for four different stacked feature subsets. Bold indicates best results for a particular
language.
for nine languages, the improvements are small (less
than 0.5%). One of the biggest concerns about this
model is the fact that it stacks two predictors that
are very similar in nature: both are graph-based and
share the features f1,a(x). It has been pointed out by
Breiman (1996), among others, that the success of
ensemble methods like stacked learning strongly de-
pends on how uncorrelated the individual decisions
made by each predictor are from the others? deci-
sions.8 This experiment provides further evidence
for the claim.
5.3 Experiment: Malt + MST 2O
We next use MaltParser at level 0 and the second-
order arc-pair-factored MST 2O at level 1. This
extends the experiments of Nivre and McDonald
(2008), replicated in our feature subset A.
Table 4 enumerates the results. Note that the
best-performing stacked configuration for each and
every language outperforms MST 2O, corroborat-
ing results reported by Nivre and McDonald (2008).
The best performing stacked configuration outper-
forms Malt as well, except for Japanese and Turk-
ish. Further, our non-arc-factored features largely
outperform subset A, except on Bulgarian, Chinese,
8This claim has a parallel in the cotraining method (Blum
and Mitchell, 1998), whose performance is bounded by the de-
gree of independence between the two feature sets.
and Japanese. On average, the best feature config-
uration is E, which is statistically significant over
Malt and MST 2O with p < 0.0001, and over fea-
ture subset A with p < 0.01.
5.4 Experiment: Malt + MST 1O
Finally, we consider stacking MaltParser with the
first-order, arc-factored MSTParser. We view this
approach as perhaps the most promising, since it is
an exact parsing method with the quadratic runtime
complexity of MST 1O.
Table 5 enumerates the results. For all twelve
languages, some stacked configuration outperforms
MST 1O and also, surprisingly, MST 2O, the sec-
ond order model. This provides empirical evi-
dence that using rich features from MaltParser at
level 0, a stacked level 1 first-order MSTParser can
outperform the second-order MSTParser.9 In only
two cases (Japanese and Turkish), the MaltParser
slightly outperforms the stacked parser.
On average, feature configuration D performs
the best, and is statistically significant over Malt ,
MST 1O, and MST 2O with p < 0.0001, and over
feature subset A with p < 0.05. Encouragingly, this
configuration is barely outperformed by configura-
9Recall that MST 2O uses approximate search, as opposed
to stacking, which uses approximate features.
163
Mal
t
MST
2O
Mal
t +
MST
2O
, A
Mal
t +
MST
2O
, B
Mal
t +
MST
2O
, C
Mal
t +
MST
2O
, D
Mal
t +
MST
2O
, E
Arabic 66.71 67.88 68.56 69.12 68.64 68.34 68.92
Bulgarian 87.41 87.31 88.99 88.89 88.89 88.93 88.91
Chinese 86.92 87.57 88.41 88.31 88.29 88.13 88.41
Danish 84.77 85.27 86.45 86.67 86.79 86.13 86.71
Dutch 78.59 79.99 80.75 81.47 81.47 81.51 81.29
German 85.82 87.44 88.16 88.50 88.56 88.68 88.38
Japanese 91.65 90.93 91.63 91.43 91.59 91.61 91.49
Portuguese 87.60 87.12 88.00 88.24 88.30 88.18 88.22
Slovene 70.30 74.02 76.62 76.00 76.60 76.18 76.72
Spanish 81.29 82.43 83.09 83.73 83.47 83.21 83.43
Swedish 84.58 82.87 84.92 84.60 84.80 85.16 84.88
Turkish 65.68 60.11 64.35 64.51 64.51 65.07 65.21
Average 80.94 81.08 82.52 82.58 82.65 82.59 82.71
Table 4: Results of stacking Malt and MST 2O at level 0 and level 1, respectively. Columns 2?4 enumerate LAS for
Malt , MST 2O and Malt + MST 2O as in Nivre and McDonald (2008). Columns 5?8 enumerate results for four other
stacked feature configurations. Bold indicates best result for a language.
tion A of Malt + MST 2O (see Table 4), the dif-
ference being statistically insignificant (p > 0.05).
This shows that stacking Malt with the exact, arc-
factored MST 1O bridges the difference between the
individual MST 1O and MST 2O models, by approx-
imating higher order features, but maintaining an
O(n2) runtime and finding the model-optimal parse.
5.5 Disagreement as a Confidence Measure
In pipelines or semisupervised settings, it is use-
ful when a parser can provide a confidence measure
alongside its predicted parse tree. Because stacked
predictors use ensembles with observable outputs,
differences among those outputs may be used to es-
timate confidence in the final output. In stacked de-
pendency parsing, this can be done (for example) by
measuring the Hamming distance between the out-
puts of the level 0 and 1 parsers, L(g(x), h(x)). In-
deed, the bound derived in ?4.2 suggests that the
second-order approximation degrades for candidate
parses y that are Hamming-far from g(x); therefore,
if L(g(x), h(x)) is large, the best score s(x, h(x))
may well be ?biased? due to misleading neighbor-
ing information provided by the level 0 parser.
We illustrate this point with an empirical analysis
of the level 0/1 disagreement for the set of exper-
iments described in ?5.3; namely, we compare the
0 2 4 6 8 10
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
L(g(x),h(x))
Sent
ence
 Ave
rage
d Ac
cura
cy
 
 Level 0Level 1Level 0 (Overall)Level 1 (Overall)
Figure 2: Accuracy as a function of token disagreement
between level 0 and level 1. The x-axis is the Hamming
distance L(g(x), h(x)), i.e., the number of tokens where
level 0 and level 1 disagree. The y-axis is the accuracy
averaged over sentences that have the specified Hamming
distance, both for level 0 and level 1.
164
Mal
t
MST
1O
MST
2O
Mal
t +
MST
1O
, A
Mal
t +
MST
1O
, B
Mal
t +
MST
1O
, C
Mal
t +
MST
1O
, D
Mal
t +
MST
1O
, E
Arabic 66.71 66.81 67.88 68.40 68.50 68.20 68.42 68.68
Bulgarian 87.41 86.65 87.31 88.55 88.67 88.75 88.71 88.79
Chinese 86.92 86.60 87.57 87.67 87.73 87.83 87.67 87.61
Danish 84.77 84.87 85.27 86.59 86.27 86.21 86.35 86.15
Dutch 78.59 78.95 79.99 80.53 81.51 80.71 81.61 81.37
German 85.82 86.26 87.44 88.18 88.30 88.20 88.36 88.42
Japanese 91.65 91.01 90.93 91.55 91.53 91.51 91.43 91.57
Portuguese 87.60 86.28 87.12 88.16 88.26 88.46 88.26 88.36
Slovene 70.30 73.96 74.02 75.84 75.64 75.42 75.96 75.64
Spanish 81.29 81.07 82.43 82.61 83.13 83.13 83.09 82.99
Swedish 84.58 81.88 82.87 84.86 84.62 84.64 84.82 84.76
Turkish 65.68 59.63 60.11 64.49 64.97 64.47 64.63 64.61
Average 80.94 80.33 81.08 82.28 82.42 82.29 82.44 82.41
Table 5: Results of stacking Malt and MST 1O at level 0 and level 1, respectively. Columns 2?4 enumerate LAS for
Malt , MST 1O and MST 2O. Columns 5?9 enumerate results for five different stacked feature configurations. Bold
indicates the best result for a language.
level 0 and level 1 predictions under the best overall
configuration (configuration E of Malt+MST2O).
Figure 2 depicts accuracy as a function of level 0-
level 1 disagreement (in number of tokens), aver-
aged over all datasets.
We can see that performance degrades steeply
when the disagreement between levels 0 and 1 in-
creases in the range 0?4, and then behaves more ir-
regularly but keeping the same trend. This suggests
that the Hamming distance L(g(x), h(x)) is infor-
mative about parser performance and may be used
as a confidence measure.
6 Conclusion
In this work, we made use of stacked learning to im-
prove dependency parsing. We considered an archi-
tecture with two layers, where the output of a stan-
dard parser in the first level provides new features
for a parser in the subsequent level. During learning,
the second parser learns to correct mistakes made by
the first one. The novelty of our approach is in the
exploitation of higher-order predicted edges to simu-
late non-local features in the second parser. We pro-
vided a novel interpretation of stacking as feature
approximation, and our experimental results show
rich-featured stacked parsers outperforming state-
of-the-art single-layer and ensemble parsers. No-
tably, using a simple arc-factored parser at level 1,
we obtain an exact O(n2) stacked parser that outper-
forms earlier approximate methods (McDonald and
Pereira, 2006).
Acknowledgments
The authors thank the anonymous reviewers for
helpful comments, Vitor Carvalho, William Cohen,
and David Smith for interesting discussions, and
Ryan McDonald and Joakim Nivre for providing
us their code and preprocessed datasets. A.M. was
supported by a grant from FCT through the CMU-
Portugal Program and the Information and Com-
munications Technologies Institute (ICTI) at CMU.
N.S. was supported by NSF IIS-0713265 and an
IBM faculty award. E.X. was supported by NSF
DBI-0546594, DBI-0640543, and IIS-0713379.
References
S. P. Abney, D. A. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In Pro-
ceedings of ACL.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of COLT.
L. Breiman. 1996. Stacked regressions. Machine Learn-
ing, 24:49.
165
E. Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
W. W. Cohen and V. Rocha de Carvalho. 2005. Stacked
sequential learning. In Proceedings of IJCAI.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mar. In Proceedings of ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner and G. Satta. 1999. Efficient parsing for bilex-
ical context-free grammars and head automaton gram-
mars. In Proceedings of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
COLING.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of ACL.
Z. Kou and W. W. Cohen. 2007. Stacked graphical mod-
els for efficient inference in Markov random fields. In
Proceedings of SDM.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
P. Liang, H. Daume?, and D. Klein. 2008. Structure com-
pilation: trading structure for features. In Proceedings
of ICML.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. T. McDonald and F. C. N. Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. In
Proceedings of EACL.
R. McDonald and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing. In
Proceedings of IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005b. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of HLT-
EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proceedings CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-HLT.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
CoNLL.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A
maximum entropy model for parsing. In Proceedings
of ICSLP.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proceedings of EMNLP.
K. Sagae and A. Lavie. 2005. A classifier-based parser
with linear run-time complexity. In Proceedings of
IWPT.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proceedings of EMNLP.
I. Titov and J. Henderson. 2007. A latent variable model
for generative dependency parsing. In Proceedings of
IWPT.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
D. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241?260.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of IWPT.
166
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219?228,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Feature-Rich Translation by Quasi-Synchronous Lattice Parsing
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We present a machine translation frame-
work that can incorporate arbitrary fea-
tures of both input and output sentences.
The core of the approach is a novel de-
coder based on lattice parsing with quasi-
synchronous grammar (Smith and Eis-
ner, 2006), a syntactic formalism that
does not require source and target trees
to be isomorphic. Using generic approx-
imate dynamic programming techniques,
this decoder can handle ?non-local? fea-
tures. Similar approximate inference tech-
niques support efficient parameter esti-
mation with hidden variables. We use
the decoder to conduct controlled exper-
iments on a German-to-English transla-
tion task, to compare lexical phrase, syn-
tax, and combined models, and to mea-
sure effects of various restrictions on non-
isomorphism.
1 Introduction
We have seen rapid recent progress in machine
translation through the use of rich features and the
development of improved decoding algorithms,
often based on grammatical formalisms.
1
If we
view MT as a machine learning problem, features
and formalisms imply structural independence as-
sumptions, which are in turn exploited by efficient
inference algorithms, including decoders (Koehn
et al, 2003; Yamada and Knight, 2001). Hence a
tension is visible in the many recent research ef-
forts aiming to decode with ?non-local? features
(Chiang, 2007; Huang and Chiang, 2007).
Lopez (2009) recently argued for a separation
between features/formalisms (and the indepen-
1
Informally, features are ?parts? of a parallel sentence pair
and/or their mutual derivation structure (trees, alignments,
etc.). Features are often implied by a choice of formalism.
dence assumptions they imply) from inference al-
gorithms in MT; this separation is widely appreci-
ated in machine learning. Here we take first steps
toward such a ?universal? decoder, making the fol-
lowing contributions:
Arbitrary feature model (?2): We define a sin-
gle, direct log-linear translation model (Papineni
et al, 1997; Och and Ney, 2002) that encodes most
popular MT features and can be used to encode
any features on source and target sentences, de-
pendency trees, and alignments. The trees are op-
tional and can be easily removed, allowing sim-
ulation of ?string-to-tree,? ?tree-to-string,? ?tree-
to-tree,? and ?phrase-based? models, among many
others. We follow the widespread use of log-linear
modeling for direct translation modeling; the nov-
elty is in the use of richer feature sets than have
been previously used in a single model.
Decoding as QG parsing (?3?4): We present a
novel decoder based on lattice parsing with quasi-
synchronous grammar (QG; Smith and Eisner,
2006).
2
Further, we exploit generic approximate
inference techniques to incorporate arbitrary ?non-
local? features in the dynamic programming algo-
rithm (Chiang, 2007; Gimpel and Smith, 2009).
Parameter estimation (?5): We exploit simi-
lar approximate inference methods in regularized
pseudolikelihood estimation (Besag, 1975) with
hidden variables to discriminatively and efficiently
train our model. Because we start with inference
(the key subroutine in training), many other learn-
ing algorithms are possible.
Experimental platform (?6): The flexibility
of our model/decoder permits carefully controlled
experiments. We compare lexical phrase and de-
pendency syntax features, as well as a novel com-
2
To date, QG has been used for word alignment (Smith
and Eisner, 2006), adaptation and projection in parsing
(Smith and Eisner, 2009), and various monolingual recog-
nition and scoring tasks (Wang et al, 2007; Das and Smith,
2009); this paper represents its first application to MT.
219
?, T source and target language vocabularies, respectively
Trans : ? ? {NULL} ? 2
T
function mapping each source word to target words to which it may translate
s = ?s
0
, . . . , s
n
? ? ?
n
source language sentence (s
0
is the NULL word)
t = ?t
1
, . . . , t
m
? ? T
m
target language sentence, translation of s
?
s
: {1, . . . , n} ? {0, . . . , n} dependency tree of s, where ?
s
(i) is the index of the parent of s
i
(0 is the root, $)
?
t
: {1, . . . ,m} ? {0, . . . ,m} dependency tree of t, where ?
t
(i) is the index of the parent of t
i
(0 is the root, $)
a : {1, . . . ,m} ? 2
{1,...,n}
alignments from words in t to words in s; ? denotes alignment to NULL
? parameters of the model
g
trans
(s,a, t) lexical translation features (?2.1):
f
lex
(s, t) word-to-word translation features for translating s as t
f
phr
(s
j
i
, t
`
k
) phrase-to-phrase translation features for translating s
j
i
as t
`
k
g
lm
(t) language model features (?2.2):
f
N
(t
j
j?N+1
) N -gram probabilities
g
syn
(t, ?
t
) target syntactic features (?2.3):
f
att
(t, j, t
?
, k) syntactic features for attaching target word t
?
at position k to target word t at position j
f
val
(t, j, I) syntactic valence features with word t at position j having children I ? {1, . . . ,m}
g
reor
(s, ?
s
,a, t, ?
t
) reordering features (?2.4):
f
dist
(i, j) distortion features for a source word at position i aligned to a target word at position j
g
tree
2
(?
s
,a, ?
t
) tree-to-tree syntactic features (?3):
f
qg
(i, i
?
, j, k) configuration features for source pair s
i
/s
i
?
being aligned to target pair t
j
/t
k
g
cov
(a) coverage features (?4.2)
f
scov
(a), f
zth
(a), f
sunc
(a) counters for ?covering? each s word each time, the zth time, and leaving it ?uncovered?
Table 1: Key notation. Feature factorings are elaborated in Tab. 2.
bination of the two. We quantify the effects
of our approximate inference. We explore the
effects of various ways of restricting syntactic
non-isomorphism between source and target trees
through the QG. We do not report state-of-the-art
performance, but these experiments reveal inter-
esting trends that will inform continued research.
2 Model
(Table 1 explains notation.) Given a sentence s
and its parse tree ?
s
, we formulate the translation
problem as finding the target sentence t
?
(along
with its parse tree ?
?
t
and alignment a
?
to the
source tree) such that
3
?t
?
, ?
?
t
,a
?
? = argmax
?t,?
t
,a?
p(t, ?
t
,a | s, ?
s
) (1)
In order to include overlapping features and permit
hidden variables during training, we use a single
globally-normalized conditional log-linear model.
That is, p(t, ?
t
,a | s, ?
s
) =
exp{?
>
g(s, ?
s
,a, t, ?
t
)}
?
a
?
,t
?
,?
?
t
exp{?
>
g(s, ?
s
,a
?
, t
?
, ?
?
t
)}
(2)
where the g are arbitrary feature functions and the
? are feature weights. If one or both parse trees or
the word alignments are unavailable, they can be
ignored or marginalized out as hidden variables.
In a log-linear model over structured objects,
the choice of feature functions g has a huge effect
3
We assume in this work that s is parsed. In principle, we
might include source-side parsing as part of decoding.
on the feasibility of inference, including decoding.
Typically these feature functions are chosen to fac-
tor into local parts of the overall structure. We
next define some key features used in current MT
systems, explaining how they factor. We will use
subscripts on g to denote different groups of fea-
tures, which may depend on subsets of the struc-
tures t, ?
t
, a, s, and ?
s
. When these features fac-
tor into parts, we will use f to denote the factored
vectors, so that if x is an object that breaks into
parts {x
i
}
i
, then g(x) =
?
i
f(x
i
).
4
2.1 Lexical Translations
Classical lexical translation features depend on s
and t and the alignment a between them. The sim-
plest are word-to-word features, estimated as the
conditional probabilities p(t | s) and p(s | t) for
s ? ? and t ? T. Phrase-to-phrase features gen-
eralize these, estimated as p(t
?
| s
?
) and p(s
?
| t
?
)
where s
?
(respectively, t
?
) is a substring of s (t).
A major difference between the phrase features
used in this work and those used elsewhere is
that we do not assume that phrases segment into
disjoint parts of the source and target sentences
4
There are two conventional definitions of feature func-
tions. One is to let the range of these functions be conditional
probability estimates (Och and Ney, 2002). These estimates
are usually heuristic and inconsistent (Koehn et al, 2003).
An alternative is to instantiate features for different structural
patterns (Liang et al, 2006; Blunsom et al, 2008). This offers
more expressive power but may require much more training
data to avoid overfitting. For this reason, and to keep training
fast, we opt for the former convention, though our decoder
can handle both, and the factorings we describe are agnostic
about this choice.
220
(Koehn et al, 2003); they can overlap.
5
Addi-
tionally, since phrase features can be any func-
tion of words and alignments, we permit features
that consider phrase pairs in which a target word
outside the target phrase aligns to a source word
inside the source phrase, as well as phrase pairs
with gaps (Chiang, 2005; Ittycheriah and Roukos,
2007).
Lexical translation features factor as in Eq. 3
(Tab. 2). We score all phrase pairs in a sentence
pair that pair a target phrase with the smallest
source phrase that contains all of the alignments in
the target phrase; if
?
k:i?k?j
a(k) = ?, no phrase
feature fires for t
j
i
.
2.2 N -gram Language Model
N -gram language models have become standard
in machine translation systems. For bigrams and
trigrams (used in this paper), the factoring is in
Eq. 4 (Tab. 2).
2.3 Target Syntax
There have been many features proposed that con-
sider source- and target-language syntax during
translation. Syntax-based MT systems often use
features on grammar rules, frequently maximum
likelihood estimates of conditional probabilities in
a probabilistic grammar, but other syntactic fea-
tures are possible. For example, Quirk et al
(2005) use features involving phrases and source-
side dependency trees and Mi et al (2008) use
features from a forest of parses of the source sen-
tence. There is also substantial work in the use
of target-side syntax (Galley et al, 2006; Marcu
et al, 2006; Shen et al, 2008). In addition, re-
searchers have recently added syntactic features to
phrase-based and hierarchical phrase-based mod-
els (Gimpel and Smith, 2008; Haque et al, 2009;
Chiang et al, 2008).
In this work, we focus on syntactic features of
target-side dependency trees, ?
t
, along with the
words t. These include attachment features that
relate a word to its syntactic parent, and valence
features. They factor as in Eq. 5 (Tab. 2). Features
that consider only target-side syntax and words
without considering s can be seen as ?syntactic
language model? features (Shen et al, 2008).
5
Segmentation might be modeled as a hidden variable in
future work.
g
trans
(s,a, t) =
P
m
j=1
P
i?a(j)
f
lex
(s
i
, t
j
) (3)
+
P
i,j:1?i<j?m
f
phr
(s
last(i,j)
first(i,j)
, t
j
i
)
g
lm
(t) =
P
N?{2,3}
P
m+1
j=1
f
N
(t
j
j?N+1
) (4)
g
syn
(t, ?
t
) =
P
m
j=1
f
att
(t
j
, j, t
?
t
(j)
, ?
t
(j))
+f
val
(t
j
, j, ?
?1
t
(j)) (5)
g
reor
(s, ?
s
,a, t, ?
t
) =
P
m
j=1
P
i?a(j)
f
dist
(i, j) (6)
g
tree
2
(?
s
,a, ?
t
) =
m
X
j=1
f
qg
(a(j),a(?
t
(j)), j, ?
t
(j)) (7)
Table 2: Factoring of global feature collections g into
f . x
j
i
denotes ?x
i
, . . . x
j
? in sequence x = ?x
1
, . . .?.
first(i, j) = min
k:i?k?j
(min(a(k))) and last(i, j) =
max
k:i?k?j
(max(a(k))).
2.4 Reordering
Reordering features take many forms in MT. In
phrase-based systems, reordering is accomplished
both within phrase pairs (local reordering) as
well as through distance-based distortion mod-
els (Koehn et al, 2003) and lexicalized reorder-
ing models (Koehn et al, 2007). In syntax-based
systems, reordering is typically parameterized by
grammar rules. For generality we permit these
features to ?see? all structures and denote them
g
reor
(s, ?
s
,a, t, ?
t
). Eq. 6 (Tab. 2) shows a factor-
ing of reordering features based on absolute posi-
tions of aligned words.
We turn next to the ?backbone? model for our
decoder; the formalism and the properties of its
decoding algorithm will inspire two additional sets
of features.
3 Quasi-Synchronous Grammars
A quasi-synchronous dependency grammar
(QDG; Smith and Eisner, 2006) specifies a
conditional model p(t, ?
t
,a | s, ?
s
). Given a
source sentence s and its parse ?
s
, a QDG induces
a probabilistic monolingual dependency grammar
over sentences ?inspired? by the source sentence
and tree. We denote this grammar by G
s,?
s
; its
(weighted) language is the set of translations of s.
Each word generated by G
s,?
s
is annotated with
a ?sense,? which consists of zero or more words
from s. The senses imply an alignment (a) be-
tween words in t and words in s, or equivalently,
between nodes in ?
t
and nodes in ?
s
. In principle,
any portion of ?
t
may align to any portion of ?
s
,
but in practice we often make restrictions on the
alignments to simplify computation. Smith and
Eisner, for example, restricted |a(j)| for all words
221
tj
to be at most one, so that each target word
aligned to at most one source word, which we also
do here.
6
Which translations are possible depends heav-
ily on the configurations that the QDG permits.
Formally, for a parent-child pair ?t
?
t
(j)
, t
j
? in ?
t
,
we consider the relationship between a(?
t
(j)) and
a(j), the source-side words to which t
?
t
(j)
and
t
j
align. If, for example, we require that, for
all j, a(?
t
(j)) = ?
s
(a(j)) or a(j) = 0, and
that the root of ?
t
must align to the root of ?
s
or to NULL, then strict isomorphism must hold
between ?
s
and ?
t
, and we have implemented a
synchronous CF dependency grammar (Alshawi
et al, 2000; Ding and Palmer, 2005). Smith and
Eisner (2006) grouped all possible configurations
into eight classes and explored the effects of per-
mitting different sets of classes in word align-
ment. (?a(?
t
(j)) = ?
s
(a(j))? corresponds to
their ?parent-child? configuration; see Fig. 3 in
Smith and Eisner (2006) for illustrations of the
rest.) More generally, we can define features on
tree pairs that factor into these local configura-
tions, as shown in Eq. 7 (Tab. 2).
Note that the QDG instantiates the model in
Eq. 2. Of the features discussed in ?2, f
lex
, f
att
,
f
val
, and f
dist
can be easily incorporated into the
QDG as described while respecting the indepen-
dence assumptions implied by the configuration
features. The others (f
phr
, f
2
, and f
3
) are non-
local, or involve parts of the structure that, from
the QDG?s perspective, are conditionally indepen-
dent given intervening material. Note that ?non-
locality? is relative to a choice of formalism; in ?2
we did not commit to any formalism, so it is only
now that we can describe phrase and N -gram fea-
tures as non-local. Non-local features will present
a challenge for decoding and training (?4.3).
4 Decoding
Given a sentence s and its parse ?
s
, at decoding
time we seek the target sentence t
?
, the target tree
?
?
t
, and the alignments a
?
that are most probable,
as defined in Eq. 1.
7
(In ?5 we will consider k-
best and all-translations variations on this prob-
6
I.e., from here on, a : {1, . . . ,m} ? {0, . . . , n} where
0 denotes alignment to NULL.
7
Arguably, we seek argmax
t
p(t | s), marginalizing out
everything else. Approximate solutions have been proposed
for that problem in several settings (Blunsom and Osborne,
2008; Sun and Tsujii, 2009); we leave their combination with
our approach to future work.
lem.) As usual, the normalization constant is not
required for decoding; it suffices to solve:
?t
?
, ?
?
t
,a
?
? = argmax
?t,?
t
,a?
?
>
g(s, ?
s
,a, t, ?
t
) (8)
For a QDG model, the decoding problem has
not been addressed before. It equates to finding the
most probable derivation under the s/?
s
-specific
grammar G
s,?
s
. We solve this by lattice parsing,
assuming that an upper bound on m (the length
of t) is known. The advantage offered by this
approach (like most other grammar-based trans-
lation approaches) is that decoding becomes dy-
namic programming (DP), a technique that is both
widely understood in NLP and for which practical,
efficient, generic techniques exist. A major advan-
tage of DP is that, with small modifications, sum-
ming over structures is also possible with ?inside?
DP algorithms. We will exploit this in training
(?5). Efficient summing opens up many possibili-
ties for training ?, such as likelihood and pseudo-
likelihood, and provides principled ways to handle
hidden variables during learning.
4.1 Translation as Monolingual Parsing
We decode by performing lattice parsing on a lat-
tice encoding the set of possible translations. The
lattice is a weighted ?sausage? lattice that permits
sentences up to some maximum length `; ` is de-
rived from the source sentence length. Let the
states be numbered 0 to `; states from b?`c to `
are final states (for some ? ? (0, 1)). For every
position between consecutive states j ? 1 and j
(0 < j ? `), and for every word s
i
in s, and
for every word t ? Trans(s
i
), we instantiate an
arc annotated with t and i. The weight of such an
arc is exp{?
>
f}, where f is the sum of feature
functions that fire when s
i
translates as t in target
position j (e.g., f
lex
(s
i
, t) and f
dist
(i, j)).
Given the lattice and G
s,?
s
, lattice parsing
is a straightforward generalization of standard
context-free dependency parsing DP algorithms
(Eisner, 1997). This decoder accounts for f
lex
,
f
att
, f
val
, f
dist
, and f
qg
as local features.
Figure 1 gives an example, showing a German
sentence and dependency tree from an automatic
parser, an English reference, and a lattice repre-
senting possible translations. In each bundle, the
arcs are listed in decreasing order according to
weight and for clarity only the first five are shown.
The output of the decoder consists of lattice arcs
222
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can you deliver it by tomorrow morning ?
can     you     deliver  it    by     tomorrow morning ?
CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
...
es:it
morgen:tomorrow
liefern:deliver
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
konnten:could konnten:could
es:it
sie:you
konnten:might
...
konnten:couldn
... ... ... ...
sie:let
sie:you
sie:them
es:it sie:you
konnten:could?bersetzen:
translate
?bersetzen:
translate
?bersetzen:
translated
?bersetzen:
translate
?bersetzen:
translated
?:?
konnten:could
es:it
es:it
?:?
es:it
?:?
NULL:to
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
...
fr?h:morning
morgen:tomorrow
morgen:morning
liefern:deliver
vorbei:by
$
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
Source:          $  konnten  sie  es  ?bersetzen  ?
Reference:         could  you  translate  it  ?
Decoder output:
Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out)
and thicker blue arcs forming a dependency tree over them.
selected at each position and a dependency tree
over them.
4.2 Source-Side Coverage Features
Most MT decoders enforce a notion of ?coverage?
of the source sentence during translation: all parts
of s should be aligned to some part of t (alignment
to NULL incurs an explicit cost). Phrase-based sys-
tems such as Moses (Koehn et al, 2007) explic-
itly search for the highest-scoring string in which
all source words are translated. Systems based
on synchronous grammars proceed by parsing the
source sentence with the synchronous grammar,
ensuring that every phrase and word has an ana-
logue in ?
t
(or a deliberate choice is made by the
decoder to translate it to NULL). In such sys-
tems, we do not need to use features to implement
source-side coverage, as it is assumed as a hard
constraint always respected by the decoder.
Our QDG decoder has no way to enforce cov-
erage; it does not track any kind of state in ?
s
apart from a single recently aligned word. This
is a problem with other direct translation models,
such as IBM model 1 used as a direct model rather
than a channel model (Brown et al, 1993). This
sacrifice is the result of our choice to use a condi-
tional model (?2).
The solution is to introduce a set of coverage
features g
cov
(a). Here, these include:
? A counter for the number of times each source
word is covered: f
scov
(a) =
?
n
i=1
|a
?1
(i)|.
? Features that fire once when a source word is
covered the zth time (z ? {2, 3, 4}) and fire
again all subsequent times it is covered; these
are denoted f
2nd
, f
3rd
, and f
4th
.
? A counter of uncovered source words:
f
sunc
(a) =
?
n
i=1
?(|a
?1
(i)|, 0).
Of these, only f
scov
is local.
4.3 Non-Local Features
The lattice QDG parsing decoder incorporates
many of the features we have discussed, but not
all of them. Phrase lexicon features f
phr
, lan-
guage model features f
N
for N > 1, and most
coverage features are non-local with respect to our
QDG. Recently Chiang (2007) introduced ?cube
pruning? as an approximate decoding method that
extends a DP decoder with the ability to incorpo-
rate features that break the Markovian indepen-
dence assumptions DP exploits. Techniques like
cube pruning can be used to include the non-local
features in our decoder.
8
5 Training
Training requires us to learn values for the param-
eters ? in Eq. 2. Given T training examples of the
form ?t
(i)
, ?
(i)
t
, s
(i)
, ?
(i)
s
?, for i = 1, ..., T , max-
imum likelihood estimation for this model con-
sists of solving Eq. 9 (Tab. 3).
9
Note that the
8
A full discussion is omitted for space, but in fact we use
?cube decoding,? a slightly less approximate, slightly more
expensive method that is more closely related to the approxi-
mate inference methods we use for training, discussed in ?5.
9
In practice, we regularize by including a term ?c???
2
2
.
223
LL(?) =
T
X
i=1
log p(t
(i)
, ?
(i)
t
| s
(i)
, ?
(i)
s
) =
T
X
i=1
log
P
a
exp{?
>
g(s
(i)
, ?
(i)
s
,a, t
(i)
, ?
(i)
t
)}
P
t,?
t
,a
exp{?
>
g(s
(i)
, ?
(i)
s
,a, t, ?
t
)}
=
T
X
i=1
log
?numerator?
?denominator?
(9)
PL(?) =
T
X
i=1
log
?
X
a
p(t
(i)
,a | ?
(i)
t
, s
(i)
, ?
(i)
s
)
?
+
T
X
i=1
log
?
X
a
p(?
(i)
t
,a | t
(i)
, s
(i)
, ?
(i)
s
)
?
(10)
?denominator? of
term 1 in Eq. 10
=
n
X
i=0
X
t
?
?Trans(s
i
)
S(?
?1
t
(0), i, t
?
) ? exp
n
?
>
`
f
lex
(s
i
, t
?
) + f
att
($, 0, t
?
, k) + f
qg
(0, i, 0, k)
?
o
(11)
S(j, i, t) =
Y
k??
?1
t
(j)
n
X
i
?
=0
X
t
?
?Trans(s
i
?
)
S(k, i
?
, t
?
) ? exp
?
?
>
?
f
lex
(s
i
?
, t
?
) + f
att
(t, j, t
?
, k)+
f
val
(t, j, ?
?1
t
(j)) + f
qg
(i, i
?
, j, k)
?ff
(12)
S(j, i, t) = exp
n
?
>
`
f
val
(t, j, ?
?1
t
(j))
?
o
if ?
?1
t
(j) = ? (13)
Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. ?. Eqs. 11?13: Recursive DP
equations for summing over t and a.
alignments are treated as a hidden variable to be
marginalized out.
10
Optimization problems of this
form are by now widely known in NLP (Koo and
Collins, 2005), and have recently been used for
machine translation as well (Blunsom et al, 2008).
Such problems are typically solved using varia-
tions of gradient ascent; in our experiments, we
will use an online method called stochastic gra-
dient ascent (SGA). This requires us to calculate
the function?s gradient (vector of first derivatives)
with respect to ?.
11
Computing the numerator in Eq. 9 involves
summing over all possible alignments; with QDG
and a hard bound of 1 on |a(j)| for all j, a fast
?inside? DP solution is known (Smith and Eisner,
2006; Wang et al, 2007). It runs in O(mn
2
) time
and O(mn) space.
Computing the denominator in Eq. 9 requires
summing over all word sequences and depen-
dency trees for the target language sentence and
all word alignments between the sentences. With
a maximum length imposed, this is tractable us-
ing the ?inside? version of the maximizing DP al-
gorithm of Sec. 4, but it is prohibitively expen-
sive. We therefore optimize pseudo-likelihood in-
stead, making the following approximation (Be-
10
Alignments could be supplied by automatic word align-
ment algorithms. We chose to leave them hidden so that we
could make the best use of our parsed training data when con-
figuration constraints are imposed, since it is not always pos-
sible to reconcile automatic word alignments with automatic
parses.
11
When the function?s value is computed by ?inside? DP,
the corresponding ?outside? algorithm can be used to obtain
the gradient. Because outside algorithms can be automati-
cally derived from inside ones, we discuss only inside algo-
rithms in this paper; see Eisner et al (2005).
sag, 1975):
p(t, ?
t
| s, ?
s
) ? p(t | ?
t
, s, ?
s
) ? p(?
t
| t, s, ?
s
)
Plugging this into Eq. 9, we arrive at Eq. 10
(Tab. 3). The two parenthesized terms in Eq. 10
each have their own numerators and denomina-
tors (not shown). The numerators are identical to
each other and to that in Eq. 9. The denominators
are much more manageable than in Eq. 9, never
requiring summation over more than two struc-
tures at a time. We must sum over target word se-
quences and word alignments (with fixed ?
t
), and
separately over target trees and word alignments
(with fixed t).
5.1 Summing over t and a
The summation over target word sequences and
alignments given fixed ?
t
bears a resemblance to
the inside algorithm, except that the tree structure
is fixed (Pereira and Schabes, 1992). Let S(j, i, t)
denote the sum of all translations rooted at posi-
tion j in ?
t
such that a(j) = i and t
j
= t.
Tab. 3 gives the equations for this DP: Eq. 11
is the quantity of interest, Eq. 12 is the recursion,
and Eq. 13 shows the base cases for leaves of ?
t
.
Letting q = max
0?i?n
|Trans(s
i
)|, this algo-
rithm runs in O(mn
2
q
2
) time and O(mnq) space.
For efficiency we place a hard upper bound on q
during training (details in ?6).
5.2 Summing over ?
t
and a
For the summation over dependency trees and
alignments given fixed t, required for p(?
t
|
t, s, ?
s
), we perform ?inside? lattice parsing with
G
s,?
s
. The technique is the summing variant of
the decoding method in ?4, except for each state j,
224
the sausage lattice only includes arcs from j?1 to
j that are labeled with the known target word t
j
.
If a is the number of arcs in the lattice, which is
O(mn), this algorithm runs in O(a
3
) time and re-
quires O(a
2
) space. Because we use a hard upper
bound on |Trans(s)| for all s ? ?, this summation
is much faster in practice than the one over words
and alignments.
5.3 Handling Non-Local Features
So far, all of our algorithms have exploited DP,
disallowing any non-local features (e.g., f
phr
, f
N
for N > 1, f
zth
, f
sunc
). We recently proposed
?cube summing,? an approximate technique that
permits the use of non-local features for inside DP
algorithms (Gimpel and Smith, 2009). Cube sum-
ming is based on a slightly less greedy variation of
cube pruning (Chiang, 2007) that maintains k-best
lists of derivations for each DP chart item. Cube
summing augments the k-best list with a residual
term that sums over remaining structures not in
the k-best list, albeit without their non-local fea-
tures. Using the machinery of cube summing, it
is straightforward to include the desired non-local
features in the summations required for pseudo-
likelihood, as well as to compute their approxi-
mate gradients.
Our approach permits an alternative to mini-
mum error-rate training (MERT; Och, 2003); it is
discriminative but handles latent structure and reg-
ularization in more principled ways. The pseudo-
likelihood calculations for a sentence pair, taken
together, are faster than (k-best) decoding, making
SGA?s inner loop faster than MERT?s inner loop.
6 Experiments
Our decoding framework allows us to perform
many experiments with the same feature rep-
resentation and inference algorithms, includ-
ing combining and comparing phrase-based and
syntax-based features and examining how isomor-
phism constraints of synchronous formalisms af-
fect translation output.
6.1 Data and Evaluation
We use the German-English portion of the Ba-
sic Travel Expression Corpus (BTEC). The cor-
pus has approximately 100K sentence pairs. We
filter sentences of length more than 15 words,
which only removes 6% of the data. We end up
with a training set of 82,299 sentences, a develop-
ment set of 934 sentences, and a test set of 500
sentences. We evaluate translation output using
case-insensitive BLEU (Papineni et al, 2001), as
provided by NIST, and METEOR (Banerjee and
Lavie, 2005), version 0.6, with Porter stemming
and WordNet synonym matching.
6.2 Features
Our base system uses features as discussed
in ?2. To obtain lexical translation features
g
trans
(s,a, t), we use the Moses pipeline (Koehn
et al, 2007). We perform word alignment us-
ing GIZA++ (Och and Ney, 2003), symmetrize
the alignments using the ?grow-diag-final-and?
heuristic, and extract phrases up to length 3. We
define f
lex
by the lexical probabilities p(t | s) and
p(s | t) estimated from the symmetrized align-
ments. After discarding phrase pairs with only
one target-side word (since we only allow a tar-
get word to align to at most one source word), we
define f
phr
by 8 features: {2, 3} target words ?
phrase conditional and ?lexical smoothing? prob-
abilities ? two conditional directions.
Bigram and trigam language model features, f
2
and f
3
, are estimated using the SRI toolkit (Stol-
cke, 2002) with modified Kneser-Ney smoothing
(Chen and Goodman, 1998).
For our target-language syntactic features g
syn
,
we use features similar to lexicalized CFG events
(Collins, 1999), specifically following the de-
pendency model of Klein and Manning (2004).
These include probabilities associated with in-
dividual attachments (f
att
) and child-generation
valence probabilities (f
val
). These probabilities
are estimated on the training corpus parsed using
the Stanford factored parser (Klein and Manning,
2003). The same probabilities are also included
using 50 hard word classes derived from the paral-
lel corpus using the GIZA++ mkcls utility (Och
and Ney, 2003). In total, there are 7 lexical and 7
word-class syntax features.
For reordering, we use a single absolute distor-
tion feature f
dist
(i, j) that returns |i?j|whenever
a(j) = i and i, j > 0. (Unlike the other feature
functions, which returned probabilities, this fea-
ture function returns a nonnegative integer.)
The tree-to-tree syntactic features g
tree
2
in our
model are binary features f
qg
that fire for particu-
lar QG configurations. We use one feature for each
of the configurations in (Smith and Eisner, 2006),
adding 7 additional features that score configura-
225
Phrase Syntactic Features:
features: +f
att
? f
val
+f
qg
(base) (target) (tree-to-tree)
(base) 0.3727 0.4458 0.4424
+f
phr
0.4682 0.4971 0.5142
Table 4: Feature set comparison (BLEU).
tions involving root words and NULL-alignments
more finely. There are 14 features in this category.
Coverage features g
cov
are as described in ?4.2.
In all, 46 feature weights are learned.
6.3 Experimental Procedure
Our model permits training the system on the full
set of parallel data, but we instead use the parallel
data to estimate feature functions and learn ? on
the development set.
12
We trained using three it-
erations of SGA over the development data with a
batch size of 1 and a fixed step size of 0.01. We
used `
2
regularization with a fixed, untuned coef-
ficient of 0.1. Cube summing used a 10-best list
for training and a 7-best list for decoding unless
otherwise specified.
To obtain the translation lexicon (Trans) we
first included the top three target words t for each
s using p(s | t) ? p(t | s) to score target words.
For any training sentence ?s, t? and t
j
for which
t
j
6?
?
n
i=1
Trans(s
i
), we added t
j
to Trans(s
i
)
for i = argmax
i
?
?I
p(s
i
?
|t
j
) ? p(t
j
|s
i
?
), where
I = {i : 0 ? i ? n ? |Trans(s
i
)| < q
i
}.
We used q
0
= 10 and q
>0
= 5, restricting
|Trans(NULL)| ? 10 and |Trans(s)| ? 5 for any
s ? ?. This made 191 of the development sen-
tences unreachable by the model, leaving 743 sen-
tences for learning ?.
During decoding, we generated lattices with all
t ? Trans(s
i
) for 0 ? i ? n, for every position.
We used ? = 0.9, causing states within 90% of the
source sentence length to be final states. Between
each pair of consecutive states, we pruned edges
that fell outside a beam of 70% of the sum of edge
weights (see ?4.1; edge weights use f
lex
, f
dist
,
and f
scov
) of all edges between those two states.
6.4 Feature Set Comparison
Our first set of experiments compares feature sets
commonly used in phrase- and syntax-based trans-
lation. In particular, we compare the effects of
combining phrase features and syntactic features.
The base model contains f
lex
, g
lm
, g
reor
, and
12
We made this choice both for similarity to standard MT
systems and a more rapid experiment cycle.
g
cov
. The results are shown in Table 4. The sec-
ond row contains scores when adding in the eight
f
phr
features. The second column shows scores
when adding the 14 target syntax features (f
att
and f
val
), and the third column adds to them the
14 additional tree-to-tree features (f
qg
). We find
large gains in BLEU by adding more features, and
find that gains obtained through phrase features
and syntactic features are partially additive, sug-
gesting that these feature sets are making comple-
mentary contributions to translation quality.
6.5 Varying k During Decoding
For models without syntactic features, we con-
strained the decoder to produce dependency trees
in which every word?s parent is immediately to its
right and ignored syntactic features while scoring
structures. This causes decoding to proceed left-
to-right in the lattice, the way phrase-based de-
coders operate. Since these models do not search
over trees, they are substantially faster during de-
coding than those that use syntactic features and
do not require any pruning of the lattice. There-
fore, we explored varying the value of k used dur-
ing k-best cube decoding; results are shown in
Fig. 2. Scores improve when we increase k up
to 10, but not much beyond, and there is still a
substantial gap (2.5 BLEU) between using phrase
features with k = 20 and using all features with
k = 5. Models without syntax perform poorly
when using a very small k, due to their reliance on
non-local language model and phrase features. By
contrast, models with syntactic features, which are
local in our decoder, perform relatively well even
with k = 1.
6.6 QG Configuration Comparison
We next compare different constraints on isomor-
phism between the source and target dependency
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0 5 10 15 20
Value of k  for Decoding
B
L
E
U
Phrase + Syntactic
Phrase
Syntactic
Neither
Figure 2: Comparison of size of k-best list for cube decoding
with various feature sets.
226
QDG Configurations BLEU METEOR
synchronous 0.4008 0.6949
+ nulls, root-any 0.4108 0.6931
+ child-parent, same node 0.4337 0.6815
+ sibling 0.4881 0.7216
+ grandparent/child 0.5015 0.7365
+ c-command 0.5156 0.7441
+ other 0.5142 0.7472
Table 5: QG configuration comparison. The name of each
configuration, following Smith and Eisner (2006), refers to
the relationship between a(?
t
(j)) and a(j) in ?
s
.
trees. To do this, we impose harsh penalties on
some QDG configurations (?3) by fixing their fea-
ture weights to ?1000. Hence they are permit-
ted only when absolutely necessary in training
and rarely in decoding.
13
Each model uses all
phrase and syntactic features; they differ only in
the sets of configurations which have fixed nega-
tive weights.
Tab. 5 shows experimental results. The
base ?synchronous? model permits parent-child
(a(?
t
(j)) = ?
s
(a(j))), any configuration where
a(j) = 0, including both words being linked to
NULL, and requires the root word in ?
t
to be linked
to the root word in ?
s
or to NULL(5 of our 14
configurations). The second row allows any con-
figuration involving NULL, including those where
t
j
aligns to a non-NULL word in s and its par-
ent aligns to NULL, and allows the root in ?
t
to
be linked to any word in ?
s
. Each subsequent
row adds additional configurations (i.e., trains its
? rather than fixing it to ?1000). In general, we
see large improvements as we permit more con-
figurations, and the largest jump occurs when we
add the ?sibling? configuration (?
s
(a(?
t
(j))) =
?
s
(a(j))). The BLEU score does not increase,
however, when we permit all configurations in the
final row of the table, and the METEOR score in-
creases only slightly. While allowing certain cate-
gories of non-isomorphism clearly seems helpful,
permitting arbitrary violations does not appear to
be necessary for this dataset.
6.7 Discussion
We note that these results are not state-of-the-
art on this dataset (on this task, Moses/MERT
achieves 0.6838 BLEU and 0.8523METEORwith
maximum phrase length 3).
14
Our aim has been to
13
In fact, the strictest ?synchronous? model used the
almost-forbidden configurations in 2% of test sentences; this
behavior disappears as configurations are legalized.
14
We believe one cause for this performance gap is the gen-
eration of the lattice and plan to address this in future work
by allowing the phrase table to inform lattice generation.
illustrate how a single model can provide a con-
trolled experimental framework for comparisons
of features, of inference methods, and of con-
straints. Our findings show that phrase features
and dependency syntax produce complementary
improvements to translation quality, that tree-to-
tree configurations (a new feature in MT) are help-
ful for translation, and that substantial gains can
be obtained by permitting certain types of non-
isomorphism. We have validated cube summing
and decoding as practical methods for approxi-
mate inference.
Our framework permits exploration of alter-
native objectives, alternative approximate infer-
ence techniques, additional hidden variables (e.g.,
Moses? phrase segmentation variable), and, of
course, additional feature representations. The
system is publicly available at www.ark.cs.
cmu.edu/Quipu.
7 Conclusion
We presented feature-rich MT using a princi-
pled probabilistic framework that separates fea-
tures from inference. Our novel decoder is based
on efficient DP-based QG lattice parsing extended
to handle ?non-local? features using generic tech-
niques that also support efficient parameter esti-
mation. Controlled experiments permitted with
this system show interesting trends in the use of
syntactic features and constraints.
Acknowledgments
We thank three anonymous EMNLP reviewers,
David Smith, and Stephan Vogel for helpful com-
ments and feedback that improved this paper. This
research was supported by NSF IIS-0836431 and
IIS-0844507, a grant from Google, and computa-
tional resources provided by Yahoo.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000.
Learning dependency translation modles as colec-
tions of finite-state head transducers. Computa-
tional Linguistics, 26(1):45?60.
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for MT evaluation with improved
correlation with human judgments. In Proc. of
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization.
J. E. Besag. 1975. Statistical analysis of non-lattice
data. The Statistician, 24:179?195.
227
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical report 10-98, Harvard University.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proc. of EMNLP.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, U. Penn.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition.
In Proc. of ACL-IJCNLP.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammar. In Proc. of ACL.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
piling Comp Ling: Practical weighted dynamic pro-
gramming and the Dyna language. In Proc. of HLT-
EMNLP.
J. Eisner. 1997. Bilexical grammars and a cubic-time
probabilistic parser. In Proc. of IWPT.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic transla-
tion models. In Proc. of COLING-ACL.
K. Gimpel and N. A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proc. of ACL-2008 Workshop on Statistical Machine
Translation.
K. Gimpel and N. A. Smith. 2009. Cube summing,
approximate inference with non-local features, and
dynamic programming without semirings. In Proc.
of EACL.
R. Haque, S. K. Naskar, Y. Ma, and A. Way. 2009.
Using supertags as source language context in SMT.
In Proc. of EAMT.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In Proc. of HLT-NAACL.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in NIPS 15.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proc. of ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
(demo session).
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. of EMNLP.
P. Liang, A. Bouchard-C?ot?e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.
A. Lopez. 2009. Translation as weighted deduction.
In Proc. of EACL.
D. Marcu, W.Wang, A. Echihabi, and K. Knight. 2006.
Statistical machine translation with syntactified tar-
get language phrases. In Proc. of EMNLP.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, and T. Ward. 1997. Feature-
based language understanding. In EUROSPEECH.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of ACL.
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside
reestimation from partially bracketed corpora. In
Proc. of ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
of ACL.
D. A. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proc. of HLT-NAACLWorkshop on
Statistical Machine Translation.
D. A. Smith and J. Eisner. 2009. Parser adaptation
and projection with quasi-synchronous features. In
Proc. of EMNLP.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Proc. of ICSLP.
X. Sun and J. Tsujii. 2009. Sequential labeling with
latent variables: An exact inference algorithm and
its efficient approximation. In Proc. of EACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for QA. In Proc. of EMNLP-CoNLL.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of ACL.
228
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 318?326,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Cube Summing, Approximate Inference with Non-Local Features,
and Dynamic Programming without Semirings
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We introduce cube summing, a technique
that permits dynamic programming algo-
rithms for summing over structures (like
the forward and inside algorithms) to be
extended with non-local features that vio-
late the classical structural independence
assumptions. It is inspired by cube prun-
ing (Chiang, 2007; Huang and Chiang,
2007) in its computation of non-local
features dynamically using scored k-best
lists, but also maintains additional resid-
ual quantities used in calculating approx-
imate marginals. When restricted to lo-
cal features, cube summing reduces to a
novel semiring (k-best+residual) that gen-
eralizes many of the semirings of Good-
man (1999). When non-local features are
included, cube summing does not reduce
to any semiring, but is compatible with
generic techniques for solving dynamic
programming equations.
1 Introduction
Probabilistic NLP researchers frequently make in-
dependence assumptions to keep inference algo-
rithms tractable. Doing so limits the features that
are available to our models, requiring features
to be structurally local. Yet many problems in
NLP?machine translation, parsing, named-entity
recognition, and others?have benefited from the
addition of non-local features that break classical
independence assumptions. Doing so has required
algorithms for approximate inference.
Recently cube pruning (Chiang, 2007; Huang
and Chiang, 2007) was proposed as a way to lever-
age existing dynamic programming algorithms
that find optimal-scoring derivations or structures
when only local features are involved. Cube prun-
ing permits approximate decoding with non-local
features, but leaves open the question of how the
feature weights or probabilities are learned. Mean-
while, some learning algorithms, like maximum
likelihood for conditional log-linear models (Laf-
ferty et al, 2001), unsupervised models (Pereira
and Schabes, 1992), and models with hidden vari-
ables (Koo and Collins, 2005; Wang et al, 2007;
Blunsom et al, 2008), require summing over the
scores of many structures to calculate marginals.
We first review the semiring-weighted logic
programming view of dynamic programming al-
gorithms (Shieber et al, 1995) and identify an in-
tuitive property of a program called proof locality
that follows from feature locality in the underlying
probability model (?2). We then provide an analy-
sis of cube pruning as an approximation to the in-
tractable problem of exact optimization over struc-
tures with non-local features and show how the
use of non-local features with k-best lists breaks
certain semiring properties (?3). The primary
contribution of this paper is a novel technique?
cube summing?for approximate summing over
discrete structures with non-local features, which
we relate to cube pruning (?4). We discuss imple-
mentation (?5) and show that cube summing be-
comes exact and expressible as a semiring when
restricted to local features; this semiring general-
izes many commonly-used semirings in dynamic
programming (?6).
2 Background
In this section, we discuss dynamic programming
algorithms as semiring-weighted logic programs.
We then review the definition of semirings and im-
portant examples. We discuss the relationship be-
tween locally-factored structure scores and proofs
in logic programs.
2.1 Dynamic Programming
Many algorithms in NLP involve dynamic pro-
gramming (e.g., the Viterbi, forward-backward,
318
probabilistic Earley?s, and minimum edit distance
algorithms). Dynamic programming (DP) in-
volves solving certain kinds of recursive equations
with shared substructure and a topological order-
ing of the variables.
Shieber et al (1995) showed a connection
between DP (specifically, as used in parsing)
and logic programming, and Goodman (1999)
augmented such logic programs with semiring
weights, giving an algebraic explanation for the
intuitive connections among classes of algorithms
with the same logical structure. For example, in
Goodman?s framework, the forward algorithm and
the Viterbi algorithm are comprised of the same
logic program with different semirings. Goodman
defined other semirings, including ones we will
use here. This formal framework was the basis
for the Dyna programming language, which per-
mits a declarative specification of the logic pro-
gram and compiles it into an efficient, agenda-
based, bottom-up procedure (Eisner et al, 2005).
For our purposes, a DP consists of a set of recur-
sive equations over a set of indexed variables. For
example, the probabilistic CKY algorithm (run on
sentence w1w2...wn) is written as
CX,i?1,i = pX?wi (1)
CX,i,k = max
Y,Z?N;j?{i+1,...,k?1}
pX?Y Z ? CY,i,j ? CZ,j,k
goal = CS,0,n
where N is the nonterminal set and S ? N is the
start symbol. Each CX,i,j variable corresponds to
the chart value (probability of the most likely sub-
tree) of an X-constituent spanning the substring
wi+1...wj . goal is a special variable of greatest in-
terest, though solving for goal correctly may (in
general, but not in this example) require solving
for all the other values. We will use the term ?in-
dex? to refer to the subscript values on variables
(X, i, j on CX,i,j).
Where convenient, we will make use of Shieber
et al?s logic programming view of dynamic pro-
gramming. In this view, each variable (e.g., CX,i,j
in Eq. 1) corresponds to the value of a ?theo-
rem,? the constants in the equations (e.g., pX?Y Z
in Eq. 1) correspond to the values of ?axioms,?
and the DP defines quantities corresponding to
weighted ?proofs? of the goal theorem (e.g., find-
ing the maximum-valued proof, or aggregating
proof values). The value of a proof is a combi-
nation of the values of the axioms it starts with.
Semirings define these values and define two op-
erators over them, called ?aggregation? (max in
Eq. 1) and ?combination? (? in Eq. 1).
Goodman and Eisner et al assumed that the val-
ues of the variables are in a semiring, and that the
equations are defined solely in terms of the two
semiring operations. We will often refer to the
?probability? of a proof, by which we mean a non-
negative R-valued score defined by the semantics
of the dynamic program variables; it may not be a
normalized probability.
2.2 Semirings
A semiring is a tuple ?A,?,?,0,1?, in which A
is a set, ? : A ? A ? A is the aggregation
operation, ? : A ? A ? A is the combina-
tion operation, 0 is the additive identity element
(?a ? A, a ? 0 = a), and 1 is the multiplica-
tive identity element (?a ? A, a ? 1 = a). A
semiring requires ? to be associative and com-
mutative, and ? to be associative and to distribute
over?. Finally, we require a?0 = 0?a = 0 for
all a ? A.1 Examples include the inside semir-
ing, ?R?0,+,?, 0, 1?, and the Viterbi semiring,
?R?0,max,?, 0, 1?. The former sums the prob-
abilities of all proofs of each theorem. The lat-
ter (used in Eq. 1) calculates the probability of the
most probable proof of each theorem. Two more
examples follow.
Viterbi proof semiring. We typically need to
recover the steps in the most probable proof in
addition to its probability. This is often done us-
ing backpointers, but can also be accomplished by
representing the most probable proof for each the-
orem in its entirety as part of the semiring value
(Goodman, 1999). For generality, we define a
proof as a string that is constructed from strings
associated with axioms, but the particular form
of a proof is problem-dependent. The ?Viterbi
proof? semiring includes the probability of the
most probable proof and the proof itself. Letting
L ? ?? be the proof language on some symbol
set ?, this semiring is defined on the set R?0 ? L
with 0 element ?0, ? and 1 element ?1, ?. For
two values ?u1, U1? and ?u2, U2?, the aggregation
operator returns ?max(u1, u2), Uargmaxi?{1,2} ui?.
1When cycles are permitted, i.e., where the value of one
variable depends on itself, infinite sums can be involved. We
must ensure that these infinite sums are well defined under
the semiring. So-called complete semirings satisfy additional
conditions to handle infinite sums, but for simplicity we will
restrict our attention to DPs that do not involve cycles.
319
Semiring A Aggregation (?) Combination (?) 0 1
inside R?0 u1 + u2 u1u2 0 1
Viterbi R?0 max(u1, u2) u1u2 0 1
Viterbi proof R?0 ? L ?max(u1, u2), Uargmaxi?{1,2} ui? ?u1u2, U1.U2? ?0, ? ?1, ?
k-best proof (R?0 ? L)?k max-k(u1 ? u2) max-k(u1 ? u2) ? {?1, ?}
Table 1: Commonly used semirings. An element in the Viterbi proof semiring is denoted ?u1, U1?, where u1 is the probability
of proof U1. The max-k function returns a sorted list of the top-k proofs from a set. The ? function performs a cross-product
on two k-best proof lists (Eq. 2).
The combination operator returns ?u1u2, U1.U2?,
where U1.U2 denotes the string concatenation of
U1 and U2.2
k-best proof semiring. The ?k-best proof?
semiring computes the values and proof strings of
the k most-probable proofs for each theorem. The
set is (R?0 ? L)?k, i.e., sequences (up to length
k) of sorted probability/proof pairs. The aggrega-
tion operator ? uses max-k, which chooses the k
highest-scoring proofs from its argument (a set of
scored proofs) and sorts them in decreasing order.
To define the combination operator ?, we require
a cross-product that pairs probabilities and proofs
from two k-best lists. We call this ?, defined on
two semiring values u = ??u1, U1?, ..., ?uk, Uk??
and v = ??v1, V1?, ..., ?vk, Vk?? by:
u ? v = {?uivj , Ui.Vj? | i, j ? {1, ..., k}} (2)
Then, u ? v = max-k(u ? v). This is similar to
the k-best semiring defined by Goodman (1999).
These semirings are summarized in Table 1.
2.3 Features and Inference
Let X be the space of inputs to our logic program,
i.e., x ? X is a set of axioms. Let L denote the
proof language and let Y ? L denote the set of
proof strings that constitute full proofs, i.e., proofs
of the special goal theorem. We assume an expo-
nential probabilistic model such that
p(y | x) ?
?M
m=1 ?
hm(x,y)
m (3)
where each ?m ? 0 is a parameter of the model
and each hm is a feature function. There is a bijec-
tion between Y and the space of discrete structures
that our model predicts.
Given such a model, DP is helpful for solving
two kinds of inference problems. The first prob-
lem, decoding, is to find the highest scoring proof
2We assume for simplicity that the best proof will never
be a tie among more than one proof. Goodman (1999) han-
dles this situation more carefully, though our version is more
likely to be used in practice for both the Viterbi proof and
k-best proof semirings.
y? ? Y for a given input x ? X:
y?(x) = argmaxy?Y
?M
m=1 ?m
hm(x,y) (4)
The second is the summing problem, which
marginalizes the proof probabilities (without nor-
malization):
s(x) =
?
y?Y
?M
m=1 ?m
hm(x,y) (5)
As defined, the feature functions hm can depend
on arbitrary parts of the input axiom set x and the
entire output proof y.
2.4 Proof and Feature Locality
An important characteristic of problems suited for
DP is that the global calculation (i.e., the value of
goal ) depend only on local factored parts. In DP
equations, this means that each equation connects
a relatively small number of indexed variables re-
lated through a relatively small number of indices.
In the logic programming formulation, it means
that each step of the proof depends only on the the-
orems being used at that step, not the full proofs
of those theorems. We call this property proof lo-
cality. In the statistical modeling view of Eq. 3,
classical DP requires that the probability model
make strong Markovian conditional independence
assumptions (e.g., in HMMs, St?1 ? St+1 | St);
in exponential families over discrete structures,
this corresponds to feature locality.
For a particular proof y of goal consisting of
t intermediate theorems, we define a set of proof
strings `i ? L for i ? {1, ..., t}, where `i corre-
sponds to the proof of the ith theorem.3 We can
break the computation of feature function hm into
a summation over terms corresponding to each `i:
hm(x, y) =
?t
i=1 fm(x, `i) (6)
This is simply a way of noting that feature func-
tions ?fire? incrementally at specific points in the
3The theorem indexing scheme might be based on a topo-
logical ordering given by the proof structure, but is not im-
portant for our purposes.
320
proof, normally at the first opportunity. Any fea-
ture function can be expressed this way. For local
features, we can go farther; we define a function
top(`) that returns the proof string corresponding
to the antecedents and consequent of the last infer-
ence step in `. Local features have the property:
hlocm (x, y) =
?t
i=1 fm(x, top(`i)) (7)
Local features only have access to the most re-
cent deductive proof step (though they may ?fire?
repeatedly in the proof), while non-local features
have access to the entire proof up to a given the-
orem. For both kinds of features, the ?f? terms
are used within the DP formulation. When tak-
ing an inference step to prove theorem i, the value
?M
m=1 ?
fm(x,`i)
m is combined into the calculation
of that theorem?s value, along with the values of
the antecedents. Note that typically only a small
number of fm are nonzero for theorem i.
When non-local hm/fm that depend on arbitrary
parts of the proof are involved, the decoding and
summing inference problems are NP-hard (they
instantiate probabilistic inference in a fully con-
nected graphical model). Sometimes, it is possible
to achieve proof locality by adding more indices to
the DP variables (for example, consider modify-
ing the bigram HMMViterbi algorithm for trigram
HMMs). This increases the number of variables
and hence computational cost. In general, it leads
to exponential-time inference in the worst case.
There have been many algorithms proposed for
approximately solving instances of these decod-
ing and summing problems with non-local fea-
tures. Some stem from work on graphical mod-
els, including loopy belief propagation (Sutton and
McCallum, 2004; Smith and Eisner, 2008), Gibbs
sampling (Finkel et al, 2005), sequential Monte
Carlo methods such as particle filtering (Levy et
al., 2008), and variational inference (Jordan et al,
1999; MacKay, 1997; Kurihara and Sato, 2006).
Also relevant are stacked learning (Cohen and
Carvalho, 2005), interpretable as approximation
of non-local feature values (Martins et al, 2008),
and M-estimation (Smith et al, 2007), which al-
lows training without inference. Several other ap-
proaches used frequently in NLP are approximate
methods for decoding only. These include beam
search (Lowerre, 1976), cube pruning, which we
discuss in ?3, integer linear programming (Roth
and Yih, 2004), in which arbitrary features can act
as constraints on y, and approximate solutions like
McDonald and Pereira (2006), in which an exact
solution to a related decoding problem is found
and then modified to fit the problem of interest.
3 Approximate Decoding
Cube pruning (Chiang, 2007; Huang and Chi-
ang, 2007) is an approximate technique for decod-
ing (Eq. 4); it is used widely in machine transla-
tion. Given proof locality, it is essentially an effi-
cient implementation of the k-best proof semiring.
Cube pruning goes farther in that it permits non-
local features to weigh in on the proof probabili-
ties, at the expense of making the k-best operation
approximate. We describe the two approximations
cube pruning makes, then propose cube decoding,
which removes the second approximation. Cube
decoding cannot be represented as a semiring; we
propose a more general algebraic structure that ac-
commodates it.
3.1 Approximations in Cube Pruning
Cube pruning is an approximate solution to the de-
coding problem (Eq. 4) in two ways.
Approximation 1: k < ?. Cube pruning uses
a finite k for the k-best lists stored in each value.
If k = ?, the algorithm performs exact decoding
with non-local features (at obviously formidable
expense in combinatorial problems).
Approximation 2: lazy computation. Cube
pruning exploits the fact that k < ? to use lazy
computation. When combining the k-best proof
lists of d theorems? values, cube pruning does not
enumerate all kd proofs, apply non-local features
to all of them, and then return the top k. Instead,
cube pruning uses a more efficient but approxi-
mate solution that only calculates the non-local
factors on O(k) proofs to obtain the approximate
top k. This trick is only approximate if non-local
features are involved.
Approximation 2 makes it impossible to formu-
late cube pruning using separate aggregation and
combination operations, as the use of lazy com-
putation causes these two operations to effectively
be performed simultaneously. To more directly
relate our summing algorithm (?4) to cube prun-
ing, we suggest a modified version of cube prun-
ing that does not use lazy computation. We call
this algorithm cube decoding. This algorithm can
be written down in terms of separate aggregation
321
and combination operations, though we will show
it is not a semiring.
3.2 Cube Decoding
We formally describe cube decoding, show that
it does not instantiate a semiring, then describe
a more general algebraic structure that it does in-
stantiate.
Consider the set G of non-local feature functions
that map X ? L ? R?0.4 Our definitions in ?2.2
for the k-best proof semiring can be expanded to
accommodate these functions within the semiring
value. Recall that values in the k-best proof semir-
ing fall inAk = (R?0?L)?k. For cube decoding,
we use a different set Acd defined as
Acd = (R?0 ? L)?k
? ?? ?
Ak
?G? {0, 1}
where the binary variable indicates whether the
value contains a k-best list (0, which we call an
?ordinary? value) or a non-local feature function
in G (1, which we call a ?function? value). We
denote a value u ? Acd by
u = ???u1, U1?, ?u2, U2?, ..., ?uk, Uk??
? ?? ?
u?
, gu, us?
where each ui ? R?0 is a probability and each
Ui ? L is a proof string.
We use ?k and ?k to denote the k-best proof
semiring?s operators, defined in ?2.2. We let g0 be
such that g0(`) is undefined for all ` ? L. For two
values u = ?u?, gu, us?,v = ?v?, gv, vs? ? Acd,
cube decoding?s aggregation operator is:
u?cd v = ?u??k v?, g0, 0? if ?us ? ?vs (8)
Under standard models, only ordinary values will
be operands of?cd, so?cd is undefined when us?
vs. We define the combination operator ?cd:
u?cd v = (9)?
?????
?????
?u??k v?, g0, 0? if ?us ? ?vs,
?max-k(exec(gv, u?)), g0, 0? if ?us ? vs,
?max-k(exec(gu, v?)), g0, 0? if us ? ?vs,
???, ?z.(gu(z)? gv(z)), 1? if us ? vs.
where exec(g, u?) executes the function g upon
each proof in the proof list u?, modifies the scores
4In our setting, gm(x, `) will most commonly be defined
as ?fm(x,`)m in the notation of ?2.3. But functions in G could
also be used to implement, e.g., hard constraints or other non-
local score factors.
in place by multiplying in the function result, and
returns the modified proof list:
g? = ?`.g(x, `)
exec(g, u?) = ??u1g?(U1), U1?, ?u2g?(U2), U2?,
..., ?ukg
?(Uk), Uk??
Here, max-k is simply used to re-sort the k-best
proof list following function evaluation.
The semiring properties fail to hold when in-
troducing non-local features in this way. In par-
ticular, ?cd is not associative when 1 < k < ?.
For example, consider the probabilistic CKY algo-
rithm as above, but using the cube decoding semir-
ing with the non-local feature functions collec-
tively known as ?NGramTree? features (Huang,
2008) that score the string of terminals and nonter-
minals along the path from word j to word j + 1
when two constituents CY,i,j and CZ,j,k are com-
bined. The semiring value associated with such
a feature is u = ???,NGramTreepi(), 1? (for a
specific path pi), and we rewrite Eq. 1 as fol-
lows (where ranges for summation are omitted for
space):
CX,i,k =
?
cd pX?Y Z ?cdCY,i,j ?cdCZ,j,k?cdu
The combination operator is not associative
since the following will give different answers:5
(pX?Y Z ?cd CY,i,j)?cd (CZ,j,k ?cd u) (10)
((pX?Y Z ?cd CY,i,j)?cd CZ,j,k)?cd u (11)
In Eq. 10, the non-local feature function is ex-
ecuted on the k-best proof list for Z, while in
Eq. 11, NGramTreepi is called on the k-best proof
list for the X constructed from Y and Z. Further-
more, neither of the above gives the desired re-
sult, since we actually wish to expand the full set
of k2 proofs of X and then apply NGramTreepi
to each of them (or a higher-dimensional ?cube?
if more operands are present) before selecting the
k-best. The binary operations above retain only
the top k proofs of X in Eq. 11 before applying
NGramTreepi to each of them. We actually would
like to redefine combination so that it can operate
on arbitrarily-sized sets of values.
We can understand cube decoding through an
algebraic structure with two operations ? and ?,
where ? need not be associative and need not dis-
tribute over?, and furthermore where? and? are
5Distributivity of combination over aggregation fails for
related reasons. We omit a full discussion due to space.
322
defined on arbitrarily many operands. We will re-
fer here to such a structure as a generalized semir-
ing.6 To define ?cd on a set of operands with N ?
ordinary operands and N function operands, we
first compute the full O(kN
?
) cross-product of the
ordinary operands, then apply each of the N func-
tions from the remaining operands in turn upon the
full N ?-dimensional ?cube,? finally calling max-k
on the result.
4 Cube Summing
We present an approximate solution to the sum-
ming problem when non-local features are in-
volved, which we call cube summing. It is an ex-
tension of cube decoding, and so we will describe
it as a generalized semiring. The key addition is to
maintain in each value, in addition to the k-best list
of proofs from Ak, a scalar corresponding to the
residual probability (possibly unnormalized) of all
proofs not among the k-best.7 The k-best proofs
are still used for dynamically computing non-local
features but the aggregation and combination op-
erations are redefined to update the residual as ap-
propriate.
We define the set Acs for cube summing as
Acs = R?0 ? (R?0 ? L)?k ? G? {0, 1}
A value u ? Acs is defined as
u = ?u0, ??u1, U1?, ?u2, U2?, ..., ?uk, Uk??
? ?? ?
u?
, gu, us?
For a proof list u?, we use ?u?? to denote the sum
of all proof scores,
?
i:?ui,Ui??u? ui.
The aggregation operator over operands
{ui}Ni=1, all such that uis = 0,
8 is defined by:
?N
i=1 ui = (12)??N
i=1 ui0 +
?
?
?Res
(?N
i=1 u?i
)?
?
? ,
max-k
(?N
i=1 u?i
)
, g0, 0
?
6Algebraic structures are typically defined with binary op-
erators only, so we were unable to find a suitable term for this
structure in the literature.
7Blunsom and Osborne (2008) described a related ap-
proach to approximate summing using the chart computed
during cube pruning, but did not keep track of the residual
terms as we do here.
8We assume that operands ui to ?cs will never be such
that uis = 1 (non-local feature functions). This is reasonable
in the widely used log-linear model setting we have adopted,
where weights ?m are factors in a proof?s product score.
where Res returns the ?residual? set of scored
proofs not in the k-best among its arguments, pos-
sibly the empty set.
For a set ofN+N ? operands {vi}Ni=1?{wj}
N ?
j=1
such that vis = 1 (non-local feature functions) and
wjs = 1 (ordinary values), the combination oper-
ator ? is shown in Eq. 13 Fig. 1. Note that the
case where N ? = 0 is not needed in this applica-
tion; an ordinary value will always be included in
combination.
In the special case of two ordinary operands
(where us = vs = 0), Eq. 13 reduces to
u? v = (14)
?u0v0 + u0 ?v??+ v0 ?u??+ ?Res(u? ? v?)? ,
max-k(u? ? v?), g0, 0?
We define 0 as ?0, ??, g0, 0?; an appropriate def-
inition for the combination identity element is less
straightforward and of little practical importance;
we leave it to future work.
If we use this generalized semiring to solve a
DP and achieve goal value of u, the approximate
sum of all proof probabilities is given by u0+?u??.
If all features are local, the approach is exact. With
non-local features, the k-best list may not contain
the k-best proofs, and the residual score, while in-
cluding all possible proofs, may not include all of
the non-local features in all of those proofs? prob-
abilities.
5 Implementation
We have so far viewed dynamic programming
algorithms in terms of their declarative speci-
fications as semiring-weighted logic programs.
Solvers have been proposed by Goodman (1999),
by Klein and Manning (2001) using a hypergraph
representation, and by Eisner et al (2005). Be-
cause Goodman?s and Eisner et al?s algorithms as-
sume semirings, adapting them for cube summing
is non-trivial.9
To generalize Goodman?s algorithm, we sug-
gest using the directed-graph data structure known
variously as an arithmetic circuit or computation
graph.10 Arithmetic circuits have recently drawn
interest in the graphical model community as a
9The bottom-up agenda algorithm in Eisner et al (2005)
might possibly be generalized so that associativity, distribu-
tivity, and binary operators are not required (John Blatz, p.c.).
10This data structure is not specific to any particular set of
operations. We have also used it successfully with the inside
semiring.
323
N?
i=1
vi ?
N ??
j=1
wj =
??
?
?
B?P(S)
?
b?B
wb0
?
c?S\B
?w?c?
?
? (13)
+ ?Res(exec(gv1 , . . . exec(gvN , w?1 ? ? ? ? ? w?N ?) . . .))? ,
max-k(exec(gv1 , . . . exec(gvN , w?1 ? ? ? ? ? w?N ?) . . .)), g0, 0
?
Figure 1: Combination operation for cube summing, where S = {1, 2, . . . , N ?} and P(S) is the power set of S excluding ?.
tool for performing probabilistic inference (Dar-
wiche, 2003). In the directed graph, there are ver-
tices corresponding to axioms (these are sinks in
the graph), ? vertices corresponding to theorems,
and ? vertices corresponding to summands in the
dynamic programming equations. Directed edges
point from each node to the nodes it depends on;
? vertices depend on? vertices, which depend on
? and axiom vertices.
Arithmetic circuits are amenable to automatic
differentiation in the reverse mode (Griewank
and Corliss, 1991), commonly used in back-
propagation algorithms. Importantly, this permits
us to calculate the exact gradient of the approx-
imate summation with respect to axiom values,
following Eisner et al (2005). This is desirable
when carrying out the optimization problems in-
volved in parameter estimation. Another differen-
tiation technique, implemented within the semir-
ing, is given by Eisner (2002).
Cube pruning is based on the k-best algorithms
of Huang and Chiang (2005), which save time
over generic semiring implementations through
lazy computation in both the aggregation and com-
bination operations. Their techniques are not as
clearly applicable here, because our goal is to sum
over all proofs instead of only finding a small sub-
set of them. If computing non-local features is a
computational bottleneck, they can be computed
only for the O(k) proofs considered when choos-
ing the best k as in cube pruning. Then, the com-
putational requirements for approximate summing
are nearly equivalent to cube pruning, but the ap-
proximation is less accurate.
6 Semirings Old and New
We now consider interesting special cases and
variations of cube summing.
6.1 The k-best+residual Semiring
When restricted to local features, cube pruning
and cube summing can be seen as proper semir-
k-best proof
(Goodman, 1999)
k-best + residual
Viterbi proof
(Goodman, 1999)
all proof
(Goodman, 1999)
Viterbi
(Viterbi, 1967)
ignore
proof
inside
(Baum et al, 1970)
ign
or
e r
es
idu
al
k = 0
k = ?k =
 1
Figure 2: Semirings generalized by k-best+residual.
ings. Cube pruning reduces to an implementation
of the k-best semiring (Goodman, 1998), and cube
summing reduces to a novel semiring we call the
k-best+residual semiring. Binary instantiations of
? and ? can be iteratively reapplied to give the
equivalent formulations in Eqs. 12 and 13. We de-
fine 0 as ?0, ??? and 1 as ?1, ?1, ??. The ? opera-
tor is easily shown to be commutative. That ? is
associative follows from associativity of max-k,
shown by Goodman (1998). Showing that ? is
associative and that ? distributes over ? are less
straightforward; proof sketches are provided in
Appendix A. The k-best+residual semiring gen-
eralizes many semirings previously introduced in
the literature; see Fig. 2.
6.2 Variations
Once we relax requirements about associativity
and distributivity and permit aggregation and com-
bination operators to operate on sets, several ex-
tensions to cube summing become possible. First,
when computing approximate summations with
non-local features, we may not always be inter-
ested in the best proofs for each item. Since the
purpose of summing is often to calculate statistics
324
under a model distribution, we may wish instead
to sample from that distribution. We can replace
the max-k function with a sample-k function that
samples k proofs from the scored list in its argu-
ment, possibly using the scores or possibly uni-
formly at random. This breaks associativity of ?.
We conjecture that this approach can be used to
simulate particle filtering for structured models.
Another variation is to vary k for different theo-
rems. This might be used to simulate beam search,
or to reserve computation for theorems closer to
goal , which have more proofs.
7 Conclusion
This paper has drawn a connection between cube
pruning, a popular technique for approximately
solving decoding problems, and the semiring-
weighted logic programming view of dynamic
programming. We have introduced a generaliza-
tion called cube summing, to be used for solv-
ing summing problems, and have argued that cube
pruning and cube summing are both semirings that
can be used generically, as long as the under-
lying probability models only include local fea-
tures. With non-local features, cube pruning and
cube summing can be used for approximate decod-
ing and summing, respectively, and although they
no longer correspond to semirings, generic algo-
rithms can still be used.
Acknowledgments
We thank three anonymous EACL reviewers, John Blatz, Pe-
dro Domingos, Jason Eisner, Joshua Goodman, and members
of the ARK group for helpful comments and feedback that
improved this paper. This research was supported by NSF
IIS-0836431 and an IBM faculty award.
A k-best+residual is a Semiring
In showing that k-best+residual is a semiring, we will restrict
our attention to the computation of the residuals. The com-
putation over proof lists is identical to that performed in the
k-best proof semiring, which was shown to be a semiring by
Goodman (1998). We sketch the proofs that ? is associative
and that ? distributes over ?; associativity of ? is straight-
forward.
For a proof list a?, ?a?? denotes the sum of proof scores,P
i:?ai,Ai??a?
ai. Note that:
?Res(a?)?+ ?max-k(a?)? = ?a?? (15)
?
?a? ? b?
?
? = ?a??
?
?b?
?
? (16)
Associativity. Given three semiring values u, v, and w, we
need to show that (u?v)?w = u?(v?w). After expand-
ing the expressions for the residuals using Eq. 14, there are
10 terms on each side, five of which are identical and cancel
out immediately. Three more cancel using Eq. 15, leaving:
LHS = ?Res(u? ? v?)? ?w??+ ?Res(max-k(u? ? v?) ? w?)?
RHS = ?u?? ?Res(v? ? w?)?+ ?Res(u? ? max-k(v? ? w?))?
If LHS = RHS, associativity holds. Using Eq. 15 again, we
can rewrite the second term in LHS to obtain
LHS = ?Res(u? ? v?)? ?w??+ ?max-k(u? ? v?) ? w??
? ?max-k(max-k(u? ? v?) ? w?)?
Using Eq. 16 and pulling out the common term ?w??, we have
LHS =(?Res(u? ? v?)?+ ?max-k(u? ? v?)?) ?w??
? ?max-k(max-k(u? ? v?) ? w?)?
= ?(u? ? v?) ? w?? ? ?max-k(max-k(u? ? v?) ? w?)?
= ?(u? ? v?) ? w?? ? ?max-k((u? ? v?) ? w?)?
The resulting expression is intuitive: the residual of (u?v)?
w is the difference between the sum of all proof scores and
the sum of the k-best. RHS can be transformed into this same
expression with a similar line of reasoning (and using asso-
ciativity of ?). Therefore, LHS = RHS and ? is associative.
Distributivity. To prove that ? distributes over ?, we must
show left-distributivity, i.e., thatu?(v?w) = (u?v)?(u?
w), and right-distributivity. We show left-distributivity here.
As above, we expand the expressions, finding 8 terms on the
LHS and 9 on the RHS. Six on each side cancel, leaving:
LHS = ?Res(v? ? w?)? ?u??+ ?Res(u? ? max-k(v? ? w?))?
RHS = ?Res(u? ? v?)?+ ?Res(u? ? w?)?
+ ?Res(max-k(u? ? v?) ?max-k(u? ? w?))?
We can rewrite LHS as:
LHS = ?Res(v? ? w?)? ?u??+ ?u? ? max-k(v? ? w?)?
? ?max-k(u? ? max-k(v? ? w?))?
= ?u?? (?Res(v? ? w?)?+ ?max-k(v? ? w?)?)
? ?max-k(u? ? max-k(v? ? w?))?
= ?u?? ?v? ? w?? ? ?max-k(u? ? (v? ? w?))?
= ?u?? ?v? ? w?? ? ?max-k((u? ? v?) ? (u? ? w?))?
where the last line follows because ? distributes over ?
(Goodman, 1998). We now work with the RHS:
RHS = ?Res(u? ? v?)?+ ?Res(u? ? w?)?
+ ?Res(max-k(u? ? v?) ?max-k(u? ? w?))?
= ?Res(u? ? v?)?+ ?Res(u? ? w?)?
+ ?max-k(u? ? v?) ?max-k(u? ? w?)?
? ?max-k(max-k(u? ? v?) ?max-k(u? ? w?))?
Since max-k(u? ? v?) and max-k(u? ? w?) are disjoint (we
assume no duplicates; i.e., two different theorems can-
not have exactly the same proof), the third term becomes
?max-k(u? ? v?)?+ ?max-k(u? ? w?)? and we have
= ?u? ? v??+ ?u? ? w??
? ?max-k(max-k(u? ? v?) ?max-k(u? ? w?))?
= ?u?? ?v??+ ?u?? ?w??
? ?max-k((u? ? v?) ? (u? ? w?))?
= ?u?? ?v? ? w?? ? ?max-k((u? ? v?) ? (u? ? w?))? .
325
References
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970.
A maximization technique occurring in the statis-
tical analysis of probabilistic functions of Markov
chains. Annals of Mathematical Statistics, 41(1).
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
W. W. Cohen and V. Carvalho. 2005. Stacked sequen-
tial learning. In Proc. of IJCAI.
A. Darwiche. 2003. A differential approach to infer-
ence in Bayesian networks. Journal of the ACM,
50(3).
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
piling Comp Ling: Practical weighted dynamic pro-
gramming and the Dyna language. In Proc. of HLT-
EMNLP.
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proc. of ACL.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005.
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In Proc.
of ACL.
J. Goodman. 1998. Parsing inside-out. Ph.D. thesis,
Harvard University.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25(4):573?605.
A. Griewank and G. Corliss. 1991. Automatic Differ-
entiation of Algorithms. SIAM.
L. Huang and D. Chiang. 2005. Better k-best parsing.
In Proc. of IWPT.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL.
L. Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul.
1999. An introduction to variational methods for
graphical models. Machine Learning, 37(2).
D. Klein and C. Manning. 2001. Parsing and hyper-
graphs. In Proc. of IWPT.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In Proc. of EMNLP.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Proc. of
ICGI.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
R. Levy, F. Reali, and T. Griffiths. 2008. Modeling the
effects of memory on human online sentence pro-
cessing with particle filters. In Advances in NIPS.
B. T. Lowerre. 1976. The Harpy Speech Recognition
System. Ph.D. thesis, Carnegie Mellon University.
D. J. C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labo-
ratory, Cambridge.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proc. of
EMNLP.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL.
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside
reestimation from partially bracketed corpora. In
Proc. of ACL, pages 128?135.
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CoNLL.
S. Shieber, Y. Schabes, and F. Pereira. 1995. Principles
and implementation of deductive parsing. Journal of
Logic Programming, 24(1-2):3?36.
D. A. Smith and J. Eisner. 2008. Dependency parsing
by belief propagation. In Proc. of EMNLP.
N. A. Smith, D. L. Vail, and J. D. Lafferty. 2007. Com-
putationally efficient M-estimation of log-linear
structure models. In Proc. of ACL.
C. Sutton and A. McCallum. 2004. Collective seg-
mentation and labeling of distant entities in infor-
mation extraction. In Proc. of ICML Workshop on
Statistical Relational Learning and Its Connections
to Other Fields.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimal decoding algo-
rithm. IEEE Transactions on Information Process-
ing, 13(2).
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for QA. In Proc. of EMNLP-CoNLL.
326
Computational Approaches to Morphology and Syntax
Brian Roark and Richard Sproat
(Oregon Health and Science University and University of Illinois at
Urbana?Champaign)
Oxford: Oxford University Press (Oxford surveys in syntax and morphology, edited by
Robert D. Van Valin Jr, volume 4), 2007, xx+316 pp; hardbound, ISBN 978-0-19-927477-2,
$110.00, ?60.00; paperbound, ISBN 978-0-19-927478-9, $45.00, ?24.99
Reviewed by
Noah A. Smith
Carnegie Mellon University
Brian Roark and Richard Sproat have written a compact and very readable book survey-
ing computational morphology and computational syntax. This text is not introductory;
instead, it will help bring computational linguists who do not work on morphology
or syntax up to date on these areas? latest developments. Certain chapters (in particu-
lar, Chapters 2 and 8) provide especially good starting points for advanced graduate
courses or seminars. The text is divided into an Introduction and Preliminaries chapter,
four chapters on computational approaches to morphology, and four chapters on com-
putational approaches to syntax. The morphology chapters focus primarily on formal
and theoretical issues, and are likely to be of interest to morphologists, computational
and not. The syntax chapters are drivenmore by engineering goals, withmore algorithm
details. Because a good understanding of probabilistic modeling is assumed, these
chapters will also be useful for machine learning researchers interested in language
processing.
Despite the authors? former affiliations, this book is not an AT&T analogue of
Beesley and Karttunen?s (2003) pedagogically motivated text on the Xerox finite-state
tools. This text is not about the AT&T FSM libraries or the algorithms underlying them
(cf. Roche and Schabes 1997).
1. Chapter 1: Introduction and Preliminaries
The first chapter is a take-no-prisoners introduction to finite-state automata and trans-
ducers and their semiring-weighted generalizations. Algorithms (e.g., for FST compo-
sition) are discussed but not presented in detail. Epsilon removal, minimization, and
determinization are mentioned but not defined. This material is probably too cursory to
serve as a lone introduction for those wishing to fully understand weighted FSTs, but
that lack of understanding will not be an impediment in the ensuing chapters because
weights do not re-surface until chapter 6, in the context of n-gram models, and even
then the algebraic view given here is not mentioned.
The chapter concludes with a defense of the place of finite-state models in linguis-
tics, followed by a clear explanation of the trade-offs in computational linguistics (e.g.,
between computational cost, expressive power, and annotation cost).
2. Part I (Chapters 2?5): Computational Approaches to Morphology
These chapters are primarily an argument for the effectiveness of finite-state transducers
in modeling natural language morphology.
Computational Linguistics Volume 34, Number 3
Chapter 2 provides a laundry list of morphological phenomena, arguing that finite-
state composition captures each of them, even in cases where there is a more obvious
solution (e.g., finite-state concatenation for concatenative phenomena). Examples of
many kinds of phenomena are given from diverse languages: prosodic restrictions in
Yowulmne, phonological effects of German affixes, and subsegmental morphology in
Welsh, to name a few. Importantly, the compile-reduce and merge operations are argued
to be syntactic sugar for effects achievable by finite-state composition, so that even root-
and-pattern Arabic morphology is explained in the same algebraic framework.
Reduplication effects, of course, challenge finite-state explanations, and so receive
their own section. Extended (non-regular) computational models are presented along-
side data from Gothic, Dakota, and Sye. The authors speculate that, in contrast with the
commonly accepted Correspondence theory, Morphological Doubling theory (Inkelas
and Zoll 1999), if correct, would imply that a non-regular ?copying? process is not at
work in reduplication. It is at this point that the reader may experience some discomfort;
should the reduplication problem be addressed in syntax rather than morphology?
Where exactly does the boundary lie? Readers hoping for a reassessment of this bound-
ary, or even a new bridge over it, will not find it here.
Chapter 3 begins with Stump?s (2001) two-dimensional taxonomy of morphological
theories, which appears rather divorced from the rich work on finite-state computational
morphology in Chapter 2. The subtleties among the four types of theories (lexical vs. in-
ferential and incremental vs. realizational, a more nuanced breakdown of the debate
over ?item-and-arrangement? vs. ?item-and-process?) may be difficult to understand
for the reader not trained in morphological theory, but resolution comes quickly. We
are presented with a series of examples showing ?proof-of-concept? fragmentary im-
plementations (in AT&T?s lextools) of phenomena in Sanskrit, Swahili, and Breton to
argue that lexical-incrementalist and inferential-realizational theories are computation-
ally equivalent; both can be implemented using FSTs and can lead to the same models.
Chapter 4 gives an algebraic analysis of Koskenniemi?s (1983) ?KIMMO? two-
level morphological analysis system. Koskenniemi?s hand-coded morphology rules are
argued to be a historical accident; if only computers had been more powerful in the
1980s, compilation of those rules into FSTs might have been automated, and in fact
Kaplan and Kay had already developed the algorithms.1 In the spirit of the previous
chapter, Sproat and Roark also note that morphological accounts that use one, two, or
more ?cascaded? levels are all computationally equivalent rational relations under the
finite-state approach, and that Optimality Theory can (under certain assumptions about
constraints) be implemented with finite-state operations as well (Ellison 1994).
Chapter 5, ?Machine learning of morphology,? focuses on unsupervised morphol-
ogy induction methods. There is about a page of discussion on statistical language
modeling approaches for disambiguation in agglutinative languages; no mention is
made of the more recent use of discriminative machine learning in morphological
disambiguation (Kudo, Yamamoto, and Matsumoto 2004; Habash and Rambow 2005;
Smith, Smith, and Tromble 2005). The chapter focuses on the approaches of Goldsmith
(2001), Schone and Jurafsky (2001), and Yarowsky and Wicentowski (2001). Although
each approach is interesting on its own, little effort is made to unify work in this area,
and none to bring the reader back full circle to finite-state models or the problem of
inducing from data regular grammars (Stolcke and Omohundro 1993) or their weights
1 The authors rightly point out that Koskenniemi deserves much credit for building an implementation
that aimed to have broad coverage, not merely a proof-of-concept.
454
Book Reviews
(Eisner 2002). Another missed opportunity here is the recent introduction of Bayesian
learning for word segmentation (Goldwater, Griffiths, and Johnson 2006).
Part I, in summary, aims to reduce many accounts of morphological phenomena
to finite-state transducer composition, drawing on a wealth of illustrative examples.
Twenty-two languages are listed in the language index at the end of the book, and,
tellingly, all of them are discussed exclusively in Part I. These chapters are good diplo-
macy toward theoretical linguistics, showing how computational arguments can have
theoretical implications.
3. Part II (Chapters 6?9): Computational Approaches to Syntax
In Part II, Roark and Sproat turn to models of syntax in computational linguistics.
Because most research in this area has been on English, English parsing is what they
present.
Chapter 6 covers finite-state approaches to syntax, including n-gram models and
smoothing, class-based language models, hidden Markov models (though without a
formal definition), part-of-speech tagging, log-linear models, and shallow parsing/
chunking. The Forward, Viterbi, Viterbi n-best, Forward?Backward algorithms, and
?Forward?Backward Decoding? (also known as posterior or minimum Bayes risk de-
coding) are covered with examples. This chapter is not as leisurely as the treatments
of HMMs by Manning and Schu?tze (1999) or Charniak (1993), and it omits basic back-
ground on probabilistic modeling. For example, why must we ensure that an n-gram
model?s total probability sums exactly to one? The answer relies on an understanding
of perplexity and its use in evaluation, now in decline (cf. ?stupid backoff? in Brants
et al 2007). The chapter does not reconnect with the algebraic view presented in Chap-
ter 1; for example, the connection between HMMs and WFSAs is never expressed.
Chapter 7 introduces context-free grammars and their parsers, broken down into
?deterministic? and ?nondeterministic? approaches.2 Probabilistic CFGs and treebanks
are introduced informally alongside the latter, which may confuse some readers. Am-
biguity is only presented as a natural phenomenon, not a problem of crude, over-
generating grammars. The probabilistic CKY and Earley algorithms are presented. The
Inside?Outside algorithm is presented in the context of Goodman?s (1996) maximum
expected recall parsing (another instance of minimum Bayes risk). As in the case of the
dynamic programming algorithms for HMMs in Chapter 6, the exposition is probably
too brisk to be an introduction to the topic.
Chapter 8 contains a thoughtful discussion of many best practices in statistical pars-
ing: treebank ?decoration? techniques such as parent annotation and lexicalization, and
the probabilitymodels underlying the parsers of Collins (1997) and Charniak (1997). De-
pendency parsing, unsupervised grammar induction, and finite-state approximations to
PCFGs are allotted short sections.
Chapter 9 covers context-sensitive models of syntax. Unification-based parsing
is presented at a high level, without formal details of unification or the differences
between theories such as LFG and HPSG. The ?lexicalized? models (TAG and CCG)
are treated more thoroughly; pseudocode for a TAG dynamic programming parser is
provided. There is brief treatment of Data-Oriented Parsing, reranking (a section that
2 These terms, though in wide use, are misnomers. All of these parsers are deterministic, since none
involve randomness or nondeterministic behavior resulting from multiple processors. Here
?(non)deterministic? refers to the grammar, not the parser.
455
Computational Linguistics Volume 34, Number 3
would have been of more practical use in Chapter 8), and transduction grammars (i.e.,
grammars over more than one string, most frequently used in machine translation).
The abundance of dynamic programming algorithms in Part II leads to the question
of whether such algorithms can be more easily taught (and unified) using recursive
equations (Manning and Schu?tze 1999), or a more declarative framework (Shieber,
Schabes, and Pereira 1995). Readers who prefer procedural pseudocode will find it here,
though the book does not address implementation tricks for storing and indexing parse
charts, or agenda-ordering methods to make parsing efficient.
These chapters are neither a gentle introduction to probabilistic modeling of syntax
for linguists nor a handbook for the language engineer who wants to build an efficient,
competitive parser. (There is also no advice on the relative merits of today?s parsers
available for download.) The audience that will find Part II most valuable will be
researchers who understand the principles of probabilistic modeling but want a more
up-to-date view of statistical parsing than offered by Manning and Schu?tze (1999), with
more coverage of advanced topics than Jurafsky and Martin (2008). This group might
include structured machine-learning researchers interested in the nuances of natural
language parsing and computational linguists who do not work on syntax but want to
keep up with the area.
4. Conclusion
The two major parts of this book stand as clear, up-to-date, and concisely written
summaries of particular sub-fields in computational linguistics: finite-state morphology
and English syntactic processing. The book does a fine job of elucidating the trade-
offs that make computational linguistics a tightrope act, and therefore serves as good
diplomacy for researchers in related fields. At 112 and 146 pages, respectively, either of
the parts is readable on a half-day plane or train trip.
Today, the strongest bridge between morphology and syntax is the Chomsky hier-
archy, which is mentioned frequently in this book (but never depicted). The contrast
between Parts I and II implies blueprints for more bridges: data resources to support
more powerful learning algorithms for morphology (as we have seen in syntax), a
stronger influence of non-English data on computational syntactic modeling (as we
have seen in morphology), and practical ways to accomplish the amalgamation of
morphology and syntax. This reviewer believes Computational Approaches to Morphology
and Syntax will re-introduce the two sub-communities to each other and help each to
leverage the successes of the other.
References
Beesley, Kenneth R. and Lauri Karttunen.
2003. Finite State Morphology. CSLI
Publications, Stanford, CA.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007. Large
language models in machine translation.
In Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, pages 858?867, Prague.
Charniak, Eugene. 1993. Statistical Language
Learning. MIT Press, Cambridge, MA.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. In Proceedings of the 14th National
Conference on Artificial Intelligence,
pages 598?603, Providence, RI.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 16?23, Madrid.
Eisner, Jason. 2002. Parameter estimation for
probabilistic finite-state transducers. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
pages 1?8, Philadelphia, PA.
Ellison, T. Mark. 1994. Phonological
derivation in Optimality Theory. In
456
Book Reviews
Proceedings of the 15th International
Conference on Computational Linguistics,
vol. 2, pages 1007?1013, Kyoto.
Goldsmith, John. 2001. Unsupervised
acquisition of the morphology of a natural
language. Computational Linguistics
27(2):153?198.
Goldwater, Sharon, Thomas L. Griffiths, and
Mark Johnson. 2006. Contextual
dependencies in unsupervised word
segmentation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 673?680, Sydney.
Goodman, Joshua. 1996. Parsing algorithms
and metrics. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 177?183,
Santa Cruz, CA.
Habash, Nizar and Owen Rambow. 2005.
Arabic tokenization, part-of-speech
tagging, and morphological
disambiguation in one fell swoop. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 573?580, Ann Arbor, MI.
Inkelas, Sharon and Cheryl Zoll. 1999.
Reduplication as morphological doubling.
Technical report 412-0800, Rutgers
Optimality Archive.
Jurafsky, Daniel and James H. Martin. 2008.
Speech and Language Processing (2nd
edition). Prentice Hall, Upper Saddle
River, NJ.
Koskenniemi, Kimmo. 1983. Two-Level
Morphology: A General Computational Model
for Word-Form Recognition and Production.
Ph.D. thesis, Department of General
Linguistics, University of Helsinki,
Helsinki, Finland.
Kudo, Taku, Kaoru Yamamoto, and Yuji
Matsumoto. 2004. Applying conditional
random fields to Japanese morphological
analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, pages 230?237, Barcelona.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Roche, Emmanuel and Yves Schabes
(editors). 1997. Finite-State Language
Processing. MIT Press, Cambridge, MA.
Schone, Patrick and Daniel Jurafsky. 2001.
Knowledge-free induction of morphology
using latent semantic analysis. In
Proceedings of the 5th Conference on
Computational Natural Language Learning,
pages 67?72, Toulouse.
Shieber, Stuart and Yves Schabes and
Fernando C. N. Pereira. 1995. Principles
and implementation of deductive
parsing. Journal of Logic Programming
24(1?2):3?36.
Smith, Noah A., David A. Smith, and Roy
W. Tromble. 2005. Context-based
morphological disambiguation with
random fields. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 475?482,
Vancouver.
Stolcke, Andreas and Stephen Omohundro.
1993. Hidden Markov model induction by
Bayesian model merging. In Stephen Jose?
Hanson, Jack D. Cowen, and C. Lee Giles,
editors, Advances in Neural Information
Processing Systems, vol. 5. Morgan
Kaufmann, San Mateo, CA, pages 11?18.
Stump, Gregory T. 2001. Inflectional
Morphology: A Theory of Paradigm
Structure. Cambridge University Press,
Cambridge, UK.
Yarowsky, David and Richard Wicentowski.
2001. Minimally supervised morphological
analysis by multimodal alignment. In
Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics,
pages 207?216, Toulouse.
Noah A. Smith is an assistant professor at Carnegie Mellon University. He conducts research in
statistical models and learning algorithms for natural language processing, including morphol-
ogy and syntax, as well as applications such as machine translation and question answering.
Smith?s address is Language Technologies Institute, School of Computer Science, Carnegie Mel-
lon University, 5000 Forbes Avenue, Pittsburgh, PA 15213; e-mail: nasmith@cs.cmu.edu; URL:
www.cs.cmu.edu/?nasmith.
457

Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 74?82,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Shared Logistic Normal Distributions for Soft Parameter Tying
in Unsupervised Grammar Induction
Shay B. Cohen and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,nasmith}@cs.cmu.edu
Abstract
We present a family of priors over probabilis-
tic grammar weights, called the shared logistic
normal distribution. This family extends the
partitioned logistic normal distribution, en-
abling factored covariance between the prob-
abilities of different derivation events in the
probabilistic grammar, providing a new way
to encode prior knowledge about an unknown
grammar. We describe a variational EM al-
gorithm for learning a probabilistic grammar
based on this family of priors. We then experi-
ment with unsupervised dependency grammar
induction and show significant improvements
using our model for both monolingual learn-
ing and bilingual learning with a non-parallel,
multilingual corpus.
1 Introduction
Probabilistic grammars have become an important
tool in natural language processing. They are most
commonly used for parsing and linguistic analy-
sis (Charniak and Johnson, 2005; Collins, 2003),
but are now commonly seen in applications like ma-
chine translation (Wu, 1997) and question answer-
ing (Wang et al, 2007). An attractive property of
probabilistic grammars is that they permit the use
of well-understood parameter estimation methods
for learning?both from labeled and unlabeled data.
Here we tackle the unsupervised grammar learning
problem, specifically for unlexicalized context-free
dependency grammars, using an empirical Bayesian
approach with a novel family of priors.
There has been an increased interest recently
in employing Bayesian modeling for probabilistic
grammars in different settings, ranging from putting
priors over grammar probabilities (Johnson et al,
2007) to putting non-parametric priors over deriva-
tions (Johnson et al, 2006) to learning the set of
states in a grammar (Finkel et al, 2007; Liang et al,
2007). Bayesian methods offer an elegant frame-
work for combining prior knowledge with data.
The main challenge in Bayesian grammar learning
is efficiently approximating probabilistic inference,
which is generally intractable. Most commonly vari-
ational (Johnson, 2007; Kurihara and Sato, 2006)
or sampling techniques are applied (Johnson et al,
2006).
Because probabilistic grammars are built out of
multinomial distributions, the Dirichlet family (or,
more precisely, a collection of Dirichlets) is a natural
candidate for probabilistic grammars because of its
conjugacy to the multinomial family. Conjugacy im-
plies a clean form for the posterior distribution over
grammar probabilities (given the data and the prior),
bestowing computational tractability.
Following work by Blei and Lafferty (2006) for
topic models, Cohen et al (2008) proposed an alter-
native to Dirichlet priors for probabilistic grammars,
based on the logistic normal (LN) distribution over
the probability simplex. Cohen et al used this prior
to softly tie grammar weights through the covariance
parameters of the LN. The prior encodes informa-
tion about which grammar rules? weights are likely
to covary, a more intuitive and expressive represen-
tation of knowledge than offered by Dirichlet distri-
butions.1
The contribution of this paper is two-fold. First,
from the modeling perspective, we present a gen-
eralization of the LN prior of Cohen et al (2008),
showing how to extend the use of the LN prior to
1Although the task, underlying model, and weights being
tied were different, Eisner (2002) also showed evidence for the
efficacy of parameter tying in grammar learning.
74
tie between any grammar weights in a probabilistic
grammar (instead of only allowing weights within
the same multinomial distribution to covary). Sec-
ond, from the experimental perspective, we show
how such flexibility in parameter tying can help in
unsupervised grammar learning in the well-known
monolingual setting and in a new bilingual setting
where grammars for two languages are learned at
once (without parallel corpora).
Our method is based on a distribution which we
call the shared logistic normal distribution, which
is a distribution over a collection of multinomials
from different probability simplexes. We provide a
variational EM algorithm for inference.
The rest of this paper is organized as follows. In
?2, we give a brief explanation of probabilistic gram-
mars and introduce some notation for the specific
type of dependency grammar used in this paper, due
to Klein and Manning (2004). In ?3, we present our
model and a variational inference algorithm for it. In
?4, we report on experiments for both monolingual
settings and a bilingual setting and discuss them. We
discuss future work (?5) and conclude in ?6.
2 Probabilistic Grammars and
Dependency Grammar Induction
A probabilistic grammar defines a probability dis-
tribution over grammatical derivations generated
through a step-by-step process. HMMs, for exam-
ple, can be understood as a random walk through
a probabilistic finite-state network, with an output
symbol sampled at each state. Each ?step? of the
walk and each symbol emission corresponds to one
derivation step. PCFGs generate phrase-structure
trees by recursively rewriting nonterminal symbols
as sequences of ?child? symbols (each itself either
a nonterminal symbol or a terminal symbol analo-
gous to the emissions of an HMM). Each step or
emission of an HMM and each rewriting operation
of a PCFG is conditionally independent of the other
rewriting operations given a single structural ele-
ment (one HMM or PCFG state); this Markov prop-
erty permits efficient inference for the probability
distribution defined by the probabilistic grammar.
In general, a probabilistic grammar defines the
joint probability of a string x and a grammatical
derivation y:
p(x,y | ?) =
K?
k=1
Nk?
i=1
?fk,i(x,y)k,i (1)
= exp
K?
k=1
Nk?
i=1
fk,i(x,y) log ?k,i
where fk,i is a function that ?counts? the number
of times the kth distribution?s ith event occurs in
the derivation. The ? are a collection of K multi-
nomials ??1, ...,?K?, the kth of which includes Nk
events. Note that there may be many derivations y
for a given string x?perhaps even infinitely many
in some kinds of grammars.
2.1 Dependency Model with Valence
HMMs and PCFGs are the best-known probabilis-
tic grammars, but there are many others. In this
paper, we use the ?dependency model with va-
lence? (DMV), due to Klein and Manning (2004).
DMV defines a probabilistic grammar for unla-
beled, projective dependency structures. Klein and
Manning (2004) achieved their best results with a
combination of DMV with a model known as the
?constituent-context model? (CCM). We do not ex-
periment with CCM in this paper, because it does
not fit directly in a Bayesian setting (it is highly defi-
cient) and because state-of-the-art unsupervised de-
pendency parsing results have been achieved with
DMV alone (Smith, 2006).
Using the notation above, DMV defines x =
?x1, x2, ..., xn? to be a sentence. x0 is a special
?wall? symbol, $, on the left of every sentence. A
tree y is defined by a pair of functions yleft and
yright (both {0, 1, 2, ..., n} ? 2{1,2,...,n}) that map
each word to its sets of left and right dependents,
respectively. Here, the graph is constrained to be a
projective tree rooted at x0 = $: each word except $
has a single parent, and there are no cycles or cross-
ing dependencies. yleft(0) is taken to be empty, and
yright(0) contains the sentence?s single head. Let
y(i) denote the subtree rooted at position i. The
probability P (y(i) | xi,?) of generating this sub-
tree, given its head word xi, is defined recursively,
as described in Fig. 1 (Eq. 2).
The probability of the entire tree is given by
p(x,y | ?) = P (y(0) | $,?). The ? are the multi-
nomial distributions ?s(? | ?, ?, ?) and ?c(? | ?, ?). To
75
P (y(i) | xi,?) = ?D?{left ,right} ?s(stop | xi,D , [yD(i) = ?]) (2)
??j?yD (i) ?s(?stop | xi,D ,firsty(j))? ?c(xj | xi,D)? P (y(j) | xj ,?)
Figure 1: The ?dependency model with valence? recursive equation. firsty(j) is a predicate defined to be true iff xj is
the closest child (on either side) to its parent xi. The probability of the tree p(x,y | ?) = P (y(0) | $,?).
follow the general setting of Eq. 1, we index these
distributions as ?1, ...,?K .
Headden et al (2009) extended DMV so that the
distributions ?c condition on the valence as well,
with smoothing, and showed significant improve-
ments for short sentences. Our experiments found
that these improvements do not hold on longer sen-
tences. Here we experiment only with DMV, but
note that our techniques are also applicable to richer
probabilistic grammars like that of Headden et al
2.2 Learning DMV
Klein and Manning (2004) learned the DMV prob-
abilities ? from a corpus of part-of-speech-tagged
sentences using the EM algorithm. EM manipulates
? to locally optimize the likelihood of the observed
portion of the data (here, x), marginalizing out the
hidden portions (here, y). The likelihood surface
is not globally concave, so EM only locally opti-
mizes the surface. Klein and Manning?s initializa-
tion, though reasonable and language-independent,
was an important factor in performance.
Various alternatives to EM were explored by
Smith (2006), achieving substantially more accu-
rate parsing models by altering the objective func-
tion. Smith?s methods did require substantial hyper-
parameter tuning, and the best results were obtained
using small annotated development sets to choose
hyperparameters. In this paper, we consider only
fully unsupervised methods, though we the Bayesian
ideas explored here might be merged with the bias-
ing approaches of Smith (2006) for further benefit.
3 Parameter Tying in the Bayesian Setting
As stated above, ? comprises a collection of multi-
nomials that weights the grammar. Taking the
Bayesian approach, we wish to place a prior on those
multinomials, and the Dirichlet family is a natural
candidate for such a prior because of its conjugacy,
which makes inference algorithms easier to derive.
For example, if we make a ?mean-field assumption,?
with respect to hidden structure and weights, the
variational algorithm for approximately inferring the
distribution over ? and trees y resembles the tradi-
tional EM algorithm very closely (Johnson, 2007).
In fact, variational inference in this case takes an ac-
tion similar to smoothing the counts using the exp-?
function during the E-step. Variational inference can
be embedded in an empirical Bayes setting, in which
we optimize the variational bound with respect to the
hyperparameters as well, repeating the process until
convergence.
3.1 Logistic Normal Distributions
While Dirichlet priors over grammar probabilities
make learning algorithms easy, they are limiting.
In particular, as noted by Blei and Lafferty (2006),
there is no explicit flexible way for the Dirichlet?s
parameters to encode beliefs about covariance be-
tween the probabilities of two events. To illustrate
this point, we describe how a multinomial ? of di-
mension d is generated from a Dirichlet distribution
with parameters ? = ??1, ..., ?d?:
1. Generate ?j ? ?(?j , 1) independently for j ?
{1, ..., d}.
2. ?j ? ?j/?i ?i.
where ?(?, 1) is a Gamma distribution with shape ?
and scale 1.
Correlation among ?i and ?j , i 6= j, cannot be
modeled directly, only through the normalization
in step 2. In contrast, LN distributions (Aitchison,
1986) provide a natural way to model such correla-
tion. The LN draws a multinomial ? as follows:
1. Generate ? ? Normal(?,?).
2. ?j ? exp(?j)/?i exp(?i).
76
I1 = {1:2, 3:6, 7:9} = { I1,1, I1,2, I1,L1 }
I2 = {1:2, 3:6} = { I2,1, I2,L2 }
I3 = {1:4, 5:7} = { I3,1, I3,L3 }
IN = {1:2} = { I4,L4 }
J1 J2 JK
?
?????
?????
partition struct. S
?1 = ??1,1, ?1,2, ?1,3, ?1,4, ?1,5, ?1,6, ?1,7, ?1,8, ?1,`1? ? Normal(?1,?1)
?2 = ??2,1, ?2,2, ?2,3, ?2,4, ?2,5, ?2,`2? ? Normal(?2,?2)
?3 = ??3,1, ?3,2, ?3,3, ?3,4, ?3,5, ?3,6, ?3,`3? ? Normal(?3,?3)
?4 = ??4,1, ?4,`4? ? Normal(?4,?4)
?
???
???
sample ?
??1 = 13 ??1,1 + ?2,1 + ?4,1, ?1,2 + ?2,2 + ?4,2?
??2 = 13 ??1,3 + ?2,3 + ?3,1, ?1,4 + ?2,4 + ?3,2, ?1,5 + ?2,5 + ?3,3, ?1,6 + ?2,6 + ?3,4?
??3 = 12 ??1,7 + ?3,5, ?1,8 + ?3,6, ?1,9 + ?3,7?
?
?
? combine ?
?1 = (exp ??1)
/?N1
i?=1 exp ??1,i?
?2 = (exp ??2)
/?N2
i?=1 exp ??2,i?
?3 = (exp ??3)
/?N3
i?=1 exp ??3,i?
?
????
????
softmax
Figure 2: An example of a shared logistic normal distribution, illustrating Def. 1. N = 4 experts are used to sample
K = 3 multinomials; L1 = 3, L2 = 2, L3 = 2, L4 = 1, `1 = 9, `2 = 6, `3 = 7, `4 = 2, N1 = 2, N2 = 4, and
N3 = 3. This figure is best viewed in color.
Blei and Lafferty (2006) defined correlated topic
models by replacing the Dirichlet in latent Dirich-
let alocation models (Blei et al, 2003) with a LN
distribution. Cohen et al (2008) compared Dirichlet
and LN distributions for learning DMV using em-
pirical Bayes, finding substantial improvements for
English using the latter.
In that work, we obtained improvements even
without specifying exactly which grammar proba-
bilities covaried. While empirical Bayes learning
permits these covariances to be discovered without
supervision, we found that by initializing the covari-
ance to encode beliefs about which grammar prob-
abilities should covary, further improvements were
possible. Specifically, we grouped the Penn Tree-
bank part-of-speech tags into coarse groups based
on the treebank annotation guidelines and biased
the initial covariance matrix for each child distri-
bution ?c(? | ?, ?) so that the probabilities of child
tags from the same coarse group covaried. For ex-
ample, the probability that a past-tense verb (VBD)
has a singular noun (NN) as a right child may be
correlated with the probability that it has a plu-
ral noun (NNS) as a right child. Hence linguistic
knowledge?specifically, a coarse grouping of word
classes?can be encoded in the prior.
A per-distribution LN distribution only permits
probabilities within a multinomial to covary. We
will generalize the LN to permit covariance among
any probabilities in ?, throughout the model. For
example, the probability of a past-tense verb (VBD)
having a noun as a right child might correlate with
the probability that other kinds of verbs (VBZ, VBN,
etc.) have a noun as a right child.
The partitioned logistic normal distribution
(PLN) is a generalization of the LN distribution
that takes the first step towards our goal (Aitchison,
1986). Generating from PLN involves drawing a
random vector from a multivariate normal distribu-
tion, but the logistic transformation is applied to dif-
ferent parts of the vector, leading to sampled multi-
nomial distributions of the required lengths from
different probability simplices. This is in principle
what is required for arbitrary covariance between
grammar probabilities, except that DMV has O(t2)
weights for a part-of-speech vocabulary of size t, re-
quiring a very large multivariate normal distribution
with O(t4) covariance parameters.
77
3.2 Shared Logistic Normal Distributions
To solve this problem, we suggest a refinement of
the class of PLN distributions. Instead of using a
single normal vector for all of the multinomials, we
use several normal vectors, partition each one and
then recombine parts which correspond to the same
multinomial, as a mixture. Next, we apply the lo-
gisitic transformation on the mixed vectors (each of
which is normally distributed as well). Fig. 2 gives
an example of a non-trivial case of using a SLN
distribution, where three multinomials are generated
from four normal experts.
We now formalize this notion. For a natural num-
ber N , we denote by 1:N the set {1, ..., N}. For a
vector in v ? RN and a set I ? 1:N , we denote
by vI to be the vector created from v by using the
coordinates in I . Recall that K is the number of
multinomials in the probabilistic grammar, and Nk
is the number of events in the kth multinomial.
Definition 1. We define a shared logistic nor-
mal distribution with N ?experts? over a collec-
tion of K multinomial distributions. Let ?n ?
Normal(?n,?n) be a set of multivariate normal
variables for n ? 1:N , where the length of ?n
is denoted `n. Let In = {In,j}Lnj=1 be a parti-
tion of 1:`n into Ln sets, such that ?Lnj=1In,j =1:`n and In,j ? In,j? = ? for j 6= j?. Let Jk
for k ? 1:K be a collection of (disjoint) sub-
sets of {In,j | n ? 1:N, j ? 1:`n, |In,j | =
Nk}, such that all sets in Jk are of the same size,
Nk. Let ??k = 1|Jk|
?
In,j?Jk ?n,In,j , and ?k,i =
exp(??k,i)
/?
i? exp(??k,i?) . We then say ? distributes
according to the shared logistic normal distribution
with partition structure S = ({In}Nn=1, {Jk}Kk=1)
and normal experts {(?n,?n)}Nn=1 and denote it by
? ? SLN(?,?, S).
The partitioned LN distribution in Aitchison
(1986) can be formulated as a shared LN distribution
where N = 1. The LN collection used by Cohen et
al. (2008) is the special case where N = K, each
Ln = 1, each `k = Nk, and each Jk = {Ik,1}.
The covariance among arbitrary ?k,i is not defined
directly; it is implied by the definition of the nor-
mal experts ?n,In,j , for each In,j ? Jk. We notethat a SLN can be represented as a PLN by relying
on the distributivity of the covariance operator, and
merging all the partition structure into one (perhaps
sparse) covariance matrix. However, if we are inter-
ested in keeping a factored structure on the covari-
ance matrices which generate the grammar weights,
we cannot represent every SLN as a PLN.
It is convenient to think of each ?i,j as a weight
associated with a unique event?s probability, a cer-
tain outcome of a certain multinomial in the prob-
abilistic grammar. By letting different ?i,j covary
with each other, we loosen the relationships among
?k,j and permit the model?at least in principle?
to learn patterns from the data. Def. 1 also implies
that we multiply several multinomials together in a
product-of-experts style (Hinton, 1999), because the
exponential of a mixture of normals becomes a prod-
uct of (unnormalized) probabilities.
Our extension to the model in Cohen et al (2008)
follows naturally after we have defined the shared
LN distribution. The generative story for this model
is as follows:
1. Generate ? ? SLN(?,?, S), where ? is a col-
lection of vectors ?k, k = 1, ...,K.
2. Generate x and y from p(x,y | ?) (i.e., sample
from the probabilistic grammar).
3.3 Inference
In this work, the partition structure S is known, the
sentences x are observed, the trees y and the gram-
mar weights ? are hidden, and the parameters of the
shared LN distribution ? and ? are learned.2
Our inference algorithm aims to find the poste-
rior over the grammar probabilities ? and the hidden
structures (grammar trees y). To do that, we use
variational approximation techniques (Jordan et al,
1999), which treat the problem of finding the pos-
terior as an optimization problem aimed to find the
best approximation q(?,y) of the posterior p(?,y |
x,?,?, S). The posterior q needs to be constrained
to be within a family of tractable and manageable
distributions, yet rich enough to represent good ap-
proximations of the true posterior. ?Best approx-
imation? is defined as the KL divergence between
q(?,y) and p(?,y | x,?,?, S).
Our variational inference algorithm uses a mean-
field assumption: q(?,y) = q(?)q(y). The distri-
bution q(?) is assumed to be a LN distribution with
2In future work, we might aim to learn S.
78
log p(x | ?,?, S) ?
(?N
n=1 Eq [log p(?k | ?k,?k)]
)
+
(?K
k=1
?Nk
i=1 f?k,i??k,i
)
+H(q)
? ?? ?
B
(3)
f?k,i , ?y q(y)fk,i(x,y) (4)
??k,i , ??Ck,i ? log ??k + 1? 1??k
?Nk
i?=1 exp
(
??Ck,i +
(??Ck,i)2
2
)
(5)
??Ck , 1|Jk|
?
Ir,j?Jk ??r,Ir,j (6)
(??Ck )2 , 1|Jk|2
?
Ir,j?Jk ??
2
r,Ir,j (7)
Figure 3: Variational inference bound. Eq. 3 is the bound itself, using notation defined in Eqs. 4?7 for clarity. Eq. 4
defines expected counts of the grammar events under the variational distribution q(y), calculated using dynamic pro-
gramming. Eq. 5 describes the weights for the weighted grammar defined by q(y). Eq. 6 and Eq. 7 describe the mean
and the variance, respectively, for the multivariate normal eventually used with the weighted grammar. These values
are based on the parameterization of q(?) by ??i,j and ??2i,j . An additional set of variational parameters is ??k, which
helps resolve the non-conjugacy of the LN distribution through a first order Taylor approximation.
all off-diagonal covariances fixed at zero (i.e., the
variational parameters consist of a single mean ??k,i
and a single variance ??2k,i for each ?k,i). There is
an additional variational parameter, ??k per multino-
mial, which is the result of an additional variational
approximation because of the lack of conjugacy of
the LN distribution to the multinomial distribution.
The distribution q(y) is assumed to be defined by a
DMV with unnormalized probabilities ??.
Inference optimizes the bound B given in Fig. 3
(Eq. 3) with respect to the variational parameters.
Our variational inference algorithm is derived simi-
larly to that of Cohen et al (2008). Because we wish
to learn the values of? and?, we embed variational
inference as the E step within a variational EM algo-
rithm, shown schematically in Fig. 4. In our exper-
iments, we use this variational EM algorithm on a
training set, and then use the normal experts? means
to get a point estimate for ?, the grammar weights.
This is called empirical Bayesian estimation. Our
approach differs from maximum a posteriori (MAP)
estimation, since we re-estimate the parameters of
the normal experts. Exact MAP estimation is prob-
ably not feasible; a variational algorithm like ours
might be applied, though better performance is ex-
pected from adjusting the SLN to fit the data.
4 Experiments
Our experiments involve data from two treebanks:
the Wall Street Journal Penn treebank (Marcus et
al., 1993) and the Chinese treebank (Xue et al,
2004). In both cases, following standard practice,
sentences were stripped of words and punctuation,
leaving part-of-speech tags for the unsupervised in-
duction of dependency structure. For English, we
train on ?2?21, tune on ?22 (without using annotated
data), and report final results on ?23. For Chinese,
we train on ?1?270, use ?301?1151 for development
and report testing results on ?271?300.3
To evaluate performance, we report the fraction
of words whose predicted parent matches the gold
standard corpus. This performance measure is also
known as attachment accuracy. We considered two
parsing methods after extracting a point estimate
for the grammar: the most probable ?Viterbi? parse
(argmaxy p(y | x,?)) and the minimum Bayes risk
(MBR) parse (argminy Ep(y?|x,?)[`(y;x,y?)]) with
dependency attachment error as the loss function
(Goodman, 1996). Performance with MBR parsing
is consistently higher than its Viterbi counterpart, so
we report only performance with MBR parsing.
4.1 Nouns, Verbs, and Adjectives
In this paper, we use a few simple heuristics to de-
cide which partition structure S to use. Our heuris-
3Unsupervised training for these datasets can be costly,
and requires iteratively running a cubic-time inside-outside dy-
namic programming algorithm, so we follow Klein and Man-
ning (2004) in restricting the training set to sentences of ten or
fewer words in length. Short sentences are also less structurally
ambiguous and may therefore be easier to learn from.
79
Input: initial parameters ?(0), ?(0), partition
structure S, observed data x, number of
iterations T
Output: learned parameters ?, ?
t? 1 ;
while t ? T do
E-step (for ` = 1, ...,M ) do: repeat
optimize B w.r.t. ??`,(t)r , r = 1, ..., N ;
optimize B w.r.t. ??`,(t)r , r = 1, ..., N ;
update ??`,(t)r , r = 1, ..., N ;
update ??`,(t)r , r = 1, ..., N ;
compute counts f? `,(t)r , r = 1, ..., N ;until convergence of B ;
M-step: optimize B w.r.t. ?(t) and ?(t);
t? t+ 1;
end
return ?(T ), ?(T )
Figure 4: Main details of the variational inference EM
algorithm with empirical Bayes estimation of ? and ?.
B is the bound defined in Fig. 3 (Eq. 3). N is the number
of normal experts for the SLN distribution defining the
prior. M is the number of training examples. The full
algorithm is given in Cohen and Smith (2009).
tics rely mainly on the centrality of content words:
nouns, verbs, and adjectives. For example, in the En-
glish treebank, the most common attachment errors
(with the LN prior from Cohen et al, 2008) happen
with a noun (25.9%) or a verb (16.9%) parent. In
the Chinese treebank, the most common attachment
errors happen with noun (36.0%) and verb (21.2%)
parents as well. The errors being governed by such
attachments are the direct result of nouns and verbs
being the most common parents in these data sets.
Following this observation, we compare four dif-
ferent settings in our experiments (all SLN settings
include one normal expert for each multinomial on
its own, equivalent to the regular LN setting from
Cohen et al):
? TIEV: We add normal experts that tie all proba-
bilities corresponding to a verbal parent (any par-
ent, using the coarse tags of Cohen et al, 2008).
Let V be the set of part-of-speech tags which be-
long to the verb category. For each direction D
(left or right), the set of multinomials of the form
?c(? | v,D), for v ? V , all share a normal expert.
For each direction D and each boolean value B
of the predicate firsty(?), the set of multinomials
?s(? | x,D , v), for v ? V share a normal expert.
? TIEN: This is the same as TIEV, only for nominal
parents.
? TIEV&N: Tie both verbs and nouns (in separate
partitions). This is equivalent to taking the union
of the partition structures of the above two set-
tings.
? TIEA: This is the same as TIEV, only for adjecti-
val parents.
Since inference for a model with parameter tying
can be computationally intensive, we first run the in-
ference algorithm without parameter tying, and then
add parameter tying to the rest of the inference algo-
rithm?s execution until convergence.
Initialization is important for the inference al-
gorithm, because the variational bound is a non-
concave function. For the expected values of the
normal experts, we use the initializer from Klein and
Manning (2004). For the covariance matrices, we
follow the setting in Cohen et al (2008) in our ex-
periments also described in ?3.1. For each treebank,
we divide the tags into twelve disjoint tag families.4
The covariance matrices for all dependency distri-
butions were initialized with 1 on the diagonal, 0.5
between tags which belong to the same family, and
0 otherwise. This initializer has been shown to be
more successful than an identity covariance matrix.
4.2 Monolingual Experiments
We begin our experiments with a monolingual set-
ting, where we learn grammars for English and Chi-
nese (separately) using the settings described above.
The attachment accuracy for this set of experi-
ments is described in Table 1. The baselines include
right attachment (where each word is attached to the
word to its right), MLE via EM (Klein and Man-
ning, 2004), and empirical Bayes with Dirichlet and
LN priors (Cohen et al, 2008). We also include a
?ceiling? (DMV trained using supervised MLE from
the training sentences? trees). For English, we see
that tying nouns, verbs or adjectives improves per-
formance compared to the LN baseline. Tying both
nouns and verbs improves performance a bit more.
4These are simply coarser tags: adjective, adverb, conjunc-
tion, foreign word, interjection, noun, number, particle, prepo-
sition, pronoun, proper noun, verb.
80
attachment acc. (%)
? 10 ? 20 all
En
gli
sh
Attach-Right 38.4 33.4 31.7
EM (K&M, 2004) 46.1 39.9 35.9
Dirichlet 46.1 40.6 36.9
LN (CG&S, 2008) 59.4 45.9 40.5
SLN, TIEV 60.2 46.2 40.0
SLN, TIEN 60.2 46.7 40.9
SLN, TIEV&N 61.3 47.4 41.4
SLN, TIEA 59.9 45.8 40.9
Biling. SLN, TIEV ?61.6 47.6 41.7
Biling. SLN, TIEN ?61.8 48.1 ?42.1
Biling. SLN, TIEV&N 62.0 ?48.0 42.2
Biling. SLN, TIEA 61.3 47.6 41.7
Supervised MLE 84.5 74.9 68.8
Ch
ine
se
Attach-Right 34.9 34.6 34.6
EM (K&M, 2004) 38.3 36.1 32.7
Dirichlet 38.3 35.9 32.4
LN 50.1 40.5 35.8
SLN, TIEV ?51.9 42.0 35.8
SLN, TIEN 43.0 38.4 33.7
SLN, TIEV&N 45.0 39.2 34.2
SLN, TIEA 47.4 40.4 35.2
Biling. SLN, TIEV ?51.9 42.0 35.8
Biling. SLN, TIEN 48.0 38.9 33.8
Biling. SLN, TIEV&N ?51.5 ?41.7 35.3
Biling. SLN, TIEA 52.0 41.3 35.2
Supervised MLE 84.3 66.1 57.6
Table 1: Attachment accuracy of different models, on test
data from the Penn Treebank and the Chinese Treebank
of varying levels of difficulty imposed through a length
filter. Attach-Right attaches each word to the word on
its right and the last word to $. Bold marks best overall
accuracy per length bound, and ? marks figures that are
not significantly worse (binomial sign test, p < 0.05).
4.3 Bilingual Experiments
Leveraging information from one language for the
task of disambiguating another language has re-
ceived considerable attention (Dagan, 1991; Smith
and Smith, 2004; Snyder and Barzilay, 2008; Bur-
kett and Klein, 2008). Usually such a setting re-
quires a parallel corpus or other annotated data that
ties between those two languages.5
Our bilingual experiments use the English and
Chinese treebanks, which are not parallel corpora,
to train parsers for both languages jointly. Shar-
5Haghighi et al (2008) presented a technique to learn bilin-
gual lexicons from two non-parallel monolingual corpora.
ing information between those two models is done
by softly tying grammar weights in the two hidden
grammars.
We first merge the models for English and Chi-
nese by taking a union of the multinomial fami-
lies of each and the corresponding prior parame-
ters. We then add a normal expert that ties be-
tween the parts of speech in the respective parti-
tion structures for both grammars together. Parts
of speech are matched through the single coarse
tagset (footnote 4). For example, with TIEV, let
V = V Eng ?V Chi be the set of part-of-speech tags
which belong to the verb category for either tree-
bank. Then, we tie parameters for all part-of-speech
tags in V . We tested this joint model for each of
TIEV, TIEN, TIEV&N, and TIEA. After running
the inference algorithm which learns the two mod-
els jointly, we use unseen data to test each learned
model separately.
Table 1 includes the results for these experiments.
The performance on English improved significantly
in the bilingual setting, achieving highest perfor-
mance with TIEV&N. Performance with Chinese is
also the highest in the bilingual setting, with TIEA
and TIEV&N.
5 Future Work
In future work we plan to lexicalize the model, in-
cluding a Bayesian grammar prior that accounts for
the syntactic patterns ofwords. Nonparametric mod-
els (Teh, 2006) may be appropriate. We also believe
that Bayesian discovery of cross-linguistic patterns
is an exciting topic worthy of further exploration.
6 Conclusion
We described a Bayesian model that allows soft pa-
rameter tying among any weights in a probabilistic
grammar. We used this model to improve unsuper-
vised parsing accuracy on two different languages,
English and Chinese, achieving state-of-the-art re-
sults. We also showed how our model can be effec-
tively used to simultaneously learn grammars in two
languages from non-parallel multilingual data.
Acknowledgments
This research was supported by NSF IIS-0836431. The
authors thank the anonymous reviewers and Sylvia Reb-
holz for helpful comments.
81
References
J. Aitchison. 1986. The Statistical Analysis of Composi-
tional Data. Chapman and Hall, London.
D. M. Blei and J. D. Lafferty. 2006. Correlated topic
models. In Proc. of NIPS.
D. M. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In Proc. of EMNLP.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. of ACL.
S. B. Cohen and N. A. Smith. 2009. Inference for proba-
bilistic grammars with shared logistic normal distribu-
tions. Technical report, Carnegie Mellon University.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589?637.
I. Dagan. 1991. Two languages are more informative
than one. In Proc. of ACL.
J. Eisner. 2002. Transformational priors over grammars.
In Proc. of EMNLP.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
G. E. Hinton. 1999. Products of experts. In Proc. of
ICANN.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. EMNLP-CoNLL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183?
233.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Proc. of
ICGI.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proc. of EMNLP, pages 49?56.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proc. of ACL.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
COLING-ACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous grammar
for question answering. In Proc. of EMNLP.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comp. Ling., 23(3):377?404.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1?30.
82
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236?244,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Preference Grammars: Softening Syntactic Constraints
to Improve Statistical Machine Translation
Ashish Venugopal Andreas Zollmann Noah A. Smith Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ashishv,zollmann,nasmith,vogel}@cs.cmu.edu
Abstract
We propose a novel probabilistic syn-
choronous context-free grammar formalism
for statistical machine translation, in which
syntactic nonterminal labels are represented
as ?soft? preferences rather than as ?hard?
matching constraints. This formalism allows
us to efficiently score unlabeled synchronous
derivations without forgoing traditional
syntactic constraints. Using this score as a
feature in a log-linear model, we are able
to approximate the selection of the most
likely unlabeled derivation. This helps
reduce fragmentation of probability across
differently labeled derivations of the same
translation. It also allows the importance of
syntactic preferences to be learned alongside
other features (e.g., the language model)
and for particular labeling procedures. We
show improvements in translation quality on
small and medium sized Chinese-to-English
translation tasks.
1 Introduction
Probabilistic synchronous context-free grammars
(PSCFGs) define weighted production rules that are
automatically learned from parallel training data. As
in classical CFGs, these rules make use of nontermi-
nal symbols to generalize beyond lexical modeling
of sentences. In MT, this permits translation and re-
ordering to be conditioned on more abstract notions
of context. For example,
VP? ne VB1 pas # do not VB1
represents the discontiguous translation of the
French words ?ne? and ?pas? to ?do not?, in the con-
text of the labeled nonterminal symbol ?VB? (rep-
resenting syntactic category ?verb?). Translation
with PSCFGs is typically expressed as the problem
of finding the maximum-weighted derivation consis-
tent with the source sentence, where the scores are
defined (at least in part) by R-valued weights asso-
ciated with the rules. A PSCFG derivation is a syn-
chronous parse tree.
Defining the translation function as finding the
best derivation has the unfortunate side effect of
forcing differently-derived versions of the same tar-
get sentence to compete with each other. In other
words, the true score of each translation is ?frag-
mented? across many derivations, so that each trans-
lation?s most probable derivation is the only one that
matters. The more Bayesian approach of finding
the most probable translation (integrating out the
derivations) instantiates an NP-hard inference prob-
lem even for simple word-based models (Knight,
1999); for grammar-based translation it is known
as the consensus problem (Casacuberta and de la
Higuera, 2000; Sima?an, 2002).
With weights interpreted as probabilities, the
maximum-weighted derivation is the maximum a
posteriori (MAP) derivation:
e? ? argmax
e
max
d
p(e, d | f)
where f is the source sentence, e ranges over tar-
get sentences, and d ranges over PSCFG deriva-
tions (synchronous trees). This is often described
as an approximation to the most probable transla-
tion, argmaxe
?
d p(e, d | f). In this paper, we
will describe a technique that aims to find the most
probable equivalence class of unlabeled derivations,
rather than a single labeled derivation, reducing the
fragmentation problem. Solving this problem ex-
actly is still an NP-hard consensus problem, but we
provide approximations that build on well-known
PSCFG decoding methods. Our model falls some-
where between PSCFGs that extract nonterminal
symbols from parse trees and treat them as part of
236
the derivation (Zollmann and Venugopal, 2006) and
unlabeled hierarchical structures (Chiang, 2005); we
treat nonterminal labels as random variables chosen
at each node, with each (unlabeled) rule express-
ing ?preferences? for particular nonterminal labels,
learned from data.
The paper is organized as follows. In Section 2,
we summarize the use of PSCFG grammars for
translation. We describe our model (Section 3).
Section 4 explains the preference-related calcula-
tions, and Section 5 addresses decoding. Experi-
mental results using preference grammars in a log-
linear translation model are presented for two stan-
dard Chinese-to-English tasks in Section 6. We re-
view related work (Section 7) and conclude.
2 PSCFGs for Machine Translation
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS , a target terminal set (target
vocabulary) TT , a shared nonterminal set N and a
setR of rules of the form: X ? ??, ?,w? where
? X ? N is a labeled nonterminal referred to as the
left-hand-side of the rule.
? ? ? (N ? TS)? is the source side of the rule.
? ? ? (N ? TT )? is the target side of the rule.
? w ? [0,?) is a nonnegative real-valued weight
assigned to the rule.
For visual clarity, we will use the # character to sep-
arate the source side of the rule ? from the target
side ?. PSCFG rules also have an implied one-to-
one mapping between nonterminal symbols in ? and
nonterminals symbols in ?. Chiang (2005), Zoll-
mann and Venugopal (2006) and Galley et al (2006)
all use parameterizations of this PSCFG formalism1.
Given a source sentence f and a PSCFG G, the
translation task can be expressed similarly to mono-
lingual parsing with a PCFG. We aim to find the
most likely derivation d of the input source sentence
and read off the English translation, identified by
composing ? from each rule used in the derivation.
This search for the most likely translation under the
1Galley et al (2006) rules are formally defined as tree trans-
ducers but have equivalent PSCFG forms.
MAP approximation can be defined as:
e? = tgt
(
argmax
d?D(G):src(d)=f
p(d)
)
(1)
where tgt(d) is the target-side yield of a derivation
d, and D(G) is the set of G?s derivations. Using an
n-gram language model to score derivations and rule
labels to constraint the rules that form derivations,
we define p(d) as log-linear model in terms of the
rules r ? R used in d as:
p(d) = pLM(tgt(d))?0 ?
( m?
i=1
pi(d)?i
)
?psyn(d)?m+1/Z(~?)
pi(d) =
?
r?R
hi(r)freq(r;d) (2)
psyn(d) =
{ 1 if d respects label constraints
0 otherwise (3)
where ~? = ?0 ? ? ??m+1 are weights that reflect the
relative importance of features in the model. The
features include the n-gram language model (LM)
score of the target yield sequence, a collection of m
rule feature functions hi : R ? R?0, and a ?syn-
tax? feature that (redundantly) requires every non-
terminal token to be expanded by a rule with that
nonterminal on its left-hand side. freq(r; d) denotes
the frequency of the rule r in the derivation d. Note
that ?m+1 can be effectively ignored when psyn is
defined as in Equation 3. Z(~?) is a normalization
constant that does not need to be computed during
search under the argmax search criterion in Equa-
tion 1. Feature weights ~? are trained discrimina-
tively in concert with the language model weight
to maximize the BLEU (Papineni et al, 2002) au-
tomatic evaluation metric via Minimum Error Rate
Training (MERT) (Och, 2003).
We use the open-source PSCFG rule extraction
framework and decoder from Zollmann et al (2008)
as the framework for our experiments. The asymp-
totic runtime of this decoder is:
O
(
|f |3
[
|N ||TT |2(n?1)
]K) (4)
where K is the maximum number of nonterminal
symbols per rule, |f | the source sentence length, and
237
n is the order of the n-gram LM that is used to com-
pute pLM. This constant factor in Equation 4 arises
from the dynamic programming item structure used
to perform search under this model. Using notation
from Chiang (2007), the corresponding item struc-
ture is:
[X, i, j, q(?)] : w (5)
whereX is the nonterminal label of a derivation, i, j
define a span in the source sentence, and q(?) main-
tains state required to compute pLM(?). Under the
MAP criterion we can discard derivations of lower
weight that share this item structure, but in practice
we often require additional lossy pruning to limit the
number of items produced. The Syntax-Augmented
MT model of Zollmann and Venugopal (2006), for
instance, produces a very large nonterminal set us-
ing ?slash? (NP/NN? the great) and ?plus? labels
(NP+VB ? she went) to assign syntactically mo-
tivated labels for rules whose target words do not
correspond to constituents in phrase structure parse
trees. These labels lead to fragmentation of prob-
ability across many derivations for the same target
sentence, worsening the impact of the MAP approx-
imation. In this work we address the increased frag-
mentation resulting from rules with labeled nonter-
minals compared to unlabeled rules (Chiang, 2005).
3 Preference Grammars
We extend the PSCFG formalism to include soft ?la-
bel preferences? for unlabeled rules that correspond
to alternative labelings that have been encountered
in training data for the unlabeled rule form. These
preferences, estimated via relative frequency counts
from rule occurrence data, are used to estimate the
feature psyn(d), the probability that an unlabeled
derivation can be generated under traditional syn-
tactic constraints. In classic PSCFG, psyn(d) en-
forces a hard syntactic constraint (Equation 3). In
our approach, label preferences influence the value
of psyn(d).
3.1 Motivating example
Consider the following labeled Chinese-to-English
PSCFG rules:
(4) S ? (?? VB1 #
a place where I can VB1
(3) S ? (?? VP1 #
a place where I can VP1
(2) SBAR ? (?? VP1 #
a place where I can VP1
(1) FRAG ? (?? AUX1 #
a place where I can AUX1
(8) VB ? m # eat
(1) VP ? m # eat
(1) NP ? m # eat
(10) NN ? m # dish
where the numbers are frequencies of the rule from
the training corpus. In classical PSCFG we can think
of the nonterminals mentioned in the rules as hard
constraints on which rules can be used to expand a
particular node; e.g., a VP can only be expanded by
a VP rule. In Equation 2, psyn(d) explicitly enforces
this hard constraint. Instead, we propose softening
these constraints. In the rules below, labels are rep-
resented as soft preferences.
(10) X ? (?? X1 #
a place where I can X1?
?
?
p(H0 = S, H1 = VB | r) = 0.4
p(H0 = S, H1 = VP | r) = 0.3
p(H0 = SBAR, H1 = VP | r) = 0.2
p(H0 = FRAG, H1 = AUX | r) = 0.1
?
?
?
(10) X ? m # eat{ p(H0 = VB | r) = 0.8
p(H0 = VP | r) = 0.1
p(H0 = NP | r) = 0.1
}
(10) X ? m # dish
{ p(H0 = NN | r) = 1.0 }
Each unlabeled form of the rule has an associated
distribution over labels for the nonterminals refer-
enced in the rule; the labels are random variables
Hi, with H0 the left-hand-side label. These un-
labeled rule forms are simply packed representa-
tions of the original labeled PSCFG rules. In ad-
dition to the usual features hi(r) for each rule, esti-
mated based on unlabeled rule frequencies, we now
238
have label preference distributions. These are esti-
mated as relative frequencies from the labelings of
the base, unlabeled rule. Our primary contribution
is how we compute psyn(d)?the probability that
an unlabeled derivation adheres to traditional syn-
tactic constraints?for derivations built from prefer-
ence grammar rules. By using psyn(d) as a feature
in the log-linear model, we allow the MERT frame-
work to evaluate the importance of syntactic struc-
ture relative to other features.
The example rules above highlight the potential
for psyn(d) to affect the choice of translation. The
translation of the Chinese word sequence  ( ?
? m can be performed by expanding the non-
terminal in the rule ?a place where I can X1? with
either ?eat? or ?dish.? A hierarchical system (Chi-
ang, 2005) would allow either expansion, relying on
features like pLM to select the best translation since
both expansions occurred the same number of times
in the data.
A richly-labeled PSCFG as in Zollmann and
Venugopal (2006) would immediately reject the rule
generating ?dish? due to hard label matching con-
straints, but would produce three identical, compet-
ing derivations. Two of these derivations would pro-
duce S as a root symbol, while one derivation would
produce SBAR. The two S-labeled derivations com-
pete, rather than reinforce the choice of the word
?eat,? which they both make. They will also com-
pete for consideration by any decoder that prunes
derivations to keep runtime down.
The rule preferences indicate that VB and VP are
both valid labels for the rule translating to ?eat?, and
both of these labels are compatible with the argu-
ments expected by ?a place where I can X1?. Al-
ternatively, ?dish? produces a NN label which is
not compatible with the arguments of this higher-
up rule. We design psyn(d) to reflect compatibility
between two rules (one expanding a right-hand side
nonterminal in the other), based on label preference
distributions.
3.2 Formal definition
Probabilistic synchronous context-free preference
grammars are defined as PSCFGs with the follow-
ing additional elements:
? H: a set of implicit labels, not to be confused
with the explicit label set N .
? pi: H ? N , a function that associates each im-
plicit label with a single explicit label. We can
therefore think ofH symbols as refinements of
the nonterminals inN (Matsusaki et al, 2005).
? For each rule r, we define a probability distri-
bution over vectors ~h of implicit label bindings
for its nonterminals, denoted ppref(~h | r). ~h
includes bindings for the left-hand side nonter-
minal (h0) as well as each right-hand side non-
terminal (h1, ..., h|~h|). Each hi ? H.
When N ,H are defined to include just a single
generic symbol as in (Chiang, 2005), we produce the
unlabeled grammar discussed above. In this work,
we define
? N = {S,X}
? H = {NP,DT,NN ? ? ? } = NSAMT
where N corresponds to the generic labels of Chi-
ang (2005) and H corresponds to the syntactically
motivated SAMT labels from (Zollmann and Venu-
gopal, 2006), and pi maps all elements of H to
X . We will use hargs(r) to denote the set of all
~h = ?h0, h1, ..., hk? ? Hk+1 that are valid bindings
for the rule with nonzero preference probability.
The preference distributions ppref from each rule
used in d are used to compute psyn(d) as described
next.
4 Computing feature psyn(d)
Let us view a derivation d as a collection of nonter-
minal tokens nj , j ? {1, ..., |d|}. Each nj takes an
explicit label in N . The score psyn(d) is a product,
with one factor per nj in the derivation d:
psyn(d) =
|d|?
j=1
?j (6)
Each ?j factor considers the two rules that nj partic-
ipates in. We will refer to the rule above nonterminal
token nj as rj (the nonterminal is a child in this rule)
and the rule that expands nonterminal token j as rj .
The intuition is that derivations in which these
two rules agree (at each j) about the implicit label
239
for nj , in H are preferable to derivations in which
they do not. Rather than making a decision about
the implicit label, we want to reward psyn when rj
and rj are consistent. Our way of measuring this
consistency is an inner product of preference distri-
butions:
?j ?
?
h?H
ppref(h | rj)ppref(h | rj) (7)
This is not quite the whole story, because ppref(? | r)
is defined as a joint distribution of all the implicit
labels within a rule; the implicit labels are not in-
dependent of each other. Indeed, we want the im-
plicit labels within each rule to be mutually consis-
tent, i.e., to correspond to one of the rule?s preferred
labelings, for both hargs(r) and hargs(r).
Our approach to calculating psyn within the dy-
namic programming algorithm is to recursively cal-
culate preferences for each chart item based on (a)
the smaller items used to construct the item and
(b) the rule that permits combination of the smaller
items into the larger one. We describe how the pref-
erences for chart items are calculated. Let a chart
item be denoted [X, i, j, u, ...] where X ? N and i
and j are positions in the source sentence, and
u : {h ? H | pi(h) = X} ? [0, 1]
(where ?h u(h) = 1) denotes a distribution over
possible X-refinement labels. We will refer to it
below as the left-hand-side preference distribution.
Additional information (such as language model
state) may also be included; it is not relevant here.
The simplest case is for a nonterminal token nj
that has no nonterminal children. Here the left-hand-
side preference distribution is simply given by
u(h) = ppref(h | rj) .
and we define the psyn factor to be ?j = 1.
Now consider the dynamic programming step
of combining an already-built item [X, i, j, u, ...]
rooted by explicit nonterminal X , spanning source
sentence positions i to j, with left-hand-side prefer-
ence distribution u, to build a larger item rooted by
Y through a rule r = Y ? ??X1??, ?X1??, w?with
preferences ppref(? | r).2 The new item will have
2We assume for the discussion that ?, ?? ? T ?S and ?, ?? ?
signature [Y, i ? |?|, j + |??|, v, ...]. The left-hand-
side preferences v for the new item are calculated as
follows:
v(h) = v?(h)?
h? v?(h?)
where (8)
v?(h) = ?
h??H:?h,h???hargs(r)
ppref(?h, h?? | r)? u(h?)
Renormalizing keeps the preference vectors on the
same scale as those in the rules. The psyn factor ?,
which is factored into the value of the new item, is
calculated as:
? = ?
h??H:?h,h???hargs(r)
u(h?) (9)
so that the value considered for the new item is w ?
? ? ..., where factors relating to pLM, for example,
may also be included. Coming back to our example,
if we let r be the leaf rule producing ?eat? at shared
nonterminal n1, we generate an item with:
u = ?u(VB) = 0.8, u(VP) = 0.1, u(NP) = 0.1?
?1 = 1
Combining this item with X ? ?  ( ? ? X1
# a place where I can X1 ? as r2 at nonterminal n2
generates a new target item with translation ?a place
where I can eat?, ?2 = 0.9 and v as calculated in
Fig. 1. In contrast, ?2 = 0 for the derivation where
r is the leaf rule that produces ?dish?.
This calculation can be seen as a kind of single-
pass, bottom-up message passing inference method
embedded within the usual dynamic programming
search.
5 Decoding Approximations
As defined above, accurately computing psyn(d) re-
quires extending the chart item structure with u. For
models that use the n-gram LM feature, the item
structure would be:
[X, i, j, q(?), u] : w (10)
Since u effectively summarizes the choice of rules
in a derivation, this extension would partition the
T ?T . If there are multiple nonterminals on the right-hand side
of the rule, we sum over the longer sequences in hargs(r) and
include appropriate values from the additional ?child? items?
preference vectors in the product.
240
v?(S) = ppref (?h = S, h? = VB? | r)u(VB) + ppref (?h = S, h? = VP? | r)u(VP) = (0.4? 0.8) + (0.3? 0.1) = 0.35
v?(SBAR) = p(?h = SBAR, h? = VP? | r)u(VP) = (0.2? 0.1) = 0.02
v = ?v(S) = 0.35/(v?(S) + v?(SBAR)), v(SBAR) = 0.02/v?(S) + v?(SBAR)? = ?v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37?
?2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9
Figure 1: Calculating v and ?2 for the running example.
search space further. To prevent this partitioning, we
follow the approach of Venugopal et al (2007). We
keep track of u for the best performing derivation
from the set of derivations that share [X, i, j, q(?)]
in a first-pass decoding. In a second top-down pass
similar to Huang and Chiang (2007), we can recal-
culate psyn(d) for alternative derivations in the hy-
pergraph; potentially correcting search errors made
in the first pass.
We face another significant practical challenge
during decoding. In real data conditions, the size
of the preference vector for a single rule can be very
high, especially for rules that include multiple non-
terminal symbols that are located on the left and
right boundaries of ?. For example, the Chinese-
to-English rule X ? ? X1 ? X2 # X1 ?s X2 ? has
over 24K elements in hargs(r) when learned for the
medium-sized NIST task used below. In order to
limit the explosive growth of nonterminals during
decoding for both memory and runtime reasons, we
define the following label pruning parameters:
? ?R: This parameter limits the size of hargs(r) to
the ?R top-scoring preferences, defaulting other
values to zero.
? ?L: This parameter is the same as ?R but applied
only to rules with no nonterminals. The stricter of
?L and ?R is applied if both thresholds apply.
? ?P : This parameter limits the number labels in
item preference vectors (Equation 8) to the ?P
most likely labels during decoding, defaulting
other preferences to zero.
6 Empirical Results
We evaluate our preference grammar model on
small (IWSLT) and medium (NIST) data Chinese-
to-English translation tasks (described in Table 1).
IWSLT is a limited domain, limited resource task
(Paul, 2006), while NIST is a broadcast news task
with wide genre and domain coverage. We use a
subset of the full training data (67M words of En-
glish text) from the annual NIST MT Evaluation.
Development corpora are used to train model pa-
rameters via MERT. We use a variant of MERT that
prefers sparse solutions where ?i = 0 for as many
features as possible. At each MERT iteration, a sub-
set of features ? are assigned 0 weight and optimiza-
tion is repeated. If the resulting BLEU score is not
lower, these features are left at zero.
All systems are built on the SAMT framework
described in Zollmann et al (2008), using a tri-
gram LM during search and the full-order LM dur-
ing a second hypergraph rescoring pass. Reorder-
ing limits are set to 10 words for all systems. Prun-
ing parameters during decoding limit the number of
derivations at each source span to 300.
The system ?Hier.? uses a grammar with a single
nonterminal label as in Chiang (2005). The system
?Syntax? applies the grammar from Zollmann and
Venugopal (2006) that generates a large number of
syntactically motivated nonterminal labels. For the
NIST task, rare rules are discarded based on their
frequency in the training data. Purely lexical rules
(that include no terminal symbols) that occur less
than 2 times, or non-lexical rules that occur less than
4 times are discarded.
IWSLT task: We evaluate the preference gram-
mar system ?Pref.? with parameters ?R = 100,
?L = 5, ?P = 2. Results comparing systems Pref.
to Hier. and Syntax are shown in Table 2. Auto-
matic evaluation results using the preference gram-
mar translation model are positive. The preference
grammar system shows improvements over both the
Hier. and Syntax based systems on both unseen eval-
uation sets IWSLT 2007 and 2008. The improve-
ments are clearest on the BLEU metric (matching
the MERT training criteria). On 2007 test data,
Pref. shows a 1.2-point improvement over Hier.,
while on the 2008 data, there is a 0.6-point improve-
ment. For the IWSLT task, we report additional au-
241
System Name Words in Target Text LM singleton 1-n-grams (n) Dev. Test
IWSLT 632K 431K (5) IWSLT06 IWSLT07,08
NIST 67M 102M (4) MT05 MT06
Table 1: Training data configurations used to evaluate preference grammars. The number of words in the target text
and the number of singleton 1-n-grams represented in the complete model are the defining statistics that characterize
the scale of each task. For each LM we also indicate the order of the n-gram model.
System Dev
BLEU
(lpen) ?
2007
BLEU
(lpen) ?
2008
BLEU
(lpen) ?
2008
WER ?
2008 PER
?
2008
MET. ?
2008
GTM ?
Hier. 28.0
(0.89)
37.0
(0.89)
45.9
(0.91)
44.5 39.9 61.8 70.7
Syntax 30.9
(0.96)
35.5
(0.94)
45.3
(0.95)
45.7 40.4 62.1 71.5
Pref. 28.3
(0.88)
38.2
(0.90)
46.3
(0.91)
43.8 40.0 61.7 71.2
Table 2: Translation quality metrics on the IWSLT translation task, with IWSLT 2006 as the development corpora, and
IWSLT 2007 and 2008 as test corpora. Each metric is annotated with an ? if increases in the metric value correspond
to increase in translation quality and a ? if the opposite is true. We also list length penalties for the BLEU metric to
show that improvements are not due to length optimizations alone.
tomatic evaluation metrics that generally rank the
Pref. system higher than Hier. and Syntax. As a fur-
ther confirmation, our feature selection based MERT
chooses to retain ?m+1 in the model. While the
IWSLT results are promising, we perform a more
complete evaluation on the NIST translation task.
NIST task: This task generates much larger rule
preference vectors than the IWSLT task simply due
to the size of the training corpora. We build sys-
tems with both ?R = 100, 10 varying ?P . Vary-
ing ?P isolates the relative impact of propagating
alternative nonterminal labels within the preference
grammar model. ?L = 5 for all NIST systems. Pa-
rameters ? are trained via MERT on the ?R = 100,
?L = 5, ?P = 2 system. BLEU scores for each
preference grammar and baseline system are shown
in Table 3, along with translation times on the test
corpus. We also report length penalties to show that
improvements are not simply due to better tuning of
output length.
The preference grammar systems outperform the
Hier. baseline by 0.5 points on development data,
and upto 0.8 points on unseen test data. While sys-
tems with ?R = 100 take significantly longer to
translate the test data than Hier., setting ?R = 10
takes approximately as long as the Syntax based sys-
tem but produces better slightly better results (0.3
points).
The improvements in translation quality with the
preference grammar are encouraging, but how much
of this improvement can simply be attributed to
MERT finding a better local optimum for parame-
ters ?? To answer this question, we use parameters
?? optimized by MERT for the preference grammar
system to run a purely hierarchical system, denoted
Hier.(??), which ignores the value of ?m+1 during
decoding. While almost half of the improvement
comes from better parameters learned via MERT for
the preference grammar systems, 0.5 points can be
still be attributed purely to the feature psyn. In addi-
tion, MERT does not set parameter ?m+1 to 0, cor-
roborating the value of the psyn feature again. Note
that Hier.(??) achieves better scores than the Hier.
system which was trained via MERT without psyn.
This highlights the local nature of MERT parameter
search, but also points to the possibility that train-
ing with the feature psyn produced a more diverse
derivation space, resulting in better parameters ?.
We see a very small improvement (0.1 point) by al-
lowing the runtime propagation of more than 1 non-
terminal label in the left-hand side posterior distribu-
tion, but the improvement doesn?t extend to ?P = 5.
Improved integration of the feature psyn(d) into de-
coding might help to widen this gap.
242
Test
Dev. Test time
System BLEU (lpen) BLEU (lpen) (h:mm)
Baseline Systems
Hier. 34.1 (0.99) 31.8 (0.95) 0:12
Syntax 34.7 (0.99) 32.3 (0.95) 0:45
Hier.(??) - 32.1 (0.95) 0:12
Preference Grammar: ?R = 100
?P = 1 - 32.5 (0.96) 3:00
?P = 2 34.6 (0.99) 32.6 (0.95) 3:00
?P = 5 - 32.5 (0.95) 3:20
Preference Grammar: ?R = 10
?P = 1 - 32.5 (0.95) 1:03
?P = 2 - 32.6 (0.95) 1:10
?P = 5 - 32.5 (0.95) 1:10
Table 3: Translation quality and test set translation time
(using 50 machines with 2 tasks per machine) measured
by the BLEU metric for the NIST task. NIST 2006 is
used as the development (Dev.) corpus and NIST 2007 is
used as the unseen evaluation corpus (Test). Dev. scores
are reported for systems that have been separately MERT
trained, Pref. systems share parameters from a single
MERT training. Systems are described in the text.
7 Related Work
There have been significant efforts in the both the
monolingual parsing and machine translation liter-
ature to address the impact of the MAP approxi-
mation and the choice of labels in their respective
models; we survey the work most closely related to
our approach. May and Knight (2006) extract n-
best lists containing unique translations rather than
unique derivations, while Kumar and Byrne (2004)
use the Minimum Bayes Risk decision rule to se-
lect the lowest risk (highest BLEU score) translation
rather than derivation from an n-best list. Tromble
et al (2008) extend this work to lattice structures.
All of these approaches only marginalize over alter-
native candidate derivations generated by a MAP-
driven decoding process. More recently, work by
Blunsom et al (2007) propose a purely discrimina-
tive model whose decoding step approximates the
selection of the most likely translation via beam
search. Matsusaki et al (2005) and Petrov et al
(2006) propose automatically learning annotations
that add information to categories to improve mono-
lingual parsing quality. Since the parsing task re-
quires selecting the most non-annotated tree, the an-
notations add an additional level of structure that
must be marginalized during search. They demon-
strate improvements in parse quality only when a
variational approximation is used to select the most
likely unannotated tree rather than simply stripping
annotations from the MAP annotated tree. In our
work, we focused on approximating the selection of
the most likely unlabeled derivation during search,
rather than as a post-processing operation; the meth-
ods described above might improve this approxima-
tion, at some computational expense.
8 Conclusions and Future Work
We have proposed a novel grammar formalism that
replaces hard syntactic constraints with ?soft? pref-
erences. These preferences are used to compute a
machine translation feature (psyn(d)) that scores un-
labeled derivations, taking into account traditional
syntactic constraints. Representing syntactic con-
straints as a feature allows MERT to train the cor-
responding weight for this feature relative to others
in the model, allowing systems to learn the relative
importance of labels for particular resource and lan-
guage scenarios as well as for alternative approaches
to labeling PSCFG rules.
This approach takes a step toward addressing
the fragmentation problems of decoding based on
maximum-weighted derivations, by summing the
contributions of compatible label configurations
rather than forcing them to compete. We have sug-
gested an efficient technique to approximate psyn(d)
that takes advantage of a natural factoring of deriva-
tion scores. Our approach results in improvements
in translation quality on small and medium resource
translation tasks. In future work we plan to focus on
methods to improve on the integration of the psyn(d)
feature during decoding and techniques that allow us
consider more of the search space through less prun-
ing.
Acknowledgements
We appreciate helpful comments from three anony-
mous reviewers. Venugopal and Zollmann were sup-
ported by a Google Research Award. Smith was sup-
ported by NSF grant IIS-0836431.
243
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2007.
A discriminative latent variable model for statistical
machine translation. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Proc. of the 5th Inter-
national Colloquium on Grammatical Inference: Al-
gorithms and Applications.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compua-
tional Linguistics (ACL).
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proceed-
ings of the Annual Meeting of the Association for Com-
puational Linguistics (ACL), Sydney, Australia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Annual Meeting of the Association
for Compuational Linguistics (ACL).
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion.
Shankar Kumar and William Byrne. 2004. Min-
imum Bayes-risk decoding for statistical machine
translation. In Proceedings of the Human Lan-
guage Technology and North American Association for
Computational Linguistics Conference (HLT/NAACL),
Boston,MA, May 27-June 1.
Takuya Matsusaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Jonathan May and Kevin Knight. 2006. A better N-best
list: Practical determinization of weighted finite tree
automata. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
Conference (HLT/NAACL).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Michael Paul. 2006. Overview of the IWSLT 2006 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the Annual
Meeting of the Association for Compuational Linguis-
tics (ACL).
Khalil Sima?an. 2002. Computational complexity of
probabilistic disambiguation. Grammars, 5(2):125?
151.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decod-
ing for statistical machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
Synchronous-CFG driven statistical MT. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics Conference (HLT/NAACL).
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, New York, June.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the Conference on Computa-
tional Linguistics (COLING).
244
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 272?280,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Predicting Risk from Financial Reports with Regression
Shimon Kogan
McCombs School of Business
University of Texas at Austin
Austin, TX 78712, USA
shimon.kogan@mccombs.utexas.edu
Dimitry Levin
Mellon College of Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dimitrylevin@gmail.com
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Jacob S. Sagi
Owen Graduate School of Management
Vanderbilt University
Nashville, TN 37203, USA
Jacob.Sagi@Owen.Vanderbilt.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We address a text regression problem: given a
piece of text, predict a real-world continuous
quantity associated with the text?s meaning. In
this work, the text is an SEC-mandated finan-
cial report published annually by a publicly-
traded company, and the quantity to be pre-
dicted is volatility of stock returns, an empiri-
cal measure of financial risk. We apply well-
known regression techniques to a large cor-
pus of freely available financial reports, con-
structing regression models of volatility for
the period following a report. Our models ri-
val past volatility (a strong baseline) in pre-
dicting the target variable, and a single model
that uses both can significantly outperform
past volatility. Interestingly, our approach is
more accurate for reports after the passage of
the Sarbanes-Oxley Act of 2002, giving some
evidence for the success of that legislation in
making financial reports more informative.
1 Introduction
We consider a text regression problem: given a piece
of text, predict a R-valued quantity associated with
that text. Specifically, we use a company?s annual
financial report to predict the financial risk of invest-
ment in that company, as measured empirically by a
quantity known as stock return volatility.
Predicting financial risk is of clear interest to
anyone who invests money in stocks and central
to modern portfolio choice. Financial reports are
a government-mandated artifact of the financial
world that?one might hypothesize?contain a large
amount of information about companies and their
value. Indeed, it is an important question whether
mandated disclosures are informative, since they are
meant to protect investors but are costly to produce.
The intrinsic properties of the problem are attrac-
tive as a test-bed for NLP research. First, there is
no controversy about the usefulness or existential
reality of the output variable (volatility). Statisti-
cal NLP often deals in the prediction of variables
ranging from text categories to linguistic structures
to novel utterances. While many of these targets are
uncontroversially useful, they often suffer from eval-
uation difficulties and disagreement among annota-
tors. The output variable in this work is a statistic
summarizing facts about the real world; it is not sub-
ject to any kind of human expertise, knowledge, or
intuition. Hence this prediction task provides a new,
objective test-bed for any kind of linguistic analysis.
Second, many NLP problems rely on costly anno-
tated resources (e.g., treebanks or aligned bilingual
corpora). Because the text and historical financial
data used in this work are freely available (by law)
and are generated as a by-product of the American
272
economy, old and new data can be obtained by any-
one with relatively little effort.
In this paper, we demonstrate that predicting fi-
nancial volatility automatically from a financial re-
port is a novel, challenging, and easily evaluated nat-
ural language understanding task. We show that a
very simple representation of the text (essentially,
bags of unigrams and bigrams) can rival and, in
combination, improve over a strong baseline that
does not use the text. Analysis of the learned models
provides insights about what can make this problem
more or less difficult, and suggests that disclosure-
related legislation led to more transparent reporting.
2 Stock Return Volatility
Volatility is often used in finance as a measure of
risk. It is measured as the standard deviation of
a stock?s returns over a finite period of time. A
stock will have high volatility when its price fluctu-
ates widely and low volatility when its price remains
more or less constant.
Let rt = PtPt?1 ? 1 be the return on a given stockbetween the close of trading day t ? 1 and day t,
where Pt is the (dividend-adjusted) closing stock
price at date t. The measured volatility over the time
period from day t? ? to day t is equal to the sample
s.d.:
v[t??,t] =
????
??
i=0
(rt?i ? r?)2
/
? (1)
where r? is the sample mean of rt over the period. In
this work, the above estimate will be treated as the
true output variable on training and testing data.
It is important to note that predicting volatility is
not the same as predicting returns or value. Rather
than trying to predict how well a stock will perform,
we are trying to predict how stable its price will be
over a future time period. It is, by now, received
wisdom in the field of economics that predicting a
stock?s performance, based on easily accessible pub-
lic information, is difficult. This is an attribute of
well-functioning (or ?efficient?) markets and a cor-
nerstone of the so-called ?efficient market hypoth-
esis? (Fama, 1970). By contrast, the idea that one
can predict a stock?s level of risk using public in-
formation is uncontroversial and a basic assumption
made by many economically sound pricing mod-
els. A large body of research in finance suggests
that the two types of quantities are very different:
while predictability of returns could be easily traded
away by the virtue of buying/selling stocks that are
under- or over-valued (Fama, 1970), similar trades
are much more costly to implement with respect to
predictability of volatility (Dumas et al, 2007). By
focusing on volatility prediction, we avoid taking
a stance on whether or not the United States stock
market is informationally efficient.
3 Problem Formulation
Given a text document d, we seek to predict the
value of a continuous variable v. We do this via a
parameterized function f :
v? = f(d;w) (2)
where w ? Rd are the parameters or weights. Our
approach is to learn a human-interpretable w from
a collection of N training examples {?di, vi?}Ni=1,
where each di is a document and each vi ? R.
Support vector regression (Drucker et al, 1997)
is a well-known method for training a regression
model. SVR is trained by solving the following op-
timization problem:
min
w?Rd
1
2?w?
2+CN
N?
i=1
max
(
0,
???vi ? f(di;w)
???? 
)
? ?? ?
-insensitive loss function(3)
where C is a regularization constant and  controls
the training error.1 The training algorithm finds
weights w that define a function f minimizing the
(regularized) empirical risk.
Let h be a function from documents into some
vector-space representation? Rd. In SVR, the func-
tion f takes the form:
f(d;w) = h(d)>w =
N?
i=1
?iK(d,di) (4)
where Equation 4 re-parameterizes f in terms of a
kernel function K with ?dual? weights ?i. K can
1Given the embedding h of documents in Rd,  defines
a ?slab? (region between two parallel hyperplanes, some-
times called the ?-tube?) in Rd+1 through which each
?h(di), f(di;w)? must pass in order to have zero loss.
273
year words documents words/doc.
1996 5.5M 1,408 3,893
1997 9.3M 2,260 4,132
1998 11.8M 2,462 4,808
1999 14.5M 2,524 5,743
2000 13.4M 2,425 5,541
2001 15.4M 2,596 5,928
2002 22.7M 2,846 7,983
2003 35.3M 3,612 9,780
2004 38.9M 3,559 10,936
2005 41.9M 3,474 12,065
2006 38.8M 3,308 11,736
total 247.7M 26,806 9,240
Table 1: Dimensions of the dataset used in this paper,
after filtering and tokenization. The near doubling in av-
erage document size during 2002?3 is possibly due to the
passage of the Sarbanes-Oxley Act of 2002 in the wake
of Enron?s accounting scandal (and numerous others).
be seen as a similarity function between two docu-
ments. At test time, a new example is compared to a
subset of the training examples (those with ?i 6= 0);
typically with SVR this set is sparse. With the linear
kernel, the primal and dual weights relate linearly:
w =
N?
i=1
?ih(di) (5)
The full details of SVR and its implementation are
beyond the scope of this paper; interested readers are
referred to Scho?lkopf and Smola (2002). SVMlight
(Joachims, 1999) is a freely available implementa-
tion of SVR training that we used in our experi-
ments.2
4 Dataset
In the United States, the Securities Exchange Com-
mission mandates that all publicly-traded corpora-
tions produce annual reports known as ?Form 10-
K.? The report typically includes information about
the history and organization of the company, equity
and subsidiaries, as well as financial information.
These reports are available to the public and pub-
lished on the SEC?s web site.3 The structure of the
10-K is specified in detail in the legislation. We have
collected 54,379 reports published over the period
2Available at http://svmlight.joachims.org.
3http://www.sec.gov/edgar.shtml
1996?2006 from 10,492 different companies. Each
report comes with a date of publication, which is im-
portant for tying the text back to the financial vari-
ables we seek to predict.
From the perspective of predicting future events,
one section of the 10-K is of special interest: Section
7, known as ?management?s discussion and anal-
ysis of financial conditions and results of opera-
tions? (MD&A), and in particular Subsection 7A,
?quantitative and qualitative disclosures about mar-
ket risk.? Because Section 7 is where the most im-
portant forward-looking content is most likely to
be found, we filter other sections from the reports.
The filtering is done automatically using a short,
hand-written Perl script that seeks strings loosely
matching the Section 7, 7A, and 8 headers, finds the
longest reasonable ?Section 7? match (in words) of
more than 1,000 whitespace-delineated tokens.
Section 7 typically begins with an introduction
like this (from ABC?s 1998 Form 10-K, before to-
kenization for readability; boldface added):
The following discussion and analysis of
ABC?s consolidated financial condition and
consolidated results of operation should be
read in conjunction with ABC?s Consoli-
dated Financial Statements and Notes thereto
included elsewhere herein. This discus-
sion contains certain forward-looking state-
ments which involve risks and uncertain-
ties. ABC?s actual results could differ mate-
rially from the results expressed in, or implied
by, such statements. See ?Regarding Forward-
Looking Statements.?
Not all of the documents downloaded pass the fil-
ter at all, and for the present work we have only used
documents that do pass the filter. (One reason for the
failure of the filter is that many 10-K reports include
Section 7 ?by reference,? so the text is not directly
included in the document.)
In addition to the reports, we used the Center
for Research in Security Prices (CRSP) US Stocks
Database to obtain the price return series along with
other firm characteristics.4 We proceeded to calcu-
late two volatilities for each firm/report observation:
the twelve months prior to the report (v(?12)) and
the twelve months after the report (v(+12)).
4The text and volatility data are publicly available at http:
//www.ark.cs.cmu.edu/10K.
274
Tokenization was applied to the text, including
punctuation removal, downcasing, collapsing all
digit sequences,5 and heuristic removal of remnant
markup. Table 1 gives statistics on the corpora
used in this research; this is a subset of the cor-
pus for which there is no missing volatility informa-
tion. The drastic increase in length during the 2002?
2003 period might be explained by the passage by
the US Congress of the Sarbanes-Oxley Act of 2002
(and related SEC and exchange rules), which im-
posed revised standards on reporting practices of
publicly-traded companies in the US.
5 Baselines and Evaluation Method
Volatility displays an effect known as autoregressive
conditional heteroscedasticity (Engle, 1982). This
means that the variance in a stock?s return tends to
change gradually. Large changes in price are pre-
saged by other changes, and periods of stability tend
to continue. Volatility is, generally speaking, not
constant, yet prior volatility (e.g., v(?12)) is a very
good predictor of future volatility (e.g., v(+12)). At
the granularity of a year, which we consider here
because the 10-K reports are annual, there are no
existing models of volatility that are widely agreed
to be significantly more accurate than our histor-
ical volatility baseline. We tested a state-of-the-
art model known as GARCH(1, 1) (Engle, 1982;
Bollerslev, 1986) and found that it was no stronger
than our historical volatility baseline on this sample.
Throughout this paper, we will report perfor-
mance using the mean squared error between the
predicted and true log-volatilities:6
MSE = 1N ?
N ??
i=1
(log(vi)? log(v?i))2 (6)
where N ? is the size of the test set, given in Table 1.
6 Experiments
In our experiments, we vary h (the function that
maps inputs to a vector space) and the subset of the
5While numerical information is surely informative about
risk, recall that our goal is to find indicators of risk expressed in
the text; automatic predictors of risk from numerical data would
use financial data streams directly, not text reports.
6We work in the log domain because it is standard in finance,
due to the dynamic range of actual volatilities; the distribution
over log v across companies tends to have a bell shape.
data used for training. We will always report perfor-
mance over test sets consisting of one year?s worth
of data (the subcorpora described in Table 1). In
this work, we focus on predicting the volatility over
the year following the report (v(+12)). In all experi-
ments,  = 0.1 and C is set using the default choice
of SVMlight , which is the inverse of the average of
h(d)>h(d) over the training data.7
6.1 Feature Representation
We first consider how to represent the 10-K reports.
We adopt various document representations, all us-
ing word features. Let M be the vocabulary size
derived from the training data.8 Let freq(xj ;d) de-
note the number of occurrences of the jth word in
the vocabulary in document d.
? TF: hj(d) = 1|d| freq(xj ;d), ?j ? {1, ...,M}.
? TFIDF: hj(d) = 1|d| freq(xj ;d)? log(N/|{d :
freq(xj ;d) > 0}|), where N is the number of
documents in the training set. This is the classic
?TFIDF? score.
? LOG1P: hj(d) = log(1 + freq(xj ;d)). Rather
than normalizing word frequencies as for TF,
this score dampens them with a logarithm. We
also include a variant of LOG1P where terms
are the union of unigrams and bigrams.
Note that each of these preserves sparsity; when
freq(xj ;d) = 0, hj(d) = 0 in all cases.
For interpretability of results, we use a linear ker-
nel. The usual bias weight b is included. We found
it convenient to work in the logarithmic domain for
the predicted variable, predicting log v instead of v,
since volatility is always nonnegative. In this setting,
the predicted volatility takes the form:
log v? = b+
M?
j=1
wjhj(d) (7)
Because the goal of this work is to explore how text
might be used to predict volatility, we also wish
7These values were selected after preliminary and cursory
exploration with 1996?2000 as training data and 2001 as the
test set. While the effects of  and C were not large, further
improvements may be possible with more careful tuning.
8Preliminary experiments that filtered common or rare
words showed a negligible or deleterious effect on performance.
275
features 2001 2002 2003 2004 2005 2006 micro-ave.
his
tor
y v(?12) (baseline) 0.1747 0.1600 0.1873 0.1442 0.1365 0.1463 0.1576
v(?12) (SVR with bias) 0.2433 0.4323 0.1869 0.2717 0.3184 5.6778 1.2061
v(?12) (SVR without bias) 0.2053 0.1653 0.2051 0.1337 0.1405 0.1517 0.1655
wo
rds
TF 0.2219 0.2571 0.2588 0.2134 0.1850 0.1862 0.2197
TFIDF 0.2033 0.2118 0.2178 0.1660 0.1544 0.1599 0.1842
LOG1P 0.2107 0.2214 0.2040 0.1693 0.1581 0.1715 0.1873
LOG1P, bigrams 0.1968 0.2015 ?0.1729 0.1500 0.1394 0.1532 0.1667
bo
th
TF+ 0.1885 0.1616 0.1925 ?0.1230 ?0.1272 ?0.1402 ?0.1541
TFIDF+ 0.1919 0.1618 0.1965 ?0.1246 ?0.1276 ?0.1403 ?0.1557
LOG1P+ 0.1846 0.1764 ?0.1671 ?0.1309 ?0.1319 0.1458 ?0.1542
LOG1P+, bigrams 0.1852 0.1792 ?0.1599 ?0.1352 ?0.1307 0.1448 ?0.1538
Table 2: MSE (Eq. 6) of different models on test data predictions. Lower values are better. Boldface denotes
improvements over the baseline, and ? denotes significance compared to the baseline under a permutation test (p <
0.05).
to see whether text adds information beyond what
can be predicted using historical volatility alone (the
baseline, v(?12)). We therefore consider models
augmented with an additional feature, defined as
hM+1 = log v(?12). Since this is historical informa-
tion, it is always available when the 10-K report is
published. These models are denoted TF+, TFIDF+,
and LOG1P+.
The performance of these models, compared to
the baseline from Section 5, is shown in Table 2.
We used as training examples all reports from the
five-year period preceding the test year (so six ex-
periments on six different training and test sets are
shown in the figure). We also trained SVR models
on the single feature v(?12), with and without bias
weights (b in Eq. 7); these are usually worse and
never signficantly better than the baseline.
Strikingly, the models that use only the text to
predict volatility come very close to the historical
baseline in some years. That a text-only method
(LOG1P with bigrams) for predicting future risk
comes within 5% of the error of a strong baseline
(2003?6) shows promise for the overall approach.
A combined model improves substantially over the
baseline in four out of six years (2003?6), and this
difference is usually robust to the representation
used. Table 3 shows the most strongly weighted
terms in each of the text-only LOG1P models (in-
cluding bigrams). These weights are recovered us-
ing the relationship expressed in Eq. 5.
6.2 Training Data Effects
It is well known that more training data tend to im-
prove the performance of a statistical method; how-
ever, the standard assumption is that the training
data are drawn from the same distribution as the test
data. In this work, where we seek to predict the
future based on data from past, that assumption is
obviously violated. It is therefore an open question
whether more data (i.e., looking farther into the past)
is helpful for predicting volatility, or whether it is
better to use only the most recent data.
Table 4 shows how performance varies when one,
two, or five years of historical training data are used,
averaged across test years. In most cases, using
more training data (from a longer historical period)
is helpful, but not always. One interesting trend,
not shown in the aggregate statistics of Table 4,
is that recency of the training set affected perfor-
mance much more strongly in earlier train/test splits
(2001?3) than later ones (2004?6). This experiment
leads us to conclude that temporal changes in fi-
nancial reporting make training data selection non-
trivial. Changes in the macro economy and spe-
cific businesses make older reports less relevant for
prediction. For example, regulatory changes like
Sarbanes-Oxley, variations in the business cycle,
and technological innovation like the Internet influ-
ence both the volatility and the 10-K text.
6.3 Effects of Sarbanes-Oxley
We noted earlier that the passage of the Sarbanes-
Oxley Act of 2002, which sought to reform financial
reporting, had a clear effect on the lengths of the
10-K reports in our collection. But are the reports
more informative? This question is important, be-
cause producing reports is costly; we present an em-
pirical argument based on our models that the legis-
276
19
96
?2
00
0
19
97
?2
00
1
19
98
?2
00
2
19
99
?2
00
3
20
00
?2
00
4
20
01
?2
00
5
ne
tlo
ss
0.0
26
ye
ar
#
0.0
28
los
s
0.0
23
los
s
0.0
26
los
s
0.0
25
los
s
0.0
26
hig
hv?
ye
ar
#
0.0
24
ne
tlo
ss
0.0
23
ne
tlo
ss
0.0
20
ne
tlo
ss
0.0
20
ne
tlo
ss
0.0
17
ne
tlo
ss
0.0
18
?
los
s
0.0
20
ex
pe
nse
s
0.0
20
ex
pe
nse
s
0.0
17
ex
pe
nse
s
0.0
17
ye
ar
#
0.0
16
go
ing
co
nc
ern
0.0
14
ex
pe
nse
s
0.0
19
los
s
0.0
20
ye
ar
#
0.0
15
go
ing
co
nc
ern
0.0
15
ex
pe
nse
s
0.0
15
ex
pe
nse
s
0.0
14
co
ve
na
nts
0.0
17
ex
pe
rie
nc
ed
0.0
15
ob
lig
ati
on
s
0.0
15
ye
ar
#
0.0
15
go
ing
co
nc
ern
0.0
14
ag
oin
g
0.0
14
dil
ute
d
0.0
14
of
$#
0.0
15
fin
an
cin
g
0.0
14
fin
an
cin
g
0.0
14
ag
oin
g
0.0
13
pe
rso
nn
el
0.0
13
co
nv
ert
ibl
e
0.0
14
co
ve
na
nts
0.0
15
co
nv
ert
ibl
e
0.0
14
ag
oin
g
0.0
14
ad
mi
nis
tra
tiv
e
0.0
13
fin
an
cin
g
0.0
13
da
te
0.0
14
ad
dit
ion
al
0.0
14
ad
dit
ion
al
0.0
14
ad
dit
ion
al
0.0
13
pe
rso
nn
el
0.0
13
ad
mi
nis
tra
tiv
e
0.0
12
lon
gte
rm
-0.
01
4
me
rge
ra
gre
em
en
t
-0.
01
5
un
sec
ure
d
-0.
01
2
dis
trib
uti
on
s
-0.
01
2
dis
trib
uti
on
s
-0.
01
1
po
lic
ies
-0.
01
1
rat
es
-0.
01
5
div
ide
nd
s
-0.
01
5
ear
nin
gs
-0.
01
2
an
nu
al
-0.
01
2
ins
ura
nc
e
-0.
01
1
by
the
-0.
01
1
div
ide
nd
-0.
01
5
un
sec
ure
d
-0.
01
7
dis
trib
uti
on
s
-0.
01
2
div
ide
nd
-0.
01
2
cri
tic
al
acc
ou
nti
ng
-0.
01
2
ear
nin
gs
-0.
01
1
un
sec
ure
d
-0.
01
5
div
ide
nd
-0.
01
7
div
ide
nd
s
-0.
01
5
div
ide
nd
s
-0.
01
2
low
er
int
ere
st
-0.
01
2
div
ide
nd
s
-0.
01
2
me
rge
ra
gre
em
en
t
-0.
01
7
pro
pe
rtie
s
-0.
01
8
inc
om
e
-0.
01
6
rat
es
-0.
01
3
div
ide
nd
s
-0.
01
3
un
sec
ure
d
-0.
01
2
pro
pe
rtie
s
-0.
01
8
ne
tin
co
me
-0.
01
9
pro
pe
rtie
s
-0.
01
6
pro
pe
rtie
s
-0.
01
5
pro
pe
rtie
s
-0.
01
4
pro
pe
rtie
s
-0.
01
3
inc
om
e
-0.
02
1
inc
om
e
-0.
02
1
ne
tin
co
me
-0.
01
9
rat
e
-0.
01
9
rat
e
-0.
01
7
rat
e
-0.
01
4
?
rat
e
-0.
02
2
rat
e
-0.
02
5
rat
e
-0.
02
2
ne
tin
co
me
-0.
02
3
ne
tin
co
me
-0.
02
1
ne
tin
co
me
-0.
01
8
low
v?
Ta
ble
3:
Mo
sts
tro
ng
ly-
we
igh
ted
ter
ms
in
mo
de
lsl
ear
ne
df
rom
var
iou
sti
me
pe
rio
ds
(LO
G1
Pm
od
el
wi
th
un
igr
am
sa
nd
big
ram
s).
?#
?d
en
ote
sa
ny
dig
its
eq
ue
nc
e. features 1 2 5
TF+ 0.1509 0.1450 0.1541
TFIDF+ 0.1512 0.1455 0.1557
LOG1P+ 0.1621 0.1611 0.1542
LOG1P+, bigrams 0.1617 0.1588 0.1538
Table 4: MSE of volatility predictions using reports from
varying historical windows (1, 2, and 5 years), micro-
averaged across six train/test scenarios. Boldface marks
best in a row. The historical baseline achieves 0.1576
MSE (see Table 2).
lation has actually been beneficial.
Our experimental results in Section 6.1, in which
volatility in the years 2004?2006 was more accu-
rately predicted from the text than in 2001?2002,
suggest that the Sarbanes-Oxley Act led to more in-
formative reports. We compared the learned weights
(LOG1P+, unigrams) between the six overlapping
five-year windows ending in 2000?2005; measured
in L1 distance, these were, in consecutive order,
?52.2, 59.9, 60.7, 55.3, 52.3?; the biggest differ-
ences came between 2001 and 2002 and between
2002 and 2003. (Firms are most likely to have be-
gun compliance with the new law in 2003 or 2004.)
The same pattern held when only words appearing
in all five models were considered. Variation in the
recency/training set size tradeoff (?6.2), particularly
during 2002?3, also suggests that there were sub-
stantial changes in the reports during that time.
6.4 Qualitative Evaluation
One of the advantages of a linear model is that we
can explore what each model discovers about dif-
ferent unigram and bigram terms. Some manually
selected examples of terms whose learned weights
(w) show interesting variation patterns over time are
shown in Figure 1, alongside term frequency pat-
terns, for the text-only LOG1P model (with bigrams).
These examples were suggested by experts in fi-
nance from terms with weights that were both large
and variable (across training sets).
A particularly interesting case, in light of
Sarbanes-Oxley, is the term accounting policies.
Sarbanes-Oxley mandated greater discussion of ac-
counting policy in the 10-K MD&A section. Be-
fore 2002 this term indicates high volatility, per-
haps due to complicated off-balance sheet transac-
tions or unusual accounting policies. Starting in
2002, explicit mention of accounting policies indi-
277
00.2
0.4
0.6
0.8
a
v
e
.
 
t
e
r
m
 
f
r
e
q
u
e
n
c
y
-0.015
-0.010
-0.005
0
0.005
w
accounting policies
estimates
-0.010
-0.005
0
0.005
w
reit
mortgages
-0.010
-0.005
0
0.005
0.010
96-00 97-01 98-02 99-03 00-04 01-05
w
higher margin
lower margin
0
0.05
0.10
0.15
0.20
a
v
e
.
 
t
e
r
m
 
f
r
e
q
u
e
n
c
y
0
2
4
6
8
a
v
e
.
 
t
e
r
m
 
f
r
e
q
u
e
n
c
y
Figure 1: Left:
learned weights for
selected terms across
models trained on
data from different
time periods (x-axis).
These weights are
from the LOG1P
(unigrams and
bigrams) models
trained on five-year
periods, the same
models whose
extreme weights are
summarized in
Tab. 3. Note that all
weights are within
0? 0.026. Right: the
terms? average
frequencies (by
document) over the
same periods.
cates lower volatility. The frequency of the term
also increases drastically over the same period, sug-
gesting that the earlier weights may have been in-
flated. A more striking example is estimates, which
averages one occurrence per document even in the
1996?2000 period, experiences the same term fre-
quency explosion, and goes through a similar weight
change, from strongly indicating high volatility to
strongly indicating low volatility.
As a second example, consider the terms mort-
gages and reit (Real Estate Investment Trust, a tax
designation for businesses that invest in real estate).
Given the importance of the housing and mortgage
market over the past few years, it is interesting to
note that the weight on both of these terms increases
over the period from a strong low volatility term to a
weak indicator of high volatility. It will be interest-
ing to see how the dramatic decline in housing prices
in late 2007, and the fallout created in credit markets
in 2008, is reflected in future models.
Finally, notice that high margin and low mar-
gin, whose frequency patterns are fairly flat ?switch
places,? over the sample: first indicating high and
low volatility, respectively, then low and high. There
is no a priori reason to expect high or low margins
would be associated with high or low stock volatil-
ity. However, this is an interesting example where
bigrams are helpful (the word margin by itself is
uninformative) and indicates that predicting risk is
highly time-dependent.
6.5 Delisting
An interesting but relatively infrequent phenomenon
is the delisting of a company, i.e., when it ceases to
be traded on a particular exchange due to dissolution
after bankruptcy, a merger, or violation of exchange
rules. The relationship between volatility and delist-
ing has been studied by Merton (1974), among oth-
ers. Our dataset includes a small number of cases
where the volatility figures for the period following
the publication of a 10-K report are unavailable be-
cause the company was delisted. Learning to predict
delisting is extremely difficult because fewer than
4% of the 2001?6 10-K reports precede delisting.
Using the LOG1P representation, we built a lin-
ear SVM classifier for each year in 2001?6 (trained
on the five preceding years? data) to predict whether
a company will be delisted following its 10-K re-
port. Performance for various precision measures is
shown in Table 5. Notably, for 2001?4 we achieve
278
precision (%) at ... ?01 ?02 ?03 ?04 ?05 ?06
recall = 10% 80 93 79 100 47 21
n = 5 100 100 40 100 60 80
n = 10 80 90 70 90 60 70
n = 100 38 48 53 29 24 20
oracle F1 (%) 35 42 44 36 31 16
6 bulletin, creditors, dip, otc
5 court
4 chapter, debtors, filing, prepetition
3 bankruptcy
2 concern, confirmation, going, liquidation
1 debtorinpossession, delisted, nasdaq, petition
Table 5: Left: precision of delisting predictions. The ?oracle F1? row shows the maximal F1 score obtained for any
n. Right: Words most strongly predicting delisting of a company. The number is how many of the six years (2001?6)
the word is among the ten most strongly weighted. There were no clear patterns across years for words predicting that
a company would not be delisted. The word otc refers to ?over-the-counter? trading, a high-risk market.
above 75% precision at 10% recall. Our best (or-
acle) F1 scores occur in 2002 and 2003, suggesting
again a difference in reports around Sarbanes-Oxley.
Table 5 shows words associated with delisting.
7 Related Work
In NLP, regression is not widely used, since most
natural language-related data are discrete. Regres-
sion methods were pioneered by Yang and Chute
(1992) and Yang and Chute (1993) for information
retrieval purposes, but the predicted continuous vari-
able was not an end in itself in that work. Blei
and McAuliffe (2007) used latent ?topic? variables
to predict movie reviews and popularity from text.
Lavrenko et al (2000b) and Lavrenko et al (2000a)
modeled influences between text and time series fi-
nancial data (stock prices) using language models.
Farther afield, Albrecht and Hwa (2007) used SVR
to train machine translation evaluation metrics to
match human evaluation scores and compared tech-
niques using correlation. Regression has also been
used to order sentences in extractive summarization
(Biadsy et al, 2008).
While much of the information relevant for in-
vestors is communicated through text (rather than
numbers), only recently is this link explored. Some
papers relate news articles to earning forecasts, stock
returns, volatility, and volume (Koppel and Shtrim-
berg, 2004; Tetlock, 2007; Tetlock et al, 2008; Gaa,
2007; Engelberg, 2007). Das and Chen (2001) and
Antweiler and Frank (2004) ask whether messages
posted on message boards can help explain stock
performance, while Li (2005) measures the associ-
ation between frequency of words associated with
risk and subsequent stock returns. Weiss-Hanley and
Hoberg (2008) study initial public offering disclo-
sures using word statistics. Many researchers have
focused the related problem of predicting sentiment
and opinion in text (Pang et al, 2002; Wiebe and
Riloff, 2005), sometimes connected to extrinsic val-
ues like prediction markets (Lerman et al, 2008).
In contrast to text regression, text classification
comprises a widely studied set of problems involv-
ing the prediction of categorial variables related to
text. Applications have included the categorization
of documents by topic (Joachims, 1998), language
(Cavnar and Trenkle, 1994), genre (Karlgren and
Cutting, 1994), author (Bosch and Smith, 1998),
sentiment (Pang et al, 2002), and desirability (Sa-
hami et al, 1998). Text categorization has served as
a test application for nearly every machine learning
technique for discrete classification.
8 Conclusion
We have introduced and motivated a new kind of
task for NLP: text regression, in which text is used
to make predictions about measurable phenomena
in the real world. We applied the technique to pre-
dicting financial volatility from companies? 10-K re-
ports, and found text regression model predictions
to correlate with true volatility nearly as well as his-
torical volatility, and a combined model to perform
even better. Further, improvements in accuracy and
changes in models after the passage of the Sarbanes-
Oxley Act suggest that financial reporting reform
has had interesting and measurable effects.
Acknowledgments
The authors are grateful to Jamie Callan, Chester Spatt,
Anthony Tomasic, Yiming Yang, and Stanley Zin for
helpful discussions, and to the anonymous reviewers for
useful feedback. This research was supported by grants
from the Institute for Quantitative Research in Finanace
and from the Center for Analytical Research in Technol-
ogy at the Tepper School of Business, Carnegie Mellon
University.
279
References
J. S. Albrecht and R. Hwa. 2007. Regression for
sentence-level MT evaluation with pseudo references.
In Proc. of ACL.
W. Antweiler and M. Z. Frank. 2004. Is all that talk
just noise? the information content of internet stock
message boards. Journal of Finance, 59:1259?1294.
F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An
unsupervised approach to biography production using
Wikipedia. In Proc. of ACL.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In Advances in NIPS 21.
T. Bollerslev. 1986. Generalized autoregressive con-
ditional heteroskedasticity. Journal of Econometrics,
31:307?327.
R. Bosch and J. Smith. 1998. Separating hyperplanes
and the authorship of the disputed Federalist papers.
American Mathematical Monthly, 105(7):601?608.
W. B. Cavnar and J. M. Trenkle. 1994. n-gram-based
text categorization. In Proc. of SDAIR.
S. Das and M. Chen. 2001. Yahoo for Amazon: Ex-
tracting market sentiment from stock mesage boards.
In Proc. of Asia Pacific Finance Association Annual
Conference.
H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and
V. Vapnik. 1997. Support vector regression machines.
In Advances in NIPS 9.
B. Dumas, A. Kurshev, and R. Uppal. 2007. Equilibrium
portfolio strategies in the presence of sentiment risk
and excess volatility. Swiss Finance Institute Research
Paper No. 07-37.
J. Engelberg. 2007. Costly information processing: Ev-
idence from earnings announcements. Working paper,
Northwestern University.
R. F. Engle. 1982. Autoregressive conditional het-
eroscedasticity with estimates of variance of united
kingdom inflation. Econometrica, 50:987?1008.
E. F. Fama. 1970. Efficient capital markets: A review
of theory and empirical work. Journal of Finance,
25(2):383?417.
C. Gaa. 2007. Media coverage, investor inattention, and
the market?s reaction to news. Working paper, Univer-
sity of British Columbia.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proc. of ECML.
T. Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
J. Karlgren and D. Cutting. 1994. Recognizing text gen-
res with simple metrics using discriminant analysis. In
Proc. of COLING.
M. Koppel and I. Shtrimberg. 2004. Good news or bad
news? let the market decide. In AAAI Spring Sympo-
sium on Exploring Attitude and Affect in Text: Theo-
ries and Applications.
V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000a. Language models for
financial news recommendation. In Proc. of CIKM.
V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000b. Mining of concurrent
text and time series. In Proc. of KDD.
K. Lerman, A. Gilder, M. Dredze, and F. Pereira. 2008.
Reading the markets: Forecasting public opinion of
political candidates by news analysis. In COLING.
F. Li. 2005. Do stock market investors understand the
risk sentiment of corporate annual reports? Working
Paper, University of Michigan.
R. Merton. 1974. On the pricing of corporate debt: The
risk structure of interest rates. Journal of Finance,
29:449?470.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP.
M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.
1998. A Bayesian approach to filtering junk email. In
Proc. of AAAI Workshop on Learning for Text Catego-
rization.
B. Scho?lkopf and A. J. Smola. 2002. Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization, and Beyond. MIT Press.
P. C. Tetlock, M. Saar-Tsechansky, and S. Macskassy.
2008. More than words: Quantifying language to
measure firms? fundamentals. Journal of Finance,
63(3):1437?1467.
P. C. Tetlock. 2007. Giving content to investor senti-
ment: The role of media in the stock market. Journal
of Finance, 62(3):1139?1168.
K. Weiss-Hanley and G. Hoberg. 2008. Strategic disclo-
sure and the pricing of initial public offerings. Work-
ing paper.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In CICLing.
Y. Yang and C. G. Chute. 1992. A linear least squares fit
mapping method for information retrieval from natural
language texts. In Proc. of COLING.
Y. Yang and C. G. Chute. 1993. An application of least
squares fit mapping to text information retrieval. In
Proc. of SIGIR.
280
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 477?485,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Predicting Response to Political Blog Posts with Topic Models
Tae Yano William W. Cohen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{taey,wcohen,nasmith}@cs.cmu.edu
Abstract
In this paper we model discussions in online
political blogs. To do this, we extend Latent
Dirichlet Allocation (Blei et al, 2003), in var-
ious ways to capture different characteristics
of the data. Our models jointly describe the
generation of the primary documents (posts)
as well as the authorship and, optionally, the
contents of the blog community?s verbal reac-
tions to each post (comments). We evaluate
our model on a novel comment prediction task
where the models are used to predict which
blog users will leave comments on a given
post. We also provide a qualitative discussion
about what the models discover.
1 Introduction
Web logging (blogging) and its social impact have
recently attracted considerable public and scientific
interest. One use of blogs is as a community dis-
cussion forum, especially for political discussion
and debate. Blogging has arguably opened a new
channel for huge numbers of people to express their
views with unprecedented speed and to unprece-
dented audiences. Their collective behavior in the
blogosphere has already been noted in the Ameri-
can political arena (Adamic and Glance, 2005). In
this paper we attempt to deliver a framework useful
for analyzing text in blogs quantitatively as well as
qualitatively. Better blog text analysis could lead to
better automated recommendation, organization, ex-
traction, and retrieval systems, and might facilitate
data-driven research in the social sciences.
Apart from the potential social utility of text pro-
cessing for this domain, we believe blog data is wor-
thy of scientific study in its own right. The sponta-
neous, reactive, and informal nature of the language
in this domain seems to defy conventional analytical
approaches in NLP such as supervised text classifi-
cation (Mullen and Malouf, 2006), yet the data are
rich in argumentative, topical, and temporal struc-
ture that can perhaps be modeled computationally.
We are especially interested in the semi-causal struc-
ture of blog discussions, in which a post ?spawns?
comments (or fails to do so), which meander among
topics and asides and show the personality of the
participants and the community.
Our approach is to develop probabilistic mod-
els for the generation of blog posts and comments
jointly within a blog site. The model is an extension
of Latent Dirichlet Allocation (Blei et al, 2003).
Unsupervised topic models can be applied to collec-
tions of unannotated documents, requiring very lit-
tle corpus engineering. They can be easily adapted
to new problems by altering the graphical model,
then applying standard probabilistic inference algo-
rithms. Different models can be compared to ex-
plore the ramifications of different hypotheses about
the data. For example, we will explore whether the
contents of posts a user has commented on in the
past and the words she has used can help predict
which posts she will respond to in the future.
The paper is organized as follows. In ?2 we re-
view prior work on topic modeling for document
collections and studies of social media like political
blogs. We then provide a qualitative characterization
of political blogs, highlighting some of the features
we believe a computational model should capture
and discuss our new corpus of political blogs (?3).
We present several different candidate topic models
that aim to capture these ideas in ?4. ?5 shows our
empirical evaluation on a new comment prediction
task and a qualitative analysis of the models learned.
2 Related Work
Network analysis, including citation analysis, has
been applied to document collections on the Web
(Cohn and Hofmann, 2001). Adamic and Glance
(2005) applied network analysis to the political bl-
477
ogosphere. The study modeled the large, complex
structure of the political blogosphere as a network
of hyperlinks among the blog sites, demonstrated the
viability of link structure for information discovery,
though their analysis of text content was less exten-
sive. In contrast, the text seems to be of interest
to social scientists studying blogs as an artifact of
the political process. Although attempts to quanti-
tatively analyze the contents of political texts have
been made, results from classical, supervised text
classification experiments are mixed (Mullen and
Malouf, 2006; Malouf and Mullen, 2007). Also, a
consensus on useful, reliable annotation or catego-
rization schemes for political texts, at any level of
granularity, has yet to emerge.
Meanwhile, latent topic modeling has become a
widely used unsupervised text analysis tool. The ba-
sic aim of those models is to discover recurring pat-
terns of ?topics? within a text collection. LDA was
introduced by Blei et al (2003) and has been espe-
cially popular because it can be understood as a gen-
erative model and because it discovers understand-
able topics in many scenarios (Steyvers and Grif-
fiths, 2007). Its declarative specification makes it
easy to extend for new kinds of text collections. The
technique has been applied to Web document collec-
tions, notably for community discovery in social net-
works (Zhang et al, 2007), opinion mining in user
reviews (Titov and McDonald, 2008), and sentiment
discovery in free-text annotations (Branavan et al,
2008). Dredze et al (2008) applied LDA to a collec-
tion of email for summary keyword extraction. The
authors evaluated the model with proxy tasks such as
recipient prediction. More closely related to the data
considered in this work, Lin et al (2008) applied a
variation of LDA to ideological discourse.
A notable trend in the recent research is to aug-
ment the models to describe non-textual evidence
alongside the document collection. Several such
studies are especially relevant to our work. Blei and
Jordan (2003) were one of the earliest results in this
trend. The concept was developed into more general
framework by Blei and McAuliffe (2008). Steyvers
et al (2004) and Rosen-Zvi et al (2004) first ex-
tended LDA to explicitly model the influence of au-
thorship, applying the model to a collection of aca-
demic papers from CiteSeer. The model combined
the ideas from the mixture model proposed by Mc-
Callum (1999) and LDA. In this model, an abstract
notion ?author? is associated with a distribution over
topics. Another approach to the same document col-
lection based on LDA was used for citation network
analysis. Erosheva et al (2004), following Cohn and
Hofmann (2001), defined a generative process not
only for each word in the text, but also its citation
to other documents in the collection, thereby cap-
turing the notion of relations between the document
into one generative process. Nallapati and Cohen
(2008) introduced the Link-PLSA-LDA model, in
which the contents of the citing document and the
?influences? on the document (its citations to exist-
ing literature), as well as the contents of the cited
documents, are modeled together. They further ap-
plied the Link-PLSA-LDA model to a blog corpus
to analyze its cross citation structure via hyperlinks.
In this work, we aim to model the data within blog
conversations, focusing on comments left by a blog
community in response to a blogger?s post.
3 Political Blog Data
We discuss next the dataset used in our experiments.
3.1 Corpus
We have collected blog posts and comments from
40 blog sites focusing on American politics during
the period November 2007 to October 2008, con-
temporaneous with the presidential elections. The
discussions on these blogs focus on American poli-
tics, and many themes appear: the Democratic and
Republican candidates, speculation about the results
of various state contests, and various aspects of
international and (more commonly) domestic poli-
tics. The sites were selected to have a variety of
political leanings. From this pool we chose five
blogs which accumulated a large number of posts
during this period: Carpetbagger (CB),1 Daily Kos
(DK),2 Matthew Yglesias (MY),3 Red State (RS),4
and Right Wing News (RWN).5 CB and MY ceased
as independent bloggers in August 2008.6 Because
1http://www.thecarpetbaggerreport.com
2http://www.dailykos.com
3http://matthewyglesias.theatlantic.com
4http://www.redstate.com
5http://www.rightwingnews.com
6The authors of those blogs now write for larger on-
line media, CB for Washingon Monthly at http://www.
478
MY RWN CB RS DK
Time span (from 11/11/07) ?8/2/08 ?10/10/08 ?8/25/08 ?6/26/08 ?4/9/08
# training posts 1607 1052 1080 2045 2146
# words (total) 110,788 194,948 183,635 321,699 221,820
(on average per post) (68) (185) (170) (157) (103)
# comments 56,507 34,734 34,244 59,687 425,494
(on average per post) (35) (33) (31) (29) (198)
(unique commenters, on average) (24) (13) (24) (14) (93)
# words in comments (total) 2,287,843 1,073,726 1,411,363 1,675,098 8,359,456
(on average per post) (1423) (1020) (1306) (819) (3895)
(on average per comment) (41) (31) (41) (27) (20)
Post vocabulary size 6,659 9,707 7,579 12,282 10,179
Comment vocabulary size 33,350 22,024 24,702 25,473 58,591
Size of user pool 7,341 963 5,059 2,789 16,849
# test posts 183 113 121 231 240
Table 1: Details of the blog data used in this paper.
our focus in this paper is on blog posts and their
comments, we discard posts on which no one com-
mented within six days. We also remove posts with
too few words: specifically, we retain a post only
if it has at least five words in the main entry, and
at least five words in the comment section. All
posts are represented as text only (images, hyper-
links, and other non-text contents are ignored). To
standardize the texts, we remove from the text 670
commonly used stop words, non-alphabet symbols
including punctuation marks, and strings consisting
of only symbols and digits. We also discard infre-
quent words from our dataset: for each word in a
post?s main entry, we kept it only if it appears at
least one more time in some main entry. We ap-
ply the same word pruning to the comment section
as well. The corpus size and the vocabulary size of
the five datasets are listed in Table 1. In addition,
each user?s handle is replaced with a unique inte-
ger. The dataset is available for download at http:
//www.ark.cs.cmu.edu/blog-data.
3.2 Qualitative Properties of Blogs
We believe that readers? reactions to blog posts are
an integral part of blogging activity. Often com-
ments are much more substantial and informative
than the post. While circumspective articles limit
themselves to allusions or oblique references, read-
ers? comments may point to heart of the matter more
washingtonmonthly.com and MY for Think Progress
athttp://yglesias.thinkprogress.org.
boldly. Opinions are expressed more blatantly in
comments. Comments may help a human (or au-
tomated) reader to understand the post more clearly
when the main text is too terse, stylized, or technical.
Although the main entry and its comments are
certainly related and at least partially address similar
topics, they are markedly different in several ways.
First of all, their vocabulary is noticeably different.
Comments are more casual, conversational, and full
of jargon. They are less carefully edited and there-
fore contain more misspellings and typographical er-
rors. There is more diversity among comments than
within the single-author post, both in style of writing
and in what commenters like to talk about. Depend-
ing on the subjects covered in a blog post, different
types of people are inspired to respond. We believe
that analyzing a piece of text based on the reaction
it causes among those who read it is a fascinating
problem for NLP.
Blog sites are also quite distinctive from each
other. Their language, discussion topics, and col-
lective political orientations vary greatly. Their vol-
umes also vary; multi-author sites (such as DK, RS)
may consistently produce over twenty posts per day,
while single-author sites (such asMY, CB) may have
a day with only one post. Single author sites also
tend to have a much smaller vocabulary and range
of interests. The sites are also culturally different
in commenting styles; some sites are full of short
interjections, while others have longer, more analyt-
ical comments. On some sites, users appear to be
479
??
z z?
u
?
w
?
D
N M
1
?
?
z z?
w? u
? ? ?
w
?
D
MN
1
Figure 1: Left:
LinkLDA (Erosheva
et al, 2004), with
variables reassigned.
Right:
CommentLDA. In
training, w, u, and
(in CommentLDA)
w? are observed. D is
the number of blog
posts, and N and M
are the word counts
in the post and the all
of its comments,
respectively. Here we
?count by verbosity.?
close-knit, while others have high turnover.
In the next section, we describe how we apply
topic models to political blogs, and how these prob-
abilistic models can put to use to make predictions.
4 Generative Models
The first model we consider is LinkLDA, which is
analogous to the model of Erosheva et al (2004),
though the variables are given different meanings
here.7 The graphical model is depicted in Fig. 1
(left). As in LDA and its many variants, this model
postulates a set of latent ?topic? variables, where
each topic k corresponds to a multinomial distribu-
tion ?k over the vocabulary. In addition to gener-
ating the words in the post from its topic mixture,
this model also generates a bag of users who respond
to the post, according to a distribution ? over users
given topics. In this model, the topic distribution ?
is all that determines the text content of the post and
which users will respond to the post.
LinkLDA models which users are likely to re-
spond to a post, but it does not model what they
will write. Our new model, CommentLDA, gen-
erates the contents of the comments (see Fig. 1,
right). In order to capture the differences in lan-
guage style between posts and comments, however,
we use a different conditional distribution over com-
ment words given topics, ??. The post text, comment
text, and commenter distributions are all interdepen-
dent through the (latent) topic distribution ?, and a
topic k is defined by:
7Instead of blog commenters, they modeled citations.
? A multinomial distribution ?k over post words;
? A multinomial distribution ??k over comment
words; and
? A multinomial distribution ?k over blog com-
menters who might react to posts on the topic.
Formally, LinkLDA and CommentLDA generate
blog data as follows: For each blog post (1 to D):
1. Choose a distribution ? over topics according
to Dirichlet distribution ?.
2. For i from 1 to Ni (the length of the post):
(a) Choose a topic zi according to ?.
(b) Choose a word wi according to the topic?s
post word distribution ?zi .
3. For j from 1 to Mi (the length of the comments
on the post, in words):
(a) Choose a topic z?j .
(b) Choose an author uj from the topic?s com-
menter distribution ?z?j .
(c) (CommentLDA only) Choose a word w?j
according to the topic?s comment word
distribution ??z?j .
4.1 Variations on Counting Users
As described, CommentLDA associates each com-
ment word token with an independent author. In
both LinkLDA and CommentLDA, this ?counting
by verbosity? will force ? to give higher probabil-
ity to users who write longer comments with more
480
words. We consider two alternative ways to count
comments, applicable to both LinkLDA and Com-
mentLDA. These both involve a change to step 3 in
the generative process.
Counting by response (replaces step 3): For j from
1 to Ui (the number of users who respond to the
post): (a) and (b) as before. (c) (CommentLDA only)
For ` from 1 to `i,j (the number of words in uj?s
comments), choose w?` according to the topic?s com-
ment word distribution ??z?j . This model collapses allcomments by a user into a single bag of words on a
single topic.8
Counting by comments (replaces step 3): For j
from 1 to Ci (the number of comments on the post):
(a) and (b) as before. (c) (CommentLDA only) For `
from 1 to `i,j (the number of words in comment j),
choose w?` according to the topic?s comment word
distribution ??z?j . Intuitively, each comment has atopic, a user, and a bag of words.
The three variations?counting users by ver-
bosity, response, or comments?correspond to dif-
ferent ways of thinking about topics in political blog
discourse. Counting by verbosity will let garrulous
users define the topics. Counting by response is
more democratic, letting every user who responds
to a blog post get an equal vote in determining what
the post is about, no matter how much that user says.
Counting by comments gives more say to users who
engage in the conversation repeatedly.
4.2 Implementation
We train our model using empirical Bayesian esti-
mation. Specifically, we fix ? = 0.1, and we learn
the values of word distributions ? and ?? and user
distribution ? by maximizing the likelihood of the
training data:
p(w,w?,u | ?, ?, ??, ?) (1)
(Obviously, ?? is not present in the LinkLDA mod-
els.) This requires an inference step that marginal-
izes out the latent variables, ?, z, and z?, for which
we use Gibbs sampling as implemented by the Hier-
archical Bayes Compiler (Daume?, 2007). The Gibbs
8The counting-by-response models are deficient, since they
assume each user will only be chosen once per blog post, though
they permit the same user to be chosen repeatedly.
sampling inference algorithm for LDA was first in-
troduced by Griffiths and Steyvers (2004) and has
since been used widely.
5 Empirical Evaluation
We adopt a typical NLP ?train-and-test? strategy that
learns the model parameters on a training dataset
consisting of a collection of blog posts and their
commenters and comments, then considers an un-
seen test dataset from a later time period. Many
kinds of predictions might be made about the test
set and then evaluated against the true comment re-
sponse. For example, the likelihood of a user to
comment on the post, given knowledge of ? can be
estimated as:9
p(u | wN1 , ?, ?) =
K?
z=1
p(u | z, ?)p(z | wN1 , ?)
=
K?
z=1
?z,u ? ?z (2)
The latter is in a sense a ?guessing game,? a pre-
diction on who is going to comment on a new blog
post. A similar task was used by Nallapati and Co-
hen (2008) for assessing the performance of Link-
PLSA-LDA: they predicted the presence or absence
of citation links between documents. We report the
performance on this prediction task using our six
blog topic models (LinkLDA and CommentLDA,
with three counting variations each).
Our aim is to explore and compare the effective-
ness of the different models in discovering topics
that are useful for a practical task. We also give a
qualitative analysis of topics learned.
5.1 Comment Prediction
For each political blog, we trained the three varia-
tions each of LinkLDA and CommentLDA. Model
parameters ?, ?, and (in CommentLDA) ?? were
learned by maximizing likelihood, with Gibbs sam-
pling for inference, as described in ?4.2. The num-
ber of topics K was fixed at 15.
A simple baseline method makes a post-
independent prediction that ranks users by their
comment frequency. Since blogs often have a ?core
constituency? of users who post frequently, this is a
9Another approach would attempt to integrate out ?.
481
n=5 n=10 n=20 n=30 oracle
MY
Freq. 23.93 18.68 14.20 11.65 13.18
NB 25.13 19.28 14.20 11.63 13.54
Link-v 20.10 14.04 11.17 9.23 11.32
Link-r 26.77 18.63 14.64 12.47 14.03
Link-c 25.13 18.85 14.61 11.91 13.84
Com-v 22.84 17.15 12.75 10.69 12.77
Com-r 27.54 20.54 14.61 12.45 14.35
Com-c 22.40 18.50 14.83 12.56 14.20
Max 94.75 89.89 73.63 58.76 92.60
RWN
Freq. 32.56 30.17 22.61 19.7 27.19
NB 25.63 34.86 27.61 22.03 18.28
Link-v 28.14 21.06 17.34 14.51 19.81
Link-r 32.92 29.29 22.61 18.96 26.32
Link-c 32.56 27.43 21.15 17.43 25.09
Com-v 29.02 24.07 19.07 16.04 22.71
Com-r 36.10 29.64 23.8 19.26 25.97
Com-c 32.03 27.43 19.82 16.25 23.88
Max 90.97 76.46 52.56 37.05 96.16
CB
Freq. 33.38 28.84 24.17 20.99 21.63
NB 36.36 31.15 25.08 21.40 23.22
Link-v 32.06 26.11 19.79 17.43 18.31
Link-r 37.02 31.65 24.62 20.85 22.34
Link-c 36.03 32.06 25.28 21.10 23.44
Com-v 32.39 26.36 20.95 18.26 19.85
Com-r 35.53 29.33 24.33 20.22 22.02
Com-c 33.71 29.25 23.80 19.86 21.68
Max 99.66 98.34 88.88 72.53 95.58
RS
Freq. 25.45 16.75 11.42 9.62 17.15
NB 22.07 16.01 11.60 9.76 16.50
Link-v 14.63 11.9 9.13 7.76 11.38
Link-r 25.19 16.92 12.14 9.82 17.98
Link-c 24.50 16.45 11.49 9.32 16.76
Com-v 14.97 10.51 8.46 7.37 11.3 0
Com-r 15.93 11.42 8.37 6.89 10.97
Com-c 17.57 12.46 8.85 7.34 12.14
Max 80.77 62.98 40.95 29.03 91.86
DK
Freq. 24.66 19.08 15.33 13.34 9.64
NB 35.00 27.33 22.25 19.45 13.97
Link-v 20.58 19.79 15.83 13.88 10.35
Link-r 33.83 27.29 21.39 19.09 13.44
Link-c 28.66 22.16 18.33 16.79 12.60
Com-v 22.16 18.00 16.54 14.45 10.92
Com-r 33.08 25.66 20.66 18.29 12.74
Com-c 26.08 20.91 17.47 15.59 11.82
Max 100.00 100.00 100.00 99.09 98.62
Table 2: Comment prediction results on 5 blogs. See text.
strong baseline. We also compared to a Na??ve Bayes
classifier (with word counts in the post?s main en-
try as features). To perform the prediction task with
our models, we took the following steps. First, we
removed the comment section (both the words and
the authorship information) from the test data set.
Then, we ran a Gibbs sampler with the partial data,
fixing the model parameters to their learned values
and the blog post words to their observed values.
This gives a posterior topic mixture for each post (?
in the above equations).10 We then computed each
user?s comment prediction score for each post as in
Eq. 2. Users are ordered by their posterior probabil-
ities. Note that these posteriors have different mean-
ings for different variations:
? When counting by verbosity, the value is the prob-
ability that the next (or any) comment word will
be generated by the user, given the blog post.
? When counting by response, the value is the prob-
ability that the user will respond at all, given the
blog post. (Intuitively, this approach best matches
the task at hand.)
? When counting by comments, the value is the
probability that the next (or any) comment will be
generated by the user, given the blog post.
We compare our commenter ranking-by-
likelihood with the actual commenters in the test
set. We report in Tab. 2 the precision (macro-
averaged across posts) of our predictions at various
cut-offs (n). The oracle column is the precision
where it is equal to the recall, equivalent to the
situation when the true number of commenters
is known. (The performance of random guessing
is well below 1% for all sites at cut-off points
shown.) ?Freq.? and ?NB? refer to our baseline
methods. ?Link? refers to LinkLDA and ?Com? to
CommentLDA. The suffixes denote the counting
methods: verbosity (?-v?), response (?-r?), and
comments (?-c?). Recall that we considered only
the comments by the users seen at least once in the
training set, so perfect precision, as well as recall, is
impossible when new users comment on a post; the
Max row shows the maximum performance possible
given the set of commenters recognizable from the
training data.
10For a few cases we checked the stability of the sampler and
found results varied by less than 1% precision across ten runs.
482
Our results suggest that, if asked to guess 5 peo-
ple who would comment on a new post given some
site history, we will get 25?37% of them right, de-
pending on the site, given the content of a new post.
We achieved some improvement over both the
baseline and Na??ve Bayes for some cut-offs on three
of the five sites, though the gains were very small
for and RS and CB. LinkLDA usually works slightly
better than CommentLDA, except for MY, where
CommentLDA is stronger, and RS, where Com-
mentLDA is extremely poor. Differences in com-
menting style are likely to blame: MY has relatively
long comments in comparison to RS, as well as DK.
MY is the only site where CommentLDA variations
consistently outperformed LinkLDA variations, as
well as Na??ve Bayes classifiers. This suggests that
sites with more terse comments may be too sparse
to support a rich model like CommentLDA.
In general, counting by response works best,
though counting by comments is a close rival in
some cases. We observe that counting by response
tends to help LinkLDA, which is ignorant of the
word contents of the comment, more than it helps
CommentLDA. Varying the counting method can
bring as much as 10% performance gain.
Each of the models we have tested makes differ-
ent assumptions about the behavior of commenters.
Our results suggest that commenters on different
sites behave differently, so that the same modeling
assumptions cannot be made universally. In future
work, we hope to permit blog-specific properties
to be automatically discovered during learning, so
that, for example, the comment words can be ex-
ploited when they are helpful but assumed indepen-
dent when they are not. Of course, improved per-
formance might also be obtained with more topics,
richer priors over topic distributions, or models that
take into account other cues, such as the time of the
post, pages it links to, etc. It is also possible that bet-
ter performance will come from more sophisticated
supervised models that do not use topics.
5.2 Qualitative Evaluation
Aside from prediction tasks such as above, the
model parameters by themselves can be informative.
? defines which words are likely to occur in the post
body for a given topic. ?? tells which words are
likely to appear in the collective response to a partic-
ular topic. Similarity or divergence of the two dis-
tributions can tell us about differences in language
used by bloggers and their readers. ? expresses
users? topic preferences. A pair or group of par-
ticipants may be seen as ?like-minded? if they have
similar topic preferences (perhaps useful in collabo-
rative filtering).
Following previous work on LDA and its exten-
sions, we show words most strongly associated with
a few topics, arguing that some coherent clusters
have been discovered. Table 3 shows topics discov-
ered in MY using CommentLDA (counting by com-
ments). This is the blog site where our models most
consistently outperformed the Na??ve Bayes classi-
fiers and LinkLDA, therefore we believe the model
was a good fit for this dataset.
Since the site is concentrated on American pol-
itics, many of the topics look alike. Table 3 shows
the most probable words in the posts, comments, and
both together for five hand-picked topics that were
relatively transparent. The probabilistic scores of
those words are computed with the scoring method
suggested by Blei and Lafferty (in press).
The model clustered words into topics pertain-
ing to religion and domestic policy (first and last
topics in Table 3) quite reasonably. Some of the
religion-related words make sense in light of cur-
rent affairs.11 Some words in the comment sec-
tion are slightly off-topic from the issue of religion,
such as dawkins12 or wright,13 but are relevant in
the context of real-world events. Notice those words
rank highly only in the comment section, showing
differences between discussion in the post and the
comments. This is also noticeable, for example, in
the ?primary? topic (second in Table 3), where the
Republican primary receives more discussion in the
main post, and in the ?Iraq war? and ?energy? top-
ics, where bloggers discuss strategy and commenters
11Mitt Romney was a candidate for the Republican nomi-
nation in 2008 presidential election. He is a member of The
Church of Jesus Christ of Latter-Day Saints. Another candi-
date, Mike Huckabee, is an ordained Southern Baptist minister.
Moktada al-Sadr is an Iraqi theologian and political activist, and
John Hagee is an influential televangelist.
12Richard Dawkins is a well known evolutionary biologist
who is a vocal critic of intelligent design.
13We believe this is a reference to Rev. Jeremiah Wright of
Trinity United Church of Christ, whose inflammatory rhetoric
was negatively associated with then-candidate Barack Obama.
483
religion; in both: people, just, american, church, believe, god, black, jesus, mormon, faith, jews, right, say,
mormons, religious, point
in posts: romney, huckabee, muslim, political, hagee, cabinet, mitt, consider, true, anti, problem,
course, views, life, real, speech, moral, answer, jobs, difference, muslims, hardly, going,
christianity
in comments: religion, think, know, really, christian, obama, white, wright, way, said, good, world, science,
time, dawkins, human, man, things, fact, years, mean, atheists, blacks, christians
primary; in both: obama, clinton, mccain, race, win, iowa, delegates, going, people, state, nomination, primary,
hillary, election, polls, party, states, voters, campaign, michigan, just
in posts: huckabee, wins, romney, got, percent, lead, barack, point, majority, ohio, big, victory, strong,
pretty, winning, support, primaries, south, rules
in comments: vote, think, superdelegates, democratic, candidate, pledged, delegate, independents, votes,
white, democrats, really, way, caucuses, edwards, florida, supporters, wisconsin, count
Iraq war; in
both:
american, iran, just, iraq, people, support, point, country, nuclear, world, power, military,
really, government, war, army, right, iraqi, think
in posts: kind, united, forces, international, presence, political, states, foreign, countries, role, need,
making, course, problem, shiite, john, understand, level, idea, security, main
in comments: israel, sadr, bush, state, way, oil, years, time, going, good, weapons, saddam, know, maliki,
want, say, policy, fact, said, shia, troops
energy; in both: people, just, tax, carbon, think, high, transit, need, live, going, want, problem, way, market,
money, income, cost, density
in posts: idea, public, pretty, course, economic, plan, making, climate, spending, economy, reduce,
change, increase, policy, things, stimulus, cuts, low, fi nancial, housing, bad, real
in comments: taxes, fuel, years, time, rail, oil, cars, car, energy, good, really, lot, point, better, prices, pay,
city, know, government, price, work, technology
domestic policy;
in both:
people, public, health, care, insurance, college, schools, education, higher, children, think,
poor, really, just, kids, want, school, going, better
in posts: different, things, point, fact, social, work, large, article, getting, inequality, matt, simply,
percent, tend, hard, increase, huge, costs, course, policy, happen
in comments: students, universal, high, good, way, income, money, government, class, problem, pay, amer-
icans, private, plan, american, country, immigrants, time, know, taxes, cost
Table 3: The most probable words for some CommentLDA topics (MY).
focus on the tangible (oil, taxes, prices, weapons).
While our topic-modeling approach achieves
mixed results on the prediction task, we believe it
holds promise as a way to understand and summa-
rize the data. Without CommentLDA, we would not
be able to easily see the differences noted above in
blogger and commenter language. In future work,
we plan to explore models with weaker indepen-
dence assumptions among users, among blog posts
over time, and even across blogs. This line of re-
search will permit a more nuanced understanding
of language in the blogosphere and in political dis-
course more generally.
6 Conclusion
In this paper we applied several probabilistic topic
models to discourse within political blogs. We in-
troduced a novel comment prediction task to assess
these models in an objective evaluation with possi-
ble practical applications. The results show that pre-
dicting political discourse behavior is challenging,
in part because of considerable variation in user be-
havior across different blog sites. Our results show
that using topic modeling, we can begin to make rea-
sonable predictions as well as qualitative discoveries
about language in blogs.
Acknowledgments
This research was supported by a gift from Microsoft
Research and NSF IIS-0836431. The authors appreciate
helpful comments from the anonymous reviewers, Ja-Hui
Chang, Hal Daume?, and Ramesh Nallapati. We thank
Shay Cohen for his help with inference algorithms and
the members of the ARK group for reviewing this paper.
484
References
L. Adamic and N. Glance. 2005. The political blogo-
sphere and the 2004 U.S. election: Divided they blog.
In Proceedings of the 2nd Annual Workshop on the We-
blogging Ecosystem: Aggregation, Analysis and Dy-
namics.
D. Blei and M. Jordan. 2003. Modeling annotated data.
In Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in
Informaion Retrieval.
D. Blei and J. Lafferty. In press. Topic models. In A. Sri-
vastava and M. Sahami, editors, Text Mining: Theory
and Applications. Taylor and Franci.
D. Blei and J. McAuliffe. 2008. Supervised topic mod-
els. In Advances in Neural Information Processing
Systems 20.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
S. R. K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic prop-
erties from free-text annotations. In Proceedings of
ACL-08: HLT.
D. Cohn and T. Hofmann. 2001. The missing link?a
probabilistic model of document content and hyper-
text connectivity. In Neural Information Processing
Systems 13.
H. Daume?. 2007. HBC: Hierarchical Bayes compiler.
http://www.cs.utah.edu/?hal/HBC.
M. Dredze, H. M. Wallach, D. Puller, and F. Pereira.
2008. Generating summary keywords for emails us-
ing topics. In Proceedings of the 13th International
Conference on Intelligent User Interfaces.
E. Erosheva, S. Fienberg, and J. Lafferty. 2004. Mixed
membership models of scientific publications. Pro-
ceedings of the National Academy of Sciences, pages
5220?5227, April.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101 Suppl. 1:5228?5235, April.
W.-H. Lin, E. Xing, and A. Hauptmann. 2008. A joint
topic and perspective model for ideological discourse.
In Proceedings of 2008 European Conference on Ma-
chine Learning and Principles and Practice of Knowl-
edge Discovery in Databases.
R. Malouf and T. Mullen. 2007. Graph-based user clas-
sification for informal online political discourse. In
Proceedings of the 1st Workshop on Information Cred-
ibility on the Web.
A. McCallum. 1999. Multi-label text classification with
a mixture model trained by EM. In AAAI Workshop on
Text Learning.
T. Mullen and R. Malouf. 2006. A preliminary investi-
gation into sentiment analysis of informal political dis-
course. In Proceedings of AAAI-2006 Spring Sympo-
sium on Computational Approaches to Analyzing We-
blogs.
R. Nallapati and W. Cohen. 2008. Link-PLSA-LDA: A
new unsupervised model for topics and influence of
blogs. In Proceedings of the 2nd International Con-
ference on Weblogs and Social Media.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P Smyth.
2004. The author-topic model for authors and docu-
ments. In Proceedings of the 20th Conference on Un-
certainty in Artificial Intelligence.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis,
and W. Kintsch, editors, Handbook of Latent Semantic
Analysis. Lawrence Erlbaum Associates.
M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Grif-
fiths. 2004. Probabilistic author-topic models for in-
formation discovery. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining.
I. Titov and R. McDonald. 2008. A joint model of text
and aspect ratings for sentiment summarization. In
Proceedings of ACL-08: HLT.
H. Zhang, B. Qiu, C. L. Giles, H. C. Foley, and J. Yen.
2007. An LDA-based community structure discovery
approach for large-scale social networks. In Proceed-
ings of the IEEE International Conference on Intelli-
gence and Security Informatics.
485
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 752?759,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Computationally Efficient M-Estimation of Log-Linear Structure Models?
Noah A. Smith and Douglas L. Vail and John D. Lafferty
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{nasmith,dvail2,lafferty}@cs.cmu.edu
Abstract
We describe a new loss function, due to Jeon
and Lin (2006), for estimating structured
log-linear models on arbitrary features. The
loss function can be seen as a (generative) al-
ternative to maximum likelihood estimation
with an interesting information-theoretic in-
terpretation, and it is statistically consis-
tent. It is substantially faster than maximum
(conditional) likelihood estimation of condi-
tional random fields (Lafferty et al, 2001;
an order of magnitude or more). We com-
pare its performance and training time to an
HMM, a CRF, an MEMM, and pseudolike-
lihood on a shallow parsing task. These ex-
periments help tease apart the contributions
of rich features and discriminative training,
which are shown to be more than additive.
1 Introduction
Log-linear models are a very popular tool in natural
language processing, and are often lauded for per-
mitting the use of ?arbitrary? and ?correlated? fea-
tures of the data by a model. Users of log-linear
models know, however, that this claim requires some
qualification: any feature is permitted in principle,
but training log-linear models (and decoding under
them) is tractable only when the model?s indepen-
dence assumptions permit efficient inference proce-
dures. For example, in the original conditional ran-
dom fields (Lafferty et al, 2001), features were con-
?This work was supported by NSF grant IIS-0427206 and
the DARPA CALO project. The authors are grateful for feed-
back from David Smith and from three anonymous ACL re-
viewers, and helpful discussions with Charles Sutton.
fined to locally-factored indicators on label bigrams
and label unigrams (with any of the observation).
Even in cases where inference in log-linear mod-
els is tractable, it requires the computation of a parti-
tion function. More formally, a log-linear model for
random variables X and Y over X,Y defines:
pw(x, y) =
ew
>f(x,y)
?
x?,y??X?Y e
w>f(x?,y?)
=
ew
>f(x,y)
Z(w)
(1)
where f : X?Y? Rm is the feature vector-function
and w ? Rm is a weight vector that parameterizes
the model. In NLP, we rarely train this model by
maximizing likelihood, because the partition func-
tion Z(w) is expensive to compute exactly. Z(w)
can be approximated (e.g., using Gibbs sampling;
Rosenfeld, 1997).
In this paper, we propose the use of a new loss
function that is computationally efficient and statis-
tically consistent (?2). Notably, repeated inference
is not required during estimation. This loss func-
tion can be seen as a case of M-estimation1 that
was originally developed by Jeon and Lin (2006) for
nonparametric density estimation. This paper gives
an information-theoretic motivation that helps eluci-
date the objective function (?3), shows how to ap-
ply the new estimator to structured models used in
NLP (?4), and compares it to a state-of-the-art noun
phrase chunker (?5). We discuss implications and
future directions in ?6.
2 Loss Function
As before, let X be a random variable over a high-
dimensional space X, and similarly Y over Y. X
1?M-estimation? is a generalization of MLE (van der Vaart,
1998); space does not permit a full discussion.
752
might be the set of all sentences in a language, and
Y the set of all POS tag sequences or the set of all
parse trees. Let q0 be a ?base? distribution that is
our first approximation to the true distribution over
X ? Y. HMMs and PCFGs, while less accurate as
predictors than the rich-featured log-linear models
we desire, might be used to define q0.
The model we estimate will have the form
pw(x, y) ? q0(x, y)e
w>f(x,y) (2)
Notice that pw(x, y) = 0 whenever q0(x, y) = 0.
It is therefore important for q0 to be smooth, since
the support of pw is a subset of the support of q0.
Notice that we have not written the partition function
explicitly in Eq. 2; it will never need to be computed
during estimation or inference. The unnormalized
distribution will suffice for all computation.
Suppose we have observations ?x1, x2, ..., xn?
with annotations ?y1, ..., yn?. The (unregularized)
loss function, due to Jeon and Lin (2006), is2
`(w) =
1
n
n?
i=1
e?w
>f(xi,yi)
+
?
x,y
q0(x, y)
(
w>f(x, y)
)
(3)
=
1
n
n?
i=1
e?w
>f(xi,yi) + w>
?
x,y
q0(x, y)f(x, y)
=
1
n
n?
i=1
e?w
>f(xi,yi) + w> Eq0(X,Y )[f(X,Y )]
? ?? ?
constant(w)
Before explaining this objective, we point out
some attractive computational properties. Notice
that f(xi, yi) (for all i) and the expectations of the
feature vectors under q0 are constant with respect
to w. Computing the function in Eq. 3, then, re-
quires no inference and no dynamic programming,
only O(nm) floating-point operations.
3 An Interpretation
Here we give an account of the loss function as a
way of ?cleaning up? a mediocre model (q0). We
2We give only the discrete version here, because it is most
relevant for an ACL audience. Also, our linear function
w>f(xi, yi) is a simple case; another kernel (for example)
could be used.
show that this estimate aims to model a presumed
perturbation that created q0, by minimizing the KL
divergence between q0 and a perturbed version of the
sample distribution p?.
Consider Eq. 2. Given a training dataset, maxi-
mizing likelihood under this model means assuming
that there is some w? for which the true distribu-
tion p?(x, y) = pw?(x, y). Carrying out MLE, how-
ever, would require computing the partition function
?
x?,y? q0(x
?, y?)ew
>f(x?,y?), which is in general in-
tractable. Rearranging Eq. 2 slightly, we have
q0(x, y) ? p
?(x, y)e?w
>f(x,y) (4)
If q0 is close to the true model, e?w
>f(x,y) should
be close to 1 and w close to zero. In the sequence
model setting, for example, if q0 is an HMM that ex-
plains the data well, then the additional features are
not necessary (equivalently, their weights should be
0). If q0 is imperfect, we might wish to make it more
powerful by adding features (e.g., f ), but q0 nonethe-
less provides a reasonable ?starting point? for defin-
ing our model.
So instead of maximizing likelihood, we will min-
imize the KL divergence between the two sides of
Eq. 4.3
DKL(q0(x, y)?p
?(x, y)e?w
>f(x,y)) (5)
=
?
x,y
q0(x, y) log
q0(x, y)
p?(x, y)e?w>f(x,y)
(6)
+
?
x,y
p?(x, y)e?w
>f(x,y) ?
?
x,y
q0(x, y)
= ?H(q0) +
?
x,y
p?(x, y)e?w
>f(x,y) ? 1
?
?
x,y
q0(x, y) log
(
p?(x, y)e?w
>f(x,y)
)
= constant(w) +
?
x,y
p?(x, y)e?w
>f(x,y)
+
?
x,y
q0(x, y)
(
w>f(x, y)
)
3The KL divergence here is generalized for unnormalized
distributions, following O?Sullivan (1998):
DKL(u?v) =
P
j
?
uj log
uj
vj
? uj + vj
?
where u and v are nonnegative vectors defining unnormal-
ized distributions over the same event space. Note that whenP
j uj =
P
j vj = 1, this formula takes on the more familiar
form, as ?
P
j uj and
P
j vj cancel.
753
If we replace p? with the empirical (sampled) dis-
tribution p?, minimizing the above KL divergence is
equivalent to minimizing `(w) (Eq. 3). It may be
helpful to think of?w as the parameters of a process
that ?damage? the true model p?, producing q0, and
the estimation ofw as learning to undo that damage.
In the remainder of the paper, we use the general
term ?M-estimation? to refer to the minimization of
`(w) as a way of training a log-linear model.
4 Algorithms for Models of Sequences and
Trees
We discuss here some implementation aspects of the
application of M-estimation to NLP models.
4.1 Expectations under q0
The base distribution q0 enters into implementation
in two places: Eq0(X,Y )[f(X,Y )] must be computed
for training, and q0(x, y) is a factor in the model
used in decoding.
If q0 is a familiar stochastic grammar, such as an
HMM or a PCFG, or any generative model from
which sampling is straightforward, it is possible to
estimate the feature expectations by sampling from
the model directly; for sample ?(x?i, y?i)?si=1 let:
Eq0(X,Y )[fj(X,Y )]?
1
s
s?
i=1
fj(x?i, y?i) (7)
If the feature space is sparse under q0 (likely in most
settings), then smoothing may be required.
If q0 is an HMM or a PCFG, the expectation vec-
tor can be computed exactly by solving a system of
equations. We will see that for the common cases
where features are local substructures, inference is
straightforward. We briefly describe how this can be
done for a bigram HMM and a PCFG.
4.1.1 Expectations under an HMM
Let S be the state space of a first-order HMM.
If s = ?s1, ..., sk? is a state sequence and x =
?x1, ..., xk? is an observed sequence of emissions,
then:
q0(s,x) =
(
k?
i=1
tsi?1(si)esi(xi)
)
tsk(stop) (8)
(Assume s0 = start is the single, silent, initial state,
and stop is the only stop state, also silent. We as-
sume no other states are silent.)
The first step is to compute path-sums into and out
of each state, under the HMM q0. To do this, define
is as the total weight of state-prefixes (beginning in
start) ending in s and os as the total weight of state-
suffixes beginning in s (and ending in stop):4
istart = ostop = 1 (9)
?s ? S \ {start, stop} :
is =
??
n=1
?
?s1,...,sn??Sn
(
n?
i=1
tsi?1(si)
)
tsn(s)
=
?
s??S
is?ts?(s) (10)
os =
??
n=1
?
?s1,...,sn??Sn
ts(s1)
(
n?
i=2
tsi?1(si)
)
=
?
s??S
ts(s
?)os? (11)
This amounts to two linear systems given the tran-
sition probabilities t, where the variables are i? and
o?, respectively. In each system there are |S| vari-
ables and |S| equations. Once solved, expected
counts of transition and emission features under q0
are straightforward:
Eq0 [s
transit
? s?] = ists(s
?)os?
Eq0 [s
emit
? x] = ises(x)os
Given i and o, Eq0 can be computed for other fea-
tures in the model in a similar way, provided they
correspond to contiguous substructures. For exam-
ple, a feature f627 that counts occurrences of ?Si =
s and Xi+3 = x? has expected value Eq0 [f627] =
?
s?,s??,s????S
ists(s
?)ts?(s
??)ts??(s
???)es???(x)os??? (12)
Non-contiguous substructure features with ?gaps?
require summing over paths between any pair of
states. This is straightforward (we omit it for space),
but of course using such features (while interesting)
would complicate inference in decoding.
4It may be helpful to think of i as forward probabilities, but
for the observation set Y? rather than a particular observation
y. o are like backward probabilities. Note that, because some
counted prefixes are prefixes of others, i can be > 1; similarly
for o.
754
4.1.2 Expectations under a PCFG
In general, the expectations for a PCFG require
solving a quadratic system of equations. The anal-
ogy this time is to inside and outside probabilities.
Let the PCFG have nonterminal set N, start symbol
S ? N, terminal alphabet ?, and rules of the form
A ? B C and A ? x. (We assume Chomsky nor-
mal form for clarity; the generalization is straight-
forward.) Let rA(B C) and rA(x) denote the proba-
bilities of nonterminal A rewriting to child sequence
B C or x, respectively. Then ?A ? N:
oA =
?
B?N
?
C?N
oBiC [rB(A C) + rB(C A)]
+
{
1 if A = S
0 otherwise
iA =
?
B?N
?
C?N
rA(B C)iBiC +
?
x
rA(x)ix
ox =
?
A?N
oArA(x),?x ? ?
ix = 1,?x ? ?
In most practical applications, the PCFG will be
?tight? (Booth and Thompson, 1973; Chi and Ge-
man, 1998). Informally, this means that the proba-
bility of a derivation rooted in S failing to terminate
is zero. If that is the case, then iA = 1 for allA ? N,
and the system becomes linear (see also Corazza
and Satta, 2006).5 If tightness is not guaranteed,
iterative propagation of weights, following Stolcke
(1995), works well in our experience for solving the
quadratic system, and converges quickly.
As in the HMM case, expected counts of arbitrary
contiguous tree substructures can be computed as
products of probabilities of rules appearing within
the structure, factoring in the o value of the struc-
ture?s root and the i values of the structure?s leaves.
4.2 Optimization
To carry out M-estimation, we minimize the func-
tion `(w) in Eq. 3. To apply gradient de-
scent or a quasi-Newton numerical optimization
method,6 it suffices to specify the fixed quantities
5The same is true for HMMs: if the probability of non-
termination is zero, then for all s ? S, os = 1.
6We use L-BFGS (Liu and Nocedal, 1989) as implemented
in the R language?s optim function.
f(xi, yi) (for all i ? {1, 2, ..., n}) and the vector
Eq0(X,Y )[f(X,Y )]. The gradient is:
7
?`
?wj
= ?
n?
i=1
e?w
>f(xi,yi)fj(xi, yi) + Eq0 [fj ]
(13)
The Hessian (matrix of second derivatives) can also
be computed with relative ease, though the space re-
quirement could become prohibitive. For problems
wherem is relatively small, this would allow the use
of second-order optimization methods that are likely
to converge in fewer iterations.
It is easy to see that Eq. 3 is convex in w. There-
fore, convergence to a global optimum is guaranteed
and does not depend on the initializing value of w.
4.3 Regularization
Regularization is a technique from pattern recogni-
tion that aims to keep parameters (likew) from over-
fitting the training data. It is crucial to the perfor-
mance of most statistical learning algorithms, and
our experiments show it has a major effect on the
success of the M-estimator. Here we use a quadratic
regularizer, minimizing `(w) + (w>w)/2c. Note
that this is also convex and differentiable if c > 0.
The value of c can be chosen using a tuning dataset.
This regularizer aims to keep each coordinate of w
close to zero.
In the M-estimator, regularization is particularly
important when the expectation of some feature fj ,
Eq0(X,Y )[fj(X,Y )] is equal to zero. This can hap-
pen either due to sampling error (fj simply failed
to appear with a positive value in the finite sample)
or because q0 assigns zero probability mass to any
x ? X, y ? Y where fj(x, y) 6= 0. Without regular-
ization, the weight wj will tend toward ??, but the
quadratic penalty term will prevent that undesirable
tendency. Just as the addition of a quadratic regular-
izer to likelihood can be interpreted as a zero-mean
Gaussian prior on w (Chen and Rosenfeld, 2000), it
can be so-interpreted here. The regularized objective
is analogous to maximum a posteriori estimation.
5 Shallow Parsing
We compared M-estimation to a hidden Markov
model and other training methods on English noun
7Taking the limit as n ? ? and setting equal to zero, we
have the basis for a proof that `(w) is statistically consistent.
755
HMM CRF MEMM PL M-est.2 sec. 64:18 3:40 9:35 1:04
Figure 1: Wall time (hours:minutes) of training the
HMM and 100 L-BFGS iterations for each of the
extended-feature models on a 2.2 GHz Sun Opteron
with 8GB RAM. See discussion in text for details.
phrase (NP) chunking. The dataset comes from
the Conference on Natural Language Learning
(CoNLL) 2000 shallow parsing shared task (Tjong
Kim Sang and Buchholz, 2000); we apply the model
to NP chunking only. About 900 sentences were re-
served for tuning regularization parameters.
Baseline/q0 In this experiment, the simple base-
line is a second-order HMM. The states correspond
to {B, I,O} labels, denoting the beginning, inside,
and outside of noun phrases. Each state emits a
tag and a word (independent of each other given the
state). We replaced the first occurrence of every tag
and of every word in the training data with an OOV
symbol, giving a fixed tag vocabulary of 46 and a
fixed word vocabulary of 9,014. Transition distribu-
tions were estimated using MLE, and tag- and word-
emission distributions were estimated using add-1
smoothing. The HMM had 27,213 parameters. This
HMM achieves 86.3% F1-measure on the develop-
ment dataset (slightly better than the lowest-scoring
of the CoNLL-2000 systems). Heavier or weaker
smoothing (an order of magnitude difference in add-
?) of the emission distributions had very little effect.
Note that HMM training time is negligible (roughly
2 seconds); it requires counting events, smoothing
the counts, and normalizing.
Extended Feature Set Sha and Pereira (2003) ap-
plied a conditional random field to the NP chunk-
ing task, achieving excellent results. To improve the
performance of the HMM and test different estima-
tion methods, we use Sha and Pereira?s feature tem-
plates, which include subsequences of labels, tags,
and words of different lengths and offsets. Here,
we use only features observed to occur at least once
in the training data, accounting (in addition to our
OOV treatment) for the slight drop in performance
prec. recall F1
HMM features:
HMM 85.60 88.68 87.11
CRF 90.40 89.56 89.98
PL 80.31 81.37 80.84
MEMM 86.03 88.62 87.31
M-est. 85.57 88.65 87.08
extended features:
CRF 94.04 93.68 93.86
PL 91.88 91.79 91.83
MEMM 90.89 92.15 91.51
M-est. 88.88 90.42 89.64
Table 1: NP chunking accuracy on test data us-
ing different training methods. The effects of dis-
criminative training (CRF) and extended feature sets
(lower section) are more than additive.
compared to what Sha and Pereira report. There are
630,862 such features.
Using the original HMM feature set and the ex-
tended feature set, we trained four models that can
use arbitrary features: conditional random fields
(a near-replication of Sha and Pereira, 2003), maxi-
mum entropy Markov models (MEMMs; McCal-
lum et al, 2000), pseudolikelihood (Besag, 1975;
see Toutanova et al, 2003, for a tagging applica-
tion), and our M-estimator with the HMM as q0.
CRFs and MEMMs are discriminatively-trained to
maximize conditional likelihood (the former is pa-
rameterized using a sequence-normalized log-linear
model, the latter using a locally-normalized log-
linear model). Pseudolikelihood is a consistent esti-
mator for the joint likelihood, like our M-estimator;
its objective function is a sum of log probabilities.
In each case, we trained seven models for
each feature set with quadratic regularizers c ?
[10?1, 10], spaced at equal intervals in the log-scale,
plus an unregularized model (c =?). As discussed
in ?4.2, we trained using L-BFGS; training contin-
ued until relative improvement fell within machine
precision or 100 iterations, whichever came first.
After training, the value of c is chosen that maxi-
mizes F1 accuracy on the tuning set.
Runtime Fig. 1 compares the wall time of
carefully-timed training runs on a dedicated server.
Note that Dyna, a high-level programming language,
was used for dynamic programming (in the CRF)
756
and summations (MEMM and pseudolikelihood).
The runtime overhead incurred by using Dyna is es-
timated as a slow-down factor of 3?5 against a hand-
tuned implementation (Eisner et al, 2005), though
the slow-down factor is almost certainly less for the
MEMM and pseudolikelihood. All training (except
the HMM, of course) was done using the R language
implementation of L-BFGS. In our implementation,
the M-estimator trained substantially faster than the
other methods. Of the 64 minutes required to train
the M-estimator, 6 minutes were spent precomput-
ing Eq0(X,Y )[f(X,Y )] (this need not be repeated if
the regularization settings are altered).
Accuracy Tab. 1 shows how NP chunking accu-
racy compares among the models. With HMM
features, the M-estimator is about the same as the
HMM and MEMM (better than PL and worse than
the CRF). With extended features, the M-estimator
lags behind the slower methods, but performs about
the same as the HMM-featured CRF (2.5?3 points
over the HMM). The full-featured CRF improves
performance by another 4 points. Performance as
a function of training set size is plotted in Fig. 2;
the different methods behave relatively similarly as
the training data are reduced. Fig. 3 plots accuracy
(on tuning data) against training time, for a vari-
ety of training dataset sizes and regularizaton set-
tings, under different training methods. This illus-
trates the training-time/accuracy tradeoff: the M-
estimator, when well-regularized, is considerably
faster than the other methods, at the expense of ac-
curacy. This experiment gives some insight into the
relative importance of extended features versus es-
timation methods. The M-estimated model is, like
the maximum likelihood-estimated HMM, a gener-
ative model. Unlike the HMM, it uses a much larger
set of features?the same features that the discrimina-
tive models use. Our result supports the claim that
good features are necessary for state-of-the-art per-
formance, but so is good training.
5.1 Effect of the Base Distribution
We now turn to the question of the base distribution
q0: how accurate does it need to be? Given that the
M-estimator is consistent, it should be clear that, in
the limit and assuming that our model family p is
correct, q0 should not matter (except in its support).
q0 selection prec. recall F1
HMM F1, prec. 88.88 90.42 89.64
l.u. F1 72.91 57.56 64.33
prec. 84.40 37.68 52.10
emp. F1 84.38 89.43 86.83
Table 2: NP chunking accuracy on test data using
different base models for the M-estimator. The ?se-
lection? column shows which accuracy measure was
optimized when selecting the hyperparameter c.
In NLP, we deal with finite datasets and imperfect
models, so q0 may have practical importance.
We next consider an alternative q0 that is far less
powerful; in fact, it is uninformative about the vari-
able to be predicted. Let x be a sequence of words,
t be a sequence of part-of-speech tags, and y be a
sequence of {B, I,O}-labels. The model is:
ql.u.0 (x, t,y)
def
=
?
?
|x|?
i=1
puni(xi)puni(ti)
1
Nyi?1
?
?
1
Ny|x|
(14)
where Ny is the number of labels (including stop)
that can follow y (3 for O and y0 = start, 4 for
B and I). puni are the tag and word unigram distri-
butions, estimated using MLE with add-1 smooth-
ing. This model ignores temporal effects. On its
own, this model achieves 0% precision and recall,
because it labels every word O (the most likely label
sequence is O|x|). We call this model l.u. (?locally
uniform?).
Tab. 2 shows that, while an M-estimate that uses
ql.u.0 is not nearly as accurate as the one based on
an HMM, the M-estimator did manage to improve
considerably over ql.u.0 . So the M-estimator is far
better than nothing, and in this case, tuning c to
maximize precision (rather than F1) led to an M-
estimated model with precision competitive with the
HMM. We point this out because, in applications in-
volving very large corpora, a model with good preci-
sion may be useful even if its coverage is mediocre.
Another question about q0 is whether it should
take into account all possible values of the input
variables (here, x and t), or only those seen in train-
ing. Consider the following model:
qemp0 (x, t,y)
def
= q0(y | x, t)p?(x, t) (15)
Here we use the empirical distribution over tag/word
757
707580
859095
100
0 2000 4000 6000 8000 10000training set size
F 1 CRFPLMEMMM-est.HMM
Figure 2: Learning curves for different estimators;
all of these estimators except the HMM use the ex-
tended feature set.
657075808590
95100
0 1 10 100 1000 10000 100000 1000000training time (seconds)F 1
M-est. CRFHMM PLMEMM
Figure 3: Accuracy (tuning data) vs. training time.
The M-estimator trains notably faster. The points
in a given curve correspond to different regulariza-
tion strengths (c); M-estimation is more damaged by
weak than strong regularization.
sequences, and the HMM to define the distri-
bution over label sequences. The expectations
Eqemp0 (X)[f(X)] can be computed using dynamic
programming over the training data (recall that this
only needs to be done once, cf. the CRF). Strictly
speaking, qemp0 assigns probability zero to any se-
quence not seen in training, but we can ignore the
p? marginal at decoding time. As shown in Tab. 2,
this model slightly improves recall over the HMM,
but damages precision; the gains of M-estimation
seen with the HMM as q0, are not reproduced. From
these experiments, we conclude that theM-estimator
might perform considerably better, given a better q0.
5.2 Input-Only Features
We present briefly one negative result. Noting that
the M-estimator is a modeling technique that esti-
mates a distribution over both input and output vari-
ables (i.e., a generative model), we wanted a way
to make the objective more discriminative while still
maintaining the computational property that infer-
ence (of any kind) not be required during the inner
loop of iterative training.
The idea is to reduce the predictive burden on
the feature weights for f . When designing a CRF,
features that do not depend on the output variable
(here, y) are unnecessary. They cannot distinguish
between competing labelings for an input, and so
their weights will be set to zero during conditional
estimation. The feature vector function in Sha and
Pereira?s chunking model does not include such
features. In M-estimation, however, adding such
?input-only? features might permit better modeling
of the data and, more importantly, use the origi-
nal features primarily for the discriminative task of
modeling y given the input.
Adding unigram, bigram, and trigram features
to f for M-estimation resulted in a very small de-
crease in performance: selecting for F1, this model
achieves 89.33 F1 on test data.
6 Discussion
M-estimation fills a gap in the plethora of train-
ing techniques that are available for NLP mod-
els today: it permits arbitrary features (like so-
called conditional ?maximum entropy? models such
as CRFs) but estimates a generative model (permit-
ting, among other things, classification on input vari-
ables and meaningful combination with other mod-
els). It is similar in spirit to pseudolikelihood (Be-
sag, 1975), to which it compares favorably on train-
ing runtime and unfavorably on accuracy.
Further, since no inference is required during
training, any features really are permitted, so long
as their expected values can be estimated under the
base model q0. Indeed, M-estimation is consider-
ably easier to implement than conditional estima-
tion. Both require feature counts from the train-
ing data; M-estimation replaces repeated calculation
and differentiation of normalizing constants with in-
ference or sampling (once) under a base model. So
758
the M-estimator is much faster to train.
Generative and discriminative models have been
compared and discussed a great deal (Ng and Jordan,
2002), including for NLP models (Johnson, 2001;
Klein and Manning, 2002). Sutton and McCallum
(2005) present approximate methods that keep a dis-
criminative objective while avoiding full inference.
We see M-estimation as a particularly promising
method in settings where performance depends on
high-dimensional, highly-correlated feature spaces,
where the desired features ?large,? making discrimi-
native training too time-consuming?a compelling
example is machine translation. Further, in some
settings a locally-normalized conditional log-linear
model (like an MEMM) may be difficult to design;
our estimator avoids normalization altogether.8 The
M-estimator may also be useful as a tool in design-
ing and selecting feature combinations, since more
trials can be run in less time. After selecting a fea-
ture set under M-estimation, discriminative training
can be applied on that set. The M-estimator might
also serve as an initializer to discriminative mod-
els, perhaps reducing the number of times inference
must be performed?this could be particularly use-
ful in very-large data scenarios. In future work we
hope to explore the use of the M-estimator within
hidden variable learning, such as the Expectation-
Maximization algorithm (Dempster et al, 1977).
7 Conclusions
We have presented a new loss function for genera-
tively estimating the parameters of log-linear mod-
els. The M-estimator is fast to train, requiring
no repeated, expensive calculation of normalization
terms. It was shown to improve performance on
a shallow parsing task over a baseline (generative)
HMM, but it is not competitive with the state-of-
the-art. Our sequence modeling experiments support
the widely accepted claim that discriminative, rich-
feature modeling works as well as it does not just
because of rich features in the model, but also be-
cause of discriminative training. Our technique fills
an important gap in the spectrum of learning meth-
ods for NLP models and shows promise for applica-
tion when discriminative methods are too expensive.
8Note that MEMMs also require local partition functions?
which may be expensive?to be computed at decoding time.
References
J. E. Besag. 1975. Statistical analysis of non-lattice data. The
Statistician, 24:179?195.
T. L. Booth and R. A. Thompson. 1973. Applying probabil-
ity measures to abstract languages. IEEE Transactions on
Computers, 22(5):442?450.
S. Chen and R. Rosenfeld. 2000. A survey of smoothing tech-
niques for ME models. IEEE Transactions on Speech and
Audio Processing, 8(1):37?50.
Z. Chi and S. Geman. 1998. Estimation of probabilis-
tic context-free grammars. Computational Linguistics,
24(2):299?305.
A. Corazza and G. Satta. 2006. Cross-entropy and estimation
of probabilistic context-free grammars. In Proc. of HLT-
NAACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likeli-
hood estimation from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compiling
Comp Ling: Practical weighted dynamic programming and
the Dyna language. In Proc. of HLT-EMNLP.
Y. Jeon and Y. Lin. 2006. An effective method for high-
dimensional log-density ANOVA estimation, with applica-
tion to nonparametric graphical model building. Statistical
Sinica, 16:353?374.
M. Johnson. 2001. Joint and conditional estimation of tagging
and parsing models. In Proc. of ACL.
D. Klein and C. D. Manning. 2002. Conditional structure vs.
conditional estimation in NLP models. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS
method for large scale optimization. Math. Programming,
45:503?528.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy Markov models for information extraction and seg-
mentation. In Proc. of ICML.
A. Ng and M. Jordan. 2002. On discriminative vs. generative
classifiers: A comparison of logistic regression and na??ve
Bayes. In NIPS 14.
J. A. O?Sullivan. 1998. Alternating minimization algo-
rithms: from Blahut-Armijo to Expectation-Maximization.
In A. Vardy, editor, Codes, Curves, and Signals: Common
Threads in Communications, pages 173?192. Kluwer.
R. Rosenfeld. 1997. A whole sentence maximum entropy lan-
guage model. In Proc. of ASRU.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of HLT-NAACL.
A. Stolcke. 1995. An efficient probabilistic context-free pars-
ing algorithm that computes prefix probabilities. Computa-
tional Linguistics, 21(2):165?201.
C. Sutton and A. McCallum. 2005. Piecewise training of undi-
rected models. In Proc. of UAI.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction
to the CoNLL-2000 shared task: Chunking. In Proc. of
CoNLL.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In Proc. of HLT-NAACL.
A. W. van der Vaart. 1998. Asymptotic Statistics. Cambridge
University Press.
759
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342?350,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Concise Integer Linear Programming Formulations
for Dependency Parsing
Andre? F. T. Martins?? Noah A. Smith? Eric P. Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith,epxing}@cs.cmu.edu
Abstract
We formulate the problem of non-
projective dependency parsing as a
polynomial-sized integer linear pro-
gram. Our formulation is able to handle
non-local output features in an efficient
manner; not only is it compatible with
prior knowledge encoded as hard con-
straints, it can also learn soft constraints
from data. In particular, our model is able
to learn correlations among neighboring
arcs (siblings and grandparents), word
valency, and tendencies toward nearly-
projective parses. The model parameters
are learned in a max-margin framework
by employing a linear programming
relaxation. We evaluate the performance
of our parser on data in several natural
languages, achieving improvements over
existing state-of-the-art methods.
1 Introduction
Much attention has recently been devoted to in-
teger linear programming (ILP) formulations of
NLP problems, with interesting results in appli-
cations like semantic role labeling (Roth and Yih,
2005; Punyakanok et al, 2004), dependency pars-
ing (Riedel and Clarke, 2006), word alignment
for machine translation (Lacoste-Julien et al,
2006), summarization (Clarke and Lapata, 2008),
and coreference resolution (Denis and Baldridge,
2007), among others. In general, the rationale for
the development of ILP formulations is to incorpo-
rate non-local features or global constraints, which
are often difficult to handle with traditional algo-
rithms. ILP formulations focus more on the mod-
eling of problems, rather than algorithm design.
While solving an ILP is NP-hard in general, fast
solvers are available today that make it a practical
solution for many NLP problems.
This paper presents new, concise ILP formu-
lations for projective and non-projective depen-
dency parsing. We believe that our formula-
tions can pave the way for efficient exploitation of
global features and constraints in parsing applica-
tions, leading to more powerful models. Riedel
and Clarke (2006) cast dependency parsing as
an ILP, but efficient formulations remain an open
problem. Our formulations offer the following
comparative advantages:
? The numbers of variables and constraints are
polynomial in the sentence length, as opposed to
requiring exponentially many constraints, elim-
inating the need for incremental procedures like
the cutting-plane algorithm;
? LP relaxations permit fast online discriminative
training of the constrained model;
? Soft constraints may be automatically learned
from data. In particular, our formulations han-
dle higher-order arc interactions (like siblings
and grandparents), model word valency, and can
learn to favor nearly-projective parses.
We evaluate the performance of the new parsers
on standard parsing tasks in seven languages. The
techniques that we present are also compatible
with scenarios where expert knowledge is avail-
able, for example in the form of hard or soft first-
order logic constraints (Richardson and Domin-
gos, 2006; Chang et al, 2008).
2 Dependency Parsing
2.1 Preliminaries
A dependency tree is a lightweight syntactic repre-
sentation that attempts to capture functional rela-
tionships between words. Lately, this formalism
has been used as an alternative to phrase-based
parsing for a variety of tasks, ranging from ma-
chine translation (Ding and Palmer, 2005) to rela-
tion extraction (Culotta and Sorensen, 2004) and
question answering (Wang et al, 2007).
Let us first describe formally the set of legal de-
pendency parse trees. Consider a sentence x =
342
?w0, . . . , wn?, where wi denotes the word at the i-
th position, and w0 = $ is a wall symbol. We form
the (complete1) directed graph D = ?V,A?, with
vertices in V = {0, . . . , n} (the i-th vertex corre-
sponding to the i-th word) and arcs in A = V 2.
Using terminology from graph theory, we say that
B ? A is an r-arborescence2 of the directed
graph D if ?V,B? is a (directed) tree rooted at r.
We define the set of legal dependency parse trees
of x (denoted Y(x)) as the set of 0-arborescences
of D, i.e., we admit each arborescence as a poten-
tial dependency tree.
Let y ? Y(x) be a legal dependency tree for
x; if the arc a = ?i, j? ? y, we refer to i as the
parent of j (denoted i = pi(j)) and j as a child of
i. We also say that a is projective (in the sense of
Kahane et al, 1998) if any vertex k in the span of
a is reachable from i (in other words, if for any k
satisfying min(i, j) < k < max(i, j), there is a
directed path in y from i to k). A dependency tree
is called projective if it only contains projective
arcs. Fig. 1 illustrates this concept.3
The formulation to be introduced in ?3 makes
use of the notion of the incidence vector associ-
ated with a dependency tree y ? Y(x). This is
the binary vector z , ?za?a?A with each compo-
nent defined as za = I(a ? y) (here, I(.) denotes
the indicator function). Considering simultane-
ously all incidence vectors of legal dependency
trees and taking the convex hull, we obtain a poly-
hedron that we call the arborescence polytope,
denoted by Z(x). Each vertex of Z(x) can be
identified with a dependency tree in Y(x). The
Minkowski-Weyl theorem (Rockafellar, 1970) en-
sures that Z(x) has a representation of the form
Z(x) = {z ? R|A| | Az ? b}, for some p-by-|A|
matrix A and some vector b in Rp. However, it is
not easy to obtain a compact representation (where
p grows polynomially with the number of words
n). In ?3, we will provide a compact represen-
tation of an outer polytope Z?(x) ? Z(x) whose
integer vertices correspond to dependency trees.
Hence, the problem of finding the dependency tree
that maximizes some linear function of the inci-
1The general case where A ? V 2 is also of interest; it
arises whenever a constraint or a lexicon forbids some arcs
from appearing in dependency tree. It may also arise as a
consequence of a first-stage pruning step where some candi-
date arcs are eliminated; this will be further discussed in ?4.
2Or ?directed spanning tree with designated root r.?
3In this paper, we consider unlabeled dependency parsing,
where only the backbone structure (i.e., the arcs without the
labels depicted in Fig. 1) is to be predicted.
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$ Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and eighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and P reira, 2006). How-
ever, i the data-dr ven parsing etting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our curr nt
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known t have exact n n-projective
implementations.
We th switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related W rk
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorit ms (Yamad and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). I the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, a
is the case for edge-factored mod ls (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A projective dependency parse (top), and a non-
projective dependency parse (bottom) for two English sen-
tences; examples from McDonald and Satta (2007).
dence vectors can be cast as an ILP. A similar idea
was applied to word alignment by Lacoste-Julien
et al (2006), where permutations (rather than ar-
borescences) were the combinatorial structure be-
ing requiring representation.
Letting X denote the set of possible sentences,
define Y ,
?
x?X Y(x). Given a labeled dataset
L , ??x1, y1?, . . . , ?xm, ym?? ? (X ? Y)m, we
aim to learn a parser, i.e., a function h : X ? Y
that given x ? X ou puts a legal dependency parse
y ? Y(x). T e f ct that t er e xponentially
ma y candidates in Y(x) makes dependency pars-
ing a structured classification problem.
2.2 Arc Factorizat o and L c lity
There has been much recent work on dependency
parsing using graph-based, transition-based, and
hybrid methods; see Nivre and McDonald (2008)
for an overview. Typical graph-based methods
consider linear classifiers of the fo m
hw(x) = argmaxy?Y w
>f(x, y), (1)
where f(x, y) is a vector of features and w is the
corresponding weight vector. One wants hw to
have small expected loss; the ty ical loss func-
tion is th Hamming loss, `(y?; y) , |{?i, j? ?
y? : ?i, j? /? y}|. Tractability is usually ensured
by strong factorization assumptions, like the one
underlying the arc-factored model (Eisner, 1996;
McDonald et al, 2005), which forbids any feature
that depends on two or more arcs. This induces a
decomposition of the feature vector f(x, y) as:
f(x, y) =
?
a?y fa(x). (2)
Under this decomposition, each arc receives a
score; parsing amounts to choosing the configu-
ration that maximizes the overall score, which, as
343
shown by McDonald et al (2005), is an instance
of the maximal arborescence problem. Combi-
natorial algorithms (Chu and Liu, 1965; Edmonds,
1967) can solve this problem in cubic time.4 If
the dependency parse trees are restricted to be
projective, cubic-time algorithms are available via
dynamic programming (Eisner, 1996). While in
the projective case, the arc-factored assumption
can be weakened in certain ways while maintain-
ing polynomial parser runtime (Eisner and Satta,
1999), the same does not happen in the nonprojec-
tive case, where finding the highest-scoring tree
becomes NP-hard (McDonald and Satta, 2007).
Approximate algorithms have been employed to
handle models that are not arc-factored (although
features are still fairly local): McDonald and
Pereira (2006) adopted an approximation based
on O(n3) projective parsing followed by a hill-
climbing algorithm to rearrange arcs, and Smith
and Eisner (2008) proposed an algorithm based on
loopy belief propagation.
3 Dependency Parsing as an ILP
Our approach will build a graph-based parser
without the drawback of a restriction to local fea-
tures. By formulating inference as an ILP, non-
local features can be easily accommodated in our
model; furthermore, by using a relaxation tech-
nique we can still make learning tractable. The im-
pact of LP-relaxed inference in the learning prob-
lem was studied elsewhere (Martins et al, 2009).
A linear program (LP) is an optimization prob-
lem of the form
minx?Rd c
>x
s.t. Ax ? b.
(3)
If the problem is feasible, the optimum is attained
at a vertex of the polyhedron that defines the con-
straint space. If we add the constraint x ? Zd, then
the above is called an integer linear program
(ILP). For some special parameter settings?e.g.,
when b is an integer vector and A is totally uni-
modular5?all vertices of the constraining polyhe-
dron are integer points; in these cases, the integer
constraint may be suppressed and (3) is guaran-
teed to have integer solutions (Schrijver, 2003).
Of course, this need not happen: solving a gen-
eral ILP is an NP-complete problem. Despite this
4There is also a quadratic algorithm due to Tarjan (1977).
5A matrix is called totally unimodular if the determinants
of each square submatrix belong to {0, 1,?1}.
fact, fast solvers are available today that make this
a practical solution for many problems. Their per-
formance depends on the dimensions and degree
of sparsity of the constraint matrix A.
Riedel and Clarke (2006) proposed an ILP for-
mulation for dependency parsing which refines
the arc-factored model by imposing linguistically
motivated ?hard? constraints that forbid some arc
configurations. Their formulation includes an ex-
ponential number of constraints?one for each
possible cycle. Since it is intractable to throw
in all constraints at once, they propose a cutting-
plane algorithm, where the cycle constraints are
only invoked when violated by the current solu-
tion. The resulting algorithm is still slow, and an
arc-factored model is used as a surrogate during
training (i.e., the hard constraints are only used at
test time), which implies a discrepancy between
the model that is optimized and the one that is ac-
tually going to be used.
Here, we propose ILP formulations that elim-
inate the need for cycle constraints; in fact, they
require only a polynomial number of constraints.
Not only does our model allow expert knowledge
to be injected in the form of constraints, it is also
capable of learning soft versions of those con-
straints from data; indeed, it can handle features
that are not arc-factored (correlating, for exam-
ple, siblings and grandparents, modeling valency,
or preferring nearly projective parses). While, as
pointed out by McDonald and Satta (2007), the
inclusion of these features makes inference NP-
hard, by relaxing the integer constraints we obtain
approximate algorithms that are very efficient and
competitive with state-of-the-art methods. In this
paper, we focus on unlabeled dependency parsing,
for clarity of exposition. If it is extended to labeled
parsing (a straightforward extension), our formu-
lation fully subsumes that of Riedel and Clarke
(2006), since it allows using the same hard con-
straints and features while keeping the ILP poly-
nomial in size.
3.1 The Arborescence Polytope
We start by describing our constraint space. Our
formulations rely on a concise polyhedral repre-
sentation of the set of candidate dependency parse
trees, as sketched in ?2.1. This will be accom-
plished by drawing an analogy with a network
flow problem.
Let D = ?V,A? be the complete directed graph
344
associated with a sentence x ? X , as stated in
?2. A subgraph y = ?V,B? is a legal dependency
tree (i.e., y ? Y(x)) if and only if the following
conditions are met:
1. Each vertex in V \ {0} must have exactly one
incoming arc in B,
2. 0 has no incoming arcs in B,
3. B does not contain cycles.
For each vertex v ? V , let ??(v) , {?i, j? ?
A | j = v} denote its set of incoming arcs, and
?+(v) , {?i, j? ? A | i = v} denote its set of
outgoing arcs. The two first conditions can be eas-
ily expressed by linear constraints on the incidence
vector z:
?
a???(j) za = 1, j ? V \ {0} (4)
?
a???(0) za = 0 (5)
Condition 3 is somewhat harder to express. Rather
than adding exponentially many constraints, one
for each potential cycle (like Riedel and Clarke,
2006), we equivalently replace condition 3 by
3?. B is connected.
Note that conditions 1-2-3 are equivalent to 1-2-
3?, in the sense that both define the same set Y(x).
However, as we will see, the latter set of condi-
tions is more convenient. Connectedness of graphs
can be imposed via flow constraints (by requir-
ing that, for any v ? V \ {0}, there is a directed
path in B connecting 0 to v). We adapt the single
commodity flow formulation for the (undirected)
minimum spanning tree problem, due to Magnanti
and Wolsey (1994), that requires O(n2) variables
and constraints. Under this model, the root node
must send one unit of flow to every other node.
By making use of extra variables, ? , ??a?a?A,
to denote the flow of commodities through each
arc, we are led to the following constraints in ad-
dition to Eqs. 4?5 (we denote U , [0, 1], and
B , {0, 1} = U ? Z):
? Root sends flow n:
?
a??+(0) ?a = n (6)
? Each node consumes one unit of flow:
?
a???(j)
?a ?
?
a??+(j)
?a = 1, j ? V \ {0} (7)
? Flow is zero on disabled arcs:
?a ? nza, a ? A (8)
? Each arc indicator lies in the unit interval:
za ? U, a ? A. (9)
These constraints project an outer bound of the ar-
borescence polytope, i.e.,
Z?(x) , {z ? R|A| | (z,?) satisfy (4?9)}
? Z(x). (10)
Furthermore, the integer points of Z?(x) are pre-
cisely the incidence vectors of dependency trees
in Y(x); these are obtained by replacing Eq. 9 by
za ? B, a ? A. (11)
3.2 Arc-Factored Model
Given our polyhedral representation of (an outer
bound of) the arborescence polytope, we can
now formulate dependency parsing with an arc-
factored model as an ILP. By storing the arc-
local feature vectors into the columns of a matrix
F(x) , [fa(x)]a?A, and defining the score vec-
tor s , F(x)>w (each entry is an arc score) the
inference problem can be written as
max
y?Y(x)
w>f(x, y) = max
z?Z(x)
w>F(x)z
= max
z,?
s>z
s.t. A
[
z
?
]
? b
z ? B
(12)
whereA is a sparse constraint matrix (withO(|A|)
non-zero elements), and b is the constraint vec-
tor; A and b encode the constraints (4?9). This
is an ILP with O(|A|) variables and constraints
(hence, quadratic in n); if we drop the integer
constraint the problem becomes the LP relaxation.
As is, this formulation is no more attractive than
solving the problem with the existing combinato-
rial algorithms discussed in ?2.2; however, we can
now start adding non-local features to build a more
powerful model.
3.3 Sibling and Grandparent Features
To cope with higher-order features of the form
fa1,...,aK (x) (i.e., features whose values depend on
the simultaneous inclusion of arcs a1, . . . , aK on
345
a candidate dependency tree), we employ a lin-
earization trick (Boros and Hammer, 2002), defin-
ing extra variables za1...aK , za1 ? . . .?zaK . This
logical relation can be expressed by the following
O(K) agreement constraints:6
za1...aK ? zai , i = 1, . . . ,K
za1...aK ?
?K
i=1 zai ?K + 1. (13)
As shown by McDonald and Pereira (2006) and
Carreras (2007), the inclusion of features that
correlate sibling and grandparent arcs may be
highly beneficial, even if doing so requires resort-
ing to approximate algorithms.7 Define Rsibl ,
{?i, j, k? | ?i, j? ? A, ?i, k? ? A} and Rgrand ,
{?i, j, k? | ?i, j? ? A, ?j, k? ? A}. To include
such features in our formulation, we need to add
extra variables zsibl , ?zr?r?Rsibl and z
grand ,
?zr?r?Rgrand that indicate the presence of sibling
and grandparent arcs. Observe that these indica-
tor variables are conjunctions of arc indicator vari-
ables, i.e., zsiblijk = zij ? zik and z
grand
ijk = zij ? zjk.
Hence, these features can be handled in our formu-
lation by adding the following O(|A| ? |V |) vari-
ables and constraints:
zsiblijk ? zij , z
sibl
ijk ? zik, z
sibl
ijk ? zij + zik ? 1
(14)
for all triples ?i, j, k? ? Rsibl, and
zgrandijk ? zij , z
grand
ijk ? zjk, z
grand
ijk ? zij+zjk?1
(15)
for all triples ?i, j, k? ? Rgrand. Let R , A ?
Rsibl ? Rgrand; by redefining z , ?zr?r?R and
F(x) , [fr(x)]r?R, we may express our inference
problem as in Eq. 12, with O(|A| ? |V |) variables
and constraints.
Notice that the strategy just described to han-
dle sibling features is not fully compatible with
the features proposed by Eisner (1996) for pro-
jective parsing, as the latter correlate only con-
secutive siblings and are also able to place spe-
cial features on the first child of a given word.
The ability to handle such ?ordered? features is
intimately associated with Eisner?s dynamic pro-
gramming parsing algorithm and with the Marko-
vian assumptions made explicitly by his genera-
tive model. We next show how similar features
6Actually, any logical condition can be encoded with lin-
ear constraints involving binary variables; see e.g. Clarke and
Lapata (2008) for an overview.
7By sibling features we mean features that depend on
pairs of sibling arcs (i.e., of the form ?i, j? and ?i, k?); by
grandparent features we mean features that depend on pairs
of grandparent arcs (of the form ?i, j? and ?j, k?).
can be incorporated in our model by adding ?dy-
namic? constraints to our ILP. Define:
znext siblijk ,
?
??
??
1 if ?i, j? and ?i, k? are
consecutive siblings,
0 otherwise,
zfirst childij ,
{
1 if j is the first child of i,
0 otherwise.
Suppose (without loss of generality) that i < j <
k ? n. We could naively compose the constraints
(14) with additional linear constraints that encode
the logical relation
znext siblijk = z
sibl
ijk ?
?
j<l<k ?zil,
but this would yield a constraint matrix with
O(n4) non-zero elements. Instead, we define aux-
iliary variables ?jk and ?ij :
?jk =
{
1, if ?l s.t. pi(l) = pi(j) < j < l < k
0, otherwise,
?ij =
{
1, if ?k s.t. i < k < j and ?i, k? ? y
0, otherwise.
(16)
Then, we have that znext siblijk = z
sibl
ijk ? (??jk) and
zfirst childij = zij?(??ij), which can be encoded via
znext siblijk ? z
sibl
ijk z
first child
ij ? zij
znext siblijk ? 1 ? ?jk z
first child
ij ? 1 ? ?ij
znext siblijk ? z
sibl
ijk ? ?jk z
first child
ij ? zij ? ?ij
The following ?dynamic? constraints encode the
logical relations for the auxiliary variables (16):
?j(j+1) = 0 ?i(i+1) = 0
?j(k+1) ? ?jk ?i(j+1) ? ?ij
?j(k+1) ?
?
i<j
zsiblijk ?i(j+1) ? zij
?j(k+1) ? ?jk +
?
i<j
zsiblijk ?i(j+1) ? ?ij + zij
Auxiliary variables and constraints are defined
analogously for the case n ? i > j > k. This
results in a sparser constraint matrix, with only
O(n3) non-zero elements.
3.4 Valency Features
A crucial fact about dependency grammars is that
words have preferences about the number and ar-
rangement of arguments and modifiers they ac-
cept. Therefore, it is desirable to include features
346
that indicate, for a candidate arborescence, how
many outgoing arcs depart from each vertex; de-
note these quantities by vi ,
?
a??+(i) za, for
each i ? V . We call vi the valency of the ith ver-
tex. We add valency indicators zvalik , I(vi = k)
for i ? V and k = 0, . . . , n? 1. This way, we are
able to penalize candidate dependency trees that
assign unusual valencies to some of their vertices,
by specifying a individual cost for each possible
value of valency. The following O(|V |2) con-
straints encode the agreement between valency in-
dicators and the other variables:
?n?1
k=0 kz
val
ik =
?
a??+(i) za, i ? V (17)
?n?1
k=0 z
val
ik = 1, i ? V
zvalik ? 0, i ? V, k ? {0, . . . , n? 1}
3.5 Projectivity Features
For most languages, dependency parse trees tend
to be nearly projective (cf. Buchholz and Marsi,
2006). We wish to make our model capable of
learning to prefer ?nearly? projective parses when-
ever that behavior is observed in the data.
The multicommodity directed flow model of
Magnanti and Wolsey (1994) is a refinement of the
model described in ?3.1 which offers a compact
and elegant way to indicate nonprojective arcs, re-
quiring O(n3) variables and constraints. In this
model, every node k 6= 0 defines a commodity:
one unit of commodity k originates at the root
node and must be delivered to node k; the vari-
able ?kij denotes the flow of commodity k in arc
?i, j?. We first replace (4?9) by (18?22):
? The root sends one unit of commodity to each
node:
?
a???(0)
?ka ?
?
a??+(0)
?ka = ?1, k ? V \ {0} (18)
? Any node consumes its own commodity and no
other:
?
a???(j)
?ka ?
?
a??+(j)
?ka = ?
k
j , j, k ? V \ {0} (19)
where ?kj , I(j = k) is the Kronecker delta.
? Disabled arcs do not carry any flow:
?ka ? za, a ? A, k ? V (20)
? There are exactly n enabled arcs:
?
a?A za = n (21)
? All variables lie in the unit interval:
za ? U, ?ka ? U, a ? A, k ? V (22)
We next define auxiliary variables ?jk that indi-
cate if there is a path from j to k. Since each ver-
tex except the root has only one incoming arc, the
following linear equalities are enough to describe
these new variables:
?jk =
?
a???(j) ?
k
a, j, k ? V \ {0}
?0k = 1, k ? V \ {0}. (23)
Now, define indicators znp , ?znpa ?a?A, where
znpa , I(a ? y and a is nonprojective).
From the definition of projective arcs in ?2.1, we
have that znpa = 1 if and only if the arc is active
(za = 1) and there is some vertex k in the span of
a = ?i, j? such that ?ik = 0. We are led to the
following O(|A| ? |V |) constraints for ?i, j? ? A:
znpij ? zij
znpij ? zij ? ?ik, min(i, j) ? k ? max(i, j)
znpij ? ?
?max(i,j)?1
k=min(i,j)+1 ?ik + |j ? i| ? 1
There are other ways to introduce nonprojectiv-
ity indicators and alternative definitions of ?non-
projective arc.? For example, by using dynamic
constraints of the same kind as those in ?3.3,
we can indicate arcs that ?cross? other arcs with
O(n3) variables and constraints, and a cubic num-
ber of non-zero elements in the constraint matrix
(omitted for space).
3.6 Projective Parsing
It would be straightforward to adapt the con-
straints in ?3.5 to allow only projective parse trees:
simply force znpa = 0 for any a ? A. But there are
more efficient ways of accomplish this. While it is
difficult to impose projectivity constraints or cycle
constraints individually, there is a simpler way of
imposing both. Consider 3 (or 3?) from ?3.1.
Proposition 1 Replace condition 3 (or 3?) with
3??. If ?i, j? ? B, then, for any k = 1, . . . , n
such that k 6= j, the parent of k must satisfy
(defining i? , min(i, j) and j? , max(i, j)):
?
??
??
i? ? pi(k) ? j?, if i? < k < j?,
pi(k) < i? ? pi(k) > j?, if k < i? or k > j?
or k = i.
347
Then, Y(x) will be redefined as the set of projec-
tive dependency parse trees.
We omit the proof for space. Conditions 1, 2, and
3?? can be encoded with O(n2) constraints.
4 Experiments
We report experiments on seven languages, six
(Danish, Dutch, Portuguese, Slovene, Swedish
and Turkish) from the CoNLL-X shared task
(Buchholz and Marsi, 2006), and one (English)
from the CoNLL-2008 shared task (Surdeanu et
al., 2008).8 All experiments are evaluated using
the unlabeled attachment score (UAS), using the
default settings.9 We used the same arc-factored
features as McDonald et al (2005) (included in the
MSTParser toolkit10); for the higher-order models
described in ?3.3?3.5, we employed simple higher
order features that look at the word, part-of-speech
tag, and (if available) morphological information
of the words being correlated through the indica-
tor variables. For scalability (and noting that some
of the models require O(|V | ? |A|) constraints and
variables, which, when A = V 2, grows cubically
with the number of words), we first prune the base
graph by running a simple algorithm that ranks the
k-best candidate parents for each word in the sen-
tence (we set k = 10); this reduces the number of
candidate arcs to |A| = kn.11 This strategy is sim-
ilar to the one employed by Carreras et al (2008)
to prune the search space of the actual parser. The
ranker is a local model trained using a max-margin
criterion; it is arc-factored and not subject to any
structural constraints, so it is very fast.
The actual parser was trained via the online
structured passive-aggressive algorithm of Cram-
mer et al (2006); it differs from the 1-best MIRA
algorithm of McDonald et al (2005) by solv-
ing a sequence of loss-augmented inference prob-
lems.12 The number of iterations was set to 10.
The results are summarized in Table 1; for the
sake of comparison, we reproduced three strong
8We used the provided train/test splits except for English,
for which we tested on the development partition. For train-
ing, sentences longer than 80 words were discarded. For test-
ing, all sentences were kept (the longest one has length 118).
9
http://nextens.uvt.nl/?conll/software.html
10
http://sourceforge.net/projects/mstparser
11Note that, unlike reranking approaches, there are still ex-
ponentially many candidate parse trees after pruning. The
oracle constrained to pick parents from these lists achieves
> 98% in every case.
12The loss-augmented inference problem can also be ex-
pressed as an LP for Hamming loss functions that factor over
arcs; we refer to Martins et al (2009) for further details.
baselines, all of them state-of-the-art parsers based
on non-arc-factored models: the second order
model of McDonald and Pereira (2006), the hy-
brid model of Nivre and McDonald (2008), which
combines a (labeled) transition-based and a graph-
based parser, and a refinement of the latter, due
to Martins et al (2008), which attempts to ap-
proximate non-local features.13 We did not repro-
duce the model of Riedel and Clarke (2006) since
the latter is tailored for labeled dependency pars-
ing; however, experiments reported in that paper
for Dutch (and extended to other languages in the
CoNLL-X task) suggest that their model performs
worse than our three baselines.
By looking at the middle four columns, we can
see that adding non-arc-factored features makes
the models more accurate, for all languages. With
the exception of Portuguese, the best results are
achieved with the full set of features. We can
also observe that, for some languages, the valency
features do not seem to help. Merely modeling
the number of dependents of a word may not be
as valuable as knowing what kinds of dependents
they are (for example, distinguishing among argu-
ments and adjuncts).
Comparing with the baselines, we observe that
our full model outperforms that of McDonald and
Pereira (2006), and is in line with the most ac-
curate dependency parsers (Nivre and McDonald,
2008; Martins et al, 2008), obtained by com-
bining transition-based and graph-based parsers.14
Notice that our model, compared with these hy-
brid parsers, has the advantage of not requiring an
ensemble configuration (eliminating, for example,
the need to tune two parsers). Unlike the ensem-
bles, it directly handles non-local output features
by optimizing a single global objective. Perhaps
more importantly, it makes it possible to exploit
expert knowledge through the form of hard global
constraints. Although not pursued here, the same
kind of constraints employed by Riedel and Clarke
(2006) can straightforwardly fit into our model,
after extending it to perform labeled dependency
parsing. We believe that a careful design of fea-
13Unlike our model, the hybrid models used here as base-
lines make use of the dependency labels at training time; in-
deed, the transition-based parser is trained to predict a la-
beled dependency parse tree, and the graph-based parser use
these predicted labels as input features. Our model ignores
this information at training time; therefore, this comparison
is slightly unfair to us.
14See also Zhang and Clark (2008) for a different approach
that combines transition-based and graph-based methods.
348
[M
P0
6]
[N
M0
8]
[M
DS
X0
8]
AR
C-F
AC
TO
RE
D
+S
IB
L/G
RA
ND
P.
+V
AL
EN
CY
+P
RO
J. (
FU
LL
)
FU
LL
, R
EL
AX
ED
DANISH 90.60 91.30 91.54 89.80 91.06 90.98 91.18 91.04 (-0.14)
DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16)
PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02)
SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20)
SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08)
TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02)
ENGLISH 90.85 ? ? 90.15 91.13 91.12 91.16 91.14 (-0.02)
Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores). The three baselines are the second order
model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al (2008). The
four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features
(see ?3.2??3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference
followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold
indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arc-
factored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) according
to Dan Bikel?s randomized method (http://www.cis.upenn.edu/?dbikel/software.html).
tures and constraints can lead to further improve-
ments on accuracy.
We now turn to a different issue: scalability. In
previous work (Martins et al, 2009), we showed
that training the model via LP-relaxed inference
(as we do here) makes it learn to avoid frac-
tional solutions; as a consequence, ILP solvers
will converge faster to the optimum (on average).
Yet, it is known from worst case complexity the-
ory that solving a general ILP is NP-hard; hence,
these solvers may not scale well with the sentence
length. Merely considering the LP-relaxed version
of the problem at test time is unsatisfactory, as it
may lead to a fractional solution (i.e., a solution
whose components indexed by arcs, z? = ?za?a?A,
are not all integer), which does not correspond to a
valid dependency tree. We propose the following
approximate algorithm to obtain an actual parse:
first, solve the LP relaxation (which can be done
in polynomial time with interior-point methods);
then, if the solution is fractional, project it onto the
feasible set Y(x). Fortunately, the Euclidean pro-
jection can be computed in a straightforward way
by finding a maximal arborescence in the directed
graph whose weights are defined by z? (we omit
the proof for space); as we saw in ?2.2, the Chu-
Liu-Edmonds algorithm can do this in polynomial
time. The overall parsing runtime becomes poly-
nomial with respect to the length of the sentence.
The last column of Table 1 compares the ac-
curacy of this approximate method with the ex-
act one. We observe that there is not a substantial
drop in accuracy; on the other hand, we observed
a considerable speed-up with respect to exact in-
ference, particularly for long sentences. The av-
erage runtime (across all languages) is 0.632 sec-
onds per sentence, which is in line with existing
higher-order parsers and is much faster than the
runtimes reported by Riedel and Clarke (2006).
5 Conclusions
We presented new dependency parsers based on
concise ILP formulations. We have shown how
non-local output features can be incorporated,
while keeping only a polynomial number of con-
straints. These features can act as soft constraints
whose penalty values are automatically learned
from data; in addition, our model is also compati-
ble with expert knowledge in the form of hard con-
straints. Learning through a max-margin frame-
work is made effective by the means of a LP-
relaxation. Experimental results on seven lan-
guages show that our rich-featured parsers outper-
form arc-factored and approximate higher-order
parsers, and are in line with stacked parsers, hav-
ing with respect to the latter the advantage of not
requiring an ensemble configuration.
Acknowledgments
The authors thank the reviewers for their com-
ments. Martins was supported by a grant from
FCT/ICTI through the CMU-Portugal Program,
and also by Priberam Informa?tica. Smith was
supported by NSF IIS-0836431 and an IBM Fac-
ulty Award. Xing was supported by NSF DBI-
0546594, DBI-0640543, IIS-0713379, and an Al-
fred Sloan Foundation Fellowship in Computer
Science.
349
References
E. Boros and P.L. Hammer. 2002. Pseudo-Boolean op-
timization. Discrete Applied Mathematics, 123(1?
3):155?225.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for effi-
cient, feature-rich parsing. In Proc. of CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL.
M. Chang, L. Ratinov, and D. Roth. 2008. Constraints
as prior knowledge. In ICML Workshop on Prior
Knowledge for Text and Language Processing.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
J. Clarke and M. Lapata. 2008. Global inference
for sentence compression an integer linear program-
ming approach. JAIR, 31:399?429.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proc. of ACL.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In Proc. of HLT-NAACL.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammar. In Proc. of ACL.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING-ACL.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. I. Jor-
dan. 2006. Word alignment via quadratic assign-
ment. In Proc. of HLT-NAACL.
T. L. Magnanti and L. A. Wolsey. 1994. Optimal
Trees. Technical Report 290-94, Massachusetts In-
stitute of Technology, Operations Research Center.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proc. of
EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Polyhedral outer approximations with application to
natural language parsing. In Proc. of ICML.
R. T. McDonald and F. C. N. Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of EACL.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of IWPT.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of HLT-EMNLP.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL-HLT.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear program-
ming inference. In Proc. of COLING.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1):107?136.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In Proc. of EMNLP.
R. T. Rockafellar. 1970. Convex Analysis. Princeton
University Press.
D. Roth and W. T. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In
ICML.
A. Schrijver. 2003. Combinatorial Optimization:
Polyhedra and Efficiency, volume 24 of Algorithms
and Combinatorics. Springer.
D. A. Smith and J. Eisner. 2008. Dependency parsing
by belief propagation. In Proc. of EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,
and J. Nivre. 2008. The conll-2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. Proc. of CoNLL.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? A quasi-synchronous gram-
mar for QA. In Proceedings of EMNLP-CoNLL.
Y. Zhang and S. Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proc. of EMNLP.
350
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468?476,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition
Dipanjan Das and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan,nasmith}@cs.cmu.edu
Abstract
We present a novel approach to decid-
ing whether two sentences hold a para-
phrase relationship. We employ a gen-
erative model that generates a paraphrase
of a given sentence, and we use proba-
bilistic inference to reason about whether
two sentences share the paraphrase rela-
tionship. The model cleanly incorporates
both syntax and lexical semantics using
quasi-synchronous dependency grammars
(Smith and Eisner, 2006). Furthermore,
using a product of experts (Hinton, 2002),
we combine the model with a comple-
mentary logistic regression model based
on state-of-the-art lexical overlap features.
We evaluate our models on the task of
distinguishing true paraphrase pairs from
false ones on a standard corpus, giving
competitive state-of-the-art performance.
1 Introduction
The problem of modeling paraphrase relation-
ships between natural language utterances (McK-
eown, 1979) has recently attracted interest. For
computational linguists, solving this problem may
shed light on how best to model the semantics
of sentences. For natural language engineers, the
problem bears on information management sys-
tems like abstractive summarizers that must mea-
sure semantic overlap between sentences (Barzi-
lay and Lee, 2003), question answering modules
(Marsi and Krahmer, 2005) and machine transla-
tion (Callison-Burch et al, 2006).
The paraphrase identification problem asks
whether two sentences have essentially the same
meaning. Although paraphrase identification is
defined in semantic terms, it is usually solved us-
ing statistical classifiers based on shallow lexical,
n-gram, and syntactic ?overlap? features. Such
overlap features give the best-published classifi-
cation accuracy for the paraphrase identification
task (Zhang and Patrick, 2005; Finch et al, 2005;
Wan et al, 2006; Corley and Mihalcea, 2005, in-
ter alia), but do not explicitly model correspon-
dence structure (or ?alignment?) between the parts
of two sentences. In this paper, we adopt a model
that posits correspondence between the words in
the two sentences, defining it in loose syntactic
terms: if two sentences are paraphrases, we expect
their dependency trees to align closely, though
some divergences are also expected, with some
more likely than others. Following Smith and Eis-
ner (2006), we adopt the view that the syntactic
structure of sentences paraphrasing some sentence
s should be ?inspired? by the structure of s.
Because dependency syntax is still only a crude
approximation to semantic structure, we augment
the model with a lexical semantics component,
based on WordNet (Miller, 1995), that models how
words are probabilistically altered in generating
a paraphrase. This combination of loose syntax
and lexical semantics is similar to the ?Jeopardy?
model of Wang et al (2007).
This syntactic framework represents a major de-
parture from useful and popular surface similarity
features, and the latter are difficult to incorporate
into our probabilistic model. We use a product of
experts (Hinton, 2002) to bring together a logis-
tic regression classifier built from n-gram overlap
features and our syntactic model. This combined
model leverages complementary strengths of the
two approaches, outperforming a strong state-of-
the-art baseline (Wan et al, 2006).
This paper is organized as follows. We intro-
duce our probabilistic model in ?2. The model
makes use of three quasi-synchronous grammar
models (Smith and Eisner, 2006, QG, hereafter) as
components (one modeling paraphrase, one mod-
eling not-paraphrase, and one a base grammar);
these are detailed, along with latent-variable in-
ference and discriminative training algorithms, in
?3. We discuss the Microsoft Research Paraphrase
Corpus, upon which we conduct experiments, in
?4. In ?5, we present experiments on paraphrase
468
identification with our model and make compar-
isons with the existing state-of-the-art. We de-
scribe the product of experts and our lexical over-
lap model, and discuss the results achieved in ?6.
We relate our approach to prior work (?7) and con-
clude (?8).
2 Probabilistic Model
Since our task is a classification problem, we re-
quire our model to provide an estimate of the pos-
terior probability of the relationship (i.e., ?para-
phrase,? denoted p, or ?not paraphrase,? denoted
n), given the pair of sentences.1 Here, pQ denotes
model probabilities, c is a relationship class (p or
n), and s1 and s2 are the two sentences. We choose
the class according to:
c? = argmax
c?{p,n}
pQ(c | s1, s2)
= argmax
c?{p,n}
pQ(c)? pQ(s1, s2 | c) (1)
We define the class-conditional probabilities of
the two sentences using the following generative
story. First, grammar G0 generates a sentence s.
Then a class c is chosen, corresponding to a class-
specific probabilistic quasi-synchronous grammar
Gc. (We will discuss QG in detail in ?3. For the
present, consider it a specially-defined probabilis-
tic model that generates sentences with a specific
property, like ?paraphrases s,? when c = p.) Given
s, Gc generates the other sentence in the pair, s?.
When we observe a pair of sentences s1 and s2
we do not presume to know which came first (i.e.,
which was s and which was s?). Both orderings
are assumed to be equally probable. For class c,
pQ(s1, s2 | c) =
0.5? pQ(s1 | G0)? pQ(s2 | Gc(s1))
+ 0.5? pQ(s2 | G0)? pQ(s1 | Gc(s2))(2)
where c can be p or n; Gp(s) is the QG that gen-
erates paraphrases for sentence s, while Gn(s) is
the QG that generates sentences that are not para-
phrases of sentence s. This latter model may seem
counter-intuitive: since the vast majority of pos-
sible sentences are not paraphrases of s, why is a
special grammar required? Our use of a Gn fol-
lows from the properties of the corpus currently
used for learning, in which the negative examples
1Although we do not explore the idea here, the model
could be adapted for other sentence-pair relationships like en-
tailment or contradiction.
were selected to have high lexical overlap. We re-
turn to this point in ?4.
3 QG for Paraphrase Modeling
Here, we turn to the models Gp and Gn in detail.
3.1 Background
Smith and Eisner (2006) introduced the quasi-
synchronous grammar formalism. Here, we de-
scribe some of its salient aspects. The model
arose out of the empirical observation that trans-
lated sentences have some isomorphic syntactic
structure, but divergences are possible. Therefore,
rather than an isomorphic structure over a pair of
source and target sentences, the syntactic tree over
a target sentence is modeled by a source sentence-
specific grammar ?inspired? by the source sen-
tence?s tree. This is implemented by associating
with each node in the target tree a subset of the
nodes in the source tree. Since it loosely links
the two sentences? syntactic structures, QG is well
suited for problems like word alignment for MT
(Smith and Eisner, 2006) and question answering
(Wang et al, 2007).
Consider a very simple quasi-synchronous
context-free dependency grammar that generates
one dependent per production rule.2 Let s =
?s1, ..., sm? be the source sentence. The grammar
rules will take one of the two forms:
?t, l? ? ?t, l??t?, k? or ?t, l? ? ?t?, k??t, l?
where t and t? range over the vocabulary of the
target language, and l and k ? {0, ...,m} are in-
dices in the source sentence, with 0 denoting null.3
Hard or soft constraints can be applied between l
and k in a rule. These constraints imply permissi-
ble ?configurations.? For example, requiring l 6= 0
and, if k 6= 0 then sk must be a child of sl in the
source tree, we can implement a synchronous de-
pendency grammar similar to (Melamed, 2004).
Smith and Eisner (2006) used a quasi-
synchronous grammar to discover the correspon-
dence between words implied by the correspon-
dence between the trees. We follow Wang et al
(2007) in treating the correspondences as latent
variables, and in using a WordNet-based lexical
semantics model to generate the target words.
2Our actual model is more complicated; see ?3.2.
3A more general QG could allow one-to-many align-
ments, replacing l and k with sets of indices.
469
3.2 Detailed Model
We describe how we model pQ(t | Gp(s)) and
pQ(t | Gn(s)) for source and target sentences s
and t (appearing in Eq. 2 alternately as s1 and s2).
A dependency tree on a sequence w =
?w1, ..., wk? is a mapping of indices of words to
indices of syntactic parents, ?p : {1, ..., k} ?
{0, ..., k}, and a mapping of indices of words to
dependency relation types in L, ?` : {1, ..., k} ?
L. The set of indices children of wi to its left,
{j : ?w(j) = i, j < i}, is denoted ?w(i), and
?w(i) is used for right children. wi has a single
parent, denoted by w?p(i). Cycles are not allowed,
and w0 is taken to be the dummy ?wall? symbol,
$, whose only child is the root word of the sen-
tence (normally the main verb). The label for wi
is denoted by ?`(i). We denote the whole tree of
a sentence w by ?w, the subtree rooted at the ith
word by ?w,i.
Consider two sentences: let the source sen-
tence s contain m words and the target sentence
t contain n words. Let the correspondence x :
{1, ..., n} ? {0, ...,m} be a mapping from in-
dices of words in t to indices of words in s. (We
require each target word to map to at most one
source word, though multiple target words can
map to the same source word, i.e., x(i) = x(j)
while i 6= j.) When x(i) = 0, the ith target word
maps to the wall symbol, equivalently a ?null?
word. Each of our QGs Gp and Gn generates the
alignments x, the target tree ? t, and the sentence
t. Both Gp and Gn are structured in the same way,
differing only in their parameters; henceforth we
discuss Gp; Gn is similar.
We assume that the parse trees of s and t are
known.4 Therefore our model defines:
pQ(t | Gp(s)) = p(?
t | Gp(?
s))
=
?
x p(?
t, x | Gp(? s)) (3)
Because the QG is essentially a context-free de-
pendency grammar, we can factor it into recur-
sive steps as follows (let i be an arbitrary index
in {1, ..., n}):
P (? t,i | ti, x(i), ?
s) = pval (|?
t(i)|, |?t(i)| | ti)
4In our experiments, we use the parser described by Mc-
Donald et al (2005), trained on sections 2?21 of the WSJ
Penn Treebank, transformed to dependency trees following
Yamada and Matsumoto (2003). (The same treebank data
were also to estimate many of the parameters of our model, as
discussed in the text.) Though it leads to a partial ?pipeline?
approximation of the posterior probability p(c | s, t), we be-
lieve that the relatively high quality of English dependency
parsing makes this approximation reasonable.
?
?
j??t(i)??t(i)
m?
x(j)=0
P (? t,j | tj , x(j), ?
s)
?pkid (tj , ?
t
` (j), x(j) | ti, x(i), ?
s) (4)
where pval and pkid are valence and child-
production probabilities parameterized as dis-
cussed in ?3.4. Note the recursion in the second-
to-last line.
We next describe a dynamic programming so-
lution for calculating p(? t | Gp(? s)). In ?3.4 we
discuss the parameterization of the model.
3.3 Dynamic Programming
Let C(i, l) refer to the probability of ? t,i, assum-
ing that the parent of ti, t?tp(i), is aligned to sl. For
leaves of ? t, the base case is:
C(i, l) = pval (0, 0 | ti)? (5)
?m
k=0 pkid (ti, ?
t
` (i), k | t?tp(i), l, ?
s)
where k ranges over possible values of x(i), the
source-tree node to which ti is aligned. The recur-
sive case is:
C(i, l) = pval (|?
t(i)|, |?t(i)| | ti) (6)
?
?m
k=0 pkid (ti, ?
t
` (i), k | t?tp(i), l, ?
s)
?
?
j??t(i)??t(i)C(j, k)
We assume that the wall symbols t0 and s0 are
aligned, so p(? t | Gp(? s)) = C(r, 0), where r is
the index of the root word of the target tree ? t. It
is straightforward to show that this algorithm re-
quires O(m2n) runtime and O(mn) space.
3.4 Parameterization
The valency distribution pval in Eq. 4 is estimated
in our model using the transformed treebank (see
footnote 4). For unobserved cases, the conditional
probability is estimated by backing off to the par-
ent POS tag and child direction.
We discuss next how to parameterize the prob-
ability pkid that appears in Equations 4, 5, and 6.
This conditional distribution forms the core of our
QGs, and we deviate from earlier research using
QGs in defining pkid in a fully generative way.
In addition to assuming that dependency parse
trees for s and t are observable, we also assume
each word wi comes with POS and named entity
tags. In our experiments these were obtained au-
tomatically using MXPOST (Ratnaparkhi, 1996)
and BBN?s Identifinder (Bikel et al, 1999).
470
For clarity, let j = ? tp(i) and let l = x(j).
pkid(ti, ?
t
` (i), x(i) | tj , l, ?
s) =
pconfig(config(ti, tj , sx(i), sl) | tj , l, ?
s) (7)
?punif (x(i) | config(ti, tj , sx(i), sl)) (8)
?plab(?
t
` (i) | config(ti, tj , sx(i), sl)) (9)
?ppos(pos(ti) | pos(sx(i))) (10)
?pne(ne(ti) | ne(sx(i))) (11)
?plsrel (lsrel(ti) | sx(i)) (12)
?pword (ti | lsrel(ti), sx(i)) (13)
We consider each of the factors above in turn.
Configuration In QG, ?configurations? refer to
the tree relationship among source-tree nodes
(above, sl and sx(i)) aligned to a pair of parent-
child target-tree nodes (above, tj and ti). In deriv-
ing ? t,j , the model first chooses the configuration
that will hold among ti, tj , sx(i) (which has yet
to be chosen), and sl (line 7). This is defined for
configuration c log-linearly by:5
pconfig(c | tj , l, ?
s) =
?c
?
c?:?sk,config(ti,tj ,sk,sl)=c?
?c?
(14)
Permissible configurations in our model are shown
in Table 1. These are identical to prior work
(Smith and Eisner, 2006; Wang et al, 2007),
except that we add a ?root? configuration that
aligns the target parent-child pair to null and the
head word of the source sentence, respectively.
Using many permissible configurations helps re-
move negative effects from noisy parses, which
our learner treats as evidence. Fig. 1 shows some
examples of major configurations that Gp discov-
ers in the data.
Source tree alignment After choosing the config-
uration, the specific node in ? s that ti will align
to, sx(i) is drawn uniformly (line 8) from among
those in the configuration selected.
Dependency label, POS, and named entity class
The newly generated target word?s dependency
label, POS, and named entity class drawn from
multinomial distributions plab , ppos , and pne that
condition, respectively, on the configuration and
the POS and named entity class of the aligned
source-tree word sx(i) (lines 9?11).
5We use log-linear models three times: for the configura-
tion, the lexical semantics class, and the word. Each time,
we are essentially assigning one weight per outcome and
renormalizing among the subset of outcomes that are possible
given what has been derived so far.
Configuration Description
parent-child ? sp(x(i)) = x(j), appended with ?
s
` (x(i))
child-parent x(i) = ? sp(x(j)), appended with ?
s
` (x(j))
grandparent-
grandchild
? sp(?
s
p(x(i))) = x(j), appended with
? s` (x(i))
siblings ? sp(x(i)) = ?
s
p(x(j)), x(i) 6= x(j)
same-node x(i) = x(j)
c-command the parent of one source-side word is an
ancestor of the other source-side word
root x(j) = 0, x(i) is the root of s
child-null x(i) = 0
parent-null x(j) = 0, x(i) is something other than
root of s
other catch-all for all other types of configura-
tions, which are permitted
Table 1: Permissible configurations. i is an index in t whose
configuration is to be chosen; j = ? tp(i) is i?s parent.
WordNet relation(s) The model next chooses a
lexical semantics relation between sx(i) and the
yet-to-be-chosen word ti (line 12). Following
Wang et al (2007),6 we employ a 14-feature log-
linear model over all logically possible combina-
tions of the 14 WordNet relations (Miller, 1995).7
Similarly to Eq. 14, we normalize this log-linear
model based on the set of relations that are non-
empty in WordNet for the word sx(i).
Word Finally, the target word is randomly chosen
from among the set of words that bear the lexical
semantic relationship just chosen (line 13). This
distribution is, again, defined log-linearly:
pword (ti | lsrel(ti) = R, sx(i)) =
?ti
?
w?:sx(i)Rw? ?w?
(15)
Here ?w is the Good-Turing unigram probability
estimate of a word w from the Gigaword corpus
(Graff, 2003).
3.5 Base Grammar G0
In addition to the QG that generates a second sen-
tence bearing the desired relationship (paraphrase
or not) to the first sentence s, our model in ?2 also
requires a base grammar G0 over s.
We view this grammar as a trivial special case
of the same QG model already described. G0 as-
sumes the empty source sentence consists only of
6Note that Wang et al (2007) designed pkid as an inter-
polation between a log-linear lexical semantics model and a
word model. Our approach is more fully generative.
7These are: identical-word, synonym, antonym (includ-
ing extended and indirect antonym), hypernym, hyponym,
derived form, morphological variation (e.g., plural form),
verb group, entailment, entailed-by, see-also, causal relation,
whether the two words are same and is a number, and no re-
lation.
471
(a) parent-child
fill
questionnaire
complete
questionnaire
dozens
wounded
injured
dozens
(b) child-parent (c) grandparent-grandchild
will
chief
will
Secretary
Liscouski
quarter
first
first-quarter
(e) same-node
U.S
refunding
massive
(f) siblings
U.S
treasury
treasury
(g) root
null
fell
null
dropped
(d) c-command
signatures
necessary
signatures
needed
897,158
the
twice
approaching
collected
Figure 1: Some example configurations from Table 1 that Gp discovers in the dev. data. Directed arrows show head-modifier
relationships, while dotted arrows show alignments.
a single wall node. Thus every word generated un-
der G0 aligns to null, and we can simplify the dy-
namic programming algorithm that scores a tree
? s under G0:
C ?(i) = pval (|?
t(i)|, |?t(i)| | si)
?plab(?
t
` (i))? ppos(pos(ti))? pne(ne(ti))
?pword(ti)?
?
j:?t(j)=iC
?(j) (16)
where the final product is 1 when ti has no chil-
dren. It should be clear that p(s | G0) = C ?(0).
We estimate the distributions over dependency
labels, POS tags, and named entity classes using
the transformed treebank (footnote 4). The dis-
tribution over words is taken from the Gigaword
corpus (as in ?3.4).
It is important to note thatG0 is designed to give
a smoothed estimate of the probability of a partic-
ular parsed, named entity-tagged sentence. It is
never used for parsing or for generation; it is only
used as a component in the generative probability
model presented in ?2 (Eq. 2).
3.6 Discriminative Training
Given training data
?
?s(i)1 , s
(i)
2 , c
(i)?
?N
i=1
, we train
the model discriminatively by maximizing regu-
larized conditional likelihood:
max
?
N?
i=1
log pQ(c
(i) | s(i)1 , s
(i)
2 ,?)
? ?? ?
Eq. 2 relates this to G{0,p,n}
?C???22
(17)
The parameters ? to be learned include the class
priors, the conditional distributions of the depen-
dency labels given the various configurations, the
POS tags given POS tags, the NE tags given NE
tags appearing in expressions 9?11, the configura-
tion weights appearing in Eq. 14, and the weights
of the various features in the log-linear model for
the lexical-semantics model. As noted, the distri-
butions pval , the word unigram weights in Eq. 15,
and the parameters of the base grammar are fixed
using the treebank (see footnote 4) and the Giga-
word corpus.
Since there is a hidden variable (x), the objec-
tive function is non-convex. We locally optimize
using the L-BFGS quasi-Newton method (Liu and
Nocedal, 1989). Because many of our parameters
are multinomial probabilities that are constrained
to sum to one and L-BFGS is not designed to han-
dle constraints, we treat these parameters as un-
normalized weights that get renormalized (using a
softmax function) before calculating the objective.
4 Data and Task
In all our experiments, we have used the Mi-
crosoft Research Paraphrase Corpus (Dolan et al,
2004; Quirk et al, 2004). The corpus contains
5,801 pairs of sentences that have been marked
as ?equivalent? or ?not equivalent.? It was con-
structed from thousands of news sources on the
web. Dolan and Brockett (2005) remark that
this corpus was created semi-automatically by first
training an SVM classifier on a disjoint annotated
10,000 sentence pair dataset and then applying
the SVM on an unseen 49,375 sentence pair cor-
pus, with its output probabilities skewed towards
over-identification, i.e., towards generating some
false paraphrases. 5,801 out of these 49,375 pairs
were randomly selected and presented to human
judges for refinement into true and false para-
phrases. 3,900 of the pairs were marked as having
472
About 120 potential jurors were being asked to complete a lengthy questionnaire . 
The jurors were taken into the courtroom in groups of 40 and asked to fill out a questionnaire .
Figure 2: Discovered alignment of Ex. 19 produced by Gp. Observe that the model aligns identical words and also ?complete?
and ?fill? in this specific case. This kind of alignment provides an edge over a simple lexical overlap model.
?mostly bidirectional entailment,? a standard def-
inition of the paraphrase relation. Each sentence
was labeled first by two judges, who averaged 83%
agreement, and a third judge resolved conflicts.
We use the standard data split into 4,076 (2,753
paraphrase, 1,323 not) training and 1,725 (1147
paraphrase, 578 not) test pairs. We reserved a ran-
domly selected 1,075 training pairs for tuning.We
cite some examples from the training set here:
(18) Revenue in the first quarter of the year dropped 15
percent from the same period a year earlier.
With the scandal hanging over Stewart?s company,
revenue in the first quarter of the year dropped 15
percent from the same period a year earlier.
(19) About 120 potential jurors were being asked to
complete a lengthy questionnaire.
The jurors were taken into the courtroom in groups of
40 and asked to fill out a questionnaire.
Ex. 18 is a true paraphrase pair. Notice the high
lexical overlap between the two sentences (uni-
gram overlap of 100% in one direction and 72%
in the other). Ex. 19 is another true paraphrase
pair with much lower lexical overlap (unigram
overlap of 50% in one direction and 30% in the
other). Notice the use of similar-meaning phrases
and irrelevant modifiers that retain the same mean-
ing in both sentences, which a lexical overlap
model cannot capture easily, but a model like a QG
might. Also, in both pairs, the relationship cannot
be called total bidirectional equivalence because
there is some extra information in one sentence
which cannot be inferred from the other.
Ex. 20 was labeled ?not paraphrase?:
(20) ?There were a number of bureaucratic and
administrative missed signals - there?s not one person
who?s responsible here,? Gehman said.
In turning down the NIMA offer, Gehman said, ?there
were a number of bureaucratic and administrative
missed signals here.
There is significant content overlap, making a de-
cision difficult for a na??ve lexical overlap classifier.
(In fact, pQ labels this example n while the lexical
overlap models label it p.)
The fact that negative examples in this corpus
were selected because of their high lexical over-
lap is important. It means that any discrimina-
tive model is expected to learn to distinguish mere
overlap from paraphrase. This seems appropriate,
but it does mean that the ?not paraphrase? relation
ought to be denoted ?not paraphrase but decep-
tively similar on the surface.? It is for this reason
that we use a special QG for the n relation.
5 Experimental Evaluation
Here we present our experimental evaluation using
pQ. We trained on the training set (3,001 pairs)
and tuned model metaparameters (C in Eq. 17)
and the effect of different feature sets on the de-
velopment set (1,075 pairs). We report accuracy
on the official MSRPC test dataset. If the poste-
rior probability pQ(p | s1, s2) is greater than 0.5,
the pair is labeled ?paraphrase? (as in Eq. 1).
5.1 Baseline
We replicated a state-of-the-art baseline model for
comparison. Wan et al (2006) report the best pub-
lished accuracy, to our knowledge, on this task,
using a support vector machine. Our baseline is
a reimplementation of Wan et al (2006), using
features calculated directly from s1 and s2 with-
out recourse to any hidden structure: proportion
of word unigram matches, proportion of lemma-
tized unigram matches, BLEU score (Papineni et
al., 2001), BLEU score on lemmatized tokens, F
measure (Turian et al, 2003), difference of sen-
tence length, and proportion of dependency rela-
tion overlap. The SVM was trained to classify
positive and negative examples of paraphrase us-
ing SVMlight (Joachims, 1999).8 Metaparameters,
tuned on the development data, were the regu-
larization constant and the degree of the polyno-
mial kernel (chosen in [10?5, 102] and 1?5 respec-
tively.).9
It is unsurprising that the SVM performs very
well on the MSRPC because of the corpus creation
process (see Sec. 4) where an SVM was applied
as well, with very similar features and a skewed
decision process (Dolan and Brockett, 2005).
8http://svmlight.joachims.org
9Our replication of the Wan et al model is approxi-
mate, because we used different preprocessing tools: MX-
POST for POS tagging (Ratnaparkhi, 1996), MSTParser
for parsing (McDonald et al, 2005), and Dan Bikel?s
interface (http://www.cis.upenn.edu/?dbikel/
software.html#wn) to WordNet (Miller, 1995) for
lemmatization information. Tuning led to C = 17 and poly-
nomial degree 4.
473
Model Accuracy Precision Recall
baselines
all p 66.49 66.49 100.00
Wan et al SVM (reported) 75.63 77.00 90.00
Wan et al SVM (replication) 75.42 76.88 90.14
pQ
lexical semantics features removed 68.64 68.84 96.51
all features 73.33 74.48 91.10
c-command disallowed (best; see text) 73.86 74.89 91.28
?6 pL 75.36 78.12 87.44
product of experts 76.06 79.57 86.05
oracles
Wan et al SVM and pL 80.17 100.00 92.07
Wan et al SVM and pQ 83.42 100.00 96.60
pQ and pL 83.19 100.00 95.29
Table 2: Accuracy,
p-class precision, and
p-class recall on the test
set (N = 1,725). See
text for differences in
implementation
between Wan et al and
our replication; their
reported score does not
include the full test set.
5.2 Results
Tab. 2 shows performance achieved by the base-
line SVM and variations on pQ on the test set. We
performed a few feature ablation studies, evaluat-
ing on the development data. We removed the lex-
ical semantics component of the QG,10 and disal-
lowed the syntactic configurations one by one, to
investigate which components of pQ contributes to
system performance. The lexical semantics com-
ponent is critical, as seen by the drop in accu-
racy from the table (without this component, pQ
behaves almost like the ?all p? baseline). We
found that the most important configurations are
?parent-child,? and ?child-parent? while damage
from ablating other configurations is relatively
small. Most interestingly, disallowing the ?c-
command? configuration resulted in the best ab-
solute accuracy, giving us the best version of pQ.
The c-command configuration allows more distant
nodes in a source sentence to align to parent-child
pairs in a target (see Fig. 1d). Allowing this con-
figuration guides the model in the wrong direction,
thus reducing test accuracy. We tried disallowing
more than one configuration at a time, without get-
ting improvements on development data. We also
tried ablating the WordNet relations, and observed
that the ?identical-word? feature hurt the model
the most. Ablating the rest of the features did not
produce considerable changes in accuracy.
The development data-selected pQ achieves
higher recall by 1 point than Wan et al?s SVM,
but has precision 2 points worse.
5.3 Discussion
It is quite promising that a linguistically-motivated
probabilistic model comes so close to a string-
similarity baseline, without incorporating string-
local phrases. We see several reasons to prefer
10This is accomplished by eliminating lines 12 and 13 from
the definition of pkid and redefining pword to be the unigram
word distribution estimated from the Gigaword corpus, as in
G0, without the help of WordNet.
the more intricate QG to the straightforward SVM.
First, the QG discovers hidden alignments be-
tween words. Alignments have been leveraged in
related tasks such as textual entailment (Giampic-
colo et al, 2007); they make the model more inter-
pretable in analyzing system output (e.g., Fig. 2).
Second, the paraphrases of a sentence can be con-
sidered to be monolingual translations. We model
the paraphrase problem using a direct machine
translation model, thus providing a translation in-
terpretation of the problem. This framework could
be extended to permit paraphrase generation, or to
exploit other linguistic annotations, such as repre-
sentations of semantics (see, e.g., Qiu et al, 2006).
Nonetheless, the usefulness of surface overlap
features is difficult to ignore. We next provide an
efficient way to combine a surface model with pQ.
6 Product of Experts
Incorporating structural alignment and surface
overlap features inside a single model can make
exact inference infeasible. As an example, con-
sider features like n-gram overlap percentages that
provide cues of content overlap between two sen-
tences. One intuitive way of including these fea-
tures in a QG could be including these only at
the root of the target tree, i.e. while calculating
C(r, 0). These features have to be included in
estimating pkid, which has log-linear component
models (Eq. 7- 13). For these bigram or trigram
overlap features, a similar log-linear model has
to be normalized with a partition function, which
considers the (unnormalized) scores of all possible
target sentences, given the source sentence.
We therefore combine pQ with a lexical overlap
model that gives another posterior probability es-
timate pL(c | s1, s2) through a product of experts
(PoE; Hinton, 2002), pJ(c | s1, s2)
=
pQ(c | s1, s2)? pL(c | s1, s2)
?
c??{p,n}
pQ(c
? | s1, s2)? pL(c
? | s1, s2)
(21)
474
Eq. 21 takes the product of the two models? poste-
rior probabilities, then normalizes it to sum to one.
PoE models are used to efficiently combine several
expert models that individually constrain different
dimensions in high-dimensional data, the product
therefore constraining all of the dimensions. Com-
bining models in this way grants to each expert
component model the ability to ?veto? a class by
giving it low probability; the most probable class
is the one that is least objectionable to all experts.
Probabilistic Lexical Overlap Model We de-
vised a logistic regression (LR) model incorpo-
rating 18 simple features, computed directly from
s1 and s2, without modeling any hidden corre-
spondence. LR (like the QG) provides a proba-
bility distribution, but uses surface features (like
the SVM). The features are of the form precisionn
(number of n-gram matches divided by the num-
ber of n-grams in s1), recalln (number of n-gram
matches divided by the number of n-grams in s2)
and Fn (harmonic mean of the previous two fea-
tures), where 1 ? n ? 3. We also used lemma-
tized versions of these features. This model gives
the posterior probability pL(c | s1, s2), where
c ? {p, n}. We estimated the model parameters
analogously to Eq. 17. Performance is reported in
Tab. 2; this model is on par with the SVM, though
trading recall in favor of precision. We view it as a
probabilistic simulation of the SVM more suitable
for combination with the QG.
Training the PoE Various ways of training a PoE
exist. We first trained pQ and pL separately as
described, then initialized the PoE with those pa-
rameters. We then continued training, maximizing
(unregularized) conditional likelihood.
Experiment We used pQ with the ?c-command?
configuration excluded, and the LR model in the
product of experts. Tab. 2 includes the final re-
sults achieved by the PoE. The PoE model outper-
forms all the other models, achieving an accuracy
of 76.06%.11 The PoE is conservative, labeling a
pair as p only if the LR and the QG give it strong
p probabilities. This leads to high precision, at the
expense of recall.
Oracle Ensembles Tab. 2 shows the results of
three different oracle ensemble systems that cor-
rectly classify a pair if either of the two individual
systems in the combination is correct. Note that
the combinations involving pQ achieve 83%, the
11This accuracy is significant over pQ under a paired t-test
(p < 0.04), but is not significant over the SVM.
human agreement level for the MSRPC. The LR
and SVM are highly similar, and their oracle com-
bination does not perform as well.
7 Related Work
There is a growing body of research that uses the
MSRPC (Dolan et al, 2004; Quirk et al, 2004)
to build models of paraphrase. As noted, the most
successful work has used edit distance (Zhang and
Patrick, 2005) or bag-of-words features to mea-
sure sentence similarity, along with shallow syn-
tactic features (Finch et al, 2005; Wan et al, 2006;
Corley and Mihalcea, 2005). Qiu et al (2006)
used predicate-argument annotations.
Most related to our approach, Wu (2005) used
inversion transduction grammars?a synchronous
context-free formalism (Wu, 1997)?for this task.
Wu reported only positive-class (p) precision (not
accuracy) on the test set. He obtained 76.1%,
while our PoE model achieves 79.6% on that mea-
sure. Wu?s model can be understood as a strict
hierarchical maximum-alignment method. In con-
trast, our alignments are soft (we sum over them),
and we do not require strictly isomorphic syntac-
tic structures. Most importantly, our approach is
founded on a stochastic generating process and es-
timated discriminatively for this task, while Wu
did not estimate any parameters from data at all.
8 Conclusion
In this paper, we have presented a probabilistic
model of paraphrase incorporating syntax, lexi-
cal semantics, and hidden loose alignments be-
tween two sentences? trees. Though it fully de-
fines a generative process for both sentences and
their relationship, the model is discriminatively
trained to maximize conditional likelihood. We
have shown that this model is competitive for de-
termining whether there exists a semantic rela-
tionship between them, and can be improved by
principled combination with more standard lexical
overlap approaches.
Acknowledgments
The authors thank the three anonymous review-
ers for helpful comments and Alan Black, Freder-
ick Crabbe, Jason Eisner, Kevin Gimpel, Rebecca
Hwa, David Smith, and Mengqiu Wang for helpful
discussions. This work was supported by DARPA
grant NBCH-1080004.
475
References
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: an unsupervised approach using
multiple-sequence alignment. In Proc. of NAACL.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1-3):211?231.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. of HLT-NAACL.
Courtney Corley and Rada Mihalcea. 2005. Mea-
suring the semantic similarity of texts. In Proc. of
ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment.
William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proc. of IWP.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
Proc. of COLING.
Andrew Finch, Young Sook Hwang, and Eiichiro
Sumita. 2005. Using machine translation evalua-
tion techniques to determine sentence-level seman-
tic equivalence. In Proc. of IWP.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proc. of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14:1771?1800.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming (Ser. B), 45(3):503?528.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proc. of EWNLG.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Kathleen R. McKeown. 1979. Paraphrasing using
given and new information in a question-answer sys-
tem. In Proc. of ACL.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proc. of ACL.
George A. Miller. 1995. Wordnet: a lexical database
for English. Commun. ACM, 38(11):39?41.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proc. of EMNLP.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proc. of EMNLP.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. of
EMNLP.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proc. of the HLT-
NAACL Workshop on Statistical Machine Transla-
tion.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003. Evaluation of machine translation and its
evaluation. In Proc. of Machine Translation Summit
IX.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2006. Using dependency-based features to
take the ?para-farce? out of paraphrase. In Proc. of
ALTW.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proc. of EMNLP-
CoNLL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3).
Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars.
In Proc. of the ACL Workshop on Empirical Model-
ing of Semantic Equivalence and Entailment.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identi-
fication by text canonicalization. In Proc. of ALTW.
476
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 1?4,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Variational Inference for Grammar Induction with Prior Knowledge
Shay B. Cohen and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,nasmith}@cs.cmu.edu
Abstract
Variational EM has become a popular
technique in probabilistic NLP with hid-
den variables. Commonly, for computa-
tional tractability, we make strong inde-
pendence assumptions, such as the mean-
field assumption, in approximating pos-
terior distributions over hidden variables.
We show how a looser restriction on the
approximate posterior, requiring it to be a
mixture, can help inject prior knowledge
to exploit soft constraints during the varia-
tional E-step.
1 Introduction
Learning natural language in an unsupervised way
commonly involves the expectation-maximization
(EM) algorithm to optimize the parameters of a
generative model, often a probabilistic grammar
(Pereira and Schabes, 1992). Later approaches in-
clude variational EM in a Bayesian setting (Beal
and Gharamani, 2003), which has been shown to
obtain even better results for various natural lan-
guage tasks over EM (e.g., Cohen et al, 2008).
Variational EM usually makes the mean-field
assumption, factoring the posterior over hidden
variables into independent distributions. Bishop et
al. (1998) showed how to use a less strict assump-
tion: a mixture of factorized distributions.
In other work, soft or hard constraints on the
posterior during the E-step have been explored
in order to improve performance. For example,
Smith and Eisner (2006) have penalized the ap-
proximate posterior over dependency structures
in a natural language grammar induction task to
avoid long range dependencies between words.
Grac?a et al (2007) added linear constraints on ex-
pected values of features of the hidden variables in
an alignment task.
In this paper, we use posterior mixtures to inject
bias or prior knowledge into a Bayesian model.
We show that empirically, injecting prior knowl-
edge improves performance on an unsupervised
Chinese grammar induction task.
2 Variational Mixtures with Constraints
Our EM variant encodes prior knowledge in an ap-
proximate posterior by constraining it to be from
a mixture family of distributions. We will use x to
denote observable random variables, y to denote
hidden structure, and ? to denote the to-be-learned
parameters of the model (coming from a subset of
R
`
for some `). ? will denote the parameters of
a prior over ?. The mean-field assumption in the
Bayesian setting assumes that the posterior has a
factored form:
q(?,y) = q(?)q(y) (1)
Traditionally, variational inference with the mean-
field assumption alternates between an E-step
which optimizes q(y) and then an M-step which
optimizes q(?).
1
The mean-field assumption
makes inference feasible, at the expense of op-
timizing a looser lower bound on the likelihood
(Bishop, 2006). The lower bound that the algo-
rithm optimizes is the following:
F (q(?,y),?) = E
q(?,y)
[log p(x,y,? | ?)]+H(q)
(2)
where H(q) denotes the entropy of distribution q.
We focus on changing the E-step and as a result,
changing the underlying bound, F (q(?,y),?).
Similarly to Bishop et al (1998), instead of mak-
ing the strict mean-field assumption, we assume
that the variational model is a mixture. One com-
ponent of the mixture might take the traditional
form, but others will be used to encourage certain
1
This optimization can be nested inside another EM al-
gorithm that optimizes ?; this is our approach. q(?) is tra-
ditionally conjugate to the likelihood for computational rea-
sons, but our method is not limited to that kind of prior, as
seen in the experiments.
1
tendencies considered a priori to be appropriate.
Denoting the probability simplex of dimension r
4
r
= {??
1
, ..., ?
r
? ? R
r
: ?
i
? 0,
?
r
i=1
?
i
=
1}, we require that:
q(?,y | ?) =
?
r
i=1
?
i
q
i
(y)q
i
(?) (3)
for ? ? 4
r
. Q
i
will denote the family of distri-
butions for the ith mixture component, and Q(4
r
)
will denote the family implied by the mixture of
Q
1
, . . . ,Q
r
where the mixture coefficients ? ?
4
r
. ? comprise r additional variational param-
eters, in addition to parameters for each q
i
(y) and
q
i
(?).
When one of the mixture components q
i
is suf-
ficiently expressive, ? will tend toward a degener-
ate solution. In order to force all mixture compo-
nents to play a role?even at the expense of the
tightness of the variational bound?we will im-
pose hard constraints on ?: ? ?
?
4
r
? 4
r
. In
our experiments (?3),
?
4
r
will be mostly a line seg-
ment corresponding to two mixture coefficients.
The role of the variational EM algorithm is to
optimize the variational bound in Eq. 2 with re-
spect to q(y), q(?), and ?. Keeping this intention
in mind, we can replace the E-step and M-step in
the original variational EM algorithm with 2r + 1
coordinate ascent steps, for 1 ? i ? r:
E-step: For each i ? {1, ..., r}, optimize the
bound given ? and q
i
?
(y)|
i
?
?{1,...,r}\{i}
and
q
i
?
(?)|
i
?
?{1,...,r}
by selecting a new distribution
q
i
(y).
M-step: For each i ? {1, ..., r}, optimize the
bound given ? and q
i
?
(?)|
i
?
?{1,...,r}\{i}
and
q
i
?
(y)|
i
?
?{1,...,r}
by selecting a new distribution
q
i
(?).
C-step: Optimize the bound by selecting a new set
of coefficients ? ?
?
4
r
in order to optimize the
bound with respect to the mixture coefficients.
We call the revised algorithm constrained mix-
ture variational EM.
For a distribution r(h), we denote by KL(Q
i
?r)
the following:
KL(Q
i
?r) = min
q?Q
i
KL(q(h)?r)) (4)
where KL(???) denotes the Kullback-Leibler di-
vergence.
The next proposition, which is based on a result
in Grac?a et al (2007), gives an intuition of how
modifying the variational EM algorithm with Q =
Q(
?
4
r
) affects the solution:
Proposition 1. Constrained mixture variational
EM finds local maxima for a function G(q,?)
such that
log p(x | ?)? min
??
?
4
r
L(?,?) ? G(q,?) ? log p(x | ?)
(5)
where L(?,?) =
r
?
i=1
?
i
KL(Q
i
?p(?,y | x,?)).
We can understand mixture variational EM as
penalizing the likelihood with a term bounded by
a linear function of the ?, minimized over
?
4
r
. We
will exploit that bound in ?2.2 for computational
tractability.
2.1 Simplex Annealing
The variational EM algorithm still identifies only
local maxima. Different proposals have been for
pushing EM toward a global maximum. In many
cases, these methods are based on choosing dif-
ferent initializations for the EM algorithm (e.g.,
repeated random initializations or a single care-
fully designed initializer) such that it eventually
gets closer to a global maximum.
We follow the idea of annealing proposed in
Rose et al (1990) and Smith and Eisner (2006) for
the ? by gradually loosening hard constraints on ?
as the variational EM algorithm proceeds. We de-
fine a sequence of
?
4
r
(t) for t = 0, 1, ... such that
?
4
r
(t) ?
?
4
r
(t+1). First, we have the inequality:
KL(Q(
?
4
r
(t))?p(?,y | x,?) (6)
? KL(Q(
?
4
r
(t + 1))?p(?,y | x,?))
We say that the annealing schedule is ? -separated
if we have for any ?:
KL(Q(
?
4
r
(t))?p(?,y | x,?)) (7)
? KL(Q(
?
4
r
(t + 1))?p(?,y | x,?)) ?
?
2
(t+1)
? -separation requires consecutive families
Q(
?
4
r
(t)) and Q(
?
4
r
(t + 1)) to be similar.
Proposition 1 stated the bound we optimize,
which penalizes the likelihood by subtracting a
positive KL divergence from it. With the ? -
separation condition we can show that even though
we penalize likelihood, the variational EM algo-
rithm will still increase likelihood by a certain
amount. Full details are omitted for space and can
be found in ?).
2
Input: initial parameters ?
(0)
, observed data x,
annealing schedule
?
4
r
: N? 2
4
r
Output: learned parameters ? and approximate
posterior q(?,y)
t? 1;
repeat
E-step: repeat
E-step: forall i ? [r] do: q
(t+1)
i
(y)? argmax
q(y)?Q
i
F
?
(
P
j 6=i
?
j
q
(t)
i
(?)q(y) + ?
i
q
(t)
i
q(y),?
(t)
)
M-step: forall i ? [r] do: q
(t+1)
i
(?)? argmax
q(?)?Q
i
F
?
(
P
j 6=i
?
j
q(?)q
(t)
i
(y) + ?
i
q
(t)
i
q(y),?
(t)
)
C-step: ?
(t+1)
?
argmax
??
?
4
r
(t)
F
?
(
P
r
j=1
?
j
q
(t)
i
(?)q
(t)
i
(y),?
(t)
)
until convergence ;
M-step: ?
(t+1)
?
argmax
?
F
?
(
P
r
i=1
?
i
q
(t+1)
i
(?)q
(t+1)
i
(y),?)
t? t + 1;
until convergence ;
return ?
(t)
,
P
r
i=1
?
i
q
(t)
i
(?)q
(t)
i
(y)
Figure 1: The constrained variational mixture EM algorithm.
[n] denotes {1, ..., n}.
2.2 Tractability
We now turn to further alterations of the bound in
Eq. 2 to make it more tractable. The main problem
is the entropy term which is not easy to compute,
because it includes a log term over a mixture of
distributions from Q
i
. We require the distributions
in Q
i
to factorize over the hidden structure y, but
this only helps with the first term in Eq. 2.
We note that because the entropy function is
convex, we can get a lower bound on H(q):
H(q) ?
?
r
i=1
?
i
H(q
i
) =
?
r
i=1
?
i
H(q
i
(?,y))
Substituting the modified entropy term into
Eq. 2 still yields a lower bound on the likeli-
hood. This change makes the E-step tractable,
because each distribution q
i
(y) can be computed
separately by optimizing a bound which depends
only on the variational parameters in that distribu-
tion. In fact, the bound on the left hand side in
Proposition 1 becomes the function that we opti-
mize instead of G(q,?).
Without proper constraints, the ? update can be
intractable as well. It requires maximizing a lin-
ear objective (in ?) while constraining the ? to
be from a particular subspace of the probability
simplex,
?
4
r
(t). To solve this issue, we require
that
?
4
r
(t) is polyhedral, making it possible to ap-
ply linear programming (Boyd and Vandenberghe,
2004).
The bound we optimize is:
2
F
?
(
r
?
i=1
?
i
q
i
(?,y),?
)
(8)
=
r
?
i=1
?
i
(
E
q
i
(?,y)
[log p(?,y,x | m)] + H(q
i
(?,y))
)
with ? ?
?
4
r
(t
final
) and (q
i
(?,y)) ? Q
i
. The
algorithm for optimizing this bound is in Fig. 1,
which includes an extra M-step to optimize? (see
extended report).
3 Experiments
We tested our method on the unsupervised learn-
ing problem of dependency grammar induction.
For the generative model, we used the dependency
model with valence as it appears in Klein andMan-
ning (2004). We used the data from the Chi-
nese treebank (Xue et al, 2004). Following stan-
dard practice, sentences were stripped of words
and punctuation, leaving part-of-speech tags for
the unsupervised induction of dependency struc-
ture, and sentences of length more than 10 were
removed from the set. We experimented with
a Dirichlet prior over the parameters and logis-
tic normal priors over the parameters, and found
the latter to still be favorable with our method, as
in Cohen et al (2008). We therefore report results
with our method only for the logistic normal prior.
We do inference on sections 1?270 and 301?1151
of CTB10 (4,909 sentences) by running the EM al-
gorithm for 20 iterations, for which all algorithms
have their variational bound converge.
To evaluate performance, we report the fraction
of words whose predicted parent matches the gold
standard (attachment accuracy). For parsing, we
use the minimum Bayes risk parse.
Our mixture componentsQ
i
are based on simple
linguistic tendencies of Chinese syntax. These ob-
servations include the tendency of dependencies to
(a) emanate from the right of the current position
and (b) connect words which are nearby (in string
distance). We experiment with six mixture com-
ponents: (1) RIGHTATTACH: Each word?s parent
is to the word?s right. The root, therefore, is al-
ways the rightmost word; (2) ALLRIGHT: The
rightmost word is the parent of all positions in the
sentence (there is only one such tree); (3) LEFT-
CHAIN: The tree forms a chain, such that each
2
This is a less tight bound than the one in Bishop et al
(1998), but it is easier to handle computationally.
3
l
e
a
r
n
i
n
g
s
e
t
t
i
n
g
LEFTCHAIN 34.9
vanilla EM 38.3
LN, mean-field 48.9
This paper: I II III
RIGHTATTACH 49.1 47.1 49.8
ALLRIGHT 49.4 49.4 48.4
LEFTCHAIN 47.9 46.5 49.9
VERBASROOT 50.5 50.2 49.4
NOUNSEQUENCE 48.9 48.9 49.9
SHORTDEP 49.5 48.4 48.4
RA+VAR+SD 50.5 50.6 50.1
Table 1: Results (attachment accuracy). The baselines are
LEFTCHAIN as a parsing model (attaches each word to the
word on its right), non-Bayesian EM, and mean-field vari-
ational EM without any constraints. These are compared
against the six mixture components mentioned in the text. (I)
corresponds to simplex annealing experiments (?
(0)
1
= 0.85);
(II?III) correspond to fixed values, 0.85 and 0.95, for the
mixture coefficients. With the last row, ?
2
to ?
4
are always
(1? ?
1
)/3. Boldface denotes the best result in each row.
word is governed by the word to its right; (4) VER-
BASROOT: Only verbs can attach to the wall node
$; (5) NOUNSEQUENCE: Every sequence of nNN
(nouns) is assumed to be a noun phrase, hence the
first n?1 NNs are attached to the last NN; and (6)
SHORTDEP: Allow only dependencies of length
four or less. This is a strict model reminiscent
of the successful application of structural bias to
grammar induction (Smith and Eisner, 2006).
These components are added to a variational
DMV model without the sum-to-1 constraint on
?. This complements variational techniques which
state that the optimal solution during the E-step
for the mean-field variational EM algorithm is a
weighted grammar of the same form of p(x,y | ?)
(DMV in our case). Using the mixture compo-
nents this way has the effect of smoothing the esti-
mated grammar event counts during the E-step, in
the direction of some prior expectations.
Let ?
1
correspond to the component of the orig-
inal DMV model, and let ?
2
correspond to one of
the components from the above list. Variational
techniques show that if we let ?
1
obtain the value
1, then the optimal solution will be ?
1
= 1 and
?
2
= 0. We therefore restrict ?
1
to be smaller than
1. More specifically, we use an annealing process
which starts by limiting ?
1
to be ? s = 0.85 (and
hence limits ?
2
to be ? 0.15) and increases s at
each step by 1% until s reaches 0.95. In addition,
we also ran the algorithm with ?
1
fixed at 0.85 and
?
1
fixed at 0.95 to check the effectiveness of an-
nealing on the simplex.
Table 1 describes the results of our experi-
ments. In general, using additional mixture com-
ponents has a clear advantage over the mean-field
assumption. The best result with a single mix-
ture is achieved with annealing, and the VERBAS-
ROOT component. A combination of the mix-
tures (RIGHTATTACH) together with VERBAS-
ROOT and SHORTDEP led to an additional im-
provement, implying that proper selection of sev-
eral mixture components together can achieve a
performance gain.
4 Conclusion
We described a variational EM algorithm that uses
a mixture model for the variational model. We
refined the algorithm with an annealing mecha-
nism to avoid local maxima. We demonstrated
the effectiveness of the algorithm on a dependency
grammar induction task. Our results show that
with a good choice of mixture components and
annealing schedule, we achieve improvements for
this task over mean-field variational inference.
References
M. J. Beal and Z. Gharamani. 2003. The variational
Bayesian EM algorithm for incomplete data: with appli-
cation to scoring graphical model structures. In Proc. of
Bayesian Statistics.
C. Bishop, N. Lawrence, T. S. Jaakkola, and M. I. Jordan.
1998. Approximating posterior distributions in belief net-
works using mixtures. In Advances in NIPS.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
S. Boyd and L. Vandenberghe. 2004. Convex Optimization.
Cambridge Press.
S. B. Cohen and N. A. Smith. 2009. Variational inference
with prior knowledge. Technical report, Carnegie Mellon
University.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logis-
tic normal priors for unsupervised probabilistic grammar
induction. In Advances in NIPS.
J. V. Grac?a, K. Ganchev, and B. Taskar. 2007. Expectation
maximization and posterior constraints. In Advances in
NIPS.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proc. of ACL.
K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical me-
chanics and phrase transitions in clustering. Physical Re-
view Letters, 65(8):945?948.
N. A. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In Proc. of
COLING-ACL.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn
Chinese Treebank: Phrase structure annotation of a large
corpus. Natural Language Engineering, 10(4):1?30.
4
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 101?104,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Leveraging Structural Relations for Fluent Compressions                    
at Multiple Compression Rates 
 
Sourish Chaudhuri, Naman K. Gupta, Noah A. Smith, Carolyn P. Ros? 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA-15213, USA. 
{sourishc, nkgupta, nasmith, cprose}@cs.cmu.edu 
 
Abstract 
Prior approaches to sentence compression 
have taken low level syntactic constraints into 
account in order to maintain grammaticality. 
We propose and successfully evaluate a more 
comprehensive, generalizable feature set that 
takes syntactic and structural relationships into 
account in order to sustain variable compres-
sion rates while making compressed sentences 
more coherent, grammatical and readable.  
1 Introduction 
We present an evaluation of the effect of syntac-
tic and structural constraints at multiple levels of 
granularity on the robustness of sentence com-
pression at varying compression rates.  Our eval-
uation demonstrates that the new feature set pro-
duces significantly improved compressions 
across a range of compression rates compared to 
existing state-of-the-art approaches. Thus, we 
name our system for generating compressions the 
Adjustable Rate Compressor (ARC).   
Knight and Marcu (2000) (K&M, henceforth) 
presented two approaches to the sentence com-
pression problem: one using a noisy channel 
model, the other using a decision-based model. 
The performances of the two models were com-
parable though their experiments suggested that 
the noisy channel model degraded more smooth-
ly than the decision-based model when tested on 
out-of-domain data. Riezler et al (2003) applied 
linguistically rich LFG grammars to a sentence 
compression system. Turner and Charniak (2005) 
achieved similar performance to K&M using an 
unsupervised approach that induced rules from 
the Penn Treebank.  
A variety of feature encodings have previous-
ly been explored for the problem of sentence 
compression.  Clarke and Lapata (2007) included 
discourse level features in their framework to 
leverage context for enhancing coherence. 
McDonald?s (2006) model (M06, henceforth) is 
similar to K&M except that it uses discriminative 
online learning to train feature weights. A key 
aspect of the M06 approach is a decoding algo-
rithm that searches the entire space of compres-
sions using dynamic programming to choose the 
best compression (details in Section 2). We use 
M06 as a foundation for this work because its 
soft constraint approach allows for natural inte-
gration of additional classes of features. Similar 
to most previous approaches, our approach com-
presses sentences by deleting words only. 
The remainder of the paper is organized as 
follows. Section 2 discusses the architectural 
framework.  Section 3 describes the innovations 
in the proposed model. We conclude after pre-
senting the results of our evaluation in Section 4. 
2 Experimental Paradigm 
Supervised approaches to sentence compression 
typically use parallel corpora consisting of origi-
nal and compressed sentences (paired corpus, 
henceforth). In this paper, we will refer to these 
pairs as a 2-tuple <x, y>, where x is the original 
sentence and y is the compressed sentence. 
We implemented the M06 system as an expe-
rimental framework in which to conduct our in-
vestigation. The system uses as input the paired 
corpus, the corresponding POS tagged corpus, 
the paired corpus parsed using the Charniak 
parser (Charniak, 2000), and dependency parses 
from the MST parser (McDonald et al, 2005). 
Features are extracted over adjacent pairs of 
words in the compressed sentence and weights 
are learnt at training time using the MIRA algo-
rithm (Crammer and Singer, 2003). We decode 
as follows to find the best compression:  
Let the score of a compression y for a sen-
tence x be s(x, y). This score is factored using a 
first-order Markov assumption over the words in 
the compressed sentence, and is defined by the 
dot product between a high dimensional feature 
representation and a corresponding weight vector 
(for details, refer to McDonald, 2006). The equa-
tions for decoding are as follows: 
 
1),,,(][max][
0.0]1[
iijxsjCiC
C
ij
 
101
where C is the dynamic programming table and 
C[i] represents the highest score for compres-
sions ending at word i for the sentence x. 
The M06 system takes the best scoring com-
pression from the set of all possible compres-
sions.  In the ARC system, the model determines 
the compression rate and enforces a target com-
pression length by altering the dynamic pro-
gramming algorithm as suggested by M06: 
 
1,]][1[
0.0]1][1[
rrC
C  
,1i  
),,(]1][[max]][[ ijxsrjCriC ij
 
 
where C is the dynamic programming table as 
before and C[i][r] is the score for the best com-
pression of length r that ends at position i in the 
sentence x. This algorithm runs in O (n2r) time.  
We define the rate of human generated com-
pressions in the training corpus as the gold stan-
dard compression rate (GSCR). We train a linear 
regression model over the training data to predict 
the GSCR for a sentence based on the ratio be-
tween the lengths of each compressed-original 
sentence pair in the training set. The predicted 
compression rate is used to force the system to 
compress sentences in the test set to a specific 
target length. Based on the computed regression, 
the formula for computing the Predicted Com-
pression Rate (PCR) from the Original Sentence 
Length (OSL) is as follows: 
 
OSLPCR 004.086.0  
 
In our work, enforcing specific compression 
rates serves two purposes. First, it allows us to 
make a more controlled comparison across ap-
proaches, since variation in compression rate 
across approaches confounds comparison of oth-
er aspects of performance.  Second, it allows us 
to investigate how alternative models work at 
higher compression rates. Here our primary con-
tribution is of robustness of the approach with 
respect to alternative feature spaces and com-
pression rates. 
3 Extended Feature Set 
A major focus of our work is the inclusion of 
new types of features derived from syntactic ana-
lyses in order to make the resulting compressions 
more grammatical and thus increase the versatili-
ty of the resulting compression models.   
The M06 system uses features extracted from 
the POS tagged paired corpus: POS bigrams, 
POS context of the words added to or dropped 
from the compression, and other information 
about the dropped words. For a more detailed 
description, please refer to McDonald, 2006.   
From the phrase structure trees, M06 extracts 
context information about nodes that subsume 
dropped words. These features attempt to ap-
proximately encode changes in the grammar 
rules between source and target sentences. De-
pendency features include information about the 
dropped words? parents as well as conjunction 
features of the word and the parent. 
Our extensions to the M06 feature set are in-
spired by an analysis of the compressions gener-
ated by it, and allow for a richer encoding of 
dropped words and phrases using properties of 
the words and their syntactic relations to the rest 
of the sentence. Consider this example (dropped 
words are marked as such):  
 
* 68000 Sweden AB of Uppsala , Sweden , intro-
duced the TeleServe , an integrated answering 
machine and voice-message handler that links a 
Macintosh to Touch-Tone phones . 
  
Note in the above example that the syntactic 
head of the sentence introduced has been 
dropped. Using the dependency parse, we add a 
class of features to be learned during training that 
lets the system decide when to drop the syntactic 
head of the sentence. Also note that answering 
machine in the original sentence was preceded 
by an while the word the was used with Tele-
serve (dropped in the compression). While POS 
information helps the system to learn that the 
answering machine is a good POS sequence, we 
do not have information that links the correct 
article to the noun. Information from the depen-
dency parse allows us to learn when we can drop 
words whose heads are retained and when we 
can drop a head and still retain the dependent.  
Now, consider the following example: 
 
Examples for editors are applicable to awk pat-
terns , grep and egrep .  
 
    Here, Examples has been dropped, while for 
editors which has Examples as a head is retained. 
Besides, in the sequence, editors are applica-
ble?, the word editors behaves as the subject of 
are although the correct compression would have 
examples as its subject. A change in the argu-
ments of the verbs will distort the meaning of the 
sentence. We augmented the feature set to in-
clude a class of features about structural informa-
tion that tells us when the subject (or object) of a 
verb can be dropped while the verb itself is re-
tained. Thus, now if the system does retain the 
102
are, it is more likely to retain the correct argu-
ments of the word from the original sentence. 
    The new classes of features use only the de-
pendency labels generated by the parser and are 
not lexicalized. Intuitively, these features help 
create units within the sentences that are tightly 
bound together, e.g., a subject and an object with 
its parent verb. We notice, as one would expect, 
that some dependency bindings are less strong 
than others. For instance, when faced with a 
choice, our system drops a relative pronoun thus 
breaking the dependency between the retained 
noun and the relative pronoun, rather than drop 
the noun, which was the retained subject. 
Below is a summary of the information that 
the new features in our system encode: 
[Parent-Child]- When a word is dropped, is its 
parent retained in the compression?  
[Dependent]- When a word is dropped, are 
other words dependent on it (its children) 
also dropped or are they retained?  
[Verb-Arg]- Information from the dependency 
parse about the subjects and objects of 
verbs can be used to encode more specific 
features (similar to the above) that say 
whether or not the subject (or object) was 
retained when the verb was dropped.  
[Sent-Head-Dep]- Is the syntactic head of a 
sentence dropped? 
4 Evaluation 
We evaluate our model in comparison with M06. 
At training time, compression rates were not en-
forced on the ARC or M06 model. Our evalua-
tion demonstrates that the proposed feature set 
produces more grammatical sentences across 
varying compression rates.  In this section, 
GSCR denotes gold standard compression rate 
(i.e., the compression rate found in training data), 
CR denotes compression rate.   
4.1 Corpora 
Sentence compression systems have been tested 
on product review data from the Ziff-Davis (ZD, 
henceforth) Corpus by Knight and Marcu (2000), 
general news articles by Clarke and Lapata (CL, 
henceforth) corpus (2007) and biomedical ar-
ticles (Lin and Wilbur, 2007). To evaluate our 
system, we used 2 test sets: Set 1 contained 50 
sentences; all 32 sentences from the ZD test set 
and 18 additional sentences chosen randomly 
from the CL test set; Set 2 contained 40 sen-
tences selected from the CL corpus, 20 of which 
were compressed at 75% of GSCR and 20 at 
50% of GSCR (the percentages denote the en-
forced compression rates). 
Three examples comparing compressed sen-
tences are given below:  
 
 
Original: Like FaceLift, much of ATM 's screen 
performance depends on the underlying applica-
tion. 
Human: Much of ATM 's performance depends 
on the underlying application . 
M06: 's screen performance depends on applica-
tion  
ARC: ATM 's screen performance depends on 
the underlying application . 
 
Original: The discounted package for the Sparc-
server 470 is priced at $89,900 , down from the 
regular $107,795 . 
Human: The Sparcserver 470 is priced at 
$89,900 , down from the regular $107,795 . 
M06: Sparcserver 470 is $89,900 regular 
$107,795 
ARC: The discounted package is priced at 
$89,900 , regular $107,795 .  
 
 
The example below has compressions at 50% 
compression rate for M06 and ARC systems: 
 
 
Original: Cutbacks in local defence establish-
ments is also a factor in some constituencies . 
M06: establishments is a factor in some consti-
tuencies . 
ARC: Cutbacks is a factor in some constituen-
cies .  
 
 
Note that the subject of is is correctly retained 
in the ARC system. 
4.2 User Study 
In order to evaluate the effect of the features that 
we added to create the ARC model, we con-
ducted a user study, adopting an experimental 
methodology similar to that used by K&M and 
M06.  Each of four human judges, who were na-
tive speakers of English and not involved in the 
research we report in this paper, were instructed 
to rate two different sets of compressions along 
two dimensions, namely Grammaticality and 
Completeness, on a scale of 1 to 5. We chose to 
replace Importance (used by K&M), which is a 
task specific and possibly user specific notion, 
with the more general notion of Completeness, 
defined as the extent to which the compressed 
sentence is a complete sentence and communi-
cates the main idea of the original sentence.  
For Set 1, raters were given the original sen-
tence and 4 compressed versions (presented in 
103
random order as in the M06 evaluation): the hu-
man compression, the compression produced by 
the original M06 system, the compression from 
the M06 system with GSCR, and the ARC sys-
tem with GSCR. For Set 2, raters were given the 
original sentence, this time with two compressed 
versions, one from the M06 system and one from 
the ARC system, which were presented in a ran-
dom order.  Table 1 presents all the results in 
terms of human ratings of Grammaticality and 
Completeness as well as automatically computed 
ROUGE F1 scores (Lin and Hovy, 2003). The 
scores in parentheses denote standard deviations. 
 
 Grammati-
cality 
(Human 
Scores) 
Com-
pleteness 
(Human 
Scores) 
 
ROUGE 
F1 
Gold 
Standard 
4.60 (0.69) 3.80(.99) 1.00 (0) 
ARC 
(GSCR) 
3.70 (1.10) 3.50(1.10) .72 (.18) 
M06 3.50 (1.30) 3.10(1.30) .70 (.20) 
M06 
(GSCR) 
3.10 (1.10) 3.10(1.10) .71 (.18) 
ARC 
(75%CR) 
2.60 (1.10) 2.60(1.10) .72 (.14) 
M06 
(75%CR) 
2.20 (1.20) 2.00(1.00) .67 (.20) 
ARC 
(50%CR) 
2.30 (1.30) 1.90(1.00) .54 (.22) 
M06 
(50%CR) 
1.90 (1.10) 1.80(1.00) .58 (.22) 
Table 1: Results of human judgments and ROUGE F1 
 
 ROUGE scores were determined to have a 
significant positive correlation both with Gram-
maticality (R = .46, p < .0001) and Completeness 
(R = .39, p < .0001) when averaging across the 4 
judges? ratings.  On Set 1, a 2-tailed paired t-test 
reveals similar patterns for Grammaticality and 
Completeness: the human compressions are sig-
nificantly better than any of the systems.  ARC is 
significantly better than M06, both with enforced 
GSCR and without. M06 without GSCR is sig-
nificantly better than M06 with GSCR.  In Set 2 
(with 75% and 50% GSCR enforced), the quality 
of compressions degrade as compression rate is 
made more severe; however, the ARC model 
consistently outperforms the M06 model with a 
statistically significant margin across compres-
sion rates on both evaluation criteria. 
5 Conclusions and Future Work 
In this paper, we designed a set of new classes of 
features to generate better compressions, and 
they were found to produce statistically signifi-
cant improvements over the state-of-the-art. 
However, although the user study demonstrates 
the expected positive impact of grammatical fea-
tures, an error analysis (Gupta et al, 2009) re-
veals some limitations to improvements that can 
be obtained using grammatical features that refer 
only to the source sentence structure, since the 
syntax of the source sentence is frequently not 
preserved in the gold standard compression. In 
our future work, we hope to explore alternative 
approaches that allow reordering or paraphrasing 
along with deleting words to make compressed 
sentences more grammatical and coherent. 
 
Acknowledgments 
The authors thank Kevin Knight and Daniel 
Marcu for sharing the Ziff-Davis corpus as well 
as the output of their systems, and the anonym-
ous reviewers for their comments. This work was 
supported by the Cognitive and Neural Sciences 
Division, grant number N00014-00-1-0600. 
References  
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. of  NAACL. 
James Clarke and Mirella Lapata, 2007. Modelling 
Compression With Discourse Constraints. In Proc. 
of EMNLP-CoNLL. 
Koby Crammer and Y. Singer. 2003. Ultraconserva-
tive online algorithms for multi-class problems. 
JMLR. 
Naman K. Gupta, Sourish Chaudhuri and Carolyn P. 
Ros?, 2009. Evaluating the Syntactic Transforma-
tions in Gold Standard Corpora for Statistical Sen-
tence Compression . In Proc. of HLT-NAACL. 
Kevin Knight and Daniel Marcu. 2000. Statistics-
Based Summarization ? Step One: Sentence Com-
pression. In Proc. of AAAI. 
Jimmy Lin and W. John Wilbur. 2007. Syntactic sen-
tence compression in the biomedical domain: faci-
litating access to related articles. Information Re-
trieval, 10(4):393-414. 
Chin-Yew Lin and Eduard H. Hovy 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proc. of HLT-NAACL. 
Ryan McDonald, 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proc. of 
EACL.  
Ryan McDonald, Koby Crammer, and Fernando Pe-
reira. 2005. Online large-margin training of depen-
dency parsers. In Proc.of ACL. 
S. Riezler, T. H. King, R. Crouch, and A. Zaenen.  
2003. Statistical sentence condensation using am-
biguity packing and stochastic disambiguation me-
thods for lexical-functional grammar. In Proc. of 
HLT-NAACL. 
104
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 97?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Competitive Grammar Writing?
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
Just as programming is the traditional introduction to
computer science, writing grammars by hand is an ex-
cellent introduction to many topics in computational lin-
guistics. We present and justify a well-tested introductory
activity in which teams of mixed background compete
to write probabilistic context-free grammars of English.
The exercise brings together symbolic, probabilistic, al-
gorithmic, and experimental issues in a way that is acces-
sible to novices and enjoyable.
1 Introduction
We describe a hands-on group activity for novices
that introduces several central topics in computa-
tional linguistics (CL). While the task is intellec-
tually challenging, it requires no background other
than linguistic intuitions, no programming,1 and
only a very basic understanding of probability.
The activity is especially appropriate for mixed
groups of linguists, computer scientists, and others,
letting them collaborate effectively on small teams
and learn from one another. A friendly competition
among the teams makes the activity intense and en-
joyable and introduces quantitative evaluation.
1.1 Task Overview
Each 3-person team is asked to write a generative
context-free grammar that generates as much of En-
? This work was supported by NSF award 0121285,
?ITR/IM+PE+SY: Summer Workshops on Human Language
Technology: Integrating Research and Education,? and by a
Fannie and John Hertz Foundation Fellowship to the second
author. We thank David A. Smith and Markus Dreyer for co-
leading the lab in 2004?2007 and for implementing various im-
provements in 2004?2007 and for providing us with data from
those years. The lab has benefited over the years from feedback
from the participants, many of whom attended the JHU sum-
mer school thanks to the generous support of NAACL. We also
thank the anonymous reviewers for helpful comments.
1In our setup, students do need the ability to invoke scripts
and edit files in a shared directory, e.g., on a Unix system.
glish as possible (over a small fixed vocabulary).
Obviously, writing a full English grammar would
take years even for experienced linguists. Thus each
team will only manage to cover a few phenomena,
and imperfectly.
To encourage precision but also recall and lin-
guistic creativity, teams are rewarded for generating
sentences that are (prescriptively) grammatical but
are not anticipated by other teams? grammars. This
somewhat resembles scoring in the Boggle word
game, where players are rewarded for finding valid
words in a grid that are not found by other players.
A final twist is that the exercise uses probabilistic
context-free grammars (PCFGs); the actual scoring
methods are based on sampling and cross-entropy.
Each team must therefore decide how to allocate
probability mass among sentences. To avoid assign-
ing probability of zero when attempting to parse an-
other team?s sentences, a team is allowed to ?back
off? (when parsing) to a simpler probability model,
such as a part-of-speech bigram model, also ex-
pressed as a PCFG.
1.2 Setting
We have run this activity for six consecutive years,
as a laboratory exercise on the very first afternoon
of an intensive 2-week summer school on various
topics in human language technology.2 We allot 3.5
hours in this setting, including about 15 minutes for
setup, 30 minutes for instructions, 15 minutes for
evaluation, and 30 minutes for final discussion.
The remaining 2 hours is barely enough time for
team members to get acquainted, understand the re-
quirements, plan a strategy, and make a small dent in
2This 2-week course is offered as a prelude to the Johns
Hopkins University summer research workshops, sponsored by
the National Science Foundation and the Department of De-
fense. In recent years the course has been co-sponsored by the
North American ACL.
97
the problem. Nonetheless, participants consistently
tell us that the exercise is enjoyable and pedagogi-
cally effective, almost always voting to stay an extra
hour to make further progress.
Our 3-person teams have consisted of approxi-
mately one undergraduate, one junior graduate stu-
dent, and one more senior graduate student. If pos-
sible, each team should include at least one member
who has basic familiarity with some syntactic phe-
nomena and phrasal categories. Teams that wholly
lack this experience have been at a disadvantage in
the time-limited setting.
1.3 Resources for Instructors
We will maintain teaching materials at http:
//www.clsp.jhu.edu/grammar-writing,
for both the laboratory exercise version and for
homework versions: scripts, data, instructions for
participants, and tips for instructors. While our
materials are designed for participants who are
fluent in English, we would gladly host translations
or adaptations into other languages, as well as other
variants and similar assignments.
2 Why Grammar Writing?
A computer science curriculum traditionally starts
with programming, because programming is acces-
sible, hands-on, and necessary to motivate or under-
stand most other topics in computer science. We be-
lieve that grammar writing should play the same role
in computational linguistics?as it often did before
the statistical revolution3?and for similar reasons.
Grammar writing remains central because many
theoretical and applied CL topics center around
grammar formalisms. Much of the field tries to de-
sign expressive formalisms (akin to programming
languages); solve linguistic problems within them
(akin to programming); enrich them with probabil-
ities; process them with efficient algorithms; learn
them from data; and connect them to other modules
in the linguistic processing pipeline.
3The first author was specifically inspired by his experience
writing a grammar in Bill Woods?s NLP course at Harvard in
1987. An anonymous reviewer remarks that such assignments
were common at the time. Our contributions are to introduce
statistical and finite-state elements, to make the exercise into a
game, and to provide reusable instructional materials.
Of course, there are interesting grammar for-
malisms at all levels of language processing. One
might ask why syntax is a good level at which to be-
gin education in computational linguistics.
First, starting with syntax establishes at the start
that there are formal and computational methods
specific to natural language. Computational linguis-
tics is not merely a set of applied tasks to be solved
with methods already standardly taught in courses
on machine learning, theory of computation,4 or
knowledge representation.
Second, we have found that syntax captures stu-
dents? interest rapidly. They quickly appreciate the
linguistic phenomena, see that they are non-trivial,
and have little trouble with the CFG formalism.
Third, beginning specifically with PCFGs pays
technical dividends in a CL course. Once one un-
derstands PCFG models, it is easy to understand the
simpler finite-state models (including n-gram mod-
els, HMMs, etc.) and their associated algorithms, ei-
ther by analogy or by explicit reduction to special
cases of PCFGs. CFGs are also a good starting point
for more complex syntactic formalisms (BNF, cate-
gorial grammars, TAG, LFG, HPSG, etc.) and for
compositional semantics. Indeed, our exercise mo-
tivates these more complex formalisms by forcing
students to work with the more impoverished PCFG
formalism and experience its limitations.
3 Educational Goals of the Exercise
Our grammar-writing exercise is intended to serve
as a touchstone for discussion of many subsequent
topics in NLP and CL (which are italicized below).
As an instructor, one can often refer back later to the
exercise to remind students of their concrete experi-
ence with a given concept.
Generative probabilistic models. The first set of
concepts concerns language models. These are eas-
iest to understand as processes for generating text.
Thus, we give our teams a script for generating ran-
dom sentences from their grammar and their backoff
4Courses on theory of computation do teach pushdown au-
tomata and CFGs, of course, but they rarely touch on parsing
or probabilistic grammars, as this exercise does. Courses on
compilers may cover parsing algorithms, but only for restricted
grammar families such as unambiguous LR(1) grammars.
98
model?a helpful way to observe the generative ca-
pacity and qualitative behavior of any model.
Of course, in practice a generative grammar is
most often run ?backwards? to parse an observed
sentence or score its inside probability, and we also
give the teams a script to do that. Most teams do ac-
tually run these scripts repeatedly to test their gram-
mars, since both scripts will be central in the evalua-
tion (where sentences are randomly generated from
one grammar and scored with another grammar).
It is common for instructors of NLP to show ex-
amples of randomly-generated text from an n-gram
model (e.g., Jurafsky and Martin, 2000, pp. 202?
203), yet this amusing demonstration may be misin-
terpreted as merely illustrating the inadequacy of n-
gram models. Our use of a hand-crafted PCFG com-
bined with a bigram-based (HMM) backoff grammar
demonstrates that although the HMM is much worse
at generating valid English sentences (precision), it
is much better at robustly assigning nonzero proba-
bility when analyzing English sentences (recall).
Finally, generative models do more than assign
probability. They often involve linguistically mean-
ingful latent variables, which can be recovered
given the observed data. Parsing with an appropri-
ate PCFG thus yields a intuitive and useful analy-
sis (a syntactic parse tree), although only for the
sentences that the PCFG covers. Even parsing with
the simple backoff grammar that we initially provide
yields some coarser analysis, in the form of a part-
of-speech tagging, since this backoff grammar is a
right-branching PCFG that captures part-of-speech
bigrams (for details see ?1.1, ?4.1, and Table 2). In
fact, parsing with the backoff PCFG is isomorphic to
Viterbi decoding in an HMM part-of-speech tagger,
a topic that is commonly covered in NLP courses.
Modeling grammaticality. The next set of con-
cepts concerns linguistic grammaticality. During the
evaluation phase of our exercise (see below), stu-
dents must make grammaticality judgments on other
teams? randomly generated sentences?which are
usually nonsensical, frequently hard for humans to
parse, and sometimes ungrammatical. This concrete
task usually prompts questions from students about
how grammaticality ought to be defined, both for
purposes of the task and in principle. It could also
be used to discuss why some of the sentences are
so hard for humans to understand (e.g., garden-path
and frequency effects) and what parsing strategies
humans or machines might use.
The exercise of modeling grammaticality with
the CFG formalism, a formalism that appears else-
where in the computer science curriculum, high-
lights some important differences between natural
languages and formal languages. A natural lan-
guage?s true grammar is unknown (and may not even
exist: perhaps the CFG formalism is inadequate).
Rather, a grammar must be induced or constructed
as an approximate model of corpus data and/or cer-
tain native-speaker intuitions. A natural language
also differs from a programming language in includ-
ing ambiguous sentences. Students observe that the
parser uses probabilities to resolve ambiguity.
Linguistic analysis. Grammar writing is an ex-
cellent way to get students thinking about linguis-
tic phenomena (e.g., adjuncts, embedded sentences,
wh-questions, clefts, point absorption of punctuation
marks). It also forces students to think about appro-
priate linguistic formalisms. Many phenomena are
tedious to describe within CFGs (e.g., agreement,
movement, subcategorization, selectional restric-
tions, morphological inflection, and phonologically-
conditioned allomorphy such as a vs. an). They
can be treated in CFGs only with a large number of
repetitive rules. Students appreciate these problems
by grappling with them, and become very receptive
to designing expressive improvements such as fea-
ture structures and slashed categories.
Parameter tuning. Students observe the effects
of changing the rule probabilities by running the
scripts. For example, teams often find themselves
generating unreasonably long (or even infinite) sen-
tences, and must damp down the probabilities of
their recursive rules. Adjusting the rule probabilities
can also change the score and optimal tree that are
returned by the parser, and can make a big difference
in the final evaluation (see ?5). This appreciation for
the role of numerical parameters helps motivate fu-
ture study of machine learning in NLP.
Quantitative evaluation. As an engineering pur-
suit, NLP research requires objective evaluation
measures to know how well systems work.
Our first measure is the precision of each team?s
99
probabilistic grammar: how much of its probability
mass is devoted to sentences that are truly grammat-
ical? Estimating this requires human grammaticality
judgments on a random sample C of sentences gen-
erated from all teams? grammars. These binary judg-
ments are provided by the participants themselves,
introducing the notion of linguistic annotation (al-
beit of a very simple kind). Details are in ?4.3.3.
Our second measure is an upper-bound approx-
imation to cross-entropy (or log-perplexity?in ef-
fect, the recall of a probability model): how well
does each team?s probabilistic model (this time in-
cluding the backoff model of ?1.1) anticipate unseen
data that are truly grammatical? (Details in ?4.3.3.)
Note that in contrast to parsing competitions, we
do not evaluate the quality of the parse trees (e.g.,
PARSEVAL). Our cross-entropy measure evaluates
only the grammars? ability to predict word strings
(language modeling). That is because we impose no
annotation standard for parse trees: each team is free
to develop its own theory of syntax. Furthermore,
many sentences will only be parsable by the backoff
grammar (e.g., a bigram model), which is not ex-
pected to produce a full syntactic analysis.
The lesson about cross-entropy evaluation is
slightly distorted by our peculiar choice of test data.
In principle, the instructors might prepare a batch
of grammatical sentences ahead of time and split
them into a test set (used to evaluate cross-entropy
at the end) and a development set (provided to the
students at the start, so that they know which gram-
matical phenomena are important to handle). The
activity could certainly be run in this way to demon-
strate proper experimental design for evaluating a
language model (discussed further in ?5 and ?6).
We have opted for the more entertaining ?Boggle-
style? evaluation described in ?1.1, where teams try
to stump one another by generating difficult test
data, using the fixed vocabulary. Thus, we evaluate
each team?s cross-entropy on all grammatical sen-
tences in the collection C, which was generated ex
post facto from all teams? grammars.
4 Important Details
4.1 Data
A few elements are provided to participants to get
them started on their grammars.
Vocabulary. The terminal vocabulary ? consists
of words from early scenes of the film Monty Python
and the Holy Grail along with some inflected forms
and function words, for a total vocabulary of 220
words. For simplicity, only 3rd-person pronouns,
nouns, and verbs are included. All words are case-
sensitive for readability (as are the grammar nonter-
minals), but we do not require or expect sentence-
initial capitalization.
All teams are restricted to this vocabulary, so that
the sentences that they generate will not frustrate
other teams? parsers with out-of-vocabulary words.
However, they are free to use words in unexpected
ways (e.g., using castle in its verbal sense from
chess, or building up unusual constructions with the
available function words).
Initial lexicon. The initial lexical rules take the
form T ? w, where w ? ?+ and T ? T , with
T being a set of six coarse part-of-speech tags:
Noun: 21 singular nouns starting with consonants
Det: 9 singular determiners
Prep: 14 prepositions
Proper: 8 singular proper nouns denoting people
(including multiwords such as Sir Lancelot)
VerbT: 6 3rd-person singular present transitive
verbs
Misc: 183 other words, divided into several com-
mented sections in the grammar file
Students are free to change this tagset. They
are especially encouraged to refine the Misc tag,
which includes 3rd-person plural nouns (including
some proper nouns), 3rd-person pronouns (nomina-
tive, accusative, and genitive), additional 3rd-person
verb forms (plural present, past, stem, and partici-
ples), verbs that cannot be used transitively, modals,
adverbs, numbers, adjectives (including some com-
parative and superlative forms), punctuation, coor-
dinating and subordinating conjunctions, wh-words,
and a few miscellaneous function words (to, not, ?s).
The initial lexicon is ambiguous: some words are
associated with more than one tag. Each rule has
weight 1, meaning that a tag T is equally likely to
rewrite as any of its allowed nonterminals.
Initial grammar. We provide the ?S1? rules in Ta-
ble 1, so that students can try generating and parsing
100
1 S1 ? NP VP .
1 VP ? VerbT NP
20 NP ? Det Nbar
1 NP ? Proper
20 Nbar ? Noun
1 Nbar ? Nbar PP
1 PP ? Prep NP
Table 1: The S1 rules: a starting point for building an En-
glish grammar. The start symbol is S1. The weights in
the first column will be normalized into generative proba-
bilities; for example, the probability of expanding a given
NP with NP ? Det Nbar is actually 20/(20 + 1).
1 S2 ?
1 S2 ? Noun
1 S2 ? Misc
1 Noun ? Noun
1 Noun ? Noun Noun
1 Noun ? Noun Misc
1 Misc ? Misc
1 Misc ? Misc Noun
1 Misc ? Misc Misc
Table 2: The S2 rules (simplified here where T =
{Noun,Misc}): a starting point for a backoff grammar.
The start symbol is S2. The Noun nonterminal gener-
ates those phrases that start with Nouns. Its 3 rules mean
that following a Noun, there is 1/3 probability each of
stopping, continuing with another Noun (via Noun), or
continuing with a Misc word (via Misc).
sentences right away. The S1 and lexical rules to-
gether implement a very small CFG. Note that no
Misc words can yet be generated. Indeed, this ini-
tial grammar will only generate some simple gram-
matical SVO sentences in singular present tense, al-
though they may be unboundedly long and ambigu-
ous because of recursion through Nbar and PP.
Initial backoff grammar. The provided ?S2?
grammar is designed to assign positive probability
to any string in ?? (see ?1.1). At least initially, this
PCFG generates only right-branching structures. Its
nonterminals correspond to the states of a weighted
finite-state machine, with start state S2 and one state
per element of T (the coarse parts of speech listed
above). Table 2 shows a simplified version.
From each state, transition into any state except
the start state S2 is permitted, and so is stopping.
These rules can be seen as specifying the transitions
Arthur is the king .
Arthur rides the horse near the castle .
riding to Camelot is hard .
do coconuts speak ?
what does Arthur ride ?
who does Arthur suggest she carry ?
are they suggesting Arthur ride to Camelot ?
Guinevere might have known .
it is Sir Lancelot who knows Zoot !
neither Sir Lancelot nor Guinevere will speak of it .
the Holy Grail was covered by a yellow fruit .
do not speak !
Arthur will have been riding for eight nights .
Arthur , sixty inches , is a tiny king .
Arthur and Guinevere migrate frequently .
he knows what they are covering with that story .
the king drank to the castle that was his home .
when the king drinks , Patsy drinks .
Table 3: Example sentences. Only the first two can be
parsed by the initial S1 and lexical rules.
in a bigram hidden Markov model (HMM) on part-
of-speech tags, whose emissions are specified by the
lexical rules. Since each rule initially has weight 1,
all part-of-speech sequences of a given length are
equally likely, but these weights could be changed
to arbitrary transition probabilities.
Start rules. The initial grammar S1 and the ini-
tial backoff grammar S2 are tied together by a single
symbol START, which has two production rules:
99 START ? S1
1 START ? S2
These two rules are obligatory, but their weights
may be changed. The resulting model, rooted at
START, is a mixture of the S1 and S2 grammars,
where the weights of these two rules implement
the mixture coefficients. This is a simple form of
backoff smoothing by linear interpolation (Jelinek
and Mercer, 1980). The teams are warned to pay
special attention to these rules. If the weight of
START ? S1 is decreased relative to START ?
S2, then the model relies more heavily on the back-
off model?perhaps a wise choice for keeping cross-
entropy small, if the team has little faith in S1?s abil-
ity to parse the forthcoming data.
Sample sentences. A set of 27 example sentences
in ?+ (subset shown in Table 3) is provided for lin-
guistic inspiration and as practice data on which to
101
run the parser. Since only 2 of these sentences can
be parsed with the initial S1 and lexical rules, there
is plenty of room for improvement. A further devel-
opment set is provided midway through the exercise
(?4.3.2).
4.2 Computing Environment
We now describe how the above data are made avail-
able to students along with some software.
4.2.1 Scripts
We provide scripts that implement two important
capabilities for PCFG development. Both scripts
are invoked with a set of grammar files specified on
the command line (typically all of them, ?*.gr?).
A PCFG is obtained by concatenating these files
and stripping their comments, then normalizing their
rule weights into probabilities (see Table 1), and
finally checking that all terminal symbols of this
PCFG are legal words of the vocabulary ?.
The random generation script prints a sample of
n sentences from this PCFG. The generator can op-
tionally print trees or flat word sequences. A start
symbol other than the default S1 may be specified
(e.g., NP, S2, START, etc.), to allow participants to
test subgrammars or the backoff grammar.5
The parsing script prints the most probable parse
tree for each sentence read from a file (or from the
standard input). A start symbol may again be speci-
fied; this time the default is START. The parser also
prints each sentence?s probability, total number of
parses, and the fraction of the probability that goes
to the best parse.
Tree outputs can be pretty-printed for readability.
4.2.2 Collaborative Setup
Teams of three or four sit at adjacent workstations
with a shared filesystem. The scripts above are pub-
licly installed; a handout gives brief usage instruc-
tions. The instructor and teaching assistant roam the
room and offer assistance as needed.
Each team works in its own shared directory. The
Emacs editor will warn users who are simultane-
ously editing the same file. Individual participants
tend to work on different sub-grammar files; all of
5For some PCFGs, the stochastic process implemented by
the script has a nonzero probability of failing to terminate. This
has not caused problems to date.
a team?s files can be concatenated (as *.gr) when
the scripts are run. (The directory initially includes
separate files for the S1 rules, S2 rules, and lexi-
cal rules.) To avoid unexpected interactions among
these grammar fragments, students are advised to di-
vide work based on nonterminals; e.g., one member
of a team may claim jurisdiction over all rules of the
form VP plural ? ? ? ?.
4.3 Activities
4.3.1 Introductory Lecture
Once students have formed themselves into teams
and managed to log in at adjacent computers, we
begin with an 30-minute introductory lecture. No
background is assumed. We explain PCFGs simply
by showing the S1 grammar and hand-simulating the
action of the random sentence generator.
We explain the goal of extending the S1 gram-
mar to cover more of English. We explain how each
team?s precision will be evaluated by human judg-
ments on a sample, but point out that this measure
gives no incentive to increase coverage (recall). This
motivates the ?Boggle? aspect of the game, where
teams must also be able to parse one another?s gram-
matical sentences, and indeed assign them as high
a probability as possible. We demonstrate how the
parser assigns a probability by running it on the sen-
tence that we earlier generated by hand.6
We describe how the parser?s probabilities are
turned into a cross-entropy measure, and discuss
strategy. Finally, we show that parsing a sentence
that is not covered by the S1 grammar will lead to
infinite cross-entropy, and we motivate the S2 back-
off grammar as an escape hatch.
4.3.2 Midpoint: Development data
Once or more during the course of the exercise,
we take a snapshot of all teams? S1 grammars and
sample 50 sentences from each. The resulting col-
lection of sentences, in random order, is made avail-
able to all teams as a kind of development data.
While we do not filter for grammaticality as in the
final evaluation, this gives all participants an idea
of what they will be up against when it comes time
6The probability will be tiny, as a product of many rule prob-
abilities. But it may be higher than expected, and students are
challenged to guess why: there are additional parses beyond the
one we hand-generated, and the parser sums over all of them.
102
to parse other teams? sentences. Teams are on their
honor not to disguise the true state of their grammar
at the time of the snapshot.
4.3.3 Evaluation procedure
Grammar development ends at an announced
deadline. The grammars are now evaluated on the
two measures discussed in ?3. The instructors run a
few scripts that handle most of this work.
First, we generate a collection C by sampling 20
sentences from each team?s probabilistic grammar,
using S1 as the start symbol. (Thus, the backoff S2
grammar is not used for generation.)
We now determine, for each team, what fraction
of its 20-sentence sample was grammatical. The par-
ticipants play the role of grammaticality judges. In
our randomized double-blind procedure, each indi-
vidual judge receives (in his or her team directory)
a file of about 20 sentences from C, with instruc-
tions to delete the ungrammatical ones and save the
file, implying coarse Boolean grammaticality judg-
ments.7 The files are constructed so that each sen-
tence in C is judged by 3 different participants; a
sentence is considered grammatical if ? 2 judges
thinks that it is.
We define the test corpus C? to consist of all sen-
tences in C that were judged grammatical. Each
team?s full grammar (using START as the start sym-
bol to allow backoff) is used to parse C?. This
gives us the log2-probability of each sentence in C?;
the cross-entropy score is the sum of these log2-
probabilities divided by the length of C?.
4.3.4 Group discussion
While the teaching assistant is running the evalua-
tion scripts and compiling the results, the instructor
leads a general discussion. Many topics are possi-
ble, according to the interests of the instructor and
participants. For example: What linguistic phenom-
ena did the teams handle, and how? Was the CFG
formalism adequately expressive? How well would
it work for languages other than English?
What strategies did the teams adopt, based on the
evaluation criteria? How were the weights chosen?
7Judges are on their honor to make fair judgments rather
than attempt to judge other teams? sentences ungrammatical.
Moreover, such an attempt might be self-defeating, as they
might unknowingly be judging some of their own team?s sen-
tences ungrammatical.
cross-entropy new rules
team precision (bits/sent.) lex. other
A 0.30 35.57 202 111
B 0.00 54.01 304 80
C 0.80 38.48 179 48
D 0.25 49.37 254 186
E 0.55 39.59 198 114
F 0.00 39.56 193 37
G 0.65 40.97 71 15
H 0.30 36.53 176 9
I 0.70 36.17 181 54
J 0.00 ? 193 29
Table 4: Teams? evaluation scores in one year, and the
number of new rules (not including weight changes) that
they wrote. Only teams A and H modified the relative
weights of the START rules (they used 80/20 and 75/25,
respectively), giving them competitive perplexity scores.
(Cross-entropy in this year was approximated by an upper
bound that uses only the probability of each sentence?s
single best parse.)
How would you build a better backoff grammar?8
How would you organize a real long-term effort
to build a full English grammar? What would such a
grammar be good for? Would you use any additional
tools, data, or evaluation criteria?
5 Outcomes
Table 4 shows scores achieved in one year (2002).
A valuable lesson for the students was the impor-
tance of backoff. None but the first two of the exam-
ple sentences (Table 3) are parseable with the small
S1 grammar. Thus, the best way to reduce perplexity
was to upweight the S2 grammar and perhaps spend
a little time improving its rules or weights. Teams
that spent all of their time on the S1 grammar may
have learned a lot about linguistics, but tended to
score poorly on perplexity.
Indeed, the winning team in a later year spent
nearly all of their effort on the S2 grammar. They
placed almost all their weight on the S2 grammar,
whose rules they edited and whose parameters they
estimated from the example sentences and develop-
ment data. As for their S1 grammar, it generated
only a small set of grammatical sentences with ob-
8E.g., training the model weights, extending it to trigrams,
or introducing syntax into the S2 model by allowing it to invoke
nonterminals of the S1 grammar.
103
scure constructions that other teams were unlikely to
model well in their S1 grammars. This gave them a
100% precision score on grammaticality while pre-
senting a difficult parsing challenge to other teams.
This team gamed our scoring system, exploiting the
idiosyncrasy that S2 would be used to parse but not
to generate. (See ?3 for an alternative system.)
We conducted a post hoc qualitative survey of the
grammars from teams in 2002. Teams were not
asked to provide comments, and nonterminal nam-
ing conventions often tend to be inscrutable, but the
intentions are mostly understandable. All 10 teams
developed more fine-grained parts of speech, includ-
ing coordinating conjunctions, modal verbs, number
words, adverbs. 9 teams implemented singular and
plural features on nouns and/or verbs, and 9 imple-
mented the distinction between base, past, present,
and gerund forms of verbs (or a subset of those). 7
teams brought in other features like comparative and
superlative adjectives and personal vs. possessive
pronouns. 4 teams modeled pronoun case. Team
C created a ?location? category.
7 teams explicitly tried to model questions, of-
ten including rules for do-support; 3 of those teams
also modeled negation with do-insertion. 2 teams
used gapped categories (team D used them exten-
sively), and 7 teams used explicit X? nonterminals,
most commonly within noun phrases (following the
initial grammar). Three teams used a rudimentary
subcategorization frame model, distinguishing be-
tween sentence-taking, transitive, and intransitive
verbs, with an exploded set of production rules as
a result. Team D modeled appositives.
The amount of effort teams put into weights var-
ied, as well. Team A used 11 distinct weight values
from 1 to 80, giving 79 rules weights> 1 (next clos-
est was team 10, with 7 weight values in [1, 99] and
only 43 up-weighted rules). Most teams set fewer
than 25 rules? weights to something other than 1.
6 Use as a Homework Assignment
Two hours is not enough time to complete a good
grammar. Our participants are ambitious but never
come close to finishing what they undertake; Table 4
reflects incomplete work. Nonetheless, we believe
that the experience still successfully fulfills many of
the goals of ?2?3 in a short time, and the participants
enjoy the energy in a roomful of peers racing toward
a deadline. The fact that the task is open-ended and
clearly impossible keeps the competition friendly.
An alternative would be to allot 2 weeks or more
as a homework assignment, allowing teams to go
more deeply into linguistic issues and/or backoff
modeling techniques. A team?s grade could be
linked to its performance. In this setting, we recom-
mend limiting the team size to 1 or 2 people each,
since larger teams may not be able to find time or
facilities to work side-by-side for long.
This homework version of our exercise might
helpfully be split into two assignments:
Part 1 (non-competitive, smaller vocabulary).
?Extend the initial S1 grammar to cover a certain
small set of linguistic phenomena, as illustrated by a
development set [e.g., Table 3]. You will be eval-
uated on the cross-entropy of your grammar on a
test set that closely resembles the development set
[see ?3], and perhaps also on the acceptability of
sentences sampled from your grammar (as judged
by you, your classmates, or the instructor). You
will also receive qualitative feedback on how cor-
rectly and elegantly your grammar solves the lin-
guistic problems posed by the development set.?
Part 2 (competitive, full 220-word vocabulary).
?Extend your S1 grammar from Part 1 to generate
phenomena that stump other teams, and add an S2
grammar to avoid being stumped by them. You will
be evaluated as follows . . . [see ?4.3.3].?
We have already experimented with simpler non-
competitive grammar-writing exercises (similar to
Part 1) in our undergraduate NLP courses. Given
two weeks, even without teammates, many students
do a fine job of covering several non-trivial syntactic
phenomena. These assignments are available for use
by others (see ?1.3). In some versions, students were
asked to write their own random generator, judge
their own sentences, explain how to evaluate per-
plexity, or guess why the S2 grammar was used.
7 Conclusion
We hope that other instructors can make use of these
materials or ideas. Our competitive PCFG-writing
game touches upon many core CL concepts, is chal-
lenging and enjoyable, allows collaboration, and is
suitable for cross-disciplinary and intro courses.
104
References
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data. In
Proc. of Workshop on Pattern Recognition in Practice.
D. Jurafsky and J.H. Martin. 2000. Speech and Lan-
guage Processing. Prentice Hall.
105
Proceedings of the Third Workshop on Statistical Machine Translation, pages 9?17,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Rich Source-Side Context for Statistical Machine Translation
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We explore the augmentation of statistical ma-
chine translation models with features of the
context of each phrase to be translated. This
work extends several existing threads of re-
search in statistical MT, including the use of
context in example-based machine translation
(Carl and Way, 2003) and the incorporation of
word sense disambiguation into a translation
model (Chan et al, 2007). The context fea-
tures we consider use surrounding words and
part-of-speech tags, local syntactic structure,
and other properties of the source language
sentence to help predict each phrase?s transla-
tion. Our approach requires very little compu-
tation beyond the standard phrase extraction
algorithm and scales well to large data sce-
narios. We report significant improvements
in automatic evaluation scores for Chinese-
to-English and English-to-German translation,
and also describe our entry in the WMT-08
shared task based on this approach.
1 Introduction
Machine translation (MT) by statistical modeling of
bilingual phrases is one of the most successful ap-
proaches in the past few years. Phrase-based MT
systems are straightforward to train from parallel
corpora (Koehn et al, 2003) and, like the origi-
nal IBM models (Brown et al, 1990), benefit from
standard language models built on large monolin-
gual, target-language corpora (Brants et al, 2007).
Many of these systems perform well in competitive
evaluations and scale well to large-data situations
(NIST, 2006; Callison-Burch et al, 2007). End-to-
end phrase-based MT systems can be built entirely
from freely-available tools (Koehn et al, 2007).
We follow the approach of Koehn et al (2003),
in which we translate a source-language sentence f
into the target-language sentence e? that maximizes a
linear combination of features and weights:1
?e?, a?? = argmax
?e,a?
score(e,a,f) (1)
= argmax
?e,a?
M?
m=1
?mhm(e,a,f) (2)
where a represents the segmentation of e and f
into phrases and a correspondence between phrases,
and each hm is a R-valued feature with learned
weight ?m. The translation is typically found us-
ing beam search (Koehn et al, 2003). The weights
??1, ..., ?M ? are typically learned to directly mini-
mize a standard evaluation criterion on development
data (e.g., the BLEU score; Papineni et al, (2002))
using numerical search (Och, 2003).
Many features are used in phrase-based MT, but
nearly ubiquitous are estimates of the conditional
translation probabilities p(eji | f
`
k) and p(f
`
k | e
j
i )
for each phrase pair ?eji ,f
l
k? in the candidate sen-
tence pair.2 In this paper, we add and evaluate fea-
1In the statistical MT literature, this is often referred to as a
?log-linear model,? but since the score is normalized during nei-
ther parameter training nor decoding, and is never interpreted as
a log-probability, it is essentially a linear combination of feature
functions. Since many of the features are actually probabilities,
this linear combination is closer to a mixture model.
2We will use xji to denote the subsequence of x containing
the ith through jth elements of x, inclusive.
9
tures that condition on additional context features on
the source (f ) side:
p(eji | Phrase = f
`
k,Context = ?f
k?1
1 ,f
F
`+1, ...?)
The advantage of considering context is well-
known and exploited in the example-basedMT com-
munity (Carl and Way, 2003). Recently researchers
have begun to use source phrase context informa-
tion in statistical MT systems (Stroppa et al, 2007).
Statistical NLP researchers understand that condi-
tioning a probability model on more information is
helpful only if there are sufficient training data to ac-
curately estimate the context probabilities.3 Sparse
data are often the death of elaborate models, though
this can be remedied through careful smoothing.
In this paper we leverage the existing linear
model (Equation 2) to bring source-side context into
phrase-based MT in a way that is robust to data
sparseness. We interpret the linear model as a mix-
ture of many probability estimates based on different
context features, some of which may be very sparse.
The mixture coefficients are trained in the usual way
(?minimum error-rate training,? Och, 2003), so that
the additional context is exploited when it is useful
and ignored when it isn?t.
The paper proceeds as follows. We first review re-
lated work that enriches statistical translation mod-
els using context (?2). We then propose a set
of source-side features to be incorporated into the
translation model, including the novel use of syntac-
tic context from source-side parse trees and global
position within f (?3). We explain why analogous
target-side features pose a computational challenge
(?4). Specific modifications to the standard training
and evaluation paradigm are presented in ?5. Exper-
imental results are reported in ?6.
2 Related Work
Stroppa et al (2007) added souce-side context fea-
tures to a phrase-based translation system, including
conditional probabilities of the same form that we
use. They consider up to two words and/or POS tags
of context on either side. Because of the aforemen-
tioned data sparseness problem, they use a decision-
3An illustrative example is the debate over the use of bilex-
icalized grammar rules in statistical parsing (Gildea, 2001;
Bikel, 2004).
tree classifier that implicitly smooths relative fre-
quency estimates. The method improved over a stan-
dard phrase-based baseline trained on small amounts
of data (< 50K sentence pairs) for Italian? English
and Chinese ? English. We explore a significantly
larger space of context features, a smoothing method
that more naturally fits into the widely used, error-
driven linear model, and report a more comprehen-
sive experimental evaluation (including feature com-
parison and scaling up to very large datasets).
Recent research on the use of word-sense dis-
ambiguation in machine translation also points to-
ward our approach. For example, Vickrey et al
(2005) built classifiers inspired by those used in
word sense disambiguation to fill in blanks in
a partially-completed translation. Gime?nez and
Ma`rquez (2007) extended the work by considering
phrases and moved to full translation instead of fill-
ing in target-side blanks. They trained an SVM for
each source language phrase using local features of
the sentences in which the phrases appear. Carpuat
and Wu (2007) and Chan et al (2007) embedded
state-of-the-art word sense disambiguation modules
into statistical MT systems, achieving performance
improvements under several automatic measures for
Chinese ? English translation.
Our approach is also reminiscent of example-
based machine translation (Nagao, 1984; Somers,
1999; Carl and Way, 2003), which has for many
years emphasized use of the context in which source
phrases appear when translating them. Indeed, like
the example-based community, we do not begin with
any set of assumptions about which kinds of phrases
require additional disambiguation (cf. the applica-
tion of word-sense disambiguation, which is moti-
vated by lexical ambiguity). Our feature-rich ap-
proach is omnivorous and can exploit any linguistic
analysis of an input sentence.
3 Source-Side Context Features
Adding features to the linear model (Equation 2)
that consider more of the source sentence requires
changing the decoder very little, if at all. The reason
is that the source sentence is fully observed, so the
information to be predicted is the same as before?
the difference is that we are using more clues to
carry out the prediction.
10
We see this as an opportunity to include many
more features in phrase-based MT without increas-
ing the cost of decoding at runtime. This discussion
is reminiscent of an advantage gained by moving
from hidden Markov models to conditional random
fields for sequence labeling tasks. While the same
core algorithm is used for decoding with both mod-
els, a CRF allows inclusion of features that consider
the entire observed sequence?i.e., more of the ob-
servable context of each label to be predicted. Al-
though this same advantage was already obtained
in statistical MT through the transition from ?noisy
channel? translation models to (log-)linear models,
the customary set of features used in most phrase-
based systems does not take full advantage of the
observed data.
The standard approach to estimating the phrase
translation conditional probability features is via rel-
ative frequencies (here e and f are phrases):
p(e | f) =
count(e,f)
?
e? count(e
?,f)
Our new features all take the form p(e |
f ,f context), where e is the target language phrase,
f is the source language phrase, and f context is the
context of the source language phrase in the sentence
in which it was observed. Like the context-bare con-
ditional probabilities, we estimate probability fea-
tures using relative frequencies:
p(e | f ,f context) =
count(e,f ,f context)?
e? count(e
?,f ,f context)
Since we expect that adding conditioning vari-
ables will lead to sparser counts and therefore more
zero estimates, we compute features for many dif-
ferent types of context. To combine the many
differently-conditioned features into a single model,
we provide them as features to the linear model
(Equation 2) and use minimum error-rate training
(Och, 2003) to obtain interpolation weights ?m.
This is similar to an interpolation of backed-off es-
timates, if we imagine that all of the different con-
texts are differently-backed off estimates of the com-
plete context. The error-driven weight training ef-
fectively smooths one implicit context-rich estimate
p(e | f ,f context) so that all of the backed-off es-
timates are taken into account, including the orig-
inal p(e | f). Our approach is asymmetrical; we
have not, for example, estimated features of the form
p(f ,f context | e).
We next discuss the specific source-side context
features used in our model.
3.1 Lexical Context Features
The most obvious kind of context of a source phrase
f `k is the m-length sequence before it (f
k?1
k?m) and
the m-length sequence after it (f `+m`+1 ). We include
context features for m ? {1, 2}, padding sentences
with m special symbols at the beginning and at the
end. For each value of m, we include three features:
? p(e | f ,fk?1k?m), the left lexical context;
? p(e | f ,f `+m`+1 ), the right lexical context;
? p(e | f ,fk?1k?m,f
`+m
`+1 ), both sides.
3.2 Shallow Syntactic Features
Lexical context features, especially when m > 1,
are expected to be sparse. Representing the context
by part-of-speech (POS) tags is one way to over-
come that sparseness. We used the same set of the
lexical context features described above, but with
POS tags replacing words in the context. We also
include a feature which conditions on the POS tag
sequence of the actual phrase being translated.
3.3 Syntactic Features
If a robust parser is available for the source lan-
guage, we can include context features from parse
trees. We used the following parse tree features:
? Is the phrase (exactly) a constituent?
? What is the nonterminal label of the lowest node
in the parse tree that covers the phrase?
? What is the nonterminal label or POS of the high-
est nonterminal node that ends immediately be-
fore the phrase? Begins immediately after the
phrase?
? Is the phrase strictly to the left of the root word,
does it contain the root word, or is it strictly to
the right of the root word? (Requires a parse with
head annotations.)
We also used a feature that conditions on both fea-
tures in the third bullet point above.
11
S[support] Syntactic Features:
Not a constituent
NP covers phrase
VBP to left
PP to right
Right of root word
PP
Shallow
Lexical
Phrase
PP
NP
VP
IN     DT       NN        ,  PRP VBP     DT   NN   IN  DT      NN         IN     NN     CC    NN    , ...
NP
SBAR
NPNPNP
NP
..
...
.
...
Positional Features:
Not at start
Not at end
Second fifth of sentence
Covers 18.5% of sentence
  (quantized to 20%)
Against this background ,  we   support   the report  of  the committee   on transport and tourism , which...
In dieser Hinsicht unterst?tzen wir   den Bericht des Ausschusses   f?r Verkehr und Fremdenverkehr , in...
Syntactic
NP
PP
Figure 1: A (partial) sentence pair from the WMT-07 Europarl training corpus. Processing of the data (parsing, word
alignment) was done as discussed in ?6. The phrase pair of interest is boxed and context features are shown in dotted
shapes. The context features help determine whether the phrase should be translated as ?der Bericht des Ausschusses?
(nominative case) or ?den Bericht des Ausschusses? (accusative case). See text for details.
3.4 Positional Features
We include features based on the position of the
phrase in the source sentence, the phrase length, and
the sentence length. These features use information
from the entire source sentence, but are not syntac-
tic. For a phrase f `k in a sentence f of length n:
? Is the phrase at the start of the sentence (k = 1)?
? Is the phrase at the end of the sentence (` = n)?
? A quantization of r =
k+ `?k+12
n , the relative po-
sition in (0, 1) of the phrase?s midpoint within f .
We choose the smallest q ? {0.2, 0.4, 0.6, 0.8, 1}
such that q > r.
? A quantization of c = `?k+1n , the fraction of the
words in f that are covered by the phrase. We
choose the smallest q ? { 140 ,
1
20 .
1
10 ,
1
5 ,
1
3 , 1} such
that q > c.
An illustration of the context features is shown
in Fig. 1. Consider the phrase pair ?the report
of the committee?/?den Bericht des Ausschusses?
extracted by our English ? German baseline MT
system (described in ?6.3). The German word
?Bericht? is a masculine noun; therefore, it takes the
article ?der? in the nominative case, ?den? in the ac-
cusative case, and ?dem? in the dative case. These
three translations are indeed available in the phrase
table for ?the report of the committee? (see Table 1,
?no context? column), with relatively high entropy.
The choice between ?den? and ?der? must be made
by the language model alone.
Knowing that the phrase follows a verb, or ap-
pears to the right of the sentence?s root word, or
within the second fifth of the sentence should help.
Indeed, a probability distribution that conditions on
context features gives more peaked distributions that
give higher probability to the correct translation,
given this context, and lower probability given some
other contexts (see Table 1).
4 Why Not Target-Side Context?
While source context is straightforward to exploit
in a model, including target-side context features
breaks one of the key independence assumptions
made by phrase-based translation models: the trans-
lations of the source-side phrases are conditionally
independent of each other, given f , thereby requir-
ing new algorithms for decoding (Marino et al,
2006).
We suggest that target-side context may already
be well accounted for in current MT systems. In-
deed, language models pay attention to the local
context of phrases, as do reordering models. The re-
cent emphasis on improving these components of a
translation system (Brants et al, 2007) is likely due
in part to the widespread availability of NLP tools
for the language that is most frequently the target:
English. We will demonstrate that NLP tools (tag-
12
Shallow: 2 POS on left Syntax: of root Positional: rel. pos.
g no context ??PRP VBP? ?VBN IN? ?right left ?2nd fifth 1st fifth
den bericht des ausschusses 0.3125 1.0000 0.3333 0.5000 0.0000 0.6000 0.0000
der bericht des ausschusses 0.3125 0.0000 0.0000 0.1000 0.6667 0.2000 0.6667
dem bericht des ausschusses 0.2500 0.0000 0.6667 0.3000 0.1667 0.0000 0.1667
Table 1: Phrase table entries for ?the report of the committee? and their scores under different contexts. These are the
top three phrases in the baseline English ? German system (?no context? column). Contexts from the source sentence
in Fig. 1 (starred) predict correctly; we show also alternative contexts that give very different distributions.
gers and parsers) for the source side can be used to
improve the translation model, exploiting analysis
tools for other languages.
5 Implementation
The additional data required to compute the context
features is extracted along with the phrase pairs dur-
ing execution of the standard phrase extraction algo-
rithm, affecting phrase extraction and scoring time
by a constant factor. We avoid the need to modify
the standard phrase-based decoder to handle context
features by appending a unique identifier to each to-
ken in the sentences to be translated. Then, we pre-
compute a phrase table for the phrases in these sen-
tences according to the phrase contexts. To avoid
extremely long lists of translations of common to-
kens, we filter the generated phrase tables, remov-
ing entries for which the estimate of p(e | f) < c,
for some small c. In our experiments, we fixed
c = 0.0002. This filtering reduced time for exper-
imentation dramatically and had no apparent effect
on the translation output. We did not perform any
filtering for the baseline system.
6 Experiments
In this section we present experimental results using
our context-endowed phrase translation model with
a variety of different context features, on Chinese ?
English, German ? English, and English ? Ger-
Chinese ? English (UN)
Context features BLEU NIST METEOR
None 0.3715 7.918 0.6486
Lexical 0.4030 8.367 0.6716
Shallow 0.3807 7.981 0.6523
Lexical + Shallow 0.4030 8.403 0.6703
Syntactic 0.3823 7.992 0.6531
Positional 0.3775 7.958 0.6510
Table 2: Chinese ? English experiments: training and
testing on UN data. Boldface marks scores significantly
higher than ?None.?
man translation tasks. Dataset details are given in
Appendices A (Chinese) and B (German).
Baseline We use the Moses MT system (Koehn et
al., 2007) as a baseline and closely follow the exam-
ple training procedure given for the WMT-07 and
WMT-08 shared tasks.4 In particular, we perform
word alignment in each direction using GIZA++
(Och and Ney, 2003), apply the ?grow-diag-final-
and? heuristic for symmetrization and use a max-
imum phrase length of 7. In addition to the two
phrase translation conditionals p(e | f) and p(f |
e), we use lexical translation probabilities in each
direction, a word penalty, a phrase penalty, a length-
based reordering model, a lexicalized reordering
model, and an n-gram language model, SRILM im-
plementation (Stolcke, 2002) with modified Kneser-
Ney smoothing (Chen and Goodman, 1998). Mini-
mum error-rate (MER) training (Och, 2003) was ap-
plied to obtain weights (?m in Equation 2) for these
features. A recaser is trained on the target side of the
parallel corpus using the script provided withMoses.
All output is recased and detokenized prior to evalu-
ation.
Evaluation We evaluate translation output using
three automatic evaluation measures: BLEU (Pap-
ineni et al, 2002), NIST (Doddington, 2002), and
METEOR (Banerjee and Lavie, 2005, version 0.6).5
All measures used were the case-sensitive, corpus-
level versions. The version of BLEU used was that
provided by NIST. Significance was tested using a
paired bootstrap (Koehn, 2004) with 1000 samples
(p < 0.05).6
4http://www.statmt.org/wmt08
5METEOR details: For English, we use exact matching,
Porter stemming, and WordNet synonym matching. For Ger-
man, we use exact matching and Porter stemming. These are the
same settings that were used to evaluate systems for the WMT-
07 shared task.
6Code implementing this test for these metrics can be freely
downloaded at http://www.ark.cs.cmu.edu/MT.
13
Chinese ? English
Testing on UN Testing on News (NIST 2005)
Context features BLEU NIST METEOR BLEU NIST METEOR
Training on in-domain data only:
None 0.3715 7.918 0.6486 0.2700 7.986 0.5314
Training on all data:
None 0.3615 7.797 0.6414 0.2593 7.697 0.5200
Lexical 0.3898 8.231 0.6697 0.2522 7.852 0.5273
Shallow: ? 1 POS tag 0.3611 7.713 0.6430 0.2669 8.243 0.5526
Shallow: ? 2 POS tags 0.3657 7.808 0.6455 0.2591 7.843 0.5288
Lexical + Shallow 0.3886 8.245 0.6675 0.2628 7.881 0.5290
Syntactic 0.3717 7.899 0.6531 0.2653 8.123 0.5403
Lexical + Syntactic 0.3926 8.224 0.6636 0.2572 7.774 0.5234
Positional 0.3647 7.766 0.6469 0.2648 7.891 0.5275
All 0.3772 8.176 0.6582 0.2566 7.775 0.5225
Feature selection (see Sec. 6.4) 0.3843 8.079 0.6594 0.2730 8.059 0.5343
Table 3: Chinese ? English experiments: first row shows baseline performance when training only on in-domain
data for each task; all other rows show results when training on all data (UN and News). Left half shows results when
tuning and testing on UN test sets; right half shows results when tuning on NIST 2004 News test set and testing on
NIST 2005. For feature selection, an additional set of unseen data was used: 2000 held-out sentences from the UN
data for the left half and the NIST 2003 test set for the right half. Boldface marks scores that are significantly higher
than the first row, in-domain baseline.
6.1 Chinese ? English
For our Chinese ? English experiments, two kinds
of data were used: UN proceedings, and newswire
as used in NIST evaluations.
UN Data UN data results are reported in Ta-
ble 2. Significant improvements are obtained on all
three evaluation measures?e.g., more than 3 BLEU
points?using lexical or lexical and shallow fea-
tures. While improvements are smaller for other fea-
tures and feature combinations, performance is not
harmed by conditioning on context features. Note
that using syntactic features gave 1 BLEU point of
improvement.
News Data In News data experiments, none of our
features obtained BLEU performance statistically
distinguishable from the baseline of 0.2700 BLEU
(neither better, nor worse). The News training cor-
pus is less than half the size of the UN training cor-
pus (in words); unsurprisingly, the context features
were too sparse to be helpful. Further, newswire are
less formulaic and repetitive than UN proceedings,
so contexts do not generalize as well from training
to test data. Fortunately, our ?error-minimizing mix-
ture? approach protects the BLEU score, which the
?m are tuned to optimize.
Combined UN + News Data Our next experi-
ment used all of the available training data (> 200M
words on each side) to train the models, in-domain
?m tuning, and testing for each domain separately;
see Table 3. Without context features, training
on mixed-domain data consistently harms perfor-
mance. With contexts that include lexical features,
the mixed-domain model significantly outperforms
the in-domain baseline for UN data. These results
suggest that context features enable better use of out-
of-domain data, an important advantage for statis-
tical MT since parallel data often arise from very
different sources than those of ?real-world? transla-
tion scenarios. On News data, context features did
not give a significant advantage on the BLEU score,
though syntactic and ? 1 POS contexts did give sig-
nificant NIST and METEOR improvements over the
in-domain baseline.
6.2 German ? English
We do not report full results for this task, because
the context features neither helped nor hurt perfor-
mance significantly. We believe this is due to data
sparseness resulting from the size of the training cor-
pus (26M German words), German?s relatively rich
morphology, and the challenges of German parsing.
14
English ? German
Context features BLEU NIST METEOR
None 0.2069 6.020 0.2811
Lexical 0.2018 6.031 0.2772
Shallow 0.2017 5.911 0.2748
Syntactic 0.2077 6.049 0.2829
Positional 0.2045 5.930 0.2772
Lex. + Shal. + Syn. 0.2045 6.061 0.2817
All 0.2053 6.009 0.2797
Feature selection 0.2080 6.009 0.2807
Table 4: English ? German experiments: training
and testing on Europarl data. WMT-07 Europarl parallel
training data was used for training, dev06 was used for
tuning, devtest06 was used for feature selection and de-
velopmental testing, and test07 was used for final testing.
Boldface marks scores significantly higher than ?None.?
6.3 English ? German
English ? German results are shown in Table 4.
The baseline system here is highly competitive, hav-
ing scored higher on automatic evaluation measures
than any other system in the WMT-07 shared task
(Callison-Burch et al, 2007). Though most results
are not statistically significant, small improvements
do tend to come from syntactic context features.
Comparing with the German? English experiment,
we attribute this effect to the high accuracy of the
English parser compared to the German parser.
6.4 Feature Selection
Translation performance does not always increase
when features are added to the model. This mo-
tivates the use of feature selection algorithms to
choose a subset of features to optimize perfor-
mance. We experimented with several feature se-
lection algorithms based on information-theoretic
quantities computed among the source phrase, the
target phrase, and the context, but found that a sim-
ple forward variable selection algorithm (Guyon and
Elisseeff, 2003) worked best. In this procedure, we
start with no context features and, at each iteration,
add the single feature that results in the largest in-
crease in BLEU score on an unseen development
set after ?m tuning. The algorithm terminates if no
features are left or if none result in an increase in
BLEU. We ran this algorithm to completion for the
two Chinese? English tune/test sets (training on all
data in each case) and the English ? German task;
see Tables 3 and 4. In all cases, the algorithm fin-
ishes after ? 4 iterations.
Feature selection for Chinese ? English (UN)
first chose the lexical feature ?1 word on each side,?
then the positional feature indicating which fifth of
the sentence contains the phrase, and finally the lex-
ical feature ?1 word on right.? For News, the fea-
tures chosen were the shallow syntactic feature ?1
POS on each side,? then the positional beginning-
of-sentence feature, then the position relative to the
root (a syntactic feature). For English ? German,
the shallow syntactic feature ?2 POS on left,? then
the lexical feature ?1 word on right? were selected.
In the case where context features were most
helpful (Chinese ? English UN data), we found
feature selection to be competitive at 2.28 BLEU
points above the no-context baseline, but not the best
achieved. In the other two cases (Chinese? English
News and English ? German Europarl), our best
results were achieved using these automatically se-
lected features, and in the Chinese ? English News
case, improvements on all three scores (including
1.37 BLEU points) are significant compared to the
no-context baseline trained on the same data.
6.5 WMT-08 Shared Task: English ? German
Since we began this research before the release
of the data for the WMT-08 shared task, we per-
formed the majority of our experiments using the
data released for the WMT-07 shared task (see Ap-
pendix B). To prepare our entry for the 2008 shared
task, we trained a baseline system on the 2008 data
using a nearly identical configuration.7 Table 5 com-
pares performance of the baseline system (with no
context features) to performance with the two con-
text features chosen automatically as described in
?6.4. In addition to the devtest06 data, we report re-
sults on the 2007 and 2008 Europarl test sets. Most
improvements were statistically significant.
7 Future Work
In future work, we plan to apply more sophisticated
learning algorithms to rich-feature phrase table esti-
mation. Context features can also be used as condi-
tioning variables in other components of translation
7The only differences were the use of a larger max sentence
length threshold of 55 tokens instead of 50, and the use of the
better-performing ?englishFactored? Stanford parser model.
15
devtest06 test07 test08
System BLEU NIST METEOR BLEU NIST METEOR BLEU NIST METEOR
Baseline 0.2009 5.866 0.2719 0.2051 5.957 0.2782 0.2003 5.889 0.2720
Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772
Table 5: English ? German shared task system results using WMT-08 Europarl parallel data for training, dev06 for
tuning, and three test sets, including the final 2008 test set. The row labeled ?Context? uses the top-performing feature
set {2 POS on left, 1 word on right}. Boldface marks scores that are significantly higher than the baseline.
models, including the lexicalized reordering model
and the lexical translation model in the Moses MT
system, or hierarchical or syntactic models (Chiang,
2005). Additional linguistic analysis (e.g., morpho-
logical disambiguation, named entity recognition,
semantic role labeling) can be used to define new
context features.
8 Conclusion
We have described a straightforward, scalable
method for improving phrase translation models by
modeling features of a phrase?s source-side context.
Our method allows incorporation of features from
any kind of source-side annotation and barely affects
the decoding algorithm. Experiments show perfor-
mance rivaling or exceeding strong, state-of-the-art
baselines on standard translation tasks. Automatic
feature selection can be used to achieve performance
gains with just two or three context features. Per-
formance is strongest when large in-domain training
sets and high-accuracy NLP tools for the source lan-
guage are available.
Acknowledgments
This research was supported in part by an ARCS
award to the first author, NSF IIS-0713265, su-
percomputing resources provided by Yahoo, and a
Google grant. We thank Abhaya Agarwal, Ashish
Venugopal, and Andreas Zollmann for helpful con-
versations and Joy Zhang for his Chinese segmenter.
We also thank the anonymous reviewers for helpful
comments.
A Dataset Details (Chinese-English)
We trained on data from the NIST MT 2008 con-
strained Chinese-English track: Hong Kong
Hansards and news (LDC2004T08), Sino-
rama (LDC2005T10), FBIS (LDC2003E14),
Xinhua (LDC2002E18), and financial news
(LDC2006E26)?total 2.5M sents., 66M Chinese
words, 68M English. For news experiments, the
newswire portion of the NIST 2004 test set was used
for tuning, the full NIST 2003 test set was used for
developmental testing and feature selection, and the
NIST 2005 test set was used for testing (900-1000
sents. each). We also used the United Nations paral-
lel text (LDC2004E12), divided into training (4.7M
sents.; words: 136M Chinese, 144M English),
tuning (2K sents.), and test sets (2K sents.). We
removed sentence pairs where either side was longer
than 80 words, segmented all Chinese text automat-
ically,8 and parsed/tagged using the Stanford parser
with the pre-trained ?xinhuaPCFG? model (Klein
and Manning, 2003). Trigram language models
were trained on the English side of the parallel
corpus along with approximately 115M words from
the Xinhua section of the English Gigaword corpus
(LDC2005T12), years 1995?2000 (total 326M
words).
B Dataset Details (German-English)
For German ? English experiments, we used data
provided for the WMT-07 shared task (1.1M sents.,
26M German words, 27M English). We used dev06
for tuning, devtest06 for feature selection and de-
velopmental testing, and test07 for final testing
(2K sents. each). We removed sentence pairs
where either side was longer than 50 words and
parsed/tagged the German and English data using
the Stanford parser (Klein andManning, 2003) (with
pre-trained ?germanFactored? and ?englishPCFG?
models). 5-gram language models were trained on
the entire target side of the parallel corpus (37M
German words, 38M English).
8Available at http://projectile.is.cs.cmu.
edu/research/public/tools/segmentation/
lrsegmenter/lrSegmenter.perl.
16
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proc. of ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization.
D. M. Bikel. 2004. A distributional analysis of a lexical-
ized statistical parsing model. In Proc. of EMNLP.
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine translation.
In Proc. of EMNLP-CoNLL.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85.
C. Callison-Burch, P. Koehn, C. Fordyce, and C. Monz,
editors. 2007. Proc. of the 2nd Workshop on Statisti-
cal Machine Translation.
M. Carl and A.Way. 2003. Recent Advances in Example-
Based Machine Translation. Kluwer Academic.
M. Carpuat and D. Wu. 2007. Improving statistical ma-
chine translation using word sense disambiguation. In
Proc. of EMNLP-CoNLL.
Y. Chan, H. Ng, and D. Chiang. 2007. Word sense dis-
ambiguation improves statistical machine translation.
In Proc. of ACL.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. of HLT.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
J. Gime?nez and L. Ma`rquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proc. of the 2ndWorkshop on Statistical
Machine Translation.
I. Guyon and A. Elisseeff. 2003. An introduction to vari-
able and feature selection. Journal of Machine Learn-
ing Research, 3:1157?1182.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proc. of HLT-NAACL,
pages 127?133.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL demonstration
session.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
Jose? B. Marino, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrik Lambert, Jose? A. R. Fonollosa, and
Marta R. Costa-jussa`. 2006. N -gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
M. Nagao. 1984. A framework of a mechanical trans-
lation between Japanese and English by analogy prin-
ciple. In Proc. of the International NATO Symposium
on Artificial and Human Intelligence. Elsevier North-
Holland, Inc.
NIST. 2006. NIST 2006 machine translation evaluation
official results.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
H. Somers. 1999. Review article: Example-based ma-
chine translation. Machine Translation, 14(2).
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In Proc. of TMI.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation.
In Proc. of HLT-EMNLP.
17
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Summarization with a Joint Model
for Sentence Extraction and Compression
Andre? F. T. Martins?? and Noah A. Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith}@cs.cmu.edu
Abstract
Text summarization is one of the oldest prob-
lems in natural language processing. Popu-
lar approaches rely on extracting relevant sen-
tences from the original documents. As a side
effect, sentences that are too long but partly
relevant are doomed to either not appear in the
final summary, or prevent inclusion of other
relevant sentences. Sentence compression is a
recent framework that aims to select the short-
est subsequence of words that yields an infor-
mative and grammatical sentence. This work
proposes a one-step approach for document
summarization that jointly performs sentence
extraction and compression by solving an in-
teger linear program. We report favorable ex-
perimental results on newswire data.
1 Introduction
Automatic text summarization dates back to the
1950s and 1960s (Luhn, 1958; Baxendale, 1958; Ed-
mundson, 1969). Today, the proliferation of digital
information makes research on summarization tech-
nologies more important than ever before. In the last
two decades, machine learning techniques have been
employed in extractive summarization of single
documents (Kupiec et al, 1995; Aone et al, 1999;
Osborne, 2002) and multiple documents (Radev and
McKeown, 1998; Carbonell and Goldstein, 1998;
Radev et al, 2000). Most of this work aims only
to extract relevant sentences from the original doc-
uments and present them as the summary; this sim-
plification of the problem yields scalable solutions.
Some attention has been devoted by the NLP
community to the related problem of sentence com-
pression (Knight and Marcu, 2000): given a long
sentence, how to maximally compress it into a gram-
matical sentence that still preserves all the rele-
vant information? While sentence compression is
a promising framework with applications, for exam-
ple, in headline generation (Dorr et al, 2003; Jin,
2003), little work has been done to include it as a
module in document summarization systems. Most
existing approaches (with some exceptions, like the
vine-growth model of Daume?, 2006) use a two-stage
architecture, either by first extracting a certain num-
ber of salient sentences and then feeding them into
a sentence compressor, or by first compressing all
sentences and extracting later. However, regardless
of which operation is performed first?compression
or extraction?two-step ?pipeline? approaches may
fail to find overall-optimal solutions; often the sum-
maries are not better that the ones produced by ex-
tractive summarization. On the other hand, a pilot
study carried out by Lin (2003) suggests that sum-
marization systems that perform sentence compres-
sion have the potential to beat pure extractive sys-
tems if they model cross-sentence effects.
In this work, we address this issue by merging the
tasks of sentence extraction and sentence compres-
sion into a global optimization problem. A careful
design of the objective function encourages ?sparse
solutions,? i.e., solutions that involve only a small
number of sentences whose compressions are to be
included in the summary. Our contributions are:
? We cast joint sentence extraction and compression
as an integer linear program (ILP);
? We provide a new formulation of sentence com-
pression using dependency parsing information
that only requires a linear number of variables,
and combine it with a bigram model;
? We show how the full model can be trained in a
max-margin framework. Since a dataset of sum-
maries comprised of extracted, compressed sen-
tences is unavailable, we present a procedure that
trains the compression and extraction models sep-
arately and tunes a parameter to interpolate the
1
two models.
The compression model and the full system are
compared with state-of-the-art baselines in standard
newswire datasets. This paper is organized as fol-
lows: ?2?3 provide an overview of our two building
blocks, sentence extraction and sentence compres-
sion. ?4 describes our method to perform one-step
sentence compression and extraction. ?5 shows ex-
periments in newswire data. Finally, ?6 concludes
the paper and suggests future work.
2 Extractive summarization
Extractive summarization builds a summary by ex-
tracting a few informative sentences from the docu-
ments. Let D , {t1, . . . , tM} be a set of sentences,
contained in a single or in multiple related docu-
ments.1 The goal is to extract the best sequence of
sentences ?ti1 , . . . , tiK ? that summarizes D whose
total length does not exceed a fixed budget of J
words. We describe some well-known approaches
that wil serve as our experimental baselines.
Extract the leading sentences (Lead). For
single-document summarization, the simplest
method consists of greedily extracting the leading
sentences while they fit into the summary. A sen-
tence is skipped if its inclusion exceeds the budget,
and the next is examined. This performs extremely
well in newswire articles, due to the journalistic
convention of summarizing the article first.
Rank by relevance (Rel). This method ranks sen-
tences by a relevance score, and then extracts the top
ones that can fit into the summary. The score is typ-
ically a linear function of feature values:
scorerel(ti) , ?>f(ti) = ?Dd=1 ?dfd(ti), (1)
Here, each fd(ti) is a feature extracted from sen-
tence ti, and ?d is the corresponding weight. In our
experiments, relevance features include (i) the recip-
rocal position in the document, (ii) a binary feature
indicating whether the sentence is the first one, and
(iii) the 1-gram and 2-gram cosine similarity with
the headline and with the full document.
1For simplicity, we describe a unified framework for single
and multi-document summarization, although they may require
specialized strategies. Here we experiment only with single-
document summarization and assume t1, ..., tM are ordered.
Maximal Marginal Relevance (MMR). For long
documents or large collections, it becomes impor-
tant to penalize the redundancy among the extracted
sentences. Carbonell and Goldstein (1998) proposed
greedily adding sentences to the summary S to max-
imize, at each step, a score of the form
? ? scorerel(ti) ? (1 ? ?) ? scorered(ti, S), (2)
where scorerel(ti) is as in Eq. 1 and scorered(ti, S)
accounts for the redundancy between ti and the cur-
rent summary S. In our experiments, redundancy is
the 1-gram cosine similarity between the sentence
ti and the current summary S. The trade-off be-
tween relevance and redundancy is controlled by
? ? [0, 1], which is tuned on development data.
McDonald (2007) proposed a non-greedy variant
of MMR that takes into account the redundancy be-
tween each pair of candidate sentences. This is cast
as a global optimization problem:
S? = argmax
S
? ??ti?S scorerel(ti) ?
(1 ? ?) ??ti,tj?S scorered(ti, tj), (3)
where scorerel(ti) , ?>relfrel(ti), scorered(ti, tj) ,
?>redfred(ti, tj), and frel(ti) and fred(ti, tj) are feature
vectors with corresponding learned weight vectors
?rel and ?red. He has shown how the relevance-based
method and the MMR framework (in the non-greedy
form of Eq. 3) can be cast as an ILP. By introducing
indicator variables ??i?i=1,...,M and ??ij?i,j=1,...,M
with the meanings
?i =
{ 1 if ti is to be extracted
0 otherwise
?ij =
{ 1 if ti and tj are both to be extracted
0 otherwise
(4)
one can reformulate Eq. 3 as an ILP with O(M2)
variables and constraints:
max
??i?,??ij?
? ??Mi=1 ?iscorerel(ti) ? (5)
(1 ? ?) ??Mi=1
?M
j=1 ?ijscorered(ti, tj),
subject to binary constraints ?i, ?ij ? {0, 1}, the
length constraint
?M
i=1 ?iNi ? J (where Ni is the
number of words of the ith sentence), and the fol-
lowing ?agreement constraints? for i, j = 1, . . . ,M
2
(that impose the logical relation ?ij = ?i ? ?j):
?ij ? ?i, ?ij ? ?j , ?ij ? ?i + ?j ? 1 (6)
Let us provide a compact representation of the pro-
gram in Eq. 5 that will be used later. Define our vec-
tor of parameters as ? , [??rel,?(1??)?red]. Pack-
ing all the feature vectors (one for each sentence, and
one for each pair of sentences) into a matrix F,
F ,
[ Frel 0
0 Fred
]
, (7)
with Frel , [frel(ti)]1?i?M and Fred ,
[fred(ti, tj)]1?i<j?M , and packing all the variables
?i and ?ij into a vector ?, the program in Eq. 5 can
be compactly written as
max
?
?>F?, (8)
subject to binary and linear constraints on ?. This
formulation requires O(M2) variables and con-
straints. If we do not penalize sentence redundancy,
the redundancy term may be dropped; in this simpler
case, F = Frel, the vector ? only contains the vari-
ables ??i?, and the program in Eq. 8 only requires
O(M) variables and constraints. Our method (to be
presented in ?4) will build on this latter formulation.
3 Sentence Compression
Despite its simplicity, extractive summarization has
a few shortcomings: for example, if the original sen-
tences are too long or embed several clauses, there
is no way of preventing lengthy sentences from ap-
pearing in the final summary. The sentence com-
pression framework (Knight and Marcu, 2000) aims
to select the best subsequence of words that still
yields a short, informative and grammatical sen-
tence. Such a sentence compressor is given a sen-
tence t , ?w1, . . . , wN ? as input and outputs a sub-
sequence of length L, c , ?wj1 , . . . , wjL?, with
1 ? j1 < . . . < jL ? N . We may represent
this output as a binary vector s of length N , where
sj = 1 iff word wj is included in the compression.
Note that there are O(2N ) possible subsequences.
3.1 Related Work
Past approaches to sentence compression include
a noisy channel formulation (Knight and Marcu,
2000; Daume? and Marcu, 2002), heuristic methods
that parse the sentence and then trim constituents ac-
cording to linguistic criteria (Dorr et al, 2003; Zajic
et al, 2006), a pure discriminative model (McDon-
ald, 2006), and an ILP formulation (Clarke and La-
pata, 2008). We next give an overview of the two
latter approaches.
McDonald (2006) uses the outputs of two parsers
(a phrase-based and a dependency parser) as fea-
tures in a discriminative model that decomposes
over pairs of consecutive words. Formally, given a
sentence t = ?w1, . . . , wN ?, the score of a compres-
sion c = ?wj1 , . . . , wjL? decomposes as:
score(c; t) = ?Ll=2 ?>f(t, jl?1, jl) (9)
where f(t, jl?1, jl) are feature vectors that depend
on the original sentence t and consecutive positions
jl?1 and jl, and ? is a learned weight vector. The
factorization in Eq. 9 allows exact decoding with dy-
namic programming.
Clarke and Lapata (2008) cast the problem as an
ILP. In their formulation, Eq. 9 may be expressed as:
score(c; t) =
N?
i=1
?i?>f(t, 0, i) +
N?
i=1
?i?>f(t, i, n + 1) +
N?1?
i=1
N?
j=i+1
?ij?>f(t, i, j), (10)
where ?i, ?i, and ?ij are additional binary variables
with the following meanings:
? ?i = 1 iff word wi starts the compression;
? ?i = 1 iff word wi ends the compression;
? ?ij = 1 iff words wi and wj appear consecutively
in the compression;
and subject to the following agreement constraints:
?N
i=1 ?i = 1?N
i=1 ?i = 1
sj = ?j +?j?1i=1 ?ij
si = ?i +?Nj=i+1 ?ij . (11)
3
This framework also allows the inclusion of con-
straints to enforce grammaticality.
To compress a sentence, one needs to maximize
the score in Eq. 10 subject to the constraints in
Eq. 11. Representing the variables through
? , ??1, . . . , ?N , ?1, . . . , ?N , ?11, . . . , ?NN ?
(12)
and packing the feature vectors into a matrix F, we
obtain the ILP
max
s,?
?>F? (13)
subject to linear and integer constraints on the vari-
ables s and ?. This particular formulation requires
O(N2) variables and constraints.
3.2 Proposed Method
We propose an alternative model for sentence com-
pression that may be formulated as an ILP, as in
Eq. 13, but with only O(N) variables and con-
straints. This formulation is based on the output of a
dependency parser.
Directed arcs in a dependency tree link pairs of
words, namely a head to its modifier. A dependency
parse tree is characterized by a set of labeled arcs
of the form (head, modifier, label); see Fig.1 for an
example. Given a sentence t = ?w1, . . . , wN ?, we
write i = pi(j) to denote that the ith word is the
head (the ?parent?) of the jth word; if j is the root,
we write pi(j) = 0. Let s be the binary vector de-
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A dependency parse for an English sentence;
example from McDonald and Satta (2007).
scribing a possible compression c for the sentence
t. For each word j, we consider four possible cases,
accounting for the inclusion or not of j and pi(j) in
the compression. We introduce (mutually exclusive)
binary variables ?j11, ?j10, ?j01, and ?j00 to indicate
eac of t ese cases, i.e., for a, b ? {0, 1},
?jab , sj = a ? spi(j) = b. (14)
Consider feature vectors f11(t, j), f10(t, j), f01(t, j),
and f00(t, j), that look at the surface sentence and at
the status of the word j and its head pi(j); these fea-
tures have corresponding weight vectors ?11, ?10,
?01, and ?00. The score of c is written as:
score(c; t) = ?Nj=1
?
a,b?{0,1} ?jab?>abfab(t, j)
= ?a,b?{0,1} ?>abFab?ab
= ?>F?, (15)
where Fab , [fab(t, 1), . . . , fab(t,N)], ?ab ,
(?jab)j=1,...,N , ? , (?11,?10,?01,?00), and F ,
Diag(F11,F10,F01,F00) (a block-diagonal matrix).
We have reached in Eq. 15 an ILP isomorphic to
the one in Eq. 13, but only with O(N) variables.
There are some agreement constraints between the
variables ? and s that reflect the logical relations in
Eq. 14; these may be written as linear inequalities
(cf. Eq. 6), yielding O(N) constraints.
Given this proposal and ?3.1, it is also straight-
forward to extend this model to include bigram fea-
tures as in Eq. 10; the combination of dependency
relation features and bigram features yields a model
that is more powerful than both models in Eq. 15 and
Eq. 10. Such a model is expressible as an ILP with
O(N2) variables and constraints, making use of the
variables s, ?, ?, ? and ?. In ?5, we compare the
performance of this model (called ?Bigram?) and the
model in Eq. 15 (called ?NoBigram?).2
4 Joint Compression and Extraction
We ext describe our joint mod l for sentenc com-
pression and extracti n. Let D , {t1, . . . , tM} be
a set of sentences as in ?2, each expressed as a se-
quence of words, ti , ?wi1, . . . , wiNi?. Following
?3, we represent a compression of ti as a binary vec-
tor si = ?si1, . . . , siNi?, where sij = 1 iff word wij
2It should be noted that more efficient decoders are possible
that do not require solving an ILP. In particular, inference in the
NoBigram variant can performed in polynomial time with dy-
namic programming algorithms that propagate messages along
the dependency parse tree; for the Bigram variant, dynamic pro-
gramming can till be employed with some additional storage.
Our ILP formulation, however, is more suited to the final goal
of performing document summarization (of which our sentence
compression model will be a component); furthermore, it also
allows the straightforward inclusion of global linguistic con-
straints, which, as shown by Clarke and Lapata (2008), can
greatly improve th grammaticality of the compressions.
4
is included in the compression. Now, define a sum-
mary of D as a set of sentences obtained by extract-
ing and compressing sentences from D. More pre-
cisely, let ?1, . . . , ?M be binary variables, one for
each sentence ti in D; define ?i = 1 iff a compres-
sion of sentence ti is used in the summary. A sum-
mary of D is then represented by the binary vari-
ables ??1, . . . , ?M , s1, . . . , sM ?. Notice that these
variables are redundant:
?i = 0 ? ?j ? {1, . . . , Ni} sij = 0, (16)
i.e., an empty compression means that the sentence
is not to be extracted. In the sequel, it will become
clear why this redundancy is convenient.
Most approaches up to now are concerned with ei-
ther extraction or compression, not both at the same
time. We will combine the extraction scores in Eq. 8
and the compression scores in Eq. 15 to obtain a sin-
gle, global optimization problem;3 we rename the
extraction features and parameters to Fe and ?e and
the compression features and parameters to Fc and
?c:
max
?,?,s
?Te Fe?+
?M
i=1 ?Tc Fci?i, (17)
subject to agreement constraints on the variables ?i
and si (see Eqs. 11 and 14), and new agreement con-
straints on the variables ? and s1, . . . , sM to enforce
the relation in Eq. 16:
sij ? ?i, ?i = 1, . . . ,M,?j = 1, . . . , Ni
?i ?
?Ni
j=1 sij , ?i = 1, . . . ,M
(18)
The constraint that the length of the summary cannot
exceed J words is encoded as:
?M
i=1
?Ni
j=1 sij ? J. (19)
All variables are further restricted to be binary. We
also want to avoid picking just a few words from
many sentences, which typically leads to ungram-
matical summaries. Hence it is desirable to obtain
?sparse? solutions with only a few sentences ex-
tracted and compressed (and most components of ?
are zero) To do so, we add the constraint
?Ni
j=1 sij ? ?i?Ni, i = 1, . . . ,M, (20)
3In what follows, we use the formulation in Eq. 8 with-
out the redundancy terms; however these can be included in
a straightforward way, naturally increasing the number of vari-
ables/constraints.
which states, for each sentence ti, that ti should be
ignored or have at least ?Ni words extracted. We fix
? = 0.8, enforcing compression rates below 80%.4
To learn the model parameters ? = ??e,?c?, we
can use a max-margin discriminative learning al-
gorithm like MIRA (Crammer and Singer, 2003),
which is quite effective and scalable. However, there
is not (to our knowledge) a single dataset of ex-
tracted and compressed sentences. Instead, as will
be described in Sec. 5.1, there are separate datasets
of extracted sentences, and datasets of compressed
sentences. Therefore, instead of globally learning
the model parameters, ? = ??e,?c?, we propose the
following strategy to learn them separately:
? Learn ??e using a corpus of extracted sentences,
? Learn ??c using a corpus of compressed sentences,
? Tune ? so that ? = ???e, ???c? has good perfor-
mance on development data. (This is necessary
since each set of weights is learned up to scaling.)
5 Experiments
5.1 Datasets, Evaluation and Environment
For our experiments, two datasets were used:
The DUC 2002 dataset. This is a collection of
newswire articles, comprised of 59 document clus-
ters. Each document within the collections (out of
a total of 567 documents) has one or two manually
created abstracts with approximately 100 words.5
Clarke?s dataset for sentence compression. This
is the dataset used by Clarke and Lapata (2008). It
contains manually created compressions of 82 news-
paper articles (1,433 sentences) from the British Na-
tional Corpus and the American News Text corpus.6
To evaluate the sentence compressor alone, we
measured the compression rate and the precision,
recall, and F1-measure (both macro and micro-
averaged) with respect to the ?gold? compressed
4There are alternative ways to achieve ?sparseness,? either
in a soft way, by adding a term ??Pi ?i to the objective, or
using a different hard constraint, like
P
i ?i ? K, to limit the
number of sentences from which to pick words.
5http://duc.nist.gov
6http://homepages.inf.ed.ac.uk/s0460084/data
5
Compression Micro-Av. Macro-Av.
Ratio P R F1 P R F1
HedgeTrimmer 57.64% 0.7099 0.5925 0.6459 0.7195 0.6547 0.6367
McDonald (2006) 71.40% 0.7444 0.7697 0.7568 0.7711 0.7852 0.7696
NoBigram 71.20% 0.7399 0.7626 0.7510 0.7645 0.7730 0.7604
Bigram 71.35% 0.7472 0.7720 0.7594 0.7737 0.7848 0.7710
Table 1: Results for sentence compression in the Clarke?s test dataset (441 sentences) for our implementation of the
baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model,
NoBigram and Bigram. The compression ratio associated with the reference compressed sentences in this dataset is
69.06%. In the rightmost column, the statistically indistinguishable best results are emboldened, based on a paired
t-test applied to the sequence of F1 measures (p < 0.01).
sentences, calculated on unigrams.7
To evaluate the full system, we used Rouge-N
(Lin and Hovy, 2002), a popular n-gram recall-
based automatic evaluation measure. This score
compares the summary produced by a system with
one or more valid reference summaries.
All our experiments were conducted on a PC with
a Intel dual-core processor with 2.66 GHz and 2 Gb
RAM memory. We used ILOG CPLEX, a commer-
cial integer programming solver. The interface with
CPLEX was coded in Java.
5.2 Sentence Compression
We split Clarke?s dataset into two partitions, one
used for training (1,188 sentences) and the other for
testing (441 sentences). This dataset includes one
manual compression for each sentence, that we use
as reference for evaluation purposes. Compression
ratio, i.e., the fraction of words included in the com-
pressed sentences, is 69.32% (micro-averaged over
the training partition).
For comparison, two baselines were imple-
mented: a simple compressor based on Hedge Trim-
mer, the headline generation system of Dorr et al
(2003) and Zajic et al (2006),8 and the discrimina-
7Notice that this evaluation score is not able to properly cap-
ture the grammaticality of the compression; this is a known is-
sue that typically is addressed by requiring human judgments.
8Hedge Trimmer applies a deterministic compression proce-
dure whose first step is to identify the lowest leftmost S node in
the parse tree that contains a NP and a VP; this node is taken as
the root of the compressed sentence (i.e., all words that are not
spanned by this node are discarded). Further steps described
by Dorr et al (2003) include removal of low content units, and
an ?iterative shortening? loop that keeps removing constituents
until a desired compression ratio is achieved. The best results
were obtained without iterative shortening, which is explained
by the fact that the selection of the lowest leftmost S node (first
tive model described by McDonald (2006), which
captures ?soft syntactic evidence? (we reproduced
the same set of features). Both systems require
a phrase-structure parser; we used Collins? parser
(Collins, 1999);9 the latter system also derives fea-
tures from a dependency parser; we used the MST-
Parser (McDonald et al, 2005).10
We implemented the two variants of our compres-
sor described in ?3.2.
NoBigram. This variant factors the compression
score as a sum over individual scores, each depend-
ing on the inclusion or not of each word and its head
in the compression (see Eq. 15). An upper bound of
70% was placed on the compression ratio. As stated
in ?3.2, inference amounts to solving an ILP with
O(N) variables and constraints, N being the sen-
tence length. We also used MSTParser to obtain the
dependency parse trees.
Bigram. This variant includes an extra term stand-
ing for a bigram score, which factors as a sum over
pairs of consecutive words. As in McDonald (2006),
we include features that depend on the ?in-between?
words in the original sentence that are to be omitted
in the compression.11 As stated in ?3.2, inference
through this model can be done by solving an ILP
with O(N2) variables and constraints.
step of the algorithm) already provides significant compression,
as illustrated in Table 1.
9http://people.csail.mit.edu/mcollins/code.
html
10http://sourceforge.net/projects/mstparser
11The major difference between this variant and model of
McDonald (2006) is that the latter employs ?soft syntactic ev-
idence? as input features, while we make the dependency rela-
tions part of the output features. All the non-syntactic features
are the same. Apart from this, notice that our variant does not
employ a phrase-structure parser.
6
For both variants, we used MSTParser to obtain
the dependency parse trees. The model parameters
are learned in a pure discriminative way through a
max-margin approach. We used the 1-best MIRA
algorithm (Crammer and Singer, 2003; McDonald
et al, 2005) for training; this is a fast online algo-
rithm that requires solving the inference problem at
each step. Although inference amounts to solving
an ILP, which in the worst case scales exponentially
with the size of the sentence, training the model is
in practice very fast for the NoBigram model (a few
minutes in the environment described in ?5.1) and
fast enough for the Bigram model (a couple of hours
using the same equipment). This is explained by the
fact that sentences don?t usually exceed a few tens
of words, and because of the structure of the ILPs,
whose constraint matrices are very sparse.
Table 1 depicts the micro- and macro-averaged
precision, recall and F1-measure. We can see that
both variants outperform the Hedge Trimmer base-
line by a great margin, and are in line with the sys-
tem of McDonald (2006); however, none of our vari-
ants employ a phrase-structure parser. We also ob-
serve that our simpler NoBigram variant, which uses
a linear-sized ILP, achieves results similar to these
two systems.
5.3 Joint Compression and Extraction
For the summarization task, we split the DUC 2002
dataset into a training partition (427 documents) and
a testing partition (140 documents). The training
partition was further split into a training and a de-
velopment set. We evaluated the performance of
Lead, Rel, and MMR as baselines (all are described
in ?2). Weights for Rel were learned via the SVM-
Rank algorithm;12 to create a gold-standard ranking,
we sorted the sentences by Rouge-2 score13 (with re-
spect to the human created summaries). We include
a Pipeline baseline as well, which ranks all sentences
by relevance, then includes their compressions (us-
ing the Bigram variant) while they fit into the sum-
mary.
We tested two variants of our joint model, com-
bining the Rel extraction model with (i) the NoBi-
12SVMRank is implemented in the SVMlight toolkit
(Joachims, 1999), http://svmlight.joachims.org.
13A similar system was implemented that optimizes the
Rouge-1 score instead, but it led to inferior performance.
Rouge-1 Rouge-2
Lead 0.384 ? 0.080 0.177 ? 0.083
Rel 0.389 ? 0.074 0.178 ? 0.080
MMR ? = 0.25 0.392 ? 0.071 0.178 ? 0.077
Pipeline 0.380 ? 0.073 0.173 ? 0.073
Rel + NoBigr ? = 1.5 0.403 ? 0.080 0.180 ? 0.082
Rel + Bigr ? = 4.0 0.403 ? 0.076 0.180 ? 0.076
Table 2: Results for sentence extraction in the DUC2002
dataset (140 documents). Bold indicates the best results
with statistical significance, according to a paired t-test
(p < 0.01); Rouge-2 scores of all systems except Pipeline
are indistinguishable according to the same test, with p >
0.05.
gram compression model (?3.2) and (ii) the Bigram
variant. Each variant was trained with the proce-
dure described in ?4. To keep tractability, the in-
ference ILP problem was relaxed (the binary con-
straints were relaxed to unit interval constraints) and
non-integer solution values were rounded to produce
a valid summary, both for training and testing.14
Whenever this procedure yielded a summary longer
than 100 words, we truncated it to fit the word limit.
Table 2 depicts the results of each of the above
systems in terms of Rouge-1 and Rouge-2 scores.
We can see that both variants of our system are able
to achieve the best results in terms of Rouge-1 and
Rouge-2 scores. The suboptimality of extracting and
compressing in separate stages is clear from the ta-
ble, as Pipeline performs worse than the pure ex-
tractive systems. We also note that the configuration
Rel + Bigram is not able to outperform Rel + No-
Bigram, despite being computationally more expen-
sive (about 25 minutes to process the whole test set,
against the 7 minutes taken by the Rel + NoBigram
variant). Fig. 2 exemplifies the summaries produced
by our system. We see that both variants were able
to include new pieces of information in the summary
without sacrificing grammaticality.
These results suggest that our system, being capa-
ble of performing joint sentence extraction and com-
pression to summarize a document, offers a power-
ful alternative to pure extractive systems. Finally, we
note that no labeled datasets currently exist on which
our full model could have been trained with super-
vision; therefore, although inference is performed
14See Martins et al (2009) for a study concerning the impact
of LP relaxations in the learning problem.
7
MMR baseline:
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, ?Oscar and Lu-
cinda?.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London?s ancient Guildhall.
Carey, who lives in Sydney with his wife and son, said in a brief
speech that like the other five finalists he had been asked to attend
with a short speech in his pocket in case he won.
Rel + NoBigram:
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, ?Oscar and Lu-
cinda?.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London?s ancient Guildhall.
The judges made their selection from 102 books published in Britain
in the past 12 months and which they read in their homes.
Carey, who lives in Sydney with his wife and son, said in a brief
speech that like the other five finalists he had been asked to attend
with a short speech in his pocket in case he won.
Rel + Bigram:
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, ?Oscar and Lu-
cinda?.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London?s ancient Guildhall.
He was unsuccessful in the prize competition in 1985 when his
novel, ?Illywhacker,? was among the final six.
Carey called the award a ?great honor? and he thanked the prize
sponsors for ?provoking so much passionate discussion about liter-
ature perhaps there will be more tomorrow?.
Carey was the only non-Briton in the final six.
Figure 2: Summaries produced by the strongest base-
line (MMR) and the two variants of our system. Deleted
words are marked as such.
jointly, our training procedure had to learn sepa-
rately the extraction and the compression models,
and to tune a scalar parameter to trade off the two
models. We conjecture that a better model could
have been learned if a labeled dataset with extracted
compressed sentences existed.
6 Conclusion and Future Work
We have presented a summarization system that per-
forms sentence extraction and compression in a sin-
gle step, by casting the problem as an ILP. The sum-
mary optimizes an objective function that includes
both extraction and compression scores. Our model
encourages ?sparse? summaries that involve only a
few sentences. Experiments in newswire data sug-
gest that our system is a valid alternative to exist-
ing extraction-based systems. However, it is worth
noting that further evaluation (e.g., human judg-
ments) needs to be carried out to assert the quality
of our summaries, e.g., their grammaticality, some-
thing that the Rouge scores cannot fully capture.
Future work will address the possibility of in-
cluding linguistic features and constraints to further
improve the grammaticality of the produced sum-
maries.
Another straightforward extension is the inclusion
of a redundancy term and a query relevance term
in the objective function. For redundancy, a simi-
lar idea of that of McDonald (2007) can be applied,
yielding a ILP with O(M2 + N) variables and con-
straints (M being the number of sentences and N the
total number of words). However, such model will
take into account the redundancy among the origi-
nal sentences and not their compressions; to model
the redundancy accross compressions, a possibil-
ity is to consider a linear redundancy score (similar
to cosine similarity, but without the normalization),
which would result in an ILP with O(N +?i P 2i )
variables and constraints, where Pi ? M is the num-
ber of sentences in which word wi occurs; this is no
worse than O(M2N).
We also intend to model discourse, which, as
shown by Daume? and Marcu (2002), plays an im-
portant role in document summarization. Another
future direction is to extend our ILP formulations
to more sophisticated models that go beyond word
deletion, like the ones proposed by Cohn and Lapata
(2008).
Acknowledgments
The authors thank the anonymous reviewers for helpful
comments, Yiming Yang for interesting discussions, and
Dipanjan Das and Sourish Chaudhuri for providing their
code. This research was supported by a grant from FCT
through the CMU-Portugal Program and the Informa-
tion and Communications Technologies Institute (ICTI)
at CMU, and also by Priberam Informa?tica.
8
References
C. Aone, M. E. Okurowski, J. Gorlinsky, and B. Larsen.
1999. A trainable summarizer with knowledge ac-
quired from robust nlp techniques. In Advances in Au-
tomatic Text Summarization. MIT Press.
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature?an experiment. IBM Journal of Re-
search Development, 2(4):354?361.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of SIGIR.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression an integer linear programming
approach. JAIR, 31:399?429.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proc. COLING.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
K. Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951?991.
H. Daume? and D. Marcu. 2002. A noisy-channel model
for document compression. In Proc. of ACL.
H. Daume?. 2006. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D. thesis,
University of Southern California.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge trim-
mer: A parse-and-trim approach to headline gener-
ation. In Proc. of HLT-NAACL Text Summarization
Workshop and DUC.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. Journal of the ACM, 16(2):264?285.
R. Jin. 2003. Statistical Approaches Toward Title Gener-
ation. Ph.D. thesis, Carnegie Mellon University.
T. Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
K. Knight and D. Marcu. 2000. Statistics-based
summarization?step one: Sentence compression. In
Proc. of AAAI/IAAI.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proc. of SIGIR.
C.-Y. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In Proc. of the ACL Work-
shop on Automatic Summarization.
C.-Y. Lin. 2003. Improving summarization performance
by sentence compression-a pilot study. In Proc. of the
Int. Workshop on Inf. Ret. with Asian Languages.
H. P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBM Journal of Research Development,
2(2):159?165.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Polyhedral outer approximations with application to
natural language parsing. In Proc. of ICML.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. of EACL.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proc. of
ECIR.
M. Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proc. of the ACL Workshop on
Automatic Summarization.
D. R. Radev and K. McKeown. 1998. Generating natural
language summaries from multiple on-line sources.
Computational Linguistics, 24(3):469?500.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple documents:
sentence extraction, utility-based evaluation, and user
studies. In Proc. of the NAACL-ANLP Workshop on
Automatic Summarization.
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006.
Sentence compression as a component of a multi-
document summarization system. In Proc. of the ACL
DUC Workshop.
9
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 281?290, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Compiling Comp Ling:
Practical Weighted Dynamic Programming and the Dyna Language?
Jason Eisner and Eric Goldlust and Noah A. Smith
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{jason,goldlust,nasmith}@cs.jhu.edu
Abstract
Weighted deduction with aggregation is a powerful theoretical
formalism that encompasses many NLP algorithms. This pa-
per proposes a declarative specification language, Dyna; gives
general agenda-based algorithms for computing weights and
gradients; briefly discusses Dyna-to-Dyna program transforma-
tions; and shows that a first implementation of a Dyna-to-C++
compiler produces code that is efficient enough for real NLP re-
search, though still several times slower than hand-crafted code.
1 Introduction
In this paper, we generalize some modern prob-
abilistic parsing techniques to a broader class of
weighted deductive algorithms. Our implemented
system encapsulates these implementation tech-
niques behind a clean interface?a small high-level
specification language, Dyna, which compiles into
C++ classes. This system should help the HLT com-
munity to experiment more easily with new models
and algorithms.
1.1 Dynamic programming as deduction
The ?parsing as deduction? framework (Pereira and
Warren, 1983) is now over 20 years old. It provides
an elegant notation for specifying a variety of pars-
ing algorithms (Shieber et al, 1995), including algo-
rithms for probabilistic or other semiring-weighted
parsing (Goodman, 1999). In the parsing commu-
nity, new algorithms are often stated simply as a set
of deductive inference rules (Sikkel, 1997; Eisner
and Satta, 1999).
It is also straightforward to specify other NLP al-
gorithms this way. Syntactic MT models, language
models, and stack decoders can be easily described
using deductive rules. So can operations on finite-
state and infinite-state machines.
?We thank Joshua Goodman, David McAllester, and Paul
Ruhlen for useful early discussions; pioneer users Markus
Dreyer, David Smith, and Roy Tromble for their feedback and
input; John Blatz for discussion of program transformations;
and several reviewers for useful criticism. This work was
supported by NSF ITR grant IIS-0313193, ONR MURI grant
N00014-01-1-0685, and a Hertz Foundation fellowship to the
third author. The views expressed are not necessarily endorsed
by the sponsors.
1.2 The role of toolkits
One might regard deductive inference as merely a
helpful perspective for teaching old algorithms and
thinking about new ones, linking NLP to logic and
classical AI. Real implementations would then be
carefully hand-coded in a traditional language.
That was the view ten years ago of finite-state
machines?that FSMs were part of the theoretical
backbone of CL, linking the field to the theory
of computation. Starting in the mid-1990?s, how-
ever, finite-state methods came to the center of ap-
plied NLP as researchers at Xerox, AT&T, Gronin-
gen and elsewhere improved the expressive power
of FSMs by moving from automata to transduc-
ers, adding semiring weights, and developing pow-
erful new regular-expression operators and algo-
rithms for these cases. They also developed soft-
ware. Karttunen et al (1996) built an FSM toolkit
that allowed construction of morphological ana-
lyzers for many languages. Mohri et al (1998)
built a weighted toolkit that implemented novel
algorithms (e.g., weighted minimization, on-the-
fly composition) and scaled up to handle large-
vocabulary continuous ASR. At the same time, re-
newed community-wide interest in shallow methods
for information extraction, chunking, MT, and di-
alogue processing meant that such off-the-shelf FS
toolkits became the core of diverse systems used in
cutting-edge research.
The weakness of FSMs, of course, is that they
are only finite-state. One would like something like
AT&T?s FSM toolkit that also handles the various
formalisms now under consideration for lexicalized
grammars, non-context-free grammars, and syntax-
based MT?and hold the promise of extending to
other formalisms and applications not yet imagined.
We believe that deductive inference should play
the role of regular expressions and FSMs, providing
the theoretical foundation for such an effort. Many
engineering ideas in the field can be regarded, we
281
1. :- double item=0. % declares that all item values are doubles, default is 0
2. constit(X,I,K) += rewrite(X,W) * word(W,I,K). % a constituent is either a word . . .
3. constit(X,I,K) += rewrite(X,Y,Z) * constit(Y,I,J) * constit(Z,J,K). % . . . or a combination of two adjacent subconstituents
4. goal += constit(?s?,0,N) whenever ?ends at(N). % a parse is any s constituent that covers the input string
Figure 1: A probabilistic CKY parser written in Dyna. Axioms are in boldface.
believe, as ideas for how to specify, transform, or
compile systems of inference rules.
2 A Language for Deductive Systems
Any toolkit needs an interface. For example, FS
toolkits offer a regular expression language. We
propose a simple but Turing-complete language,
Dyna, for specifying weighted deductive-inference
algorithms. We illustrate it here by example; see
http://dyna.org for more details and a tutorial.
The short Dyna program in Fig. 1 expresses the
inside algorithm for PCFGs (i.e., the probabilistic
generalization of CKY recognition). Its 3 inference
rules schematically specify many equations, over
an arbitrary number of unknowns. This is possible
bcause the unknowns (items) have structured names
(terms) such as constit(?s?,0,3). They resemble typed
variables in a C program, but we use variable in-
stead to refer to the capitalized identifiers X, I, K,
. . . in lines 2?4. Each rule gives a consequent on
the left-hand side of the +=, which can be built by
combining the antecedents on the right-hand side.1
Lines 2?4 are equational schemas that spec-
ify how to compute the value of items such as
constit(?s?,0,3) from the values of other items.
Using the summation operator +=, lines 2?
3 say that for any X, I, and K, constit(X,I,K)
is defined by summing over the remaining
variables, as
?
W rewrite(X,W)*word(W,I,K) +?
Y,Z,J rewrite(X,Y,Z)*constit(Y,I,J)*constit(Z,J,K). For
example, constit(?s?,0,3) is a sum of quantities such as
rewrite(?s?, ?np?, ?vp?)*constit(?np?,0,1)*constit(?vp?,1,3).
The whenever operator in line 4 specifies a side
condition that restricts the set of expressions in the
sum (i.e., only when N is the sentence length).
To fully define the system of equations, non-
default values (in this case, non-zero values) should
be asserted for some axioms at runtime. (Axioms,
shown in bold in Fig. 1, are items that never appear
1Much of our notation and terminology comes from
logic programming: term, variable, inference rule, an-
tecedent/consequent, assert/retract, axiom/theorem.
as a consequent.) If the PCFG contains a rewrite rule
np ? Mary with probability p(Mary | np)=0.005,
the user should assert that rewrite(?np?, ?Mary?) has
value 0.005. If the input is John loves Mary, val-
ues of 1 should be asserted for word(?John?,0,1),
word(?loves?,1,2), word(?Mary?,2,3), and ends at(3).
Given the axioms as base cases, the equations in
Fig. 1 enable deduction of values for other items.
The value of the theorem constit(?s?,0,3) will be the
inside probability ?s(0, 3),2 and the value of goal
will be the total probability of all parses.
If one replaces += by max= throughout, then con-
stit(?s?,0,3) will accumulate the maximum rather than
the sum of these quantities, and goal will accumulate
the probability of the best parse.
With different input, the same program car-
ries out lattice parsing. Simply assert axioms
that correspond to (weighted) lattice arcs, such as
word(?John?,17,50), where 17 and 50 are arbitrary
terms denoting states in the lattice. It is also quite
straightforward to lexicalize the nonterminals or ex-
tend to synchronous grammars.
A related context-free parsing strategy, shown in
Fig. 2, is Earley?s algorithm. These equations illus-
trate nested terms such as lists. The side condition
in line 2 prevents building any constituent until one
has built a left context that calls for it.
3 Relation to Previous Work
There is a large relevant literature. Some of the well-
known CL papers, notably Goodman (1999), were
already mentioned in section 1.1. Our project has
three main points of difference from these.
First, we provide an efficient, scalable, open-
source implementation, in the form of a compiler
from Dyna to C++ classes. (Related work is in ?7.2.)
The C++ classes are efficient and easy to use, with
statements such as c[rewrite(?np?,2,3)]=0.005 to assert
axiom values into a chart named c (i.e., a deduc-
2That is, the probability that s would stochastically rewrite
to the first three words of the input. If this can happen in more
than one way, the probability sums over multiple derivations.
282
1. need(??s??,0) = 1. % begin by looking for an s that starts at position 0
2. constit(Nonterm/Needed,I,I) += rewrite(Nonterm,Needed) whenever ?need(Nonterm, I). % traditional predict step
3. constit(Nonterm/Needed,I,K) += constit(Nonterm/cons(W,Needed),I,J) * word(W,J,K). % traditional scan step
4. constit(Nonterm/Needed,I,K) += constit(Nonterm,cons(X,Needed),I,J) * constit(X/nil,J,K). % traditional complete step
5. goal += constit(?s?/nil,0,N) whenever ?ends at(N). % we want a complete s constituent covering the sentence
6. need(Nonterm,J) += constit( /cons(Nonterm, ), ,J). % Note: underscore matches anything (anonymous wildcard)
Figure 2: An Earley parser that recovers inside probabilities (Earley, 1970; Stolcke, 1995). The rule np ? det n should be encoded
as the axiom rewrite(?np?,cons(?det?,cons(?n?,nil))), a nested term. ?np?/Needed is the label of a partial np constituent that is
still missing the list of subconstituents in Needed. need(?np?,3) is derived if some partial constituent seeks an np subconstituent
starting at position 3. As in Fig. 1, lattice parsing comes for free, as does training.
tive database) and expressions like c[goal] to extract
the values of the resulting theorems, which are com-
puted as needed. The C++ classes also give access to
the proof forest (e.g., the forest of parse trees), and
integrate with parameter optimization code.
Second, we fully generalize the agenda-based
strategy of Shieber et al (1995) to the weighted
case?in particular supporting a prioritized agenda.
That allows probabilities to guide the search for
the best parse(s), a crucial technique in state-of-the-
art context-free parsers.3 We also give a ?reverse?
agenda algorithm to compute gradients or outside
probabilities for parameter estimation.
Third, regarding weights, the Dyna language is
designed to express systems of arbitrary, hetero-
geneous equations over item values. In previous
work such as (Goodman, 1999; Nederhof, 2003),
one only specifies the inference rules as unweighted
Horn clauses, and then weights are added automat-
ically in a standard way: all values have the same
type W, and all rules transform to equations of the
form c ?= a1 ? a2 ? ? ? ? ? ak, where ? and ?
give W the structure of a semiring.4 In Dyna one
writes these equations explicitly in place of Horn
clauses (Fig. 1). Accordingly, heterogeneous Dyna
programs, to be supported soon by our compiler,
will allow items of different types to have values
of different types, computed by different aggrega-
tion operations over arbitrary right-hand-side ex-
3Previous treatments of weighted deduction have used an
agenda only for an unweighted parsing phase (Goodman, 1999)
or for finding the single best parse (Nederhof, 2003). Our algo-
rithm works in arbitrary semirings, including non-idempotent
ones, taking care to avoid double-counting of weights and to
handle side conditions.
4E.g., the inside algorithm in Fig. 1 falls into Goodman?s
framework, with ?W,?,?? = ?R?0,+, ???the PLUSTIMES
semiring. Because ? distributes over ? in a semiring, com-
puting goal is equivalent to an aggregation over many separate
parse trees. That is not the case for heterogeneous programs.
pressions. This allows specification of a wider class
of algorithms from NLP and elsewhere (e.g., mini-
mum expected loss decoding, smoothing formulas,
neural networks, game tree analysis, and constraint
programming). Although ?4 and ?5 have space to
present only techniques for the semiring case, these
can be generalized.
Our approach may be most closely related to de-
ductive databases, which even in their heyday were
apparently ignored by the CL community (except for
Minnen, 1996). Deductive database systems per-
mit inference rules that can derive new database
facts from old ones.5 They are essentially declara-
tive logic programming languages (with restrictions
or extensions) that are?or could be?implemented
using efficient database techniques. Some imple-
mented deductive databases such as CORAL (Ra-
makrishnan et al, 1994) and LOLA (Zukowski and
Freitag, 1997) support aggregation (as in Dyna?s
+=, log+=, max=, . . . ), although only ?stratified?
forms of it that exclude unary CFG rule cycles.6
Ross and Sagiv (1992) (and in a more restricted
way, Kifer and Subrahmanian, 1992) come closest to
our notion of attaching aggregable values to terms.
Among deductive or other database systems,
Dyna is perhaps unusual in that its goal is not to sup-
port transactional databases or ad hoc queries, but
rather to serve as an abstract layer for specifying an
algorithm, such as a dynamic programming (DP) al-
gorithm. Thus, the Dyna program already implicitly
or explicitly specifies all queries that will be needed.
This allows compilation into a hard-coded C++ im-
plementation. The compiler?s job is to support these
queries by laying out and indexing the database re-
5Often they use some variant of the unweighted agenda-
based algorithm, which is known in that community as ?semi-
naive bottom-up evaluation.?
6An unweighted parser was implemented in an earlier ver-
sion of LOLA (Specht and Freitag, 1995).
283
lations in memory7 in a way that resembles hand-
designed data structures for the algorithm in ques-
tion. The compiler has many choices to make here;
we ultimately hope to implement feedback-directed
optimization, using profiled sample runs on typical
data. For example, a sparse grammar should lead to
different strategies than a dense one.
4 Computing Theorem Values
Fig. 1 specifies a set of equations but not how to
solve them. Any declarative specification language
must be backed up by a solver for the class of speci-
fiable problems. In our continuing work to develop a
range of compiler strategies for arbitrary Dyna pro-
grams, we have been inspired by the CL commu-
nity?s experience in building efficient parsers.
In this paper and in our current implementa-
tion, we give only the algorithms for what we call
weighted dynamic programs, in which all axioms
and theorems are variable-free. This means that
a consequent may only contain variables that al-
ready appear elsewhere in the rule. We further re-
strict to semiring-weighted programs as in (Good-
man, 1999). But with a few more tricks not given
here, the algorithms can be generalized to a wider
class of heterogeneous weighted logic programs.8
4.1 Desired properties
Computation is triggered when the user requests the
value of one or more particular items, such as goal.
Our algorithm must have several properties in order
to substitute for manually written code.
Soundness. The algorithm cannot be guaranteed
to terminate (since it is possible to write arbitrary
Turing machines in Dyna). However, if it does ter-
minate, it should return values from a valid model of
the program, i.e., values that simultaneously satisfy
all the equations expressed by the program.
Reasonable completeness. The computation
should indeed terminate for programs of interest
to the NLP community, such as parsing under a
probabilistic grammar?even if the grammar has
7Some relations might be left unmaterialized and computed
on demand, with optional memoization and flushing of memos.
8Heterogeneous programs may propagate non-additive up-
dates, which arbitrarily modify one of the inputs to an aggrega-
tion. Non-dynamic programs require non-ground items in the
chart, complicating both storage and queries against the chart.
1. for each axiom a, set agenda[a] := value of axiom a
2. while there is an item a with agenda[a] 6= 0
3. (* remove an item from the agenda and move its value to the chart *)
4. choose such an a
5. ? := agenda[a]; agenda[a] := 0
6. old := chart[a]; chart[a] := chart[a]??
7. if chart[a] 6= old (* only propagate actual changes *)
8. (* compute new resulting updates and place them on the agenda *)
9. for each inference rule ?c ?= a1 ? a2 ? ? ? ? ? ak?
10. for i from 1 to k
11. for each way of instantiating the rule?s variables
such that ai = a
12. agenda[c] ?=
kO
j=1
8
><
>:
old if j < i and
aj = a
? if j = i
chart[aj ] otherwise
(* can skip this line if any multiplicand is 0 *)
Figure 3: Weighted agenda-based deduction in a semiring, with-
out side conditions (see text).
left recursion, unary rule cycles, or ?-productions.
This appears to rule out pure top-down (?backward-
chaining?) approaches.
Efficiency. Returning the value of goal should
do only as much computation as necessary. To re-
turn goal, one may not need to compute the values
of all items.9 In particular, finding the best parse
should not require finding all parses (in contrast to
Goodman (1999) and Zhou and Sato (2003)). Ap-
proximation techniques such as pruning and best-
first search must also be supported for practicality.
4.2 The agenda algorithm
Our basic algorithm (Fig. 3) is a weighted agenda-
based algorithm that works only with rules of the
form c ?= a1?a2?? ? ??ak. ? must distribute over ?.
Further, the default value for items (line 1 of Fig. 1)
must be the semiring?s zero element, denoted 0.10
Agenda-based deduction maintains two indexed
data structures: the agenda and the chart. chart[a]
stores the current value of item a. The agenda holds
future work that arises from assertions or from pre-
vious changes to the chart: agenda[a] stores an in-
cremental update to be added (using ?) to chart[a]
in future. If chart[a] or agenda[a] is not stored, it is
9This also affects completeness, as it sometimes enables the
computation of goal to terminate even if the program as a whole
contains some irrelevant non-terminating computation. Even
in practical cases, the runtime of computing all items is often
prohibitive, e.g., proportional to n6 or worse for a dense tree-
adjoining grammar or synchronous grammar.
10It satisfies x ? 0 = x, x ? 0 = 0 for all x. Also, this
algorithm requires ? to distribute over ?. Dyna?s semantics
requires ? to be associative and commutative.
284
taken to be the default 0.
When item a is removed from the agenda, its
chart weight is updated by the increment value. This
change is then propagated to other items c, via rules
of the form c ?= ? ? ? with a on the right-hand-side.
The resulting changes to c are placed back on the
agenda and carried out only later.
The unweighted agenda-based algorithm (Shieber
et al, 1995) may be regarded as the case where
?W,?,?? = ?{T, F},?,??. It has previously
been generalized (Nederhof, 2003) to the case
?W,?,?? = ?R?0,max,+?. In Fig. 3, we make
the natural further generalization to any semiring.
How is this a further generalization? Since ? (un-
like ? and max) might not be idempotent, we must
take care to avoid erroneous double-counting if the
antecedent a combines with, or produces, another
copy of itself.11 For instance, if the input contains ?
words, line 2 of Fig. 1 may get instantiated as con-
stit(?np?,5,5) += rewrite(?np?,?np?,?np?) * constit(?np?,5,5) *
constit(?np?,5,5). This is why we save the old values
of agenda[a] and chart[a] as ? and old, and why
line 12 is complex.
4.3 Side conditions
We now extend Fig. 3 to handle Dyna?s
side conditions, i.e., rules of the form
c ?= expression whenever boolean-expression.
We discuss only the simple side conditions
treated in previous literature, which we write as
c ?= a1?a2?? ? ??ak? whenever ?bk?+1 & ? ? ? & ?bk.
Here, ?bj is true or false according to whether there
exists an unweighted proof of bj .
Again, what is new here? Nederhof (2003) con-
siders only max= with a uniform-cost agenda disci-
pline (see ?4.5), which guarantees that no item will
be removed more than once from the agenda. We
wish to support other cases, so we must take care
that a second update to ai will not retrigger rules of
which ai is a side condition.
For simplicity, let us reformulate the above rule
as c ?= a1 ? a2 ? ? ? ? ? ak? ? ?bk?+1 ? ? ? ? ? ?bk,
where ?bi is now treated as having value 0 or 1 (the
identity for ?) rather than false or true respectively.
11An agenda update that increases x by 0.3 will increase r ?
x ? x by r ? (0.6x+0.09). Hence, the rule x += r ? x ? x must
propagate a new increase of that size to x, via the agenda.
We may now use Fig. 3, but now any aj might
have the form ?bj . Then in line 12, chart[aj ] will be
chart[?bj ], which is defined as 1 or 0 according to
whether chart[bj ] is stored (i.e., whether bj has been
derived). Also, if ai = ?a at line 11 (rather than
ai = a), then ? in line 12 is replaced by ??, where
we have set ?? := chart[?a] at line 5.
4.4 Convergence
Whether the agenda algorithm halts depends on the
Dyna program and the input. Like any other Turing-
complete language, Dyna gives you enough freedom
to write undesirable programs.
Most NLP algorithms do terminate, of course,
and this remains true under the agenda algorithm.
For typical algorithms, only finitely many differ-
ent items (theorems) can be derived from a given
finite input (set of axioms).12 This ensures termi-
nation if one is doing unweighted deduction with
?W,?,?? = ?{T, F},?,??, since the test at line 7
ensures that no item is processed more than once.13
The same test ensures termination if one is
searching for the best proof or parse with (say)
?W,?,?? = ?R?0,min,+?, where values are
negated log probabilities. Positive-weight cycles
will not affect the min. (Negative-weight cycles,
however, would correctly cause the computation to
diverge; these do not arise with probabilities.)
If one is using ?W,?,?? = ?R?0,+, ?? to com-
pute the total weight of all proofs or parses, as in
the inside algorithm, then Dyna must solve a sys-
tem of nonlinear equations. The agenda algorithm
does this by iterative approximation (propagating
updates around any cycles in the proof graph until
numerical convergence), essentially as suggested by
Stolcke (1995) for the case of Earley?s algorithm.14
Again, the computation may diverge.
12This holds for all Datalog programs, for instance.
13This argument does not hold if Dyna is used to express
programs outside the semiring. In particular, one can write in-
stances of SAT and other NP-hard constraint satisfaction prob-
lems by using cyclic rules with negation over finitely many
boolean-valued items (Niemela?, 1998). Here the agenda algo-
rithm can end up flipping values forever between false and true;
a more general solver would have to be called in order to find a
stable model of a SAT problem?s equations.
14Still assuming the number of items is finite, one could in
principle materialize the system of equations and call a ded-
icated numerical solver. In some special cases only a linear
solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or
?-cycles in FSMs (Eisner, 2002).
285
One can declare the conditions under which items
of a particular type (constit or goal) should be treated
as having converged. Then asking for the value
of goal will run the agenda algorithm not until the
agenda is empty, but only until chart[goal] has con-
verged by this criterion.
4.5 Prioritization
The order in which items are chosen at line 4 does
not affect the soundness of the agenda algorithm,
but can greatly affect its speed. We implement the
agenda as a priority queue whose priority function
may be specified by the user.15
Charniak et al (1998) and Caraballo and Char-
niak (1998) showed that, when seeking the best
parse (using min= or max=), best-first parsing can
be extremely effective. Klein and Manning (2003a)
went on to describe admissible heuristics and an A*
framework for parsing. For A* in our general frame-
work, the priority of item a should be an estimate of
the value of the best proof of goal that uses a. (This
non-standard formulation is carefully chosen.16) If
so, goal is guaranteed to converge the very first time
it is selected from the priority-queue agenda.
Prioritizing ?good? items first can also be useful
in other circumstances. The inside-outside training
algorithm requires one to find all parses, but finding
the high-probability parses first allows one to ignore
the rest by ?early stopping.?
In all these schemes (even A*), processing
promising items as soon as possible risks having to
reprocess them if their values change later. Thus,
this strategy should be balanced against the ?topo-
logical sort? strategy of waiting to process an item
until its value has (probably) converged.17 Ulti-
15At present by writing a C++ function; ultimately within
Dyna, by defining items such as priority(constit(?s?,0,3)).
16It is correct for proofs that incorporate two copies of a?s
value, or?more important?no copies of a?s value because a is
a side condition. Thus, it recognizes that a low-probability item
must have high priority if it could be used as a side condition
in a higher-probability parse (though this cannot happen for the
side conditions derived by the magic templates transformation
(?6)). Note also that a?s own value (Nederhof, 2003) might not
be an optimistic estimate, if negative weights are present.
17In parsing, for example, one often processes narrower con-
stituents before wider ones. But such strategies do not always
exist, or break down in the presence of unary rule cycles, or
cannot be automatically found. Goodman?s (1999) strategy
of building all items and sorting them before computing any
weights is wise only if one genuinely wants to build all items.
mately we hope to learn priority functions that ef-
fectively balance these two strategies (especially in
the context of early stopping).
4.6 Matching, indexing, and interning
The crucial work in Fig. 3 occurs in the iteration over
instantiated rules at lines 9?11. In practice, we re-
structure this triply nested loop as follows, where
each line retains the variable bindings that result
from the unification in the previous line:
9. for each antecedent pattern ai that appears in some
program rule r and unifies with a
10. for each way of simultaneously unifying r?s remain-
ing antecedent patterns a1, . . . ai?1, ai+1, . . . ak
with items that may have non-0 value in the chart
11. construct r?s consequent c (* all vars are bound *)
Our implementation of line 9 tests a against all of the
antecedent patterns at once, using a tree of simple
?if? tests (generated by the Dyna-to-C++ compiler)
to share work across patterns. As an example, a =
constit(?np?,3,8) will match two antecedents at line 3
of Fig. 1, but will fail to match in line 4. Because a is
variable-free (for DPs), a full unification algorithm
is not necessary, even though an antecedent pattern
can contain repeated variables and nested subterms.
Line 10 rapidly looks up the rule?s other an-
tecedents using indices that are automatically main-
tained on the chart. For example, once con-
stit(?np?,4,8) has matched antecedent 2 of line 3 of
Fig. 1, the compiled code consults a maintained
list of the chart constituents that start at position 8
(i.e., items of the form constit(Z,8,K) that have al-
ready been derived). Suppose one of these is con-
stit(?vp?,8,15): then the code finds the rule?s remain-
ing antecedent by consulting a list of items of the
form rewrite(X,?np?,?vp?). That leads it to construct
consequents such as constit(?s?,4,15) at line 11.
By default, equal terms are represented by equal
pointers. While this means terms must be ?interned?
when constructed (requiring hash lookup), it en-
forces structure-sharing and allows any term to be
rapidly copied, hashed, or equality-tested without
dereferencing the pointer.18
Each of the above paragraphs conceals many deci-
sions that affect runtime. This presents future oppor-
tunities for feedback-directed optimization, where
profiled runs on typical data influence the compiler.
18The compiled code provides garbage collection on the
terms; this is important when running over large datasets.
286
5 Computing Gradients
The value of goal is a function of the axioms? values.
If the function is differentiable, we may want to get
its gradient with respect to its parameters (the axiom
values), to aid in numerically optimizing it.
5.1 Gradients by symbolic differentiation
The gradient computation can be derived from the
original by a program transformation. For each item
a in the original program?in particular, for each
axiom?the new program will also compute a new
item g(a), whose value is ?goal/?a.
Thus, given weighted axioms, the new program
computes both goal and ?goal. An optimization al-
gorithm such as conjugate gradient can use this in-
formation to tune the axiom weights to maximize
goal. An alternative is the EM algorithm (Dempster
et al, 1977) for probabilistic generative models such
as PCFGs. Luckily the same program serves, since
for such models, the E count (expected count) of an
item a can be found as a ? g(a)/goal. In other words,
the inside-outside algorithm has the same structure
as computing the function and its gradient.
The GRADIENT transformation is simple. For
example,19 given a rule c += a1 ? a2 ? ? ? ? ?
ak? whenever ?bk?+1 & ? ? ? & ?bk, we add a new rule
g(ai) += g(c) ? a1 ? ? ? ? ? ai?1 ? ai+1 ? ? ? ? ?
ak? whenever ?ai, for each i = 1, 2, ..., k?. (The orig-
inal rule remains, since we need inside values to
compute outside values.) This strategy for comput-
ing the gradient ?goal/?a via the chain rule is an
example of automatic differentiation in the reverse
mode (Griewank and Corliss, 1991), known in the
neural network community as back-propagation.
5.2 Gradients by back-propagation
However, what if goal might be computed only ap-
proximately, by early stopping before convergence
(?4.5)? To avoid confusing the optimizer, we want
the exact gradient of the approximate function.
To do this, we ?unwind? the computation of
goal, undoing the value updates while building up
the gradient values. The idea is to differentiate
an ?unrolled? version of the original computation
(Williams and Zipser, 1989), in which an item at
19More generally, g(ai) = ?goal/?ai = Pc ?goal/?c ?
?c/?ai =
P
c g(c) ? ?c/?ai by the chain rule.
1. for each a, gchart[a] := 0 and gagenda[a] := 0
(* respectively hold ?goal/?chart[a] and ?goal/?agenda[a] *)
2. gchart[goal] := 1
3. for each ?a,?, old? triple that was considered at line 8
of Fig. 3, but in the reverse order (* ? is agenda[a] *)
4. ? := gchart[a] (* will accumulate gagenda[a] here *)
5. for each inference rule ?c += a1 ? a2 ? ? ? ? ? ak?
6. for i from 1 to k
7. for each way of instantiating the rule?s variables
such that ai = a
8. for h from 1 to k such that ah is not a side cond.
(* find ?goal/?agenda[c] ? ?agenda[c]/?(ah factor) *)
9. ? :=
kY
j=1
8
>
><
>
>:
gagenda[c] if j = h
old if j 6= h and j < i
and aj = a
? if j 6= h and j = i
chart[aj ] otherwise
10. if h 6= i then gchart[ah] += ?
11. if h ? i and ah = a then ? += ?
12. gagenda[a] := ?
13. chart[a] := old
14. return gagenda[a] for each axiom a
Figure 4: An efficient algorithm for computing ?goal (even
when goal is an early-stopping approximation), specialized to
the case ?W,?,?? = ?R,+, ??. The proof is suppressed for
lack of space.
time t is considered to be a different variable (possi-
bly with different value) than the same item at time
t + 1. The reverse pass must recover earlier values.
Our somewhat tricky algorithm is shown in Fig. 4.
At line 3, a stack is needed to remember the se-
quence of ?a, old,?? triples from the original com-
putation.20 It is a more efficient version of the ?tape?
usually used in automatic differentiation. For exam-
ple, it uses O(n2) rather than O(n3) space for the
CKY algorithm. The trick is that Fig. 3 does not
record all its computations, but only its sequence of
items. Fig. 4 then re-runs the inference rules to re-
construct the computations in an acceptable order.
This method is a generalization of Eisner?s (2001)
prioritized forward-backward algorithm for infinite-
state machines. As Eisner (2001) pointed out, the
tape created on the first forward pass can also be
used to speed up later passes (i.e., after the numeri-
cal optimizer has adjusted the axiom weights).21
20If one is willing to risk floating-point error, then one can
store only ?a, old? on the stack and recover ? as chart[a]?old.
Also, agenda[a] and gagenda[a] can be stored in the same loca-
tion, as they are only used during the forward and the backward
pass, respectively.
21In brief, a later forward pass that chooses a at Fig. 3, line 4
according to the recorded tape order (1) is faster than using a
priority queue, (2) avoids ordering-related discontinuities in the
objective function as the axiom weights change, (3) can prune
by skipping useless updates a that scarcely affected goal (e.g.,
287
5.3 Parameter estimation
To support parameter training using these gradi-
ents, our implementation of Dyna includes a train-
ing module, DynaMITE. DynaMITE supports the
EM algorithm (and many variants), supervised and
unsupervised training of log-linear (?maximum en-
tropy?) models using quasi-Newton methods, and
smoothing-parameter tuning on development data.
As an object-oriented C++ library, it also facilitates
rapid implementation of new estimation techniques
(Smith and Eisner, 2004; Smith and Eisner, 2005).
6 Program Transformations
Another interest of Dyna is that its high-level speci-
fications can be manipulated by mechanical source-
to-source program transformations. This makes it
possible to derive new algorithms from old ones.
?5.1 already sketched the gradient transformation
for finding ?goal. We note a few other examples.
Bounding transformations generate a new pro-
gram that computes upper or lower bounds on goal,
via generic bounding techniques (Prieditis, 1993;
Culberson and Schaeffer, 1998). The A* heuristics
explored by Klein and Manning (2003a) can be seen
as resulting from bounding transformations.
With John Blatz, we are also exploring trans-
formations that can result in asymptotically more
efficient computations of goal. Their unweighted
versions are well-known in the logic programming
community (Tamaki and Sato, 1984; Ramakrish-
nan, 1991). Folding introduces new intermediate
items, perhaps exploiting the distributive law; ap-
plications include parsing speedups such as (Eisner
and Satta, 1999), as well as well-known techniques
for speeding up multi-way database joins, constraint
programming, or marginalization of graphical mod-
els. Unfolding eliminates items; it can be used to
specialize a parser to a particular grammar and then
to eliminate unary rules. Magic templates introduce
top-down filtering into the search strategy and can be
used to derive Earley?s algorithm (Minnen, 1996), to
introduce left-corner filters, and to restrict FSM con-
structions to build only accessible states.
Finally, there are low-level optimizations. Term
constituents not in any good parse) by consulting gagenda[a]
values that the previous backward pass can have written onto
the tape (overwriting ? or old).
transformations restructure terms to change their
layout in memory. We are also exploring the intro-
duction of declarations that control which items use
the agenda or are memoized in the chart. This can
be used to support lazy or ?on-the-fly? computation
(Mohri et al, 1998) and asymptotic space-saving
tricks (Binder et al, 1997).
7 Usefulness of the Implementation
7.1 Applications
The current Dyna compiler has proved indispens-
able in our own recent projects, in the sense that we
would not have attempted many of them without it.
In some cases, we were experimenting with gen-
uinely new algorithms not supported by any ex-
isting tool, as in our work on dependency-length-
limited parsing (Eisner and Smith, 2005b) and
loosely syntax-based machine translation (Eisner
and D. Smith, 2005). (Dyna would have been
equally helpful in the first author?s earlier work on
new algorithms for lexicalized and CCG parsing,
syntactic MT, transformational syntax, trainable pa-
rameterized FSMs, and finite-state phonology.)
In other cases (Smith and Eisner, 2004; Smith and
Smith, 2004; Smith et al, 2005), Dyna let us quickly
replicate, tweak, and combine useful techniques
from the literature. These techniques included un-
weighted FS morphology, conditional random fields
(Lafferty et al, 2001), synchronous parsers (Wu,
1997; Melamed, 2003), lexicalized parsers (Eisner
and Satta, 1999),22 partially supervised training a` la
(Pereira and Schabes, 1992),23 and grammar induc-
tion (Klein and Manning, 2002). These replications
were easy to write and extend, and to train via ?5.2.
7.2 Experiments
We compared the current Dyna compiler to hand-
built systems on a variety of parsing tasks. These
problems were chosen not for their novelty or inter-
esting structure, but for the availability of existing
well-tuned implementations.
Best parse. We compared a Dyna CFG parser
to the Java parser of Klein and Manning (2003b),24
22Markus Dreyer?s reimplementation of the complex
Collins (1999) parser uses under 30 lines of Dyna.
23For example, lines 2?3 of Fig. 1 can be extended with
whenever permitted(X,I,K).
24Neither uses heuristics from Klein and Manning (2003a).
288
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0  1  2  3  4  5  6  7  8  9  10
Dy
na 
par
ser
 ru
ntim
e (s
eco
nds
)
Klein & Manning runtime (seconds)
y=3.99x
y=7.16x
y=9.94x
y=11.25x
10 words20 words30 words40 words
Figure 5: Dyna CKY parser vs. Klein & Manning hand-built
parser, comparing runtime.
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 0  5  10  15  20  25  30  35
Dy
na 
par
ser
 ru
ntim
e (s
eco
nds
)
Handwritten parser runtime (seconds)
y=5.4x
10 words20 words30 words
Figure 6: Dyna CKY parser vs. C++PARSE, a similar hand-
built parser. The implementation differences amount to storage
and indexing and give a consistent 5-fold speedup.
on the same grammar. Fig. 5 shows the re-
sults. Dyna?s disadvantage is greater on longer
sentences?probably because its greater memory
consumption results in worse cache behavior.25
We also compared a Dyna CKY parser to
our own hand-built implementation, C++PARSE.
C++PARSE is designed like the Dyna parser but
includes a few storage and indexing optimizations
that Dyna does not yet have. Fig. 6 shows the 5-
fold speedup from these optimizations on binarized-
Treebank parsing with a large 119K-rule grammar.
The sharp diagonal indicates that C++PARSE is sim-
ply a better-tuned version of the Dyna parser.
These optimizations and others are now being in-
corporated into the Dyna compiler, and are expected
25Unlike Java, Dyna does not yet decide automatically when
to perform garbage collection. In our experiment, garbage col-
lection was called explicitly after each sentence and counted
as part of the runtime (typically 0.25 seconds for 10-word sen-
tences, 5 seconds for 40-word sentences).
99% 99.99%
uniform 89.3 (4.5) 90.3 (4.6)
after 1 EM iteration 82.9 (6.8) 85.2 (6.9)
after 2 EM iterations 77.1 (8.4) 79.1 (8.3)
after 3 EM iterations 71.6 (9.4) 73.7 (9.5)
after 4 EM iterations 66.8 (10.0) 68.8 (10.2)
after 5 iterations 62.9 (10.3) 65.0 (10.5)
Table 1: Early stopping. Each row describes a PCFG at a differ-
ent stage of training; later PCFGs are sharper. The table shows
the percentage of agenda runtime (mean across 1409 sentences,
and standard deviation) required to get within 99% or 99.99%
of the true value of goal.
to provide similar speedups, putting Dyna?s parser
in the ballpark of the Klein & Manning parser. Im-
portantly, these improvements will speed up existing
Dyna programs through recompilation.
Inside parsing. Johnson (2000) provides a C im-
plementation of the inside-outside algorithm for EM
training of PCFGs. We ran five iterations of EM
on the WSJ10 corpus26 using the Treebank grammar
from that corpus. Dyna took 4.1 times longer.
Early stopping. An advantage of the weighted
agenda discipline (?4.2) is that, with a reasonable
priority function such as an item?s inside probabil-
ity, the inside algorithm can be stopped early with
an estimate of goal?s value. To measure the goodness
of this early estimate, we tracked the progression of
goal?s value as each sentence was being parsed. In
most instances, and especially after more EM itera-
tions, the estimate was very tight long before all the
weight had been accumulated (Table 1). This sug-
gests that early stopping is a useful training speedup.
PRISM. The implemented tool most similar to
Dyna that we have found is PRISM (Zhou and Sato,
2003), a probabilistic Prolog with efficient tabling
and compilation. PRISM inherits expressive power
from Prolog but handles only probabilities, not gen-
eral semirings (or even side conditions).27 In CKY
parsing tests, PRISM was able to handle only a small
fraction of the Penn Treebank ruleset (2,400 high-
probability rules) and tended to crash on long sen-
tences. Dyna is designed for real-world use: it con-
sistently parses over 10? faster than PRISM and
scales to full-sized problems.
IBAL (Pfeffer, 2001) is an elegant and power-
ful language for probabilistic modeling; it general-
izes Bayesian networks in interesting ways.28 Since
26Sentences with ?10 words, stripping punctuation.
27Thus it can handle a subset of the cases described by
Goodman (1999), again by building the whole parse forest.
28It might be possible to implement IBAL in Dyna (Pfeffer,
289
PCFGs and marginalization can be succinctly ex-
pressed in IBAL, we attempted a performance com-
parison on the task of the inside algorithm (Fig. 1).
Unfortunately, IBAL?s algorithm appears not to ter-
minate if the PCFG contains any kind of recursion
reachable from the start symbol.
8 Conclusions
Weighted deduction is a powerful theoretical for-
malism that encompasses many NLP algorithms
(Goodman, 1999). We have given a bottom-up ?in-
side? algorithm for general semiring-weighted de-
duction, based on a prioritized agenda, and a general
?outside? algorithm that correctly computes weight
gradients even when the inside algorithm is pruned.
We have also proposed a declarative language,
Dyna, that replaces Prolog?s Horn clauses with
?Horn equations? over terms with values. Dyna can
express more than the semiring-weighted dynamic
programs treated in this paper. Our ongoing work
concerns the full Dyna language, program transfor-
mations, and feedback-directed optimization.
Finally, we evaluated our first implementation of
a Dyna-to-C++ compiler (download and documen-
tation at http://dyna.org). We hope it will facili-
tate EMNLP research, just as FS toolkits have done
for the FS case. It produces code that is slower than
hand-crafted code but acceptably fast for our NLP
research, where it has been extremely helpful.
References
J. Binder, K. Murphy, and S. Russell. 1997. Space-efficient inference in dynamic
probabilistic networks. In Proc. of IJCAI.
S. A. Caraballo and E. Charniak. 1998. New figures of merit for best-first proba-
bilistic chart parsing. CL, 24(2):275?298.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-based best-first chart
parsing. In Proc. of COLING-ACL.
M. J. Collins. 1999. Head-Driven Statistical Models for Natural Language Pars-
ing. Ph.D. thesis, U. of Pennsylvania.
J. C. Culberson and J. Schaeffer. 1998. Pattern databases. Computational Intelli-
gence.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood estimation
from incomplete data via the EM algorithm. Journal of the Royal Statistical
Society B, 39:1?38.
J. Earley. 1970. An efficient context-free parsing algorithm. Communications of
the ACM, 13(2):94?102.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical CFGs and head-
automaton grammars. In Proc. of ACL.
J. Eisner and D. A. Smith. 2005a. Quasi-synchronous grammars: Alignment by
soft projection of syntactic dependencies. Technical report, Johns Hopkins U.
J. Eisner and N. A. Smith. 2005b. Parsing with soft and hard constraints on
dependency length. In Proc. of IWPT.
p.c.). Dyna is a lower-level language that itself knows nothing
about the semantics of probability models, but whose inference
rules could be used to implement any kind of message passing.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declarative language for
implementing dynamic programs. In Proc. of ACL (companion vol.).
J. Eisner. 2001. Smoothing a Probabilistic Lexicon via Syntactic Transforma-
tions. Ph.D. thesis, U. of Pennsylvania.
J. Eisner. 2002. Parameter estimation for probabilistic FS transducers. In Proc.
of ACL.
J. Goodman. 1999. Semiring parsing. CL, 25(4):573?605.
A. Griewank and G. Corliss, editors. 1991. Automatic Differentiation of Algo-
rithms. SIAM.
M. Johnson. 2000. Inside-outside (computer program).
http://www.cog.brown.edu/?mj/Software.htm.
L. Karttunen, J.-P. Chanod, G. Grefenstette, and A. Schiller. 1996. Regular ex-
pressions for language engineering. JNLE, 2(4):305?328.
M. Kifer and V. S. Subrahmanian. 1992. Theory of generalized annotated
logic programming and its applications. Journal of Logic Programming,
12(4):335?368.
D. Klein and C. D. Manning. 2002. A generative constituent-context model for
grammar induction. In Proc. of ACL.
D. Klein and C. D. Manning. 2003a. A? parsing: Fast exact Viterbi parse selec-
tion. In Proc. of HLT-NAACL.
D. Klein and C. D. Manning. 2003b. Accurate unlexicalized parsing. In Proc. of
ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling sequence data. In Proc. of ICML.
I. D. Melamed. 2003. Multitext grammars and synchronous parsers. In Proc.
HLT-NAACL.
G. Minnen. 1996. Magic for filter optimization in dynamic bottom-up processing.
In Proc. of ACL.
M. Mohri, F. Pereira, and M. Riley. 1998. A rational design for a weighted FST
library. LNCS, 1436.
M.-J. Nederhof. 2003. Weighted deductive parsing and Knuth?s algorithm. CL,
29(1):135?143.
I. Niemela?. 1998. Logic programs with stable model semantics as a constraint
programming paradigm. In Proc. Workshop on Computational Aspects of
Nonmonotonic Reasoning.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially brack-
eted corpora. In Proc. of ACL.
F. Pereira and D. H. D. Warren. 1983. Parsing as deduction. In Proc. of ACL.
A. Pfeffer. 2001. IBAL: An integrated Bayesian agent language. In Proc. of
IJCAI.
A. Prieditis. 1993. Machine discovery of effective admissible heuristics. Ma-
chine Learning, 12:117?41.
R. Ramakrishnan, D. Srivastava, S. Sudarshan, and P. Seshadri. 1994. The
CORAL deductive system. The VLDB Journal, 3(2):161?210.
R. Ramakrishnan. 1991. Magic templates: a spellbinding approach to logic pro-
grams. J. Log. Program., 11(3-4):189?216.
K. A. Ross and Y. Sagiv. 1992. Monotonic aggregation in deductive databases.
In Proc. of the ACM SIGACT-SIGMOD-SIGART Symposium on Principles of
Database Systems.
S. M. Shieber, Y. Schabes, and F. Pereira. 1995. Principles and implementation
of deductive parsing. Journal of Logic Programming, 24(1?2):3?36.
K. Sikkel. 1997. Parsing Schemata: A Framework for Specification and Analysis
of Parsing Algorithms. Texts in Theoretical Computer Science. Springer.
N. A. Smith and J. Eisner. 2004. Annealing techniques for unsupervised statisti-
cal language learning. In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation: Training log-linear
models on unlabeled data. In Proc. of ACL.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing with factored estimation:
Using English to parse Korean. In Proc. of EMNLP.
N. A. Smith, D. A. Smith, and R. W. Tromble. 2005. Context-based morphologi-
cal disambiguation with random fields. In Proc. of HLT-EMNLP.
G. Specht and B. Freitag. 1995. AMOS: A NL parser implemented as a deductive
database in LOLA. In Applications of Logic Databases. Kluwer.
A. Stolcke. 1995. An efficient probabilistic CF parsing algorithm that computes
prefix probabilities. CL, 21(2):165?201.
H. Tamaki and T. Sato. 1984. Unfold/fold transformation of logic programs.
In S. A?. Ta?rnlund, editor, Proceedings Second International Conference on
Logic Programming, pages 127?138, Uppsala University.
R. J. Williams and D. Zipser. 1989. A learning algorithm for continually running
fully recurrent neural networks. Neural Computation, 1(2):270?280.
D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing
of parallel corpora. CL, 23(3):377?404.
N.-F. Zhou and T. Sato. 2003. Toward a high-performance system for symbolic
and statistical modeling. In Proc. of Workshop on Learning Statistical Models
from Relational Data.
U. Zukowski and B. Freitag. 1997. The deductive database system LOLA. In
Logic Programming and Nonmonotonic Reasoning, LNAI 1265. Springer.
290
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 475?482, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Context-Based Morphological Disambiguation with Random Fields?
Noah A. Smith and David A. Smith and Roy W. Tromble
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,dasmith,royt}@cs.jhu.edu
Abstract
Finite-state approaches have been highly successful at describ-
ing the morphological processes of many languages. Such
approaches have largely focused on modeling the phone- or
character-level processes that generate candidate lexical types,
rather than tokens in context. For the full analysis of words
in context, disambiguation is also required (Hakkani-Tu?r et al,
2000; Hajic? et al, 2001). In this paper, we apply a novel
source-channel model to the problem of morphological disam-
biguation (segmentation into morphemes, lemmatization, and
POS tagging) for concatenative, templatic, and inflectional lan-
guages. The channel model exploits an existing morphological
dictionary, constraining each word?s analysis to be linguistically
valid. The source model is a factored, conditionally-estimated
random field (Lafferty et al, 2001) that learns to disambiguate
the full sentence by modeling local contexts. Compared with
baseline state-of-the-art methods, our method achieves statisti-
cally significant error rate reductions on Korean, Arabic, and
Czech, for various training set sizes and accuracy measures.
1 Introduction
One of the great successes in computational linguistics
has been the construction of morphological analyzers for
diverse languages. Such tools take in words and enu-
merate the possible morphological analyses?typically a
sequence of morphemes, perhaps part-of-speech tagged.
They are often encoded as finite-state transducers (Ka-
plan and Kay, 1981; Koskenniemi, 1983; Beesley and
Karttunen, 2003).
What such tools do not provide is a means to dis-
ambiguate a word in context. For languages with com-
plex morphological systems (inflective, agglutinative,
and polysynthetic languages, for example), a word form
may have many analyses. To pick the right one, we
must consider the word?s context. This problem has
been tackled using statistical sequence models for Turk-
ish (Hakkani-Tu?r et al, 2000) and Czech (Hajic? et al,
2001); their approaches (and ours) are not unlike POS
tagging, albeit with complex tags.
?This work was supported by a Fannie and John Hertz
Foundation Fellowship, a NSF Fellowship, and a NDSEG Fel-
lowship (sponsored by ARO and DOD). The views expressed
are not necessarily endorsed by sponsors. We thank Eric Gold-
lust and Markus Dreyer for Dyna language support and Jason
Eisner, David Yarowsky, and three anonymous reviewers for
comments that improved the paper. We also thank Jan Hajic?
and Pavel Krbec for sharing their Czech tagger.
In this paper, we describe context-based models for
morphological disambiguation that take full account of
existing morphological dictionaries by estimating condi-
tionally against only dictionary-accepted analyses of a
sentence (?2). These models are an instance of condi-
tional random fields (CRFs; Lafferty et al, 2001) and
include overlapping features. Our applications include
diverse disambiguation frameworks and we make use of
linguistically-inspired features, such as local lemma de-
pendencies and inflectional agreement. We apply our
model to Korean and Arabic, demonstrating state-of-the-
art results in both cases (?3). We then describe how our
model can be expanded to complex, structured morpho-
logical tagging, including an efficient estimation method,
demonstrating performance on Czech (?4).
2 Modeling Framework
Our framework is a source-channel model (Jelinek,
1976). The source (modeled probabilistically by ps) gen-
erates a sequence of unambiguous tagged morphemes
y = ?y1, y2, ...? ? Y+ (Y is the set of unambiguous
tagged morphemes in the language).1 The precise con-
tents of the tag will vary by language and corpus but
will minimally include POS. y passes through a chan-
nel (modeled by pc), which outputs x = ?x1, x2, ...? ?
(X ? {OOV})+, a sequence of surface-level words in the
language and out-of-vocabulary words (OOV; X is the
language?s vocabulary). Note that |x| may be smaller
than |y|, since some morphemes may combine to make
a word. We will denote by yi the contiguous subse-
quence of y that generates xi; ~y will refer to a dictionary-
recognized type in Y+.
At test time, we decode the observed x into the most
probable sequence of tag/morpheme pairs:
y? = argmax
y
p(y | x) = argmax
y
ps(y) ? pc(x | y) (1)
Training involves constructing ps and pc. We assume
that there exists a training corpus of text (each word xi
annotated with its correct analysis y?i ) and a morpholog-
ical dictionary. We next describe the channel model and
the source model.
1The sequence also includes segmentation markings be-
tween words, not shown to preserve clarity.
475
a. There are many kinds of trench mortars.
b. . ? 1998 1998?Sanaa accuses Riyadh of occupying border territories.
0 1NUM/1998 2PUNC/- 3NOUN_PROP/SnEA?NOUN_PROP/SanoEA? 4
IV3FS/tuIV2MS/tu
5IV3FS/taIV2MS/ta 6NOUN_PROP/tthm
IV_PASS/t~ahamIV/t~ahim 7 8 9 10 11 12 13 14
c. Klimatizovana? j??delna, sve?tla? m??stnost pro sn??dane?. Air-conditioned dining room, well-lit breakfast room.
0 1
Adj {Neu Pl Acc Pos Aff}/klimatizovan?Adj {Neu Pl Voc Pos Aff}/klimatizovan?Adj {Fem Si Voc Pos Aff}/klimatizovan?Adj {Fem Si Nom Pos Aff}/klimatizovan?Adj {Neu Pl Nom Pos Aff}/klimatizovan? 2Noun {Fem Si Nom Aff}/j?delna 3Punc/, 4
Adj {Neu Pl Pos Aff}/svetl?Adj {Fem Si Voc Pos Aff}/svetl?Adj {Neu Pl Acc Pos Aff}/svetl?Adj {Neu Pl Voc Pos Aff}/svetl?Adj {Fem Si Nom Pos Aff}/svetl? 5Noun {Fem Si Acc Aff}/m?stnostNoun {Fem Si Nom Aff}/m?stnost 6 7 8
Figure 1: Lattices for example sentences in Korean (a), Arabic (b), and Czech (c). Arabic lemmas are not shown, and some Arabic
and Czech arcs are unlabeled, for readability. The Arabic morphemes are shown in Buckwalter?s encoding. The arcs in the correct
path through each lattice are solid (incorrect arcs are dashed). Note the adjective-noun agreement in the correct path through the
Czech lattice (c). The Czech lattice has no lemma-ambiguity; this is typical in Czech (see ?4).
2.1 Morphological dictionaries and the channel
A great deal of research has gone into developing mor-
phological analysis tools that enumerate valid analyses
~y ? Y+ for a particular word x ? X. Typically these
tools are unweighted and therefore do not enable token
disambiguation.2
They are available for many languages. We will refer
to this source of categorial lexical information as a mor-
phological dictionary d that maps X ? 2Y+ . The set d(x)
is the set of analyses for word x; the set d(x) is the set of
whole-sentence analyses for sentence x = ?x1, x2, ...?.
d(x) can be represented as an acyclic lattice with a
?sausage? shape familiar from work in speech recogni-
tion (Mangu et al, 1999). Note that for languages with
bound morphemes, d(x) will consist of a set of sequences
of tokens, so a given ?link? in the sausage lattice may
contain paths of different lengths. Fig. 1 shows sausage
lattices for sentences in three languages.
In this paper, the dictionary defines the support set of
the channel model. That is, pc(x | y) > 0 if and only
if y ? d(x). This is a clean way to incorporate do-
main knowledge into the probabilistic model; this kind
of constraint has been applied in previous work at decod-
ing time (Hakkani-Tu?r et al, 2000; Hajic? et al, 2001). In
such a model, each word is independent of its neighbors
(because the dictionary ignores context).
Estimation. A unigram channel model defines
2Probabilistic modeling of what we call the morphologi-
cal channel was first carried out by Levinger et al (1995), who
used unlabeled data to estimate p(~y | x) for Hebrew, with the
support defined by a dictionary.
pc(x | y)
def
=
|x|?
i=1
p(xi | yi) (2)
The simplest estimate of this model is to make p(?, ?)
uniform over (x, ~y) such that ~y ? d(x). Doing so and
marginalizing to get p(x | ~y) makes the channel model
encode categorial information only, leaving all learning
to the source model.3
Another way to estimate this model is, of course,
from data. This is troublesome, because?modulo
optionality?x is expected to be known given ~y, result-
ing in a huge model with mostly 1-valued probabili-
ties. Our solution is to take a projection pi of ~y and let
p(? | ~y) ? p(? | pi(~y)). In this paper, pi maps the analysis
to its morphological tag (or tag sequence). We will refer
to this as the ?tag channel.?
OOV. Morphological dictionaries typically do not have
complete coverage of a language. We can augment them
in two ways using the training data. If a known word x
(one for which d(x) is non-empty) appears in the training
dataset with an analysis not in d(x), we add the entry to
the dictionary. Unknown words (those not recognized by
the dictionary) are replaced by an OOV symbol. d(OOV)
is taken to be the set of all analyses for any OOV word
seen in training. Rather than attempt to recover the mor-
pheme sequence for an OOV word, in this paper we try
only for the tag sequence, replacing all of an OOV?s mor-
phemes with the OOV symbol. Since OOV symbols ac-
count for less than 2% of words in our corpora, we leave
3Note that this makes the channel term in Eq. 1 a constant.
Then decoding means maximizing ps(y) over y ? d(x), equiv-
alently maximizing p(y | d(x)).
476
more sophisticated channel models to future work.
2.2 The source model
The source model ps defines a probability distribution
over Y+, sequences of (tag, morpheme) pairs. Our source
models can be viewed as weighted multi-tape finite-state
automata, where the weights are associated with local, of-
ten overlapping features of the path through the automa-
ton.
Estimation. We estimate the source conditionally from
annotated data. That is, we maximize
?
(x,y)?X+?Y+
p?(x,y) log ps
(
y | d(x), ~?
)
(3)
where p?(?, ?) is the empirical distribution defined by the
training data and ~? are the model parameters. In terms
of Fig. 1, our learner maximizes the weight of the correct
(solid) path through each lattice, at the expense of the
other incorrect (dashed) paths. Note that
log ps
(
y | d(x), ~?
)
= log
ps
(
y | ~?
)
?
y??d(x) ps
(
y? | ~?
) (4)
The sum in the denominator is computed using a dynamic
programming algorithm (akin to the forward algorithm);
it involves computing the sum of all paths through the
?sausage? lattice of possible analyses for x. By doing
this, we allow knowledge of the support of the channel
model to enter into our estimation of the source model. It
is important to note that the estimation of the model (the
objective function used in training, Eq. 3) is distinct from
the source-channel structure of the model (Eq. 1).
The lattice-conditional estimation approach was
first used by Kudo et al (2004) for Japanese seg-
mentation and hierarchical POS-tagging and by
Smith and Smith (2004) for Korean morphological
disambiguation. The resulting model is an instance of
a conditional random field (CRF; Lafferty et al, 2001).
When training a CRF for POS tagging, IOB chunking
(Sha and Pereira, 2003), or word segmentation (Peng
et al, 2004), one typically structures the conditional
probabilities (in the objective function) using domain
knowledge: in POS tagging, the set of allowed tags for
a word is used; in IOB chunking, the bigram ?O I? is
disallowed; and in segmentation, a lexicon is used to
enumerate the possible word boundaries.4
4This refinement is in the same vein as the move from max-
imum likelihood estimation to conditional estimation. MLE
would make the sum in the denominator of Eq. 4 Y+, which
for log-linear models is often intractable to compute (and for
sequence models may not converge). Conditional estimation
limits the sum to the subset of Y+ that is consistent with x, and
our variant further stipulates consistency with the dictionary en-
tries for x.
Our approach is the same, with two modifications.
First, we model the relationship between labels yi and
words xi in a separately-estimated channel model (?2.1).
Second, our labels are complex. Each word xi is tagged
with a sequence of one or more tagged morphemes; the
tags may include multiple fields. This leads to models
with more parameters. It also makes the dictionary es-
pecially important for limiting the size of the sum in the
denominator, since a complex label set Y could in prin-
ciple lead to a huge hypothesis space for a given sen-
tence x. Importantly, it makes training conditions more
closely match testing conditions, ruling out hypotheses a
dictionary-aware decoder would never consider.
Optimization. The objective function (Eq. 3) is con-
cave and known to have a unique global maximum. Be-
cause log-linear models and CRFs have been widely de-
scribed elsewhere (e.g., Lafferty, 2001), we note only that
we apply a standard first-order numerical optimization
method (L-BFGS; Liu and Nocedal, 1989). The struc-
ture, features, and regularization of our models will be
described in ?3 and ?4.
Prior work (morphological source models).
Hakkani-Tu?r et al (2000) described a system for Turkish
that was essentially a source model; Hajic? et al (2001)
described an HMM-based system for Czech that could
be viewed as a combined source and channel. Both
used dictionaries and estimated their (generative) models
using maximum likelihood (with smoothing).5 Given
enough data, a ML-estimated model will learn to recog-
nize a good path y, but it may not learn to discriminate
a good y from wrong alternatives per se. The generative
framework is limiting as well, disallowing the straight-
forward inclusion of arbitrary overlapping features. We
present a competitive Czech model in ?4.
3 Concatenative Models
The beauty of log-linear models is that estimation is
straightforward given any features, even ones that are
not orthogonal (i.e., ?overlap?). This permits focusing
on feature (or feature template) selection without worries
about the mathematics of training.
We consider two languages modeled by concatenative
processes with surface changes at morpheme boundaries:
Korean and Arabic.
Our model includes features for tag n-grams, mor-
pheme n-grams, and pairs of the two (possibly of differ-
ent lengths and offsets). Fig. 2 illustrates TM3, our base
model. TM3 includes feature templates for some tuples
of three or fewer elements, plus begin and end templates.
5Hajic? et al also included a rule-based system for pruning
hypotheses, which gave slight performance gains.
477
i?1
i?1
T
M
Tn
Mn
?2iT
?2iMM1
1T Ti
Mi
morpheme trigram
tag trigram
begin
features
end
features
tag/morpheme pair
tag + prev. morpheme
tag bigram
morpheme unigram
Figure 2: The base two-level trigram source model, TM3. Each
polygon corresponds to a feature template. This is a two level,
second-order Markov model (weighted finite-state machine) pa-
rameterized with overlapping features. Note that only some fea-
tures are labeled in the diagram.
A variant, TM3H, includes all of the same templates,
plus a similar set of templates that look only at head mor-
phemes. For instance, a feature fires for each trigram
of heads, even though there are (bound) morphemes be-
tween them. This increases the domain of locality for se-
mantic content-bearing morphemes. This model requires
slight changes to the dynamic programming algorithms
for inference and training (the previous two heads must
be remembered at each state).
Every instantiation of the templates seen in any lattice
d(x) built from training data is included in the model, not
just those seen in correct analyses y?.6
3.1 Experimental design
In all of our experiments, we vary the training set size
and the amount of smoothing, which is enforced by a di-
agonal Gaussian prior (L2 regularizer) with variance ?2.
The ?2 = ? case is equivalent to not smoothing. We
compare performance to the expected performance of a
randomized baseline that picks for each word token x an
analysis from d(x); this gives a measure of the amount of
ambiguity and is denoted ?channel only.? Performance
of unigram, bigram, and trigram HMMs estimated us-
ing maximum likelihood (barely smoothed, using add-
10?14) is also reported. (The unigram HMM simply
picks the most likely ~y for each x, based on training data
and is so marked.)
In the experiments in this section, we report three per-
formance measures. Tagging accuracy is the fraction
of words whose tag sequence was correctly identified
in entirety; morpheme accuracy is defined analogously.
6If we used only features observed to occur in y?, we would
not be able to learn negative weights for unlikely bits of structure
seen in the lattice d(x) but not in y?.
Lemma accuracy is the fraction of words whose lemma
was correctly identified.
3.2 Korean experiments
We applied TM3 and TM3H to Korean. The dataset is
the Korean Treebank (Han et al, 2002), with up to 90%
used for training and 10% (5K words) for test. The mor-
phological dictionary is klex (Han, 2004). There are 27
POS tags in the tag set; the corpus contains 10K word
types and 3,272 morpheme types. There are 1.7 mor-
phemes per word token on average (? = 0.75). A Ko-
rean word generally consists of a head morpheme with a
series of enclitic suffixes. In training the head-augmented
model TM3H, we assume the first morpheme of every
word is the head and lemma.
Results are shown in Tab. 1. TM3H achieved very slight
gains over TM3, and the tag channel model was helpful
only with the smaller training set. The oracle (last line
of Tab. 1) demonstrates that the coverage of the dictio-
nary remains an obstacle, particularly for recovering mor-
phemes. Another limitation is the small amount of train-
ing data, which may be masking differences among esti-
mation conditions. We report the performance of TM3H
with ?factored? estimation. This will be discussed in
detail in ?4; it means that a model containing only the
head features was trained on its own, then combined with
the independently trained TM3 model at test time. Fac-
tored training was slightly faster and did not affect per-
formance at all; accuracy scores were identical with un-
factored training.
Prior work (Korean). Similar results were presented
by Smith and Smith (2004), using a similar estimation
strategy with a model that included far more feature tem-
plates. TM3 has about a third as many parameters and
TM3H about half; performance is roughly the same (num-
bers omitted for space). Korean disambiguation results
were also reported by Cha et al (1998), who applied a
deterministic morpheme pattern dictionary to segment
words, then used a bigram HMM tagger. They also ap-
plied transformation-based learning to fix common er-
rors. Due to differences in tag set and data, we cannot
compare to that model; a bigram baseline is included.
3.3 Arabic experiments
We applied TM3 and TM3H to Arabic. The dataset is the
Arabic Treebank (Maamouri et al, 2003), with up to 90%
used for training and 10% (13K words) for test. The mor-
phological dictionary is Buckwalter?s analyzer (version
2), made available by the LDC (Buckwalter, 2004).7 This
analyzer has total coverage of the corpus; there are no
7Arabic morphological processing was also addressed by
Kiraz (2000), who gives a detailed review of symbolic work in
that area, and by Darwish (2002).
478
Korean Arabic
POS tagging morpheme lemma POS tagging morpheme lemma
accuracy accuracy accuracy accuracy accuracy accuracy
?2 32K 49K 32K 49K 32K 49K 38K 76K 114K 38K 76K 114K 38K 76K 114K
most likely ~y 86.0 86.9 87.5 88.8 95.3 95.7 84.5 87.0 88.3 83.2 86.2 87.0 37.9 39.8 40.9
channel only 62.6 62.6 70.3 70.8 86.4 86.4 43.7 43.7 43.7 41.2 41.2 41.2 27.2 27.2 27.2
bigram HMM 90.7 91.2 83.2 86.1 96.9 97.2 90.3 92.0 92.8 89.2 91.4 91.6 85.7? 87.8? 87.9?
trigram HMM 91.5 91.8 83.3 86.0 97.0 97.2 89.8 92.0 93.0 88.5 91.3 91.3 85.2? 87.8? 87.7?
TM3 ? 90.7 91.3 89.3 90.5 97.1 97.4 94.6 95.4 95.9 93.4 94.3 94.9 89.7? 90.5? 90.7?
u
n
ifo
rm
ch
an
ne
l
10 91.2 91.7 89.4 90.6 97.1 97.6 95.3 95.7 96.1 93.9 94.5 95.0 90.2? 90.6? 91.1?
1 91.5 92.2 89.4 90.6 97.1 97.5 95.2 95.7 96.0 93.9 94.5 94.7 90.0? 90.7? 91.0?
TM3H ? 91.1 91.1 89.3 90.4 97.2 97.5 95.0 95.7 96.0 94.0 94.8 95.3 93.3 93.9 94.2
(factored) 10 91.3 91.9 89.5 90.6 97.3 97.6 95.3 95.7 96.1 94.2 94.7 95.4 93.4 93.6 94.4
1 91.4 92.2 89.5 90.7 97.3 97.6 95.4 95.8 96.1 94.4 94.8 95.1 93.3 93.8 94.2
channel only 51.4 51.3 60.6 60.4 81.2 81.7 41.4 40.6 40.1 39.9 39.1 38.6 26.7? 26.5? 26.4?
bigram HMM 91.2 90.9 88.9 90.1 97.0 97.3 91.0 92.3 93.4 89.7 91.5 91.9 88.1? 89.9? 90.0?
trigram HMM 91.6 91.9 88.9 90.2 97.1 97.4 91.1 92.9 93.7 89.6 92.2 92.0 88.1? 90.6? 90.4?
TM3 ? 90.8 91.0 89.5 90.5 97.4 97.5 95.1 95.7 96.0 93.8 94.6 95.0 92.2? 93.1? 93.2?
ta
g
ch
an
ne
l
10 90.6 91.1 89.5 90.7 97.2 97.6 95.2 95.6 96.0 93.9 94.7 95.0 92.4? 93.2? 93.5?
1 90.1 90.9 89.5 90.7 97.1 97.6 94.9 95.5 95.8 93.8 94.5 94.8 92.2? 93.0? 93.1?
TM3H ? 91.0 91.0 89.4 90.5 97.2 97.6 95.1 95.8 96.0 94.0 95.1 95.4 93.3 94.3 94.4
(factored) 10 90.4 91.2 89.6 90.7 97.4 97.6 95.2 95.7 96.0 94.1 94.8 95.4 93.3 94.0 94.6
1 90.1 91.0 89.5 90.7 97.3 97.6 95.1 95.5 95.9 94.1 94.9 95.1 93.3 94.0 94.4
oracle given d(x) 95.3 95.7 90.2 91.2 98.1 98.3 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
Table 1: Korean (left, 5K test-set) and Arabic (right, 13K test-set) disambiguation. A word is marked correct only if its entire
tag (or morpheme) sequence (or lemma) was correctly identified. Morpheme and lemma accuracy do not include OOV words. The
oracle is an upper bound on accuracy given the morphological dictionary. ?These models do not explicitly predict lemmas; the
lemma is chosen arbitrarily from those that match the hypothesized tag/morpheme sequence for each word. Bold scores indicate a
significant improvement over the trigram HMM (binomial sign test, p < 0.05).
OOV words. There are 139 distinct POS tags; these con-
tain some inflectional information which we treat atom-
ically. For speed, TM3H was trained in two separate
pieces: TM3 and the lemma features added by TM3H.
Arabic has a templatic morphology in which conso-
nantal roots are transformed into surface words by the
insertion of vowels and ancillary consonants. Our sys-
tem does not model this process except through the use
of Buckwalter?s dictionary to define the set of analyses
for each word (cf., Daya et al, 2004, who modeled inter-
digitation in Hebrew). We treat the analysis of an Ara-
bic word as a sequence ~y of pairs of morphemes and
POS tags, plus a lemma. The lemma, given in the dic-
tionary, provides further disambiguation beyond the head
morpheme. The lemma is a standalone dictionary head-
word and not merely the consonantal root, as in some
other work. The ?heads? modeled by TM3H correspond
to these lemmas. There are 20K word types, and 34K
morpheme types. There are 1.7 morphemes per word to-
ken on average (? = 0.77).
Results are shown in Tab. 1. Across tasks and training
set sizes, our models reduce error rates by more than 36%
compared to the trigram HMM source with tag channel.
The TM3H model and the tag channel offer slight gains
over the base TM3 model (especially on lemmatization),
though the tag channel offers no help in POS tagging.
Prior work (Arabic). Both Diab et al (2004) and
Habash and Rambow (2005) use support-vector ma-
chines with local features; the former for tokenization,
POS tagging, and base phrase chunking; the latter for
full morphological disambiguation. Diab et al report
results for a coarsened 24-tag set, while we use the full
139 tags from the Arabic Treebank, so the systems are
not directly comparable. Habash and Rambow present
even better results on the same POS tag set. Our full dis-
ambiguation results appear to be competitive with theirs.
Khoja (2001) and Freeman (2001) describe Arabic POS
taggers and many of the issues involved in developing
them, but because tagged corpora did not yet exist, there
are no comparable quantitative results.
4 Czech: Model and Experiments
Inflective languages like Czech present a new set of chal-
lenges. Our treatment of Czech is not concatenative;
following prior work, the analysis for each word x is a
single tag/lemma pair y. Inflectional affixes in the sur-
face form are represented as features in the tag. While
lemmatization of Czech is not hard (there is little ambi-
guity), tagging is quite difficult, because morphological
tags are highly complex. Our tag set is the Prague Depen-
dency Treebank (PDT; Hajic?, 1998) set, which consists of
fifteen-field tags that indicate POS as well as inflectional
information (case, number, gender, etc.). There are over
479
full model (decoding) factored models (training)
gender
number case
POSmo
rpholo
gical tagsmorph. tag
num.gen. case POS
lemma lemm
asy
y
y
y
1
2
3
4
Figure 3: The Czech model, shown as an undirected graphi-
cal model. The structure of the full model is on the left; fac-
tored components for estimation are shown on the right. Each
of these five models contains a subset of the TM3 features. The
full model is only used to decode. The factored models make
training faster and are used for pruning.
1,400 distinct tag types in the PDT.
Czech has been treated probabilistically before, per-
haps most successfully by Hajic? et al (2001).8 In con-
trast, we estimate conditionally (rather than by maximum
likelihood for a generative HMM) and separate the train-
ing of the source and the channel. We also introduce a
novel factored treatment of the morphological tags.
4.1 Factored tags and estimation
Because Czech morphological tags are not monolithic,
the choice among them can be treated as several more or
less orthogonal decisions. The case feature of one word,
for example, is expected to be conditionally independent
of the next word?s gender, given the next word?s case.
Constraints in the language are expected to cause features
like case, number, and gender to agree locally (on words
that have such features) and somewhat independently of
each other. Coarser POS tagging may be treated as an-
other, roughly independent stream.
Log-linear models and the use of a morphological dic-
tionary make this kind of tag factoring possible. Our
approach is to separately train five log-linear models.
Each model is itself an instance of some of the templates
from TM3, modeling a projection of the full analysis.
The model and its factored components are illustrated in
Fig. 3.
POS model. The full tag is replaced by the POS tag
(the first two fields); there are 60 POS tags. The TM3
8Czech morphological processing was studied by
Petkevic? (2001), Hlava?cova? (2001) (who focuses on han-
dling OOV words), and Mra?kova? and Sedlacek (2003) (who use
partial parsing to reduce the set of possible analyses), inter alia.
feature templates are included twice: once for the full tag
and once for a coarser tag (the first PDT field, for which
there are 12 possible values).9
Gender, number, and case models. The full tag is re-
placed by the gender (or case or number) field. This
model includes bigrams and trigrams as well as field-
morpheme unigram features. These models are intended
to learn to predict local agreement.
Tag-lemma model. This model contains unigram fea-
tures of full PDT tags, both alone and with lemmas. It is
intended to learn to penalize morphological tags that are
rare, or that are rare with a particular lemma. In our for-
mulation, this is not a channel model, because it ignores
the surface word forms.
Each model is estimated independently of the others.
The lattice d(x) against which the conditional probabili-
ties are estimated contains the relevant projection of the
full morphological tags (with lemmas). To decode, we
run a Viterbi-like algorithm that uses the union of all
models? features to pick the best analysis (full morpho-
logical tags and lemmas) allowed by the dictionary.
There are two important advantages of factored train-
ing. First, each model is faster to train alone than a model
with all features merged; in fact, training the fully merged
model takes far too long to be practical. Second, factored
models can be held out at test time to measure their effect
on the system, without retraining.
Prior work (factored training). Separately training
different models that predict the same variables (e.g., x
and y) then combining them for consensus-based infer-
ence (either through a mixture or a product of proba-
bilities) is an old idea (Genest and Zidek, 1986). Re-
cent work in learning weights for the component ?ex-
pert? models has turned to cooperative techniques (Hin-
ton, 1999). Decoding that finds y (given x) to maximize
some weighted average of log-probabilities is known as
a logarithmic opinion pool (LOP). LOPs were applied
to CRFs (for named entity recognition and tagging) by
Smith et al (2005), with an eye toward regularization.
Their experts (each a CRF) contained overlapping feature
sets, and the combined model achieved much the same
effect as training a single model with smoothing. Note
that our models, unlike theirs, partition the feature space;
there is only one CRF, but some parameters are ignored
when estimating other parameters. We have not estimated
log-domain mixing coefficients?we weight all models?
contributions equally. Sutton and McCallum (2005) have
applied factored estimation to CRFs, motivated (like us)
by speed; they also describe how factored estimation
9Lemma-trigram and fine POS-unigram/lemma-bigram fea-
tures were eliminated to limit model size.
480
full morph. lemma POS OOV POS
accuracy accuracy accuracy accuracy
?2 376K 768K 376K 768K 376K 768K 376K 768K
channel only 61.4 60.3 85.1 84.2 88.5 87.2 17.8 16.4
most likely ~y 80.0 80.8 98.1 98.1 97.9 97.8 52.0 52.0
Hajic? et al HMM 88.8 89.2 97.9 97.9 95.8 95.8 52.0 52.0
+ OOV model 90.5 90.8 97.9 97.9 96.7 96.6 93.0 92.9
full ? 88.1 88.5 98.3 98.5 98.3 98.3 60.2 61.8
oracle given pruning 98.6 99.3 99.5 99.6 99.1 99.7 60.2 90.3
10 88.4 88.5 98.4 98.4 98.3 98.2 61.8 59.4
oracle given pruning 99.3 99.3 99.5 99.6 99.8 99.7 93.4 90.6
1 88.6 88.6 98.4 98.4 98.2 98.1 60.0 56.7
oracle given pruning 99.3 99.3 99.5 99.6 99.8 99.8 95.0 94.0
? POS ? 87.9 88.0? 98.2 98.2? 98.0 97.9? 55.7 51.7?
10 88.1 88.3? 98.2 98.3? 98.0 97.9? 55.4 51.6?
1 88.4 88.5? 98.2 98.2? 98.0 97.9? 55.0 51.9?
? tag-lemma ? 87.8 88.3 98.3 98.6 98.3 98.3 60.2 59.7
10 88.0 88.1 98.4 98.5 98.3 98.2 59.1 59.1
1 88.0 88.1 98.4 98.4 98.2 98.1 59.0 58.1
POS only ? 65.6? 65.5? 98.3 98.6 98.3 98.4 60.2 63.7
10 65.7? 65.5? 98.5 98.6 98.5 98.5 65.2 66.4
1 65.7? 65.5? 98.6 98.7 98.6 98.6 67.2 67.2
POS & ? 81.2 82.3 98.3 98.6 98.3 98.4 60.2 63.9
tag-lemma? 10 81.9 82.3 98.5 98.6 98.4 98.5 65.8 67.2
1 82.0 82.3 98.4 98.5 98.5 98.4 67.8 66.3
oracle given d(x) 99.8 99.8 99.5 99.6 99.9 99.9 100.0 100.0
Table 2: Czech disambiguation:
test-set (109K words) accuracy. A
word is marked correct only if its
entire morphological tag (or mor-
pheme or POS tag) was correctly
identified. Note that the full tag
is a complex, 15-field morphologi-
cal label, while ?POS? is a projec-
tion down to a tagset of size 60.
Lemma accuracy does not include
OOV words. ?The POS-only model
selects only POS, not full tags; these
measures are expected performance
if the full tag is selected randomly
from those in the dictionary that
match the selected POS. ?Required
more aggressive pruning. Bold
scores were significantly better than
the HMM of Hajic? et al (binomial
sign test, p < 0.05). Our models
were slightly but significantly worse
on full tagging, but showed signif-
icant improvements on recovering
POS tags and lemmas.
maximizes a lower bound on the unfactored objective.
Smith and Smith (2004) applied factored estimation to a
bilingual weighted grammar, driven by data limitations.
4.2 Experiments
Our corpus is the PDT (Hajic?, 1998), with up to 60% used
for training and 10% (109K words) used for test.10 The
morphological dictionary is the one packaged with the
PDT; it covers about 98% of the tokens in the corpus. The
remaining 2% have (unsurprisingly) a diverse set of 300?
400 distinct tags, depending on the training set size.11
Results are shown in Tab. 2. We compare to the HMM
of (Hajic? et al, 2001) without its OOV component.12 We
report morphological tagging accuracy on words; we also
report lemma accuracy (on non-OOV words), POS accu-
10We used less than the full corpus to keep training time
down; note that the training sets are nonetheless substantially
larger than in the Korean and Arabic experiments.
11During training, these project down to manageable num-
bers of hypotheses in the factored models. At test-time, how-
ever, Viterbi search is quite difficult when OOV symbols occur
consecutively. To handle this, we prune OOV arcs from the lat-
tices using the factored POS and inflectional models. For each
OOV, every model prunes a projection of the analysis (e.g., the
POS model prunes POS tags) until 90% of the posterior mass or
3 arcs remain (whichever is more conservative). Viterbi decod-
ing is run on a lattice containing OOV arcs consistent with the
pruned projected lattices.
12Results with the OOV component are also reported in Tab. 2,
but we cannot guarantee their experimental validity, since the
OOV component is pre-trained and may have been trained on
data in our test set.
racy on all words, and POS accuracy on OOV words. The
channel model (not shown) tended to have a small, harm-
ful effect on performance.
Without any explicit OOV treatment, our POS-only
component model significantly reduces lemma and POS
errors compared to Hajic? et al?s model. On recovering
full morphological tags, our full model is close in perfor-
mance to Hajic? et al, but still significantly worse. It is
likely that for many tasks, these performance gains are
more helpful than the loss on full tagging is harmful.
Why doesn?t our full model perform as well as Hajic? et
al.?s model? An error analysis reveals that our full model
(768K, ?2 = 1), compared to the HMM (768K) had 91%
as many number errors but 0.1% more gender and 31%
more case errors. Taking out those three models (?POS
& tag-lemma? in Fig. 2) is helpful on all measures ex-
cept full tagging accuracy, due in part to substantially
increased errors on gender (87% increase), case (54%),
and number (35%). The net effect of these components,
then, is helpful, but not quite helpful enough to match
a well-smoothed HMM on complex tagging. We com-
pared the models on the training set and found the same
pattern, demonstrating that this is not merely a matter of
over-fitting.
5 Future Work
Two clear ways to improve our models present them-
selves. The first is better OOV handling, perhaps through
an improved channel model. Possibilities include learn-
ing weights to go inside the FST-encoded dictionaries and
481
directly modeling spelling changes. The second is to turn
our factored model into a LOP. Training the mixture co-
efficients should be straightforward (if time-consuming)
with a development dataset.
A drawback of our system (especially for Czech) is
that some components (most notably, the Czech POS
model) take a great deal of time to train (up to two weeks
on 2GHz Pentium systems). Speed improvements are
expected to come from eliminating some of the over-
lapping feature templates, generalized speedups for log-
linear training, and perhaps further factoring.
6 Conclusion
We have explored morphological disambiguation of di-
verse languages using log-linear sequence models. Our
approach reduces error rates significantly on POS tag-
ging (Arabic and Czech), morpheme sequence recovery
(Korean and Arabic), and lemmatization (all three lan-
guages), compared to baseline state-of-the-art methods
For complex analysis tasks (e.g., Czech tagging), we have
demonstrated that factoring a large model into smaller
components can simplify training and achieve excel-
lent results. We conclude that a conditionally-estimated
source model informed by an existing morphological dic-
tionary (serving as an unweighted channel) is an effective
approach to morphological disambiguation.
References
K. R. Beesley and L. Karttunen. 2003. Finite State Morphol-
ogy. CSLI.
T. Buckwalter. 2004. Arabic morphological analyzer version
2.0. LDC2004L02.
J. Cha, G. Lee, and J.-H. Lee. 1998. Generalized unknown
morpheme guessing for hybrid POS tagging of Korean. In
Proc. of VLC.
K. Darwish. 2002. Building a shallow Arabic morphological
analyser in one day. In Proc. of ACL Workshop on Computa-
tional Approaches to Semitic Languages.
E. Daya, D. Roth, and S. Wintner. 2004. Learning Hebrew
roots: Machine learning with linguistic constraints. In Proc.
of EMNLP.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic tag-
ging of Arabic text: From raw text to base phrase chunks. In
Proc. of HLT-NAACL.
A. Freeman. 2001. Brill?s POS tagger and a morphology parser
for Arabic. In Proc. of ACL Workshop on Arabic Language
Processing.
C. Genest and J. V. Zidek. 1986. Combining probability distri-
butions: A critique and an annotated bibliography. Statistical
Science, 1:114?48.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-
of-speech tagging and morphological disambiguation in one
fell swoop. In Proc. of ACL.
J. Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petkevic?. 2001.
Serial combination of rules and statistics: A case study in
Czech tagging. In Proc. of ACL.
J. Hajic?. 1998. Building a syntactically annotated corpus:
The Prague Dependency Treebank. In Issues of Valency and
Meaning.
D. Z. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2000. Statistical
morphological disambiguation for agglutinative languages.
In Proc. of COLING.
C.-H. Han, N.-R. Han, E.-S. Ko, H. Yi, and M. Palmer. 2002.
Penn Korean Treebank: Development and evaluation. In
Proc. Pacific Asian Conf. Language and Comp.
N.-R. Han. 2004. Klex: Finite-state lexical transducer for Ko-
rean. LDC2004L01.
G. Hinton. 1999. Products of experts. In Proc. of ICANN.
J. Hlava?cova?. 2001. Morphological guesser of Czech words.
In Proc. of TSD.
F. Jelinek. 1976. Continuous speech recognition by statistical
methods. Proc. of the IEEE, 64(4):532?557.
R. M. Kaplan and M. Kay. 1981. Phonological rules and finite-
state transducers. Presented at Linguistic Society of Amer-
ica.
S. Khoja. 2001. APT: Arabic part-of-speech tagger. In Proc.
of NAACL Student Workshop.
G. Kiraz. 2000. Multitiered nonlinear morphology using mul-
titape finite automata: A case study on Syriac and Arabic.
Computational Linguistics, 26(1):77?105.
K. Koskenniemi. 1983. Two-level morphology: A general
computational model of word-form recognition and produc-
tion. Technical Report 11, University of Helsinki.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Applying
conditional random fields to Japanese morphological analy-
sis. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
M. Levinger, U. Ornan, and A. Itai. 1995. Learning morpho-
lexical probabilities from an untagged corpus with an appli-
cation to Hebrew. Computational Linguistics, 21(3):383?
404.
D. C. Liu and J. Nocedal. 1989. On the limited memory method
for large scale optimization. Mathematical Programming B,
45(3):503?28.
M. Maamouri, A. Bies, H. Jin, and T. Buckwalter. 2003. Arabic
Treebank part 1 version 2.0. LDC2003T06.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding consensus
among words: Lattice-based word error minimization. In
Proc. of ECSCT.
E. Mra?kova? and R. Sedlacek. 2003. From Czech morphol-
ogy through partial parsing to disambiguation. In Proc. of
CLITP.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random fields.
In Proc. of COLING.
V. Petkevic?. 2001. Grammatical agreement and automatic
morphological disambiguation of inflectional languages. In
Proc. of TSD.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of HLT-NAACL.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing with
factored estimation: Using English to parse Korean. In Proc.
of EMNLP.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opin-
ion pools for conditional random fields. In Proc. of ACL.
C. Sutton and A. McCallum. 2005. Cliquewise training for
undirected models. In Proc. of UAI.
482
c? 2003 Association for Computational Linguistics
The Web as a Parallel Corpus
Philip Resnik? Noah A. Smith?
University of Maryland Johns Hopkins University
Parallel corpora have become an essential resource for work in multilingual natural language
processing. In this article, we report on our work using the STRAND system for mining parallel
text on the World Wide Web, first reviewing the original algorithm and results and then presenting
a set of significant enhancements. These enhancements include the use of supervised learning
based on structural features of documents to improve classification performance, a new content-
based measure of translational equivalence, and adaptation of the system to take advantage of the
Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these
techniques is demonstrated in the construction of a significant parallel corpus for a low-density
language pair.
1. Introduction
Parallel corpora?bodies of text in parallel translation, also known as bitexts?have
taken on an important role in machine translation and multilingual natural language
processing. They represent resources for automatic lexical acquisition (e.g., Gale and
Church 1991; Melamed 1997), they provide indispensable training data for statistical
translation models (e.g., Brown et al 1990; Melamed 2000; Och and Ney 2002), and
they can provide the connection between vocabularies in cross-language information
retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard
1997). More recently, researchers at Johns Hopkins University and the University of
Maryland have been exploring new ways to exploit parallel corpora in order to de-
velop monolingual resources and tools, using a process of annotation, projection, and
training: Given a parallel corpus in English and a less resource-rich language, we
project English annotations across the parallel corpus to the second language, using
word-level alignments as the bridge, and then use robust statistical techniques in learn-
ing from the resulting noisy annotations (Cabezas, Dorr, and Resnik 2001; Diab and
Resnik 2002; Hwa et al 2002; Lopez et al 2002; Yarowsky, Ngai, and Wicentowski 2001;
Yarowsky and Ngai 2001; Riloff, Schafer, and Yarowsky 2002).
For these reasons, parallel corpora can be thought of as a critical resource. Unfor-
tunately, they are not readily available in the necessary quantities. Until very recently,
for example, statistical work in machine translation focused heavily on French-English
translation because the Canadian parliamentary proceedings (Hansards) in English
and French were the only large bitext available. Things have improved somewhat, but
it is still fair to say that for all but a relatively few language pairs, parallel corpora
tend to be accessible only in specialized forms such as United Nations proceedings
(e.g., via the Linguistic Data Consortium, ?http://www.ldc.upenn.edu?), religious texts
(Resnik, Olsen, and Diab 1999), localized versions of software manuals (Resnik and
? Department of Linguistics and Institute for Advanced Computer Studies, University of Maryland,
College Park, MD 20742. E-mail: resnik@umd.edu
? Department of Computer Science and Center for Language and Speech Processing, Johns Hopkins
University, Baltimore, MD 21218. E-mail: nasmith@cs.jhu.edu
350
Computational Linguistics Volume 29, Number 3
Melamed 1997; Menezes and Richardson 2001), and the like. Even for the top handful
of majority languages, the available parallel corpora tend to be unbalanced, represent-
ing primarily governmental or newswire-style texts. In addition, like other language
resources, parallel corpora are often encumbered by fees or licensing restrictions. For
all these reasons, it is difficult to follow the ?more data are better data? advice of
Church and Mercer (1993), abandoning balance in favor of volume, with respect to
parallel text.
Then there is the World Wide Web. People tend to see the Web as a reflection of
their own way of viewing the world?as a huge semantic network, or an enormous
historical archive, or a grand social experiment. We are no different: As computational
linguists working on multilingual issues, we view the Web as a great big body of text
waiting to be mined, a huge fabric of linguistic data often interwoven with parallel
threads.
This article describes our techniques for mining the Web in order to extract the
parallel text it contains. It presents, in revised and considerably extended form, our
early work on mining the Web for bilingual text (STRAND) (Resnik 1998, 1999), incor-
porating new work on content-based detection of translations (Smith 2001, 2002), and
efficient exploitation of the Internet Archive. In Section 2 we lay out the STRAND ar-
chitecture, which is based on the insight that translated Web pages tend quite strongly
to exhibit parallel structure, permitting them to be identified even without looking at
content; we also show how we have improved STRAND?s performance by training
a supervised classifier using structural parameters rather than relying on manually
tuned thresholds. In Section 3 we present an approach to detecting translations that
relies entirely on content rather than structure, demonstrating performance comparable
to STRAND?s using this orthogonal source of information. In Section 4 we describe
how we have adapted the STRAND approach to the Internet Archive, dramatically
improving our ability to identify parallel Web pages on a large scale. Section 5 puts
all the pieces together, using structural and combined content-structure matching of
pages on the Internet Archive in order to obtain a sizable corpus of English-Arabic
Web document pairs. Finally we present our thoughts on future work and conclusions.
2. The STRAND Web-Mining Architecture
STRAND (Resnik 1998, 1999) is an architecture for structural translation recognition,
acquiring natural data. Its goal is to identify pairs of Web pages that are mutual
translations. In order to do this, it exploits an observation about the way that Web
page authors disseminate information in multiple languages: When presenting the
same content in two different languages, authors exhibit a very strong tendency to
use the same document structure (e.g., Figure 1). STRAND therefore locates pages that
might be translations of each other, via a number of different strategies, and filters out
page pairs whose page structures diverge by too much.
In this section we describe how STRAND works, and we also discuss several
related Web-mining methods, focusing on the overall architecture these systems have
in common and the important system-specific variations. We then show how tuning
STRAND?s structural parameters using supervised training can significantly increase
its performance.
2.1 STRAND
Finding parallel text on the Web consists of three main steps:
? Location of pages that might have parallel translations
351
Resnik and Smith The Web as a Parallel Corpus
Figure 1
Example of a candidate pair.
? Generation of candidate pairs that might be translations
? Structural filtering out of nontranslation candidate pairs
We consider each of these steps in turn.
2.1.1 Locating Pages. The original STRAND architecture accomplished the first step by
using the AltaVista search engine?s ?http://www.av.com? advanced search to search
for two types of Web pages: parents and siblings.
A parent page is one that contains hypertext links to different-language versions of
a document; for example, if we were looking for English and French bitexts, the page
at the left in Figure 2 would lead us to one such candidate pair. To perform this search
for the English-French language pair, we ask AltaVista for pages in any language
that satisfy this Boolean expression: (anchor:"english" OR anchor:"anglais") AND
(anchor:"french" OR anchor:"franc?ais"). A 10-line distance filter is used to re-
strict attention to pages on which the English and French pointers occur reason-
ably close to one another?specifically, those for which the regular expression (in
Perl) /(english|anglais)/ is satisfied within 10 lines of the Perl regular expres-
sion /(french|fran\w+ais)/ in the HTML source. This helps filter out a page that
contained, for example, a link to ?English literature courses? and also contained an
unrelated link to ?French version? at the top.
A sibling page is a page in one language that itself contains a link to a version
of the same page in another language; for example, the page at the right of Figure 2
contains a link on the left that says ?This page in english.? To perform this search for
English pages matching a given French page, we request pages in French that match
the Boolean expression anchor:"english" OR anchor:"anglais".
More recent versions of STRAND (unpublished) have added a ?spider? compo-
nent for locating pages that might have translations. Given a list of Web sites thought
to contain bilingual text for a given language pair (e.g., sites identified using the
AltaVista-based search), it is possible to download all the pages on each site, any
352
Computational Linguistics Volume 29, Number 3
Figure 2
Excerpts from a parent page (left) and a sibling page (right). The parent page is in Italian and
contains links marked ?Italiano/Italian,? ?Francese/French,? and ?Inglese/English.? The
sibling page is in Dutch and contains a link marked ?This page in english? in the leftmost
column.
of which might have a translation on that site. Although simple to implement, this
method of locating pages shifts the burden of narrowing down the possibilities to
the process of generating candidate document pairs. The results reported here do not
make use of the spider.
2.1.2 Generating Candidate Pairs. Pairing up potentially translated pages is simple
when a search engine has been used to generate parent or sibling pages: One simply
pairs the two child pages to which the parent links, or the sibling page together with
the page to which it links.
When all the pages on a site are under consideration, the process is rather differ-
ent. The simplest possibility is to separate the pages on a site into the two languages of
interest using automatic language identification (Ingle 1976; Beesley 1988; Cavnar and
Trenkle 1994; Dunning 1994), throwing away any pages that are not in either language,
and then generate the cross product. This potentially leads to a very large number of
candidate page pairs, and there is no particular reason to believe that most of them
are parallel translations, other than the fact that they appear on the same Web site. The
spider component of STRAND adds a URL-matching stage, exploiting the fact that the
directory structure on many Web sites reflects parallel organization when pages are
translations of each other. Matching is performed by manually creating a list of substi-
tution rules (e.g., english ? big5),1 and for each English URL, applying all possible
rules to generate URLs that might appear on the list of pages for the other language. If
such a URL is found, the pair with similar URLs is added to the list of candidate doc-
ument pairs. For example, suppose an English-Chinese site contains a page with URL
?http://mysite.com/english/home en.html?, on which one combination of substitu-
tions might produce the URL ?http://mysite.com/big5/home ch.html?. The original
page and the produced URL are probably worth considering as a likely candidate pair.
1 Big5 is the name of a commonly used character encoding for Chinese.
353
Resnik and Smith The Web as a Parallel Corpus
Owing to the combinatorics (an exponential number of possible substitutions), only a
fixed number of substitution combinations can be tried per English URL; however, in
Section 4.3 we describe a more scalable URL-matching algorithm.
Another possible criterion for matching is the use of document lengths. Texts
that are translations of one another tend to be similar in length, and it is reasonable to
assume that for text E in language 1 and text F in language 2, length(E) ? C ? length(F),
where C is a constant tuned for the language pair. The use of a document length filter
is described in Smith (2001), in which such a filter is shown, at the sentence level,
to reduce the size of the search space exponentially in the confidence p in a (1 ? p)
confidence interval for a linear regression model with only linear loss of good pairs.
2.1.3 Structural Filtering. The heart of STRAND is a structural filtering process that
relies on analysis of the pages? underlying HTML to determine a set of pair-specific
structural values, and then uses those values to decide whether the pages are transla-
tions of one another. The first step in this process is to linearize the HTML structure
and ignore the actual linguistic content of the documents. We do not attempt to exploit
nonlinear structure (e.g., embedded chunks), for two reasons. First, we suspect that
many HTML authors use tags for formatting text rather than for indicating document
structure; therefore any ?tree? structure is likely to be inconsistent or poorly matched.
Second, we required the matching algorithm to be fast, and algorithms for aligning
tree structures are more demanding than those for linear structures.
Both documents in the candidate pair are run through a markup analyzer that
acts as a transducer, producing a linear sequence containing three kinds of token:
[START:element_label] e.g., [START:A], [START:LI]
[END:element_label] e.g., [END:A]
[Chunk:length] e.g., [Chunk:174]
The chunk length is measured in nonwhitespace bytes, and the HTML tags are nor-
malized for case. Attribute-value pairs within the tags are treated as nonmarkup text
(e.g., <FONT COLOR="BLUE"> produces [START:FONT] followed by [Chunk:12]).
The second step is to align the linearized sequences using a standard dynamic pro-
gramming technique (Hunt and McIlroy 1975). For example, consider two documents
that begin as follows:
<HTML> <HTML>
<TITLE>Emergency Exit</TITLE> <TITLE>Sortie de Secours</TITLE>
<BODY> <BODY>
<H1>Emergency Exit</H1> Si vous e?tes assis a`
If seated at an exit and co?te? d?une . . .
...
...
The aligned linearized sequence would be as follows:
[START:HTML] [START:HTML]
[START:TITLE] [START:TITLE]
[Chunk:13] [Chunk:15]
[END:TITLE] [END:TITLE]
[START:BODY] [START:BODY]
[START:H1]
[Chunk:13]
[END:H1]
[Chunk:112] [Chunk:122]
354
Computational Linguistics Volume 29, Number 3
Using this alignment, we compute four scalar values that characterize the quality
of the alignment:
dp The difference percentage, indicating nonshared material (i.e., alignment
tokens that are in one linearized file but not the other).
n The number of aligned nonmarkup text chunks of unequal length.
r The correlation of lengths of the aligned nonmarkup chunks.
p The significance level of the correlation r.
The difference percentage (dp) quantifies the extent to which there are mismatches
in the alignment: sequence tokens on one side that have no corresponding token on
the other side. In the example above, one document contains an H1 header that is
missing from the second document. Large numbers of such mismatches can indicate
that the two documents do not present the same material to a great enough extent to
be considered translations. This can happen, for example, when two documents are
translations up to a point (e.g., an introduction), but one document goes on to include a
great deal more content than another. Even more frequently, the difference percentage
is high when two documents are prima facie bad candidates for a translation pair.
The number of aligned nonmarkup text chunks (n) helps characterize the quality of
the alignment. The dynamic programming algorithm tries to optimize the correspon-
dence of identical tokens, which represent markup.2 As a side effect, the nonmarkup
text chunks are placed in correspondence with one another (e.g., the ?Emergency Exit?
and ?Sortie de Secours? chunks in the above example). The more such pairings are
found, the more likely the candidate documents are to represent a valid translation
pair.
The remaining two parameters (r and p) quantify the extent to which the cor-
responding nonmarkup chunks are correlated in length. When two documents are
aligned with one another and are valid translations, there is a reliably linear relation-
ship in the length of translated chunks of text: short pieces correspond with short
pieces, medium with medium, and long with long. The Pearson correlation coefficient
r for the lengths will be closer to one when the alignment has succeeded in lining
up translated pieces of text, and the p value quantifies the reliability of the correla-
tion; for example, the standard threshold of p < .05 indicates 95% confidence that the
correlation was not obtained by chance.
In our original work, we used fixed thresholds, determined manually by inspection
of development (nontest) data for English-Spanish, to decide whether a candidate pair
should be kept or filtered out. Thresholds of dp < 20% and p < 0.05 were used.
2.2 STRAND Results
As with most search tasks, performance at finding parallel Web pages can be evaluated
using standard measures of precision and recall and by combining those figures using
the F-measure. It is not possible for us to measure recall relative to the entire set of
document pairs that should have been found; this would require exhaustive evaluation
using the entire Web, or pooling results from a large number of different systems, as
is done in the TREC information retrieval evaluations. Therefore, recall in this setting
is measured relative to the set of candidate pairs that was generated.
2 ?Nonmarkup? tokens with exactly the same length almost always turn out to be pieces of
markup-related text (e.g., key=value pairs within HTML tags).
355
Resnik and Smith The Web as a Parallel Corpus
Since the ?truth? in this task is a matter for human judgment, we rely on bilingual
speakers to judge independently whether page pairs are actually translations of each
other for any given test set. In our experience no bilingual speaker is completely
comfortable saying that another person?s translation is a good translation, so in creating
the gold standard, we instead ask, ?Was this pair of pages intended to provide the
same content in the two different languages?? Asking the question in this way leads
to high rates of interjudge agreement, as measured using Cohen?s ? measure.
2.2.1 Using Manually Set Parameters. Using the manually set thresholds for dp and n,
we have obtained 100% precision and 68.6% recall in an experiment using STRAND
to find English-French Web pages (Resnik 1999). In that experiment, 326 candidate
pairs, randomly selected from a larger set of 16,763 candidates, were judged by two
human annotators. The humans agreed (i.e., both marked a page ?good? or both
marked a page ?bad?) on 261 page pairs (86 ?good? and 175 ?bad?), and it is rela-
tive to those 261 that we compute recall. A modified version of STRAND was used
to obtain English-Chinese pairs (see related work, below), and in a similar formal
evaluation, we found that the resulting set had 98% precision and 61% recall for
Chinese ?http://umiacs.umd.edu/?resnik/strand/?. Both these results are consistent
with our preliminary findings for English-Spanish using a less rigorous evaluation
(using the judgments of the first author rather than independent bilingual evaluators)
and a very small test set; precision in this preliminary experiment was near ceiling
and recall was in the vicinity of 60% (Resnik 1998).
2.2.2 Assessing the STRAND Data. Although our focus here is finding parallel text,
not using it, a natural question is whether parallel text from the Web is in fact of value.
Two sources of evidence suggest that it is.
First, Web-based parallel corpora have already demonstrated their utility in cross-
language information retrieval experiments. Resnik, Oard, and Levow (2001) showed
that a translation lexicon automatically extracted from the French-English STRAND
data could be combined productively with a bilingual French-English dictionary in or-
der to improve retrieval results using a standard cross-language IR test collection (En-
glish queries against the CLEF-2000 French collection, which contains approximately
21 million words from articles in Le Monde). During document translation, backing off
from the dictionary to the STRAND translation lexicon accounted for over 8% of the
lexicon matches (by token), reducing the number of untranslatable terms by a third
and producing a statistically significant 12% relative improvement in mean average
precision as compared to using the dictionary alone. Similarly, Nie and Cai (2001) have
demonstrated improved cross-language IR results for English and Chinese using data
gathered by the PTMiner system (Chen and Nie 2000), a related approach that we
discuss in Section 2.3.
Second, since bag-of-words IR experiments are not very illuminating with respect
to fluency and translation quality, we conducted a ratings-based assessment of English-
Chinese data, asking two native Chinese speakers (who are fluent in English) to assign
ratings to a set of English-Chinese items. The set contained:
? 30 human-translated sentence pairs from the FBIS (Release 1)
English-Chinese parallel corpus, sampled at random.
? 30 Chinese sentences from the FBIS corpus, sampled at random, paired
with their English machine translation output from AltaVista?s Babelfish
?http://babelfish.altavista.com?.
356
Computational Linguistics Volume 29, Number 3
? 30 paired items from Chinese-English Web data, sampled at random
from ?sentence-like? aligned chunks as identified using the HTML-based
chunk alignment process of Section 2.1.3.
The human-translated and machine-translated pairs were included in order to provide
upper-bound and lower-bound comparisons. The items were presented to one judge
in a random order, and to the other judge in the reverse order.
Chinese-English Web data were those collected by Jinxi Xu using a modified ver-
sion of STRAND (see Section 2.3), excluding those that did not pass STRAND?s struc-
tural filter with the manually set thresholds. Sentence-like chunk pairs were defined
as those in which the English side was 5?50 whitespace-delimited tokens long and
that began with an uppercase alphabetic character and contained at least one token
from an English stop list.3 This fairly strict filter provided a set of approximately 7,000
pairs from which the 30 test items were sampled.
Participants were asked to provide each pair of items with three ratings, assess-
ing English fluency, Chinese fluency, and adequacy of the translation. The choice and
wording of the ratings criteria were derived from the human evaluation measures pro-
posed by Dabbadie et al (2002), with the wording of the translation assessment crite-
rion modified to eliminate references to the direction of translation. (See Appendix B.)
For all three measures, the two judges? ratings were significantly correlated (p <
0.0001). Figure 3 shows additional quantitative results of the assessment, comparing
judgments among human-translated, Web-generated, and machine-translated data.
The ratings indicate that pairs from the Web contain on average somewhere be-
tween ?mostly the same meaning? and ?entirely the same meaning? (median 3.25).4
In comparison, current commercial-quality machine translation output achieves per-
formance only between ?much of the same meaning? and ?mostly the same meaning?
(median 2.5). Moreover, it is very likely that the Web translation quality is an under-
estimate: Some of the low-scoring outliers within the Web data could be eliminated
by using state-of-the-art sentence alignment techniques and automatic detection and
0
5
10
15
20
25
30
0 1 2 3 4
N
u
m
be
r 
of
 p
ai
rs
Rating
Human
WWW
MT
0
5
10
15
20
25
30
0 1 2 3 4
N
u
m
be
r 
of
 p
ai
rs
Rating
Human
WWW
MT
Figure 3
Translation adequacy ratings: Distribution over scores for human-translated (Human),
Web-generated (WWW), and machine-translated (MT) data. The left plot provides results for
judge 1, the right plot for judge 2.
3 We also excluded pairs either side of which contained a curly bracket, since these were almost
invariably fragments of Javascript code.
4 The medians noted refer to the median of R1+R22 within the set, where Rn is the rating given by judge
n. We use the median rather than the mean because it is less sensitive to outliers.
357
Resnik and Smith The Web as a Parallel Corpus
elimination of noisy pairs at the sentence level (cf. Nie and Cai [2001]). We observe
that the distribution of scores for Web data peaks at the highest rating and that the
data are in both cases modestly bimodally distributed. Machine-translated pairs, on
the other hand, have generally lower quality. This suggests that high-quality parallel
translations are present in this corpus and that poor-quality parallel translations are
very poor (whether because of misalignment or simply because of poor translation
quality at the document level) and might therefore be easily distinguishable from the
better material. We plan to address this in future work.
Qualitatively, the results are a source of optimism about parallel data from the
Web. Looking monolingually, fluency of the English side is statistically comparable to
that of English sentences from the FBIS parallel corpus (essentially at ceiling), and on
average the Chinese Web data are judged somewhere between ?fairly fluent? and ?very
fluent,? with the median at ?very fluent? (only the second judge found the fluency
of the Chinese Web sentences to be significantly worse than the human-generated
Chinese sentences, Mann-Whitney test, p < 0.02).
2.2.3 Optimizing Parameters Using Machine Learning. Based on experiments with
several language pairs, it appears that STRAND?s structure-based filter consistently
throws out around one-third of the candidate document pairs it has found in order
to maintain its precision in the 98?100% range. It does so by respecting parameter
thresholds that were determined manually using English-Spanish development data;
the same parameters seem to have worked reasonably well not only for English-
Spanish, but also for English-French and English-Chinese pairs. It is possible, however,
that classification can be tuned for better performance. In order to investigate this
possibility, we took a machine-learning approach: We used the four structural values
(dp, n, r, and p) as features characterizing each document pair and treated the problem
as a binary decision task, using supervised learning to make an attempt at better
predicting human judgments.
Using the English-French data, we constructed a ninefold cross-validation experi-
ment using decision tree induction to predict the class assigned by the human judges.
The decision tree software was the widely used C5.0 ?http://www.rulequest.com/
demoeula.html?. We used a decision tree learner because it is transparent (it is easy to
see which features are being used to classify page pairs). In addition, a decision tree
produced by C5.0 can be translated into a fast C program that is a rapid classifier of
document pairs.
Each fold had 87 test items and 174 training items; the fraction of good and bad
pairs in each fold?s test and training sets was roughly equal to the overall division
(33% to 67%, respectively). Precision and recall results are reported in Table 1, together
with baseline results from STRAND?s untuned classifier as reported above.
Looking at the decision trees learned, we see that they are very similar to one
another. In every case a tree that looked like the following was learned:
if dp > 37 then BAD
else
if n > 11 then GOOD
else ...
where the remaining branch involved various additional partitionings of the candi-
date pairs that had few aligned text chunks (small n) and relatively low (but perhaps
unreliable) difference percentage (dp). That branch handled around 10% of the docu-
ment set and was prone to overtraining. (The documents handled by this branch were
mostly marked ?bad? by the judges but appear to have been difficult to classify based
358
Computational Linguistics Volume 29, Number 3
Table 1
Effects of parameter tuning.
Precision Recall
Untuned 1.000 0.686
Fold 1 0.875 1.000
Fold 2 0.857 0.667
Fold 3 1.000 1.000
Fold 4 1.000 0.923
Fold 5 1.000 0.875
Fold 6 1.000 1.000
Fold 7 0.889 0.667
Fold 8 1.000 0.889
Fold 9 1.000 0.545
Average 0.958 0.841
on the structural features.) Note that the learned classifiers were substantially different
from the heuristic threshold used earlier.
Without tuning, the manually set parameters result in good document pairs? be-
ing discarded 31% of the time. Our cross-validation results indicate that tuning the
parameters cuts that figure in half: Only 16% of the good pairs will be discarded, at a
cost of admitting 4 false positives from every 100 candidate pairs.
This approach is quite general and uses only a minimum of language-dependent
knowledge. The features we are using are the same for any language pair. The tuning
process needs to be done only once per language pair and requires only a few hours
of annotation from untrained speakers of both languages to obtain the small labeled
sample.
2.3 Related Work
Several other systems for discovering parallel text, developed independently, can be
described as operating within the same three-stage framework as STRAND.
Parallel Text Miner (PTMiner) (Chen and Nie 2000) exploits already-existing Web
search engines to locate pages by querying for pages in a given language that contain
links to pages that are likely to be in the other language of interest. Once bilingual sites
are located, they are crawled exhaustively. In order to generate candidate pairs, PT-
Miner uses a URL-matching process similar to the one described above; for example,
the French translation of a URL like ?http://www.foo.ca/english-index.html? might
be ?http://www.foo.ca/french-index.html?. PTMiner?s matching process uses a map-
ping of language-specific prefixes and suffixes and does not handle cases in which URL
matching requires multiple substitutions. PTMiner also applies a length filter and auto-
matic language identification to verify that the pages are in the appropriate languages.
Chen and Nie report a 95% precise English-French corpus of 118MB/135MB of text and
a 90% precise English-Chinese corpus of 137MB/117MB of text, based on inspection.
Later versions of PTMiner include a final filtering stage to clean the extracted
corpus; Nie and Cai (2001) independently used features similar to those described
here to eliminate noisy pairs. Specifically, they used a file length ratio filter, the pro-
portion of sentences that are aligned to empty (after a sentence alignment algorithm
is applied), and a criterion that rewards sentence pairs that contain elements from a
bilingual dictionary. Nie and Cai showed that hand-tuned combination of these crite-
359
Resnik and Smith The Web as a Parallel Corpus
ria improved the quality of their parallel English-Chinese corpus by 8% (F-measure)
at the text level.
Bilingual Internet Text Search (BITS) (Ma and Liberman 1999) starts with a given
list of domains to search for parallel text. It operates by sampling pages from each
domain and identifying their languages; if a domain is deemed to be multilingual,
all pages on the site are crawled exhaustively. BITS appears to consider all possible
combinations of Web page pairs in the two languages (i.e., the full cross product within
each site) and filters out bad pairs by using a large bilingual dictionary to compute a
content-based similarity score and comparing that score to a threshold. For each page
pair, the similarity score is
similarity(A, B) =
number of translation token pairs
number of tokens in A
(1)
Translation token pairs are considered within a fixed window (i.e., a distance-based
measure of co-occurrence is used).5 In addition to cross-lingual lexical matching, BITS
filters out candidate pairs that do not match well in terms of file size, anchors (num-
bers, acronyms, and some named entities), or paragraph counts. Using an English-
German bilingual lexicon of 117,793 entries, Ma and Liberman report 99.1% precision
and 97.1% recall on a hand-picked set of 600 documents (half in each language) con-
taining 240 translation pairs (as judged by humans). This technique yielded a 63MB
parallel corpus of English-German.
Other work on Web mining has been done by Jinxi Xu of BBN (personal com-
munication), who began with our STRAND implementation and added a module for
automatically learning string substitution patterns for URLs and also implemented
a different dynamic programming algorithm for assessing structural alignment. Xu
used the modified STRAND to obtain 3,376 Chinese-English document pairs, which
we evaluated formally (see above), determining that the set has 98% precision and
61% recall.
In addition, STRAND has been reimplemented by David Martinez and colleagues
at Informatika Fakultatea in the Basque Country (personal communication), in order
to perform exploratory experiments for discovering English-Basque document pairs.
It is worth noting that STRAND, PTMiner, and BITS are all largely independent of
linguistic knowledge about the particular languages, and therefore very easily ported
to new language pairs. With the exception of the use of a bilingual dictionary (in BITS
and later versions of PTMiner), these systems require, at most, a set of URL substring
patterns for the URL pattern-matching stage (e.g., big5 ? english in the example above;
see further discussion in Section 4.3), and a modest amount of monolingual data for
training n-gram-based language identifiers (typically 50,000 to 100,000 characters of
text per language).
Word-level translations are worth exploiting when they are available. In Section 3
we describe a bitext-matching process using a content-based similarity score grounded
in information theory, and in Section 5 we show how structural and content-based
criteria can be combined in order to obtain performance superior to that obtained
using either method alone.
5 Many details of this technique are left unspecified in Ma and Liberman (1999), such as the threshold
for the similarity score, the distance threshold, and matching of non-one-word to one-word entries in
the dictionary.
360
Computational Linguistics Volume 29, Number 3
Maria does n?t like fruit
Maria n? aime pas de fruits
X:
Y:
NULLNULL
NULL
Figure 4
An example of two texts with links shown. There are seven link tokens, five of which are
lexical (non-NULL) in X (the English side), six in Y (French).
3. Content-Based Matching
The approach discussed thus far relies heavily on document structure. However, as Ma
and Liberman (1999) point out, not all translators create translated pages that look like
the original page. Moreover, structure-based matching is applicable only in corpora
that include markup, and there are certainly multilingual collections on the Web and
elsewhere that contain parallel text without structural tags. Finally, other applications
for translation detection exist, such as subdocument text alignment and cross-lingual
duplicate detection (i.e., location of already-existing translations in a multilingual cor-
pus). All these considerations motivate an approach to matching translations that pays
attention to similarity of content, whether or not similarities of structure exist.
We present here a generic score of translational similarity that is based upon
any word-to-word translation lexicon (hand-crafted or automatically generated, or a
combination, and possibly highly noisy). The technique is shown to perform competi-
tively to the structure-based approach of STRAND on the task of identifying English-
French document translations.
3.1 Quantifying Translational Similarity
We define a cross-language similarity score, tsim , for two texts by starting with a
generative, symmetric word-to-word model of parallel texts (Melamed?s [2000]
Method A). Let a link be a pair (x, y) in which x is a word in language L1 and y
is a word in L2. The model consists of a bilingual dictionary that gives a probability
distribution p over all possible link types. Within a particular link, one of the words
may be NULL, but not both. In the generative process, a sequence of independent link
tokens is sampled from the distribution. The model does not account for word order.
An example of two texts with links is illustrated in Figure 4.
Next, we desire to compute the probability of the most probable link sequence
that could have accounted for the two texts.6 The probability of a link sequence is
simply the product of the probabilities p of the links it contains. As noted by Melamed
(2000), the problem of finding the best set of links is the maximum-weighted bipartite
matching (MWBM) problem: Given a weighted bipartite graph G = (V1 ? V2, E) with
edge weights ci,j(i ? V1, j ? V2), find a matching M ? E such that each vertex has at
most one edge in M and
?
e?M ci,j is maximized. The fastest known MWBM algorithm
runs in O(ve+v2 log v) time (Ahuja, Magnati, and Orlin 1993). Applied to this problem,
that is O(max(|X|, |Y|)3), where X and Y are the text lengths in words.
To use MWBM to find the most probable link sequence, let the L1 words be V1 and
the L2 words be V2. If two words x, y have p(x, y) > 0, an edge exists between them
with weight log p(x, y). If a word x (or y) may link to NULL with nonzero probability,
then that potential link is added as an additional edge in the graph between x (or y)
6 Of course, all permutations of a given link sequence will have the same probability (since the links are
sampled independently from the same distribution), so the order of the sequence is not important.
361
Resnik and Smith The Web as a Parallel Corpus
and a NULL vertex added to V2 (or V1). Each such x (or y) gets its own NULL vertex,
so that multiple words may ultimately link to NULL. A sum of weights of links in a
matching will be the log-probability of the (unordered) link sequence, and maximizing
that sum maximizes the probability.
The similarity score should be high when many of the link tokens in the best
sequence do not involve NULL tokens. Further, it should normalize for text length.
Specifically, the score is
tsim =
log Pr (two-word links in best matching)
log Pr (all links in best matching)
(2)
This score is an application of Lin?s (1998) information-theoretic definition of similarity.
Starting with a set of axioms relating intuitions about similarity to the mathematical
notion of mutual information (Shannon 1948), Lin derives the measure
sim(X, Y) =
log Pr (common(X, Y))
log Pr (description(X, Y))
(3)
where X and Y are any objects generated by a probabilistic model.
This technique of using a translation model to define translational similarity is
generic to different sources of lexical translation information. An important feature
is that it can be used with any symmetric translation model in which events can be
divided into those that both sides of a bitext have in common and those that affect
only one side.
The measure is simplified by assuming that all links in a given translation lexicon
are equiprobable. The assumption reduces the formula for tsim to
tsim =
number of two-word links in best matching
number of links in best matching
(4)
The key reason to compute tsim under the equiprobability assumption is that we
need not compute the MWBM, but may find just the maximum cardinality bipartite
matching (MCBM), since all potential links have the same weight. An O(e
?
v) (or
O(|X| ? |Y| ?
?
|X|+ |Y|) for this purpose) algorithm exists for MCBM (Ahuja, Magnati,
and Orlin 1993). For example, if the matching shown in Figure 4 is the MCBM (for
some translation lexicon), then tsim(X, Y) = 47 under the simplifying assumption.
In earlier work (Smith 2002), we sought to show how multiple linguistic resources
could be exploited in combination to recognize translation, and how the equiproba-
bility assumption allowed straightforward combination of resources (i.e., set union of
translation lexicon entries). In Section 3.2.1 we provide a clean solution to the problem
of using unweighted translation lexicons along with probabilistic ones that improves
performance over the earlier result.
This would appear to make the equiprobability assumption unnecessary (apart
from concerns about computational expense). However, we found that, if p(x, y) is set
to the empirically estimated joint probability of the lexical link type (x, y), then per-
formance turns out to be dismal. This is understandable: Using parameter estimation
techniques like the one we used, a great deal of probability mass in the distribution
p tends to go to frequent words, which are relatively uninformative with regard to
whether texts are mutual translations. The equiprobability assumption helps to coun-
teract this; in fact one could apply scoring techniques from information retrieval and
cross-lingual information retrieval in weighting the lexicon. We leave this area of ex-
ploration to future work.
362
Computational Linguistics Volume 29, Number 3
Melamed (2000) used a greedy approximation to MWBM called competitive link-
ing. Competitive linking iteratively selects the edge with the highest weight, links
the two vertices of the edge, then removes them from the graph. (Ties are broken at
random.) A heap-based implementation of competitive linking runs in O(max(|X|, |Y|)
log max(|X|, |Y|)). Under the equiprobability assumption, all the weights are the same,
so that competitive linking proceeds simply by randomly making legal links (those
allowed by the translation lexicon) until no more can be made.
If definition (4) is applied to pairs of documents in the same language, with a
??translation lexicon? defined by the identity relation, then tsim is a variant of resem-
blance (r), as defined by Broder et al (1997) for the problem of monolingual duplicate
detection, except that tsim has the advantage of being token-based rather than type-
based, incorporating word frequency.
We have demonstrated that the tsim score can be used to extract translationally
equivalent English-Chinese sentence pairs from even a noisy space with high precision
(Smith 2002). It was also shown that combining multiple sources of word-level trans-
lation information (dictionaries, word-to-word translation models, cognates) had pos-
itive effects on performance on the sentence-matching task. These information sources
were presumed to be extremely noisy (they are so presumed here, as well), though
no independent evaluation was carried out on them. If the ad hoc translation lexicon
induction techniques used here give good performance, then better techniques might
lead to further improvement. In addition, the competitive linking approximation was
shown to perform nearly as well as MCBM.
3.2 Experiment
We now apply our content-based similarity measure to the candidate pair classification
task presented by STRAND. Recall that both the original STRAND classifier and those
learned using decision tree methods, described in Section 2.2.3, employ only structural
features of the documents to determine whether they are translations. Here we apply
the tsim score to the same task and compare the results with those of the original
STRAND classifier.
3.2.1 Translation Lexicon. The word-level translation lexicon is derived from several
sources. We begin with an English-French dictionary (a total of 34,808 entries, 4,021 of
which are not one-to-one).7 Next, a word-to-word translation model (Melamed 2000)
was trained on the dictionary. Note that the parameter estimation task here is very
simple; in most cases the pairs are one-word to one-word, making the hidden link
structure unambiguous (modulo NULLs). The training primarily served the purpose
of breaking down multiword entries, informed by the rest of the entries, so as to obtain
a fully one-word-to-one-word dictionary. The training procedure was an expectation-
maximization (EM) procedure like that used by Melamed (2000), except that maximum
weighted bipartite matching was used instead of competitive linking. Any entry con-
taining a NULL was then removed.
We add to this dictionary a list of English-French cognate pairs, identified using
the method of Tiedemann (1999). Tiedemann?s approach involved learning language-
specific character weights for the computation of weighted edit distance to measure
cognateness. He used a list of known cognates to train the weights. We instead used
7 This dictionary was generated by Gina Levow, who kindly made it available to us. It was derived from
data available at ?http://www.freedict.com? and contains morphological variants but not character
accents.
363
Resnik and Smith The Web as a Parallel Corpus
Table 2
Fifteen randomly chosen cognate pairs. The noise is apparent.
English French English French
closest choses TOXLINE LIABLE
extensions extension RELATIONS ENNEMIS
answer passer TARNOS TARNOS
APPLICATION PROTECTION Generation information
missions maisons Commerce Community
proportion prestation private pre?tant
Fischer Fischer traditions attributions
Anglais dAlcatel
the weighted translation pairs in a translation model lexicon built from the Bible.8
The result is 35,513 word pairs from the corpus of Web pages under consideration.
An additional set of 11,264 exact string matches were added (also from the corpus of
Web pages). Qualitatively, these entries were highly noisy; a random selection of the
cognate pairs is shown in Table 2. All of these word pairs were added to the dictionary,
each with a count of one.
We took the enhanced dictionary with counts to define a Dirichlet prior, which is
the conjugate prior to a multinomial distribution over discrete events (like the distri-
bution p over link types we seek to estimate) (MacKay and Peto 1995). Such a prior is
characterized by counts of all such events; when it is used in an EM procedure, these
prior counts are added to those produced by the E step on every iteration. Intuitively,
if a word pair (x, y) is expected to be a likely lexical word pair in the dictionary and
cognate set, then models that make (x, y) probable are more likely (according to the
prior). Therefore the expected count of (x, y) is increased at each iteration of training
to the extent that the prior favors it.
Using the enhanced, weighted lexicon as a Dirichlet prior (containing 77,699 entries
and a total count of 85,332), a word-to-word translation model (Melamed 2000) was
trained on a verse-aligned Bible (15,548 verses, averaging 25.5 English words, 23.4
French words after tokenization). As before, we used the maximum weighted bipar-
tite matching algorithm. The final lexicon consists of all word pairs with nonzero
probability and contains 132,155 entries. Note that all word pairs in the enhanced
dictionary are included; we have merely added to that dictionary by bootstrapping
additional entries from the Bible.
3.2.2 Results. In order to compare tsim with structural similarity scoring, we applied
it to 325 English-French Web document pairs for which human evaluations were car-
ried out in Section 2. As there is only one feature under consideration (tsim), the
classifier must be a threshold on that value. At different thresholds, Cohen?s ? score
of agreement (with each of Resnik?s (1999) two judges and their intersection) may be
computed for comparison with STRAND, along with recall and precision against a
gold standard (for which we use the intersection of the judges: the set of examples
8 There is some circularity here; the cognates were derived using weighted word pairs from the Bible,
then used again in the prior distribution. We note that the resources required to extract cognates in this
way are no different from those required for the translation model.
364
Computational Linguistics Volume 29, Number 3
Table 3
Comparison with STRAND. The test set contains 293 of the 326 pairs in Resnik?s (1999) test
set. The 32 development pairs were used to select manually the 0.44 threshold. N is the
number of examples for which judgment comparison was possible in each case (human judges
were sometimes undecided; those cases are ignored in computing ?).
Comparison N Pr(Agree) ? prec rec F
J1, J2 245 0.98 0.96
J1, STRAND 250 0.88 0.70
J2, STRAND 284 0.88 0.69
J1 ? J2, STRAND 241 0.90 0.75 1.000 0.684 0.812
J1, tsim(? = 0.44) 249 0.96 0.92
J2, tsim(? = 0.44) 283 0.95 0.88
J1 ? J2, tsim(? = 0.44) 240 0.97 0.92 0.833 0.921 0.875
for which the judges agreed). The gold standard contained 86 page pairs marked as
?good? by both judges and 174 page pairs marked as ?bad? by both judges.9
Computing tsim (MCBM on the words in the document pair) is not tractable for
very large documents and translation lexicons. However, in preliminary comparisons,
we found that representing tsim for long documents by as few as their first 500 words
results in excellent performance on the ? measure.10 This allows O(1) estimation of
tsim for two documents. Further, the competitive linking algorithm appears to be as
reliable as MCBM, and it runs significantly faster in practice. The results reported here
approximated tsim in this way.
Of the 325 pairs, 32 were randomly selected as a development set, which we used
to select manually a threshold ? = 0.44. This value maximized the ? score against gold-
standard human judgments on the development set.11 ? scores against each judge and
their intersection were then computed at that threshold on the test set (the remaining
293 pairs). These are compared to ? scores of the STRAND system (with original,
untuned parameters), on the same test set, in Table 3. In every case, the tsim classifier
agreed more strongly with the human evaluations, and its F score is higher than that
of STRAND. Figure 5 shows ? and the F measure plotted against ? .
In this application, the content-based classifier (at its approximate best perfor-
mance, thresholding at 0.44) complements the structural classifier?s high precision.
Given two high-performing methods that use orthogonal information for identifying
good candidate pairs (one using only structure, the other using only content), the nat-
ural question is whether the techniques can be combined for even better performance.
We repeated the experiment presented in Section 2.2.3, adding the tsim score as a
feature. The same cross-validation setup was used, with the same division into folds.
Precision and recall results are reported in Table 4.
9 One additional pair was thrown out because it contained compressed data; it is assumed that this pair
would not pass a language identification filter.
10 This does, of course, run the risk of failing to identify that a page pair is similar only in the beginning,
with diverging content in later text. When the content-based classifier is combined with a structural
classifier, this problem is expected to be eliminated, since the structural method detects such cases. See
the last experiment described in this section.
11 One could select such a threshold to maximize any objective function over the development set. We
note that this threshold differs from that reported in Smith (2002); it was chosen by the same procedure,
though the translation lexicon is different, moving the distribution of tsim scores into a higher range.
365
Resnik and Smith The Web as a Parallel Corpus
STRAND?s 
?
?
STRAND?s F
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Threshold
F
Cohen?s    
Figure 5
Performance measures as the threshold varies (all measures are on the test set): the ?
agreement score with the two judges? intersection and F measure. Scores obtained by
STRAND are shown as well.
Table 4
Effects of parameter tuning with the additional tsim feature.
Precision Recall
Untuned STRAND 1.000 0.686
Tuned STRAND (average) 0.958 0.841
tsim(? = .44) 0.833 0.921
Fold 1 0.875 1.000
Fold 2 1.000 1.000
Fold 3 1.000 1.000
Fold 4 1.000 1.000
Fold 5 1.000 1.000
Fold 6 0.889 1.000
Fold 7 1.000 1.000
Fold 8 1.000 1.000
Fold 9 1.000 0.818
Average 0.974 0.980
The decision trees learned were once again all quite similar, with eight of the nine
rooted as follows (a few of the trees differed slightly in the numerical values used as
thresholds):
if tsim > 0.432 then GOOD
else
if dp > 22.9 then BAD
else ...
The remainder of each tree varied, and there was some evidence of overtraining.
366
Computational Linguistics Volume 29, Number 3
These results clearly demonstrate the benefit of combining structural and content-
based approaches. We next describe how we have adapted the STRAND architecture
to the Internet Archive, in order to generate the candidate pairs on a scale that has
previously been unattainable.
4. Exploiting the Internet Archive
One of the difficulties with doing research on Web mining is that large-scale crawling of
the Web is a significant enterprise. Chen and Nie?s (2000) PTMiner represents one solu-
tion, a carefully thought-out architecture for mining on a large scale. Here we present a
different solution, taking advantage of an existing large-scale repository of Web pages
maintained on an ongoing basis by an organization known as the Internet Archive.
4.1 The Internet Archive
The Internet Archive ?http://www.archive.org/web/researcher/? is a nonprofit or-
ganization attempting to archive the entire publicly available Web, preserving the
content and providing free access to researchers, historians, scholars, and the general
public. Data come from crawls done by Alexa Internet, and hence they represent an
industry-level resource of the sort not easily constructed within academia. At present,
the Archive contains 120TB (terabytes) of data, by a conservative estimate, and it is
growing at approximately 8TB per month. Text on the archive comprises over 10 bil-
lion Web pages, and the estimated duplicate rate is a factor of two (i.e., two copies of
everything).12
The Internet Archive provides public access to the data via the Wayback Machine
Web interface. As of this writing, a search for the ACL home page brings up links
to 72 snapshots of that page dating back to June 7, 1997.13 The reader can get to that
page directly on the Wayback Machine using a URL that points to the Internet Archive
and provides both the desired page and the time stamp indicating which snapshot to
retrieve.14
The Archive also provides researchers with free, direct access to its data via ac-
counts on their cluster. The data are stored on the disk drives of approximately 300 ma-
chines, each running some variety of UNIX, creating what is in essence one huge file
system. This provides a researcher with the remarkable sensation of having the entire
Web on his or her hard drive.
4.2 Properties of the Archive
Mining terabytes on the Archive presents a number of challenges:
? The Archive is a temporal database, but it is not stored in temporal
order. Hence a document and its translation may be in files on different
machines; a global merge of data is required for any hope of complete
extraction.
? Extracting a document for inspection is an expensive operation involving
text decompression.
12 We are grateful to Paula Keezer of Alexa for these figures.
13 http://www.cs.columbia.edu/?acl/home.html
14 http://web.archive.org/web/19970607032410/http://www.cs.columbia.edu/?acl/home.html (this is a
single, long URL.)
367
Resnik and Smith The Web as a Parallel Corpus
? The Archive?s size makes it essential to keep computational complexity
low.
On the other hand, as it turns out, aspects of the Archive?s architecture make rapid
development remarkably feasible:
? Almost all data are stored in compressed plain-text files, rather than in
databases.
? The data relevant for our purposes are organized into archive files
(arcfiles), which contain the stored pages, and index files, which contain
plain text tuples ?URL, time stamp, arcfile, offset, . . .?.
? A suite of tools exists for processing the archive (e.g., for extracting
individual pages from archive files).
? The Archive?s infrastructure for cluster computing makes it easy to write
UNIX scripts or programs and run them in parallel across machines.
The last of these, the cluster computing tools,15 is turned out to reduce drastically the
time needed to port STRAND to the Archive, as well as the size of the STRAND code
base. The centerpiece in Archive cluster computing is a parallelization tool called p2,
which offers a UNIX command-line interface that allows one to specify (1) a paral-
lelizable task, (2) a way to split it up, (3) a way to combine the results, and (4) a set of
processors among which to divide the task. The p2 tool divides up tasks intelligently,
invoking each parallel computation on the local machine where the data reside.
4.3 STRAND on the Archive
In adapting STRAND?s three-stage process to the Internet Archive, the primary chal-
lenge was in the first two steps, locating possible translations and matching them
up to produce candidate document pairs. Structural filtering remained essentially un-
changed.
Generating candidate pairs on the Archive involves the following steps:
1. Extracting URLs from index files using simple pattern matching
2. Combining the results from step 1 into a single huge list
3. Grouping URLs into buckets by handles
4. Generating candidate pairs from buckets
Steps 1 and 2 are performed via a parallel search operation plus combination of results;
for example, extracting all URLs in the Hong Kong, Taiwan, or China domains (and
their associated bookkeeping data) using a pattern like /(.hk|.tw|.cn)/.16
Step 3 is potentially tricky owing to computational complexity issues. As noted
in Section 2.1.2, examining the cross product of a site?s page sets in two different
languages is potentially very expensive, and matching documents by similarity of
URLs can represent a combinatoric process in the general case.
15 The Archive intends to release these cluster tools under the GNU Public License.
16 We also take advantage of the Archive?s list of .com domains paired with the nation in which each is
registered, making it possible to include commercial sites in the search without an explosion of
irrelevant possibilities.
368
Computational Linguistics Volume 29, Number 3
URLs:
saudifrenchbank.com.sa/English/English.htm
? saudifrenchbank.com.sa/English/English.htm
patterns removed:
a e a a english english
saudifrenchbank.com.sa/Arabic/arabic.htm
? saudifrenchbank.com.sa/Arabic/arabic.htm
patterns removed:
a e a a arabic arabic
Same handle for both URLs:
sudifrchbnk.com.s//.htm
Figure 6
Example of LSS subtraction.
We arrived at an algorithmically simple solution that avoids this problem but is
still based on the idea of language-specific substrings (LSSs). The idea is to identify a
set of language-specific URL substrings that pertain to the two languages of interest,
(e.g., based on language names, countries, character codeset labels, abbreviations, etc.).
For example, a set of LSSs for English-Arabic might be as follows (those containing
numerals correspond to character code sets):
1256, 437, 864, 8859-1, 8859-6, a, ar, ara, arab, arabic, cp1256, cp437,
cp864, e, en, eng, english, gb, iso, iso-8859-1, iso-8859-6, latin, latin-1,
latin1, uk, us, usa
For each URL, we form a ?handle? by subtracting any substrings that match (insensi-
tive to case) any item on the LSS pattern list. The subtraction process is implemented
reasonably efficiently: If there are p patterns with maximum length l, and the URL?s
length in characters is u, then the current implementation will do at most p ? u string
matches of length no more than l.17 (Currently we use C strcmp for string matching.)
In practice, this is extremely fast: We can generate handles for nearly 5,000 URLs per
second on a six-year-old Sun Ultra 1 workstation.
Figure 6 illustrates handle generation on two real URLs. As one would hope, these
two URLs produce the same handle, and as a result, they wind up in the same bucket
in step 3.18
In step 4, the URLs in each bucket are used to generate candidate pairs by taking
the cross product and keeping those URL pairs for which the URL bookkeeping data
indicate pages that are in the correct languages. For example, given the bucket con-
taining the two URLs in Figure 6, this step would generate a single pair consisting of
17 We are grateful to Bill Pugh of the University of Maryland for suggesting this algorithm.
18 Conceptually, they hash to the same bucket in a hash table; in practice on the Archive, it turns out to
be more efficient to create buckets by doing a parallel sort of the entire URL set using the handle as the
key, and then creating buckets based on identical handles? being on adjacent lines.
369
Resnik and Smith The Web as a Parallel Corpus
the URL for the English page and the URL for the Arabic page, assuming the language
ID information associated with each URL confirmed it was in the proper language.19
At this point, the candidate generation process is complete. The final step is to
apply STRAND?s filtering step to each candidate pair, an operation that can itself be
parallelized, since each candidate pair can be processed independently. The filtering
pass will eliminate those page pairs (roughly 10% in our experience) whose URLs
show similarity to each other but whose content and/or structure do not.
It is interesting to note that by taking advantage of the Archive?s p2 cluster
computing tool, together with its simple flat-text representations, adapting STRAND?s
candidate generation process resulted in a dramatic reduction in the size of the pro-
gram, cutting it literally in half, as measured in lines of code.
5. Building an English-Arabic Corpus
In the previous sections, we have described methods and results for structural match-
ing, for content-based matching, and for dramatically scaling up the number of candi-
date pairs that can be generated for any given language pair by using the industrial-
strength Web crawls stored on the Internet Archive. In this section we put all these
pieces together, describing an experiment in mining the Internet Archive to find
English-Arabic parallel text. The language pair English-Arabic is of particular global
importance, but resources for it, particularly bilingual text, have generally not been
easy to obtain. Moreover, Arabic is far behind on the Web?s exponential growth curve:
Arabic text (as opposed to images) did not really start emerging on the Web until
the release of Microsoft Windows 98, which provided Arabic support in its version of
Internet Explorer.20
5.1 Finding English-Arabic Candidate Pairs on the Internet Archive
The input resources for our search for English-Arabic candidate pairs were a list of
Internet domains likely to contain Arabic text.21 The list included 24 top-level national
domains for countries where Arabic is spoken by a significant portion of the popula-
tion: Egypt (.eg), Saudi Arabia (.sa), Kuwait (.kw), etc. In addition, we used a list of
.com domains known to originate in Arabic-speaking countries. This list provided an
additional 21 specific domains (e.g., ?emiratesbank.com?, ?checkpoint.com?); note that
the list is by no means exhaustive.
In the experiments we report here, we mined two crawls from 2001, compris-
ing 8TB and 12TB (i.e., less than one-sixth of the Archive as it existed at the time
of the mining effort in early 2002) spread over 27 machines. Our list of URLs with
relevant domains, obtained through pattern matching in Archive index files, numbers
19,917,923 pages.22 The language-specific substrings given earlier were subtracted from
these URLs to generate handles, resulting in 786,880 buckets with an average of 25
pages per bucket. When all possible English-Arabic page pairs were generated from all
19 The Internet Archive tags its data for language using standard n-gram language identification
techniques.
20 While this article was under review, a large Arabic-English parallel corpus of United Nations
proceedings was released by the Linguistic Data Consortium ?http://www.ldc.upenn.edu?. Although
this corpus is certainly of great import, its availability does not detract from the main point of this
study. As noted in Section 1, UN parallel text from LDC is a specialized genre and is encumbered by
fees and licensing restrictions. The experiment reported here provides, at minimum, a supplementary
resource for English-Arabic, and it provides evidence for the viability of the approach.
21 We are grateful to Nizar Habash for constructing this list.
22 Pages with the same URL but different time stamps are counted separately; there were 10,701,622
unique URLs.
370
Computational Linguistics Volume 29, Number 3
Table 5
English-Arabic structural classification results.
Precision Recall
Baseline (on the full set) 0.8993 1.0000
Without tuning Fold 1 1.0000 0.0227
Fold 2 1.0000 0.1364
Fold 3 0.6667 0.0435
Average 0.8889 0.0675
With tuning Fold 1 0.9111 0.9318
Fold 2 0.9302 0.9090
Fold 3 0.9565 0.9565
Average 0.9326 0.9324
buckets, the result was 8,294 candidate pairs. This number is lower than what might
be expected, given the huge number of buckets, because many buckets were mono-
lingual; note that only pairs of one English and one Arabic document are deemed to
be candidates.
A random sample of two hundred candidate pairs was given to two human eval-
uators, bilingual in English and Arabic, who were asked (independently) to answer,
for each pair, the question ?Is this pair of pages intended to show the same material
to two different users, one a reader of English and the other a reader of Arabic?? The
judges? answers showed a Cohen?s ? agreement of 0.6955, which is generally consid-
ered fair to good reliability. (Qualitatively, one judge was rather more strict than the
other; when the stricter judge identified a page pair as valid translations, the less strict
judge virtually always agreed.)
5.2 Evaluating Structure-Based Matching
Taking the set of 149 labeled pairs on which the two judges agreed (134 were marked
?good,? 15 ?bad?), we carried out an evaluation of the full candidate set similar to the
one for English-French discussed in Section 2.2.3. This was a threefold cross-validation
experiment in which decision tree classifiers were tuned on the features extracted for
each candidate pair by structure-based classification.23 In addition to the four structural
scores, we included two language identification confidence scores (one for the English
page, one for the Arabic page); these were available as part of the Internet Archive?s
bookkeeping information for each URL and required no additional computation on
our part. Table 5 shows precision and recall of each fold?s classifier applied to the
corresponding test set of page pairs. The value of the parameter-tuning process is
dramatically confirmed by comparing the learned parameters with STRAND?s default
parameters (manually determined by Resnik [1999]).
Note, however, that the candidate generation system is highly precise to begin
with; only around 10% of the pairs in the random sample of candidates were con-
sidered ?bad? by both judges. A baseline system in which no filtering is done at all
achieves 89.93% precision on the full labeled set (with 100% recall). Depending on the
relative importance of precision and recall, these structure-based classifiers might be
considered worse than that baseline.
23 We did not use more than three folds in the cross-validation, since there were only 15 bad pairs and
more folds would have made random division into folds that each contained both ?good? and ?bad?
pairs difficult.
371
Resnik and Smith The Web as a Parallel Corpus
Upon inspection, we discovered that nearly 5,000 of the pairs in our candidate
set were from a single domain, ?maktoob.com?. This site supports an online market-
place, and many of the pages discovered by our search were dedicated to specific
merchandise categories within that service; a large portion of these were simply ?no
items available? and one or two similar messages. We ignored this domain completely
in order to be conservative about the yield of page pairs, though we note that many
of the pages within it are legitimate parallel text that could be extracted if a good
duplicates filter were applied.24
In order to construct a final classifier, we trained a decision tree on all 149 of the
manually judged examples on which both judges agreed. This was then applied to
the candidate pairs, producing a set of 1,741 HTML document pairs hypothesized to
be valid translations of one another. By way of simple duplicate detection, if a pair of
URLs appeared multiple times (under case-insensitive matching), it was counted only
once. (Note that when this occurs, the duplicate pair will differ by at least one time
stamp, and therefore a more sophisticated technique for eliminating duplication might
extract more text.) The remaining set contained 1,399 pairs.25 Converting from HTML
to plain text and tokenizing, the English documents in this corpus total approximately
673,108 tokens, with an average of 481 tokens per document; the Arabic side contains
845,891 tokens, averaging 605 tokens per document.26
5.3 Combining Structural and Content-Based Matching
We combined the structural and content-based approaches to detecting translations
by adding the tsim score to the set of structural features associated with each candi-
date pair, and then training a new decision tree classifier. Because Arabic is a highly
inflected language with many surface forms, we found it necessary to use morpho-
logical preprocessing in order to make effective use of a dictionary. For English, we
tokenized the text and used the WordNet lemmatizer to strip suffixes. The Arabic texts
were tokenized at punctuation, then romanized and converted to root forms using a
morphological analysis tool (Darwish 2002). This approximately halved the vocabulary
size for the Arabic texts (from 89,047 types to 48,212 types).
The translation lexicon used to compute tsim contained 52,211 entries, each contain-
ing one English lemma and one Arabic root.27 Of these, 16,944 contained two items that
were both present in the candidate set of 8,294 Web page pairs. The approximations
discussed in Section 3.2.2 were employed: Competitive linking on the first 500 words
in each document was used to compute the score.
Carrying out the same cross-validation experiment (on the same random split
of data), the combined structural and content-based classifier produced the results
in Table 6. Also shown is the performance of the tsim-only classifier, assuming an
24 One of our human evaluators confirmed that no other domains appeared to significantly dominate the
candidate pairs as did ?maktoob.com?, providing some assurance that the rest of the data are diverse.
25 Within that set, some documents (not pairs) were present multiple times; there were 1,385 unique
(apart from case) English URLs and 1,385 unique (apart from case) URLs. Since this is only a 1%
duplication rate for each language, we did not attempt to filter further.
26 We converted HTML to text using the lynx browser, performed cleanups such as removing references,
and tokenized using the tokenizers included with the Egypt statistical MT package (Al-Onaizan et
al. 1999). Those tokenizers are somewhat aggressive about separating out punctuation, so, being
aggressive in the opposite direction, we also tried counting only tokens containing at least one of
[A-Za-z] (which excludes punctuation as well as dates, percentages, etc.). Using that very conservative
counting method, the size of the English side is 493,702 words. Counting only tokens on the Arabic
side that contained at least one non-numeric, nonpunctuation character yielded 745,480 words.
27 This translation lexicon was used with the kind permission of Kareem Darwish.
372
Computational Linguistics Volume 29, Number 3
Table 6
English-Arabic combined structural/content-based classification results. The baseline and
content-only classifiers are on the full set, and the structure-only classifier is repeated for
reference.
Precision Recall
Baseline 0.8993 1.0000
Structure only (tuned, average) 0.9326 0.9324
Content only (oracle threshold, ? = 0.058) 0.7688 0.9925
Fold 1 0.9167 1.0000
Fold 2 0.9767 0.9545
Fold 3 0.9583 1.0000
Average 0.9506 0.9848
Table 7
Yield: The English-Arabic Internet Archive corpus, tokenized several ways.
Tokenization method English Tokens Arabic Tokens
Without tsim English lemmas, Arabic roots 620,826 641,654
Egypt tokenizers (Al-Onaizan et al 1999) 673,108 845,891
Egypt tokenizers, words with letters 493,702 745,480
With tsim English lemmas, Arabic roots 960,280 972,392
Egypt tokenizers (Al-Onaizan et al 1999) 1,021,839 1,326,803
Egypt tokenizers, words with letters 800,231 1,213,066
optimal threshold is chosen. Averaged over three folds, the classifier achieved 95.06%
precision and 98.48% recall (1.8% and 5.24% better than without tsim, respectively).
After building a single classifier on all 149 test pairs (the set on which both human
judges agreed), we reclassified the entire candidate set. Ignoring again pages from the
?maktoob.com? domain, 2,206 pairs were marked as translations. The same crude du-
plicate filter was applied, cutting the set back to 1,821 pairs.28 Table 7 shows word
counts for various tokenization schemes: the morphological analysis used for com-
puting tsim, the Egypt tokenizer (which is aggressive), and counting only tokens with
some alphabetic character from the Egypt tokenizer (a conservative approximation).
The analogous results, using the classifier from Section 5.2, are shown for comparison.
To summarize the results, using the content-based similarity score as a feature not
only improved precision, it increased the size of the corpus (in words) by 51?63%,
depending on the tokenization scheme.29
6. Future Work
A number of the techniques we have used to mine parallel data from the Web can be
improved, and we suggest here some directions.
28 There were 1,796 unique English URLs and 1,779 unique Arabic URLs, giving document duplication
rates of 1.4% and 2.4%, respectively.
29 A list of Wayback Machine URLs is available at ?http://umiacs.umd.edu/?resnik/strand/?; a sample
of the document pairs is included in Appendix A.
373
Resnik and Smith The Web as a Parallel Corpus
With respect to classifying document pairs as translations, the reader will notice
that our approach to content-based cross-lingual similarity essentially boils down to
a greedy matching of some of the words in a document pair using a dictionary. It
remains to be seen whether weights in the dictionary can be exploited (Smith [2001]
suggests that empirically estimated joint translation probabilities for word pairs are
not useful). We suggest that the incorporation of scores from information retrieval (e.g.,
inverse document frequency) might be useful in discerning which lexicon entries are
the strongest cues of translational equivalence. We also have not explored any filtering
on the noisy translation lexicon; doing so might improve the quality of the tsim score.
The competitive linking approximation (which, without weights, is essentially
random matching of word pairs) and the use of only the initial portion of each doc-
ument provide significant computational savings. In our experience, neither of these
has significantly hurt the performance of tsim-based classifiers (as compared to find-
ing the maximum cardinality bipartite matching and/or using the full documents),
and in some cases competitive linking seems to improve performance. It is possible
that some sample selection of words from document candidates might be profitable
(e.g., creating a sample of size proportional to the document length, as opposed to a
fixed size, or sampling only from content words, or sampling only words present in
the dictionary).
Smith (2002) suggested a bootstrapping paradigm for the construction of parallel
corpora. Beginning with a seed set of translation information (either parallel corpora
or a bilingual dictionary), high-precision initial classifiers might be constructed using
content and/or structural features (whichever are available). We might then iteratively
select additional page pairs in which the current classifier has high confidence of
translational equivalence, gradually increasing the pool of parallel data and at the
same time expanding the bilingual lexicon. This approach to minimally supervised
classifier construction has been widely studied (Yarowsky 1995), especially in cases in
which the features of interest are orthogonal in some sense (e.g., Blum and Mitchell
1998; Abney 2002).
With respect to the generation of candidate pairs, we have described a progression
from index-based searches on AltaVista to exhaustive matching of URLs on the Inter-
net Archive. The combination of these approaches may be profitable, particularly for
languages that are represented only very sparsely on the Web. For such languages,
index-based searches on words from a language of interest might be used to identify
sites potentially containing parallel text. Within such sites, it would likely be profitable
to look for parallel documents in the full cross product of documents in the two lan-
guages of interest, obtained both on the Internet Archive and via crawling all pages
on relevant sites.
Finally, we plan to utilize parallel texts mined from the Web in our work on
machine translation and acquisition of bilingual lexicons, and in the creation of re-
sources for new languages via projection of annotations from English.
7. Conclusions
Although efforts at discovering parallel text on the Web were first reported in 1998,
Web-based parallel corpora appear to have had only a limited impact on the commu-
nity. Three reasons for this suggest themselves.
Too few languages. Parallel text from the Web has been made available to the
community in only a few pairs of languages. As of this writing, the STRAND Web
site ?http://umiacs.umd.edu/?resnik/strand/?, presenting URL pairs discovered via
STRAND runs, contains collections only for English-French, English-Chinese, English-
374
Computational Linguistics Volume 29, Number 3
Basque, and now English-Arabic, and we are not aware of any other efforts to dissem-
inate Web-based parallel data publicly. Up to this point, it simply has not been easy
to search the Web for parallel text in new language pairs. The most difficult part is
finding the candidates: A year or two ago, we attempted to apply the original Web-
based STRAND to the problem of finding English-Arabic text, and we were unable to
locate enough search engine hits or sites to yield useful results.
Too little data. Very large Web-based parallel text collections are not available to
the community. The largest appear to have been obtained by Chen and Nie (2000),
who acquired collections on the order of 15,000 document pairs for English-French,
English-German, English-Dutch, English-Italian, and English-Chinese using the PT-
Miner system. However, these collections have not been made available to the gen-
eral community.30 In contrast, the STRAND collections, which are available to the
community in the form of URL pairs, are modest in size: The English-Chinese col-
lection contains fewer than 3,500 document pairs, and the English-French fewer than
2,500.
Difficulty with dissemination. Web-based collections are difficult to distribute.
Standard mechanisms of the sort used by the LDC (a CD or downloadable file) are
fraught with difficult legal issues, since, technically speaking, redistributing the ac-
tual content of Web pages could require permission from the author of every page.
For example, presumably as a risk reduction strategy, the Web track for TREC-2002
(Text Retrieval Conference) limited its attention to the .gov domain and required the
recipient of the data to sign a form that reduced the distributor?s liability.31 Simi-
larly, the Google Programming Contest data set arrived with a limited-use license,
indemnification from third-party claims, and a collection limited to the .edu do-
main, from which, presumably, authors are less likely to bring expensive lawsuits
?http://www.google.com/programming-contest/?.
A possible fourth reason may have to do with questions about the utility of the
data. For example, a Web-based parallel collection may be unpredictable in terms of its
coverage, and the community is well aware of the dangers of using training data that
are not representative of the test domain. A solution to this problem might be to extract
topically relevant subsets of the collection for particular domains or applications, but
of course this requires a ?more is better? approach in order to obtain subsets that are
large enough to be useful.
The work reported in this article addresses each of these major problems. With
respect to the number of language pairs, the Internet Archive offers us a huge sample
of pages on the Web, and our techniques make it easy to explore that collection in an
efficient way. Although it is probably impossible to crawl more than a small fraction of
the Web, the Internet Archive is storing the results of commercial-scale Web crawling
and has as its explicit mission the permanent storage of everything that can be found.
The fact that we were able to find a substantial quantity of English-Arabic text (on
the order of a million words per side, looking at less than a sixth of the Archive in
2002) offers the hope that it will be possible to find data for the less well-represented
language pairs, if and when those data actually exist. Moreover, the final implemen-
tation we described here retains the almost entirely language-independent character
of the original STRAND system, adding only the requirement of a reasonable transla-
30 These data were made available only to participants in the iCLEF evaluations for interactive
cross-language IR (C. Picci, personal communication).
31 ?The limitation on permitted use . . . is intended to reduce the risk of any action being brought by
copyright owners, but if this happens the Organisation [recipient] agrees to bear all associated liability?
?http://www.ted.cmis.csiro.au/TRECWeb/access to data.html?.
375
Resnik and Smith The Web as a Parallel Corpus
tion lexicon. Therefore success in mining for parallel text in other languages depends
primarily on whether the data exist in the Archive.
With regard to corpus size, we demonstrated that the recall of structural matching,
and hence its yield, can be significantly improved by simple and automatic classifier
construction, requiring only a few hours? work from a bilingual annotator to create
the training material. These results are further improved by adding content-based
similarity as a feature. Our success with English-Arabic, a language pair that is not
one of those usually considered well represented on the Web, encourages us to believe
that for other languages of interest, we will be similarly successful. We have also
done a bit of exploration to gauge the potential of the Archive for better-represented
language pairs, using English-Chinese as an example. By way of context, Chen and Nie
(2000) reported that PTMiner found around 15,000 English-Chinese document pairs by
crawling 185 sites in the .hk (Hong Kong) domain, with the run taking about a week.
We did a STRAND search of the two Internet Archive crawls used in the English-Arabic
study, seeking English-Chinese parallel text in multiple domains where Chinese is a
dominant language (e.g., .hk, .tw, .cn). Our initial candidate pair set was generated
in approximately 30 hours, and contains over 70,000 candidate page pairs. We are
optimistic that this can be improved still further by expanding the search to include
all sites that contain at least one Chinese document, regardless of the domain.
In terms of dissemination, the STRAND distribution mechanism models itself af-
ter Web search engines, distributing the URLs rather than the pages themselves. This
places the legal burden on individual users, who are presumably safe under fair use
provisions if they download pages for their individual use. Until recently the diffi-
culty with this solution has been that the collection of URLs deteriorates over time as
sites disappear, pages are reorganized, and underlying content changes: For example,
in April 2002, we attempted to download the documents in the STRAND English-
French, English-Chinese, and English-Basque collections, and we were able to access
successfully only around 67%, 43%, and 40% of the URL pairs, respectively. However,
the Internet Archive?s Wayback Machine provides a way to distribute persistent URLs.
With regard to the quality of the data, in Section 2.2.2 we discussed two studies
that demonstrate the utility of parallel Web data in acquiring translation lexicons for
cross-language information retrieval. We also reported on the results of a human rat-
ings study, which provided evidence that English-Chinese data mined from the Web
contain reasonably fluent, reasonably translated sentence pairs. It is worth pointing
out that, because STRAND expects pages to be very similar in structural terms, the
resulting document collections are particularly amenable to sentence- or segment-level
alignment. Indeed, just using dynamic programming to align the markup, ignoring
the text, produces reasonable first-pass alignments of the intervening text as a side
effect. We are currently adapting statistical text-based sentence alignment techniques
to take advantage of the markup available in Web-based document pairs.
Ultimately, the utility of parallel data from the Web is a question that will need
to be addressed in practice. The potential, of course, is as rich and diverse as the Web
itself, and what we as researchers can do with it is an exciting question that remains
to be answered.
Appendix A: Examples
The following page pairs (Figures 7?8) are representative of English-Arabic paral-
lel corpus extracted from the Internet Archive. The text from the pages is shown
in full. Note that the full corpus is available as a list of Wayback Machine URLs at
?http://umiacs.umd.edu/?resnik/strand?. These pages show the generally high qual-
376
Computational Linguistics Volume 29, Number 3
Figure 7
Representative English-Arabic page pair: ?petranews.gov.jo:80/2001/86/tocen.htm? (8 April
2001) and ?petranews.gov.jo:80/2001/86/tocar.htm? (6 April 2001).
Figure 8
Representative English-Arabic page pair: ?www.ole.com.kw:80/pictures.htm? (9 April 2001)
and ?www.ole.com.kw:80/picturesa.htm? (17 April 2001).
377
Resnik and Smith The Web as a Parallel Corpus
ity of the corpus and also illustrate some of the potential difficulties with parallel Web
data. For example, the Arabic page in the first pair includes an additional caption
not present in the English side. These kinds of problems are expected to be overcome
during sentence alignment processing.
Appendix B: Translation Ratings Criteria
For each item, participants were instructed to provide three ratings.
Quality of the English:
3. Very fluent: All of the English is comprehensible.
2. Fairly fluent: The major part of the English passes, but there are
noticeable errors.
1. Barely fluent: Only part of the English meaning is understandable.
0. Unintelligible: Nothing or almost nothing of the English is
comprehensible.
Quality of the Chinese:
3. Very fluent: All of the Chinese is comprehensible.
2. Fairly fluent: The major part of the Chinese passes, but there are
noticeable errors.
1. Barely fluent: Only part of the Chinese meaning is understandable.
0. Unintelligible: Nothing or almost nothing of the Chinese is
comprehensible.
Adequacy of translation:
4. The English and Chinese contain entirely the same meaning.
3. The English and Chinese contain mostly the same meaning.
2. The English and Chinese contain much of the same meaning.
1. The English and Chinese contain little of the same meaning.
0. The English and Chinese contain none of the same meaning.
Acknowledgments
This work has been supported in part by
Department of Defense Contract
RD-02-5700, DARPA/ITO Cooperative
Agreement N660010028910, ONR MURI
Contract FCPO.810548265, and National
Science Foundation Grant EIA0130422. The
second author is supported by a Fannie and
John Hertz Foundation Fellowship. We
would like to thank Nizar Habash, Kareem
Darwish, Mona Diab, Usama Soltan, and
David Smith for their assistance with
Arabic, and Jason Eisner, Rebecca Hwa,
Mark Liberman, Dan Melamed, Michael
Nossal, and Doug Oard for helpful
conversations. We are grateful to Yevgeniy
Lyudovyk for allowing us to use his
language identification software. We would
also like to thank Sang Hong, Greg Marton,
and Michael Subotin for assistance with
implementation, and Brewster Kahle, Andy
Jewell, Jad DeFanti, Brad Tofel, and Paula
378
Computational Linguistics Volume 29, Number 3
Keezer for permitting and facilitating our
use of the Internet Archive. Finally, we are
indebted to several Computational Linguistics
reviewers, whose comments helped us to
greatly improve this article.
References
Abney, Steven. 2002. Bootstrapping. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), Philadelphia, pages 360?367.
Ahuja, Ravindra K., Thomas L. Magnati,
and James B. Orlin. 1993. Network Flows:
Theory, Algorithms, and Applications.
Prentice Hall, Englewood Cliffs, NJ.
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, I. Dan
Melamed, Franz-Josef Och, David Purdy,
Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation.
Technical report, Johns Hopkins
University. Available at ?citeseer.
nj.nec.com/al-onaizan99statistical.html?.
Beesley, Kenneth R. 1988. Language
identifier: A computer program for
automatic natural-language identification
of on-line text. In D. L. Hammond, editor,
Language at the Crossroads: Proceedings of the
29th Annual Conference of the American
Translators Association, Medford, NJ, pages
47?54.
Blum, Avrim and Tom Mitchell. 1998.
Combining labeled and unlabeled data
with co-training. In Proceedings of the
Eleventh Annual Conference on
Computational Learning Theory, pages
92?100, Madison, WI, July.
Broder, Andrei Z., Steven C. Glassman,
Mark S. Manasse, and Geoffrey Zweig.
1997. Syntactic clustering of the Web. In
Proceedings of the Sixth International
World-Wide Web Conference, pages 391?404,
Santa Clara, CA, April.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Cabezas, Clara, Bonnie Dorr, and Philip
Resnik. 2001. Spanish language
processing at University of Maryland:
Building infrastructure for multilingual
applications. In Proceedings of the Second
International Workshop on Spanish Language
Processing and Language Technologies
(SLPLT-2). Available at ?ftp://ftp.
umiacs.umd.edu/pub/bonnie/slplt-
01.htm?.
Cavnar, William B. and John M. Trenkle.
1994. N-gram-based text categorization.
In Proceedings of Third Annual Symposium
on Document Analysis and Information
Retrieval (SDAIR-94), pages 161?175, Las
Vegas, NV.
Chen, Jiang and Jian-Yun Nie. 2000. Parallel
Web text mining for cross-language
information retrieval. In Recherche
d?Informations Assiste?e par Ordinateur
(RIAO), pages 62?77, Paris, April.
Church, Kenneth W. and Robert Mercer.
1993. Introduction to the special issue on
computational linguistics using large
corpora. Computational Linguistics,
19(1):1?24.
Dabbadie, Marianne, Anthony Hartley,
Margaret King, Keith J. Miller, Widad
Mustafa El Hadi, Andrei Popescu-Bellis,
Florence Reeder, and Michelle Vanni.
2002. A hands-on study of the reliability
and coherence of evaluation metrics. In
Workshop on Machine Translation Evaluation:
Human Evaluators Meet Automated Metrics
at the Third International Conference on
Language Resources and Evaluation
(LREC-2000), pages 8?16, Las Palmas,
Canary Islands, Spain, May.
Darwish, Kareem. 2002. Building a shallow
Arabic morphological analyser in one
day. In Proceedings of the Workshop on
Computational Approaches to Semitic
Languages, pages 22?29, Philadelphia, July.
Davis, Mark and Ted Dunning. 1995. A
TREC evaluation of query translation
methods for multi-lingual text retrieval. In
Fourth Text Retrieval Conference (TREC-4),
pages 483?498. NIST, Gaithersburg, MD.
Diab, Mona and Philip Resnik. 2002. An
unsupervised method for word sense
tagging using parallel corpora. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), Philadelphia, July.
Dunning, Ted. 1994. Statistical identification
of language. Computing Research
Laboratory Technical Memo MCCS
94-273, New Mexico State University, Las
Cruces, NM. Available at ?http://
citeseer.nj.nec.com/dunning94statistical.
html?.
Gale, William A. and Kenneth W. Church.
1991. Identifying word correspondences
in parallel texts. In Fourth DARPA
Workshop on Speech and Natural Language,
pages 152?157, Asilomar, CA, February.
Hunt, James W. and M. Douglas McIlroy.
1975. An algorithm for differential file
comparison. Technical Memorandum
75-1271-11, Bell Laboratories, October.
379
Resnik and Smith The Web as a Parallel Corpus
Hwa, Rebecca, Philip Resnik, Amy
Weinberg, and Okan Kolak. 2002.
Evaluating translational correspondence
using annotation projection. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
392?399, Philadelphia, July.
Ingle, Norman C. 1976. A language
identification table. The Incorporated
Linguist, 15(4):98?101.
Landauer, Thomas K. and Michael L.
Littman. 1990. Fully automatic
cross-language document retrieval using
latent semantic indexing. In Proceedings of
the Sixth Annual Conference of the UW Centre
for the New Oxford English Dictionary and
Text Research, pages 31?38, Waterloo,
Ontario, October.
Lin, Dekang. 1998. An information-theoretic
definition of similarity. In Proceedings of the
Fifteenth International Conference on Machine
Learning (ICML-98), pages 296?304,
Madison, WI.
Lopez, Adam, Michael Nossal, Rebecca
Hwa, and Philip Resnik. 2002. Word-level
alignment for multilingual resource
acquisition. In Workshop on Linguistic
Knowledge Acquisition and Representation:
Bootstrapping Annotated Language Data, at
the Third International Conference on
Language Resources and Evaluation
(LREC-2000), Las Palmas, Canary Islands,
Spain, June. Available at ?http://
www.umiacs.umd.edu/?hwa/lnhr02.ps?.
Ma, Xiaoyi and Mark Liberman. 1999. Bits:
A method for bilingual text search over
the Web. In Machine Translation Summit
VII, September. Available at ?http://
www.ldc.upenn.edu/Papers/
MTSVII1999/BITS.ps?.
MacKay, David and Linda Peto. 1995. A
hierarchical Dirichlet language model.
Journal of Natural Language Engineering,
1(3):1?19.
Melamed, I. Dan. 1997. Automatic discovery
of non-compositional compounds in
parallel data. In Proceedings of the 2nd
Conference on Empirical Methods in Natural
Language Processing (EMNLP-97), pages
97?108, Providence, RI, August.
Melamed, I. Dan. 2000. Models of
translational equivalence among words.
Computational Linguistics, 26(2):221?249.
Menezes, Arul and Stephen D. Richardson.
2001. A best-first alignment algorithm for
automatic extraction of transfer mappings
from bilingual corpora. In Proceedings of
the ACL 2001 Workshop on Data-Driven
Methods in Machine Translation, pages
39?46, Toulouse, France.
Nie, Juan-Yun and Jian Cai. 2001. Filtering
noisy parallel corpora of Web pages. In
IEEE Symposium on Natural Language
Processing and Knowledge Engineering,
pages 453?458, Tucson, AZ, October.
Oard, Douglas W. 1997. Cross-language text
retrieval research in the USA. In Third
DELOS Workshop on Cross-Language
Information Retrieval, Zurich, March.
European Research Consortium for
Informatics and Mathematics, Sophia
Antipolis, France. Available at
?http://www.ercim.org/publication/ws-
proceedings/DELOS3/Oard.ps.gz?.
Och, Franz-Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL), pages
295?302, Philadelphia, July.
Resnik, Philip. 1998. Parallel strands: A
preliminary investigation into mining the
Web for bilingual text. In Proceedings of the
Third Conference of the Association for
Machine Translation in the Americas
(AMTA-98). Langhorne, PA, October
28-31. Lecture Notes in Artificial
Intelligence 1529. Available at
?http://umiacs.umd.edu/?resnik/pubs/
amta98.ps.gz?.
Resnik, Philip. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the ACL, pages 527?534,
College Park, MD, June.
Resnik, Philip and I. Dan Melamed. 1997.
Semi-automatic acquisition of
domain-specific translation lexicons. In
Fifth Conference on Applied Natural Language
Processing, Washington, DC.
Resnik, Philip, Douglas Oard, and Gina
Levow. 2001. Improved cross-language
retrieval using backoff translation. In
Proceedings of the First International
Conference on Human Language Technology
Research (HLT-2001), pages 153?155, San
Diego, CA, March.
Resnik, Philip, Mari Broman Olsen, and
Mona Diab. 1999. The Bible as a parallel
corpus: Annotating the ?Book of 2000
Tongues.? Computers and the Humanities,
33:129?153.
Riloff, Ellen, Charles Schafer, and David
Yarowsky. 2002. Inducing information
extraction systems for new languages via
cross-language projection. In Nineteenth
International Conference on Computational
Linguistics (COLING-2002), pages 828?834,
Taipei, Taiwan, August.
Shannon, Claude E. 1948. A mathematical
theory of communication. Bell Systems
Technical Journal, 27:379?423, 623?656.
380
Computational Linguistics Volume 29, Number 3
Smith, Noah A. 2001. Detection of
translational equivalence. Undergraduate
honors thesis, University of Maryland
College Park. Available at
?http://nlp.cs.jhu.edu/?nasmith/cmsc-
thesis.ps?.
Smith, Noah A. 2002. From words to
corpora: Recognizing translation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 95?102, Philadelphia,
July.
Tiedemann, Jo?rg. 1999. Automatic
construction of weighted string similarity
measures. In Joint SIGDAT Conference on
Empirical Methods in Natural Language
Processing and Very Large Corpora, pages
213?219, College Park, MD, June.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 189?196,
Cambridge, MA.
Yarowsky, David and Grace Ngai. 2001.
Inducing multilingual POS taggers and
NP bracketers via robust projection across
aligned corpora. In Proceedings of the
Second Meeting of the North American
Association for Computational Linguistics
(NAACL-2001), pages 200?207, Pittsburgh,
PA, June.
Yarowsky, David, Grace Ngai, and Richard
Wicentowski. 2001. Inducing multilingual
text analysis tools via robust projection
across aligned corpora. In Proceedings of
the First International Conference on Human
Language Technology Research (HLT-2001),
pages 161?168, San Diego, CA, March.
Annealing Techniques for Unsupervised Statistical Language Learning
Noah A. Smith and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,jason}@cs.jhu.edu
Abstract
Exploiting unannotated natural language data is hard
largely because unsupervised parameter estimation is
hard. We describe deterministic annealing (Rose et al,
1990) as an appealing alternative to the Expectation-
Maximization algorithm (Dempster et al, 1977). Seek-
ing to avoid search error, DA begins by globally maxi-
mizing an easy concave function and maintains a local
maximum as it gradually morphs the function into the
desired non-concave likelihood function. Applying DA
to parsing and tagging models is shown to be straight-
forward; significant improvements over EM are shown
on a part-of-speech tagging task. We describe a vari-
ant, skewed DA, which can incorporate a good initializer
when it is available, and show significant improvements
over EM on a grammar induction task.
1 Introduction
Unlabeled data remains a tantalizing potential re-
source for NLP researchers. Some tasks can thrive
on a nearly pure diet of unlabeled data (Yarowsky,
1995; Collins and Singer, 1999; Cucerzan and
Yarowsky, 2003). But for other tasks, such as ma-
chine translation (Brown et al, 1990), the chief
merit of unlabeled data is simply that nothing else
is available; unsupervised parameter estimation is
notorious for achieving mediocre results.
The standard starting point is the Expectation-
Maximization (EM) algorithm (Dempster et al,
1977). EM iteratively adjusts a model?s parame-
ters from an initial guess until it converges to a lo-
cal maximum. Unfortunately, likelihood functions
in practice are riddled with suboptimal local max-
ima (e.g., Charniak, 1993, ch. 7). Moreover, max-
imizing likelihood is not equivalent to maximizing
task-defined accuracy (e.g., Merialdo, 1994).
Here we focus on the search error problem. As-
sume that one has a model for which improving
likelihood really will improve accuracy (e.g., at pre-
dicting hidden part-of-speech (POS) tags or parse
trees). Hence, we seek methods that tend to locate
mountaintops rather than hilltops of the likelihood
function. Alternatively, we might want methods that
find hilltops with other desirable properties.1
1Wang et al (2003) suggest that one should seek a high-
In ?2 we review deterministic annealing (DA)
and show how it generalizes the EM algorithm. ?3
shows how DA can be used for parameter estimation
for models of language structure that use dynamic
programming to compute posteriors over hidden
structure, such as hidden Markov models (HMMs)
and stochastic context-free grammars (SCFGs). In
?4 we apply DA to the problem of learning a tri-
gram POS tagger without labeled data. We then de-
scribe how one of the received strengths of DA?
its robustness to the initializing model parameters?
can be a shortcoming in situations where the ini-
tial parameters carry a helpful bias. We present
a solution to this problem in the form of a new
algorithm, skewed deterministic annealing (SDA;
?5). Finally we apply SDA to a grammar induc-
tion model and demonstrate significantly improved
performance over EM (?6). ?7 highlights future di-
rections for this work.
2 Deterministic annealing
Suppose our data consist of a pairs of random vari-
ables X and Y , where the value of X is observed
and Y is hidden. For example, X might range
over sentences in English and Y over POS tag se-
quences. We use X and Y to denote the sets of
possible values of X and Y , respectively. We seek
to build a model that assigns probabilities to each
(x, y) ? X?Y. Let ~x = {x1, x2, ..., xn} be a corpus
of unlabeled examples. Assume the class of models
is fixed (for example, we might consider only first-
order HMMs with s states, corresponding notion-
ally to POS tags). Then the task is to find good pa-
rameters ~? ? RN for the model. The criterion most
commonly used in building such models from un-
labeled data is maximum likelihood (ML); we seek
the parameters ~??:
argmax
~?
Pr(~x | ~?) = argmax
~?
n?
i=1
?
y?Y
Pr(xi, y | ~?) (1)
entropy hilltop. They argue that to account for partially-
observed (unlabeled) data, one should choose the distribution
with the highest Shannon entropy, subject to certain data-driven
constraints. They show that this desirable distribution is one of
the local maxima of likelihood. Whether high-entropy local
maxima really predict test data better is an empirical question.
Input: ~x, ~?(0) Output: ~??
i? 0
do:
(E) p?(~y)?
Pr(~x,~y|~?(i))
?
~y??Yn Pr(~x,~y?|~?(i))
,?~y
(M) ~?(i+1) ? argmax~? Ep?(~Y )
[
log Pr(~x, ~Y | ~?)
]
i? i+ 1
until ~?(i) ? ~?(i?1)
~?? ? ~?(i)
Fig. 1: The EM algorithm.
Each parameter ?j corresponds to the conditional
probability of a single model event, e.g., a state tran-
sition in an HMM or a rewrite in a PCFG. Many
NLP models make it easy to maximize the likeli-
hood of supervised training data: simply count the
model events in the observed (xi, yi) pairs, and set
the conditional probabilities ?i to be proportional to
the counts. In our unsupervised setting, the yi are
unknown, but solving (1) is almost as easy provided
that we can obtain the posterior distribution of Y
given each xi (that is, Pr(y | xi) for each y ? Y
and each xi). The only difference is that we must
now count the model events fractionally, using the
expected number of occurrences of each (xi, y) pair.
This intuition leads to the EM algorithm in Fig. 1.
It is guaranteed that Pr(~x | ~?(i+1)) ? Pr(~x | ~?(i)).
For language-structure models like HMMs and
SCFGs, efficient dynamic programming algorithms
(forward-backward, inside-outside) are available to
compute the distribution p? at the E step of Fig. 1
and use it at the M step. These algorithms run in
polynomial time and space by structure-sharing the
possible y (tag sequences or parse trees) for each
xi, of which there may be exponentially many in
the length of xi. Even so, the majority of time spent
by EM for such models is on the E steps. In this pa-
per, we can fairly compare the runtime of EM and
other training procedures by counting the number of
E steps they take on a given training set and model.
2.1 Generalizing EM
Figure 2 shows the deterministic annealing (DA) al-
gorithm derived from the framework of Rose et al
(1990). It is quite similar to EM.2 However, DA
adds an outer loop that iteratively increases a value
?, and computation of the posterior in the E step is
modified to involve this ?.
2Other expositions of DA abound; we have couched ours in
data-modeling language. Readers interested in the Lagrangian-
based derivations and analogies to statistical physics (including
phase transitions and the role of ? as the inverse of temperature
in free-energy minimization) are referred to Rose (1998) for a
thorough discussion.
Input: ~x, ~?(0), ?max>?min>0, ?>1 Output: ~??
i? 0; ? ? ?min
while ? ? ?max:
do:
(E) p?(~y)?
Pr(~x,~y|~?(i))
?
?
~y??Yn Pr(~x,~y?|~?(i))
? ,?~y
(M) ~?(i+1) ? argmax~? Ep?(~Y )
[
log Pr(~x, ~Y | ~?)
]
i? i+ 1
until ~?(i) ? ~?(i?1)
? ? ? ? ?
end while
~?? ? ~?(i)
Fig. 2: The DA algorithm: a generalization of EM.
When ? = 1, DA?s inner loop will behave exactly
like EM, computing p? at the E step by the same for-
mula that EM uses. When ? ? 0, p? will be close
to a uniform distribution over the hidden variable ~y,
since each numerator Pr(~x, ~y | ~?)? ? 1. At such
?-values, DA effectively ignores the current param-
eters ? when choosing the posterior p? and the new
parameters. Finally, as ? ? +?, p? tends to place
nearly all of the probability mass on the single most
likely ~y. This winner-take-all situation is equivalent
to the ?Viterbi? variant of the EM algorithm.
2.2 Gradated difficulty
In both the EM and DA algorithms, the E step se-
lects a posterior p? over the hidden variable ~Y and
the M step selects parameters ~?. Neal and Hinton
(1998) show how the EM algorithm can be viewed
as optimizing a single objective function over both ~?
and p?. DA can also be seen this way; DA?s objective
function at a given ? is
F
(
~?, p?, ?
)
=
1
?
H(p?) +Ep?(~Y )
[
log Pr(~x, ~Y | ~?)
]
(2)
The EM version simply sets ? = 1. A complete
derivation is not difficult but is too lengthy to give
here; it is a straightforward extension of that given
by Neal and Hinton for EM.
It is clear that the value of ? allows us to manip-
ulate the relative importance of the two terms when
maximizing F. When ? is close to 0, only the H
term matters. The H term is the Shannon entropy
of the posterior distribution p?, which is known to be
concave in p?. Maximizing it is simple: set al x to be
equiprobable (the uniform distribution). Therefore
a sufficiently small ? drives up the importance of
H relative to the other term, and the entire problem
becomes concave with a single global maximum to
which we expect to converge.
In gradually increasing ? from near 0 to 1, we
start out by solving an easy concave maximization
problem and use the result to initialize the next max-
imization problem, which is slightly more difficult
(i.e., less concave). This continues, with the solu-
tion to each problem in the series being used to ini-
tialize the subsequent problem. When ? reaches 1,
DA behaves just like EM. Since the objective func-
tion is continuous in ? where ? > 0, we can vi-
sualize DA as gradually morphing the easy concave
objective function into the one we really care about
(likelihood); we hope to ?ride the maximum? as ?
moves toward 1.
DA guarantees iterative improvement of the ob-
jective function (see Ueda and Nakano (1998) for
proofs). But it does not guarantee convergence to
a global maximum, or even to a better local maxi-
mum than EM will find, even with extremely slow
?-raising. A new mountain on the surface of the
objective function could arise at any stage that is
preferable to the one that we will ultimately find.
To run DA, we must choose a few control param-
eters. In this paper we set ?max = 1 so that DA
will approach EM and finish at a local maximum of
likelihood. ?min and the ?-increase factor ? can be
set high for speed, but at a risk of introducing lo-
cal maxima too quickly for DA to work as intended.
(Note that a ?fast? schedule that tries only a few ?
values is not as fast as one might expect, since it will
generally take longer to converge at each ? value.)
To conclude the theoretical discussion of DA, we
review its desirable properties. DA is robust to ini-
tial parameters, since when ? is close to 0 the ob-
jective hardly depends on ~?. DA gradually increases
the difficulty of search, which may lead to the avoid-
ance of some local optima. By modifying the an-
nealing schedule, we can change the runtime of the
DA algorithm. DA is almost exactly like EM in im-
plementation, requiring only a slight modification to
the E step (see ?3) and an additional outer loop.
2.3 Prior work
DA was originally described as an algorithm for
clustering data in RN (Rose et al, 1990). Its pre-
decessor, simulated annealing, modifies the objec-
tive function during search by applying random per-
turbations of gradually decreasing size (Kirkpatrick
et al, 1983). Deterministic annealing moves the
randomness ?inside? the objective function by tak-
ing expectations. DA has since been applied to
many problems (Rose, 1998); we describe two key
applications in language and speech processing.
Pereira, Tishby, and Lee (1993) used DA for soft
hierarchical clustering of English nouns, based on
the verbs that select them as direct objects. In their
case, when ? is close to 0, each noun is fuzzily
placed in each cluster so that Pr(cluster | noun)
is nearly uniform. On the M step, this results in
clusters that are almost exactly identical; there is
one effective cluster. As ? is increased, it becomes
increasingly attractive for the cluster centroids to
move apart, or ?split? into two groups (two effective
clusters), and eventually they do so. Continuing to
increase ? yields a hierarchical clustering through
repeated splits. Pereira et al describe the tradeoff
given through ? as a control on the locality of influ-
ence of each noun on the cluster centroids, so that as
? is raised, each noun exerts less influence on more
distant centroids and more on the nearest centroids.
DA has also been applied in speech recognition.
Rao and Rose (2001) used DA for supervised dis-
criminative training of HMMs. Their goal was
to optimize not likelihood but classification error
rate, a difficult objective function that is piecewise-
constant (hence not differentiable everywhere) and
riddled with shallow local minima. Rao and Rose
applied DA,3 moving from training a nearly uni-
form classifier with a concave cost surface (? ? 0)
toward the desired deterministic classifier (? ?
+?). They reported substantial gains in spoken
letter recognition accuracy over both a ML-trained
classifier and a localized error-rate optimizer.
Brown et al (1990) gradually increased learn-
ing difficulty using a series of increasingly complex
models for machine translation. Their training al-
gorithm began by running an EM approximation on
the simplest model, then used the result to initialize
the next, more complex model (which had greater
predictive power and many more parameters), and
so on. Whereas DA provides gradated difficulty
in parameter search, their learning method involves
gradated difficulty among classes of models. The
two are orthogonal and could be used together.
3 DA with dynamic programming
We turn now to the practical use of determinis-
tic annealing in NLP. Readers familiar with the
EM algorithm will note that, for typical stochas-
tic models of language structure (e.g., HMMs and
SCFGs), the bulk of the computational effort is re-
quired by the E step, which is accomplished by
a two-pass dynamic programming (DP) algorithm
(like the forward-backward algorithm). The M step
for these models normalizes the posterior expected
counts from the E step to get probabilities.4
3With an M step modified for their objective function: it im-
proved expected accuracy under p?, not expected log-likelihood.
4That is, assuming the usual generative parameterization of
such models; if we generalize to Markov random fields (also
known as log-linear or maximum entropy models) the M step,
while still concave, might entail an auxiliary optimization rou-
tine such as iterative scaling or a gradient-based method.
Running DA for such models is quite simple and
requires no modifications to the usual DP algo-
rithms. The only change to make is in the values
of the parameters passed to the DP algorithm: sim-
ply replace each ?j by ??j . For a given x, the forward
pass of the DP computes (in a dense representation)
Pr(y | x, ~?) for all y. Each Pr(y | x, ~?) is a product
of some of the ?j (each ?j is multiplied in once for
each time its corresponding model event is present
in (x, y)). Raising the ?j to a power will also raise
their product to that power, so the forward pass will
compute Pr(y | x, ~?)? when given ~?? as parameter
values. The backward pass normalizes to the sum;
in this case it is the sum of the Pr(y | x, ~?)? , and
we have the E step described in Figure 2. We there-
fore expect an EM iteration of DA to take the same
amount of time as a normal EM iteration.5
4 Part-of-speech tagging
We turn now to the task of inducing a trigram POS
tagging model (second-order HMM) from an unla-
beled corpus. This experiment is inspired by the
experiments in Merialdo (1994). As in that work,
complete knowledge of the tagging dictionary is as-
sumed. The task is to find the trigram transition
probabilities Pr(tag i | tag i?1, tag i?2) and emis-
sion probabilities Pr(word i | tag i). Merialdo?s key
result:6 If some labeled data were used to initialize
the parameters (by taking the ML estimate), then it
was not helpful to improve the model?s likelihood
through EM iterations, because this almost always
hurt the accuracy of the model?s Viterbi tagging on
a held-out test set. If only a small amount of labeled
data was used (200 sentences), then some accuracy
improvement was possible using EM, but only for
a few iterations. When no labeled data were used,
EM was able to improve the accuracy of the tagger,
and this improvement continued in the long term.
Our replication of Merialdo?s experiment used
the Wall Street Journal portion of the Penn Tree-
bank corpus, reserving a randomly selected 2,000
sentences (48,526 words) for testing. The remain-
ing 47,208 sentences (1,125,240 words) were used
in training, without any tags. The tagging dictionary
was constructed using the entire corpus (as done by
Merialdo). To initialize, the conditional transition
and emission distributions in the HMM were set to
uniform with slight perturbation. Every distribution
was smoothed using add-0.1 smoothing (at every M
5With one caveat: less pruning may be appropriate because
probability mass is spread more uniformly over different recon-
structions of the hidden data. This paper uses no pruning.
6Similar results were found by Elworthy (1994).
Fig. 3:  Learning curves for
EM and DA.  Steps in DA?s curve
correspond to   ?changes.  The shape of
the DA curve is partly a function of the an?
nealing schedule, which only gradually (and
away from the uniform distribution.
in steps) allows the parameters to move 
?
 40
 45
 50
 55
 60
 65
 70
 75
 0  200  400  600  800  1000  1200%
 co
rre
ct a
mb
igu
ous
 tes
t ta
gs
EM iterations
DAEM
step). The criterion for convergence is that the rela-
tive increase in the objective function between two
iterations fall below 10?9.
4.1 Experiment
In the DA condition, we set ?min = 0.0001, ?max =
1, and ? = 1.2. Results for the completely unsuper-
vised condition (no labeled data) are shown in Fig-
ure 3 and Table 1. Accuracy was nearly monotonic:
the final model is approximately the most accurate.
DA happily obtained a 10% reduction in tag er-
ror rate on training data, and an 11% reduction on
test data. On the other hand, it did not manage to
improve likelihood over EM. So was the accuracy
gain mere luck? Perhaps not. DA may be more re-
sistant to overfitting, because it may favor models
whose posteriors p? have high entropy. At least in
this experiment, its initial bias toward such models
carried over to the final learned model.7
In other words, the higher-entropy local maxi-
mum found by DA, in this case, explained the ob-
served data almost as well without overcommit-
ting to particular tag sequences. The maximum en-
tropy and latent maximum entropy principles (Wang
et al, 2003, discussed in footnote 1) are best justi-
fied as ways to avoid overfitting.
For a supervised tagger, the maximum entropy
principle prefers a conditional model Pr(~y | ~x) that
is maximally unsure about what tag sequence ~y to
apply to the training word sequence ~x (but expects
the same feature counts as the true ~y). Such a model
is hoped to generalize better to unsupervised data.
We can make the same argument. But in our case,
the split between supervised/unsupervised data is
not the split between training/test data. Our super-
vised data are, roughly, the fragments of the training
corpus that are unambiguously tagged thanks to the
tag dictionary.8 The EM model may overfit some
7We computed the entropy over possible tags for each word
in the test corpus, given the sentence the word occurs in. On
average, the DA model had 0.082 bits per tag, while EM had
only 0.057 bits per tag, a statistically significant difference (p <
10?6) under a binomial sign test on word tokens.
8Without the tag dictionary, our learners would treat the tag
final training cross- final test cross- % correct training tags % correct test tags
E steps entropy (bits/word) entropy (bits/word) (all) (ambiguous) (all) (ambiguous)
EM 279 9.136 9.321 82.04 66.61 82.08 66.63
DA 1200 9.138 9.325 83.85 70.02 84.00 70.25
Table 1: EM vs. DA on unsupervised trigram POS tagging, using a tag dictionary. Each of the accuracy results is significant when
accuracy is compared at either the word-level or sentence-level. (Significance at p < 10?6 under a binomial sign test in each
case. E.g., on the test set, the DA model correctly tagged 1,652 words that EM?s model missed while EM correctly tagged 726
words that DA missed. Similarly, the DA model had higher accuracy on 850 sentences, while EM had higher accuracy on only 287.
These differences are extremely unlikely to occur due to chance.) The differences in cross-entropy, compared by sentence, were
significant in the training set but not the test set (p < 0.01 under a binomial sign test). Recall that lower cross entropy means higher
likelihood.
parameters to these fragments. The higher-entropy
DA model may be less likely to overfit, allowing it
to do better on the unsupervised data?i.e., the rest
of the training corpus and the entire test corpus.
We conclude that DA has settled on a local maxi-
mum of the likelihood function that (unsurprisingly)
corresponds well with the entropy criterion, and per-
haps as a result, does better on accuracy.
4.2 Significance
Seeking to determine how well this result general-
ized, we randomly split the corpus into ten equally-
sized, nonoverlapping parts. EM and DA were run
on each portion;9 the results were inconclusive. DA
achieved better test accuracy than EM on three of
ten trials, better training likelihood on five trials,
and better test likelihood on all ten trials.10 Cer-
tainly decreasing the amount of data by an order of
magnitude results in increased variance of the per-
formance of any algorithm?so ten small corpora
were not enough to determine whether to expect an
improvement from DA more often than not.
4.3 Mixing labeled and unlabeled data (I)
In the other conditions described by Merialdo, vary-
ing amounts of labeled data (ranging from 100 sen-
tences to nearly half of the corpus) were used to
initialize the parameters ~?, which were then trained
using EM on the remaining unlabeled data. Only
in the case where 100 labeled examples were used,
and only for a few iterations, did EM improve the
names as interchangeable and could not reasonably be evalu-
ated on gold-standard accuracy.
9The smoothing parameters were scaled down so as to be
proportional to the corpus size.
10It is also worth noting that runtimes were longer with the
10%-sized corpora than the full corpus (EM took 1.5 times as
many E steps; DA, 1.3 times). Perhaps the algorithms traveled
farther to find a local maximum. We know of no study of the
effect of unlabeled training set size on the likelihood surface,
but suggest two issues for future exploration. Larger datasets
contain more idiosyncrasies but provide a stronger overall sig-
nal. Hence, we might expect them to yield a bumpier likelihood
surface whose local maxima are more numerous but also dif-
fer more noticeably in height. Both these tendencies of larger
datasets would in theory increase DA?s advantage over EM.
accuracy of this model. We replicated these experi-
ments and compared EM with DA; DA damaged the
models even more than EM. This is unsurprising; as
noted before, DA effectively ignores the initial pa-
rameters ~?(0). Therefore, even if initializing with a
model trained on small amounts of labeled data had
helped EM, DA would have missed out on this ben-
efit. In the next section we address this issue.
5 Skewed deterministic annealing
The EM algorithm is quite sensitive to the initial pa-
rameters ~?(0). We touted DA?s insensitivity to those
parameters as an advantage, but in scenarios where
well-chosen initial parameters can be provided (as
in ?4.3), we wish for DA to be able exploit them.
In particular, there are at least two cases where
?good? initializers might be known. One is the
case explored by Merialdo, where some labeled data
were available to build an initial model. The other is
a situation where a good distribution is known over
the labels y; we will see an example of this in ?6.
We wish to find a way to incorporate an initializer
into DA and still reap the benefit of gradated diffi-
culty. To see how this will come about, consider
again the E step for DA, which for all y:
p?(y)?
Pr(x, y | ~?)?
Z ?(~?, ?)
=
Pr(x, y | ~?)?u(y)1??
Z(~?, ?)
where u is the uniform distribution over Y and
Z ?(~?, ?) and Z(~?, ?) = Z ?(~?, ?) ? u(y)1?? are nor-
malizing terms. (Note that Z(~?, ?) does not depend
on y because u(y) is constant with respect to y.) Of
course, when ? is close to 0, DA chooses the uni-
form posterior because it has the highest entropy.
Seen this way, DA is interpolating in the log do-
main between two posteriors: the one given by y
and ~? and the uniform one u; the interpolation coef-
ficient is ?. To generalize DA, we will replace the
uniform u with another posterior, the ?skew? pos-
terior p?, which is an input to the algorithm. This
posterior might be specified directly, as it will be in
?6, or it might be computed using an M step from
some good initial ~?(0).
The skewed DA (SDA) E step is given by:
p?(y)?
1
Z(?)
Pr(x, y | ?)? p?(y)1?? (3)
When ? is close to 0, the E step will choose p? to
be very close to p?. With small ?, SDA is a ?cau-
tious? EM variant that is wary of moving too far
from the initializing posterior p? (or, equivalently, the
initial parameters ~?(0)). As ? approaches 1, the ef-
fect of p? will diminish, and when ? = 1, the algo-
rithm becomes identical to EM. The overall objec-
tive (matching (2) except for the boxed term) is:
F?
(
~?, p?, ?
)
=
1
?
H(p?) +Ep?(~Y )
[
log Pr
(
~x, ~Y | ~?
)]
+ 1? ?
?
Ep?(~Y )
[
log p?
(
~Y
)]
Mixing labeled and unlabeled data (II) Return-
ing to Merialdo?s mixed conditions (?4.3), we found
that SDA repaired the damage done by DA but did
not offer any benefit over EM. Its behavior in the
100-labeled sentence condition was similar to that
of EM?s, with a slightly but not significantly higher
peak in training set accuracy. In the other condi-
tions, SDA behaved like EM, with steady degrada-
tion of accuracy as training proceeded. It ultimately
damaged performance only as much as EM did or
did slightly better than EM (but still hurt).
This is unsurprising: Merialdo?s result demon-
strated that ML and maximizing accuracy are gener-
ally not the same; the EM algorithm consistently de-
graded the accuracy of his supervised models. SDA
is simply another search algorithm with the same
criterion as EM. SDA did do what it was expected
to do?it used the initializer, repairing DA damage.
6 Grammar induction
We turn next to the problem of statistical grammar
induction: inducing parse trees over unlabeled text.
An excellent recent result is by Klein and Manning
(2002). The constituent-context model (CCM) they
present is a generative, deficient channel model of
POS tag strings given binary tree bracketings. We
first review the model and describe a small mod-
ification that reduces the deficiency, then compare
both models under EM and DA.
6.1 Constituent-context model
Let (x, y) be a (tag sequence, binary tree) pair. xji
denotes the subsequence of x from the ith to the
jth word. Let yi,j be 1 if the yield from i to j is a
constituent in the tree y and 0 if it is not. The CCM
gives to a pair (x, y) the following probability:
Pr(x, y) = Pr(y) ?
?
1?i?j?|x|
[
?
(
xji
?
?
? yi,j
)
? ? (xi?1, xj+1| yi,j)
]
where ? is a conditional distribution over possi-
ble tag-sequence yields (given whether the yield is
a constituent or not) and ? is a conditional distribu-
tion over possible contexts of one tag on either side
of the yield (given whether the yield is a constituent
or not). There are therefore four distributions to be
estimated; Pr(y) is taken to be uniform.
The model is initialized using expected counts of
the constituent and context features given that all
the trees are generated according to a random-split
model.11
The CCM generates each tag not once but O(n2)
times, once by every constituent or non-constituent
span that dominates it. We suggest the following
modification to alleviate some of the deficiency:
Pr(x, y) = Pr(y) ?
?
1?i?j?|x|
[
?
(
xji
?
?
? yi,j , j ? i+ 1
)
?? (xi?1, xj+1| yi,j)
]
The change is to condition the yield feature ? on
the length of the yield. This decreases deficiency by
disallowing, for example, a constituent over a four-
tag yield to generate a seven-tag sequence. It also
decreases inter-parameter dependence by breaking
the constituent (and non-constituent) distributions
into a separate bin for each possible constituent
length. We will refer to Klein and Manning?s CCM
and our version as models 1 and 2, respectively.
6.2 Experiment
We ran experiments using both CCM models on
the tag sequences of length ten or less in the Wall
Street Journal Penn Treebank corpus, after extract-
ing punctuation. This corpus consists of 7,519 sen-
tences (52,837 tag tokens, 38 types). We report
PARSEVAL scores averaged by constituent (rather
than by sentence), and do not give the learner credit
for getting full sentences or single tags as con-
stituents.12 Because the E step for this model is
computationally intensive, we set the DA parame-
ters at ?min = 0.01, ? = 1.5 so that fewer E steps
would be necessary.13 The convergence criterion
was relative improvement < 10?9 in the objective.
The results are shown in Table 2. The first point
to notice is that a uniform initializer is a bad idea,
as Klein and Manning predicted. All conditions but
11We refer readers to Klein and Manning (2002) or Cover
and Thomas (1991, p. 72) for details; computing expected
counts for a sentence is a closed form operation. Klein and
Manning?s argument for this initialization step is that it is less
biased toward balanced trees than the uniform model used dur-
ing learning; we also found that it works far better in practice.
12This is why the CCM 1 performance reported here differs
from Klein and Manning?s; our implementation of the EM con-
dition gave virtually identical results under either evaluation
scheme (D. Klein, personal communication).
13A pilot study got very similar results for ?min = 10?6.
E steps cross-entropy (bits/tag) UR UP F CB
CCM 1 EM (uniform) 146 103.1654 61.20 45.62 52.27 1.69
DA 403 103.1542 55.13 41.10 47.09 1.91
EM (split) 124 103.1951 78.14 58.24 66.74 0.98
SDA (split) 339 103.1651 62.71 46.75 53.57 1.62
CCM 2 EM (uniform) 26 84.8106 57.60 42.94 49.20 1.86
DA 331 84.7899 40.81 30.42 34.86 2.66
EM (split) 44 84.8049 78.56 58.56 67.10 0.98
SDA (split) 290 84.7940 79.64 59.37 68.03 0.93
Table 2: The two CCM models, trained with two unsupervised algorithms, each with two initializers. Note that DA is equivalent
to SDA initialized with a uniform distribution. The third line corresponds to the setup reported by Klein and Manning (2002).
UR is unlabeled recall, UP is unlabeled precision, F is their harmonic mean, and CB is the average number of crossing brackets
per sentence. All evaluation is on the same data used for unsupervised learning (i.e., there is no training/test split). The high
cross-entropy values arise from the deficiency of models 1 and 2, and are not comparable across models.
one find better structure when initialized with Klein
and Manning?s random-split model. (The exception
is SDA on model 1; possibly the high deficiency of
model 1 interacts poorly with SDA?s search in some
way.)
Next we note that with the random-split initial-
izer, our model 2 is a bit better than model 1 on
PARSEVAL measures and converges more quickly.
Every instance of DA or SDA achieved higher
log-likelihood than the corresponding EM condi-
tion. This is what we hoped to gain from annealing:
better local maxima. In the case of model 2 with
the random-split initializer, SDA significantly out-
performed EM (comparing both matches and cross-
ing brackets per sentence under a binomial sign test,
p < 10?6); we see a > 5% reduction in average
crossing brackets per sentence. Thus, our strategy
of using DA but modifying it to accept an initial-
izer worked as desired in this case, yielding our best
overall performance.
The systematic results we describe next suggest
that these patterns persist across different training
sets in this domain.
6.3 Significance
The difficulty we experienced in finding generaliza-
tion to small datasets, discussed in ?4.2, was appar-
ent here as well. For 10-way and 3-way random,
nonoverlapping splits of the dataset, we did not have
consistent results in favor of either EM or SDA. In-
terestingly, we found that training model 2 (using
EM or SDA) on 10% of the corpus resulted on av-
erage in models that performed nearly as well on
their respective training sets as the full corpus con-
dition did on its training set; see Table 3. In ad-
dition, SDA sometimes performed as well as EM
under model 1. For a random two-way split, EM
and SDA converged to almost identical solutions on
one of the sub-corpora, and SDA outperformed EM
significantly on the other (on model 2).
In order to get multiple points of comparison of
EM and SDA on this task with a larger amount of
data, we jack-knifed the WSJ-10 corpus by split-
ting it randomly into ten equally-sized nonoverlap-
ping parts then training models on the corpus with
each of the ten sub-corpora excluded.14 These trials
are not independent of each other; any two of the
sub-corpora have 89 of their training data in com-
mon. Aggregate results are shown in Table 3. Using
model 2, SDA always outperformed EM, and in 8 of
10 cases the difference was significant when com-
paring matching constituents per sentence (7 of 10
when comparing crossing constituents).15 The vari-
ance of SDA was far less than that of EM; SDA not
only always performed better with model 2, but its
performance was more consistent over the trials.
We conclude this experimental discussion by cau-
tioning that both CCM models are highly deficient
models, and it is unknown how well they generalize
to corpora of longer sentences, other languages, or
corpora of words (rather than POS tags).
7 Future work
There are a number of interesting directions for fu-
ture work. Noting the simplicity of the DA algo-
rithm, we hope that current devotees of EM will
run comparisons of their models with DA (or SDA).
Not only might this improve performance of exist-
14Note that this is not a cross-validation experiment; results
are reported on the unlabeled training set, and the excluded sub-
corpus remains unused.
15Binomial sign test, with significance defined as p < 0.05,
though all significant results had p < 0.001.
10% corpus 90% corpus
?F ?F ?F ?F
CCM 1 EM 65.00 1.091 66.12 0.6643
SDA 63.00 4.689 53.53 0.2135
CCM 2 EM 66.74 1.402 67.24 0.7077
SDA 66.77 1.034 68.07 0.1193
Table 3: The mean ? and standard deviation ? of F -measure
performance for 10 trials using 10% of the corpus and 10 jack-
knifed trials using 90% of the corpus.
ing systems, it will contribute to the general under-
standing of the likelihood surface for a variety of
problems (e.g., this paper has raised the question of
how factors like dataset size and model deficiency
affect the likelihood surface).
DA provides a very natural way to gradually
introduce complexity to clustering models (Rose
et al, 1990; Pereira et al, 1993). This comes about
by manipulating the ? parameter; as it rises, the
number of effective clusters is allowed to increase.
An open question is whether the analogues of ?clus-
ters? in tagging and parsing models?tag symbols
and grammatical categories, respectively?might be
treated in a similar manner under DA. For instance,
we might begin with the CCM, the original formula-
tion of which posits only one distinction about con-
stituency (whether a span is a constituent or not) and
gradually allow splits in constituent-label space, re-
sulting in multiple grammatical categories that, we
hope, arise naturally from the data.
In this paper, we used ?max = 1. It would
be interesting to explore the effect on accuracy of
?quenching,? a phase at the end of optimization
that rapidly raises ? from 1 to the winner-take-all
(Viterbi) variant at ? = +?.
Finally, certain practical speedups may be possi-
ble. For instance, increasing ?min and ?, as noted
in ?2.2, will vary the number of E steps required for
convergence. We suggested that the change might
result in slower or faster convergence; optimizing
the schedule using an online algorithm (or deter-
mining precisely how these parameters affect the
schedule in practice) may prove beneficial. Another
possibility is to relax the convergence criterion for
earlier ? values, requiring fewer E steps before in-
creasing ?, or even raising ? slightly after every E
step (collapsing the outer and inner loops).
8 Conclusion
We have reviewed the DA algorithm, describing
it as a generalization of EM with certain desir-
able properties, most notably the gradual increase
of difficulty of learning and the ease of imple-
mentation for NLP models. We have shown how
DA can be used to improve the accuracy of a tri-
gram POS tagger learned from an unlabeled cor-
pus. We described a potential shortcoming of DA
for NLP applications?its failure to exploit good
initializers?and then described a novel algorithm,
skewed DA, that solves this problem. Finally, we re-
ported significant improvements to a state-of-the-art
grammar induction model using SDA and a slight
modification to the parameterization of that model.
These results support the case that annealing tech-
niques in some cases offer performance gains over
the standard EM approach to learning from unla-
beled corpora, particularly with large corpora.
Acknowledgements
This work was supported by a fellowship to the first au-
thor from the Fannie and John Hertz Foundation, and
by an NSF ITR grant to the second author. The views
expressed are not necessarily endorsed by the sponsors.
The authors thank Shankar Kumar, Charles Schafer,
David Smith, and Roy Tromble for helpful comments
and discussions; three ACL reviewers for advice that im-
proved the paper; Eric Goldlust for keeping the Dyna
compiler (Eisner et al, 2004) up to date with the de-
mands made by this work; and Dan Klein for sharing
details of his CCM implementation.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Je-
linek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990.
A statistical approach to machine translation. Computational
Linguistics, 16(2):79?85.
E. Charniak. 1993. Statistical Language Learning. MIT Press.
M. Collins and Y. Singer. 1999. Unsupervised models for
named-entity classification. In Proc. of EMNLP.
T. M. Cover and J. A. Thomas. 1991. Elements of Information
Theory. John Wiley and Sons.
S. Cucerzan and D. Yarowsky. 2003. Minimally supervised
induction of grammatical gender. In Proc. of HLT/NAACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likeli-
hood estimation from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declar-
ative language for implementing dynamic programs. In Proc.
of ACL (companion volume).
D. Elworthy. 1994. Does Baum-Welch re-estimation help tag-
gers? In Proc. of ANLP.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220:671?680.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for grammar induction. In Proc. of ACL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?72.
R. Neal and G. Hinton. 1998. A view of the EM algorithm
that justifies incremental, sparse, and other variants. In M. I.
Jordan, editor, Learning in Graphical Models. Kluwer.
F. C. N. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proc. of ACL.
A. Rao and K. Rose. 2001. Deterministically annealed design
of Hidden Markov Model speech recognizers. IEEE Transac-
tions on Speech and Audio Processing, 9(2):111?126.
K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical me-
chanics and phase transitions in clustering. Physical Review
Letters, 65(8):945?948.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimization
problems. Proc. of the IEEE, 86(11):2210?2239.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271?282.
S. Wang, D. Schuurmans, and Y. Zhao. 2003. The latent maxi-
mum entropy principle. In review.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of ACL.
Dyna: A Declarative Language for Implementing Dynamic Programs?
Jason Eisner and Eric Goldlust and Noah A. Smith
Department of Computer Science, Johns Hopkins University
Baltimore, MD 21218 U.S.A.
{jason,eerat,nasmith}@cs.jhu.edu
Abstract
We present the first version of a new declarative pro-
gramming language. Dyna has many uses but was de-
signed especially for rapid development of new statis-
tical NLP systems. A Dyna program is a small set of
equations, resembling Prolog inference rules, that spec-
ify the abstract structure of a dynamic programming al-
gorithm. It compiles into efficient, portable, C++ classes
that can be easily invoked from a larger application. By
default, these classes run a generalization of agenda-
based parsing, prioritizing the partial parses by some
figure of merit. The classes can also perform an exact
backward (outside) pass in the service of parameter train-
ing. The compiler already knows several implementation
tricks, algorithmic transforms, and numerical optimiza-
tion techniques. It will acquire more over time: we in-
tend for it to generalize and encapsulate best practices,
and serve as a testbed for new practices. Dyna is now be-
ing used for parsing, machine translation, morphological
analysis, grammar induction, and finite-state modeling.
1 Introduction
Computational linguistics has become a more experi-
mental science. One often uses real-world data to test
one?s formal models (grammatical, statistical, or both).
Unfortunately, as in other experimental sciences, test-
ing each new hypothesis requires much tedious lab
work: writing and tuning code until parameter estimation
(?training?) and inference over unknown variables (?de-
coding?) are bug-free and tolerably fast. This is intensive
work, given complex models or a large search space (as
in modern statistical parsing and machine translation). It
is a major effort to break into the field with a new system,
and modifying existing systems?even in a conceptually
simple way?can require significant reengineering.
Such ?lab work? mainly consists of reusing or rein-
venting various dynamic programming architectures. We
propose that it is time to jump up a level of abstraction.
We offer a new programming language, Dyna, that al-
lows one to quickly and easily specify a model?s com-
binatorial structure. We also offer a compiler, dynac,
that translates from Dyna into C++ classes. The com-
piler does all the tedious work of writing the training and
decoding code. It is intended to do as good a job as a
clever graduate student who already knows the tricks of
the trade (and is willing to maintain hand-tuned C++).
? We would like to thank Joshua Goodman, David McAllester, and
Paul Ruhlen for useful early discussions, and pioneer users Markus
Dreyer, David Smith, and Roy Tromble for their feedback and input.
This work was supported by NSF ITR grant IIS-0313193 to the first
author, by a Fannie & John Hertz Foundation fellowship to the third
author, and by ONR MURI grant N00014-01-1-0685. The views ex-
pressed are not necessarily endorsed by the sponsors.
2 A Basic Example: PCFG Parsing
We believe Dyna is a flexible and intuitive specification
language for dynamic programs. Such a program spec-
ifies how to combine partial solutions until a complete
solution is reached.
2.1 The Inside Algorithm, in Dyna
Fig. 1 shows a simple Dyna program that corresponds
to the inside algorithm for PCFGs (i.e., the probabilis-
tic generalization of CKY parsing). It may be regarded
as a system of equations over an arbitrary number of
unknowns, which have structured names such as con-
stit(s,0,3). These unknowns are called items. They re-
semble variables in a C program, but we use variable
instead to refer to the capitalized identifiers X, I, K, . . . in
lines 2?4.1
At runtime, a user must provide an input sentence and
grammar by asserting values for certain items. If the
input is John loves Mary, the user should assert values
of 1 for word(John,0,1), word(loves,1,2), word(Mary,2,3),
and end(3). If the PCFG contains a rewrite rule np ?
Mary with probability p(Mary | np) = 0.003, the user
should assert that rewrite(np,Mary) has value 0.003.
Given these base cases, the equations in Fig. 1 en-
able Dyna to deduce values for other items. The de-
duced value of constit(s,0,3) will be the inside probability
?s(0, 3),2 and the deduced value of goal will be the total
probability of all parses of the input.
Lines 2?4 are equational schemas that specify how to
compute the value of items such as constit(s,0,3) from
the values of other items. By using the summation op-
erator +=, lines 2?3 jointly say that for any X, I, and
K, constit(X,I,K) is defined by summation over the re-
maining variables, as
?
W rewrite(X,W)*word(W,I,K) +?
Y,Z,J rewrite(X,Y,Z)*constit(Y,I,J)*constit(Z,J,K). For
example, constit(s,0,3) is a sum of quantities such as
rewrite(s,np,vp)*constit(np,0,1)*constit(vp,1,3).
2.2 The Execution Model
Dyna?s declarative semantics state only that it will find
values such that all the equations hold.3 Our implemen-
tation?s default strategy is to propagate updates from an
equation?s right-hand to its left-hand side, until the sys-
tem converges. Thus, by default, Fig. 1 yields a bottom-
up or data-driven parser.
1Much of our terminology (item, chart, agenda) is inherited from
the parsing literature. Other terminology (variable, term, inference rule,
antecedent/consequent, assert/retract, chaining) comes from logic pro-
gramming. Dyna?s syntax borrows from both Prolog and C.
2That is, the probability that s would stochastically rewrite to the
first three words of the input. If this can happen in more than one way,
the probability sums over multiple derivations.
3Thus, future versions of the compiler are free to mix any efficient
strategies, even calling numerical equation solvers.
1. :- valtype(term, real). % declares that all item values are real numbers
2. constit(X,I,K) += rewrite(X,W) * word(W,I,K). % a constituent is either a word . . .
3. constit(X,I,K) += rewrite(X,Y,Z) * constit(Y,I,J) * constit(Z,J,K). % . . . or a combination of two adjacent subconstituents
4. goal += constit(s,0,N) * end(N). % a parse is any s constituent that covers the input string
Figure 1: A probabilistic CKY parser written in Dyna.
Dyna may be seen as a new kind of tabled logic
programming language in which theorems are not just
proved, but carry values. This suggests some terminol-
ogy. Lines 2?4 of Fig. 1 are called inference rules. The
items on the right-hand side are antecedents, and the
item on the left-hand side is their consequent. Asser-
tions can be regarded as axioms. And the default strategy
(unlike Prolog?s) is forward chaining from the axioms,
as in some theorem provers.
Suppose constit(verb,1,2) increases by ?. Then
the program in Fig. 1 must find all the instantiated
rules that have constit(verb,1,2) as an antecedent,
and must update their consequents. For example,
since line 3 can be instantiated as constit(vp,1,3) +=
rewrite(vp,verb,np)*constit(verb,1,2)*constit(np,2,3),
then constit(vp,1,3) must be increased by
rewrite(vp,verb,np) * ? * constit(np,2,3).
Line 3 actually requires infinitely many such up-
dates, corresponding to all rule instantiations of
the form constit(X,1,K) += rewrite(X,verb,Z)*con-
stit(verb,1,2)*constit(Z,2,K).4 However, most of these
updates would have no effect. We only need to consider
the finitely many instantiations where rewrite(X,verb,Z)
and constit(Z,2,K) have nonzero values (because they
have been asserted or updated in the past).
The compiled Dyna program rapidly computes this set
of needed updates and adds them to a worklist of pend-
ing updates, the agenda. Updates from the agenda are
processed in some prioritized order (which can strongly
affect the speed of the program). When an update is car-
ried out (e.g., constit(vp,1,3) is increased), any further
updates that it triggers (e.g., to constit(s,0,3)) are placed
back on the agenda in the same way. Multiple updates
to the same item are consolidated on the agenda. This
cascading update process begins with axiom assertions,
which are treated like other updates.
2.3 Closely Related Algorithms
We now give some examples of variant algorithms.
Fig. 1 provides lattice parsing for free. Instead of
being integer positions in an string, I, J and K can be
symbols denoting states in a finite-state automaton. The
code does not have to change, only the input. Axioms
should now correspond to weighted lattice arcs, e.g.,
word(loves,q,r) with value p(portion of speech signal |
loves).
To find the probability of the best parse instead of the
total probability of all parses, simply change the value
type: replace real with viterbi in line 1. If a and b are
viterbi values, a+b is implemented as max(a, b).5
4As well as instantiations constit(X,I,2) += rewrite(X,Y,
verb)*constit(Y,I,1)*constit(verb,1,2).
5Also, a*b is implemented as a+ b, as viterbi values actually rep-
resent log probabilities (for speed and dynamic range).
Similarly, replacing real with boolean obtains an un-
weighted parser, in which a constituent is either derived
(true value) or not (false value) Then a*b is implemented
as a ? b, and a+b as a ? b.
The Dyna programmer can declare the agenda disci-
pline?i.e., the order in which updates are processed?to
obtain variant algorithms. Although Dyna supports stack
and queue (LIFO and FIFO) disciplines, its default is to
use a priority queue prioritized by the size of the update.
When parsing with real values, this quickly accumulates
a good approximation of the inside probabilities, which
permits heuristic early stopping before the agenda is
empty. With viterbi values, it amounts to uniform-cost
search for the best parse, and an item?s value is guaran-
teed not to change once it is nonzero. Dyna will soon al-
low user-defined priority functions (themselves dynamic
programs), which can greatly speed up parsing (Cara-
ballo and Charniak, 1998; Klein and Manning, 2003).
2.4 Parameter Training
Dyna provides facilities for training parameters. For ex-
ample, from Fig. 1, it automatically derives the inside-
outside (EM) algorithm for training PCFGs.
How is this possible? Once the program of Fig. 1 has
run, goal?s value is the probability of the input sentence
under the grammar. This is a continuous function of
the axiom values, which correspond to PCFG parame-
ters (e.g., the weight of rewrite(np,Mary)). The function
could be written out explicitly as a sum of products of
sums of products of . . . of axiom values, with the details
depending on the sentence and grammar.
Thus, Dyna can be regarded as computing a function
F (~?), where ~? is a vector of axiom values and F (~?) is an
objective function such as the probability of one?s train-
ing data. In learning, one wishes to repeatedly adjust ~?
so as to increase F (~?).
Dyna can be told to evaluate the gradient of the func-
tion with respect to the current parameters ~?: e.g., if
rewrite(vp,verb,np) were increased by , what would hap-
pen to goal? Then any gradient-based optimization
method can be applied, using Dyna to evaluate both F (~?)
and its gradient vector. Also, EM can be applied where
appropriate, since it can be shown that EM?s E counts can
be derived from the gradient. Dyna?s strategy for com-
puting the gradient is automatic differentiation in the re-
verse mode (Griewank and Corliss, 1991), known in the
neural network community as back-propagation.
Dyna comes with a constrained optimization module,
DynaMITE,6 that can locally optimize F (~?). At present,
DynaMITE provides the conjugate gradient and variable
metric methods, using the Toolkit for Advanced Opti-
mization (Benson et al, 2000) together with a softmax
6DynaMITE = Dyna Module for Iterative Training and Estimation.
technique to enforce sum-to-one constraints. It supports
maximum-entropy training and the EM algorithm.7
DynaMITE provides an object-oriented API that al-
lows independent variation of such diverse elements of
training as the model parameterization, optimization al-
gorithm, smoothing techniques, priors, and datasets.
How about supervised or partly supervised training?
The role of supervision is to permit some constituents
to be built but not others (Pereira and Schabes, 1992).
Lines 2?3 of Fig. 1 can simply be extended with an addi-
tional antecedent permitted(X,I,K), which must be either
asserted or derived for constit(X,I,K) to be derived. In
?soft? supervision, the permitted axioms may have val-
ues between 0 and 1.8
3 C++ Interface and Implementation
A Dyna program compiles to a set of portable C++
classes that manage the items and perform inference.
These classes can be used in a larger C++ application.9
This strategy keeps Dyna both small and convenient.
A C++ chart object supports the computation of item
values and gradients. It keeps track of built items, their
values, and their derivations, which form a proof for-
est. It also holds an ordered agenda of pending updates.
Some built items may be ?transient,? meaning that they
are not actually stored in the chart at the moment but will
be transparently recomputed upon demand.
The Dyna compiler generates a hard-coded decision
tree that analyzes the structure of each item popped from
the agenda to decide which inference rules apply to it.
To enable fast lookup of the other items that participate
in these inference rules, it generates code to maintain ap-
propriate indices on the chart.
Objects such as constit(vp,1,3) are called terms and
may be recursively nested to any depth. (Items are just
terms with values.) Dyna has a full first-order type sys-
tem for terms, including primitive and disjunctive types,
and permitting compile-time type inference. These types
are compiled into C++ classes that support construc-
tors and accessors, garbage-collection, subterm sharing
(which may lead to asymptotic speedups, as in CCG pars-
ing (Vijay-Shanker and Weir, 1990)), and interning.10
Dyna can import new primitive term types and value
types from C++, as well as C++ functions to combine
values and to user-define the weights of certain terms.
In the current implementation, every rule must have
the restricted form c += a1*a2* ? ? ? *ak (where each ai
is an item or side condition and (X, +, *) is a semiring
of values). The design for Dyna?s next version lifts this
restriction to allow arbitrary, type-heterogeneous expres-
sions on the right-hand side of an inference rule.11
7It will eventually offer additional methods, such as deterministic
annealing, simulated annealing, and iterative scaling.
8Such item values are not probabilities. We are generally interested
in log-linear models for parsing (Riezler et al, 2000) and other tasks.
9We are also now developing a default application: a visual debug-
ger that allows a user to assert axioms and explore the proof forest
created during inference.
10Interned values are hashed so that equal values are represented by
equal pointers. It is very fast to compare and hash such representations.
11That will make Dyna useful for a wider variety of non-NLP algo-
4 Some Further Applications
Dyna is useful for any problem where partial hypothe-
ses are assembled, or where consistency has to be main-
tained. It is already being used for parsing, syntax-based
machine translation, morphological analysis, grammar
induction, and finite-state operations.
It is well known that various parsing algorithms for
CFG and other formalisms can be simply written in terms
of inference rules. Fig. 2 renders one such example
in Dyna, namely Earley?s algorithm. Two features are
worth noting: the use of recursively nested subterms such
as lists, and the SIDE function, which evaluates to 1 or 0
according to whether its argument has a defined value
yet. These side conditions are used here to prevent hy-
pothesizing a constituent until there is a possible left con-
text that calls for it.
Several recent syntax-directed statistical machine
translation models are easy to build in Dyna. The sim-
plest (Wu, 1997) uses constit(np,3,5,np,4,8) to denote a
NP spanning positions 3?5 in the English string that is
aligned with an NP spanning positions 4?8 in the Chi-
nese string. When training or decoding, the hypotheses
of better-trained monolingual parsers can provide either
hard or soft partial supervision (section 2.4).
Dyna can manipulate finite-state transducers. For in-
stance, the weighted arcs of the composed FST M1 ?M2
can be deduced from the arcs of M1 and M2. Training
M1 ?M2 back-propagates to train the original weights in
M1 and M2, as in (Eisner, 2002).
5 Speed and Code Size
One of our future priorities is speed. Comparing infor-
mally to the best hand-written C++ code we found online
for inside-outside and Dijkstra?s algorithms, Dyna (like
Java) currently runs up to 5 times slower. We mainly un-
derstand the reasons (memory layout and overreliance on
hashing) and are working actively to close the gap.12
Programmer time is also worth considering. Our
inside-outside and Dijkstra?s algorithms are each about
5 lines of Dyna code (plus a short C driver program),
but were compared in the previous paragraph against ef-
ficient C++ implementations of 5500 and 900 lines.13
Our colleague Markus Dreyer, as his first Dyna pro-
gram, decided to replicate the Collins parser (3400 lines
of C). His implementation used under 40 lines of Dyna
code, plus a 300-line C++ driver program that mostly
dealt with I/O. One of us (Smith) has written substan-
tially more complex Dyna programs (e.g., 56 types +
46 inference rules), enabling research that he would not
have been willing to undertake in another language.
6 Related Work
This project tries to synthesize much folk wisdom. For
NLP algorithms, three excellent longer papers have at-
rithms (e.g., neural networks, constraint programming, clustering, and
dynamic graph algorithms). However, it introduces several interesting
design complications in the Dyna language and the implementation.
12Dyna spends most of its time manipulating hash tables and the
priority queue. Inference is very fast because it is compiled.
13The code size comparisons are rough ones, because of mismatches
between the programs being compared.
1. need(s,0) = 1. % begin by looking for an s that starts at position 0
2. constit(Nonterm/Needed,I,I) += SIDE(need(Nonterm,I)) * rewrite(Nonterm,Needed). % traditional predict step
3. constit(Nonterm/Needed,I,K) += constit(Nonterm/cons(W,Needed),I,J) * word(W,J,K). % traditional scan step
4. constit(Nonterm/Needed,I,K) += constit(Nonterm,cons(X,Needed),I,J) * constit(X/nil,J,K). % traditional complete step
5. goal += constit(s/nil,0,N) * end(N). % we want a complete s constituent covering the sentence
6. need(Nonterm,J) += constit( /cons(Nonterm, ), ,J). % Note: underscore matches anything (anonymous wildcard)
Figure 2: An Earley parser in Dyna. np/Needed is syntactic sugar for slash(np,Needed), which is the label of a partial
np constituent that is still missing the list of subconstituents in Needed. In particular, np/nil is a complete np. (A list
[n,pp] is encoded here as cons(n,cons(pp,nil)), although syntactic sugar for lists is also available.) need(np,3) is derived
if some partial constituent seeks an np subconstituent starting at position 3. As usual, probabilistic, agenda-based
lattice parsing comes for free, as does training.
tempted similar syntheses (though without covering vari-
ant search and storage strategies, which Dyna handles).
Shieber et al (1995) (already noting that ?many of the
ideas we present are not new?) showed that several un-
weighted parsing algorithms can be specified in terms of
inference rules, and used Prolog to implement an agenda-
based interpreter for such rules. McAllester (1999) made
a similar case for static analysis algorithms, with a more
rigorous discussion of indexing the chart.
Goodman (1999) generalized this line of work
to weighted parsing, using rules of the form
c += a1*a2* ? ? ? *ak (with side conditions allowed);
he permitted values to fall in any semiring, and gen-
eralized the inside-outside algorithm. Our approach
extends this to a wider variety of processing orders, and
in particular shows how to use a prioritized agenda in
the general case, using novel algorithms. We also extend
to a wider class of formulas (e.g., neural networks).
The closest implemented work we have found is
PRISM (Zhou and Sato, 2003), a kind of probabilis-
tic Prolog that claims to be efficient (thanks to tabling,
compilation, and years of development) and can handle
a subset of the cases described by Goodman. It is in-
teresting because it inherits expressive power from Pro-
log. On the other hand, its rigid probabilistic framework
does not permit side conditions (Fig. 2), general semir-
ings (Goodman), or general formulas (Dyna). PRISM
does not currently seem practical for statistical NLP re-
search: in CKY parsing tests, it was only able to handle
a small fraction of the Penn Treebank ruleset (2400 high-
probability rules) and tended to crash on sentences over
50 words. Dyna, by contrast, is designed for real-world
use: it consistently parses over 10x faster than PRISM,
scales to full-sized problems, and attempts to cover real-
world necessities such as prioritization, bottom-up infer-
ence, pruning, smoothing, underflow avoidance, maxent,
non-EM optimization techniques, etc.
7 Conclusions
Dyna is a declarative programming language for building
efficient systems quickly. As a language, it is inspired
by previous work in deductive parsing, adding weights
in a particularly general way. Dyna?s compiler has been
designed with an eye toward low-level issues (indexing,
structure-sharing, garbage collection, etc.) so that the
cost of this abstraction is minimized.
The goal of Dyna is to facilitate experimentation: a
new model or algorithm automatically gets a new mem-
ory layout, indexing, and training code. We hope this
will lower the barrier to entry in the field, in both research
and education. In Dyna we seek to exploit as many al-
gorithmic tricks as we can, generalizing them to as many
problems as possible on behalf of future Dyna programs.
In turn the body of old programs can provide a unified
testbed for new training and decoding techniques.
Our broader vision is to unify a problem?s possible al-
gorithms by automatically deriving all of them and their
possible training procedures from a single high-level
Dyna program, using source-to-source program transfor-
mations and compiler directives. We plan to choose auto-
matically among these variants by machine learning over
runs on typical data. This involves, for example, auto-
matically learning a figure of merit to guide decoding.
The Dyna compiler, documentation, and examples can
be found at www.dyna.org. The compiler is available
under an open-source license. The commented C++ code
that it generates is free to modify.
References
S. Benson, L. C. McInnes, and J. J. More?. 2000. TAO users
manual. Tech Rpt ANL/MCS-TM-242, Argonne Nat. Lab.
S. A. Caraballo, E. Charniak. 1998. New figures of merit for
best-first probabilistic chart parsing. Comp. Ling., 24(2).
Jason Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proc. of ACL.
Joshua Goodman. 1999. Semiring parsing. Comp. Ling, 25(4).
Andreas Griewank and George Corliss, editors. 1991. Auto-
matic Differentiation of Algorithms. SIAM.
Dan Klein and Christopher D. Manning. 2003. A? parsing:
Fast exact Viterbi parse selection. Proc. of HLT-NAACL.
David McAllester. 1999. On the complexity analysis of static
analyses. 6th Intl. Static Analysis Symposium.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. Proc. of ACL.
S. Riezler, D. Prescher, J. Kuhn, M. Johnson. 2000. Lexical-
ized stochastic modeling of constraint-based grammars us-
ing log-linear measures and EM training. Proc. of ACL.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira. 1995.
Principles and implementation of deductive parsing. Jour-
nal of Logic Programming.
K. Vijay-Shanker and D. Weir. 1990. Polynomial-time parsing
of combinatory categorial grammars. Proc. of ACL.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
N.-F. Zhou and T. Sato. 2003. Toward a high-performance sys-
tem for symbolic and statistical modeling. IJCAI-03 Work-
shop on Learning Statistical Models from Relational Data.
Proceedings of the 43rd Annual Meeting of the ACL, pages 354?362,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Contrastive Estimation: Training Log-Linear Models on Unlabeled Data?
Noah A. Smith and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,jason}@cs.jhu.edu
Abstract
Conditional random fields (Lafferty et al, 2001) are
quite effective at sequence labeling tasks like shal-
low parsing (Sha and Pereira, 2003) and named-
entity extraction (McCallum and Li, 2003). CRFs
are log-linear, allowing the incorporation of arbi-
trary features into the model. To train on unlabeled
data, we require unsupervised estimation methods
for log-linear models; few exist. We describe a novel
approach, contrastive estimation. We show that the
new technique can be intuitively understood as ex-
ploiting implicit negative evidence and is computa-
tionally efficient. Applied to a sequence labeling
problem?POS tagging given a tagging dictionary
and unlabeled text?contrastive estimation outper-
forms EM (with the same feature set), is more robust
to degradations of the dictionary, and can largely re-
cover by modeling additional features.
1 Introduction
Finding linguistic structure in raw text is not easy.
The classical forward-backward and inside-outside
algorithms try to guide probabilistic models to dis-
cover structure in text, but they tend to get stuck in
local maxima (Charniak, 1993). Even when they
avoid local maxima (e.g., through clever initializa-
tion) they typically deviate from human ideas of
what the ?right? structure is (Merialdo, 1994).
One strategy is to incorporate domain knowledge
into the model?s structure. Instead of blind HMMs
or PCFGs, one could use models whose features
?This work was supported by a Fannie and John Hertz
Foundation fellowship to the first author and NSF ITR grant IIS-
0313193 to the second author. The views expressed are not nec-
essarily endorsed by the sponsors. The authors also thank three
anonymous ACL reviewers for helpful comments, colleagues
at JHU CLSP (especially David Smith and Roy Tromble) and
Miles Osborne for insightful feedback, and Eric Goldlust and
Markus Dreyer for Dyna language support.
are crafted to pay attention to a range of domain-
specific linguistic cues. Log-linear models can be so
crafted and have already achieved excellent perfor-
mance when trained on annotated data, where they
are known as ?maximum entropy? models (Ratna-
parkhi et al, 1994; Rosenfeld, 1994).
Our goal is to learn log-linear models from
unannotated data. Since the forward-backward
and inside-outside algorithms are instances of
Expectation-Maximization (EM) (Dempster et al,
1977), a natural approach is to construct EM algo-
rithms that handle log-linear models. Riezler (1999)
did so, then resorted to an approximation because
the true objective function was hard to normalize.
Stepping back from EM, we may generally en-
vision parameter estimation for probabilistic mod-
eling as pushing probability mass toward the train-
ing examples. We must consider not only where
the learner pushes the mass, but also from where the
mass is taken. In this paper, we describe an alterna-
tive to EM: contrastive estimation (CE), which (un-
like EM) explicitly states the source of the probabil-
ity mass that is to be given to an example.1
One reason is to make normalization efficient. In-
deed, CE generalizes EM and other practical tech-
niques used to train log-linear models, including
conditional estimation (for the supervised case) and
Riezler?s approximation (for the unsupervised case).
The other reason to use CE is to improve accu-
racy. CE offers an additional way to inject domain
knowledge into unsupervised learning (Smith and
Eisner, 2005). CE hypothesizes that each positive
example in training implies a domain-specific set
of examples which are (for the most part) degraded
(?2). This class of implicit negative evidence pro-
vides the source of probability mass for the observed
example. We discuss the application of CE to log-
linear models in ?3.
1Not to be confused with contrastive divergence minimiza-
tion (Hinton, 2003), a technique for training products of experts.
354
We are particularly interested in log-linear models
over sequences, like the conditional random fields
(CRFs) of Lafferty et al (2001) and weighted CFGs
(Miyao and Tsujii, 2002). For a given sequence, im-
plicit negative evidence can be represented as a lat-
tice derived by finite-state operations (?4). Effec-
tiveness of the approach on POS tagging using un-
labeled data is demonstrated (?5). We discuss future
work (?6) and conclude (?7).
2 Implicit Negative Evidence
Natural language is a delicate thing. For any plausi-
ble sentence, there are many slight perturbations of
it that will make it implausible. Consider, for ex-
ample, the first sentence of this section. Suppose
we choose one of its six words at random and re-
move it; on this example, odds are two to one that
the resulting sentence will be ungrammatical. Or,
we could randomly choose two adjacent words and
transpose them; none of the results are valid conver-
sational English. The learner we describe here takes
into account not only the observed positive exam-
ple, but also a set of similar but deprecated negative
examples.
2.1 Learning setting
Let ~x = ?x1, x2, ...?, be our observed example sen-
tences, where each xi ? X, and let y?i ? Y be the
unobserved correct hidden structure for xi (e.g., a
POS sequence). We seek a model, parameterized by
~?, such that the (unknown) correct analysis y?i is the
best analysis for xi (under the model). If y?i were ob-
served, a variety of training criteria would be avail-
able (see Tab. 1), but y?i is unknown, so none apply.
Typically one turns to the EM algorithm (Dempster
et al, 1977), which locally maximizes
?
i
p
(
X = xi | ~?
)
=
?
i
?
y?Y
p
(
X = xi, Y = y | ~?
)
(1)
where X is a random variable over sentences and
Y a random variable over analyses (notation is of-
ten abbreviated, eliminating the random variables).
An often-used alternative to EM is a class of so-
called Viterbi approximations, which iteratively find
the probabilistically-best y? and then, on each itera-
tion, solve a supervised problem (see Tab. 1).
joint likelihood (JL) ?
i
p
(
xi, y
?
i | ~?
)
conditional
likelihood (CL)
?
i
p
(
y?i | xi, ~?
)
classification
accuracy (Juang
and Katagiri, 1992)
?
i
?(y?i , y?(xi))
expected
classification
accuracy (Klein and
Manning, 2002)
?
i
p
(
y?i | xi, ~?
)
negated boosting
loss (Collins, 2000) ?
?
i
p
(
y?i | xi, ~?
)?1
margin (Crammer
and Singer, 2001) ? s.t. ?
~?? ? 1;?i,?y 6= y?i ,
~? ? (~f(xi, y?i ) ? ~f(xi, y)) ? ?
expected local
accuracy (Altun et
al., 2003)
?
i
?
j
p
(
`j(Y ) = `j(y
?
i ) | xi, ~?
)
Table 1: Various supervised training criteria. All functions are
written so as to be maximized. None of these criteria are avail-
able for unsupervised estimation because they all depend on the
correct label, y?.
2.2 A new approach: contrastive estimation
Our approach instead maximizes
?
i
p
(
Xi = xi | Xi ? N(xi), ~?
)
(2)
where the ?neighborhood? N(xi) ? X is a set of
implicit negative examples plus the example xi it-
self. As in EM, p(xi | ..., ~?) is found by marginal-
izing over hidden variables (Eq. 1). Note that the
x? ? N(xi) are not treated as hard negative exam-
ples; we merely seek to move probability mass from
them to the observed x.
The neighborhood of x, N(x), contains examples
that are perturbations of x. We refer to the mapping
N : X ? 2X as the neighborhood function, and the
optimization of Eq. 2 as contrastive estimation (CE).
CE seeks to move probability mass from the
neighborhood of an observed xi to xi itself. The
learner hypothesizes that good models are those
which discriminate an observed example from its
neighborhood. Put another way, the learner assumes
not only that xi is good, but that xi is locally op-
timal in example space (X), and that alternative,
similar examples (from the neighborhood) are infe-
rior. Rather than explain all of the data, the model
must only explain (using hidden variables) why the
355
observed sentence is better than its neighbors. Of
course, the validity of this hypothesis will depend
on the form of the neighborhood function.
Consider, as a concrete example, learning nat-
ural language syntax. In Smith and Eisner (2005),
we define a sentence?s neighborhood to be a set of
slightly-altered sentences that use the same lexemes,
as suggested at the start of this section. While their
syntax is degraded, the inferred meaning of any of
these altered sentences is typically close to the in-
tended meaning, yet the speaker chose x and not
one of the other x? ? N(x). Why? Deletions
are likely to violate subcategorization requirements,
and transpositions are likely to violate word order
requirements?both of which have something to do
with syntax. x was the most grammatical option that
conveyed the speaker?s meaning, hence (we hope)
roughly the most grammatical option in the neigh-
borhood N(x), and the syntactic model should make
it so.
3 Log-Linear Models
We have not yet specified the form of our probabilis-
tic model, only that it is parameterized by ~? ? Rn.
Log-linear models, which we will show are a natural
fit for CE, assign probability to an (example, label)
pair (x, y) according to
p
(
x, y | ~?
)
def
=
1
Z
(
~?
)u
(
x, y | ~?
)
(3)
where the ?unnormalized score? u(x, y | ~?) is
u
(
x, y | ~?
)
def
= exp
(
~? ? ~f(x, y)
)
(4)
The notation above is defined as follows. ~f : X ?
Y ? Rn?0 is a nonnegative vector feature function,
and ~? ? Rn are the corresponding feature weights
(the model?s parameters). Because the features can
take any form and need not be orthogonal, log-linear
models can capture arbitrary dependencies in the
data and cleanly incorporate them into a model.
Z(~?) (the partition function) is chosen so that
?
(x,y) p(x, y |
~?) = 1; i.e., Z(~?) =
?
(x,y) u(x, y |
~?). u is typically easy to compute for a given (x, y),
but Z may be much harder to compute. All the ob-
jective functions in this paper take the form
?
i
?
(x,y)?Ai
p
(
x, y | ~?
)
?
(x,y)?Bi
p
(
x, y | ~?
) (5)
likelihood criterion Ai Bi
joint {(xi, y?i )} X? Y
conditional {(xi, y?i )} {xi} ? Y
marginal (a la` EM) {xi} ? Y X? Y
contrastive {xi} ? Y N(xi) ? Y
Table 2: Supervised (upper box) and unsupervised (lower box)
estimation with log-linear models in terms of Eq. 5.
where Ai ? Bi (for each i). For log-linear models
this is simply
?
i
?
(x,y)?Ai
u
(
x, y | ~?
)
?
(x,y)?Bi
u
(
x, y | ~?
) (6)
So there is no need to compute Z(~?), but we do need
to compute sums over A and B. Tab. 2 summarizes
some concrete examples; see also ?3.1?3.2.
We would prefer to choose an objective function
such that these sums are easy. CE focuses on choos-
ing appropriate small contrast sets Bi, both for effi-
ciency and to guide the learner. The natural choice
for Ai (which is usually easier to sum over) is the set
of (x, y) that are consistent with what was observed
(partially or completely) about the ith training ex-
ample, i.e., the numerator
?
(x,y)?Ai p(x, y |
~?) is
designed to find p(observation i | ~?). The idea is to
focus the probability mass within Bi on the subset
Ai where the i the training example is known to be.
It is possible to build log-linear models where
each xi is a sequence.2 In this paper, each model
is a weighted finite-state automaton (WFSA) where
states correspond to POS tags. The parameter vector
~? ? Rn specifies a weight for each of the n transi-
tions in the automaton. y is a hidden path through
the automaton (determining a POS sequence), and x
is the string it emits. u(x, y | ~?) is defined by ap-
plying exp to the total weight of all transitions in y.
This is an example of Eqs. 4 and 6 where fj(x, y) is
the number of times the path y takes the jth transi-
tion.
The partition function Z(~?) of the WFSA is found
by adding up the u-scores of all paths through the
WFSA. For a k-state WFSA, this equates to solving
a linear system of k equations in k variables (Tarjan,
1981). But if the WFSA contains cycles this infi-
nite sum may diverge. Alternatives to exact com-
2These are exemplified by CRFs (Lafferty et al, 2001),
which can be viewed alternately as undirected dynamic graph-
ical models with a chain topology, as log-linear models over
entire sequences with local features, or as WFSAs. Because
?CRF? implies CL estimation, we use the term ?WFSA.?
356
putation, like random sampling (see, e.g., Abney,
1997), will not help to avoid this difficulty; in addi-
tion, convergence rates are in general unknown and
bounds difficult to prove. We would prefer to sum
over finitely many paths in Bi.
3.1 Parameter estimation (supervised)
For log-linear models, both CL and JL estimation
(Tab. 1) are available. In terms of Eq. 5, both
set Ai = {(xi, y?i )}. The difference is in B: for
JL, Bi = X ? Y, so summing over Bi is equiva-
lent to computing the partition function Z(~?). Be-
cause that sum is typically difficult, CL is preferred;
Bi = {xi} ? Y for xi, which is often tractable.
For sequence models like WFSAs it is computed us-
ing a dynamic programming algorithm (the forward
algorithm for WFSAs). Klein and Manning (2002)
argue for CL on grounds of accuracy, but see also
Johnson (2001). See Tab. 2; other contrast sets Bi
are also possible.
When Bi contains only xi paired with the current
best competitor (y?) to y?i , we have a technique that
resembles maximum margin training (Crammer and
Singer, 2001). Note that y? will then change across
training iterations, making Bi dynamic.
3.2 Parameter estimation (unsupervised)
The difference between supervised and unsuper-
vised learning is that in the latter case, Ai is forced
to sum over label sequences y because they weren?t
observed. In the unsupervised case, CE maximizes
LN
(
~?
)
= log
?
i
?
y?Y
u
(
xi, y | ~?
)
?
(x,y)?N(xi)?Y
u
(
x, y | ~?
) (7)
In terms of Eq. 5, A = {xi}?Y and B = N(xi)?Y.
EM?s objective function (Eq. 1) is a special case
where N(xi) = X, for all i, and the denomina-
tor becomes Z(~?). An alternative is to restrict the
neighborhood to the set of observed training exam-
ples rather than all possible examples (Riezler, 1999;
Johnson et al, 1999; Riezler et al, 2000):
?
i
[
u
(
xi | ~?
)
/
?
j
u
(
xj | ~?
)
]
(8)
Viewed as a CE method, this approach (though ef-
fective when there are few hypotheses) seems mis-
guided; the objective says to move mass to each ex-
ample at the expense of all other training examples.
Another variant is conditional EM. Let xi be a
pair (xi,1, xi,2) and define the neighborhood to be
N(xi) = {x? = (x?1, xi,2)}. This approach has
been applied to conditional densities (Jebara and
Pentland, 1998) and conditional training of acoustic
models with hidden variables (Valtchev et al, 1997).
Generally speaking, CE is equivalent to some
kind of EM when N(?) is an equivalence relation
on examples, so that the neighborhoods partition X.
Then if q is any fixed (untrained) distribution over
neighborhoods, CE equates to running EM on the
model defined by
p?
(
x, y | ~?
)
def
= q (N(x)) ? p
(
x, y | N(x), ~?
)
(9)
CE may also be viewed as an importance sam-
pling approximation to EM, where the sample space
X is replaced by N(xi). We will demonstrate ex-
perimentally that CE is not just an approximation to
EM; it makes sense from a modeling perspective.
In ?4, we will describe neighborhoods of se-
quences that can be represented as acyclic lattices
built directly from an observed sequence. The sum
over Bi is then the total u-score in our model of all
paths in the neighborhood lattice. To compute this,
intersect the WFSA and the lattice, obtaining a new
acyclic WFSA, and sum the u-scores of all its paths
(Eisner, 2002) using a simple dynamic programming
algorithm akin to the forward algorithm. The sum
over Ai may be computed similarly.
CE with lattice neighborhoods is not confined to
the WFSAs of this paper; when estimating weighted
CFGs, the key algorithm is the inside algorithm for
lattice parsing (Smith and Eisner, 2005).
3.3 Numerical optimization
To maximize the neighborhood likelihood (Eq. 7),
we apply a standard numerical optimization method
(L-BFGS) that iteratively climbs the function using
knowledge of its value and gradient (Liu and No-
cedal, 1989). The partial derivative of LN with re-
spect to the jth feature weight ?j is
?LN
??j
=
?
i
E~? [fj | xi] ?E~? [fj | N(xi)] (10)
This looks similar to the gradient of log-linear like-
lihood functions on complete data, though the ex-
pectation on the left is in those cases replaced by an
observed feature value fj(xi, y?i ). In this paper, the
357
natural language is a delicate thing
a. DEL1WORD:
natural language is a delicate thing
language is a delicate thing
is a
delicate
thing ?:?
? ?
b. TRANS1:
natural language a delicate thingis
delicate
is
is
a
natural
a
is a delicate thing
language
language
delicate
thing
: xx2 1x2x1 :
:x x2 3 :x x3 2
:x xm m?1 xm?1:xm
? ?
.
.
.
(Each bigram xi+1i in the sentence has an
arc pair (xi : xi+1, xi+1 : xi).)
c. DEL1SUBSEQ:
natural language is a delicate thing
language
is
is
a
a
a delicate thing
?:?
?:?
?:?
?
?
?
?
?
?
Figure 1: A sentence and three lattices representing some of its neighborhoods. The transducer used to generate each neighborhood
lattice (via composition with the sentence, followed by determinization and minimization) is shown to its right.
expectations in Eq. 10 are computed by the forward-
backward algorithm generalized to lattices.
We emphasize that the function LN is not glob-
ally concave; our search will lead only to a local op-
timum.3 Therefore, as with all unsupervised statisti-
cal learning, the bias in the initialization of ~? will af-
fect the quality of the estimate and the performance
of the method. In future we might wish to apply
techniques for avoiding local optima, such as deter-
ministic annealing (Smith and Eisner, 2004).
4 Lattice Neighborhoods
We next consider some non-classical neighborhood
functions for sequences. When X = ?+ for some
symbol alphabet ?, certain kinds of neighborhoods
have natural, compact representations. Given an in-
put string x = ?x1, x2, ..., xm?, we write xji for
the substring ?xi, xi+1, ..., xj? and xm1 for the whole
string. Consider first the neighborhood consisting of
all sequences generated by deleting a single symbol
from the m-length sequence xm1 :
DEL1WORD(xm1 ) =
{
x`?11 x
m
`+1 | 1 ? ` ? m
}
? {xm1 }
This set consists of m + 1 strings and can be com-
pactly represented as a lattice (see Fig. 1a). Another
3Without any hidden variables, LN is globally concave.
neighborhood involves transposing any pair of adja-
cent words:
TRANS1(xm1 ) =
{
x`?11 x`+1x`x
m
`+2 | 1 ? ` < m
}
? {xm1 }
This set can also be compactly represented as a lat-
tice (Fig. 1b). We can combine DEL1WORD and
TRANS1 by taking their union; this gives a larger
neighborhood, DELORTRANS1.4
The DEL1SUBSEQ neighborhood allows the dele-
tion of any contiguous subsequence of words that is
strictly smaller than the whole sequence. This lattice
is similar to that of DEL1WORD, but adds some arcs
(Fig. 1c); the size of this neighborhood is O(m2).
A final neighborhood we will consider is
LENGTH, which consists of ?m. CE with the
LENGTH neighborhood is very similar to EM; it is
equivalent to using EM to estimate the parameters
of a model defined by Eq. 9 where q is any fixed
(untrained) distribution over lengths.
When the vocabulary ? is the set of words in a
natural language, it is never fully known; approx-
imations for defining LENGTH = ?m include us-
ing observed ? from the training set (as we do) or
adding a special OOV symbol.
4In general, the lattices are obtained by composing the ob-
served sequence with a small FST and determinizing and mini-
mizing the result; the relevant transducers are shown in Fig. 1.
358
 30
 40
 50
 60
 70
 80
 90
 100
 0.1  1  10
% c
orre
ct ta
gs
smoothing parameter0 8
 
 
 
 
 
 
 
 
 
12K 24K 48K 96K
sel. oracle sel. oracle sel. oracle sel. oracle
CRF (supervised) 100.0 99.8 99.8 99.5
HMM (supervised) 99.3 98.5 97.9 97.2
LENGTH 74.9 77.4 78.7 81.5 78.3 81.3 78.9 79.3
DELORTR1 70.8 70.8 78.6 78.6 78.3 79.1 75.2 78.8
TRANS1 72.7 72.7 77.2 77.2 78.1 79.4 74.7 79.0
EM 49.5 52.9 55.5 58.0 59.4 60.9 60.9 62.1
DEL1WORD 55.4 55.6 58.6 60.3 59.9 60.2 59.9 60.4
DEL1SSQ 53.0 53.3 55.0 56.7 55.3 55.4 57.3 58.7
random expected 35.2 35.1 35.1 35.1
ambiguous words 6,244 12,923 25,879 51,521
Figure 2: Percent ambiguous words tagged correctly in the 96K dataset, as the smoothing parameter (? in the case of EM, ?2 in the
CE cases) varies. The model selected from each criterion using unlabeled development data is circled in the plot. Dataset size is
varied in the table at right, which shows models selected using unlabeled development data (?sel.?) and using an oracle (?oracle,?
the highest point on a curve). Across conditions, some neighborhood roughly splits the difference between supervised models and
EM.
5 Experiments
We compare CE (using neighborhoods from ?4)
with EM on POS tagging using unlabeled data.
5.1 Comparison with EM
Our experiments are inspired by those in
Merialdo (1994); we train a trigram tagger using
only unlabeled data, assuming complete knowledge
of the tagging dictionary.5 In our experiments,
we varied the amount of data available (12K?96K
words of WSJ), the heaviness of smoothing, and the
estimation criterion. In all cases, training stopped
when the relative change in the criterion fell below
10?4 between steps (typically ? 100 steps). For this
corpus and tag set, on average, a tagger must decide
between 2.3 tags for a given token.
The generative model trained by EM was identical
to Merialdo?s: a second-order HMM. We smoothed
using a flat Dirichlet prior with single parameter ?
for all distributions (?-values from 0 to 10 were
tested).6 The model was initialized uniformly.
The log-linear models trained by CE used the
same feature set, though the feature weights are no
longer log-probabilities and there are no sum-to-one
constraints. In addition to an unsmoothed trial, we
tried diagonal Gaussian priors (quadratic penalty)
with ?2 ranging from 0.1 to 10. The models were
initialized with all ?j = 0.
Unsupervised model selection. For each (crite-
5Without a tagging dictionary, tag names are interchange-
able and cannot be evaluated on gold-standard accuracy. We
address the tagging dictionary assumption in ?5.2.
6This is equivalent to add-? smoothing within every M step.
rion, dataset) pair, we selected the smoothing trial
that gave the highest estimation criterion score on a
5K-word development set (also unlabeled).
Results. The plot in Fig. 2 shows the Viterbi ac-
curacy of each criterion trained on the 96K-word
dataset as smoothing was varied; the table shows,
for each (criterion, dataset) pair the performance of
the selected ? or ?2 and the one chosen by an oracle.
LENGTH, TRANS1, and DELORTRANS1 are con-
sistently the best, far out-stripping EM. These gains
dwarf the performance of EM on over 1.1M words
(66.6% as reported by Smith and Eisner (2004)),
even when the latter uses improved search (70.0%).
DEL1WORD and DEL1SUBSEQ, on the other hand,
are poor, even worse than EM on larger datasets.
An important result is that neighborhoods do not
succeed by virtue of approximating log-linear EM;
if that were so, we would expect larger neighbor-
hoods (like DEL1SUBSEQ) to out-perform smaller
ones (like TRANS1)?this is not so. DEL1SUBSEQ
and DEL1WORD are poor because they do not give
helpful classes of negative evidence: deleting a word
or a short subsequence often does very little dam-
age. Put another way, models that do a good job of
explaining why no word or subsequence should be
deleted do not do so using the familiar POS cate-
gories.
The LENGTH neighborhood is as close to log-
linear EM as it is practical to get. The inconsis-
tencies in the LENGTH curve (Fig. 2) are notable
and also appeared at the other training set sizes.
Believing this might be indicative of brittleness in
Viterbi label selection, we computed the expected
359
DELORTRANS1 TRANS1 LENGTH EM
words in trigram trigram+ spelling trigram
trigram
+ spelling trigram
trigram
+ spelling trigram
tagging dict. sel. oracle sel. oracle sel. oracle sel. oracle sel. oracle sel. oracle sel. oracle ran
do
m
ex
pe
ct
ed
am
bi
gu
ou
s w
or
ds
av
e.
ta
gs
/to
ke
n
all train & dev. 78.3 90.1 80.9 91.1 90.4 90.4 88.7 90.9 87.8 90.4 87.1 91.9 78.0 84.4 69.5 13,150 2.3
1st 500 sents. 72.3 84.8 80.2 90.8 80.8 82.9 88.1 90.1 68.1 78.3 76.9 83.2 77.2 80.5 60.5 13,841 3.7
count ? 2 69.5 81.3 79.5 90.3 77.0 78.6 78.7 90.1 65.3 75.2 73.3 73.8 70.1 70.9 56.6 14,780 4.4
count ? 3 65.0 77.2 78.3 89.8 71.7 73.4 78.4 89.5 62.8 72.3 73.2 73.6 66.5 66.5 51.0 15,996 5.5
Table 3: Percent of all words correctly tagged in the 24K dataset, as the tagging dictionary is diluted. Unsupervised model selection
(?sel.?) and oracle model selection (?oracle?) across smoothing parameters are shown. Note that we evaluated on all words (unlike
Fig. 3) and used 17 coarse tags, giving higher scores than in Fig. 2.
accuracy of the LENGTH models; the same ?dips?
were present. This could indicate that the learner
was trapped in a local maximum, suggesting that,
since other criteria did not exhibit this behavior,
LENGTH might be a bumpier objective surface. It
would be interesting to measure the bumpiness (sen-
sitivity to initial conditions) of different contrastive
objectives.7
5.2 Removing knowledge, adding features
The assumption that the tagging dictionary is com-
pletely known is difficult to justify. While a POS
lexicon might be available for a new language, cer-
tainly it will not give exhaustive information about
all word types in a corpus. We experimented with
removing knowledge from the tagging dictionary,
thereby increasing the difficulty of the task, to see
how well various objective functions could recover.
One means to recovery is the addition of features to
the model?this is easy with log-linear models but
not with classical generative models.
We compared the performance of the best
neighborhoods (LENGTH, DELORTRANS1, and
TRANS1) from the first experiment, plus EM, us-
ing three diluted dictionaries and the original one,
on the 24K dataset. A diluted dictionary adds (tag,
word) entries so that rare words are allowed with
any tag, simulating zero prior knowledge about the
word. ?Rare? might be defined in different ways;
we used three definitions: words unseen in the first
500 sentences (about half of the 24K training cor-
pus); singletons (words with count ? 1); and words
with count ? 2. To allow more trials, we projected
the original 45 tags onto a coarser set of 17 (e.g.,
7A reviewer suggested including a table comparing different
criterion values for each learned model (i.e., each neighborhood
evaluated on each other neighborhood). This table contained no
big surprises; we note only that most models were the best on
their own criterion, and among unsupervised models, LENGTH
performed best on the CL criterion.
RB? ?ADV).
To take better advantage of the power of log-
linear models?specifically, their ability to incorpo-
rate novel features?we also ran trials augmenting
the model with spelling features, allowing exploita-
tion of correlations between parts of the word and a
possible tag. Our spelling features included all ob-
served 1-, 2-, and 3-character suffixes, initial capital-
ization, containing a hyphen, and containing a digit.
Results. Fig. 3 plots tagging accuracy (on am-
biguous words) for each dictionary on the 24K
dataset. The x-axis is the smoothing parameter (?
for EM, ?2 for CE). Note that the different plots are
not comparable, because their y-axes are based on
different sets of ambiguous words.
So that models under different dilution conditions
could be compared, we computed accuracy on all
words; these are shown in Tab. 3. The reader will
notice that there is often a large gap between unsu-
pervised and oracle model selection; this draws at-
tention to a need for better unsupervised regulariza-
tion and model selection techniques.
Without spelling features, all models perform
worse as knowledge is removed. But LENGTH suf-
fers most substantially, relative to its initial perfor-
mance. Why is this? LENGTH (like EM) requires
the model to explain why a given sentence was seen
instead of some other sentence of the same length.
One way to make this explanation is to manipulate
emission weights (i.e., for (tag, word) features): the
learner can construct a good class-based unigram
model of the text (where classes are tags). This is
good for the LENGTH objective, but not for learning
good POS tag sequences.
In contrast, DELORTRANS1 and TRANS1 do not
allow the learner to manipulate emission weights for
words not in the sentence. The sentence?s good-
ness must be explained in a way other than by the
words it contains: namely through the POS tags. To
360
check this intuition, we built local normalized mod-
els p(word | tag) from the parameters learned by
TRANS1 and LENGTH. For each tag, these were
compared by KL divergence to the empirical lexical
distributions (from labeled data). For the ten tags
accounting for 95.6% of the data, LENGTH more
closely matched the empirical lexical distributions.
LENGTH is learning a correct distribution, but that
distribution is not helpful for the task.
The improvement from adding spelling features
is striking: DELORTRANS1 and TRANS1 recover
nearly completely (modulo the model selection
problem) from the diluted dictionaries. LENGTH
sees far less recovery. Hence even our improved fea-
ture sets cannot compensate for the choice of neigh-
borhood. This highlights our argument that a neigh-
borhood is not an approximation to log-linear EM;
LENGTH tries very hard to approximate log-linear
EM but requires a good dictionary to be on par with
the other criteria. Good neighborhoods, rather, per-
form well in their own right.
6 Future Work
Foremost for future work is the ?minimally super-
vised? paradigm in which a small amount of la-
beled data is available (see, e.g., Clark et al (2003)).
Unlike well-known ?bootstrapping? approaches
(Yarowsky, 1995), EM and CE have the possible ad-
vantage of maintaining posteriors over hidden labels
(or structure) throughout learning; bootstrapping ei-
ther chooses, for each example, a single label, or
remains completely agnostic. One can envision a
mixed objective function that tries to fit the labeled
examples while discriminating unlabeled examples
from their neighborhoods.8
Regardless of how much (if any) data are labeled,
the question of good smoothing techniques requires
more attention. Here we used a single zero-mean,
constant-variance Gaussian prior for all parameters.
Better performance might be achieved by allowing
different variances for different feature types. This
8Zhu and Ghahramani (2002) explored the semi-supervised
classification problem for spatially-distributed data, where
some data are labeled, using a Boltzmann machine to model
the dataset. For them, the Markov random field is over label-
ing configurations for all examples, not, as in our case, com-
plex structured labels for a particular example. Hence their B
(Eq. 5), though very large, was finite and could be sampled.
All train & development words are in the tagging dictionary:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
Tagging dictionary taken from the first 500 sentences:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
Tagging dictionary contains words with count ? 2:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
Tagging dictionary contains words with count ? 3:
 40
 45
 50 55
 60
 65
 70
 75
 80
 85
 40 45
 50 55
 60 65
 70 5
80 85
 0.1  1  10smoothing parameter0 8
50
DELORTRANS1  
TRANS1  
LENGTH 4 5
EM trig
ram
m
od
el
? trig
ram
+
spe
llin
g
Figure 3: Percent ambiguous words tagged correctly (with
coarse tags) on the 24K dataset, as the dictionary is diluted and
with spelling features. Each graph corresponds to a different
level of dilution. Models selected using unlabeled development
data are circled. These plots (unlike Tab. 3) are not compara-
ble to each other because each is measured on a different set of
ambiguous words.
361
leads to a need for more efficient tuning of the prior
parameters on development data.
The effectiveness of CE (and different neighbor-
hoods) for dependency grammar induction is ex-
plored in Smith and Eisner (2005) with considerable
success. We introduce there the notion of design-
ing neighborhoods to guide learning for particular
tasks. Instead of guiding an unsupervised learner to
match linguists? annotations, the choice of neighbor-
hood might be made to direct the learner toward hid-
den structure that is helpful for error-correction tasks
like spelling correction and punctuation restoration
that may benefit from a grammatical model.
Wang et al (2002) discuss the latent maximum
entropy principle. They advocate running EM many
times and selecting the local maximum that maxi-
mizes entropy. One might do the same for the local
maxima of any CE objective, though theoretical and
experimental support for this idea remain for future
work.
7 Conclusion
We have presented contrastive estimation, a new
probabilistic estimation criterion that forces a model
to explain why the given training data were better
than bad data implied by the positive examples. We
have shown that for unsupervised sequence model-
ing, this technique is efficient and drastically out-
performs EM; for POS tagging, the gain in accu-
racy over EM is twice what we would get from ten
times as much data and improved search, sticking
with EM?s criterion (Smith and Eisner, 2004). On
this task, with certain neighborhoods, contrastive
estimation suffers less than EM does from dimin-
ished prior knowledge and is able to exploit new
features?that EM can?t?to largely recover from
the loss of knowledge.
References
S. P. Abney. 1997. Stochastic attribute-value grammars. Com-
putational Linguistics, 23(4):597?617.
Y. Altun, M. Johnson, and T. Hofmann. 2003. Investigating
loss functions and optimization methods for discriminative
learning of label sequences. In Proc. of EMNLP.
E. Charniak. 1993. Statistical Language Learning. MIT Press.
S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrapping
POS taggers using unlabelled data. In Proc. of CoNLL.
M. Collins. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. of ICML.
K. Crammer and Y. Singer. 2001. On the algorithmic imple-
mentation of multiclass kernel-based vector machines. Jour-
nal of Machine Learning Research, 2(5):265?92.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likeli-
hood estimation from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1?38.
J. Eisner. 2002. Parameter estimation for probabilistic finite-
state transducers. In Proc. of ACL.
G. E. Hinton. 2003. Training products of experts by mini-
mizing contrastive divergence. Technical Report GCNU TR
2000-004, University College London.
T. Jebara and A. Pentland. 1998. Maximum conditional like-
lihood via bound maximization and the CEM algorithm. In
Proc. of NIPS.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999.
Estimators for stochastic ?unification-based? grammars. In
Proc. of ACL.
M. Johnson. 2001. Joint and conditional estimation of tagging
and parsing models. In Proc. of ACL.
B.-H. Juang and S. Katagiri. 1992. Discriminative learning for
minimum error classification. IEEE Trans. Signal Process-
ing, 40:3043?54.
D. Klein and C. D. Manning. 2002. Conditional structure vs.
conditional estimation in NLP models. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
D. C. Liu and J. Nocedal. 1989. On the limited memory method
for large scale optimization. Mathematical Programming B,
45(3):503?28.
A. McCallum and W. Li. 2003. Early results for named-
entity extraction with conditional random fields. In Proc.
of CoNLL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?72.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estimation for
feature forests. In Proc. of HLT.
A. Ratnaparkhi, S. Roukos, and R. T. Ward. 1994. A maximum
entropy model for parsing. In Proc. of ICSLP.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000. Lex-
icalized stochastic modeling of constraint-based grammars
using log-linear measures and EM training. In Proc. of ACL.
S. Riezler. 1999. Probabilistic Constraint Logic Programming.
Ph.D. thesis, Universita?t Tu?bingen.
R. Rosenfeld. 1994. Adaptive Statistical Language Modeling:
A Maximum Entropy Approach. Ph.D. thesis, CMU.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of HLT-NAACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Guiding unsupervised gram-
mar induction using contrastive estimation. In Proc. of IJ-
CAI Workshop on Grammatical Inference Applications.
R. E. Tarjan. 1981. A unified approach to path problems. Jour-
nal of the ACM, 28(3):577?93.
V. Valtchev, J. J. Odell, P. C. Woodland, and S. J. Young. 1997.
MMIE training of large vocabulary speech recognition sys-
tems. Speech Communication, 22(4):303?14.
S. Wang, R. Rosenfeld, Y. Zhao, and D. Schuurmans. 2002.
The latent maximum entropy principle. In Proc. of ISIT.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of ACL.
X. Zhu and Z. Ghahramani. 2002. Towards semi-supervised
classification with Markov random fields. Technical Report
CMU-CALD-02-106, Carnegie Mellon University.
362
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 569?576,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Annealing Structural Bias in Multilingual Weighted Grammar Induction?
Noah A. Smith and Jason Eisner
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{nasmith,jason}@cs.jhu.edu
Abstract
We first show how a structural locality bias can improve the
accuracy of state-of-the-art dependency grammar induction
models trained by EM from unannotated examples (Klein
and Manning, 2004). Next, by annealing the free parame-
ter that controls this bias, we achieve further improvements.
We then describe an alternative kind of structural bias, to-
ward ?broken? hypotheses consisting of partial structures
over segmented sentences, and show a similar pattern of im-
provement. We relate this approach to contrastive estimation
(Smith and Eisner, 2005a), apply the latter to grammar in-
duction in six languages, and show that our new approach
improves accuracy by 1?17% (absolute) over CE (and 8?30%
over EM), achieving to our knowledge the best results on this
task to date. Our method, structural annealing, is a gen-
eral technique with broad applicability to hidden-structure
discovery problems.
1 Introduction
Inducing a weighted context-free grammar from
flat text is a hard problem. A common start-
ing point for weighted grammar induction is
the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977; Baker, 1979). EM?s
mediocre performance (Table 1) reflects two prob-
lems. First, it seeks to maximize likelihood, but a
grammar that makes the training data likely does
not necessarily assign a linguistically defensible
syntactic structure. Second, the likelihood surface
is not globally concave, and learners such as the
EM algorithm can get trapped on local maxima
(Charniak, 1993).
We seek here to capitalize on the intuition that,
at least early in learning, the learner should search
primarily for string-local structure, because most
structure is local.1 By penalizing dependencies be-
tween two words that are farther apart in the string,
we obtain consistent improvements in accuracy of
the learned model (?3).
We then explore how gradually changing ? over
time affects learning (?4): we start out with a
?This work was supported by a Fannie and John Hertz
Foundation fellowship to the first author and NSF ITR grant
IIS-0313193 to the second author. The views expressed are
not necessarily endorsed by the sponsors. We thank three
anonymous COLING-ACL reviewers for comments.
1To be concrete, in the corpora tested here, 95% of de-
pendency links cover ? 4 words (English, Bulgarian, Por-
tuguese), ? 5 words (German, Turkish), ? 6 words (Man-
darin).
model selection among values of ? and ?(0)
worst unsup. sup. oracle
German 19.8 19.8 54.4 54.4
English 21.8 41.6 41.6 42.0
Bulgarian 24.7 44.6 45.6 45.6
Mandarin 31.8 37.2 50.0 50.0
Turkish 32.1 41.2 48.0 51.4
Portuguese 35.4 37.4 42.3 43.0
Table 1: Baseline performance of EM-trained dependency
parsing models: F1 on non-$ attachments in test data, with
various model selection conditions (3 initializers? 6 smooth-
ing values). The languages are listed in decreasing order by
the training set size. Experimental details can be found in the
appendix.
strong preference for short dependencies, then re-
lax the preference. The new approach, structural
annealing, often gives superior performance.
An alternative structural bias is explored in ?5.
This approach views a sentence as a sequence
of one or more yields of separate, independent
trees. The points of segmentation are a hidden
variable, and during learning all possible segmen-
tations are entertained probabilistically. This al-
lows the learner to accept hypotheses that explain
the sentences as independent pieces.
In ?6 we briefly review contrastive estimation
(Smith and Eisner, 2005a), relating it to the new
method, and show its performance alone and when
augmented with structural bias.
2 Task and Model
In this paper we use a simple unlexicalized depen-
dency model due to Klein and Manning (2004).
The model is a probabilistic head automaton gram-
mar (Alshawi, 1996) with a ?split? form that ren-
ders it parseable in cubic time (Eisner, 1997).
Let x = ?x1, x2, ..., xn? be the sentence. x0 is a
special ?wall? symbol, $, on the left of every sen-
tence. A tree y is defined by a pair of functions
yleft and yright (both {0, 1, 2, ..., n} ? 2{1,2,...,n})
that map each word to its sets of left and right de-
pendents, respectively. The graph is constrained
to be a projective tree rooted at $: each word ex-
cept $ has a single parent, and there are no cycles
569
or crossing dependencies.2 yleft(0) is taken to be
empty, and yright(0) contains the sentence?s single
head. Let yi denote the subtree rooted at position
i. The probability P (yi | xi) of generating this
subtree, given its head word xi, is defined recur-
sively:
?
D?{left ,right}
pstop(stop | xi,D , [yD(i) = ?]) (1)
?
?
j?yD (i)
pstop(?stop | xi,D ,firsty(j))
?pchild(xj | xi,D)? P (yj | xj)
where firsty(j) is a predicate defined to be true iff
xj is the closest child (on either side) to its parent
xi. The probability of the entire tree is given by
p?(x,y) = P (y0 | $). The parameters ? are the
conditional distributions pstop and pchild.
Experimental baseline: EM. Following com-
mon practice, we always replace words by part-of-
speech (POS) tags before training or testing. We
used the EM algorithm to train this model on POS
sequences in six languages. Complete experimen-
tal details are given in the appendix. Performance
with unsupervised and supervised model selec-
tion across different ? values in add-? smoothing
and three initializers ?(0) is reported in Table 1.
The supervised-selected model is in the 40?55%
F1-accuracy range on directed dependency attach-
ments. (Here F1 ? precision ? recall; see ap-
pendix.) Supervised model selection, which uses
a small annotated development set, performs al-
most as well as the oracle, but unsupervised model
selection, which selects the model that maximizes
likelihood on an unannotated development set, is
often much worse.
3 Locality Bias among Trees
Hidden-variable estimation algorithms?
including EM?typically work by iteratively
manipulating the model parameters ? to improve
an objective function F (?). EM explicitly
alternates between the computation of a posterior
distribution over hypotheses, p?(y | x) (where
y is any tree with yield x), and computing a new
parameter estimate ?.3
2A projective parser could achieve perfect accuracy on our
English and Mandarin datasets, > 99% on Bulgarian, Turk-
ish, and Portuguese, and > 98% on German.
3For weighted grammar-based models, the posterior does
not need to be explicitly represented; instead expectations un-
der p? are used to compute updates to ?.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-1 -0.8 -0.6 -0.4 -0.2  0  0.2
?
F (E
M b
ase
line
)
German
English
Bulgarian
Mandarin
Turkish
Portuguese
Figure 1: Test-set F1 performance of models trained by EM
with a locality bias at varying ?. Each curve corresponds
to a different language and shows performance of supervised
model selection within a given ?, across ? and ?(0) values.
(See Table 3 for performance of models selected across ?s.)
We decode with ? = 0, though we found that keeping the
training-time value of ? would have had almost no effect. The
EM baseline corresponds to ? = 0.
One way to bias a learner toward local expla-
nations is to penalize longer attachments. This
was done for supervised parsing in different ways
by Collins (1997), Klein and Manning (2003),
and McDonald et al (2005), all of whom con-
sidered intervening material or coarse distance
classes when predicting children in a tree. Eis-
ner and Smith (2005) achieved speed and accuracy
improvements by modeling distance directly in a
ML-estimated (deficient) generative model.
Here we use string distance to measure the
length of a dependency link and consider the inclu-
sion of a sum-of-lengths feature in the probabilis-
tic model, for learning only. Keeping our original
model, we will simply multiply into the probabil-
ity of each tree another factor that penalizes long
dependencies, giving:
p??(x,y) ? p?(x,y)?e
?
?
?
??
n?
i=1
?
j?y(i)
|i? j|
?
?
?
?
(2)
where y(i) = yleft(i) ? yright(i). Note that if
? = 0, we have the original model. As ? ? ??,
the new model p?? will favor parses with shorter
dependencies. The dynamic programming algo-
rithms remain the same as before, with the appro-
priate e?|i?j| factor multiplied in at each attach-
ment between xi and xj . Note that when ? = 0,
p?? ? p?.
Experiment. We applied a locality bias to the
same dependency model by setting ? to different
570
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-1 -0.5  0  0.5  1  1.5
?
F
German
Bulgarian
Turkish
Figure 2: Test-set F1 performance of models trained by EM
with structural annealing on the distance weight ?. Here
we show performance with add-10 smoothing, the all-zero
initializer, for three languages with three different initial val-
ues ?0. Time progresses from left to right. Note that it is
generally best to start at ?0  0; note also the importance of
picking the right point on the curve to stop. See Table 3 for
performance of models selected across smoothing, initializa-
tion, starting, and stopping choices, in all six languages.
values in [?1, 0.2] (see Eq. 2). The same initial-
izers ?(0) and smoothing conditions were tested.
Performance of supervised model selection among
models trained at different ? values is plotted in
Fig. 1. When a model is selected across all condi-
tions (3 initializers ? 6 smoothing values ? 7 ?s)
using annotated development data, performance is
notably better than the EM baseline using the same
selection procedure (see Table 3, second column).
4 Structural Annealing
The central idea of this paper is to gradually
change (anneal) the bias ?. Early in learning, local
dependencies are emphasized by setting ?  0.
Then ? is iteratively increased and training re-
peated, using the last learned model to initialize.
This idea bears a strong similarity to determin-
istic annealing (DA), a technique used in clus-
tering and classification to smooth out objective
functions that are piecewise constant (hence dis-
continuous) or bumpy (non-concave) (Rose, 1998;
Ueda and Nakano, 1998). In unsupervised learn-
ing, DA iteratively re-estimates parameters like
EM, but begins by requiring that the entropy of
the posterior p?(y | x) be maximal, then gradu-
ally relaxes this entropy constraint. Since entropy
is concave in ?, the initial task is easy (maximize
a concave, continuous function). At each step the
optimization task becomes more difficult, but the
initializer is given by the previous step and, in
practice, tends to be close to a good local max-
imum of the more difficult objective. By the last
iteration the objective is the same as in EM, but the
annealed search process has acted like a good ini-
tializer. This method was applied with some suc-
cess to grammar induction models by Smith and
Eisner (2004).
In this work, instead of imposing constraints on
the entropy of the model, we manipulate bias to-
ward local hypotheses. As ? increases, we penal-
ize long dependencies less. We call this structural
annealing, since we are varying the strength of a
soft constraint (bias) on structural hypotheses. In
structural annealing, the final objective would be
the same as EM if our final ?, ?f = 0, but we
found that annealing farther (?f > 0) works much
better.4
Experiment: Annealing ?. We experimented
with annealing schedules for ?. We initialized at
?0 ? {?1,?0.4,?0.2}, and increased ? by 0.1 (in
the first case) or 0.05 (in the others) up to ?f = 3.
Models were trained to convergence at each ?-
epoch. Model selection was applied over the same
initialization and regularization conditions as be-
fore, ?0, and also over the choice of ?f , with stop-
ping allowed at any stage along the ? trajectory.
Trajectories for three languages with three dif-
ferent ?0 values are plotted in Fig. 2. Generally
speaking, ?0  0 performs better. There is con-
sistently an early increase in performance as ? in-
creases, but the stopping ?f matters tremendously.
Selected annealed-? models surpass EM in all six
languages; see the third column of Table 3. Note
that structural annealing does not always outper-
form fixed-? training (English and Portuguese).
This is because we only tested a few values of ?0,
since annealing requires longer runtime.
5 Structural Bias via Segmentation
A related way to focus on local structure early
in learning is to broaden the set of hypothe-
ses to include partial parse structures. If x =
?x1, x2, ..., xn?, the standard approach assumes
that x corresponds to the vertices of a single de-
pendency tree. Instead, we entertain every hypoth-
esis in which x is a sequence of yields from sepa-
rate, independently-generated trees. For example,
?x1, x2, x3? is the yield of one tree, ?x4, x5? is the
4The reader may note that ?f > 0 actually corresponds to
a bias toward longer attachments. A more apt description in
the context of annealing is to say that during early stages the
learner starts liking local attachments too much, and we need
to exaggerate ? to ?coax? it to new hypotheses. See Fig. 2.
571
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-1.5-1-0.5 0 0.5
?
F
German
Bulgarian
Turkish
Figure 3: Test-set F1 performance of models trained by EM
with structural annealing on the breakage weight ?. Here
we show performance with add-10 smoothing, the all-zero
initializer, for three languages with three different initial val-
ues ?0. Time progresses from left (large ?) to right. See Ta-
ble 3 for performance of models selected across smoothing,
initialization, and stopping choices, in all six languages.
yield of a second, and ?x6, ..., xn? is the yield of a
third. One extreme hypothesis is that x is n single-
node trees. At the other end of the spectrum is the
original set of hypotheses?full trees on x. Each
has a nonzero probability.
Segmented analyses are intermediate represen-
tations that may be helpful for a learner to use
to formulate notions of probable local structure,
without committing to full trees.5 We only allow
unobserved breaks, never positing a hard segmen-
tation of the training sentences. Over time, we in-
crease the bias against broken structures, forcing
the learner to commit most of its probability mass
to full trees.
5.1 Vine Parsing
At first glance broadening the hypothesis space
to entertain all 2n?1 possible segmentations may
seem expensive. In fact the dynamic program-
ming computation is almost the same as sum-
ming or maximizing over connected dependency
trees. For the latter, we use an inside-outside al-
gorithm that computes a score for every parse tree
by computing the scores of items, or partial struc-
tures, through a bottom-up process. Smaller items
are built first, then assembled using a set of rules
defining how larger items can be built.6
Now note that any sequence of partial trees
over x can be constructed by combining the same
items into trees. The only difference is that we
5See also work on partial parsing as a task in its own right:
Hindle (1990) inter alia.
6See Eisner and Satta (1999) for the relevant algorithm
used in the experiments.
are willing to consider unassembled sequences of
these partial trees as hypotheses, in addition to
the fully connected trees. One way to accom-
plish this in terms of yright(0) is to say that the
root, $, is allowed to have multiple children, in-
stead of just one. Here, these children are inde-
pendent of each other (e.g., generated by a uni-
gram Markov model). In supervised dependency
parsing, Eisner and Smith (2005) showed that im-
posing a hard constraint on the whole structure?
specifically that each non-$ dependency arc cross
fewer than k words?can give guaranteed O(nk2)
runtime with little to no loss in accuracy (for sim-
ple models). This constraint could lead to highly
contrived parse trees, or none at all, for some
sentences?both are avoided by the allowance of
segmentation into a sequence of trees (each at-
tached to $). The construction of the ?vine? (se-
quence of $?s children) takes only O(n) time once
the chart has been assembled.
Our broadened hypothesis model is a proba-
bilistic vine grammar with a unigram model over
$?s children. We allow (but do not require) seg-
mentation of sentences, where each independent
child of $ is the root of one of the segments. We do
not impose any constraints on dependency length.
5.2 Modeling Segmentation
Now the total probability of an n-length sentence
x, marginalizing over its hidden structures, sums
up not only over trees, but over segmentations of
x. For completeness, we must include a proba-
bility model over the number of trees generated,
which could be anywhere from 1 to n. The model
over the number T of trees given a sentence of
length n will take the following log-linear form:
P (T = t | n) = et?
/
n?
i=1
ei?
where ? ? R is the sole parameter. When ? = 0,
every value of T is equally likely. For ?  0, the
model prefers larger structures with few breaks.
At the limit (? ? ??), we achieve the standard
learning setting, where the model must explain x
using a single tree. We start however at ?  0,
where the model prefers smaller trees with more
breaks, in the limit preferring each word in x to be
its own tree. We could describe ?brokenness? as a
feature in the model whose weight, ?, is chosen
extrinsically (and time-dependently), rather than
empirically?just as was done with ?.
572
model selection among values of ?2 and ?(0)
worst unsup. sup. oracle
DORT1 32.5 59.3 63.4 63.4
Ge
r.
LENGTH 30.5 56.4 57.3 57.8
DORT1 20.9 56.6 57.4 57.4
En
g.
LENGTH 29.1 37.2 46.2 46.2
DORT1 19.4 26.0 40.5 43.1
Bu
l.
LENGTH 25.1 35.3 38.3 38.3
DORT1 9.4 24.2 41.1 41.1
Ma
n.
LENGTH 13.7 17.9 26.2 26.2
DORT1 7.3 38.6 58.2 58.2
Tu
r.
LENGTH 21.5 34.1 55.5 55.5
DORT1 35.0 59.8 71.8 71.8
Po
r.
LENGTH 30.8 33.6 33.6 33.6
Table 2: Performance of CE on test data, for different neigh-
borhoods and with different levels of regularization. Bold-
face marks scores better than EM-trained models selected the
same way (Table 1). The score is the F1 measure on non-$
attachments.
Annealing ? resembles the popular bootstrap-
ping technique (Yarowsky, 1995), which starts out
aiming for high precision, and gradually improves
coverage over time. With strong bias (?  0), we
seek a model that maintains high dependency pre-
cision on (non-$) attachments by attaching most
tags to $. Over time, as this is iteratively weak-
ened (? ? ??), we hope to improve coverage
(dependency recall). Bootstrapping was applied
to syntax learning by Steedman et al (2003). Our
approach differs in being able to remain partly ag-
nostic about each tag?s true parent (e.g., by giving
50% probability to attaching to $), whereas Steed-
man et al make a hard decision to retrain on a
whole sentence fully or leave it out fully. In ear-
lier work, Brill and Marcus (1992) adopted a ?lo-
cal first? iterative merge strategy for discovering
phrase structure.
Experiment: Annealing ?. We experimented
with different annealing schedules for ?. The ini-
tial value of ?, ?0, was one of {?12 , 0,
1
2}. After
EM training, ? was diminished by 110 ; this was re-
peated down to a value of ?f = ?3. Performance
after training at each ? value is shown in Fig. 3.7
We see that, typically, there is a sharp increase
in performance somewhere during training, which
typically lessens as ? ? ??. Starting ? too high
can also damage performance. This method, then,
7Performance measures are given using a full parser that
finds the single best parse of the sentence with the learned
parsing parameters. Had we decoded with a vine parser, we
would see a precision?, recall? curve as ? decreased.
is not robust to the choice of ?, ?0, or ?f , nor does
it always do as well as annealing ?, although con-
siderable gains are possible; see the fifth column
of Table 3.
By testing models trained with a fixed value of ?
(for values in [?1, 1]), we ascertained that the per-
formance improvement is due largely to annealing,
not just the injection of segmentation bias (fourth
vs. fifth column of Table 3).8
6 Comparison and Combination with
Contrastive Estimation
Contrastive estimation (CE) was recently intro-
duced (Smith and Eisner, 2005a) as a class of alter-
natives to the likelihood objective function locally
maximized by EM. CE was found to outperform
EM on the task of focus in this paper, when ap-
plied to English data (Smith and Eisner, 2005b).
Here we review the method briefly, show how it
performs across languages, and demonstrate that
it can be combined effectively with structural bias.
Contrastive training defines for each example xi
a class of presumably poor, but similar, instances
called the ?neighborhood,? N(xi), and seeks to
maximize
CN(?) =
?
i
log p?(xi | N(xi))
=
?
i
log
?
y p?(xi,y)
?
x??N(xi)
?
y p?(x
?,y)
At this point we switch to a log-linear (rather
than stochastic) parameterization of the same
weighted grammar, for ease of numerical opti-
mization. All this means is that ? (specifically,
pstop and pchild in Eq. 1) is now a set of nonnega-
tive weights rather than probabilities.
Neighborhoods that can be expressed as finite-
state lattices built from xi were shown to give sig-
nificant improvements in dependency parser qual-
ity over EM. Performance of CE using two of
those neighborhoods on the current model and
datasets is shown in Table 2.9 0-mean diagonal
Gaussian smoothing was applied, with different
variances, and model selection was applied over
smoothing conditions and the same initializers as
8In principle, segmentation can be combined with the lo-
cality bias in ?3 (?). In practice, we found that this usually
under-performed the EM baseline.
9We experimented with DELETE1, TRANSPOSE1, DELE-
TEORTRANSPOSE1, and LENGTH. To conserve space we
show only the latter two, which tend to perform best.
573
EM fixed ? annealed ? fixed ? annealed ? CE fixed ? + CE
? ?0 ? ?f ? ?0 ? ?f N N, ?
German 54.4 61.3 0.2 70.0 -0.4 ? 0.4 66.2 0.4 68.9 0.5 ? -2.4 63.4 DORT1 63.8 DORT1, -0.2
English 41.6 61.8 -0.6 53.8 -0.4 ? 0.3 55.6 0.2 58.4 0.5 ? 0.0 57.4 DORT1 63.5 DORT1, -0.4
Bulgarian 45.6 49.2 -0.2 58.3 -0.4 ? 0.2 47.3 -0.2 56.5 0 ? -1.7 40.5 DORT1 ?
Mandarin 50.0 51.1 -0.4 58.0 -1.0 ? 0.2 38.0 0.2 57.2 0.5 ? -1.4 43.4 DEL1 ?
Turkish 48.0 62.3 -0.2 62.4 -0.2 ? -0.15 53.6 -0.2 59.4 0.5 ? -0.7 58.2 DORT1 61.8 DORT1, -0.6
Portuguese 42.3 50.4 -0.4 50.2 -0.4 ? -0.1 51.5 0.2 62.7 0.5 ? -0.5 71.8 DORT1 72.6 DORT1, -0.2
Table 3: Summary comparing models trained in a variety of ways with some relevant hyperparameters. Supervised model
selection was applied in all cases, including EM (see the appendix). Boldface marks the best performance overall and trials
that this performance did not significantly surpass under a sign test (i.e., p 6< 0.05). The score is the F1 measure on non-$
attachments. The fixed ? + CE condition was tested only for languages where CE improved over EM.
before. Four of the languages have at least one ef-
fective CE condition, supporting our previous En-
glish results (Smith and Eisner, 2005b), but CE
was harmful for Bulgarian and Mandarin. Perhaps
better neighborhoods exist for these languages, or
there is some ideal neighborhood that would per-
form well for all languages.
Our approach of allowing broken trees (?5) is
a natural extension of the CE framework. Con-
trastive estimation views learning as a process of
moving posterior probability mass from (implicit)
negative examples to (explicit) positive examples.
The positive evidence, as in MLE, is taken to be
the observed data. As originally proposed, CE al-
lowed a redefinition of the implicit negative ev-
idence from ?all other sentences? (as in MLE)
to ?sentences like xi, but perturbed.? Allowing
segmentation of the training sentences redefines
the positive and negative evidence. Rather than
moving probability mass only to full analyses of
the training example xi, we also allow probability
mass to go to partial analyses of xi.
By injecting a bias (? 6= 0 or ? > ??) among
tree hypotheses, however, we have gone beyond
the CE framework. We have added features to
the tree model (dependency length-sum, number
of breaks), whose weights we extrinsically manip-
ulate over time to impose locality bias CN and im-
prove search on CN. Another idea, not explored
here, is to change the contents of the neighborhood
N over time.
Experiment: Locality Bias within CE. We
combined CE with a fixed-? locality bias for
neighborhoods that were successful in the earlier
CE experiment, namely DELETEORTRANSPOSE1
for German, English, Turkish, and Portuguese.
Our results, shown in the seventh column of Ta-
ble 3, show that, in all cases except Turkish, the
combination improves over either technique on its
own. We leave exploration of structural annealing
with CE to future work.
Experiment: Segmentation Bias within CE.
For (language, N) pairs where CE was effec-
tive, we trained models using CE with a fixed-
? segmentation model. Across conditions (? ?
[?1, 1]), these models performed very badly, hy-
pothesizing extremely local parse trees: typically
over 90% of dependencies were length 1 and
pointed in the same direction, compared with the
60?70% length-1 rate seen in gold standards. To
understand why, consider that the CE goal is to
maximize the score of a sentence and all its seg-
mentations while minimizing the scores of neigh-
borhood sentences and their segmentations. An n-
gram model can accomplish this, since the same
n-grams are present in all segmentations of x,
and (some) different n-grams appear in N(x)
(for LENGTH and DELETEORTRANSPOSE1). A
bigram-like model that favors monotone branch-
ing, then, is not a bad choice for a CE learner that
must account for segmentations of x and N(x).
Why doesn?t CE without segmentation resort to
n-gram-like models? Inspection of models trained
using the standard CE method (no segmentation)
with transposition-based neighborhoods TRANS-
POSE1 and DELETEORTRANSPOSE1 did have
high rates of length-1 dependencies, while the
poorly-performing DELETE1 models found low
length-1 rates. This suggests that a bias toward
locality (?n-gram-ness?) is built into the former
neighborhoods, and may partly explain why CE
works when it does. We achieved a similar locality
bias in the likelihood framework when we broad-
ened the hypothesis space, but doing so under CE
over-focuses the model on local structures.
574
7 Error Analysis
We compared errors made by the selected EM con-
dition with the best overall condition, for each lan-
guage. We found that the number of corrected at-
tachments always outnumbered the number of new
errors by a factor of two or more.
Further, the new models are not getting better
by merely reversing the direction of links made
by EM; undirected accuracy also improved signif-
icantly under a sign test (p < 10?6), across all six
languages. While the most common corrections
were to nouns, these account for only 25?41% of
corrections, indicating that corrections are not ?all
of the same kind.?
Finally, since more than half of corrections in
every language involved reattachment to a noun
or a verb (content word), we believe the improved
models to be getting closer than EM to the deeper
semantic relations between words that, ideally,
syntactic models should uncover.
8 Future Work
One weakness of all recent weighted grammar
induction work?including Klein and Manning
(2004), Smith and Eisner (2005b), and the present
paper?is a sensitivity to hyperparameters, includ-
ing smoothing values, choice of N (for CE), and
annealing schedules?not to mention initializa-
tion. This is quite observable in the results we have
presented. An obstacle for unsupervised learn-
ing in general is the need for automatic, efficient
methods for model selection. For annealing, in-
spiration may be drawn from continuation meth-
ods; see, e.g., Elidan and Friedman (2005). Ideally
one would like to select values simultaneously for
many hyperparameters, perhaps using a small an-
notated corpus (as done here), extrinsic figures of
merit on successful learning trajectories, or plau-
sibility criteria (Eisner and Karakos, 2005).
Grammar induction serves as a tidy example
for structural annealing. In future work, we envi-
sion that other kinds of structural bias and anneal-
ing will be useful in other difficult learning prob-
lems where hidden structure is required, including
machine translation, where the structure can con-
sist of word correspondences or phrasal or recur-
sive syntax with correspondences. The technique
bears some similarity to the estimation methods
described by Brown et al (1993), which started
by estimating simple models, using each model to
seed the next.
9 Conclusion
We have presented a new unsupervised parameter
estimation method, structural annealing, for learn-
ing hidden structure that biases toward simplic-
ity and gradually weakens (anneals) the bias over
time. We applied the technique to weighted de-
pendency grammar induction and achieved a sig-
nificant gain in accuracy over EM and CE, raising
the state-of-the-art across six languages from 42?
54% to 58?73% accuracy.
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Floresta
sinta?(c)tica: a treebank for Portuguese. In Proc. of LREC.
H. Alshawi. 1996. Head automata and bilingual tiling:
Translation with minimal representations. In Proc. of
ACL.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation
process in the Turkish treebank. In Proc. of LINC.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Proc. of the Acoustical Society of America.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER Treebank. In Proc. of Workshop on
Treebanks and Linguistic Theories.
E. Brill and M. Marcus. 1992. Automatically acquiring
phrase structure using distributional analysis. In Proc. of
DARPA Workshop on Speech and Natural Language.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. of CoNLL.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
M. Collins. 1997. Three generative, lexicalised models for
statistical parsing. In Proc. of ACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum like-
lihood estimation from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society B, 39:1?38.
J. Eisner and D. Karakos. 2005. Bootstrapping without the
boot. In Proc. of HLT-EMNLP.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head automaton grammars. In
Proc. of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and hard
constraints on dependency length. In Proc. of IWPT.
J. Eisner. 1997. Bilexical grammars and a cubic-time proba-
bilistic parser. In Proc. of IWPT.
G. Elidan and N. Friedman. 2005. Learning hidden variable
networks: the information bottleneck approach. Journal
of Machine Learning Research, 6:81?127.
D. Hindle. 1990. Noun classification from predicate-
argument structure. In Proc. of ACL.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proc.
of ACL.
D. Klein and C. D. Manning. 2003. Fast exact inference with
a factored model for natural language parsing. In NIPS 15.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
575
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc. of
ACL.
K. Oflazer, B. Say, D. Z. Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In A. Abeille, editor,
Building and Exploiting Syntactically-Annotated Cor-
pora. Kluwer.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimiza-
tion problems. Proc. of the IEEE, 86(11):2210?2239.
K. Simov and P. Osenova. 2003. Practical annotation scheme
for an HPSG treebank of Bulgarian. In Proc. of LINC.
K. Simov, G. Popova, and P. Osenova. 2002. HPSG-
based syntactic treebank of Bulgarian (BulTreeBank). In
A. Wilson, P. Rayson, and T. McEnery, editors, A Rain-
bow of Corpora: Corpus Linguistics and the Languages
of the World, pages 135?42. Lincom-Europa.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2004.
Design and implementation of the Bulgarian HPSG-based
Treebank. Journal of Research on Language and Compu-
tation, 2(4):495?522.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In Proc.
of ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc. of
ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In Proc.
of IJCAI Workshop on Grammatical Inference Applica-
tions.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003.
Bootstrapping statistical parsers from small datasets. In
Proc. of EACL.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271?282.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn
Chinese Treebank: Phrase structure annotation of a large
corpus. Natural Language Engineering, 10(4):1?30.
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proc. of ACL.
A Experimental Setup
Following the usual conventions (Klein and Man-
ning, 2002), our experiments use treebank POS
sequences of length ? 10, stripped of words and
punctuation. For smoothing, we apply add-?, with
six values of ? (in CE trials, we use a 0-mean di-
agonal Gaussian prior with five different values of
?2). Our training datasets are:
? 8,227 German sentences from the TIGER Tree-
bank (Brants et al, 2002),
? 5,301 English sentences from the WSJ Penn
Treebank (Marcus et al, 1993),
? 4,929 Bulgarian sentences from the BulTree-
Bank (Simov et al, 2002; Simov and Osenova,
2003; Simov et al, 2004),
? 2,775 Mandarin sentences from the Penn Chi-
nese Treebank (Xue et al, 2004),
? 2,576 Turkish sentences from the METU-
Sabanci Treebank (Atalay et al, 2003; Oflazer et
al., 2003), and
? 1,676 Portuguese sentences from the Bosque
portion of the Floresta Sinta?(c)tica Treebank
(Afonso et al, 2002).
The Bulgarian, Turkish, and Portuguese datasets
come from the CoNLL-X shared task (Buchholz
and Marsi, 2006); we thank the organizers.
When comparing a hypothesized tree y to a
gold standard y?, precision and recall measures
are available. If every tree in the gold standard and
every hypothesis tree is such that |yright(0)| = 1,
then precision = recall = F1, since |y| = |y?|.
|yright(0)| = 1 for all hypothesized trees in this
paper, but not all treebank trees; hence we report
the F1 measure. The test set consists of around
500 sentences (in each language).
Iterative training proceeds until either 100 it-
erations have passed, or the objective converges
within a relative tolerance of  = 10?5, whichever
occurs first.
Models trained at different hyperparameter set-
tings and with different initializers are selected
using a 500-sentence development set. Unsuper-
vised model selection means the model with the
highest training objective value on the develop-
ment set was chosen. Supervised model selection
chooses the model that performs best on the anno-
tated development set. (Oracle and worst model
selection are chosen based on performance on the
test data.)
We use three initialization methods. We run a
single special E step (to get expected counts of
model events) then a single M step that renormal-
izes to get a probabilistic model ?(0). In initializer
1, the E step scores each tree as follows (only con-
nected trees are scored):
u(x,yleft ,yright) =
n?
i=1
?
j?y(i)
(
1 +
1
|i? j|
)
(Proper) expectations under these scores are com-
puted using an inside-outside algorithm. Initial-
izer 2 computes expected counts directly, without
dynamic programming. For an n-length sentence,
p(yright(0) = {i}) = 1n and p(j ? y(i)) ?
1
|i?j| .
These are scaled by an appropriate constant for
each sentence, then summed across sentences to
compute expected event counts. Initializer 3 as-
sumes a uniform distribution over hidden struc-
tures in the special E step by setting all log proba-
bilities to zero.
576
From Words to Corpora: Recognizing Translation
Noah A. Smith
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
nasmith@cs.jhu.edu
Abstract
This paper presents a technique for discover-
ing translationally equivalent texts. It is com-
prised of the application of a matching algo-
rithm at two different levels of analysis and a
well-founded similarity score. This approach
can be applied to any multilingual corpus us-
ing any kind of translation lexicon; it is there-
fore adaptable to varying levels of multilingual
resource availability. Experimental results are
shown on two tasks: a search for matching
thirty-word segments in a corpus where some
segments are mutual translations, and classifi-
cation of candidate pairs of web pages that may
or may not be translations of each other. The
latter results compare competitively with pre-
vious, document-structure-based approaches to
the same problem.
1 Introduction
As in most areas of natural language process-
ing, recent approaches to machine translation
have turned increasingly to statistical modeling
of the phenomenon (translation models) (Berger
et al, 1994). Such models are learned auto-
matically from data, typically parallel corpora:
texts in two or more languages that are mutual
translations. As computational resources have
become more powerful and less expensive, the
task of training translation models has become
feasible (Al-Onaizan et al, 1999), as has the
task of translating (or ?decoding?) text using
such models (Germann et al, 2001). However,
the success of the statistical approach to trans-
lation (and also to other multilingual applica-
tions that utilize parallel text) hangs crucially
on the quality, quantity, and diversity of data
used in parameter estimation.
If translation is a generative process, then one
might consider its reverse process of recognition:
Given two documents, might it be determined
fully automatically whether they are transla-
tions of each other?
The ability to detect translations of a doc-
ument has numerous applications. The most
obvious is as a means to build a parallel corpus
from a set of multilingual documents that con-
tains some translation pairs. Examples include
mining the World-Wide Web for parallel text
(Resnik, 1999; Nie et al, 1999; Ma and Liber-
man, 1999) and building parallel corpora from
comparable corpora such as multilingual collec-
tions of news reports. Another use of trans-
lation detection might be as an aid in align-
ment tasks at any level. For example, consider
the task of aligning NP chunks (and perhaps
also the extra-NP material) in an NP-bracketed
parallel corpus; a chunk-level similarity score
(Fluhr et al, 2000) built from a word-level
model could be incorporated into a framework
that involves bootstrapping more complex mod-
els of translation from simpler ones (Berger et
al., 1994). Finally, reliable cross-lingual dupli-
cate detection might improve performance in n-
best multilingual information retrieval systems;
at the same time, by detecting the existence of a
translation in a multilingual corpus, the cost of
translating a document of interest is eliminated.
I present here an algorithm for classifying
document pairs as either translationally equiv-
alent or not, which can be built upon any
kind of word-to-word translation lexicon (au-
tomatically learned or hand-crafted). I pro-
pose a score of translational similarity, then
describe an evaluation task involving a con-
strained search for texts (of arbitrary size) that
are translation pairs, in a noisy space, and
present precision/recall results. Finally, I show
that this algorithm performs competitively with
the approach of Resnik (1999), in which only
                                            Association for Computational Linguistics.
                     Language Processing (EMNLP), Philadelphia, July 2002, pp. 95-102.
                         Proceedings of the Conference on Empirical Methods in Natural
Maria does n?t like fruit
Maria n? aime pas de fruits
X:
Y:
NULLNULL NULL
Figure 1: An example of two texts with links shown.
structural information (HTML-markup) is used
to detect translation pairs, though the new algo-
rithm does not require structural information.
2 Quantifying Similarity
This section shows how to compute a cross-
lingual similarity score, tsim, for two texts.1
Suppose parallel texts are generated according
to Melamed?s (2000) symmetric word-to-word
model (Model A). Let a link be a pair (x, y)
where x is a word in language L1 and y is a
word in L2. Within a link, one of the words
may be NULL, but not both. The model con-
sists of a bilingual dictionary that gives a prob-
ability distribution over all possible link types.
In the generative process, a sequence of inde-
pendent link tokens is generated according to
that distribution.
The links are not observed; only the lexical
(non-NULL) words in each language are ob-
served. The texts whose similarity score is to be
computed, X and Y , correspond to the mono-
lingual lexical projections of the links. For the
purposes of this discussion, the texts are viewed
as unordered bags of words; scrambling of the
link tokens in the two texts is not modeled.
An example is illustrated in Figure 1; there are
seven link tokens shown, five of which are lexi-
cal in X (the English side) and six of which are
lexical in Y (the French side).
The next step is to compute the probability of
the most probable sequence that could have ac-
counted for the two texts. All permutations of
a given link sequence will have the same prob-
ability (since the links are generated indepen-
dently), so the order of the sequence is not im-
portant. As noted by Melamed (2000), under
the assumption that the quality of a link col-
lection is the sum of the quality of the links,
then this problem of finding the best set of
links is equivalent to the maximum-weighted bi-
partite matching (MWBM) problem: Given a
weighted bipartite graph G = (V1 ? V2, E) with
|V1| = |V2| and edge weights ci,j(i ? V1, j ? V2),
1I use the term ?text? to refer to a piece of text of
any length.
find a matching M ? E such that each ver-
tex has at most one edge in M , and
?
e?M ci,j
is maximized. The fastest known MWBM al-
gorithm runs in O(ve + v2 log v) time (Ahuja
et al, 1993). Applied to this problem, that is
O(max(|X|, |Y |)3).
The similarity score should be high when
many of the link tokens in the best link col-
lection do not involve NULL tokens. Further,
it should normalize for text length. Specifically,
the score I use is:
tsim =
log Pr(two-word links in best matching)
log Pr(all links in best matching)
(1)
This score is an example of Lin?s (1998) math-
ematical definition of similarity, which is moti-
vated by information theory:
sim(X,Y ) =
log Pr(common(X,Y ))
log Pr(description(X,Y ))
(2)
where X and Y are any objects generated by a
probabilistic model.2
In this research, I seek to show how multiple
linguistic resources can be exploited together to
recognize translation. The measure in (1) is
simplified by assuming that all links in a given
translation lexicon are equiprobable. (In some
cases I use an automatically induced translation
lexicon that assigns probabilities to the entries,
but for generality the probabilities are ignored.)
This reduces the formula in (1) to
tsim =
# two-word links in best matching
# links in best matching
. (3)
Further, to compute tsim under the equiprob-
ability assumption, we need not compute the
MWBM, but only find the maximum cardi-
nality bipartite matching (MCBM), since all
potential links have the same weight. An
2Another approach, due to Jason Eisner (personal
communication) would be to use a log-likelihood ratio
of two hypotheses: joint vs. separate generation of the
two texts (log Pr(all links in the best sequence)Pr(all words in X) Pr(all words in Y ) ). In order
to make this value (which is the Viterbi approximation to
point-wise mutual information between the two texts) a
score suitable for comparison between different pairs of
texts, it must be normalized by length. With normal-
ization, this score is monotonic in Lin?s (1998) sim if a
uniform unigram model is assumed for the tokens in the
single-language models (the denominator terms).
O(e
?
v) (or O(|X| ? |Y | ?
?
|X|+ |Y |) for this
purpose) algorithm exists for MCBM (Ahuja et
al., 1993). If the matching shown in Figure 1 is
the MCBM (for some translation lexicon), then
tsim(X,Y ) = 47 under the simplifying assump-
tion.
If Equation (3) is applied to pairs of docu-
ments in the same language, with a ?transla-
tion lexicon? defined by the identity relation,
then tsim is a variant of resemblance (r), as de-
fined by Broder et al (1997) for the problem of
monolingual duplicate detection:
r(X,Y ) =
|S(X) ? S(Y )|
|S(X) ? S(Y )|
(4)
where S(Z) is a shingling of the words in Z; a
shingling is the set of unique n-gram types in the
text for some fixed n (Damashek, 1995). Unlike
Broder et al?s r, however, tsim is token-based,
incorporating word frequency. Specifically, the
intersection of two bags (rather than sets) of to-
kens contains the minimum count (over the in-
tersected bags) of each type; the union contains
the maximum counts, e.g.,
{a, a, a, b, b} ? {a, a, b, b, b} = {a, a, b, b}
{a, a, a, b, b} ? {a, a, b, b, b} = {a, a, a, b, b, b}
With the assumption of equiprobability, any
translation lexicon (or, importantly, union
thereof) containing a set of word-to-word en-
tries, can be used in computing tsim.
3 Finding Translations
Formally, the evaluation task I propose can be
described as follows: Extract all translation
pairs from a pool of 2n texts, where n of them
are known to be in language L1 and the other
n are known to be in L2. Each text can have
one or zero translations in the corpus; let the
number of true translation pairs be k.
The general technique for completing the task
is to first find the best matching of words in
text pairs (posed as a bipartite matching prob-
lem) in order to compute the tsim similarity
score. Next, to extract translation pairs of texts
from a corpus, find the best matching of texts
based on their pairwise tsim scores, which can
be posed as a ?higher-level? MWBM problem:
by matching the texts using their pair-wise sim-
ilarity scores, a corpus of pairs of highly similar
texts is extracted from the pool.
If k is known, then the text-matching problem
is a generalization of MWBM: Given a weighted
bipartite graph G = (V1?V2, E) with |V1| = |V2|
and edge weights ci,j , find a matching M ? E
of size k such that each vertex has at most one
edge in M , and
?
e?M ci,j is maximized. The set
of texts in L1 is V1, and the set of texts in L2 is
V2; the weights ci,j are the scores tsim(vi, vj). I
do not seek a solution to the generalized prob-
lem here; one way of approximating it is by tak-
ing the top k tsim-scored elements from the set
M (the MWBM).
If k is not known, it can be estimated (via
sampling and human evaluation); I take the ap-
proach of varying the estimate of k by applying
a threshold ? on the tsim scores, then comput-
ing precision and recall for those pairs in M
whose score is above ? (call this set M? ):
prec? =
|M? ? T |
|M? |
, rec? =
|M? ? T |
k
(5)
where T is the set of k true translation pairs.
Performance results are presented as (precision,
recall) pairs as ? is lowered.3
Melamed (2000) used a greedy approxi-
mation to MWBM called competitive link-
ing, which iteratively selects the edge with
the highest weight, links those two vertices,
then removes them from the graph. (Ties
are broken at random.) A heap-based im-
plementation of competitive linking runs in
O(max(|X|, |Y |) log max(|X|, |Y |)). In the first
experiment, I show a performance comparison
between MWBM and competitive linking.
4 Experiment: English-Chinese
This experiment used the Hong Kong Hansard
English-Chinese parallel corpus. The training
corpus is aligned at the sentence level, with seg-
ment lengths averaging fifteen words (in each
language). The test corpus is aligned at the two-
sentence level, with segment lengths averaging
thirty words. The first experiment involved ten-
fold cross-validation with (for each fold) a train-
ing corpus of 9,400 sentence pairs and a test
corpus of 1,000 two-sentence pairs. The corpus
3The selection of an appropriate ? will depend on
the application, the corpus, the lexicons, etc. In my
evaluation on WWW data, I use a small development
set to choose a threshold that maximizes one measure of
performance.
was randomly divided into folds, and no noise
was introduced (i.e., k = n).4
4.1 Translation Lexicon
The main translation lexicon of interest is a
union of three word-to-word translation lexicons
from different sources. I refer to this translation
lexicon as UTL.
The first component translation lexicon,
DICT, was made from the union of two
English-Chinese electronic dictionaries, specifi-
cally, those from Meng et al (2000) and Levow
et al (2000) (a total of 735,908 entries, many of
which are not one-to-one). To make the dictio-
nary exclusively one-to-one entries, each n-to-
m entry was processed by removing all function
words in either side of the entry (according to
a language-specific stoplist), then, if both sides
have one or two words (no more), adding all
word-pairs in the cross-product (otherwise the
entry is discarded).5 The resulting translation
lexicon contains 577,655 word pairs, 48,193 of
which contain two words that are present in the
corpus. This translation lexicon has the advan-
tage of broad coverage, though it does not gen-
erally contain names or domain-specific words,
which are likely to be informative, and does not
capture morphological variants.
The second translation lexicon, TMTL, is au-
tomatically generated by training a symmet-
ric word-to-word translation model (Model A,
(Melamed, 2000)) on the training corpus.6 All
word pairs with nonzero probability were added
to the translation lexicon (no smoothing or
thresholding was applied). On average (over ten
folds), this translation lexicon contained 6,282
entries. The TMTL translation lexicons are ex-
pected to capture words specific to the domain
(Hong Kong government transcripts), as well as
common inflections of words, though they will
4It is possible that random division gives a favorable
bias in the translation model translation lexicon by in-
creasing the probability that rare words that appear only
in certain portions of the corpus will be present in both
training and test data.
5The limit of two words per side is an arbitrary choice
intended to minimize the noise introduced by this pro-
cessing step.
6In parameter estimation, I used the aforementioned
MWBM algorithm (instead of Melamed?s (2000) com-
petitive linking), which is the maximum posterior ap-
proximation to EM. It is not clear, however, that this
change yields performance gains.
also contain noise.
The third translation lexicon, STR, is the
string identity lexicon: (x, y) is in the trans-
lation lexicon iff the string x is identical to
the string y. This translation lexicon captures
punctuation, numerals, alphanumeric strings
used to label sections, and English words in-
cluded as-is in the Chinese corpus. There were
3,083 such pairs of word types in the corpus.
4.2 Filtering
Chen and Nie (2000) note that text pairs that
are highly disparate in length are unlikely to
be translations. In order to avoid computing
tsim scores for all pairs in the cross-product, I
eliminated all segment pairs whose lengths are
outliers in a linear regression model estimated
from the training corpus. Earlier experiments
(on a different corpus) showed that, if a (1?p)-
confidence interval is used, the size of the search
space reduces exponentially as p increases, while
the number of correct translation pairs that do
not pass the filter is only linear in p (i.e., the
filter gives high recall and high precision). For
these experiments, p = 0.05; this value was se-
lected based on the results presented in Smith
(2001).
4.3 Results
When the length filter was applied to the
1,000,000 possible pairs in the cross-product,
47.9% of the pairs were eliminated, while 94.5%
of the correct pairs were kept, on average (over
ten folds). tsim was computed for each pair
that passed the filter, then each matching al-
gorithm (MWBM and competitive linking) was
applied. As discussed above, a threshold can
then be applied to the matching to select the
pairs about whose translational equivalence the
score is most confident. Precision and recall
plots are shown in Figure 2a. Each line corre-
sponds to a (translation lexicon, matching algo-
rithm) pair, showing average precision and re-
call over the ten folds as the threshold varies.
The plots should be read from left to right, with
recall increasing as the threshold is lowered.
When many resources are used, the technique
is highly adept at selecting the translation pairs.
TMTL alone outperforms DICT alone, proba-
bly due to its coverage of domain-specific terms.
The competitive linking algorithm lags behind
MWBM in most cases, though its performance
was slightly better in the case of TMTL. In
the case of UTL, for recall up to 0.8251, the
thresholded MWBM matching had significantly
higher precision than the thresholded competi-
tive linking matching at a comparable level of
recall (based on a Sign Test over the ten cross-
validation folds, p < 0.01).
Table 1 shows the maximum performance
(by F -score) for each translation lexicon under
MWBM and competitive linking.
4.4 Effects of Noise
Next, I performed an experiment to test the
technique?s robustness to noise. In this case,
the test corpus contained 300 known transla-
tion pairs (again, two-sentence texts). From 0
to 2700 additional English texts and the same
number of Chinese texts were added. These
?noise? texts were from the same corpus and
were guaranteed not to be aligned with each
other.7 The length filter eliminated 48.6% of the
9,000,000 possible pairs in the cross-product,
keeping 95.7% of the true pairs. The filtered
pairs were tsim-scored using UTL, then the
MWBM was computed. Precision and recall
are plotted for various levels of noise in Fig-
ure 2b.8 Only in the highest-noise condition
( kn = 0.1) do we observe a situation where a
sufficiently strict threshold cannot be used to
guarantee an extracted corpus of (nearly) ar-
bitrarily high precision. For example, if 90%
precision is required, 88.3%, 60.3%, and 43.7%
recall can be guaranteed when kn is 1, 0.5, and
0.25, respectively.
These experiments show that with a strict
threshold this technique is capable of produc-
ing a highly precise matching of parallel text
from a noisy corpus, though attainable recall
levels drop as noise is added. Performance can
be boosted by incorporating additional bilingual
resources. Finally, even a fast, greedy approxi-
7In general, robustness to noise will depend on the
source of the noise and how much the noise looks like
the true translations. Hence the results presented here
may be better or worse than those achieved in specific
applications to which this technique might be applied,
depending on those factors, filtering, etc.
8Experiments were carried out for the TMTL and
DICT translation lexicons, and also under competitive
linking. Space does not permit a full discussion, though
it is worth mentioning that, as in the noiseless experi-
ment, UTL outperformed the others, likewise MWBM
outperformed competitive linking.
Tr. lex. Algorithm ? prec? rec? F?
UTL MWBM 0.20000 0.908 0.836 0.871
CL 0.22078 0.917 0.805 0.857
DICT MWBM 0.10638 0.776 0.647 0.706
CL 0.12121 0.770 0.590 0.668
TMTL MWBM 0.00971 0.841 0.711 0.771
CL 0.00909 0.854 0.711 0.776
Table 1: Comparison of translation lexicons and
matching algorithms at their maximal F -scores.
Note that thresholds, and tsim scores in general, are
comparable only for a given translation lexicon. The
STR translation lexicon offered a boost only when
used to supplement TMTL ? DICT; when added to
each alone it had little or no effect.
top 300 pairs maximum F
n prec rec ? prec? rec? F?
300 0.904 0.883 0.16667 0.925 0.863 0.893
400 0.813 0.813 0.25641 0.897 0.787 0.838
500 0.770 0.770 0.28000 0.881 0.717 0.791
600 0.727 0.727 0.28000 0.782 0.707 0.743
900 0.663 0.663 0.32142 0.829 0.600 0.696
1200 0.630 0.630 0.32142 0.733 0.593 0.656
3000 0.483 0.483 0.35849 0.617 0.440 0.514
Table 2: Precision and recall when the top k (300)
pairs are taken (i.e., k is known; in the case of n =
300, the matching contained only 293 pairs), and at
maximal F -scores for various levels of noise.
mation to the best matching can be useful.
5 Experiment: English-French
An important application of translation recog-
nition is the construction of parallel text cor-
pora. One source of raw text in this task is
the World-Wide Web, for which several paral-
lel text search systems currently exist (Resnik,
1999; Nie et al, 1999; Ma and Liberman, 1999).
These systems propose candidate pairs of pages,
which are then classified as either translation-
ally equivalent or not. The STRAND system
(Resnik, 1999), for example, uses structural
markup information from the pages, without
looking at their content, to attempt to align
them.
If the tsim technique can provide a classi-
fier that rivals or complements the structural
one, using as it does an entirely orthogonal set
of features, then perhaps a combined classifier
could provide even greater reliability. In addi-
tion, custom-quality parallel corpora could be
generated from comparable corpora that lack
DICT
TMTL
UTL
MWBMCL
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.2 0.4 0.6 0.8 1
Pre
cisi
on
Recall
0 0.2 0.4 0.6 0.8 1Recall 0 0.2
0.4 0.6
0.8 1
k / n
00.2
0.40.6
0.81
Precision
(a) (b)
Figure 2: (a.) Precision and recall with no noise. This plot shows precision and recall averaged over all
ten folds. Each point corresponds to a threshold value; the threshold becomes less strict from left to right.
Shown are curves for each of UTL, TMTL, and DICT under both algorithms (MWBM, CL); the maximum
F scores are marked (see Table 1). (b.) Precision-recall curves at varying levels of noise. k = 300 in
all cases; the circles and dashed line show precision and recall for the top 300 pairs in the matching (i.e., if k
were known, it would not make sense to use a lower threshold, so the only reasonable thresholds are to the
left), and the squares and dotted line show precision and recall at each condition?s maximum F -score?the
values are shown in Table 2. (Note that the curves ?stop? before reaching a point where recall is 1.0, since
a point is eventually reached where no more matches are possible (because of filtering).)
structural features. This experiment also shows
that tsim is scalable to larger texts.
5.1 Translation Lexicon
In this experiment, the language pair is English-
French. Multiple sources for the translation lex-
icon are used in a manner similar to Section 4.1.
? An English-French dictionary (a total of
34,808 entries, 4,021 of which are not one-to-
one).9 It contains morphological variants but
does not include character accents. Each n-to-
m entry was processed by stoplisting and then
extracting all word-pairs in the remaining cross-
product as in section 4.1. Result: 39,348 word
pairs, 9,045 of which contain two words present
in the corpora.
? A word-to-word translation model (Melamed,
2000) trained on a verse-aligned Bible using
MWBM (15,548 verses, averaging 25.5 English
words, 23.4 French words after tokenization).
Result: 13,762 word pairs.
? English-French cognate pairs, identified us-
ing the method of Tiedemann (1999). Space
does not permit a full description of the tech-
nique; I simply note that cognates were iden-
tified by thresholding on a specially-trained
9This dictionary was generated using a dictionary de-
rived from one available at http://www.freedict.com.
string-similarity score based on language-
specific character-to-character weights.10 Re-
sult: 35,513 word pairs. An additional set of
11,264 exact string matches were added. These
entries are quite noisy.
The union of these translation lexicons consists
of 68,003 unique word pairs. The experiment
used only this union translation lexicon.
5.2 Results
In order to compare tsim with structural simi-
larity scoring, I applied it to 325 English-French
web-document pairs. These were the same pairs
for which human evaluations were carried out by
Resnik (1999).11 Note that this is not a match-
ing task; the documents are presented as candi-
date pairs, and there is no competition among
pages for matches in the other language. At dif-
ferent thresholds, a ? score of agreement (with
each of Resnik?s (1999) two judges and their
10Tiedemann trained these weights using a list of
known cognates; I use a noisy list of weighted translation
pairs (specifically, TMTL) Hence the resources required
to extract cognates in this way are no different from those
required for the translation model.
11One additional pair was thrown out because it con-
tained compressed data; it is assumed that pair would
not pass a language identification filter.
intersection) may be computed for comparison
with Resnik?s STRAND system, along with re-
call and precision against a gold standard (for
which I use the intersection of the judges?the
set of examples where the judges agreed). Note
that recall in this experiment is relative to the
candidate set proposed by the STRAND search
module, not the WWW or even the set of pages
encountered in the search.
The estimate of tsim (MWBM on the words
in the document pair) is not computationally
feasible for very large documents and transla-
tion lexicons. In preliminary comparisons, I
found that representing long documents by as
few as their first 500 words results in excel-
lent performance on the ? measure. This al-
lows O(1) estimation of tsim for two documents:
look only at the first (fixed) n words of each
document. Further, the competitive linking al-
gorithm appears to be as reliable as MWBM.
The results reported here approximated tsim in
using competitive linking on the first 500 words.
Of the 325 pairs, 32 were randomly selected
as a development set. Maximizing ? on this set
yielded a value of ? = 0.15.12 ? scores against
each judge and their intersection were then com-
puted at that threshold on the test set (the re-
maining 293 pairs). These are compared to ?
scores of the STRAND system, on the same test
set, in Table 3. In every case, the tsim classifier
agreed more strongly with the human evalua-
tions.
At ? = 0.15, precision was 0.680 and re-
call was 0.921, F = 0.782 (on the same
set, STRAND structural classification achieved
0.963 precision and 0.684 recall, F = 0.800).
Figure 3 shows ?, precision, and recall plotted
against ? .
6 Future Directions
The success of this approach suggests a way to
construct parallel corpora from any large, seg-
mented comparable corpus: start with a trans-
lation model estimated on a small, high-quality
parallel text, and a core dictionary; then extract
document pairs with high similarity (tsim) and
add them to the parallel corpus. Next, esti-
mate word-level translational equivalence em-
pirically from the enlarged corpus and update
12One could select such a threshold to maximize any
objective function over the development set.
Comparison N Pr(Agree) ?
J1, J2 245 0.98 0.96
J1, STRAND 250 0.88 0.70
J2, STRAND 284 0.88 0.69
J1 ? J2, STRAND 241 0.90 0.75
J1, tsim(? = 0.15) 249 0.92 0.83
J2, tsim(? = 0.15) 283 0.92 0.82
J1 ? J2, tsim(? = 0.15) 240 0.93 0.85
Table 3: Comparison with STRAND. The test set is
294 of the 326 pairs in Resnik?s (1999) test set. The
STRAND ? scores are similar to those published by
Resnik (1999). The 32 development pairs were used
to select the 0.15 threshold. N is the number of
examples for which judgement-comparison was pos-
sible in each case (human judges were sometimes
undecided; those cases are ignored in computing ?).
STRAND:  best ?
?
0
0.2
0.4
0.6
0.8
1
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
Threshold
PrecisionRecall
Cohen?s    
Figure 3: Performance measures as functions of the
threshold ? : the ? agreement score with the two
judges? intersection, precision, and recall. All mea-
sures are on the test set. The ? score obtained by
STRAND is shown as well.
the translation lexicon; extract documents iter-
atively. The experiments presented here show
that, even in highly noisy search spaces, tsim
can be used with a threshold to extract a high-
precision parallel corpus at moderate recall.
It is worth noting that the STRAND classi-
fier and the tsim classifier disagreed 15% of the
time on the test set. A simple combination by
disjunction (i.e., ?(X,Y ) is a translation pair if
either classifier says so?) yields precision 0.768,
recall 0.961, F = 0.854, and ? (with the judges?
intersection) at 0.878. In future work, more so-
phisticated combinations of the two classifiers
might integrate the advantages of both.
7 Conclusion
I have proposed a language-independent ap-
proach to the detection of translational equiva-
lence in texts of any size that works at various
bilingual resource levels. Fast, effective approx-
imations have also been described, suggesting
scalability to very large corpora. Notably, tsim
is adaptable to any probabilistic model of trans-
lational equivalence, because it is an instance
of a model-independent definition of similarity.
The core of the technique is the computation
of optimal matchings at two levels: between
words, to generate the tsim score, and between
texts, to detect translation pairs.
I have demonstrated the performance of
this technique on English-Chinese and English-
French.13 It is capable of pulling parallel texts
out of a large multilingual collection, and it
rivals the performance of structure-based ap-
proaches to pair classification (Resnik, 1999),
having better ? agreement with human judges.
Acknowledgements
This work was supported in part by the Na-
tional Science Foundation and DARPA/ITO
Cooperative Agreement N660010028910 (at the
University of Maryland) and a Fannie and
John Hertz Foundation Fellowship. The au-
thor thanks Dan Melamed, Philip Resnik, Doug
Oard, Rebecca Hwa, Jason Eisner, Hans Flo-
rian, and Gideon Mann for advice and insight-
ful conversations; also Gina Levow for making
available the bilingual dictionaries and Philip
Resnik for sharing the STRAND test data and
human judgements.
References
R. K. Ahuja, T. L. Magnati, and J. B. Orlin. 1993.
Network Flows: Theory, Algorithms, and Appli-
cations. Prentice Hall, Englewood Cliffs, NJ.
Y. Al-Onaizan, J. Cur?in, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, N. A. Smith, F.-J. Och,
D. Purdy, and D. Yarowsky. 1999. Statistical Ma-
chine Translation. Technical report, Johns Hop-
kins University.
A. L. Berger, P. F. Brown, S. A. Della Pietra,
V. J. Della Pietra, J. R. Gillett, J. D. Lafferty,
R. L. Mercer, H. Printz, and L. Ures?. 1994.
13Comparable experiments using another version of
the score showed performance for English-Spanish on the
matching task to be even better than for English-Chinese
(using that same score) (Smith, 2001).
The Candide system for machine translation. In
ARPA Workshop on Speech and Natural Lan-
guage Technology, pages 157?163. Morgan Kauf-
man.
A. Z. Broder, S. C. Glassman, M. S. Manasse, and
G. Zweig. 1997. Syntactic clustering of the Web.
In Sixth International World-Wide Web Confer-
ence, Santa Clara, CA.
J. Chen and J.-Y. Nie. 2000. Web parallel text min-
ing for chinese-english cross-language information
retrieval. In International Conference on Chinese
Language Computing, Chicago, IL.
M. Damashek. 1995. Gauging similarity with n-
grams: language independent categorization of
text. Science, 267:843?8.
C. Fluhr, F. Bisson, and F. Elkateb. 2000. Mutual
benefit of sentence/word alignment and cross-
lingual information retrieval. In Ve?ronis, J. (ed.),
Parallel Text Processing. Kluwer Academic Pub-
lishers, Dordrecht.
U. Germann, M. Jahr, K. Knight, D. Marcu, and
K. Yamada. 2001. Fast decoding and optimal
decoding for machine translation. In 39th ACL,
Toulouse, France.
G.-A. Levow, D. W. Oard, and C. I. Cabezas. 2000.
Translingual topic tracking with PRISE. In Topic
Detection and Tracking Workshop, Tysons Cor-
ner, VA.
D. Lin. 1998. An information-theoretic definition
of similarity. In International Conference on Ma-
chine Learning, Madison, WI.
X. Ma and M. Liberman. 1999. BITS: a method for
bilingual text search over the web. In Machine
Translation Summit VII, Singapore.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
H. Meng, B. Chen, E. Grams, S. Khudanpur, G.-A.
Levow, W.-K. Lo, D. W. Oard, P. Schone, H.-M.
Wang, and J. Wang. 2000. Mandarin-English: In-
vestigating Translingual Speech Retrieval. Tech-
nical report, Johns Hopkins University.
J. Nie, P. Isabelle, M. Simard, and R. Durand.
1999. Cross-language information retrieval based
on parallel texts and automatic mining of parallel
texts from the Web. In ACM-SIGIR Conference,
pages 74?81, Berkeley, CA.
P. Resnik. 1999. Mining the Web for bilingual text.
In 37th ACL, College Park, MD.
N. A. Smith. 2001. Detection of Translational
Equivalence. Undergraduate honors thesis, Uni-
versity of Maryland.
J. Tiedemann. 1999. Automatic construction of
weighted string similarity measures. In Confer-
ence on EMNLP and VLC, College Park, MD.
Bilingual Parsing with Factored Estimation:
Using English to Parse Korean
David A. Smith and Noah A. Smith
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
{d,n}asmith@cs.jhu.edu
Abstract
We describe how simple, commonly understood statisti-
cal models, such as statistical dependency parsers, proba-
bilistic context-free grammars, and word-to-word trans-
lation models, can be effectively combined into a uni-
fied bilingual parser that jointly searches for the best En-
glish parse, Korean parse, and word alignment, where
these hidden structures all constrain each other. The
model used for parsing is completely factored into the
two parsers and the TM, allowing separate parameter es-
timation. We evaluate our bilingual parser on the Penn
Korean Treebank and against several baseline systems
and show improvements parsing Korean with very lim-
ited labeled data.
1 Introduction
Consider the problem of parsing a language L for which
annotated resources like treebanks are scarce. Suppose
we have a small amount of text data with syntactic an-
notations and a fairly large corpus of parallel text, for
which the other language (e.g., English) is not resource-
impoverished. How might we exploit English parsers to
improve syntactic analysis tools for this language?
One idea (Yarowsky and Ngai, 2001; Hwa et al, 2002)
is to project English analysis onto L data, ?through?
word-aligned parallel text. To do this, we might use an
English parser to analyze the English side of the parallel
text and a word-alignment algorithm to induce word cor-
respondences. By positing a coupling of English syntax
with L syntax, we can induce structure on the L side of
the parallel text that is in some sense isomorphic to the
English parse.
We might take the projection idea a step farther. A
statistical English parser can tell us much more than the
hypothesized best parse. It can be used to find every
parse admitted by a grammar, and also scores of those
parses. Similarly, translation models, which yield word
alignments, can be used in principle to score competing
alignments and offer alternatives to a single-best align-
ment. It might also be beneficial to include the predic-
tions of an L parser, trained on any available annotated
L data, however few.
This paper describes how simple, commonly under-
stood statistical models?statistical dependency parsers,
probabilistic context-free grammars (PCFGs), and word
translation models (TMs)?can be effectively combined
into a unified framework that jointly searches for the best
English parse, L parse, and word alignment, where these
hidden structures are all constrained to be consistent.
This inference task is carried out by a bilingual parser.
At present, the model used for parsing is completely fac-
tored into the two parsers and the TM, allowing separate
parameter estimation.
First, we discuss bilingual parsing (?2) and show how
it can solve the problem of joint English-parse, L-parse,
and word-alignment inference. In ?3 we describe param-
eter estimation for each of the factored models, includ-
ing novel applications of log-linear models to English
dependency parsing and Korean morphological analysis.
?4 presents Korean parsing results with various mono-
lingual and bilingual algorithms, including our bilingual
parsing algorithm. We close by reviewing prior work in
areas related to this paper (?5).
2 Bilingual parsing
The joint model used by our bilingual parser is an in-
stance of a stochastic bilingual multitext grammar (2-
MTG), formally defined by Melamed (2003). The 2-
MTG formalism generates two strings such that each
syntactic constituent?including individual words?in
one side of the bitext corresponds either to a constituent
in the other side or to ?.
Melamed defines bilexicalized MTG (L2MTG), which
is a synchronous extension of bilexical grammars such
as those described in Eisner and Satta (1999) and applies
the latter?s algorithmic speedups to L2MTG-parsing.
Our formalism is not a precise fit to either unlexical-
ized MTG or L2MTG since we posit lexical dependency
structure only in one of the languages (English). The pri-
mary rationale for this is that we are dealing with only a
small quantity of labeled data in language L and there-
fore do not expect to be able to accurately estimate its
lexical affinities. Further, synchronous parsing is in prac-
tice computationally expensive, and eliminating lexical-
ization on one side reduces the run-time of the parser
from O(n8) to O(n7). Our parsing algorithm is a simple
transformation of Melamed?s R2D parser that eliminates
head information in all Korean parser items.
The model event space for our stochastic ?half-
bilexicalized? 2-MTG consists of rewrite rules of the fol-
lowing two forms, with English above and L below:
(
X[h1]
A ?
h1
h2
)
,
(
X[h1]
A ?
Y [h1]Z[c1]
BC
)
where upper-case symbols are nonterminals and lower-
case symbols are words (potentially ?). One approach
to assigning a probability to such a rule is to make an
independence assumption, for example:
Pr
bi
(
X[h]
A ?
Y [h1]Z[c]
BC
)
=
Pr
English
(X[h] ? Y [h1]Z[c1]) ? Pr
L
(A ? BC)
There are two powerful reasons to model the bilingual
grammar in this factored way. First, we know of no tree-
aligned corpora from which bilingual rewrite probabili-
ties could be estimated; this rules out the possibility of
supervised training of the joint rules. Second, separat-
ing the probabilities allows separate estimation of the
probabilities?resulting in two well-understood param-
eter estimation tasks which can be carried out indepen-
dently.1
This factored modeling approach bears a strong re-
semblance to the factored monolingual parser of Klein
and Manning (2002), which combined an English depen-
dency model and an unlexicalized PCFG. The generative
model used by Klein and Manning consisted of multiply-
ing the two component models; the model was therefore
deficient.
We go a step farther, replacing the deficient genera-
tive model with a log-linear model. The underlying pars-
ing algorithm remains the same, but the weights are no
longer constrained to sum to one. (Hereafter, we assume
weights are additive real values; a log-probability is an
example of a weight.) The weights may be estimated
using discriminative training (as we do for the English
model, ?3.1) or as if they were log-probabilities, using
smoothed maximum likelihood estimation (as we do for
the Korean model, ?3.3). Because we use this model only
for inference, it is not necessary to compute a partition
function for the combined log-linear model.
In addition to the two monolingual syntax models, we
add a word-to-word translation model to the mix. In
this paper we use a translation model to induce only a
single best word matching, but in principle the transla-
tion model could be used to weight all possible word-
word links, and the parser would solve the joint align-
ment/parsing problem.2
As a testbed for our experiments, the Penn Korean
Treebank (KTB; Han et al, 2002) provides 5,083 Ko-
rean constituency trees along with English translations
and their trees. The KTB also analyzes Korean words
into their component morphemes and morpheme tags,
which allowed us to train a morphological disambigua-
tion model.
To make the most of this small corpus, we performed
all our evaluations using five-fold cross-validation. Due
to the computational expense of bilingual parsing, we
1Of course, it might be the case that some information is known
about the relationship between the two languages. In that case, our log-
linear framework would allow the incorporation of additional bilingual
production features.
2Although polynomial, we found this to be too computationally de-
manding to do with our optimal parser in practice, but with pruning
and/or A? heuristics it is likely to be feasible.
produced a sub-corpus of the KTB limiting English sen-
tence length to 10 words, or 27% of the full data. We then
randomized the order of sentences and divided the data
into five equal test sets of 280 sentences each (?1,700
Korean words, ?2,100 English words). Complementing
each test set, the remaining data were used for training
sets of increasing size to simulate various levels of data
scarcity.
3 Parameter estimation
We now describe parameter estimation for the four com-
ponent models that combine to make our full system (Ta-
ble 1).
3.1 English syntax model
Our English syntax model is based on weighted bilexi-
cal dependencies. The model predicts the generation of
a child (POS tag, word) pair, dependent upon its parent
(tag, word) and the tag of the parent?s most recent child
on the same side (left or right). These events correspond
quite closely to the parser described by Eisner?s (1996)
model C, but instead of the rules receiving conditional
probabilities, we use a log-linear model and allow arbi-
trary weights. The model does not predict POS tags; it
assumes they are given, even in test.
Note that the dynamic program used for inference
of bilexical parses is indifferent to the origin of the
rule weights; they could be log-probabilities or arbitrary
numbers, as in our model. The parsing algorithm need
not change to accommodate the new parameterization.
In this model, the probability of a (sentence, tree) pair
(E, T ) is given by:
Pr(E, T ) =
exp (f(E, T ) ? ?)
?
E?,T ? exp(f(E
?, T ?) ? ?)
(1)
where ? are the model parameters and f is a vector func-
tion such that fi is equal to the number of times a feature
(e.g., a production rule) fires in (E, T ).
Parameter estimation consists of selecting weights ?
to maximize the conditional probability of the correct
parses given observed sentences:3
?
i
Pr(Ti|Si) =
?
i
exp (f(Ei, Ti) ? ?)
?
T ? exp(f(Ei, T
?) ? ?)
(2)
Another important advantage of moving to log-linear
models is the simple handling of data sparseness. The
feature templates used by our model are shown in Ta-
ble 2. The first feature corresponds to the fully-described
child-generation event; others are similar but less infor-
mative. These ?overlapping? features offer a kind of
backoff, so that each child-generation event?s weight re-
ceives a contribution from several granularities of de-
scription.
Feature selection is done by simple thresholding: if a
feature is observed 5 times or more in the training set,
its weight is estimated; otherwise its weight is locked at
3This can be done using iterative scaling or gradient-based numeri-
cal optimization methods, as we did.
Model Formalism Estimation Role
English syntax (?3.1) bilexical dependency discriminative estimation combines with Korean
grammar syntax for bilingual parsing
Korean morphology (?3.2) two-sequence discriminative estimation best analysis used as input
trigram model over a lattice to TM training and to parsing
Korean syntax (?3.3) PCFG smoothed MLE combines with English
syntax for bilingual parsing
Translation model (?3.4) IBM models 1?4, unsupervised estimation best analysis used as
both directions (approximation to EM) input to bilingual parsing
Table 1: A summary of the factored models described in this paper and their interactions.
?TP ,WP , TA, TC ,WC , D? ?TP , TA, TC , D?
?TP , TA, TC ,WC , D? ?TP ,WP , TA, TC , D?
?TP ,WP , stop, TC , D? ?TP , stop, TC , D?
Table 2: Feature templates used by the English depen-
dency parser. TX is a tag and WX is a word. P indi-
cates the parent, A the previous child, and C the next-
generated child. D is the direction (left or right). The
last two templates correspond to stopping.
0. If a feature is never seen in training data, we give it
the same weight as the minimum-valued feature from the
training set (?min). To handle out-of-vocabulary (OOV)
words, we treat any word seen for the first time in the
final 300 sentences of the training corpus as OOV. The
model is smoothed using a Gaussian prior with unit vari-
ance on every weight.
Because the left and right children of a parent are in-
dependent of each other, our model can be described as
a weighted split head automaton grammar (Eisner and
Satta, 1999). This allowed us to use Eisner and Satta?s
O(n3) parsing algorithm to speed up training.4 This
speedup could not, however, be applied to the bilingual
parsing algorithm since a split parsing algorithm will pre-
clude inference of certain configurations of word align-
ments that are allowed by a non-split parser (Melamed,
2003).
We trained the parser on sentences of 15 words or
fewer in the WSJ Treebank sections 01?21.5 99.49% de-
pendency attachment accuracy was achieved on the train-
ing set, and 76.68% and 75.00% were achieved on sec-
tions 22 and 23, respectively. Performance on the En-
glish side of our KTB test set was 71.82% (averaged
across 5 folds, ? = 1.75).
This type of discriminative training has been applied
to log-linear variants of hidden Markov models (Lafferty
et al, 2001) and to lexical-functional grammar (Johnson
et al, 1999; Riezler et al, 2002). To our knowledge, it
has not been explored for context-free models (includ-
ing bilexical dependency models like ours). A review
4Our split HAG?s head automaton states correspond to the POS tags
of the dependent words; this makes the head automaton deterministic
and offers an additional speedup.
5The parser does not model POS-tags; we assume they are known.
Head words in the WSJ corpus were obtained using R. Hwa?s
const2dep tool.
of discriminative approaches to parsing can be found in
Chiang (2003).
3.2 Korean morphological analysis
A Korean word typically consists of a head morpheme
followed by a series of closed-class dependent mor-
phemes such as case markers, copula, topicalizers, and
conjunctions. Since most of the semantic content re-
sides in the leading head morpheme, we eliminate for
word alignment all trailing morphemes, which reduces
the KTB?s vocabulary size from 10,052 to 3,104.
Existing morphological processing tools for many lan-
guages are often unweighted finite-state transducers that
encode the possible analyses for a surface form word.
One such tool, klex, is available for Korean (Han,
2004).
Unfortunately, while the unweighted FST describes
the set of valid analyses, it gives no way to choose
among them. We treat this as a noisy channel: Korean
morpheme-tag pairs are generated in sequence by some
process, then passed through a channel that turns them
into Korean words (with loss of information). The chan-
nel is given by the FST, but without any weights. To
select the best output, we model the source process.
We model the sequence of morphemes and their tags
as a log-linear trigram model. Overlapping trigram, bi-
gram, and unigram features provide backoff information
to deal with data sparseness (Table 3). For each training
sentence, we used the FST-encoded morphological dic-
tionary to construct a lattice of possible analyses. The
lattice has a ?sausage? form with all paths joining be-
tween each word.
We train the feature weights to maximize the weight
of the correct path relative to all paths in the lattice. In
contrast, Lafferty et al (2001) train to maximize the the
probability of the tags given the words. Over training
sentences, maximize:
?
i
Pr(Ti,Mi|lattice)
=
?
i
exp(f(Ti,Mi) ? ?)
?
(T ?,M ?)?lattice exp(f(T
?,M ?) ? ?)
(3)
where Ti is the correct tagging for sentence i, Mi is the
correct morpheme sequence.
There are a few complications. First, the coverage of
the FST is of course not universal; in fact, it cannot ana-
lyze 4.66% of word types (2.18% of tokens) in the KTB.
?Ti?2,Mi?2, Ti?1,Mi?1, Ti,Mi? ?Ti?
?Ti?2,Mi?2, Ti?1,Mi?1, Ti? ?Ti,Mi?
?Ti?2, Ti?1,Mi?1, Ti,Mi? ?Ti?1, Ti?
?Ti?2, Ti?1,Mi?1, Ti, ? ?Ti?1, Ti,Mi?
?Ti?2, Ti?1, Ti,Mi? ?Ti?1,Mi?1, Ti, ?
?Ti?1,Mi?1, Ti,Mi? ?Ti?2, Ti?1, Ti?
Table 3: Feature templates used by the Korean morphol-
ogy model. Tx is a tag, Mx is a morpheme.
We tag such words as atomic common nouns (the most
common tag). Second, many of the analyses in the KTB
are not admitted by the FST: 21.06% of correct analy-
ses (by token) are not admitted by the FST; 6.85% do
not have an FST analysis matching in the first tag and
morpheme, 3.63% do not have an FST analysis matching
the full tag sequence, and 1.22% do not have an analysis
matching the first tag. These do not include the 2.18%
of tokens with no analysis at all. When this happened in
training, we added the correct analysis to the lattice.
To perform inference on new data, we construct a lat-
tice from the FST (adding in any analyses of the word
seen in training) and use a dynamic program (essentially
the Viterbi algorithm) to find the best path through the
lattice. Unseen features are given the weight ?min. Ta-
ble 4 shows performance on ambiguous tokens in train-
ing and test data (averaged over five folds).
3.3 Korean syntax model
Because we are using small training sets, parameter esti-
mates for a lexicalized Korean probabilistic grammar are
likely to be highly unreliable due to sparse data. There-
fore we use an unlexicalized PCFG. Because the POS
tags are given by the morphological analyzer, the PCFG
need not predict words (i.e., head morphemes), only POS
tags.
Rule probabilities were estimated with MLE. Since
only the sentence nonterminal S was smoothed (using
add-0.1), the grammar could parse any sequence of tags
but was relatively sparse, which kept bilingual run-time
down. 6
When we combine the PCFG with the other models to
do joint bilingual parsing, we simply use the logs of the
PCFG probabilities as if they were log-linear weights.
A PCFG treated this way is a perfectly valid log-linear
model; the exponentials of its weights just happen to sat-
isfy certain sum-to-one constraints.
In the spirit of joint optimization, we might have also
combined the Korean morphology and syntax models
into one inference task. We did not do this, largely out of
concerns over computational expense (see the discussion
of translation models in ?3.4). This parser, independent
of the bilingual parser, is evaluated in ?4.
6We also found that this type of smoothing and smoothing all non-
terminals gave indistinguishable results on monolingual parsing. Al-
ternatively, we could have trained the PCFG discriminatively (treating
the PCFG rules as log-linear features), but because our training sets are
small we do not expect such training to be very different from training
the PCFG as a generative model with probabilities.
3.4 Translation model
In our bilingual parser, the English and Korean parses are
mediated through word-to-word translational correspon-
dence links. Unlike the syntax models, the translation
models were trained without the benefit of labeled data.
We used the GIZA++ implementation of the IBM statisti-
cal translation models (Brown et al, 1993; Och and Ney,
2003).
To obtain reliable word translation estimates, we
trained on a bilingual corpus in addition to the KTB
training set. The Foreign Broadcast Information Service
dataset contains about 99,000 sentences of Korean and
72,000 of English translation. For our training, we ex-
tracted a relatively small parallel corpus of about 19,000
high-confidence sentence pairs.
As noted above, Korean?s productive agglutinative
morphology leads to sparse estimates of word frequen-
cies. We therefore trained our translation models af-
ter replacing each Korean word with its first morpheme
stripped of its closed-class dependent morphemes, as de-
scribed in ?3.2.
The size of the translation tables made optimal bilin-
gual parsing prohibitive by exploding the number of
possible analyses. We therefore resorted to using
GIZA++?s hypothesized alignments. Since the IBM
models only hypothesize one-to-many alignments from
target to source, we trained using each side of the bitext
as source and target in turn. We could then produce two
kinds of alignment graphs by taking either the intersec-
tion or the union of the links in the two GIZA++ align-
ment graphs. All words not in the resulting alignment
graph are set to align to ?.
Our bilingual parser deals only in one-to-one align-
ments (mappings); the intersection graph yields a map-
ping. The union graph yields a set of links which may
permit different one-to-one mappings. Using the union
graph therefore allows for flexibility in the word align-
ments inferred by the bilingual parser, but this comes at
computational expense (because more analyses are per-
mitted).
Even with over 20,000 sentence pairs of training data,
the hypothesized alignments are relatively sparse. For
the intersection alignments, an average of 23% of non-
punctuation Korean words and 17% of non-punctuation
English words have a link to the other language. For the
union alignments, this improves to 88% for Korean and
22% for English.
A starker measure of alignment sparsity is the accu-
racy of English dependency links projected onto Korean.
Following Hwa et al (2002), we looked at dependency
links in the true English parses from the KTB where both
the dependent and the head were linked to words on the
Korean side using the intersection alignment. Note that
Hwa et al used not only the true English trees, but also
hand-produced alignments. If we hypothesize that, if En-
glish words i and j are in a parent-child relationship,
then so are their linked Korean words, then we infer an
incomplete dependency graph for the Korean sentences
whose precision is around 49%?53% but whose recall is
Training sentences All tags All morphemes First tag First morpheme
Training set accuracy 32 91.14 (1.41) 94.25 (2.59) 91.14 (1.41) 95.74 (2.49)
on ambiguous tokens 64 89.76 (0.34) 93.39 (1.12) 89.76 (0.34) 95.23 (1.43)
128 88.19 (0.91) 92.48 (1.25) 88.38 (1.08) 94.43 (1.02)
512 83.69 (0.94) 89.59 (0.27) 85.03 (1.08) 91.95 (0.21)
1024 82.55 (0.68) 89.28 (0.30) 84.22 (0.77) 91.67 (0.19)
Test set accuracy 32 59.34 (2.52) 53.13 (2.09) 72.81 (1.96) 84.99 (3.11)
on ambiguous tokens 64 59.34 (2.41) 54.76 (1.64) 72.68 (1.79) 85.54 (2.03)
128 60.85 (2.15) 57.20 (2.01) 74.44 (1.17) 86.29 (1.14)
512 63.99 (2.02) 63.24 (1.28) 75.14 (0.86) 85.82 (1.01)
1024 65.26 (1.85) 66.03 (1.72) 75.22 (1.25) 85.62 (1.08)
Table 4: Korean morphological analysis accuracy on ambiguous tokens in the training and test sets: means (and
standard deviations) are shown over five-fold cross-validation. Over 65% of word tokens are ambiguous. The accuracy
of the first tag in each word affects the PCFG and the accuracy of the first morpheme affects the translation model
(under our aggressive morphological lemmatization).
an abysmal 2.5%?3.6%. 7
4 Evaluation
Having trained each part of the model, we bring them
together in a unified dynamic program to perform infer-
ence on the bilingual text as described in ?2. In order to
experiment easily with different algorithms, we imple-
mented all the morphological disambiguation and pars-
ing models in this paper in Dyna, a new language for
weighted dynamic programming (Eisner et al, 2004).
For parameter estimation, we used the complementary
DynaMITE tool. Just as CKY parsing starts with words
in its chart, the dynamic program chart for the bilingual
parser is seeded with the links given in the hypothesized
word alignment.
All our current results are optimal under the model,
but as we scale up to more complex data, we might in-
troduce A? heuristics or, at the possible expense of opti-
mality, a beam search or pruning techniques. Our agenda
discipline is uniform-cost search, which guarantees that
the first full parse discovered will be optimal?if none of
the weights are positive. In our case we are maximizing
sums of negative weights, as if working with log proba-
bilities.8
When evaluating our parsing output against the test
data from the KTB, we do not claim credit for the sin-
gle outermost bracketing or for unary productions. Since
unary productions do not translate well from language to
language (Hwa et al, 2002), we collapse them to their
lower nodes.
4.1 Baseline systems
We compare our bilingual parser to several baseline sys-
tems. The first is the Korean PCFG trained on the small
7We approximated head-words in the Korean gold-standard trees
by assuming all structures to be head-final, with the exception of punc-
tuation. That is, the head-words of sister constituents will elect the
right-most, non-punctuation word among them as the head.
8In fact the English syntax model is not constrained to have non-
positive weights, but we decrement every parameter by ?max. For a
given sentence, this will reduce every possible parse?s weight by a con-
stant value, since the same number of features fire in every parse; thus,
the classification properties of the parser are not affected.
KTB training sets, as described in ?3.3. We also consider
Wu?s (1997) stochastic inversion transduction grammar
(SITG) as well as strictly left- and right-branching trees.
We report the results of five-fold cross-validation with
the mean and standard deviation (in parentheses).
Since it is unlexicalized, the PCFG parses sequences
of tags as output by the morphological analysis model.
By contrast, we can build translation tables for the SITG
directly from surface words?and thus not use any la-
beled training data at all?or from the sequence of head
morphemes. Experiments showed, however, that the
SITG using words consistently outperformed the SITG
using morphemes. We also implemented Wu?s tree-
transformation algorithm to turn full binary-branching
SITG output into flatter trees. Finally, we can provide
extra information to the SITG by giving it a set of En-
glish bracketings that it must respect when constructing
the joint tree. To get an upper bound on performance, we
used the true parses from the English side of the KTB.
Only the PCFG, of course, can be evaluated on la-
beled bracketing (Table 6). Although labeled precision
and recall on test data generally increase with more train-
ing data, the slightly lower performance at the highest
training set size may indicate overtraining of this simple
model. Unlabeled precision and recall show continued
improvement with more Korean training data.
Even with help from the true English trees, the unsu-
pervised SITGs underperform PCFGs trained on as few
as 32 sentences, with the exception of unlabeled recall in
one experiment. It seems that even some small amount
of knowledge of the language helps parsing. Crossing
brackets for the flattened SITG parses are understandably
lower.
4.2 Bilingual parsing
The output of our bilingual parser contains three types of
constituents: English-only (aligned to ?), Korean-only
(aligned to ?), and bilingual. The Korean parse induced
by the Korean-only and bilingual constituents is filtered
so constituents with intermediate labels (generated by the
binarization process) are eliminated.
A second filter we consider is to keep only the (re-
maining) bilingual constituents corresponding to an En-
glish head word?s maximal span. This filter will elimi-
nate constituents whose English correspondent is a head
word with some (but not all) of its dependents. Such par-
tial English constituents are by-products of the parsing
and do not correspond to the modeled syntax.
With good word alignments, the English parser can
help disambiguate Korean phrase boundaries and over-
come erroneous morphological analyses (Table 5). Re-
sults without and with the second filter are shown in
Table 7. Because larger training datasets lead to larger
PCFGs (with more rules), the grammar constant in-
creases. Our bilingual parser implementation is on the
cusp of practicality (in terms of memory requirements);
when the grammar constant increased, we were unable
to parse longer sentences. Therefore the results given for
bilingual parsing are on reduced test sets, where a length
filter was applied: sentences with |E| + |F | > ? were
removed, for varying values of ? .
4.3 Discussion
While neither bilingual parser consistently beats the
PCFG on its own, they offer slight, complementary im-
provements on small training datasets of 32 and 64 sen-
tences (Table 7). The bilingual parser without the En-
glish head span filter gives a small recall improvement
on average at similar precision. Neither of these differ-
ences is significant when measured with a paired-sample
t-test.
In contrast, the parser with the English head span filter
sacrifices significantly on recall for a small but signifi-
cant gain in precision at the 0.01 level. Crossing brack-
ets at all levels are significantly lower with the English
head span filter. We can describe this effect as a filtering
of Korean constituents by the English model and word
alignments. Constituents that are not strongly evident on
the English side are simply removed. On small train-
ing datasets, this effect is positive: although good con-
stituents are lost so that recall is poor compared to the
PCFG, precision and crossing brackets are improved.
As one would expect, as the amount of training data
increases, the advantage of using a bilingual parser
vanishes?there is no benefit from falling back on the
English parser and word alignments to help disambiguate
the Korean structure. Since we have not pruned our
search space in these experiments, we can be confident
that all variations are due to the influence of the transla-
tion and English syntax models.
Our approach has this principal advantage: the various
morphology, parsing, and alignment components can be
improved or replaced easily without needing to retrain
the other modules. The low dependency projection re-
sults (?3.4), in conjunction with our modest overall gains,
indicate that the alignment/translation model should re-
ceive the most attention. In all the bilingual experiments,
there is a small positive correlation (0.3), for sentences
at each length, between the proportion of Korean words
aligned to English and measures of parsing accuracy. Im-
proved English parsers?such as Collins? models?have
also been implemented in Dyna, the dynamic program-
ming framework used here (Eisner et al, 2004).
5 Prior work
Combining separately trained systems and then search-
ing for an (ideally) optimal solution is standard prac-
tice in statistical continuous speech recognition (Jelinek,
1998) and statistical machine translation (Brown et al,
1990). Composition is even more of a staple in finite-
state frameworks (Knight and Graehl, 1998). Finally,
factored models involving parses have been used to guide
search. Charniak et al (2003) combine separately
trained parse production probabilities with translation
probabilities to prune a parse forest hypothesized by the
translation model. As discussed in ?2, Klein and Man-
ning (2002) guide their parser?s search using a combina-
tion of separate unlexicalized PCFG and lexical depen-
dency models.
The extent to which assumptions about similarity of
syntax across languages are empirically valid has re-
ceived attention in a few pilot studies. Fox (2002) has
considered English and French, and Hwa et al (2002) in-
vestigate Chinese and English. Xia et al (2000) compare
the rule templates of lexicalized tree adjoining grammars
extracted from treebanks in English, Chinese, and Ko-
rean. In the context of machine translation, Dorr (1994)
investigated divergences between two languages? struc-
tures.
Some proposals have sidestepped the empirical issue
entirely. Wu (1997) and Alshawi et al (2000) used un-
supervised learning on parallel text to induce syntactic
analysis that was useful for their respective applications
in phrasal translation extraction and speech translation,
though not necessarily similar to what a human anno-
tator would select. Note a point of divergence of the
SITG from our bilingual parsing system: SITG only al-
lows words, but not higher structures, to match null in the
other language and thus requires that the trees in parallel
sentences be isomorphic. Yamada and Knight (2001) in-
troduced tree-to-string alignment on Japanese data, and
Gildea (2003) performed tree-to-tree alignment on the
Korean Treebank, allowing for non-isomorphic struc-
tures; he applied this to word-to-word alignment. Fi-
nally, inspired by these intuitive notions of translational
correspondence, Cherry and Lin (2003) include depen-
dency features in a word alignment model to improve
non-syntactic baseline systems.
In more formal work, Melamed (2003) proposes
multitext grammars and algorithms for parsing them.
Shieber and Schabes (1990) describe a synchronous tree
adjoining grammar. While both of these formalisms re-
quire bilingual grammar rules, Eisner (2003) describes
an algorithm for learning tree substitution grammars
from unaligned trees.
Working on the Penn Korean Treebank, Sarkar and
Han (2002) made a single training/test split and used
91% of the sentences to train a morphological disam-
biguator and lexicalized tree adjoining grammar (LTAG)
based parsing system.
For a monolingual approach to training a parser with
scarce resources, see (Steedman et al, 2003), who apply
co-training and corrected co-training to bootstrapping an
English parser starting with 1000 parsed training sen-
Truth [TOP [NP ngyen.tay/NNC kong.pyeng/NNC cwung.tay/NNC] [VP [NP ku/DAN to/NNC] ken.sel/NNC] ./SFN]
PCFG [TOP ngyen.tay/VV [S [NP kong.pyeng/NNC cwung.tay/NNC] [VP [NP ku/NPN to/NNX] ken.sel/NNC] ./SFN]]
Bilingual [TOP [NP ngyen.tay/VV kong.pyeng/NNC1 cwung.tay/NNC] [VP [NP ku/NPN to/NNX] ken.sel/NNC] ./SFN2]
Translation The regimental1 engineer company constructed that road .2
Truth [TOP [NP ku/DAN sa.lam/NNC] [NP ceng.chi/NNC kwun.kwan/NNC] ?/SFN]
PCFG [TOP [VP [NP ku/DAN sa.lam/NNC ceng.chi/NNC] kwun.kwan/NNC] ?/SFN]
Bilingual [TOP [NP ku/DAN1 sa.lam/NNC] [NP ceng.chi/NNC2 kwun.kwan/NNC3] ?/SFN4]
Translation He1 is a political2 officer3 ?4
Table 5: The gold-standard parse, PCFG parse, bilingual parse, and English translation for two selected test sentences.
GIZA-aligned words are coindexed with subscripts. The bilingual parser recovers from erroneous morphological
tagging in the first sentence and finds the proper NP bracketing in the second.
Method Training Unlabeled Unlabeled Labeled Labeled Crossing
Sentences Precision Recall Precision Recall Brackets
PCFG training 32 57.03 (5.45) 78.45 (5.71) 51.13 (6.14) 70.26 (6.40) 0.71 (0.22)
64 54.96 (4.98) 76.91 (6.71) 46.94 (4.38) 65.69 (5.99) 0.72 (0.25)
128 52.60 (3.15) 73.20 (4.97) 43.46 (3.34) 60.48 (5.14) 0.82 (0.18)
512 50.82 (1.46) 70.98 (2.00) 39.47 (2.49) 55.12 (3.42) 0.87 (0.06)
1024 50.25 (0.82) 70.31 (1.32) 37.93 (1.45) 53.07 (2.16) 0.89 (0.04)
PCFG test 32 43.63 (4.40) 45.96 (5.38) 31.67 (3.47) 33.36 (4.19) 1.27 (0.16)
64 45.90 (2.30) 46.68 (2.92) 34.29 (2.35) 34.91 (3.22) 1.18 (0.12)
128 48.07 (4.14) 48.47 (4.45) 36.39 (3.37) 36.68 (3.50) 1.15 (0.14)
512 50.88 (2.97) 51.89 (2.92) 38.10 (3.22) 38.82 (2.68) 1.10 (0.10)
1024 51.15 (2.17) 52.65 (1.74) 37.47 (1.89) 38.58 (1.64) 1.12 (0.08)
SITG ? 30.65 (1.97) 45.22 (3.43) ? ? 1.93 (0.17)
Flat SITG ? 41.78 (1.98) 33.59 (3.36) ? ? 0.94 (0.08)
SITG w/Eng. constit. ? 36.28 (0.70) 52.68 (1.03) ? ? 1.60 (0.07)
Flat SITG w/Eng. constit. ? 42.55 (1.32) 30.64 (1.37) ? ? 0.77 (0.06)
L-branching ? 25.62 (1.07) 35.83 (1.39) ? ? 2.04 (0.04)
R-branching ? 27.59 (1.03) 38.60 (1.75) ? ? 2.06 (0.11)
Table 6: Baseline parsing performance on Korean: the table shows means (and standard deviations) for five-fold cross-
validation. The SITG system is evaluated on test data, but is trained without labeled data; the SITG with English trees
uses true treebank English parses to constrain the search and thus represents an upper bound. The table shows means
and standard deviations for five-fold cross-validation. The best test results in each column are in bold.
Method Max. |E| + |F | Training Unlabeled Unlabeled Labeled Labeled Crossing
Test Sen. Length Sentences Precision Recall Precision Recall Brackets
PCFG 20 32 44.19 (4.41) 46.51 (5.32) 32.10 (3.47) 33.78 (4.14) 1.23 (0.16)
20 64 46.39 (2.45) 47.03 (3.01) 34.69 (2.40) 35.20 (3.22) 1.15 (0.11)
18 128 49.86 (4.83) 49.63 (4.74) 37.78 (3.74) 37.60 (3.61) 1.03 (0.13)
17 512 53.89 (3.60) 54.60 (3.73) 40.61 (3.84) 41.10 (3.19) 0.87 (0.11)
15 1024 57.87 (3.75) 59.39 (3.35) 43.92 (3.52) 45.07 (3.26) 0.61 (0.09)
Bilingual 20 32 44.17 (3.97) 47.10 (4.81) 31.67 (3.65) 33.78 (4.29) 1.22 (0.14)
parsing 20 64 46.30 (2.46) 47.73 (2.83) 34.14 (2.60) 35.23 (3.35) 1.15 (0.12)
18 128 48.75 (3.64) 49.51 (4.08) 36.95 (2.65) 37.52 (2.92) 1.04 (0.10)
17 512 52.77 (3.92) 54.21 (4.42) 39.73 (3.68) 40.78 (3.56) 0.88 (0.12)
15 1024 56.70 (4.79) 58.85 (4.10) 43.09 (4.24) 44.71 (3.69) 0.60 (0.12)
Bilingual 20 32 45.65 (5.81) 28.83 (4.35) 32.92 (4.60) 20.82 (3.53) 0.72 (0.11)
parsing, 20 64 47.15 (2.88) 28.73 (1.79) 34.65 (2.36) 21.14 (1.73) 0.68 (0.08)
English 18 128 49.65 (4.52) 28.74 (2.30) 38.62 (3.69) 22.35 (1.76) 0.59 (0.09)
head span 17 512 52.03 (4.21) 29.47 (2.71) 39.80 (2.92) 22.51 (1.32) 0.50 (0.08)
filter 15 1024 54.78 (5.20) 29.74 (1.91) 42.01 (5.05) 22.78 (1.84) 0.34 (0.09)
Table 7: Bilingual parsing performance on Korean: the table shows means (and standard deviations) for five-fold cross-
validation. Bold-faced numbers in the bilingual parsers indicate significant improvements on the PCFG baseline using
the paired-sample t-test at the 0.01 level.
tences. Although this technique has interesting proper-
ties, our combined optimization should be more stable
since it does not involve iterative example selection.
6 Conclusion
We have presented a novel technique for merging sim-
ple, separately trained models for Korean parsing, En-
glish dependency parsing, and word translation, and opti-
mizing the joint result using dynamic programming. We
showed small but significant improvements for Korean
parsers trained on small amounts of labeled data.
7 Acknowledgements
We would like to thank Elliott Dra?bek, Jason Eisner,
Eric Goldlust, Philip Resnik, Charles Schafer, David
Yarowsky, and the reviewers for their comments and as-
sistance and Chung-hye Han, Na-Rae Han, and Anoop
Sarkar for their help with the Korean resources. This
work was supported under a National Science Founda-
tion Graduate Research Fellowship and a Fannie and
John Hertz Foundation Fellowship.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learn-
ing dependency translation models as collections of
finite-state head transducers. Computational Linguis-
tics, 26(1):45?60.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.
Roossin. 1990. A statistical approach to machine trans-
lation. Computational Linguistics, 16(2):79?85.
P. E. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proc. MT Summit IX.
C. Cherry and D. Lin. 2003. A probability model to im-
prove word alignment. In Proc. ACL.
D. Chiang. 2003. Mildly context-sensitive grammars for
estimating maximum entropy models. In Proc. Formal
Grammar.
B. J. Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational
Linguistics, 20(4):597?633.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head automaton grammars.
In Proc. ACL.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna:
A declarative language for implementing dynamic pro-
grams. In ACL Companion Vol.
J. Eisner. 1996. An empirical comparison of probabil-
ity models for dependency grammar. Technical Report
IRCS-96-11, U. Penn.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL Companion Vol.
H. J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. EMNLP.
D. Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. ACL.
C.-H. Han, N.-R. Han, E.-S. Ko, H. Yi, and M. Palmer.
2002. Penn Korean Treebank: Development and evalu-
ation. In Proc. Pacific Asian Conf. Language and Comp.
N.-R. Han. 2004. Klex: Finite-state lexical trans-
ducer for Korean. http://wave.ldc.upenn.edu/Catalog/-
CatalogEntry.jsp?catalogId=LDC2004L01.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using annota-
tion projection. In Proc. ACL.
F. Jelinek. 1998. Statistical Methods for Speech Recogni-
tion. MIT Press, Cambridge, MA.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Rie-
zler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proc. ACL.
D. Klein and C. D. Manning. 2002. Fast exact natural
language parsing with a factored model. In NIPS.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4).
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
I. D. Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proc. HLT-NAACL.
F.-J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computational
Linguistics, 29(1):19?51.
S. Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell,
and M. Johnson. 2002. Parsing the WSJ using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. ACL.
A. Sarkar and C.-H. Han. 2002. Statistical morphological
tagging and parsing of Korean with an LTAG grammar.
In Proc. TAG+6, pages 48?56.
S. M. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In Proc. ACL, pages 253?258.
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sarkar,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003.
Example selection for bootstrapping statistical parsers.
In Proc. HLT-NAACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?404.
F. Xia, C.-H. Han, M. Palmer, and A. Joshi. 2000. Com-
paring lexicalized treebank grammars extracted from
Chinese, Korean, and English corpora. In Proc. 2nd
Chinese Language Processing Workshop.
K. Yamada and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proc. ACL.
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proc. NAACL.
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 30?41,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Parsing with Soft and Hard Constraints on Dependency Length?
Jason Eisner and Noah A. Smith
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{jason,nasmith}@cs.jhu.edu
Abstract
In lexicalized phrase-structure or dependency parses, a word?s
modifiers tend to fall near it in the string. We show that a crude
way to use dependency length as a parsing feature can sub-
stantially improve parsing speed and accuracy in English and
Chinese, with more mixed results on German. We then show
similar improvements by imposing hard bounds on dependency
length and (additionally) modeling the resulting sequence of
parse fragments. This simple ?vine grammar? formalism has
only finite-state power, but a context-free parameterization with
some extra parameters for stringing fragments together. We ex-
hibit a linear-time chart parsing algorithm with a low grammar
constant.
1 Introduction
Many modern parsers identify the head word of
each constituent they find. This makes it possible
to identify the word-to-word dependencies implicit
in a parse.1 (Some parsers, known as dependency
parsers, even return these dependencies as their pri-
mary output.)
Why bother to identify these dependencies? The
typical reason is to model the fact that some word
pairs are more likely than others to engage in a de-
pendency relationship.2 In this paper, we propose a
different reason to identify dependencies in candi-
date parses: to evaluate not the dependency?s word
pair but its length (i.e., the string distance between
the two words). Dependency lengths differ from
? This work was supported by NSF ITR grant IIS-0313193
to the first author and a fellowship from the Fannie and John
Hertz Foundation to the second author. The views expressed
are not necessarily endorsed by the sponsors. The authors thank
Mark Johnson, Eugene Charniak, Charles Schafer, Keith Hall,
and John Hale for helpful discussion and Elliott Dra?bek and
Markus Dreyer for insights on (respectively) Chinese and Ger-
man parsing. They also thank an anonymous reviewer for sug-
gesting the German experiments.
1In a phrase-structure parse, if phrase X headed by word
token x is a subconstituent of phrase Y headed by word token
y 6= x, then x is said to depend on y. In a more powerful
compositional formalism like LTAG or CCG, dependencies can
be extracted from the derivation tree.
2It has recently been questioned whether these ?bilexical?
features actually contribute much to parsing performance (Klein
and Manning, 2003; Bikel, 2004), at least when one has only a
million words of training.
typical parsing features in that they cannot be deter-
mined from tree-local information. Though lengths
are not usually considered, we will see that bilexical
dynamic-programming parsing algorithms can eas-
ily consider them as they build the parse.
Soft constraints. Like any other feature of trees,
dependency lengths can be explicitly used as fea-
tures in a probability model that chooses among
trees. Such a model will tend to disfavor long de-
pendencies (at least of some kinds), as these are em-
pirically rare. In the first part of the paper, we show
that such features improve a simple baseline depen-
dency parser.
Hard constraints. If the bias against long de-
pendencies is strengthened into a hard constraint
that absolutely prohibits long dependencies, then the
parser turns into a partial parser with only finite-state
power. In the second part of the paper, we show how
to perform chart parsing in asymptotic linear time
with a low grammar constant. Such a partial parser
does less work than a full parser in practice, and in
many cases recovers a more precise set of dependen-
cies (with little loss in recall).
2 Short Dependencies in Langugage
We assume that correct parses exhibit a ?short-
dependency preference?: a word?s dependents tend
to be close to it in the string.3 If the j th word of a sen-
tence depends on the ith word, then |i?j| tends to be
3 In this paper, we consider only a crude notion of ?close-
ness?: the number of intervening words. Other distance mea-
sures could be substituted or added (following the literature on
heavy-shift and sentence comprehension), including the phono-
logical, morphological, syntactic, or referential (given/new)
complexity of the intervening material (Gibson, 1998). In pars-
ing, the most relevant previous work is due to Collins (1997),
who considered three binary features of the intervening mate-
rial: did it contain (a) any word tokens at all, (b) any verbs,
(c) any commas or colons? Note that (b) is effective because
it measures the length of a dependency in terms of the number
of alternative attachment sites that the dependent skipped over,
a notion that could be generalized. Similarly, McDonald et al
(2005) separately considered each of the intervening POS tags.
30
small. This implies that neither i nor j is modified by
complex phrases that fall between i and j. In terms
of phrase structure, it implies that the phrases mod-
ifying word i from a given side tend to be (1) few
in number, (2) ordered so that the longer phrases fall
farther from i, and (3) internally structured so that
the bulk of each phrase falls on the side of j away
from i.
These principles can be blamed for several lin-
guistic phenomena. (1) helps explain the ?late clo-
sure? or ?attach low? heuristic (e.g., Frazier, 1979;
Hobbs and Bear, 1990): a modifier such as a PP is
more likely to attach to the closest appropriate head.
(2) helps account for heavy-shift: when an NP is
long and complex, take NP out, put NP on the ta-
ble, and give NP to Mary are likely to be rephrased
as take out NP, put on the table NP, and give Mary
NP. (3) explains certain non-canonical word orders:
in English, a noun?s left modifier must become a
right modifier if and only if it is right-heavy (a taller
politician vs. a politician taller than all her rivals4),
and a verb?s left modifier may extrapose its right-
heavy portion (An aardvark walked in who had cir-
cumnavigated the globe5).
Why should sentences prefer short dependencies?
Such sentences may be easier for humans to produce
and comprehend. Each word can quickly ?discharge
its responsibilities,? emitting or finding all its depen-
dents soon after it is uttered or heard; then it can
be dropped from working memory (Church, 1980;
Gibson, 1998). Such sentences also succumb nicely
to disambiguation heuristics that assume short de-
pendencies, such as low attachment. Thus, to im-
prove comprehensibility, a speaker can make stylis-
tic choices that shorten dependencies (e.g., heavy-
shift), and a language can categorically prohibit
some structures that lead to long dependencies (*a
taller-than-all-her-rivals politician; *the sentence
4Whereas *a politician taller and *a taller-than-all-her-
rivals politician are not allowed. The phenomenon is pervasive.
5This actually splits the heavy left dependent [an aardvark
who ...] into two non-adjacent pieces, moving the heavy second
piece. By slightly stretching the aardvark-who dependency in
this way, it greatly shortens aardvark-walked. The same is pos-
sible for heavy, non-final right dependents: I met an aardvark
yesterday who had circumnavigated the globe again stretches
aardvark-who, which greatly shortens met-yesterday. These ex-
amples illustrate (3) and (2) respectively. However, the resulting
non-contiguous constituents lead to non-projective parses that
are beyond the scope of this paper.
that another sentence that had center-embedding
was inside was incomprehensible).
Such functionalist pressures are not all-powerful.
For example, many languages use SOV basic word
order where SVO (or OVS) would give shorter de-
pendencies. However, where the data exhibit some
short-dependency preference, computer parsers as
well as human parsers can obtain speed and accu-
racy benefits by exploiting that fact.
3 Soft Constraints on Dependency Length
We now enhance simple baseline probabilistic
parsers for English, Chinese, and German so that
they consider dependency lengths. We confine our-
selves (throughout the paper) to parsing part-of-
speech (POS) tag sequences. This allows us to ig-
nore data sparseness, out-of-vocabulary, smoothing,
and pruning issues, but it means that our accuracy
measures are not state-of-the-art. Our techniques
could be straightforwardly adapted to (bi)lexicalized
parsers on actual word sequences, though not neces-
sarily with the same success.
3.1 Grammar Formalism
Throughout this paper we will use split bilexical
grammars, or SBGs (Eisner, 2000), a notationally
simpler variant of split head-automaton grammars,
or SHAGs (Eisner and Satta, 1999). The formalism
is context-free. We define here a probabilistic ver-
sion,6 which we use for the baseline models in our
experiments. They are only baselines because the
SBG generative process does not take note of de-
pendency length.
An SBG is an tuple G = (?, $, L,R). ? is an
alphabet of words. (In our experiments, we parse
only POS tag sequences, so ? is actually an alpha-
bet of tags.) $ 6? ? is a distinguished root symbol;
let ?? = ? ? {$}. L and R are functions from ??
to probabilistic ?-free finite-state automata over ?.
Thus, for each w ? ??, the SBG specifies ?left? and
?right? probabilistic FSAs, Lw and Rw.
We use Lw(G) : ??? ? [0, 1] to denote the prob-
abilistic context-free language of phrases headed by
w. Lw(G) is defined by the following simple top-
down stochastic process for sampling from it:
6There is a straightforward generalization to weighted
SBGs, which need not have a stochastic generative model.
31
1. Sample from the finite-state language L(Lw) a
sequence ? = w?1w?2 . . . w?` ? ?? of left
children, and from L(Rw) a sequence ? =
w1w2 . . . wr ? ?? of right children. Each se-
quence is found by a random walk on its proba-
bilistic FSA. We say the children depend on w.
2. For each i from ?` to r with i 6= 0, recursively
sample ?i ? ?? from the context-free language
Lwi(G). It is this step that indirectly determines
dependency lengths.
3. Return ??` . . . ??2??1w?1?2 . . . ?r ? ???, a
concatenation of strings.
Notice that w?s left children ? were generated in
reverse order, so w?1 and w1 are its closest children
while w?` and wr are the farthest.
Given an input sentence ? = w1w2 . . . wn ? ??,
a parser attempts to recover the highest-probability
derivation by which $? could have been generated
from L$(G). Thus, $ plays the role of w0. A sample
derivation is shown in Fig. 1a. Typically, L$ and
R$ are defined so that $ must have no left children
(` = 0) and at most one right child (r ? 1), the
latter serving as the conventional root of the parse.
3.2 Baseline Models
In the experiments reported here, we defined only
very simple automata for Lw and Rw (w ? ?).
However, we tried three automaton types, of vary-
ing quality, so as to evaluate the benefit of adding
length-sensitivity at three different levels of baseline
performance.
In model A (the worst), each automaton has topol-
ogy }  ff, with a single state q1, so token w?s left
dependents are conditionally independent of one an-
other given w. In model C (the best), each au-
tomaton }??}  ff has an extra state q0 that al-
lows the first (closest) dependent to be chosen dif-
ferently from the rest. Model B is a compromise:7
it is like model A, but each type w ? ? may
have an elevated or reduced probability of having
no dependents at all. This is accomplished by us-
ing automata }??}  ff as in model C, which al-
lows the stopping probabilities p(STOP | q0) and
p(STOP | q1) to differ, but tying the conditional dis-
7It is equivalent to the ?dependency model with valence? of
Klein and Manning (2004).
tributions p(q0 w??q1 | q0,?STOP) and p(q1 w??q1 |
q1,?STOP).
Finally, in ?3, L$ and R$ are restricted as above,
so R$ gives a probability distribution over ? only.
3.3 Length-Sensitive Models
None of the baseline models A?C explicitly model
the distance between a head and child. We enhanced
them by multiplying in some extra length-sensitive
factors when computing a tree?s probability. For
each dependency, an extra factor p(? | . . .) is mul-
tiplied in for the probability of the dependency?s
length ? = |i ? j|, where i and j are the positions
of the head and child in the surface string.8
Again we tried three variants. In one version, this
new probability p(?| . . .) is conditioned only on the
direction d = sign(i ? j) of the dependency. In
another version, it is conditioned only on the POS
tag h of the head. In a third version, it is conditioned
on d, h, and the POS tag c of the child.
3.4 Parsing Algorithm
Fig. 2a gives a variant of Eisner and Satta?s (1999)
SHAG parsing algorithm, adapted to SBGs, which
are easier to understand.9 (We will modify this al-
gorithm later in ?4.) The algorithm obtains O(n3)
runtime, despite the need to track the position of
head words, by exploiting the conditional indepen-
dence between a head?s left children and right chil-
dren. It builds ?half-constituents? denoted by @
(a head word together with some modifying phrases
on the right, i.e., w?1 . . . ?r) and   (a head word
together with some modifying phrases on the left,
i.e., ??` . . . ??1w). A new dependency is intro-
duced when @ +   are combined to get H
or
 (a pair of linked head words with all the
intervening phrases, i.e., w?1 . . . ?r???`? . . . ???1w?,
where w is respectively the parent or child of w?).
One can then combine H + @ = @ , or
8Since the ? values are fully determined by the tree but ev-
ery p(? | . . .) ? 1, this crude procedure simply reduces the
probability mass of every legal tree. The resulting model is de-
ficient (does not sum to 1); the remaining probability mass goes
to impossible trees whose putative dependency lengths ? are
inconsistent with the tree structure. We intend in future work
to explore non-deficient models (log-linear or generative), but
even the present crude approach helps.
9The SHAG notation was designed to highlight the connec-
tion to non-split HAGs.
32
  +  =   . Only O(n3) combinations
are possible in total when parsing a length-n sen-
tence.
3.5 A Note on Word Senses
[This section may be skipped by the casual reader.]
A remark is necessary about :w and :w? in Fig. 2a,
which represent senses of the words at positions
h and h?. Like past algorithms for SBGs (Eisner,
2000), Fig. 2a is designed to be a bit more general
and integrate sense disambiguation into parsing. It
formally runs on an input ? = W1 . . .Wn ? ??,
where each Wi ? ? is a ?confusion set? over pos-
sible values of the ith word wi. The algorithm re-
covers the highest-probability derivation that gener-
ates $? for some ? ? ? (i.e., ? = w1 . . . wn with
(?i)wi ?Wi).
This extra level of generality is not needed for any
of our experiments, but it is needed for SBG parsers
to be as flexible as SHAG parsers. We include it in
this paper to broaden the applicability of both Fig. 2a
and our extension of it in ?4.
The ?senses? can be used in an SBG to pass a
finite amount of information between the left and
right children of a word, just as SHAGs allow.10 For
example, to model the fronting of a direct object, an
SBG might use a special sense of a verb, whose au-
tomata tend to generate both one more noun in ? and
one fewer noun in ?.
Senses can also be used to pass information be-
tween parents and children. Important uses are
to encode lexical senses, or to enrich the de-
pendency parse with constituent labels or depen-
10Fig. 2a enhances the Eisner-Satta version with explicit
senses while matching its asymptotic performance. On this
point, see (Eisner and Satta, 1999, ?8 and footnote 6). How-
ever, it does have a practical slowdown, in that START-LEFT
nondeterministically guesses every possible sense of Wi, and
these senses are pursued separately. To match the Eisner-Satta
algorithm, we should not need to commit to a word?s sense un-
til we have seen all its left children. That is, left triangles and
left trapezoids should not carry a sense :w at all, except for the
completed left triangle (marked F) that is produced by FINISH-
LEFT. FINISH-LEFT should choose a sense w of Wh accord-
ing to the final state q, which reflects knowledge of Wh?s left
children. For this strategy to work, the transitions in Lw (used
by ATTACH-LEFT) must not depend on the particular sense w
but only on W . In other words, all Lw : w ? Wh are really
copies of a shared LWh , except that they may have different fi-
nal states. This requirement involves no loss of generality, since
the nondeterministic shared LWh is free to branch as soon as it
likes onto paths that commit to the various senses w.
dency labels (Eisner, 2000). For example, the in-
put token Wi = {bank1/N/NP , bank2/N/NP ,
bank3/V/VP , bank3/V/S} ? ? allows four
?senses? of bank, namely two nominal meanings,
and two syntactically different versions of the verbal
meaning, whose automata require them to expand
into VP and S phrases respectively.
The cubic runtime is proportional to the num-
ber of ways of instantiating the inference rules in
Fig. 2a: O(n2(n + t?)tg2), where n = |?| is the
input length, g = maxni=1 |Wi| bounds the size of
a confusion set, t bounds the number of states per
automaton, and t? ? t bounds the number of au-
tomaton transitions from a state that emit the same
word. For deterministic automata, t? = 1.11
3.6 Probabilistic Parsing
It is easy to make the algorithm of Fig. 2a length-
sensitive. When a new dependency is added by an
ATTACH rule that combines @ +   , the an-
notations on @ and   suffice to determine
the dependency?s length ? = |h ? h?|, direction
d = sign(h ? h?), head word w, and child word
w?.12 So the additional cost of such a dependency,
e.g. p(? | d,w,w?), can be included as the weight
of an extra antecedent to the rule, and so included in
the weight of the resulting  or H .
To execute the inference rules in Fig. 2a, we
use a prioritized agenda. Derived items such as
@ ,   ,

, and H are prioritized by
their Viterbi-inside probabilities. This is known
as uniform-cost search or shortest-hyperpath search
(Nederhof, 2003). We halt as soon as a full parse
(the accept item) pops from the agenda, since
uniform-cost search (as a special case of the A?
algorithm) guarantees this to be the maximum-
probability parse. No other pruning is done.
11Confusion-set parsing may be regarded as parsing a par-
ticular lattice with n states and ng arcs. The algorithm can
be generalized to lattice parsing, in which case it has runtime
O(m2(n + t?)t) for a lattice of n states and m arcs. Roughly,
h : w is replaced by an arc, while i is replaced by a state and
i? 1 is replaced by the same state.
12For general lattice parsing, it is not possible to determine ?
while applying this rule. There h and h? are arcs in the lattice,
not integers, and different paths from h to h? might cover dif-
ferent numbers of words. Thus, if one still wanted to measure
dependency length in words (rather than in, say, milliseconds
of speech), each item would have to record its width explicitly,
leading in general to more items and increased runtime.
33
With a prioritized agenda, a probability model
that more sharply discriminates among parses will
typically lead to a faster parser. (Low-probability
constituents languish at the back of the agenda and
are never pursued.) We will see that the length-
sensitive models do run faster for this reason.
3.7 Experiments with Soft Constraints
We trained models A?C, using unsmoothed maxi-
mum likelihood estimation, on three treebanks: the
Penn (English) Treebank (split in the standard way,
?2?21 train/?23 test, or 950K/57K words), the Penn
Chinese Treebank (80% train/10% test or 508K/55K
words), and the German TIGER corpus (80%/10%
or 539K/68K words).13 Estimation was a simple
matter of counting automaton events and normaliz-
ing counts into probabilities. For each model, we
also trained the three length-sensitive versions de-
scribed in ?3.3.
The German corpus contains non-projective trees.
None of our parsers can recover non-projective de-
pendencies (nor can our models produce them). This
fact was ignored when counting events for maxi-
mum likelihood estimation: in particular, we always
trained Lw and Rw on the sequence of w?s immedi-
ate children, even in non-projective trees.
Our results (Tab. 1) show that sharpening the
probabilities with the most sophisticated distance
factors p(? | d, h, c), consistently improved the
speed of all parsers.14 The change to the code is
trivial. The only overhead is the cost of looking up
and multiplying in the extra distance factors.
Accuracy also improved over the baseline mod-
els of English and Chinese, as well as the simpler
baseline models of German. Again, the most so-
phisticated distance factors helped most, but even
the simplest distance factor usually obtained most
of the accuracy benefit.
German model C fell slightly in accuracy. The
speedup here suggests that the probabilities were
sharpened, but often in favor of the wrong parses.
We did not analyze the errors on German; it may
13Heads were extracted for English using Michael Collins?
rules and Chinese using Fei Xia?s rules (defaulting in both cases
to right-most heads where the rules fail). German heads were
extracted using the TIGER Java API; we discarded all resulting
dependency structures that were cyclic or unconnected (6%).
14We measure speed abstractly by the number of items built
and pushed on the agenda.
be relevant that 25% of the German sentences con-
tained a non-projective dependency between non-
punctuation tokens.
Studying the parser output for English, we found
that the length-sensitive models preferred closer at-
tachments, with 19.7% of tags having a nearer parent
in the best parse under model C with p(? | d, h, c)
than in the original model C, 77.7% having a par-
ent at the same distance, and only 2.5% having a
farther parent. The surviving long dependencies (at
any length > 1) tended to be much more accurate,
while the (now more numerous) length-1 dependen-
cies were slightly less accurate than before.
We caution that length sensitivity?s most dramatic
improvements to accuracy were on the worse base-
line models, which had more room to improve. The
better baseline models (B and C) were already able
to indirectly capture some preference for short de-
pendencies, by learning that some parts of speech
were unlikely to have multiple left or multiple right
dependents. Enhancing B and C therefore con-
tributed less, and indeed may have had some harmful
effect by over-penalizing some structures that were
already appropriately penalized.15 It remains to
be seen, therefore, whether distance features would
help state-of-the art parsers that are already much
better than model C. Such parsers may already in-
corporate features that indirectly impose a good
model of distance, though perhaps not as cheaply.
4 Hard Dependency-Length Constraints
We have seen how an explicit model of distance can
improve the speed and accuracy of a simple proba-
bilistic dependency parser. Another way to capital-
ize on the fact that most dependencies are local is
to impose a hard constraint that simply forbids long
dependencies.
The dependency trees that satisfy this constraint
yield a regular string language.16 The constraint pre-
vents arbitrarily deep center-embedding, as well as
arbitrarily many direct dependents on a given head,
15Owing to our deficient model. A log-linear or discrimina-
tive model would be trained to correct for overlapping penalties
and would avoid this risk. Non-deficient generative models are
also possible to design, along lines similar to footnote 16.
16One proof is to construct a strongly equivalent CFG without
center-embedding (Nederhof, 2000). Each nonterminal has the
form ?w, q, i, j?, where w ? ?, q is a state of Lw or Rw, and
i, j ? {0, 1, . . . k?1,? k}. We leave the details as an exercise.
34
English (Penn Treebank) Chinese (Chinese Treebank) German (TIGER Corpus)
recall (%) runtime model recall (%) runtime model recall (%) runtime model
model train test test size train test test size train test test size
A (1 state) 62.0 62.2 93.6 1,878 50.7 49.3 146.7 782 70.9 72.0 53.4 1,598
+ p(? | d) 70.1 70.6 97.0 2,032 59.0 58.0 161.9 1,037 72.3 73.0 53.2 1,763
+ p(? | h) 70.5 71.0 94.7 3,091 60.5 59.1 148.3 1,759 73.1 74.0 48.3 2,575
+ p(? | d, h, c) 72.8 73.1 70.4 16,305 62.2 60.6 106.7 7,828 75.0 75.1 31.6 12,325
B (2 states, tied arcs) 69.7 70.4 93.5 2,106 56.7 56.2 151.4 928 73.7 75.1 52.9 1,845
+ p(? | d) 72.6 73.2 95.3 2,260 60.2 59.5 156.9 1,183 72.9 73.9 52.6 2,010
+ p(? | h) 73.1 73.7 92.1 3,319 61.6 60.7 144.2 1,905 74.1 75.3 47.6 2,822
+ p(? | d, h, c) 75.3 75.6 67.7 16,533 62.9 61.6 104.0 7,974 75.2 75.5 31.5 12,572
C (2 states) 72.7 73.1 90.3 3,233 61.8 61.0 148.3 1,314 75.6 76.9 48.5 2,638
+ p(? | d) 73.9 74.5 91.7 3,387 61.5 60.6 154.7 1,569 74.3 75.0 48.9 2,803
+ p(? | h) 74.3 75.0 88.6 4,446 63.1 61.9 141.9 2,291 75.2 76.3 44.3 3,615
+ p(? | d, h, c) 75.3 75.5 66.6 17,660 63.4 61.8 103.4 8,360 75.1 75.2 31.0 13,365
Table 1: Dependency parsing of POS tag sequences with simple probabilistic split bilexical grammars. The models differ only
in how they weight the same candidate parse trees. Length-sensitive models are larger but can improve dependency accuracy
and speed. (Recall is measured as the fraction of non-punctuation tags whose correct parent (if not the $ symbol) was correctly
recovered by the parser; it equals precision, unless the parser left some sentences unparsed (or incompletely parsed, as in ?4), in
which case precision is higher. Runtime is measured abstractly as the average number of items (i.e., @ ,   ,  , H )
built per word. Model size is measured as the number of nonzero parameters.)
either of which would allow the non-regular lan-
guage {anbcn : 0 < n < ?}. It does allow ar-
bitrarily deep right- or left-branching structures.
4.1 Vine Grammars
The tighter the bound on dependency length, the
fewer parse trees we allow and the faster we can find
them using the algorithm of Fig. 2a. If the bound
is too tight to allow the correct parse of some sen-
tence, we would still like to allow an accurate partial
parse: a sequence of accurate parse fragments (Hin-
dle, 1990; Abney, 1991; Appelt et al, 1993; Chen,
1995; Grefenstette, 1996). Furthermore, we would
like to use the fact that some fragment sequences are
presumably more likely than others.
Our partial parses will look like the one in Fig. 1b.
where 4 subtrees rather than 1 are dependent on $.
This is easy to arrange in the SBG formalism. We
merely need to construct our SBG so that the au-
tomaton R$ is now permitted to generate multiple
children?the roots of parse fragments.
This R$ is a probabilistic finite-state automaton
that describes legal or likely root sequences in ??.
In our experiments in this section, we will train it
to be a first-order (bigram) Markov model. (Thus
we construct R$ in the usual way to have |?| + 1
states, and train it on data like the other left and right
automata. During generation, its state remembers
the previously generated root, if any. Recall that we
are working with POS tag sequences, so the roots,
like all other words, are tags in ?.)
The 4 subtrees in Fig. 1b appear as so many
bunches of grapes hanging off a vine. We refer to
the dotted dependencies upon $ as vine dependen-
cies, and the remaining, bilexical dependencies as
tree dependencies.
One might informally use the term ?vine gram-
mar? (VG) for any generative formalism, intended
for partial parsing, in which a parse is a constrained
sequence of trees that cover the sentence. In gen-
eral, a VG might use a two-part generative process:
first generate a finite-state sequence of roots, then
expand the roots according to some more powerful
formalism. Conveniently, however, SBGs and other
dependency grammars can integrate these two steps
into a single formalism.
4.2 Feasible Parsing
Now, for both speed and accuracy, we will restrict
the trees that may hang from the vine. We define a
feasible parse under our SBG to be one in which all
tree dependencies are short, i.e., their length never
exceeds some hard bound k. The vine dependencies
may have unbounded length, of course, as in Fig. 1b.
Sentences with feasible parses form a regular lan-
guage. This would also be true under other defini-
tions of feasibility, e.g., we could have limited the
depth or width of each tree on the vine. However,
that would have ruled out deeply right-branching
trees, which are very common in language, and
35
(a) $ would
``````````````````
``````````````````
```
aaaaaaaaa
aaaaaaaaa
aa
eee
ee YYY
YY
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
[[[[
According
YYY
YY
, changes
bbbbb
bbbbb
eee
ee
cut
\\\\\
\\\\\
]]]]]]]
]]]]]]]
]
to
\\\\\
\\\\\
the rule filings
eee
ee
by
YYY
YY
.
estimates
eee
ee
insider more
YYY
YY
some than
\\\\\
\\\\\
third
eee
ee
a
(b) $ According
WW
WW
, would
gg
gg WW
WW
to
[[[[
[[[[
changes
cccc
cccc
gg
gg
cut
[[[[
[[[[
\\\\\
\\\\\
\
estimates
gg
gg
the rule filings
gg
gg
by
WW
WW
.
some insider more
WW
WW
than
[[[[
[[[[
third
gg
gg
a
Figure 1: (a) A dependency tree on words. (Our experiments use only POS tags.) (b) A partial
parse for the same sentence retaining only tree dependencies of length ? k = 3. The roots of the
4 resulting parse fragments are now connected only by their dotted-line ?vine dependencies? on $.
Transforming (a) into (b) involves grafting subtrees rooted at ?According?, ?,?, and ?.? onto the vine.
are also the traditional way to describe finite-state
sublanguages within a context-free grammar. By
contrast, our limitation on dependency length en-
sures regularity while still allowing (for any bound
k ? 1) arbitrarily wide and deep trees, such as
a? b? . . .? root? . . .? y ? z.
Our goal is to find the best feasible parse (if
any). Rather than transform the grammar as in foot-
note 16, our strategy is to modify the parser so that it
only considers feasible parses. The interesting prob-
lem is to achieve linear-time parsing with a grammar
constant that is as small as for ordinary parsing.
We also correspondingly modify the training data
so that we only train on feasible parses. That is, we
break any long dependencies and thereby fragment
each training parse (a single tree) into a vine of one
or more restricted trees. When we break a child-
to-parent dependency, we reattach the child to $.17
This process, grafting, is illustrated in Fig. 1. Al-
though this new parse may score less than 100% re-
call of the original dependencies, it is the best feasi-
ble parse, so we would like to train the parser to find
it.18 By training on the modified data, we learn more
17Any dependency covering the child must also be broken to
preserve projectivity. This case arises later; see footnote 25.
18Although the parser will still not be able to find it if it is
non-projective (possible in German). Arguably we should have
defined ?feasible? to also require projectivity, but we did not.
appropriate statistics for both R$ and the other au-
tomata. If we trained on the original trees, we would
inaptly learn that R$ always generates a single root
rather than a certain kind of sequence of roots.
For evaluation, we score tree dependencies in our
feasible parses against the tree dependencies in the
unmodified gold standard parses, which are not nec-
essarily feasible. We also show oracle performance.
4.3 Approach #1: FSA Parsing
Since we are now dealing with a regular language,
it is possible in principle to use a weighted finite-
state automaton (FSA) to search for the best feasible
parse. The idea is to find the highest-weighted path
that accepts the input string ? = w1w2 . . . wn. Us-
ing the Viterbi algorithm, this takes time O(n).
The trouble is that this linear runtime hides a con-
stant factor, which depends on the size of the rele-
vant part of the FSA and may be enormous for any
correct FSA.19
Consider an example from Fig 1b. Af-
ter nondeterministically reading w1 . . . w11 =
According. . . insider along the correct path, the FSA
state must record (at least) that insider has no parent
yet and that R$ and Rcut are in particular states that
19The full runtime is O(nE), where E is the number of FSA
edges, or for a tighter estimate, the number of FSA edges that
can be traversed by reading ?.
36
may still accept more children. Else the FSA cannot
know whether to accept a continuation w12 . . . wn.
In general, after parsing a prefix w1 . . . wj , the
FSA state must somehow record information about
all incompletely linked words in the past. It must
record the sequence of past words wi (i ? j) that
still need a parent or child in the future; if wi still
needs a child, it must also record the state of Rwi .
Our restriction to dependency length ? k is what
allows us to build a finite-state machine (as opposed
to some kind of pushdown automaton with an un-
bounded number of configurations). We need only
build the finitely many states where the incompletely
linked words are limited to at most w0 = $ and the k
most recent words, wj?k+1 . . . wj . Other states can-
not extend into a feasible parse, and can be pruned.
However, this still allows the FSA to be in
O(2ktk+1) different states after reading w1 . . . wj .
Then the runtime of the Viterbi algorithm, though
linear in n, is exponential in k.
4.4 Approach #2: Ordinary Chart Parsing
A much better idea for most purposes is to use a
chart parser. This allows the usual dynamic pro-
gramming techniques for reusing computation. (The
FSA in the previous section failed to exploit many
such opportunities: exponentially many states would
have proceeded redundantly by building the same
wj+1wj+2wj+3 constituent.)
It is simple to restrict our algorithm of Fig. 2a to
find only feasible parses. It is the ATTACH rules
@ +   that add dependencies: simply use a
side condition to block them from applying unless
|h? h?| ? k (short tree dependency) or h = 0 (vine
dependency). This ensures that all H and 
will have width ? k or have their left edge at 0.
One might now incorrectly expect runtime linear
in n: the number of possible ATTACH combinations
is reduced from O(n3) to O(nk2), because i and h?
are now restricted to a narrow range given h.
Unfortunately, the half-constituents @ and
  may still be arbitrarily wide, thanks to arbi-
trary right- and left-branching: a feasible vine parse
may be a sequence of wide trees   @ . Thus there
are O(n2k) possible COMPLETE combinations, not
to mention O(n2) ATTACH-RIGHT combinations
for which h = 0. So the runtime remains quadratic.
4.5 Approach #3: Specialized Chart Parsing
How, then, do we get linear runtime and a rea-
sonable grammar constant? We give two ways to
achieve runtime of O(nk2).
First, we observe without details that we can eas-
ily achieve this by starting instead with the algo-
rithm of Eisner (2000),20 rather than Eisner and
Satta (1999), and again refusing to add long tree de-
pendencies. That algorithm effectively concatenates
only trapezoids, not triangles. Each is spanned by a
single dependency and so has width ? k. The vine
dependencies do lead to wide trapezoids, but these
are constrained to start at 0, where $ is. So the algo-
rithm tries at most O(nk2) combinations of the form
h i+ i j (like the ATTACH combinations above)
and O(nk) combinations of the form 0 i + i j,
where i? h ? k, j ? i ? k. The precise runtime is
O(nk(k + t?)tg3).
We now propose a hybrid linear-time algorithm
that further improves runtime to O(nk(k + t?)tg2),
saving a factor of g in the grammar constant.21 We
observe that since within-tree dependencies must
have length ? k, they can all be captured within
Eisner-Satta trapezoids of width ? k. So our VG
parse   @ ? can be assembled by simply concate-
nating a sequence (    ? H ? @ )? of these
narrow trapezoids interspersed with width-0 trian-
gles. As this is a regular sequence, we can assem-
ble it in linear time from left to right (rather than in
the order of Eisner and Satta (1999)), multiplying
the items? probabilities together. Whenever we start
adding the right half H ? @ of a tree along the
vine, we have discovered that tree?s root, so we mul-
tiply in the probability of a $? root dependency.
Formally, our hybrid parsing algorithm restricts
the original rules of Fig. 2a to build only trapezoids
of width ? k and triangles of width < k.22 The
additional inference rules in Fig. 2b then assemble
the final VG parse as just described.
20With a small change that when two items are combined, the
right item (rather than the left) must be simple.
21This savings comes from building the internal structure of
a trapezoid from both ends inward rather than from left to right.
The corresponding unrestricted algorithms (Eisner, 2000; Eis-
ner and Satta, 1999, respectively) have exactly the same run-
times with k replaced by n.
22For the experiments of ?4.7, where k varied by type, we
restricted these rules as tightly as possible given h and h?.
37
(a)
ST
A
R
T-
LE
FT
:
w
?
W
h
q
?
in
it(
L
w
)
  
q
h
h
:w
1
?
h
?
n
ST
A
R
T-
R
IG
H
T:
q
?
in
it(
R
w
)
@@q
h
:w
h
  
F
i
h
:w
ST
A
R
T-
V
IN
E:
q
?
in
it(
R
$)
@@q
0
:$
0
FI
N
IS
H
-
LE
FT
:
  
q
i
h
:w
q
?
fin
al(
L
w
)
  
F
i
h
:w
FI
N
IS
H
-
R
IG
H
T:
@@q
h
:w
i
q
?
fin
al(
R
w
)
@@F
h
:w
i
EN
D
-
V
IN
E:
@@F
0
:$
n
ac
ce
pt
AT
TA
C
H
-
LE
FT
:
? ?
@@F
h
? :
w
?
i?
1
  
q
i
h
:w
? ?
q
w
?
??
r
?
L
w

r
h
? :
w
?
h
:w
AT
TA
C
H
-
R
IG
H
T:
? ?
@@q
h
:w
i?
1
  
F
i
h
? :
w
?? ?
q
w
?
??
r
?
R
w
HHr
h
? :
w
?
h
:w
C
O
M
PL
E
T
E
-
LE
FT
:
  
F
i
h
? :
w
?

q
h
? :
w
?
h
:w
  
q
i
h
:w
C
O
M
PL
E
T
E
-
R
IG
H
T:
HHq
h
? :
w
?
h
:w
@@F
h
? :
w
?
i
@@q
h
:w
i
Fi
gu
re
2:
(a)
A
n
al
go
rit
hm
th
at
pa
rs
es
W
1
..
.W
n
in
cu
-
bi
ct
im
eO
(n
2
(n
+
t?
)t
g2
).
A
da
pt
ed
w
ith
im
pr
ov
e-
m
en
ts
fro
m
(E
isn
er
an
d
Sa
tta
,1
99
9,
Fi
g.
3).
Th
e
pa
re
nt
he
se
s
in
th
e
AT
TA
C
H
ru
le
s
in
di
ca
te
th
e
de
-
du
ct
io
n
o
fa
n
in
te
rm
ed
ia
te
ite
m
th
at
?
fo
rg
et
s?
i.
(b)
If
th
e
AT
TA
C
H
ru
le
sa
re
re
st
ric
te
d
to
ap
pl
y
o
n
ly
w
he
n
ca
se
|h
?
h
? |
?
k
,
an
d
th
e
C
O
M
PL
E
T
E
ru
le
s
o
n
ly
w
he
n
|h
?
i|
<
k
,
th
en
th
e
ad
di
tio
na
l
ru
le
s
in
(b)
w
ill
as
se
m
bl
e
th
e
re
su
lti
ng
fra
gm
en
ts
in
to
a
v
in
e
pa
rs
e.
In
th
is
ca
se
,
AT
TA
C
H
-
R
IG
H
T
sh
ou
ld
al
so
be
re
st
ric
te
d
to
h
>
0,
to
pr
ev
en
t
du
pl
ic
at
e
de
riv
at
io
ns
.
Th
e
ru
n
tim
e
is
O
(n
k
(k
+
t?
)t
g2
),
do
m
in
at
ed
by
th
e
AT
TA
C
H
ru
le
s;
th
e
ru
le
s
in
(b)
re
qu
ire
o
n
ly
O
(n
k
tg
2
+
n
gt
t?
)
tim
e.
Ea
ch
al
go
rit
hm
is
sp
ec
ifi
ed
as
a
co
lle
ct
io
n
o
f
de
du
ct
iv
e
in
fe
re
nc
er
u
le
s.
O
nc
eo
n
e
ha
sd
er
iv
ed
al
l
an
te
ce
de
nt
ite
m
sa
bo
v
e
th
eh
or
iz
on
ta
ll
in
ea
n
d
an
y
sid
ec
o
n
di
tio
ns
to
th
er
ig
ht
o
ft
he
lin
e,
o
n
e
m
ay
de
-
riv
e
th
e
co
n
se
qu
en
ti
te
m
be
lo
w
th
e
lin
e.
W
ei
gh
te
d
ag
en
da
-b
as
ed
de
du
ct
io
n
is
ha
nd
le
d
in
th
e
u
su
al
w
ay
(N
ed
erh
of,
20
03
;E
isn
er
et
al
.,
20
05
).
Th
ep
ro
ba
bi
lit
ie
sg
ov
er
n
in
g
th
ea
u
to
m
at
on
L
w
,
n
am
el
y
p(
st
ar
ta
tq
),
p(
q
w
?
?
?
r
|
q)
,
an
d
p(
st
op
|
q)
,
ar
e
re
sp
ec
tiv
el
y
as
so
ci
at
ed
w
ith
th
e
ax
io
m
at
ic
ite
m
s
q
?
in
it(
L
w
),
q
w
?
?
?
r
?
L
w
,
an
d
q
?
fin
al(
L
w
).
A
n
ac
o
u
st
ic
sc
o
re
p(
o
bs
er
va
tio
n
at
h
|
w
)
co
u
ld
be
as
so
ci
at
ed
w
ith
th
e
ite
m
w
?
W
h
.
(b)
TR
E
E
-
ST
A
R
T:
@@q
0
:$
i
?
1
  
F
i
i:
w
 
@@q
i:
w
0
:$
TR
E
E
-
LE
FT
:
 
@@q
i:
w
0
:$

F
i:
w
j
:x
 
@@q
j
:x
0
:$
G
R
A
FT
-
V
IN
E:
 
@@q
i:
w
0
:$
q
w ??
r
?
R
$
XXy
XXyr
i:
w
0
:$
TR
E
E
-
R
IG
H
T:
XXy
XXyq
i:
w
0
:$
HHF
j
:x
i:
w
XXy
XXyq
j
:x
0
:$
TR
E
E
-
EN
D
:
XXy
XXyq
i:
w
0
:$
@@F
i:
w
i
@@q
0
:$
i
SE
A
L
-
LE
FT
:

q
h
? :
w
?
h
:w
q
?
fin
al(
L
w
)

F
h
? :
w
?
h
:w
SE
A
L
-
R
IG
H
T:
HHq
h
? :
w
?
h
:w
q
?
fin
al(
R
w
)
HHF
h
? :
w
?
h
:w
38
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0.3  0.4  0.5  0.6  0.7  0.8  0.9
recall
pre
cis
ion E
C
G
k = 1
Model C, no bound
single bound (English)
 (Chinese)
 (German)
Figure 3: Trading precision and recall: Imposing bounds can
improve precision at the expense of recall, for English and Chi-
nese. German performance suffers more. Bounds shown are
k = {1, 2, ..., 10, 15, 20}. The dotted lines show constant F -
measure of the unbounded model.
4.6 Experiments with Hard Constraints
Our experiments used the asymptotically fast hybrid
parsing algorithm above. We used the same left and
right automata as in model C, the best-performing
model from ?3.2. However, we now define R$ to
be a first-order (bigram) Markov model (?4.1). We
trained and tested on the same headed treebanks as
before (?3.7), except that we modified the training
trees to make them feasible (?4.2).
Results are shown in Figures 3 (precision/recall
tradeoff) and 4 (accuracy/speed tradeoff), for k ?
{1, 2, ..., 10, 15, 20}. Dots correspond to different
values of k. On English and Chinese, some values of
k actually achieve better F -measure accuracy than
the unbounded parser, by eliminating errors.23
We observed that changing R$ from a bigram
to a unigram model significantly hurt performance,
showing that it is in fact useful to empirically model
likely sequences of parse fragments.
4.7 Finer-Grained Hard Constraints
The dependency length bound k need not be a sin-
gle value. Substantially better accuracy can be re-
tained if each dependency type?each (h, c, d) =
(head tag, child tag, direction) tuple?has its own
23Because our prototype implementation of each kind of
parser (baseline, soft constraints, single-bound, and type-
specific bounds) is known to suffer from different inefficiencies,
runtimes in milliseconds are not comparable across parsers. To
give a general idea, 60-word English sentences parsed in around
300ms with no bounds, but at around 200ms with either a dis-
tance model p(?|d, h, c) or a generous hard bound of k = 10.
bound k(h, c, d). We call these type-specific bounds:
they create a many-dimensional space of possible
parsers. We measured speed and accuracy along a
sensible path through this space, gradually tighten-
ing the bounds using the following process:
1. Initialize each bound k(h, c, d) to the maximum
distance observed in training (or 1 for unseen
triples).24
2. Greedily choose a bound k(h, c, d) such that, if
its value is decremented and trees that violate the
new bound are accordingly broken, the fewest de-
pendencies will be broken.25
3. Decrement the bound k(h, c, d) and modify the
training data to respect the bound by breaking de-
pendencies that violate the bound and ?grafting?
the loose portion onto the vine. Retrain the parser
on the training data.
4. If all bounds are not equal to 1, go to step 2.
The performance of every 200th model along the
trajectory of this search is plotted in Fig. 4.26 The
graph shows that type-specific bounds can speed up
the parser to a given level with less loss in accuracy.
5 Related Work
As discussed in footnote 3, Collins (1997) and Mc-
Donald et al (2005) considered the POS tags inter-
vening between a head and child. These soft con-
straints were very helpful, perhaps in part because
they helped capture the short dependency preference
(?2). Collins used them as conditioning variables
and McDonald et al as log-linear features, whereas
our ?3 predicted them directly in a deficient model.
As for hard constraints (?4), our limitation on de-
pendency length can be regarded as approximating
a context-free language by a subset that is a regular
24In the case of the German TIGER corpus, which contains
non-projective dependencies, we first make the training trees
into projective vines by raising all non-projective child nodes to
become heads on the vine.
25Not counting dependencies that must be broken indirectly
in order to maintain projectivity. (If word 4 depends on word
7 which depends on word 2, and the 4 ? 7 dependency is
broken, making 4 a root, then we must also break the 2 ? 7
dependency.)
26Note that k(h, c, right) = 7 bounds the width of @ +
  =

. For a finer-grained approach, we could in-
stead separately bound the widths of @ and   , say by
kr(h, c, right) = 4 and kl(h, c, right) = 2.
39
language. Our ?vines? then let us concatenate sev-
eral strings in this subset, which typically yields a
superset of the original context-free language. Sub-
set and superset approximations of (weighted) CFLs
by (weighted) regular languages, usually by pre-
venting center-embedding, have been widely ex-
plored; Nederhof (2000) gives a thorough review.
We limit all dependency lengths (not just center-
embedding).27 Further, we derive weights from a
modified treebank rather than by approximating the
true weights. And though regular grammar approxi-
mations are useful for other purposes, we argue that
for parsing it is more efficient to perform the approx-
imation in the parser, not in the grammar.
Brants (1999) described a parser that encoded the
grammar as a set of cascaded Markov models. The
decoder was applied iteratively, with each iteration
transforming the best (or n-best) output from the
previous one until only the root symbol remained.
This is a greedy variant of CFG parsing where the
grammar is in Backus-Naur form.
Bertsch and Nederhof (1999) gave a linear-time
recognition algorithm for the recognition of the reg-
ular closure of deterministic context-free languages.
Our result is related; instead of a closure of deter-
ministic CFLs, we deal in a closure of CFLs that are
assumed (by the parser) to obey some constraint on
trees (like a maximum dependency length).
6 Future Work
The simple POS-sequence models we used as an ex-
perimental baseline are certainly not among the best
parsers available today. They were chosen to illus-
trate how modeling and exploiting distance in syntax
can affect various performance measures. Our ap-
proach may be helpful for other kinds of parsers as
well. First, we hope that our results will generalize
to more expressive grammar formalisms such as lex-
icalized CFG, CCG, and TAG, and to more expres-
sively weighted grammars, such as log-linear mod-
els that can include head-child distance among other
rich features. The parsing algorithms we presented
also admit inside-outside variants, allowing iterative
estimation methods for log-linear models (see, e.g.,
Miyao and Tsujii, 2002).
27Of course, this still allows right-branching or left-
branching to unbounded depth.
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  20  40  60  80  100
runtime (items/word)
English
F
k = 1
2
3 15 20
Model C, baseline
soft constraint
single bound
type-specific bounds
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  20  40  60  80  100  120  140  160
runtime (items/word)
Chinese
F
k = 1
2 3
15 20
Model C, baseline
soft constraint
single bound
type-specific bounds
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  10  20  30  40  50  60
runtime (items/word)
German
F
k = 1
2
3
15 20
Model C, baseline
soft constraint
single bound
type-specific bounds
Figure 4: Trading off speed and accuracy by varying the set
of feasible parses: The baseline (no length bound) is shown
as +. Tighter bounds always improve speed, except for the
most lax bounds, for which vine construction overhead incurs
a slowdown. Type-specific bounds tend to maintain good F -
measure at higher speeds than the single-bound approach. The
vertical error bars show the ?oracle? accuracy for each experi-
ment (i.e., the F -measure if we had recovered the best feasible
parse, as constructed from the gold-standard parse by grafting:
see ?4.2). Runtime is measured as the number of items per word
(i.e., @ ,   ,  , H , @ , Xy
Xy
) built
by the agenda parser. The ?soft constraint? point marked with
? represents the p(? | d, h, c)-augmented model from ?3.
40
Second, fast approximate parsing may play a role
in more accurate parsing. It might be used to rapidly
compute approximate outside-probability estimates
to prioritize best-first search (e.g., Caraballo and
Charniak, 1998). It might also be used to speed up
the early iterations of training a weighted parsing
model, which for modern training methods tends to
require repeated parsing (either for the best parse, as
by Taskar et al, 2004, or all parses, as by Miyao and
Tsujii, 2002).
Third, it would be useful to investigate algorith-
mic techniques and empirical benefits for limiting
dependency length in more powerful grammar for-
malisms. Our runtime reduction from O(n3) ?
O(nk2) for a length-k bound applies only to a
?split? bilexical grammar.28 Various kinds of syn-
chronous grammars, in particular, are becoming im-
portant in statistical machine translation. Their high
runtime complexity might be reduced by limiting
monolingual dependency length (for a related idea
see Schafer and Yarowsky, 2003).
Finally, consider the possibility of limiting depen-
dency length during grammar induction. We reason
that a learner might start with simple structures that
focus on local relationships, and gradually relax this
restriction to allow more complex models.
7 Conclusion
We have described a novel reason for identifying
headword-to-headword dependencies while parsing:
to consider their length. We have demonstrated
that simple bilexical parsers of English, Chinese,
and German can exploit a ?short-dependency pref-
erence.? Notably, soft constraints on dependency
length can improve both speed and accuracy, and
hard constraints allow improved precision and speed
with some loss in recall (on English and Chinese,
remarkably little loss). Further, for the hard con-
straint ?length ? k,? we have given an O(nk2) par-
tial parsing algorithm for split bilexical grammars;
the grammar constant is no worse than for state-of-
the-art O(n3) algorithms. This algorithm strings to-
gether the partial trees? roots along a ?vine.?
28The obvious reduction for unsplit head automaton gram-
mars, say, is only O(n4) ? O(n3k), following (Eisner and
Satta, 1999). Alternatively, one can convert the unsplit HAG to
a split one that preserves the set of feasible (length ? k) parses,
but then g becomes prohibitively large in the worst case.
Our approach might be adapted to richer parsing
formalisms, including synchronous ones, and should
be helpful as an approximation to full parsing when
fast, high-precision recovery of syntactic informa-
tion is needed.
References
S. P. Abney. Parsing by chunks. In Principle-Based Parsing:
Computation and Psycholinguistics. Kluwer, 1991.
D. E. Appelt, J. R. Hobbs, J. Bear, D. Israel, and M. Tyson.
FASTUS: A finite-state processor for information extraction
from real-world text. In Proc. of IJCAI, 1993.
E. Bertsch and M.-J. Nederhof. Regular closure of deterministic
languages. SIAM J. on Computing, 29(1):81?102, 1999.
D. Bikel. A distributional analysis of a lexicalized statistical
parsing model. In Proc. of EMNLP, 2004.
T. Brants. Cascaded Markov models. In Proc. of EACL, 1999.
S. A. Caraballo and E. Charniak. New figures of merit for best-
first probabilistic chart parsing. Computational Linguistics,
24(2):275?98, 1998.
S. Chen. Bayesian grammar induction for language modeling.
In Proc. of ACL, 1995.
K. W. Church. On memory limitations in natural language pro-
cessing. Master?s thesis, MIT, 1980.
M. Collins. Three generative, lexicalised models for statistical
parsing. In Proc. of ACL, 1997.
J. Eisner. Bilexical grammars and their cubic-time parsing al-
gorithms. In Advances in Probabilistic and Other Parsing
Technologies. Kluwer, 2000.
J. Eisner, E. Goldlust, and N. A. Smith. Compiling Comp Ling:
Practical weighted dynamic programming and the Dyna lan-
guage. In Proc. of HLT-EMNLP, 2005.
J. Eisner and G. Satta. Efficient parsing for bilexical cfgs and
head automaton grammars. In Proc. of ACL, 1999.
L. Frazier. On Comprehending Sentences: Syntactic Parsing
Strategies. PhD thesis, University of Massachusetts, 1979.
E. Gibson. Linguistic complexity: Locality of syntactic depen-
dencies. Cognition, 68:1?76, 1998.
G. Grefenstette. Light parsing as finite-state filtering. In Proc.
of Workshop on Extended FS Models of Language, 1996.
D. Hindle. Noun classification from predicate-argument struc-
ture. In Proc. of ACL, 1990.
J. R. Hobbs and J. Bear. Two principles of parse preference. In
Proc. of COLING, 1990.
D. Klein and C. D. Manning. Accurate unlexicalized parsing.
In Proc. of ACL, 2003.
D. Klein and C. D. Manning. Corpus-based induction of syn-
tactic structure: Models of dependency and constituency. In
Proc. of ACL, 2004.
R. McDonald, K. Crammer, and F. Pereira. Online large-margin
training of dependency parsers. In Proc. of ACL, 2005.
Y. Miyao and J. Tsujii. Maximum entropy estimation for feature
forests. In Proc. of HLT, 2002.
M.-J. Nederhof. Practical experiments with regular approxima-
tion of context-free languages. CL, 26(1):17?44, 2000.
M.-J. Nederhof. Weighted deductive parsing and Knuth?s algo-
rithm. Computational Linguistics, 29(1):135?143, 2003.
C. Schafer and D. Yarowsky. A two-level syntax-based ap-
proach to Arabic-English statistical machine translation. In
Proc. of Workshop on MT for Semitic Languages, 2003.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
Max-margin parsing. In Proc. of EMNLP, 2004.
41
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 201?205, New York City, June 2006. c?2006 Association for Computational Linguistics
Vine Parsing and Minimum Risk Reranking for Speed and Precision?
Markus Dreyer, David A. Smith, and Noah A. Smith
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{markus,{d,n}asmith}@cs.jhu.edu
Abstract
We describe our entry in the CoNLL-X shared task.
The system consists of three phases: a probabilistic
vine parser (Eisner and N. Smith, 2005) that pro-
duces unlabeled dependency trees, a probabilistic
relation-labeling model, and a discriminative mini-
mum risk reranker (D. Smith and Eisner, 2006). The
system is designed for fast training and decoding and
for high precision. We describe sources of cross-
lingual error and ways to ameliorate them. We then
provide a detailed error analysis of parses produced
for sentences in German (much training data) and
Arabic (little training data).
1 Introduction
Standard state-of-the-art parsing systems (e.g.,
Charniak and Johnson, 2005) typically involve two
passes. First, a parser produces a list of the most
likely n parse trees under a generative, probabilistic
model (usually some flavor of PCFG). A discrim-
inative reranker then chooses among trees in this
list by using an extended feature set (Collins, 2000).
This paradigm has many advantages: PCFGs are
fast to train, can be very robust, and perform bet-
ter as more data is made available; and rerankers
train quickly (compared to discriminative models),
require few parameters, and permit arbitrary fea-
tures.
We describe such a system for dependency pars-
ing. Our shared task entry is a preliminary system
developed in only 3 person-weeks, and its accuracy
is typically one s.d. below the average across sys-
tems and 10?20 points below the best system. On
?This work was supported by NSF ITR grant IIS-0313193,
an NSF fellowship to the second author, and a Fannie and John
Hertz Foundation fellowship to the third author. The views ex-
pressed are not necessarily endorsed by the sponsors. We thank
Charles Schafer, Keith Hall, Jason Eisner, and Sanjeev Khudan-
pur for helpful conversations.
the positive side, its decoding algorithms have guar-
anteed O(n) runtime, and training takes only a cou-
ple of hours. Having designed primarily for speed
and robustness, we sacrifice accuracy. Better esti-
mation, reranking on larger datasets, and more fine-
grained parsing constraints are expected to boost ac-
curacy while maintaining speed.
2 Notation
Let a sentence x = ?x1, x2, ..., xn?, where each xi is
a tuple containing a part-of-speech tag ti and a word
wi, and possibly more information.1 x0 is a special
wall symbol, $, on the left. A dependency tree y
is defined by three functions: yleft and yright (both
{0, 1, 2, ..., n} ? 2{1,2,...,n}) that map each word to
its sets of left and right dependents, respectively, and
ylabel : {1, 2, ..., n} ? D, which labels the relation-
ship between word i and its parent from label set D.
In this work, the graph is constrained to be a pro-
jective tree rooted at $: each word except $ has a sin-
gle parent, and there are no cycles or crossing depen-
dencies. Using a simple dynamic program to find the
minimum-error projective parse, we find that assum-
ing projectivity need not harm accuracy very much
(Tab. 1, col. 3).
3 Unlabeled Parsing
The first component of our system is an unlabeled
parser that, given a sentence, finds the U best un-
labeled trees under a probabilistic model using a
bottom-up dynamic programming algorithm.2 The
model is a probabilistic head automaton grammar
(Alshawi, 1996) that assumes conditional indepen-
1We used words and fine tags in our parser and labeler, with
coarse tags in one backoff model. Other features are used in
reranking; we never used the given morphological features or
the ?projective? annotations offered in the training data.
2The execution model we use is best-first, exhaustive search,
as described in Eisner et al (2004). All of our dynamic pro-
gramming algorithms are implemented concisely in the Dyna
language.
201
B` Br
projective oracle
(B
` , B
r )-vine oracle
20-best unlabeled oracle
1-best unlabeled
unlabeled, reranked
20?50-best labeled oracle
1?1-best labeled
reranked (labeled)
(unlabeled)
(non-$ unl. recall)
(non-$ unl. precision)
Arabic 10 4 99.8 90.7 71.5 68.1 68.7 59.7 52.0 53.4 68.5 63.4 76.0
Bulgarian 5 4 99.6 90.7 86.4 80.1 80.5 85.1 73.0 74.8 82.0 74.3 86.3
Chinese 4 4 100.0 93.1 89.9 79.4 77.7 88.6 72.6 71.6 77.6 61.4 80.8
Czech 6 4 97.8 90.5 79.2 70.3 71.5 72.8 58.1 60.5 70.7 64.8 75.7
Danish 5 4 99.2 91.4 84.6 77.7 78.6 79.3 65.5 66.6 77.5 71.4 83.4
Dutch 6 5 94.6 88.3 77.5 67.9 68.8 73.6 59.4 61.6 68.3 60.4 73.0
German 8 7 98.8 90.9 83.4 75.5 76.2 82.3 70.1 71.0 77.0 70.2 82.9
Japanese 4 1 99.2 92.2 90.7 86.3 85.1 89.4 81.6 82.9 86.0 68.5 91.5
Portuguese 5 5 98.8 91.5 85.9 81.4 82.5 83.7 73.4 75.3 82.4 76.2 87.0
Slovene 6 4 98.5 91.7 80.5 72.0 73.3 72.8 57.5 58.7 72.9 66.3 78.5
Spanish 5 6 100.0 91.2 77.3 71.5 72.6 74.9 66.2 67.6 72.9 69.3 80.7
Swedish 4 5 99.7 94.0 87.5 79.3 79.6 81.0 65.5 67.6 79.5 72.6 83.3
Turkish 6 1 98.6 89.5 73.0 61.0 61.8 64.4 44.9 46.1 60.5 48.5 61.6
parser reranker labeler reranker
1 2 3 4 5 6 7 8 9 10 11 12 13
Table 1: Parameters and performance on test data. B` and Br were chosen to retain 90% of dependencies
in training data. We show oracle, 1-best, and reranked performance on the test set at different stages of the
system. Boldface marks oracle performance that, given perfect downstream modules, would supercede the
best system. Italics mark the few cases where the reranker increased error rate. Columns 8?10 show labeled
accuracy; column 10 gives the final shared task evaluation scores.
dence between the left yield and the right yield of
a given head, given the head (Eisner, 1997).3 The
best known parsing algorithm for such a model is
O(n3) (Eisner and Satta, 1999). The U -best list is
generated using Algorithm 3 of Huang and Chiang
(2005).
3.1 Vine parsing (dependency length bounds)
Following Eisner and N. Smith (2005), we also im-
pose a bound on the string distance between every
3To empirically test this assumption across languages, we
measured the mutual information between different features of
yleft(j) and yright(j), given xj . (Mutual information is a statis-
tic that equals zero iff conditional independence holds.) A de-
tailed discussion, while interesting, is omitted for space, but we
highlight some of our findings. First, unsurprisingly, the split-
head assumption appears to be less valid for languages with
freer word order (Czech, Slovene, German) and more valid for
more fixed-order languages (Chinese, Turkish, Arabic) or cor-
pora (Japanese). The children of verbs and conjunctions are the
most frequent violators. The mutual information between the
sequence of dependency labels on the left and on the right, given
the head?s (coarse) tag, only once exceeded 1 bit (Slovene).
child and its parent, with the exception of nodes at-
taching to $. Bounds of this kind are intended to im-
prove precision of non-$ attachments, perhaps sac-
rificing recall. Fixing bound B`, no left dependency
may exist between child xi and parent xj such that
j?i > B` (similarly for right dependencies and Br).
As a result, edge-factored parsing runtime is reduced
from O(n3) to O(n(B2` +B2r )). For each language,
we choose B` (Br) to be the minimum value that
will allow recovery of 90% of the left (right) depen-
dencies in the training corpus (Tab. 1, cols. 1, 2, and
4). In order to match the training data to the parsing
model, we re-attach disallowed long dependencies
to $ during training.
3.2 Estimation
The probability model predicts, for each parent word
xj , {xi}i?yleft (j) and {xi}i?yright (j). An advantage
of head automaton grammars is that, for a given par-
ent node xj , the children on the same side, yleft(j),
202
for example, can depend on each other (cf. McDon-
ald et al, 2005). Child nodes in our model are gener-
ated outward, conditional on the parent and the most
recent same-side sibling (MRSSS). This increases
our parser?s theoretical runtime to O(n(B3` + B3r )),
which we found was quite manageable.
Let pary : {1, 2, ..., n} ? {0, 1, ..., n} map each
node to its parent in y. Let predy : {1, 2, ..., n} ?
{?, 1, 2, ..., n} map each node to the MRSSS in y if
it exists and ? otherwise. Let ?i = |i ? j| if j is i?s
parent. Our (probability-deficient) model defines
p(y) =
n?
j=1
?
?
?
i?yleft (j)
p(xi,?i | xj , xpredy(i), left)
?
?
?p(STOP | xj , xminyleft (j) j , left)
?
?
?
?
i?yright (j)
p(xi,?i | xj ,predy(i), right)
?
?
?p(STOP | xj , xmaxyright (j) j , right) (1)
Due to the familiar sparse data problem, a maxi-
mum likelihood estimate for the ps in Eq. 1 performs
very badly (2?23% unlabeled accuracy). Good sta-
tistical parsers smooth those distributions by mak-
ing conditional independence assumptions among
variables, including backoff and factorization. Ar-
guably the choice of assumptions made (or interpo-
lated among) is central to the success of many exist-
ing parsers.
Noting that (a) there are exponentially many such
options, and (b) the best-performing independence
assumptions will almost certainly vary by language,
we use a mixture among 8 such models. The same
mixture is used for all languages. The models were
not chosen with particular care,4 and the mixture is
not trained?the coefficients are fixed at uniform,
with a unigram coarse-tag model for backoff. In
principle, this mixture should be trained (e.g., to
maximize likelihood or minimize error on a devel-
opment dataset).
The performance of our unlabeled model?s top
choice and the top-20 oracle are shown in Tab. 1,
cols. 5?6. In 5 languages (boldface), perfect label-
ing and reranking at this stage would have resulted in
performance superior to the language?s best labeled
4Our infrastructure provides a concise, interpreted language
for expressing the models to be mixed, so large-scale combina-
tion and comparison are possible.
system, although the oracle is never on par with the
best unlabeled performance.
4 Labeling
The second component of our system is a labeling
model that independently selects a label from D for
each parent/child pair in a tree. Given the U best
unlabeled trees for a sentence, the labeler produces
the L best labeled trees for each unlabeled one.
The computation involves an O(|D|n) dynamic pro-
gramming algorithm, the output of which is passed
to Huang and Chiang?s (2005) algorithm to generate
the L-best list.
We separate the labeler from the parser for two
reasons: speed and candidate diversity. In prin-
ciple the vine parser could jointly predict depen-
dency labels along with structures, but parsing run-
time would increase by at least a factor of |D|. The
two stage process also forces diversity in the candi-
date list (20 structures with 50 labelings each); the
1,000-best list of jointly-decoded parses often con-
tained many (bad) relabelings of the same tree.
In retrospect, assuming independence among de-
pendency labels damages performance substantially
for some languages (Turkish, Czech, Swedish, Dan-
ish, Slovene, and Arabic); note the often large drop
in oracle performance between Tab. 1, cols. 5 and
8. This assumption is necessary in our framework,
because the O(|D|M+1n) runtime of decoding with
an M th-order Markov model of labels5 is in general
prohibitive?in some cases |D| > 80. Pruning and
search heuristics might ameliorate runtime.
If xi is a child of xj in direction D, and xpred is
the MRSSS (possibly ?), where ?i = |i? j|, we es-
timate p(`, xi, xj , xpred ,?i | D) by a mixture (un-
trained, as in the parser) of four backed-off, factored
estimates.
After parsing and labeling, we have for each sen-
tence a list of U ? L candidates. Both the oracle
performance of the best candidate in the (20 ? 50)-
best list and the performance of the top candidate are
shown in Tab. 1, cols. 8?9. It should be clear from
the drop in both oracle and 1-best accuracy that our
labeling model is a major source of error.
5We tested first-order Markov models that conditioned on
parent or MRSSS dependency labels.
203
5 Reranking
We train a log-linear model combining many feature
scores (see below), including the log-probabilities
from the parser and labeler. Training minimizes
the expected error under the model; we use deter-
ministic annealing to smooth the error surface and
avoid local minima (Rose, 1998; D. Smith and Eis-
ner, 2006).
We reserved 200 sentences in each language for
training the reranker, plus 200 for choosing among
rerankers trained on different feature sets and differ-
ent (U ? L)-best lists.6
Features Our reranking features predict tags, la-
bels, lemmata, suffixes and other information given
all or some of the following non-local conditioning
context: bigrams and trigrams of tags or dependency
labels; parent and grandparent dependency labels;
subcategorization frames (in terms of tags or depen-
dency labels); the occurrence of certain tags between
head and child; surface features like the lemma7 and
the 3-character suffix. In some cases the children of
a node are considered all together, and in other cases
left and right are separated.
The highest-ranked features during training, for
all languages, are the parser and labeler probabil-
ities, followed by p(?i | tparent), p(direction |
tparent), p(label | labelpred , label succ , subcat), and
p(coarse(t) | D, coarse(tparent),Betw), where
Betw is TRUE iff an instance of the coarse tag type
with the highest mutual information between its left
and right children (usually verb) is between the child
and its head.
Feature and Model Selection For training speed
and to avoid overfitting, only a subset of the above
features are used in reranking. Subsets of differ-
ent sizes (10, 20, and 40, plus ?all?) are identified
for each language using two na??ve feature-selection
heuristics based on independent performance of fea-
tures. The feature subset with the highest accuracy
on the 200 heldout sentences is selected.
6In training our system, we made a serious mistake in train-
ing the reranker on only 200 sentences. As a result, our pre-
testing estimates of performance (on data reserved for model
selection) were very bad. The reranker, depending on condition,
had only 2?20 times as many examples as it had parameters to
estimate, with overfitting as the result.
7The first 4 characters of a word are used where the lemma
is not available.
Performance Accuracy of the top parses after
reranking is shown in Tab. 1, cols. 10?11. Reranking
almost always gave some improvement over 1-best
parsing.8 Because of the vine assumption and the
preprocessing step that re-attaches all distant chil-
dren to $, our parser learns to over-attach to $, treat-
ing $-attachment as a default/agnostic choice. For
many applications a local, incomplete parse may be
sufficiently useful, so we also measured non-$ unla-
beled precision and recall (Tab. 1, cols. 12?13); our
parser has > 80% precision on 8 of the languages.
We also applied reranking (with unlabeled features)
to the 20-best unlabeled parse lists (col. 7).
6 Error Analysis: German
The plurality of errors (38%) in German were er-
roneous $ attachments. For ROOT dependency la-
bels, we have a high recall (92.7%), but low pre-
cision (72.4%), due most likely to the dependency
length bounds. Among the most frequent tags, our
system has most trouble finding the correct heads of
prepositions (APPR), adverbs (ADV), finite auxil-
iary verbs (VAFIN), and conjunctions (KON), and
finding the correct dependency labels for preposi-
tions, nouns, and finite auxiliary verbs.
The German conjunction und is the single word
with the most frequent head attachment errors. In
many of these cases, our system does not learn
the subtle difference between enumerations that are
headed by A in A und B, with two children und and
B on the right, and those headed by B, with und and
A as children on its left.
Unlike in some languages, our labeled oracle ac-
curacy is nearly as good as our unlabeled oracle ac-
curacy (Tab. 1, cols. 8, 5). Among the ten most fre-
quent dependency labels, our system has the most
difficulty with accusative objects (OA), genitive at-
tributes (AG), and postnominal modifiers (MNR).
Accusative objects are often mistagged as subject
(SB), noun kernel modifiers (NK), or AG. About
32% of the postnominal modifier relations (ein Platz
in der Geschichte, ?a place in history?) are labeled
as modifiers (in die Stadt fliegen, ?fly into the city?).
Genitive attributes are often tagged as NK since both
are frequently realized as nouns.
8The exception is Chinese, where the training set for rerank-
ing is especially small (see fn. 6).
204
7 Error Analysis: Arabic
As with German, the greatest portion of Arabic er-
rors (40%) involved attachments to $. Prepositions
are consistently attached too low and accounted for
26% of errors. For example, if a form in construct
(idafa) governed both a following noun phrase and
a prepositional phrase, the preposition usually at-
taches to the lower noun phrase. Similarly, prepo-
sitions usually attach to nearby noun phrases when
they should attach to verbs farther to the left.
We see a more serious casualty of the dependency
length bounds with conjunctions. In ground truth
test data, 23 conjunctions are attached to $ and 141
to non-$ to using the COORD relation, whereas 100
conjunctions are attached to $ and 67 to non-$ us-
ing the AUXY relation. Our system overgeneralizes
and attaches 84% of COORD and 71% of AUXY
relations to $. Overall, conjunctions account for
15% of our errors. The AUXY relation is defined
as ?auxiliary (in compound expressions of various
kinds)?; in the data, it seems to be often used for
waw-consecutive or paratactic chaining of narrative
clauses. If the conjunction wa (?and?) begins a sen-
tence, then that conjunction is tagged in ground truth
as attaching to $; if the conjunction appears in the
middle of the sentence, it may or may not be at-
tached to $.
Noun attachments exhibit a more subtle problem.
The direction of system attachments is biased more
strongly to the left than is the case for the true data.
In canonical order, Arabic nouns do generally attach
on the right: subjects and objects follow the verb; in
construct, the governed noun follows its governor.
When the data deviate from this canonical order?
when, e.g, a subject precedes its verb?the system
prefers to find some other attachment point to the
left. Similarly, a noun to the left of a conjunction
often erroneously attaches to its left. Such ATR re-
lations account for 35% of noun-attachment errors.
8 Conclusion
The tradeoff between speed and accuracy is famil-
iar to any parsing researcher. Rather than starting
with an accurate system and then applying corpus-
specific speedups, we start by imposing carefully-
chosen constraints (projectivity and length bounds)
for speed, leaving accuracy to the parsing and
reranking models. As it stands, our system performs
poorly, largely because the estimation is not state-
of-the-art, but also in part due to dependency length
bounds, which are rather coarse at present. Better re-
sults are achievable by picking different bounds for
different head tags (Eisner and N. Smith, 2005). Ac-
curacy should not be difficult to improve using bet-
ter learning methods, especially given our models?
linear-time inference and decoding.
References
H. Alshawi. 1996. Head automata and bilingual
tiling: Translation with minimal representations.
In Proc. of ACL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL.
M. Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
J. Eisner and G. Satta. 1999. Efficient parsing
for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In
Proc. of IWPT.
J. Eisner, E. Goldlust, and N. A. Smith. 2004.
Dyna: A declarative language for implementing
dynamic programs. In Proc. of ACL (companion
volume).
J. Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proc. of IWPT.
L. Huang and D. Chiang. 2005. Better k-best pars-
ing. In Proc. of IWPT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proc. of HLT-
EMNLP.
K. Rose. 1998. Deterministic annealing for cluster-
ing, compression, classification, regression, and
related optimization problems. Proc. of the IEEE,
86(11):2210?2239.
D. A. Smith and J. Eisner. 2006. Minimum risk an-
nealing for training log-linear models. To appear
in Proc. of COLING-ACL.
205
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 815?823,
Beijing, August 2010
Nonparametric Word Segmentation for Machine Translation
ThuyLinh Nguyen Stephan Vogel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
{thuylinh,vogel,nasmith}@cs.cmu.edu
Abstract
We present an unsupervised word seg-
mentation model for machine translation.
The model uses existing monolingual seg-
mentation techniques and models the joint
distribution over source sentence segmen-
tations and alignments to the target sen-
tence. During inference, the monolin-
gual segmentation model and the bilin-
gual word alignment model are coupled
so that the alignments to the target sen-
tence guide the segmentation of the source
sentence. The experiments show improve-
ments on Arabic-English and Chinese-
English translation tasks.
1 Introduction
In statistical machine translation, the smallest unit
is usually the word, defined as a token delimited
by spaces. Given a parallel corpus of source and
target text, the training procedure first builds a
word alignment, then extracts phrase pairs from
this word alignment. However, in some languages
(e.g., Chinese) there are no spaces between words.
The same problem arises when translating be-
tween two very different languages, such as from
a language with rich morphology like Hungarian
or Arabic to a language with poor morphology
like English or Chinese. A single word in a mor-
phologically rich language is often the composi-
tion of several morphemes, which correspond to
separate words in English.1
1We will use the terms word segmentation, morphologi-
cal analysis, and tokenization more or less interchangeably.
Often some preprocessing is applied involving
word segmentation or morphological analysis of
the source and/or target text. Such preprocess-
ing tokenizes the text into morphemes or words,
which linguists consider the smallest meaning-
bearing units of the language. Take as an ex-
ample the Arabic word ?fktbwha? and its En-
glish translation ?so they wrote it?. The preferred
segmentation of ?fktbwha? would be ?f-ktb-w-ha
(so-wrote-they-it),? which would allow for a one-
to-one mapping between tokens in the two lan-
guages. However, the translation of the phrase in
Hebrew is ?wktbw ath?. Now the best segmen-
tation of the Arabic words would be ?fktbw-ha,?
corresponding to the two Hebrew words. This ex-
ample shows that there may not be one correct
segmentation that can be established in a prepro-
cessing step. Rather, tokenization depends on the
language we want to translate into and needs to
be tied in with the alignment process. In short,
we want to find the tokenization yielding the best
alignment, and thereby the best translation sys-
tem.
We propose an unsupervised tokenization
method for machine translation by formulating a
generative Bayesian model to ?explain? the bilin-
gual training data. Generation of a sentence pair
is described as follows: first a monolingual to-
kenization model generates the source sentence,
then the alignment model generates the target sen-
tence through the alignments with the source sen-
tence. Breaking this generation process into two
steps provides flexibility to incorporate existing
monolingual morphological segmentation mod-
els such as those of Mochihashi et al (2009) or
Creutz and Lagus (2007). Using nonparametric
815
models and the Bayesian framework makes it pos-
sible to incorporate linguistic knowledge as prior
distributions and obtain the posterior distribution
through inference techniques such as MCMC or
variational inference.
As new test source sentences do not have trans-
lations which can help to infer the best segmenta-
tion, we decode the source string according to the
posterior distribution from the inference step.
In summary, our segmentation technique con-
sists of the following steps:
? A joint model of segmented source text and
its target translation.
? Inference of the posterior distribution of the
model given the training data.
? A decoding algorithm for segmenting source
text.
? Experiments in translation using the prepro-
cessed source text.
Our experiments show that the proposed seg-
mentation method leads to improvements on
Arabic-English and Chinese-English translation
tasks.
In the next section we will discuss related work.
Section 3 will describe our model in detail. The
inference will be covered in Section 4, and decod-
ing in Section 5. Experiments and results will be
presented in Section 6.
2 Related Work
The problem of segmentation for machine trans-
lation has been studied extensively in recent lit-
erature. Most of the work used some linguistic
knowledge about the source and the target lan-
guages (Nie?en and Ney, 2004; Goldwater and
McClosky, 2005). Sadat and Habash (2006) ex-
perimented with a wide range of tokenization
schemes for Arabic-English translation. These
experiments further show that even for a single
language pair, different tokenizations are needed
depending on the training corpus size. The ex-
periments are very expensive to conduct and do
not generalize to other language pairs. Recently,
Dyer (2009) created manually crafted lattices for
a subset of source words as references for seg-
mentation when translating into English, and then
learned the segmentation of the source words to
optimize the translation with respect to these ref-
erences. He showed that the parameters of the
model can be applied to similar languages when
translating into English. However, manually cre-
ating these lattices is time-consuming and requires
a bilingual person with some knowledge of the un-
derlying statistical machine translation system.
There have been some attempts to apply un-
supervised methods for tokenization in machine
translation (Chung and Gildea, 2009; Xu et al,
2008). The alignment model of Chung and
Gildea (2009) forces every source word to align
with a target word. Xu et al (2008) mod-
eled the source-to-null alignment as in the source
word to target word model. Their models are
special cases of our proposed model when the
source model2 is a unigram model. Like Xu et
al. (2008), we use Gibbs sampling for inference.
Chung and Gildea (2009) applied efficient dy-
namic programming-based variational inference
algorithms.
We benefit from existing unsupervised mono-
lingual segmentation. The source model uses the
nested Pitman-Yor model as described by Mochi-
hashi et al (2009). When sampling each potential
word boundary, our inference technique is a bilin-
gual extension of what is described by Goldwater
et al (2006) for monolingual segmentation.
Nonparametric models have received attention
in machine translation recently. For example,
DeNero et al (2008) proposed a hierarchical
Dirichlet process model to learn the weights of
phrase pairs to address the degeneration in phrase
extraction. Teh (2006) used a hierarchical Pitman-
Yor process as a smoothing method for language
models.
Recent work on multilingual language learning
successfully used nonparametric models for lan-
guage induction tasks such as grammar induction
(Snyder et al, 2009; Cohen et al, 2010), morpho-
logical segmentation (Goldwater et al, 2006; Sny-
der and Barzilay, 2008), and part-of-speech tag-
ging (Goldwater and Griffiths, 2007; Snyder et al,
2Note that ?source model? here means a model of source
text, not a source model in the noisy channel paradigm.
816
2008).
3 Models
We start with the generative process for a source
sentence and its alignment with a target sentence.
Then we describe individual models employed by
this generation scheme.
3.1 Generative Story
A source sentence is a sequence of word tokens,
and each word is either aligned or not aligned. We
focus only on the segmentation problem and not
reordering source words; therefore, the model will
not generate the order of the target word tokens.
A sentence pair and its alignment are captured by
four components:
? a sequence of words in the source sentence,
? a set of null-aligned source tokens,
? a set of null-aligned target tokens, and
? a set of (source word to target word) align-
ment pairs.
We will start with a high-level story of how the
segmentation of the source sentence and the align-
ment are generated.
1. A source language monolingual segmenta-
tion model generates the source sentence.
2. Generate alignments:
(a) Given the sequence of words of the
source sentence already generated in
step 1, the alignment model marks each
source word as either aligned or un-
aligned. If a source word is aligned, the
model also generates the target word.
(b) Unaligned target words are generated.
The model defines the joint probability of a seg-
mented source language sentence and its align-
ment. During inference, the two parts are cou-
pled, so that the alignment will influence which
segmentation is selected. However, there are sev-
eral advantages in breaking the generation process
into two steps.
First of all, in principle the model can incor-
porate any existing probabilistic monolingual seg-
mentation to generate the source sentence. For
example, the source model can be the nested
Pitman-Yor process as described by Mochihashi et
al. (2009), the minimum description length model
presented by Creutz and Lagus (2007), or some-
thing else. Also the source model can incorporate
linguistic knowledge from a rule-based or statisti-
cal morphological disambiguator.
The model generates the alignment after the
source sentence with word boundaries already
generated. Therefore, the alignment model can
be any existing word alignment model (Brown
et al, 1993; Vogel et al, 1996). Even though
the choices of source model or alignment model
can lead to different inference methods, the model
we propose here is highly extensible. Note that
we assume that the alignment consists of at most
one-to-one mappings between source and target
words, with null alignments possible on both
sides.
Another advantage of a separate source model
lies in the segmentation of an unseen test set. In
section 5 we will show how to apply the source
model distribution learned from training data to
find the best segmentation of an unseen test set.
Notation and Parameters
We will use bold font for a sequence or bags
of words and regular font for an individual word.
A source sentence s is a sequence of |s| words
si:
(
s1, . . . , s|s|
); the translation of sentence s is
the target sentence t of |t| words (t1, . . . , t|t|
).
In sentence s the list of unaligned words is snal
and the list of aligned source words is sal. In
the target sentence t the list of unaligned words
is tnal and the list of target words having one-
to-one alignment with source words sal is tal.
The alignment a of s and t is represented by
{?si, null? | si ? snal} ? {?si, tai? | si ? sal; tai ?
tal} ? {?null, tj? | tj ? tnal} where ai denotes
the index in t of the word aligned to si.
The probability of a sequence or a set is denoted
by P (.), probability at the word level is p (.). For
example, the probability of sentence s is P (s), the
probability of a word s is p (s), the probability
that the target word t aligns to an aligned source
817
word s is p (t |s).
A sentence pair and its alignment are generated
from the following models:
? The source model generates sentence s with
probability P (s).
? The source-to-null alignment model de-
cides independently for each word s
whether it is unaligned with probability
p (null | si) or aligned with probabil-
ity: 1 ? p (null | si). The probability
of this step, for all source words, is:
P (snal, sal | s) = ?si?snal p (null | si) ??
si?sal (1 ? p (null | si)) .
We will also refer to the source-to-null model
as the deletion model, since words in snal are
effectively deleted for the purposes of align-
ment.
? The source-to-target algnment model gen-
erates a bag of target words tal aligned
to the source words sal with probability:
P (tal |sal) = ?si?sal;tai?tal p (tai |si). Notethat we do not need to be concerned with
generating a explicitly, since we do not
model word order on the target side.
? The null-to-target algnment model gen-
erates the list of unaligned target words
tnal given aligned target words tal with
P (tnal |tal) as follows:
? Generate the number of unaligned tar-
get words |tnal| given the number of
aligned target words |tal| with probabil-
ity P (|tnal| | |tal|).
? Generate |tnal| unaligned words t ?
tnal independently, each with probabil-
ity p (t |null).
The resulting null-to-target proba-
bility is therefore: P (tnal | tal) =
P (|tnal| | |tal|)?t?tnal p (t |null) .
We also call the null-to-target model the in-
sertion model.
The above generation process defines the joint
probability of source sentence s and its alignment
a as follows:
P (s, a) = P (s)????
source model
? P (a | s)? ?? ?
alignment model
(1)
P (a | s) = P (tal |sal) ? P (tnal |tal) (2)
?
?
si?snal
p (null | si) ?
?
si?sal
(1 ? p (null | si))
3.2 Source Model
Our generative process provides the flexibility of
incorporating different monolingual models into
the probability distribution of a sentence pair.
In particular we use the existing state-of-the-art
nested Pitman-Yor n-gram language model as de-
scribed by Mochihashi et al (2009). The proba-
bility of s is given by
P (s) = P (|s|)
|s|?
i=1
p (si |si?n, . . . , si?1) (3)
where the n-gram probability is a hierarchical
Pitman-Yor language model using (n ? 1)-gram
as the base distribution.
At the unigram level, the model uses the base
distribution p (s) as the infinite-gram character-
level Pitman-Yor language model.
3.3 Modeling Null-Aligned Source Words
The probability that a source word aligns to null
p (null | s) is defined by a binomial distribution
with Beta prior Beta (?p, ? (1 ? p)), where ?
and p are model parameters. When p ? 0 and
? ? ? the probability p (null | s) converges to 0
forcing each source words align to a target word.
We fixed p = 0.1 and ? = 20 in our experiment.
Xu et al (2008) view the null word as another
target word, hence in their model the probability
that a source word aligns to null can only depend
on itself.
By modeling the source-to-null alignment sep-
arately, our model lets the distribution depend
on the word?s n-gram context as in the source
model. p (null | si?n, . . . , si) stands for the prob-
ability that the word si is not aligned given its con-
text (si?n, . . . , si?1).
The n-gram source-to-null distribution
p (null | si?n, . . . , si) is defined similarly to
818
p (null | si) definition above in which the base
distribution p now becomes the (n ? 1)-gram:
p (null | si?n+1, . . . , si).3
3.4 Source-Target Alignment Model
The probability p (t |s) that a target word t aligns
to a source word s is a Pitman-Yor process:
t | s ? PY (d, ?, p0 (t |s))
here d and ? are the input parameters, and
p0 (t |s) is the base distribution.
Let |s, ?| denote the number of times s is aligned
to any t in the corpus and let |s, t| denote the num-
ber of times s is aligned to t anywhere in the cor-
pus. And let ty(s) denote the number of different
target words t the word s is aligned to anywhere
in the corpus. In the Chinese Restaurant Process
metaphor, there is one restaurant for each source
word s, the s restaurant has ty(s) tables and total
|s, ?| customers; table t has |s, t| customers.
Then, at a given time in the generative process
for the corpus, we can write the probability that t
is generated by the word s as:
? if |s, t| > 0:
p (t |s) =
|s, t| ? d + [? + dty(s)]p0 (t |s)
|s, ?| + ?
? if |s, t| = 0:
p (t |s) = [? + dty(s)]p0 (t |s)|s, ?| + ?
For language pairs with similar character sets
such as English and French, words with similar
surface form are often translations of each other.
The base distribution can be defined based on
the edit distance between two words (Snyder and
Barzilay, 2008).
We are working with diverse language pairs
(Arabic-English and Chinese-English), so we
use the base distribution as the flat distribution
p0 (t |s) = 1T ; T is the number of distinct targetwords in the training set. In our experiment, the
model parameters are ? = 20 and d = .5.
3We also might have conditioned this decision on words
following si, since those have all been generated already at
this stage.
3.5 Modeling Null-Aligned Target Words
The null-aligned target words are modeled condi-
tioned on previously generated target words as:
P (tnal |tal) = P (|tnal| | |tal|)
?
t?tnal
p (t |null)
This model uses two probability distributions:
? the number of unaligned target words:
P (|tnal| | |tal|), and
? the probability that each word in tnal is gen-
erated by null: p (t |null).
We model the number of unaligned target
words similarly to the distribution in the IBM3
word alignment model (Brown et al, 1993).
IBM3 assumes that each aligned target words gen-
erates a null-aligned target word with probabil-
ity p0 and fails to generate a target word with
probability 1 ? p0. So the parameter p0 can
be used to control the number of unaligned tar-
get words. In our experiments, we fix p0 =
.05. Following this assumption, the probability of
|tnal| unaligned target words generated from |tal|
words is: P (|tnal| | |tal|) =
( |tal|
|tnal|
)
p|tnal|0 (1 ?
p0)|tal|?|tnal|.
The probability that a target word t aligns to
null, p (t |null), also has a Pitman-Yor process
prior. The base distribution of the model is similar
to the source-to-target model?s base distribution
which is the flat distribution over target words.
4 Inference
We have defined a probabilistic generative model
to describe how a corpus of alignments and seg-
mentations can be generated jointly. In this sec-
tion we discuss how to obtain the posterior distri-
butions of the missing alignments and segmenta-
tions given the training corpus, using Gibbs sam-
pling.
Suppose we are provided a morphological
disambiguator for the source language such as
MADA morphology tokenization toolkit (Sadat
and Habash, 2006) for Arabic.4 The morpho-
logical disambiguator segments a source word to
4MADA provides several segmentation schemes; among
them the MADA-D3 scheme seeks to separate all mor-
phemes of each word.
819
morphemes of smallest meaning-bearing units of
the source language. Therefore, a target word is
equivalent to one or several morphemes. Given
a morphological disambiguation toolkit, we use
its output to bias our inference by not consider-
ing word boundaries after every character but only
considering potential word boundaries as a subset
of the morpheme boundaries set. In this way, the
inference uses the morphological disambiguation
toolkit to limit its search space.
The inference starts with an initial segmenta-
tion of the source corpus and also its alignment
to the target corpus. The Gibbs sampler consid-
ers one potential word boundary at a time. There
are two hypotheses at any given boundary posi-
tion of a sentence pair (s, t): the merge hypothe-
sis stands for no word boundary and the resulting
source sentence smerge has a word s spanning over
the sample point; the split hypothesis indicates the
resulting source sentence ssplit has a word bound-
ary at the sample point separating two words s1s2.
Similar to Goldwater et al (2006) for monolingual
segmentation, the sampler randomly chooses the
boundary according to the relative probabilities of
the merge hypothesis and the split hypothesis.
The model consists of source and alignment
model variables; given the training corpora size of
a machine translation system, the number of vari-
ables is large. So if the Gibbs sampler samples
both source variables and alignment variables, the
inference requires many iterations until the sam-
pler mixes. Xu et al (2008) fixed this by repeat-
edly applying GIZA++ word alignment after each
sampling iteration through the training corpora.
Our inference technique is not precisely Gibbs
sampling. Rather than sampling the alignment or
attempting to collapse it out (by summing over
all possible alignments when calculating the rel-
ative probabilities of the merge and split hypothe-
ses), we seek the best alignment for each hypoth-
esis. In other words, for each hypothesis, we per-
form a local search for a high-probability align-
ment of the merged word or split words, given
the rest of alignment for the sentence. Up to one
word may be displaced and realigned. This ?local-
best? alignment is used to score the hypothesis,
and after sampling merge or split, we keep that
best alignment.
This inference technique is motivated by run-
time demands, but we do not yet know of a the-
oretical justification for combining random steps
with maximization over some variables. A more
complete analysis is left to future work.
5 Decoding for Unseen Test Sentences
Section 4 described how to get the model?s pos-
terior distribution and the segmentation and align-
ment of the training data under the model. We are
left with the problem of decoding or finding the
segmentation of test sentences where the transla-
tions are not available. This is needed when we
want to translate new sentences. Here, tokeniza-
tion is performed as a preprocessing step, decou-
pled from the subsequent translation steps.
The decoding step uses the model?s posterior
distribution for the training data to segment un-
seen source sentences. Because of the clear sep-
aration of the source model and the alignment
model, the source model distribution learned from
the Gibbs sampling directly represents the distri-
bution over the source language and can therefore
also handle the segmentation of unknown words
in new test sentences. Only the source model is
used in preprocessing.
The best segmentation s? of a string of charac-
ters c = (c1, . . . , c|c|
) according to the n-gram
source model is:
s? = argmax
s from c
p (|s|)
i=|s|?
i=1
p (si |si?n, . . . , si?1)
We use a stochastic finite-state machine for de-
coding. This is possible by composition of the fol-
lowing two finite state machines:
? Acceptor Ac. The string of characters c is
represented as an finite state acceptor ma-
chine where any path through the machine
represents an unweighted segmentation of c.
? Source model weighted finite state trans-
ducer Lc. Knight and Al-Onaizan (1998)
show how to build an n-gram language
model by a weighted finite state machine.
The states of the transducer are (n ? 1)-
gram history, the edges are words from the
language. The arc si coming from state
820
(si?n, . . . , si?1) to state (si?n+1, . . . , si) has
weight p (si |si?n, . . . , si?1).
The best segmentation s? is given as s? =
BestPath(Ac ? Lc).
6 Experiments
This section presents experimental results on
Arabic-English and Chinese-English translation
tasks using the proposed segmentation technique.
6.1 Arabic-English
As a training set we use the BTEC corpus dis-
tributed by the International Workshop on Spo-
ken Language Translation (IWSLT) (Matthias and
Chiori, 2005). The corpus is a collection of
conversation transcripts from the travel domain.
The ?Supplied Data? track consists of nearly 20K
Arabic-English sentence pairs. The development
set consists of 506 sentences from the IWSLT04
evaluation test set and the unseen set consists of
500 sentences from the IWSLT05 evaluation test
set. Both development set and test set have 16 ref-
erences per Arabic sentence.
6.2 Chinese-English
The training set for Chinese-English translation
task is also distributed by the IWSLT evaluation
campaign. It consists of 67K Chinese-English
sentence pairs. The development set and the test
set each have 489 Chinese sentences and each sen-
tence has 7 English references.
6.3 Results
We will report the translation results where the
preprocessing of the source text are our unigram,
bigram, and trigram source models and source-to-
null model.
The MCMC inference algorithm starts with an
initial segmentation of the source text into full
word forms. For Chinese, we use the original
word segmentation as distributed by IWSLT. To
get an initial alignment, we generate the IBM4
Viterbi alignments in both directions using the
GIZA++ toolkit (Och and Ney, 2003) and com-
bine them using the ?grow-diag-final-and? heuris-
tic. The output of combining GIZA++ align-
ment for a sentence pair is a sequence of si-tj
entries where i is an index of the source sen-
tence and j is an index of the target sentence.
As our model allows only one-to-one mappings
between the words in the source and target sen-
tences, we remove si-tj from the sequence if ei-
ther the source word si or target word tj is al-
ready in a previous entry of the combined align-
ment sequence. The resulting alignment is our ini-
tial alignment for the inference.
We also apply the MADA morphology seg-
mentation toolkit (Habash and Rambow, 2005) to
preprocess the Arabic corpus. We use the D3
scheme (each Arabic word is segmented into mor-
phemes in sequence [CONJ+ [PART+ [Al+ BASE
+PRON]]]), mark the morpheme boundaries, and
then combine the morphemes again to have words
in their original full word form. During inference,
we only sample over these morpheme boundaries
as potential word boundaries. In this way, we
limit the search space, allowing only segmenta-
tions consistent with MADA-D3.
The inference samples 150 iterations through
the whole training set and uses the posterior prob-
ability distribution from the last iteration for de-
coding. The decoding process is then applied
to the entire training set as well as to the devel-
opment and test sets to generate a consistent to-
kenization across all three data sets. We used
the OpenFST toolkit (Allauzen et al, 2007) for
finite-state machine implementation and opera-
tions. The output of the decoding is the pre-
processed data for translation. We use the open
source Moses phrase-based MT system (Koehn et
al., 2007) to test the impact of the preprocessing
technique on translation quality.5
6.3.1 Arabic-English Translation Results
We consider the Arabic-English setting. We
use two baselines: original full word form
and MADA-D3 tokenization scheme for Arabic-
English translation. Table 1 compares the trans-
lation results of our segmentation methods with
these baselines. Our segmentation method shows
improvement over the two baselines on both the
development and test sets. According to Sadat
and Habash (2006), the MADA-D3 scheme per-
5The Moses translation alignment is the output of
GIZA++, not from our MCMC inference.
821
Dev. Test
Original 59.21 54.00
MADA-D3 58.28 54.92
Unigram 59.44 56.18
Bigram 58.88 56.18
Trigram 58.76 56.82
Table 1: Arabic-English translation results
(BLEU).
forms best for their Arabic-English translation es-
pecially for small and moderate data sizes. In our
experiments, we see an improvement when using
the MADA-D3 preprocessing over using the orig-
inal Arabic corpus on the unseen test set, but not
on the development set.
The Gibbs sampler only samples on the mor-
phology boundary points of MADA-D3, so the
improvement resulting from our segmentation
technique does not come from removing unknown
words. It is due to a better matching between
the source and target sentences by integrating seg-
mentation and alignment. We therefore expect the
same impact on a larger training data set in future
experiments.
6.3.2 Chinese-English Translation Results
Dev. Test
Whole word 23.75 29.02
Character 23.39 27.74
Unigram 24.90 28.97
Trigram 23.98 28.20
Table 2: Chinese-English translation result in
BLEU score metric.
We next consider the Chinese-English setting.
The translation performance using our word seg-
mentation technique is shown in Table 2. There
are two baselines for Chinese-English translation:
(a) the source text in the full word form distributed
by the IWSLT evaluation and (b) no segmentation
of the source text, which is equivalent to interpret-
ing each Chinese character as a single word.
Taking development and test sets into account,
the best Chinese-English translation system re-
sults from our unigram model. It is significantly
better than other systems on the development set
and performs almost equally well with the IWSLT
segmentation on the test set. Note that the seg-
mentation distributed by IWSLT is a manual seg-
mentation for the translation task.
Chung and Gildea (2009) and Xu et al (2008)
also showed improvement over a simple mono-
lingual segmentation for Chinese-English trans-
lation. Our character-based translation result is
comparable to their monolingual segmentations.
Both trigram and unigram translation results out-
perform the character-based translation.
We also observe that there are no additional
gains for Chinese-English translation when using
a higher n-gram model. Our Gibbs sampler has
the advantage that the samples are guaranteed to
converge eventually to the model?s posterior dis-
tributions, but in each step the modification to the
current hypothesis is small and local. In itera-
tions 100?150, the average number of boundary
changes for the unigram model is 14K boundaries
versus only 1.5K boundary changes for the tri-
gram model. With 150 iterations, the inference
output of trigram model might not yet represent
its posterior distribution. We leave a more de-
tailed investigation of convergence behavior to fu-
ture work.
Conclusion and Future Work
We presented an unsupervised segmentation
method for machine translation and presented
experiments for Arabic-English and Chinese-
English translation tasks. The model can incor-
porate existing monolingual segmentation mod-
els and seeks to learn a segmenter appropriate for
a particular translation task (target language and
dataset).
Acknowledgements
We thank Kevin Gimpel for interesting discus-
sions and technical advice. We also thank the
anonymous reviewers for useful feedback. This
work was supported by DARPA Gale project,
NSF grants 0844507 and 0915187.
822
References
Allauzen, C., M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A General and Efficient
Weighted Finite-State Transducer Library. In Pro-
ceedings of the CIAA 2007, volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Brown, Peter F., Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Comput. Linguist., 19(2):263?
311.
Chung, T. and D. Gildea. 2009. Unsupervised Tok-
enization for Machine Translation. In Proceedings
of EMNLP 2009, pages 718?726, Singapore, Au-
gust. Association for Computational Linguistics.
Cohen, S. B., D. M. Blei, and N. A. Smith. 2010. Vari-
ational Inference for Adaptor Grammars. In Pro-
ceedings of NAACL-HLT, pages 564?572, June.
Creutz, Mathias and Krista Lagus. 2007. Unsu-
pervised Models for Morpheme Segmentation and
Morphology Learning. ACM Trans. Speech Lang.
Process., 4(1):1?34.
DeNero, J., A. Bouchard-Co?te?, and D. Klein. 2008.
Sampling Alignment Structure under a Bayesian
Translation Model. In Proceedings of EMNLP
2008, pages 314?323, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Dyer, C. 2009. Using a Maximum Entropy model to
build segmentation lattices for MT. In Proceedings
of HLT 2009, pages 406?414, Boulder, Colorado,
June.
Goldwater, S. and T. L. Griffiths. 2007. A Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. In Proceedings of ACL.
Goldwater, S. and D. McClosky. 2005. Improving Sta-
tistical Machine Translation Through Morphologi-
cal Analysis. In Proc. of EMNLP.
Goldwater, S., T. L. Griffiths, and M. Johnson. 2006.
Contextual Dependencies in Unsupervised Word
Segmentation. In Proc. of COLING-ACL.
Habash, N. and O. Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging, and Morpholog-
ical Disambiguation in One Fell Swoop. In Proc. of
ACL.
Knight, K. and Y. Al-Onaizan. 1998. Translation
with Finite-State Devices. In Proceedings of AMTA,
pages 421?437.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of ACL (demo session).
Matthias, E. and H. Chiori. 2005. Overview of the
IWSLT 2005 Evaluation Campaign. In Proceedings
of IWSLT.
Mochihashi, D., T. Yamada, and N. Ueda. 2009.
Bayesian Unsupervised Word Segmentation with
Nested Pitman-Yor Language Modeling. In Pro-
ceedings of 47th ACL, pages 100?108, Suntec, Sin-
gapore, August.
Nie?en, S. and H. Ney. 2004. Statistical Machine
Translation with Scarce Resources Using Morpho-
Syntactic Information. Computational Linguistics,
30(2), June.
Och, F. and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1).
Sadat, F. and N. Habash. 2006. Combination of Ara-
bic Preprocessing Schemes for Statistical Machine
Translation. In Proceedings of the ACL, pages 1?8.
Snyder, B. and R. Barzilay. 2008. Unsupervised Mul-
tilingual Learning for Morphological Segmentation.
In Proceedings of ACL-08: HLT, pages 737?745,
June.
Snyder, B., T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised Multilingual Learning for POS
Tagging. In Proceedings of EMNLP.
Snyder, B., T. Naseem, and R. Barzilay. 2009. Unsu-
pervised Multilingual Grammar Induction. In Pro-
ceedings of ACL-09, pages 73?81, August.
Teh, Y. W. 2006. A Hierarchical Bayesian Language
Model Based On Pitman-Yor Processes. In Pro-
ceedings of ACL, pages 985?992, July.
Vogel, S., H. Ney, and C. Tillmann. 1996. HMM-
Based Word Alignment in Statistical Translation. In
Proceedings of COLING, pages 836?841.
Xu, J., J. Gao, K. Toutanova, and H. Ney. 2008.
Bayesian Semi-Supervised Chinese Word Segmen-
tation for Statistical Machine Translation. In
Proceedings of (Coling 2008), pages 1017?1024,
Manchester, UK, August.
823
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 884?894, Dublin, Ireland, August 23-29 2014.
A Step Towards Usable Privacy Policy:
Automatic Alignment of Privacy Statements
Fei Liu Rohan Ramanath Norman Sadeh Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{feiliu, rrohan, sadeh, nasmith}@cs.cmu.edu
Abstract
With the rapid development of web-based services, concerns about user privacy have height-
ened. The privacy policies of online websites, which serve as a legal agreement between service
providers and users, are not easy for people to understand and therefore offer an opportunity for
natural language processing. In this paper, we consider a corpus of these policies, and tackle the
problem of aligning or grouping segments of policies based on the privacy issues they address.
A dataset of pairwise judgments from humans is used to evaluate two methods, one based on
clustering and another based on a hidden Markov model. Our analysis suggests a five-point gap
between system and median-human levels of agreement with a consensus annotation, of which
half can be closed with bag of words representations and half requires more sophistication.
1 Introduction
Privacy policies are legal documents, authored by privacy lawyers to protect the interests of companies
offering services through the web. According to a study conducted by McDonald and Cranor (2008), if
every internet user in the U.S. read the privacy notice of each new website she visited, it would take the
nation 54 billion hours annually to read privacy policies. It is not surprising that they often go unread
(Federal Trade Commission, 2012).
Users, nonetheless, might do well to understand the implications of agreeing to a privacy policy, and
might make different choices if they did. Researchers in the fields of internet privacy and security have
made various attempts to standardize the format of privacy notices, so that they are easier to understand
and to allow the general public to have better control of their personal information. An early effort is the
Platform for Privacy Preferences Project (P3P), which defines a machine-readable language that enables
the websites to explicitly declare their intended use of personal information (Cranor, 2002). Many other
studies primarily focus on the qualitative perspective of policies and use tens of carefully selected privacy
notices. For example, Kelley et al. (2010) proposed a ?nutrition label? approach that formalizes the
privacy policy into a standardized table format. Breaux et al. (2014) map privacy requirements encoded
in text to a formal logic, in order to detect conflicts in requirements and trace data flows (e.g., what data
might be collected, to whom the data will be transferred and for what purposes).
The need for automatically or semi-automatically generating simple, easy-to-digest privacy summaries
is further exacerbated by the emergence of the mobile Web and the Internet of Things, with early efforts
in this area including the use of static analysis to identify sensitive data flows in mobile apps (Lin et al.,
2012) and the development of mobile app privacy profiles (Liu et al., 2014).
Increased automation for such efforts motivates our interest in privacy policies as a text genre for NLP,
with the general goal of supporting both user-oriented tools that interpret policies and studies of the
contents of policies by legal scholars.
In this paper, we start with a corpus of 1,010 policies collected from widely-used websites (Ramanath
et al., 2014),
1
and seek to automatically align segments of policies. We believe this is a worthwhile first
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.usableprivacy.org/data
884
Amazon.com Privacy Notice
...
What About Cookies?
Cookies are unique identifiers that we transfer to your device to enable
our systems to recognize your device and to provide features such as
1-Click purchasing, Recommended for You, personalized advertisements
on other Web sites...
...Because cookies allow you to take advantage of some of Amazon.com?s
essential features, we recommend that you leave them turned on. For
instance, if you block or otherwise reject our cookies, you will not be
able to add items to your Shopping Cart, proceed to Checkout, or use any
Amazon.com products and services that require you to Sign in...
Walmart Privacy Policy
...
Information We Collect
...We use ?cookies? to recognize you as you use or return to our sites.
This is done so that we can provide a continuous and more personalized
shopping experience for you. A cookie is a small text file that a website
or email may save to your browser and store on your hard drive...
Your Choices
...You may exercise choices related to our online operations and adver-
tising. For instance, you can choose to browse our websites without ac-
cepting cookies. Please know that cookies allow us to recognize you from
page to page, and they support your transactions with us. Without cookies
enabled, you will still be able to browse our websites, but will not be able
to complete a purchase or take advantage of certain website features...
Table 1: Example privacy statements from Amazon.com (left) and Walmart.com (right). The statements
are concerned with the websites? cookie policy. The top-most level section subtitles are shown in bold.
step toward interpretation of the documents of direct interest here, and also that automatic alignment of
a large set of similarly-constructed documents might find application elsewhere.
Consider the example in Table 1, where we show privacy statements from Amazon.com
2
and Wal-
mart.com.
3
These statements are concerned with the usage of cookies?small data files transferred by a
website to the user?s computer hard drive?often used for tracking a user?s browsing behavior. Cookies
are one issue among many that are addressed by privacy policies; by aligning segments by issue, across
policies, we can begin to understand the range of policy approaches for each issue.
We contribute pairwise annotations of segment pairs drawn from different policies, for use in evalu-
ating the quality of alignments, an analysis of the inter-annotator reliability, and an experimental assess-
ment of three alignment methods, one based on clustering and two based on a hidden Markov model.
This paper?s results refine the findings of Ramanath et al. (2014). Our key finding is that these unsuper-
vised methods reach far better agreement with the consensus of crowdworkers than originally estimated,
and that the gap between these methods and the ?median? crowdworker is about half due to the greedy
nature of such methods and about half due to the bag of words representation.
2 Privacy Dataset and Annotations
For completeness, we review the corpus of privacy policies presented by Ramanath et al. (2014), and
then present the new annotations created for evaluation of alignment.
2.1 Corpus
We collected 1,010 unique privacy policy documents from the top websites ranked by Alexa.com.
4
These
policies were collected during a period of six weeks during December 2013 and January 2014. They are a
snapshot of privacy policies of mainstream websites covering fifteen of Alexa.com?s seventeen categories
(Table 2).
5
Finding a website?s policy is not trivial. Though many well-regulated commercial websites provide a
?privacy? link on their homepages, not all do. We found university websites to be exceptionally unlikely
to provide such a link. Even once the policy?s URL is identified, extracting the text presents the usual
challenges associated with scraping documents from the web. Since every site is different in its place-
ment of the document (e.g., buried deep within the website, distributed across several pages, or mingled
together with Terms of Service) and format (e.g., HTML, PDF, etc.), and since we wish to preserve as
much document structure as possible (e.g., section labels), full automation was not a viable solution.
2
https://www.amazon.com/gp/help/customer/display.html?nodeId=468496
3
http://corporate.walmart.com/privacy-security/walmart-privacy-policy
4
http://www.alexa.com
5
The ?Adult? category was excluded; the ?World? category was excluded since it contains mainly popular websites in
different languages, and we opted to focus on policies in English in this first stage of research, though multilingual policy
analysis presents interesting challenges for future work.
885
Sections Paragraphs Sections Paragraphs
Category Count Length Count Length Category Count Length Count Length
Arts 11.1 254.8 39.2 72.1 Recreation 11.9 218.8 38.5 67.4
Business 10.0 244.2 37.6 65.1 Reference 9.7 179.4 26.2 66.3
Computers 10.5 213.4 34.4 65.4 Regional 10.2 207.7 36.0 59.1
Games 10.0 244.1 34.9 70.1 Science 8.7 155.0 22.1 61.0
Health 9.9 228.2 32.4 69.4 Shopping 11.9 213.9 39.3 64.8
Home 11.6 201.5 32.4 72.0 Society 9.8 230.8 32.6 69.3
Kids and Teens 9.6 231.5 32.3 68.6 Sports 10.1 217.1 29.1 75.6
News 10.3 248.4 35.5 72.4 Average 10.4 221.9 34.1 68.0
Table 2: Fifteen website categories, average number of sections and paragraphs per document in that
category, and average length in word tokens.
We therefore crowdsourced the privacy policy document collection using Amazon Mechanical Turk.
For each website, we created a HIT in which a worker was asked to copy and paste the following privacy
policy-related information into text boxes: (i) privacy policy URL; (ii) last updated date (or effective
date) of the current privacy policy; (iii) privacy policy full text; and (iv) the section subtitles in the
top-most layer of the privacy policy. To identify the privacy policy URL, workers were encouraged to
go to the website and search for the privacy link. Alternatively, they could form a search query using
the website name and ?privacy policy? (e.g., ?Amazon.com privacy policy?) and search in the returned
results for the most appropriate privacy policy URL. Each HIT was completed by three workers, paid
$0.05, for a total cost of $380 (including Amazon?s surcharge). After excluding dupliates, the dataset
contains 1,010 unique documents.
6
Given the privacy policy full text and the section subtitles, we partition the full privacy document into
different sections, delimited by the section subtitles. To generate paragraphs, we break the sections by
lines, and consider each line as a paragraph. We require a paragraph to end with a period, if not, it will
be concatenated with the next paragraph. Using this partition scheme, sections contain 12 sentences on
average; and paragraphs contain 4 sentences on average. More statistics are presented in Table 2.
2.2 Pairwise Annotations
Ramanath et al. (2014) described an evaluation method in which pairs of privacy policy sections were
annotated by crowdworkers.
7
A sample of section pairs from different policies was drawn, stratified
by cosine similarity of unigram tfidf vectors. In a single task, a crowdworker was asked whether two
sections broadly discussed the same topic. The question was presented alongside three answer options,
essentially a strong yes, a yes, and a no. In that initial exploration, each item was annotated at least three
times, and up to fifteen, until an absolute majority was reached.
The annotations conducted for this study were done somewhat differently. Our motivations were to
enable a more careful exploration of inter-annotator agreement, which was complicated in the earlier
work by the variable number of annotations per pair, from three to fifteen. We also sought to explore a
more fine-grained problem at the paragraph level.
We sampled 1,000 document pairs from each of the 15 categories, then generated pairs (separately of
sections and of paragraphs) by choosing one at random from each document. In total, 1,278,204 section
pairs and 7,968,487 paragraph pairs were produced. These pairs were stratified by cosine similarity
intervals: [0, 0.25], (0.25, 0.5], (0.5, 0.75], (0.75, 1], as in Ramanath et al. (2014). We sampled 250 pairs
from each interval, resulting in 1,000 pairs each of sections and paragraphs.
These pairs were annotated on Amazon Mechanical Turk. The crowdworkers were instructed to care-
fully read the privacy statements and answer a ?yes/no? question, indicating whether the two texts are
discussing the same privacy issue or not. Several key privacy issues are provided as examples, including
6
Note that different websites may be covered by the same privacy policy provided by the parent company. For example,
espn.go.com, abc.go.com, and marvel.com are all covered under the Walt Disney privacy policy.
7
Another evaluation, based on text selected by humans in a separate, unrelated task, was also explored. Because such an
evaluation seems less broadly applicable, we did not pursue it here.
886
Sections Paragraphs
Cosine similarity: [0, .25] (.25, .5] (.5, .75] (.75, 1] All [0, .25] (.25, .5] (.5, .75] (.75, 1] All
5 workers agree 36.4 12.4 28.0 85.2 40.5 42.4 12.0 32.8 77.6 41.2
4 workers agree 42.8 42.4 42.0 13.6 35.2 39.6 36.8 35.6 17.6 32.4
3 workers agree 20.8 45.2 30.0 1.2 24.3 18.0 51.2 31.6 4.8 26.4
Consensus-yes 4.4 45.2 87.2 99.2 59.0 9.2 66.0 88.8 98.0 65.5
Consensus-no 95.6 54.8 12.8 0.8 41.0 90.8 34.0 11.2 2.0 34.5
Table 3: Inter-annotator agreement of section and paragraph pairs.
collection of personal information, sharing of information with third parties, cookies and other tracking
techniques, data security, children policies, and contact of the websites. To encourage the crowdwork-
ers to carefully read the privacy statements, we also asked them to copy and paste 1?3 keywords from
each section/paragraph, before answering the question.
8
Each section/paragraph pair was judged by five
crowdworkers and was rewarded $0.05. In total, $550 was spent on the annotations.
On average, it took a crowdworker 2.15 minutes to complete a section pair and 1.67 minutes for a
paragraph pair. Interestingly, although a section is roughly three times the length of a paragraph (see
Table 2), the time spent on annotation is not proportional to the text length.
In Table 3, we present the inter-annotator agreement results for section and paragraph pairs, broken
down by cosine-similarity bin and by the majority answer. 75.7% (73.6%) of section (paragraph) pairs
were agreed upon by four or more out of five annotators. Unsurprisingly, disagreement is greatest in
the (.25, .5] similarity bin. Cosine similarity is a very strong predictor of the consensus answer (Pearson
correlation 0.72 for section pairs, 0.67 for paragraphs, on this stratified sample).
Ramanath et al. (2014) considered only sections. A different method was used to obtain consensus
annotations; we simply kept adding annotators to a pair until consensus was reached. For a fair compar-
ison with the new data, we calculated pairwise agreement among three annotators per item, randomly
selected if there were more than three to choose from. On the old section-level data, this was 60.5%; on
the new data, it was 71.3% (using five annotators). Although a controlled experiment in the task setup
was not conducted, we take this as a sign that our binary question with keywords led to a higher quality
set of annotations than the three-way question in the older data. Our experiments in this paper use only
the new data.
2.3 Discussion
We had expected higher agreement at the paragraph level, since paragraphs are shorter, presumably easier
to read and compare, and presumably more focused on a smaller number of issues. This was not borne
out empirically, though a slightly different analysis presented in ?4.2 suggests that, among crowdworkers
who completed ten or more tasks, paragraphs were easier to agree on.
Privacy policies are generally written by attorneys with expertise in privacy law, though there are
automatic generation solutions available that allow a non-expert to quickly fill in a template to create a
policy document.
9
Example 1 in Table 4 shows a case of very high text overlap (five out of five annotators
agreed on a ?yes? answer for this pair). While this kind of localized alignment is not our aim here, we
believe that such ?boilerplate? text, to the extent that it occurs in large numbers of policies, will make
automatic alignment easier.
A case where annotators seem not to have understood, or not taken care to read carefully, is illustrated
by Example 2 in Table 4. Both sections describe ?opt-out? options for unsubscribing from mailing lists
that send promotional messages, though the first is more generally about ?communications? and the
second only addresses email. Three out of five crowdworkers labeled this example with?no.? Achieving
better consensus might require more careful training of annotators about a predefined set of concepts at
the right granularity.
8
We have not used these keywords for any other purpose.
9
For example: http://www.rendervisionsconsulting.com/blog/wp-content/uploads/2011/09/
Privacy-policy-solutions-list_rvc.pdf
887
Example 1 Example 2
Policy excerpt from Urban Outfitters website:
To serve you better, we may combine information you give us online, in
our stores or through our catalogs. We may also combine that information
with publicly available information and information we receive from or
cross-reference with our Select Partners and others. We use that com-
bined information to enhance and personalize the shopping experience of
you and others with us, to communicate with you about our products and
events that may be of interest to you, and for other promotional purposes.
Policy excerpt from Williams-Sonoma website:
To serve you better and improve our performance, we may combine
information you give us online, in our stores or through our catalogs. We
may also combine that information with publicly available information
and information we receive from or cross-reference with select partners
and others. By combining this information we are better able to com-
municate with you about our products, special events and promotional
purposes and to personalize your shopping experience.
Policy excerpt from IKEA website:
What if I prefer not to receive communications from IKEA? If you prefer
not to receive product information or promotions from us by U.S. Mail,
please click here. To unsubscribe from our email list, please follow the
opt-out instructions at the bottom of the email you received, or click here
and update your profile by deselecting ?Please send me: Inspirational
emails and updates.?
Policy excerpt from Neiman Marcus website:
Emails. You will receive promotional emails from us only if you have
asked to receive them. If you do not want to receive email from Neiman
Marcus or its affiliates you can click on the ?Manage Your Email
Preferences? link at the bottom of any email communication sent by us.
Choose ?Unsubscribe? at the bottom of the page that opens. Please allow
us 3 business days from when the request was received to complete the
removal, as some of our promotion s may already have been in process
before you submitted your request.
Table 4: Privacy policy excerpts. Example 1 (a pair of paragraphs) illustrates the likely use of boilerplate;
identical text is marked in gray. Example 2 shows a pair of sections where our intuitions disagree with
the annotations.
3 Problem Formulation
Given a collection of privacy policy documents and assuming each document consists of a sequence
of naturally-occurring text segments (e.g., sections or paragraphs), our goal is to automatically group
the text segments that address the same privacy issue, without pre-specifying the set of such issues.
We believe this exemplifies many scenarios where a collection of documents follow a similar content
paradigm, such as legal documents and, in some cases, scientific literature. Our interest in algorithms
that characterize each individual document?s parts in the context of the corpus is inspired by biological
sequence alignment in computational biology (Durbin et al., 1998).
In our experiments, we consider a hidden Markov model (HMM) that captures local transitions be-
tween topics. The motivation for the HMM is that privacy policies might tend to order issues similarly,
e.g., the discussion on ?sharing information to third parties? appears to often follow the discussion of
?personal information collection.? If each of these corresponds to an HMM state, then the regularity in
ordering is captured by the transition distribution, and each state is characterized by its emission dis-
tribution over words. In this section, we discuss the HMM and two estimation procedures based on
Expectation-Maximization (EM) and variational Bayesian (VB) inference.
3.1 Hidden Markov Model
Assume we have a sequence of observed text segments
10
O = [O
1
, O
2
, ..., O
T
], and each O
t
represents
a text segment in a privacy document (t ? {1, 2, ..., T}). We denote O
t
= [O
1
t
, O
2
t
, ..., O
N
t
t
], where each
O
j
t
corresponds to a word token in the tth text segment; N
t
is the total number of word tokens in the
segment; T represents the total number of segments in the observation sequence. Each text segment O
t
is associated with a hidden state S
t
(S
t
? {1, 2, . . . ,K}, where K is the total number of states). Given
an observation sequence O, our goal is to decode the corresponding hidden state sequence S.
We employ a first-order hidden Markov model where the next state depends only on the previous state.
A notable difference from the familiar HMM used in NLP (e.g., as used for part-of-speech tagging) is that
we allow multiple observation symbols to be emitted from each hidden state. Each symbol corresponds
to a word token in the text segment. Hence the likelihood for a single document can be written as:
L(?, ?) =
?
S?{1,...,K}
T
p(O,S | ?, ?) =
?
S?{1,...,K}
T
T+1
?
t=1
?
S
t
|S
t?1
N
t
?
j=1
?
O
j
t
|S
t
(1)
10
We use segments to refer abstractly to either sections or paragraphs. In any given instantiation, one or the other is used,
never a blend.
888
E-step:
Forward pass: ?
1
(?) = 1; ?
t
(k) =
?
K
k
?
=1
?
t?1
(k
?
) ? ?
k|k
?
?
?
N
t
j=1
?
O
j
t
|k
, ?t ? {2, . . . , T}, ?k ? {1, . . . ,K}(2)
Backward pass: ?
T+1
(?) = 1; ?
t
(k) =
?
K
k
?
=1
?
k
?
|k
?
?
N
t
j=1
?
O
j
t
|k
?
? ?
t+1
(k
?
), ?t ? {T, . . . , 1}, ?k ? {1, . . . ,K}
(3)
Likelihood: p(O | ?, ?) = p(O
1,
O
2
, ..., O
T
| ?, ?) =
?
K
k=1
?
t
(k) ? ?
t
(k) (for any t) (4)
Posteriors: ?
t
(k) = p(S
t
= k | O, ?, ?) =
?
t
(k) ? ?
t
(k)
p(O | ?, ?)
(5)
Pair posteriors: ?
t
(k, k
?
) = p(S
t
= k, S
t+1
= k
?
| O, ?, ?) =
?
t
(k) ? ?
k
?
|k
?
(
?
N
t+1
j=1
?
O
j
t+1
|k
?
)
? ?
t+1
(k
?
)
p(O | ?, ?)
(6)
M-step (in EM):
Transitions: ?
k
?
|k
=
?
T
t=1
?
t
(k, k
?
)
?
T
t=1
?
K
k
??
=1
?
t
(k, k
??
)
; Emissions: ?
v|k
=
?
T
t=1
?
t
(k) ?
?
N
t
j=1
1{O
j
t
= v}
?
T
t=1
?
t
(k) ?N
t
(7)
Variational update (in VB):
?
k
?
|k
=
exp ?
(
?
T
t=1
?
t
(k, k
?
) + ?
)
exp ?
(
?
T
t=1
?
K
k
??
=1
?
t
(k, k
??
) + ? ?K
)
; ?
v|k
=
exp ?
(
?
T
t=1
?
t
(k) ?
?
N
t
j=1
1{O
j
t
= v}+ ?
?
)
exp ?
(
?
T
t=1
?
t
(k) ?N
t
+ ?
?
? V
)
(8)
Table 5: Equations for parameter estimation of the HMM with multiple emissions at each state and a
single sequence. K is the number of states, V is the emission vocabulary size, and T is the length of the
sequence in sections. ?(?) is the digamma function.
?
k
?
|k
denotes the probability of transitioning to state k
?
given that the preceding state is k. ?
v|k
denotes
the probability that a particular symbol emitted during a visit to state k is the word v. As in standard
treatments, we assume an extra final state at the end of the sequence that emits a stop symbol.
Ramanath et al. (2014) considered three variants of the HMM, with different constraints on the tran-
sitions, such as a ?strict forward? variant that orders the states and only allows transition to ?later? states
than the current one. In the evaluation against direct human judgments, they found a slight benefit from
such constraints, but they increased performance variance considerably. Here we only consider an un-
constrained HMM.
3.2 EM and VB
We consider two estimation methods, neither novel. Both are greedy hillclimbing methods that locally
optimize functions based on likelihood under the HMM.
The first method is EM, adapted for the multiple emission case; the equations for the E-step (forward-
backward algorithm and subsequent posterior calculations) and the M-step are shown in Table 5.
We also consider Bayesian inference, which seeks to marginalize out the parameter values, since we
are really only interested in the assignment of sections to hidden states. Further, Bayesian inference
has been found favorable on small datasets (Gao and Johnson, 2008). We assume symmetric Dirichlet
priors on the transition and emission distributions, parameterized respectively by ? = 1 and ?
?
= 0.1.
We apply mean-field variational approximate inference as described by Beal (2003), which amounts to
an EM-like procedure. The E-step is identical to EM, and the M-step involves a transformation of the
expected counts, shown in Table 5. (We also explored Gibbs sampling; performance was less stable but
generally similar; for clarity we do not report the results here.)
889
3.3 Implementation Details
In modeling, the vocabulary excludes 429 stopwords,
11
words whose document frequency is less than
ten, and a set of terms specific to website privacy polices: privacy, policy, personal, information, service,
web, site, website, com, and please. After lemmatizing, the vocabulary contains V = 2,876 words.
We further exclude sections and paragraphs that contain less than 10 words. Many of these are not
meaningful statements, e.g., ?return to top.? This results in 9,935 sections and 27,594 paragraphs in the
experiments.
During estimation, we concatenate all segments into a single sequence, delimited by a special bound-
ary symbol. This does not affect the outcome (due to the first-order conditions; it essentially conflates
?start? and ?stop? states), but gave some efficiency gains in our implementation.
EM or VB iterations continue until one of two stopping criteria is met: either 100 iterations have
passed, or the relative change in log-likelihood (or the variational bound in the case of VB) falls below
10
?4
; this consistently happens within forty iterations.
After estimating parameters, we decode using the Viterbi algorithm.
4 Experiments
Our experiments compare three methods for aligning segments of privacy policies, at both the paragraph
and the section level:
? A greedy divising clustering algorithm, as implemented in CLUTO.
12
The algorithm performs a
sequence of bisections until the desired number of clusters is reached. In each step, a cluster is
selected and partitioned so as to optimize the clustering criterion. CLUTO demonstrated robust
performance in several related NLP tasks (Zhong and Ghosh, 2005; Lin and Bilmes, 2011; Chen et
al., 2011).
? The Viterbi state assignment from the EM-estimated HMM. We report averaged results over ten
runs, with random initialization.
? The Viterbi state assignment after VB inference, using the mean field parameters. We report aver-
aged results over ten runs, with random initialization.
Our evaluation metrics are precision, recall, and F -score on the identification of section or paragraph
pairs annotated ?yes.?
4.1 Results
In Figure 1, we present performance of different algorithms using a range of hidden state values K ?
{1, 2, . . . , 20}. The top row shows precision, recall and F -scores on section pairs, the bottom row on
paragraph pairs.
The algorithms mostly perform similarly. At the section level, we find the clustering algorithm to
perform better in terms of F -score than the HMM with larger K; at K = 10 the two are very close.
13
CLUTO?s best performance, 85%, was achieved by K = 14.
At the paragraph level, the HMMs outperform clustering in the K ? [5, 15) range, and this is where
the peak F -score is obtained (87%). We do not believe these differences among algorithms are espe-
cially important, noting only that the HMM?s advantage is that it does not require pairwise similarity
calculations between all section pairs.
890
Figure 1: Performance results against pairwise annotations when using different number of hidden states
K ? {1, . . . , 20}. The top row is at the section level, the bottom row at the paragraph level.
4.2 Upper Bounds
How do these automatic alignment methods compare with the levels of agreement reached among crowd-
workers? We consider the agreement rate of each method, at varying values of K, with the majority vote
of the annotators. Note that this is distinct from the positive-match?focused precision, recall, and F -
score measures presented in ?4.1. For each crowdworker who completed ten tasks or more, and therefore
for whom we have hope of a reliable estimate, we calculated her agreement rate with the majority. For
sections, this set included 65 out of 162 crowdworkers; for paragraphs, 76 out of 197.
In Figure 2 we show the three quartile points for this agreement measure, across the pool of ten-or-
more-item crowdworkers, in comparison to the various automatic methods. For sections, our systems
perform on par with the 25% of crowdworkers just below the median. For paragraphs, which show a
generally higher level of agreement among this subset of crowdworkers, our systems are on par with the
lowest 25% of workers. We take all of this to suggest that there is room for improvement in methods
overall.
Given the observation in ?2 that cosine similarity of two segments? tfidf vectors is a very strong pre-
dictor of human agreement on whether they are about the same issue, we also consider a threshold on
cosine similarity for deciding whether a pair is about the same issue. This is not a complete solution to
the problem of alignment, since pairwise scores only provide groupings if they are coupled with a tran-
sitivity constraint. The clustering and HMM methods can be understood as greedy approximations to
such an approach. We therefore view cosine similarity thresholding as an upper bound for bag of words
representations on the pairwise evaluation task. Figure 2 includes agreement levels for oracle cosine
similarity thresholding.
14
11
http://www.lextek.com/manuals/onix/stopwords1.html
12
http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
13
Ramanath et al. (2014) only considered K = 10 and found a K = 10 HMM to outperform clustering at the section level;
the scores reported there, on the earlier dataset, are much lower and not comparable to those reported here. There are numerous
differences between the setup here and the earlier one. The most important, we believe, are the improved quality of the dataset
and greater care given to preprocessing, most notably the pruning of documents and vocabulary, in the present experiments.
14
For comparison with the results in ?4.1, we found that, for sections, oracle thresholding (at 0.3) achieved F -score of 0.87,
and for paragraphs, oracle thresholding (at 0.2) achieved 0.90.
891
Figure 2: Agreement rates, as compared to crowdworkers and a cosine similarity oracle.
Taken together, this analysis suggests that?in principle?an automated approach based on word-level
similarity could close about half of the gap between our methods and median crowdworkers, and further
gains would require more sophisticated representations or similarity measures.
5 Related Work
There has been little work on applying NLP to privacy policies. Some have sought to parse privacy
policies into machine-readable representations (Brodie et al., 2006) or extract sub-policies from larger
documents (Xiao et al., 2012). Machine learning has been applied to assess certain attributes of policies
(Costante et al., 2012; Costante et al., 2013), e.g., compliance of privacy policies to legal regulations
(Krachina et al., 2007) or simple categorical questions about privacy policies (Ammar et al., 2012; Zim-
meck and Bellovin, 2014).
Our alignment-style analysis is motivated by an expectation that many policies will address similar
issues,
15
such as collection of a user?s contact, location, health, and financial information, sharing with
third parties, and deletion of data. This expectation is supported by recommendation by privacy experts
(Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector,
the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al.
(2013) describe our larger research initiative to incorporate automation into privacy policy analysis.
Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS
tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson,
2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques,
and future exploration might consider their use in automatically discovering document sections (Eisen-
stein and Barzilay, 2008), rather than fixing section or paragraph boundaries.
6 Conclusion
This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies.
We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an explo-
ration of clustering and HMM-based alignment methods. Our results show that these algorithms achieve
agreement on par with the lower half of crowdworkers, with about half of the difference from the median
due to the bag of words representation and half due to the inherent greediness of the methods.
Acknowledgments
The authors gratefully acknowledge helpful comments from Lorrie Cranor, Joel Reidenberg, Florian
Schaub, and several anonymous reviewers. This research was supported by NSF grant SaTC-1330596.
15
Personal communication, Joel Reidenberg.
892
References
Waleed Ammar, Shomir Wilson, Norman Sadeh, and Noah A. Smith. 2012. Automatic categorization of privacy
policies: A pilot study. Technical Report CMU-LTI-12-019, Carnegie Mellon University.
Matthew J. Beal. 2003. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, Gatsby Com-
putational Neuroscience unit, University College London.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine
Learning Research.
Travis D. Breaux, Hanan Hibshi, and Ashwini Rao. 2014. Eddy, A formal language for specifying and analyzing
data flow specifications for conflicting privacy requirements. Requirements Engineering Journal.
Carolyn A. Brodie, Clare-Marie Karat, and John Karat. 2006. An empirical study of natural language parsing
of privacy policy rules using the SPARCLE policy workbench. In Proceedings of the Second Symposium on
Usable Privacy and Security (SOUPS).
Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay. 2011. In-domain relation discovery with meta-
constraints via posterior regularization. In Proceedings of ACL-HLT.
Elisa Costante, Yuanhao Sun, Milan Petkovi?c, and Jerry den Hartog. 2012. A machine learning solution to assess
privacy policy completeness. In Proceedings of the ACM Workshop on Privacy in the Electronic Society.
Elisa Costante, Jerry Hartog, and Milan Petkovi. 2013. What websites know about you. In Roberto Pietro, Javier
Herranz, Ernesto Damiani, and Radu State, editors, Data Privacy Management and Autonomous Spontaneous
Security, volume 7731 of Lecture Notes in Computer Science, pages 146?159. Springer Berlin Heidelberg.
Lorrie Faith Cranor. 2002. Web Privacy with P3P. O?Reilly & Associates.
Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis:
Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of ACL.
Federal Trade Commission. 2012. Protecting consumer privacy in an era of rapid change: Recom-
mendations for businesses and policymakers. Available at http://www.ftc.gov/reports/
protecting-consumer-privacy-era-rapid-change-recommendations-businesses-policymakers.
Jianfeng Gao and Mark Johnson. 2008. A comparison of Bayesian estimators for unsupervised Hidden Markov
model POS taggers. In Proceedings of EMNLP.
Robert Gellman. 2014. Fair information practices: a basic history (v. 2.11). Available at http://www.
bobgellman.com/rg-docs/rg-FIPShistory.pdf.
Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of ACL.
Mark Johnson. 2007. Why doesnt EM find good HMM POS-taggers? In Proceedings of EMNLP?CoNLL.
Patrick Gage Kelley, Lucian Cesca, Joanna Bresee, and Lorrie Faith Cranor. 2010. Standardizing privacy notices:
An online study of the nutrition label approach. In Proceedings of CHI.
Olga Krachina, Victor Raskin, and Katrina Triezenberg. 2007. Reconciling privacy policies and regulations:
Ontological semantics perspective. In Michael J. Smith and Gavriel Salvendy, editors, Human Interface and the
Management of Information. Interacting in Information Environments, pages 730?739. Springer.
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of
ACL.
Jialiu Lin, Shahriyar Amini, Jason I. Hong, Norman Sadeh, Janne Lindqvist, and Joy Zhang. 2012. Expectation
and purpose: Understanding users? mental models of mobile app privacy through crowdsourcing. In Proceed-
ings of ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp).
Bin Liu, Jialiu Lin, and Norman Sadeh. 2014. Reconciling mobile app privacy and usability on smartphones:
Could user privacy profiles help? In Proceedings of WWW.
Aleecia M. McDonald and Lorrie Faith Cranor. 2008. The cost of reading privacy policies. I/S: A Journal of Law
and Policy for the Information Society.
893
Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155?
171.
Rohan Ramanath, Fei Liu, Norman Sadeh, and Noah A. Smith. 2014. Unsupervised alignment of privacy policies
using hidden Markov models. In Proceedings of ACL.
Norman Sadeh, Alessandro Acquisti, Travis Breaux, Lorrie Cranor, Aleecia McDonald, Joel Reidenberg, Noah
Smith, Fei Liu, Cameron Russel, Florian Schaub, and Shomir Wilson. 2013. The usable privacy policy project:
Combining crowdsourcing, machine learning and natural language processing to semi-automatically answer
those privacy questions users care about. Technical Report CMU-ISR-13-119, Carnegie Mellon University.
Xusheng Xiao, Amit Paradkar, Suresh Thummalapenta, and Tao Xie. 2012. Automated extraction of security
policies from natural-language software documents. In Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering.
Shi Zhong and Joydeep Ghosh. 2005. Generative model-based document clustering: a comparative study. Knowl-
edge and Information Systems, 8(3):374?384.
Sebastian Zimmeck and Steven M. Bellovin. 2014. Privee: An architecture for automatically analyzing web
privacy policies. In Proceedings of the 23rd USENIX Security Symposium.
894
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34?44,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Turbo Parsers: Dependency Parsing by Approximate Variational Inference
Andre? F. T. Martins?? Noah A. Smith? Eric P. Xing?
?School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{afm,nasmith,epxing}@cs.cmu.edu
Pedro M. Q. Aguiar?
?Instituto de Sistemas e Robo?tica
Instituto Superior Te?cnico
Lisboa, Portugal
aguiar@isr.ist.utl.pt
Ma?rio A. T. Figueiredo?
?Instituto de Telecomunicac?o?es
Instituto Superior Te?cnico
Lisboa, Portugal
mtf@lx.it.pt
Abstract
We present a unified view of two state-of-the-
art non-projective dependency parsers, both
approximate: the loopy belief propagation
parser of Smith and Eisner (2008) and the re-
laxed linear program of Martins et al (2009).
By representing the model assumptions with
a factor graph, we shed light on the optimiza-
tion problems tackled in each method. We also
propose a new aggressive online algorithm to
learn the model parameters, which makes use
of the underlying variational representation.
The algorithm does not require a learning rate
parameter and provides a single framework for
a wide family of convex loss functions, includ-
ing CRFs and structured SVMs. Experiments
show state-of-the-art performance for 14 lan-
guages.
1 Introduction
Feature-rich discriminative models that break local-
ity/independence assumptions can boost a parser?s
performance (McDonald et al, 2006; Huang, 2008;
Finkel et al, 2008; Smith and Eisner, 2008; Martins
et al, 2009; Koo and Collins, 2010). Often, infer-
ence with such models becomes computationally in-
tractable, causing a demand for understanding and
improving approximate parsing algorithms.
In this paper, we show a formal connection be-
tween two recently-proposed approximate inference
techniques for non-projective dependency parsing:
loopy belief propagation (Smith and Eisner, 2008)
and linear programming relaxation (Martins et al,
2009). While those two parsers are differently moti-
vated, we show that both correspond to inference in
a factor graph, and both optimize objective functions
over local approximations of the marginal polytope.
The connection is made clear by writing the explicit
declarative optimization problem underlying Smith
and Eisner (2008) and by showing the factor graph
underlying Martins et al (2009). The success of
both approaches parallels similar approximations in
other fields, such as statistical image processing and
error-correcting coding. Throughtout, we call these
turbo parsers.1
Our contributions are not limited to dependency
parsing: we present a general method for inference
in factor graphs with hard constraints (?2), which
extends some combinatorial factors considered by
Smith and Eisner (2008). After presenting a geo-
metric view of the variational approximations un-
derlying message-passing algorithms (?3), and clos-
ing the gap between the two aforementioned parsers
(?4), we consider the problem of learning the model
parameters (?5). To this end, we propose an ag-
gressive online algorithm that generalizes MIRA
(Crammer et al, 2006) to arbitrary loss functions.
We adopt a family of losses subsuming CRFs (Laf-
ferty et al, 2001) and structured SVMs (Taskar et
al., 2003; Tsochantaridis et al, 2004). Finally, we
present a technique for including features not at-
tested in the training data, allowing for richer mod-
els without substantial runtime costs. Our experi-
ments (?6) show state-of-the-art performance on de-
pendency parsing benchmarks.
1The name stems from ?turbo codes,? a class of high-
performance error-correcting codes introduced by Berrou et al
(1993) for which decoding algorithms are equivalent to running
belief propagation in a graph with loops (McEliece et al, 1998).
34
2 Structured Inference and Factor Graphs
Denote by X a set of input objects from which we
want to infer some hidden structure conveyed in an
output set Y. Each input x ? X (e.g., a sentence)
is associated with a set of candidate outputs Y(x) ?
Y (e.g., parse trees); we are interested in the case
where Y(x) is a large structured set.
Choices about the representation of elements of
Y(x) play a major role in algorithm design. In
many problems, the elements of Y(x) can be rep-
resented as discrete-valued vectors of the form y =
?y1, . . . , yI?, each yi taking values in a label set Yi.
For example, in unlabeled dependency parsing, I is
the number of candidate dependency arcs (quadratic
in the sentence length), and each Yi = {0, 1}. Of
course, the yi are highly interdependent.
Factor Graphs. Probabilistic models like CRFs
(Lafferty et al, 2001) assume a factorization of the
conditional distribution of Y ,
Pr(Y = y | X = x) ?
?
C?C ?C(x,yC), (1)
where each C ? {1, . . . , I} is a factor, C is the set
of factors, each yC , ?yi?i?C denotes a partial out-
put assignment, and each ?C is a nonnegative po-
tential function that depends on the output only via
its restriction to C. A factor graph (Kschischang
et al, 2001) is a convenient representation for the
factorization in Eq. 1: it is a bipartite graph Gx com-
prised of variable nodes {1, . . . , I} and factor nodes
C ? C, with an edge connecting the ith variable
node and a factor node C iff i ? C. Hence, the fac-
tor graph Gx makes explicit the direct dependencies
among the variables {y1, . . . , yI}.
Factor graphs have been used for several NLP
tasks, such as dependency parsing, segmentation,
and co-reference resolution (Sutton et al, 2007;
Smith and Eisner, 2008; McCallum et al, 2009).
Hard and Soft Constraint Factors. It may be
the case that valid outputs are a proper subset of
Y1 ? ? ? ? ? YI?for example, in dependency pars-
ing, the entries of the output vector y must jointly
define a spanning tree. This requires hard constraint
factors that rule out forbidden partial assignments
by mapping them to zero potential values. See Ta-
ble 1 for an inventory of hard constraint factors used
in this paper. Factors that are not of this special kind
are called soft factors, and have strictly positive po-
tentials. We thus have a partition C = Chard ? Csoft.
We let the soft factor potentials take the form
?C(x,yC) , exp(?>?C(x,yC)), where ? ? R
d
is a vector of parameters (shared across factors) and
?C(x,yC) is a local feature vector. The conditional
distribution of Y (Eq. 1) thus becomes log-linear:
Pr?(y|x) = Zx(?)
?1 exp(?>?(x,y)), (2)
where Zx(?) ,
?
y??Y(x) exp(?
>?(x,y?)) is the
partition function, and the features decompose as:
?(x,y) ,
?
C?Csoft
?C(x,yC). (3)
Dependency Parsing. Smith and Eisner (2008)
proposed a factor graph representation for depen-
dency parsing (Fig. 1). The graph has O(n2) vari-
able nodes (n is the sentence length), one per candi-
date arc a , ?h,m? linking a head h and modifier
m. Outputs are binary, with ya = 1 iff arc a belongs
to the dependency tree. There is a hard factor TREE
connected to all variables, that constrains the overall
arc configurations to form a spanning tree. There is a
unary soft factor per arc, whose log-potential reflects
the score of that arc. There are also O(n3) pair-
wise factors; their log-potentials reflect the scores
of sibling and grandparent arcs. These factors cre-
ate loops, thus calling for approximate inference.
Without them, the model is arc-factored, and ex-
act inference in it is well studied: finding the most
probable parse tree takes O(n3) time with the Chu-
Liu-Edmonds algorithm (McDonald et al, 2005),2
and computing posterior marginals for all arcs takes
O(n3) time via the matrix-tree theorem (Smith and
Smith, 2007; Koo et al, 2007).
Message-passing algorithms. In general
factor graphs, both inference problems?
obtaining the most probable output (the MAP)
argmaxy?Y(x) Pr?(y|x), and computing the
marginals Pr?(Yi = yi|x)?can be addressed
with the belief propagation (BP) algorithm (Pearl,
1988), which iteratively passes messages between
variables and factors reflecting their local ?beliefs.?
2There is a faster but more involvedO(n2) algorithm due to
Tarjan (1977).
35
A general binary factor: ?C(v1, . . . , vn) =
?
1 v1, . . . , vn ? SC
0 otherwise,
where SC ? {0, 1}n.
?Message-induced distribution: ? , ?mj?C?j=1,...,n ? Partition function: ZC(?) ,
P
?v1,...,vn??SC
Qn
i=1m
vi
i?C
?Marginals: MARGi(?) , Pr?{Vi = 1|?V1, . . . , Vn? ? SC} ?Max-marginals: MAX-MARGi,b(?) , maxv?SC Pr?(v|vi = b)
? Sum-prod.: mC?i = m
?1
i?C ? MARGi(?)/(1? MARGi(?)) ?Max-prod.: mC?i = m
?1
i?C ? MAX-MARGi,1(?)/MAX-MARGi,0(?)
? Local agreem. constr.: z ? conv SC , where z = ??i(1)?ni=1 ? Entropy: HC = logZC(?)?
Pn
i=1 MARGi(?) logmi?C
TREE ?TREE(?ya?a?A) =
?
1 y ? Ytree (i.e., {a ? A | ya = 1} is a directed spanning tree)
0 otherwise,
where A is the set of candidate arcs.
? Partition function Ztree(?) and marginals ?MARGa(?)?a?A computed via the matrix-tree theorem, with ? , ?ma?TREE?a?A
? Sum-prod.: mTREE?a = m
?1
a?TREE ? MARGa(?)/(1? MARGa(?))
?Max-prod.: mTREE?a = m
?1
a?TREE ? MAX-MARGa,1(?)/MAX-MARGa,0(?), where MAX-MARGa,b(?) , maxy?Ytree Pr?(y|ya = b)
? Local agreem. constr.: z ? Ztree, where Ztree , convYtree is the arborescence polytope
? Entropy: Htree = logZtree(?)?
P
a?A MARGa(?) logma?TREE
XOR (?one-hot?) ?XOR(v1, . . . , vn) =
?
1
Pn
i=1 vi = 1
0 otherwise.
? Sum-prod.: mXOR?i =
?P
j 6=imj?XOR
??1
?Max-prod.: mXOR?i =
`
maxj 6=imj?XOR
??1
? Local agreem. constr.:
P
i zi = 1, zi ? [0, 1],?i ?HXOR = ?
P
i(mi?XOR/
P
j mj?XOR) log(mi?XOR/
P
j mj?XOR)
OR ?OR(v1, . . . , vn) =
?
1
Pn
i=1 vi ? 1
0 otherwise.
? Sum-prod.: mOR?i =
?
1?
Q
j 6=i(1 +mj?OR)
?1
??1
?Max-prod.: mOR?i = max{1,minj 6=im
?1
j?OR}
? Local agreem. constr.:
P
i zi ? 1, zi ? [0, 1],?i
OR-WITH-OUTPUT ?OR-OUT(v1, . . . , vn) =
?
1 vn =
Wn?1
i=1 vi
0 otherwise.
? Sum-prod.: mOR-OUT?i =
( ?
1? (1?m?1n?OR-OUT)
Q
j 6=i,n(1 +mj?OR-OUT)
?1
??1
i < n
Q
j 6=n(1 +mj?OR-OUT)? 1 i = n.
?Max-prod.: mOR-OUT?i =
(
min mn?OR-OUT
Q
j 6=i,n max{1,mj?OR-OUT},max{1,minj 6=i,nm
?1
j?OR-OUT}
o
i < n
Q
j 6=n max{1,mj?OR-OUT}min{1,maxj 6=nmj?OR-OUT} i = n.
Table 1: Hard constraint factors, their potentials, messages, and entropies. The top row shows expressions for a
general binary factor: each outgoing message is computed from incoming marginals (in the sum-product case), or
max-marginals (in the max-product case); the entropy of the factor (see ?3) is computed from these marginals and the
partition function; the local agreement constraints (?4) involve the convex hull of the set SC of allowed configurations
(see footnote 5). The TREE, XOR, OR and OR-WITH-OUTPUT factors allow tractable computation of all these quantities
(rows 2?5). Two of these factors (TREE and XOR) had been proposed by Smith and Eisner (2008); we provide further
information (max-product messages, entropies, and local agreement constraints). Factors OR and OR-WITH-OUTPUT
are novel to the best of our knowledge. This inventory covers many cases, since the above formulae can be extended
to the case where some inputs are negated: just replace the corresponding messages by their reciprocal, vi by 1? vi,
etc. This allows building factors NAND (an OR factor with negated inputs), IMPLY (a 2-input OR with the first input
negated), and XOR-WITH-OUTPUT (an XOR factor with the last input negated).
In sum-product BP, the messages take the form:3
Mi?C(yi) ?
?
D 6=CMD?i(yi) (4)
MC?i(yi) ?
?
yC?yi
?C(yC)
?
j 6=iMj?C(yj). (5)
In max-product BP, the summation in Eq. 5 is re-
placed by a maximization. Upon convergence, vari-
able and factor beliefs are computed as:
?i(yi) ?
?
CMC?i(yi) (6)
?C(yC) ? ?C(yC)
?
iMi?C(yi). (7)
BP is exact when the factor graph is a tree: in the
sum-product case, the beliefs in Eqs. 6?7 correspond
3We employ the standard ? notation, where a summa-
tion/maximization indexed by yC ? yi means that it is over
all yC with the i-th component held fixed and set to yi.
to the true marginals, and in the max-product case,
maximizing each ?i(yi) yields the MAP output. In
graphs with loops, BP is an approximate method, not
guaranteed to converge, nicknamed loopy BP. We
highlight a variational perspective of loopy BP in ?3;
for now we consider algorithmic issues. Note that
computing the factor-to-variable messages for each
factorC (Eq. 5) requires a summation/maximization
over exponentially many configurations. Fortu-
nately, for all the hard constraint factors in rows 3?5
of Table 1, this computation can be done in linear
time (and polynomial for the TREE factor)?this ex-
tends results presented in Smith and Eisner (2008).4
4The insight behind these speed-ups is that messages on
binary-valued potentials can be expressed as MC?i(yi) ?
36
TREE
1
ARC
(T,RE)
SIB
(T1RE1RE)1 2
SIB
(T1RE1RE)1 3
SIB
(T1RE1RE)2 3GRAND
(A,T1RE)1
2
ARC
(T,RE) 3
ARC
(T,RE)ARC(A,T)
Figure 1: Factor graph corresponding to the dependency
parsing model of Smith and Eisner (2008) with sibling
and grandparent features. Circles denote variable nodes,
and squares denote factor nodes. Note the loops created
by the inclusion of pairwise factors (GRAND and SIB).
In Table 1 we present closed-form expressions
for the factor-to-variable message ratios mC?i ,
MC?i(1)/MC?i(0) in terms of their variable-to-
factor counterparts mi?C , Mi?C(1)/Mi?C(0);
these ratios are all that is necessary when the vari-
ables are binary. Detailed derivations are presented
in an extended version of this paper (Martins et al,
2010b).
3 Variational Representations
Let Px , {Pr?(.|x) | ? ? Rd} be the family of all
distributions of the form in Eq. 2. We next present
an alternative parametrization for the distributions in
Px in terms of factor marginals. We will see that
each distribution can be seen as a point in the so-
called marginal polytope (Wainwright and Jordan,
2008); this will pave the way for the variational rep-
resentations to be derived next.
Parts and Output Indicators. A part is a pair
?C,yC?, where C is a soft factor and yC a partial
output assignment. We let R = {?C,yC? | C ?
Csoft,yC ?
?
i?C Yi} be the set of all parts. Given
an output y? ? Y(x), a part ?C,yC? is said to be ac-
tive if it locally matches the output, i.e., if yC = y?C .
Any output y? ? Y(x) can be mapped to a |R|-
dimensional binary vector ?(y?) indicating which
parts are active, i.e., [?(y?)]?C,yC? = 1 if yC = y
?
C
Pr{?C(YC) = 1|Yi = yi} and MC?i(yi) ?
max?C(yC)=1 Pr{YC = yC |Yi = yi}, respectively for the
sum-product and max-product cases; these probabilities are in-
duced by the messages in Eq. 4: for an event A ?
Q
i?C Yi,
Pr{YC ? A} ,
P
yC
I(yC ? A)
Q
i?CMi?C(yi).
and 0 otherwise; ?(y?) is called the output indicator
vector. This mapping allows decoupling the feature
vector in Eq. 3 as the product of an input matrix and
an output vector:
?(x,y) =
?
C?Csoft
?C(x,yC) = F(x)?(y), (8)
where F(x) is a d-by-|R| matrix whose columns
contain the part-local feature vectors ?C(x,yC).
Observe, however, that not every vector in {0, 1}|R|
corresponds necessarily to a valid output in Y(x).
Marginal Polytope. Moving to vector representa-
tions of outputs leads naturally to a geometric view
of the problem. The marginal polytope is the convex
hull5 of all the ?valid? output indicator vectors:
M(Gx) , conv{?(y) | y ? Y(x)}.
Note that M(Gx) only depends on the factor graph
Gx and the hard constraints (i.e., it is independent of
the parameters ?). The importance of the marginal
polytope stems from two facts: (i) each vertex of
M(Gx) corresponds to an output in Y(x); (ii) each
point in M(Gx) corresponds to a vector of marginal
probabilities that is realizable by some distribution
(not necessarily in Px) that factors according to Gx.
Variational Representations. We now describe
formally how the points in M(Gx) are linked to the
distributions in Px. We extend the ?canonical over-
complete parametrization? case, studied by Wain-
wright and Jordan (2008), to our scenario (common
in NLP), where arbitrary features are allowed and
the parameters are tied (shared by all factors). Let
H(Pr?(.|x)) , ?
?
y?Y(x) Pr?(y|x) log Pr?(y|x)
denote the entropy of Pr?(.|x), and E?[.] the ex-
pectation under Pr?(.|x). The component of ? ?
M(Gx) indexed by part ?C,yC? is denoted ?C(yC).
Proposition 1. There is a map coupling each distri-
bution Pr?(.|x) ? Px to a unique ? ? M(Gx) such
that E?[?(Y )] = ?. Define H(?) , H(Pr?(.|x))
if some Pr?(.|x) is coupled to ?, and H(?) = ??
if no such Pr?(.|x) exists. Then:
1. The following variational representation for the
log-partition function (mentioned in Eq. 2) holds:
logZx(?) = max
??M(Gx)
?>F(x)? +H(?). (9)
5The convex hull of {z1, . . . , zk} is the set of points that can
be written as
Pk
i=1 ?izi, where
Pk
i=1 ?i = 1 and each ?i ? 0.
37
Parameter?space Factor?log-potentials?
space???????
Marginal?polytope?
Figure 2: Dual parametrization of the distributions in
Px. Our parameter space (left) is first linearly mapped to
the space of factor log-potentials (middle). The latter is
mapped to the marginal polytope M(Gx) (right). In gen-
eral only a subset of M(Gx) is reachable from our param-
eter space. Any distribution in Px can be parametrized by
a vector ? ? Rd or by a point ? ?M(Gx).
2. The problem in Eq. 9 is convex and its solution
is attained at the factor marginals, i.e., there is a
maximizer ?? s.t. ??C(yC) = Pr?(YC = yC |x)
for each C ? C. The gradient of the log-partition
function is? logZx(?) = F(x)??.
3. The MAP y? , argmaxy?Y(x) Pr?(y|x) can be
obtained by solving the linear program
?? , ?(y?) = argmax
??M(Gx)
?>F(x)?. (10)
A proof of this proposition can be found in Mar-
tins et al (2010a). Fig. 2 provides an illustration of
the dual parametrization implied by Prop. 1.
4 Approximate Inference & Turbo Parsing
We now show how the variational machinery just
described relates to message-passing algorithms and
provides a common framework for analyzing two re-
cent dependency parsers. Later (?5), Prop. 1 is used
constructively for learning the model parameters.
4.1 Loopy BP as a Variational Approximation
For general factor graphs with loops, the marginal
polytope M(Gx) cannot be compactly specified and
the entropy term H(?) lacks a closed form, render-
ing exact optimizations in Eqs. 9?10 intractable. A
popular approximate algorithm for marginal infer-
ence is sum-product loopy BP, which passes mes-
sages as described in ?2 and, upon convergence,
computes beliefs via Eqs. 6?7. Were loopy BP exact,
these beliefs would be the true marginals and hence
a point in the marginal polytope M(Gx). However,
this need not be the case, as elucidated by Yedidia et
al. (2001) and others, who first analyzed loopy BP
from a variational perspective. The following two
approximations underlie loopy BP:
? The marginal polytope M(Gx) is approximated by
the local polytope L(Gx). This is an outer bound;
its name derives from the fact that it only imposes
local agreement constraints ?i, yi ? Yi, C ? C:
?
yi
?i(yi) = 1,
?
yC?yi
?C(yC) = ?i(yi). (11)
Namely, it is characterized by L(Gx) , {? ?
R|R|+ | Eq. 11 holds ?i, yi ? Yi, C ? C}. The
elements of L(Gx) are called pseudo-marginals.
Clearly, the true marginals satisfy Eq. 11, and
therefore M(Gx) ? L(Gx).
? The entropy H is replaced by its Bethe approx-
imation HBethe(? ) ,
?I
i=1(1 ? di)H(? i) +?
C?CH(?C), where di = |{C | i ? C}| is the
number of factors connected to the ith variable,
H(? i) , ?
?
yi
?i(yi) log ?i(yi) and H(?C) ,
?
?
yC
?C(yC) log ?C(yC).
Any stationary point of sum-product BP is a lo-
cal optimum of the variational problem in Eq. 9
with M(Gx) replaced by L(Gx) and H replaced by
HBethe (Yedidia et al, 2001). Note however that
multiple optima may exist, since HBethe is not nec-
essarily concave, and that BP may not converge.
Table 1 shows closed form expressions for the
local agreement constraints and entropies of some
hard-constraint factors, obtained by invoking Eq. 7
and observing that ?C(yC) must be zero if configu-
ration yC is forbidden. See Martins et al (2010b).
4.2 Two Dependency Turbo Parsers
We next present our main contribution: a formal
connection between two recent approximate depen-
dency parsers, which at first sight appear unrelated.
Recall that (i) Smith and Eisner (2008) proposed a
factor graph (Fig. 1) in which they run loopy BP,
and that (ii) Martins et al (2009) approximate pars-
ing as the solution of a linear program. Here, we
fill the blanks in the two approaches: we derive ex-
plicitly the variational problem addressed in (i) and
we provide the underlying factor graph in (ii). This
puts the two approaches side-by-side as approximate
methods for marginal and MAP inference. Since
both rely on ?local? approximations (in the sense
38
of Eq. 11) that ignore the loops in their graphical
models, we dub them turbo parsers by analogy with
error-correcting turbo decoders (see footnote 1).
Turbo Parser #1: Sum-Product Loopy BP. The
factor graph depicted in Fig. 1?call it Gx?includes
pairwise soft factors connecting sibling and grand-
parent arcs.6 We next characterize the local polytope
L(Gx) and the Bethe approximationHBethe inherent
in Smith and Eisner?s loopy BP algorithm.
Let A be the set of candidate arcs, and P ?
A2 the set of pairs of arcs that have factors. Let
? = ??A, ?P ? with ?A = ??a?a?A and ?P =
??ab??a,b??P . Since all variables are binary, we may
write, for each a ? A, ?a(1) = za and ?a(0) =
1 ? za, where za is a variable constrained to [0, 1].
Let zA , ?za?a?A; the local agreement constraints
at the TREE factor (see Table 1) are written as zA ?
Ztree(x), where Ztree(x) is the arborescence poly-
tope, i.e., the convex hull of all incidence vectors
of dependency trees (Martins et al, 2009). It is
straightforward to write a contingency table and ob-
tain the following local agreement constraints at the
pairwise factors:
?ab(1, 1) = zab, ?ab(0, 0) = 1? za ? zb + zab
?ab(1, 0) = za ? zab, ?ab(0, 1) = zb ? zab.
Noting that all these pseudo-marginals are con-
strained to the unit interval, one can get rid of all
variables ?ab and write everything as
za ? [0, 1], zb ? [0, 1], zab ? [0, 1],
zab ? za, zab ? zb, zab ? za + zb ? 1,
(12)
inequalities which, along with zA ? Ztree(x), de-
fine the local polytope L(Gx). As for the factor en-
tropies, start by noting that the TREE-factor entropy
Htree can be obtained in closed form by computing
the marginals z?A and the partition function Zx(?)
(via the matrix-tree theorem) and recalling the vari-
ational representation in Eq. 9, yielding Htree =
logZx(?)? ?>F(x)z?A. Some algebra allows writ-
ing the overall Bethe entropy approximation as:
HBethe(? ) = Htree(zA)?
?
?a,b??P
Ia;b(za, zb, zab), (13)
where we introduced the mutual information asso-
ciated with each pairwise factor, Ia;b(za, zb, zab) =
6Smith and Eisner (2008) also proposed other variants with
more factors, which we omit for brevity.
TRE1
ACTRTE(
TRE1
A1TRTE(
,)SIARTE(
,)SIB23GRNDARTE(
TRE1AATTE( TRE1AAT1TE(
,)SI
AATE(
TRE1BNDRS)AATE(
)
AATR(TRE1AATRTE(
TRE1BG,RGDB)AATRTE(
)ACTR(
)A1TR(
GRDB,)DS
AR(
E
E
E
Figure 3: Details of the factor graph underlying the parser
of Martins et al (2009). Dashed circles represent auxil-
iary variables. See text and Table 1.
?
ya,yb
?ab(ya, yb) log
?ab(ya,yb)
?a(ya)?b(yb)
. The approximate
variational expression becomes logZx(?) ?
maxz ?
>F(x)z +Htree(zA)?
?
?a,b??P
Ia;b(za, zb, zab)
s.t. zab ? za, zab ? zb,
zab ? za + zb ? 1, ??a, b? ? P,
zA ? Ztree,
(14)
whose maximizer corresponds to the beliefs re-
turned by the Smith and Eisner?s loopy BP algorithm
(if it converges).
Turbo Parser #2: LP-Relaxed MAP. We now
turn to the concise integer LP formulation of Mar-
tins et al (2009). The formulation is exact but NP-
hard, and so an LP relaxation is made there by drop-
ping the integer constraints. We next construct a fac-
tor graph G?x and show that the LP relaxation corre-
sponds to an optimization of the form in Eq. 10, with
the marginal polytope M(G?x) replaced by L(G
?
x).
G?x includes the following auxiliary variable
nodes: path variables ?pij?i=0,...,n,j=1,...,n, which
indicate whether word j descends from i in the de-
pendency tree, and flow variables ?fka ?a?A,k=1,...,n,
which evaluate to 1 iff arc a ?carries flow? to k,
i.e., iff there is a path from the root to k that passes
through a. We need to seed these variables imposing
p0k = pkk = 1,?k, f
h
?h,m? = 0, ?h,m; (15)
i.e., any word descends from the root and from it-
self, and arcs leaving a word carry no flow to that
39
word. This can be done with unary hard constraint
factors. We then replace the TREE factor in Fig. 1 by
the factors shown in Fig. 3:
? O(n) XOR factors, each connecting all arc vari-
ables of the form {?h,m?}h=0,...,n. These ensure
that each word has exactly one parent. Each factor
yields a local agreement constraint (see Table 1):
?n
h=0 z?h,m? = 1, m ? {1, . . . , n} (16)
? O(n3) IMPLY factors, each expressing that if an
arc carries flow, then that arc must be active. Such
factors are OR factors with the first input negated,
hence, the local agreement constraints are:
fka ? za, a ? A, k ? {1, . . . , n}. (17)
? O(n2) XOR-WITH-OUTPUT factors, which im-
pose the constraint that each path variable pmk is
active if and only if exactly one incoming arc in
{?h,m?}h=0,...,n carries flow to k. Such factors
are XOR factors with the last input negated, and
hence their local constraints are:
pmk =
?n
h=0 f
k
?h,m?, m, k ? {1, . . . , n} (18)
? O(n2) XOR-WITH-OUTPUT factors to impose the
constraint that words don?t consume other words?
commodities; i.e., if h 6= k and k 6= 0, then there
is a path from h to k iff exactly one outgoing arc
in {?h,m?}m=1,...,n carries flow to k:
phk =
?n
m=1 f
k
?h,m?, h, k ? {0, . . . , n}, k /? {0, h}.
(19)
L(G?x) is thus defined by the constraints in Eq. 12
and 15?19. The approximate MAP problem, that
replaces M(G?x) by L(G
?
x) in Eq. 10, thus becomes:
maxz,f ,p ?
>F(x)z
s.t. Eqs. 12 and 15?19 are satisfied.
(20)
This is exactly the LP relaxation considered by Mar-
tins et al (2009) in their multi-commodity flow
model, for the configuration with siblings and grand-
parent features.7 They also considered a config-
uration with non-projectivity features?which fire
if an arc is non-projective.8 That configuration
can also be obtained here if variables {n?h,m?} are
7To be precise, the constraints of Martins et al (2009) are
recovered after eliminating the path variables, via Eqs. 18?19.
8An arc ?h,m? is non-projective if there is some word in its
span not descending from h (Kahane et al, 1998).
added to indicate non-projective arcs and OR-WITH-
OUTPUT hard constraint factors are inserted to en-
force n?h,m? = z?h,m??
?
min(h,m)<j<min(h,m) ?phj .
Details are omitted for space.
In sum, although the approaches of Smith and Eis-
ner (2008) and Martins et al (2009) look very dif-
ferent, in reality both are variational approximations
emanating from Prop. 1, respectively for marginal
and MAP inference. However, they operate on dis-
tinct factor graphs, respectively Figs. 1 and 3.9
5 Online Learning
Our learning algorithm is presented in Alg. 1. It is a
generalized online learner that tackles `2-regularized
empirical risk minimization of the form
min??Rd
?
2???
2 + 1m
?m
i=1 L(?;xi,yi), (21)
where each ?xi,yi? is a training example, ? ? 0 is
the regularization constant, and L(?;x,y) is a non-
negative convex loss. Examples include the logistic
loss used in CRFs (? log Pr?(y|x)) and the hinge
loss of structured SVMs (maxy??Y(x) ?
>(?(x,y?)?
?(x,y)) + `(y?,y) for some cost function `). These
are both special cases of the family defined in Fig. 4,
which also includes the structured perceptron?s loss
(? ? ?, ? = 0) and the softmax-margin loss of
Gimpel and Smith (2010; ? = ? = 1).
Alg. 1 is closely related to stochastic or online
gradient descent methods, but with the key advan-
tage of not needing a learning rate hyperparameter.
We sketch the derivation of Alg. 1; full details can
be found in Martins et al (2010a). On the tth round,
one example ?xt,yt? is considered. We seek to solve
min?,? ?m2 ?? ? ?t?
2 + ?
s.t. L(?;xt,yt) ? ?, ? ? 0,
(23)
9Given what was just exposed, it seems appealing to try
max-product loopy BP on the factor graph of Fig. 1, or sum-
product loopy BP on the one in Fig. 3. Both attempts present se-
rious challenges: the former requires computing messages sent
by the tree factor, which requires O(n2) calls to the Chu-Liu-
Edmonds algorithm and hence O(n5) time. No obvious strat-
egy seems to exist for simultaneous computation of all mes-
sages, unlike in the sum-product case. The latter is even more
challenging, as standard sum-product loopy BP has serious is-
sues in the factor graph of Fig. 3; we construct in Martins et al
(2010b) a simple example with a very poor Bethe approxima-
tion. This might be fixed by using other variants of sum-product
BP, e.g., ones in which the entropy approximation is concave.
40
L?,?(?;x,y) , 1? log
?
y??Y(x) exp
[
?
(
?>
(
?(x,y?)? ?(x,y)
)
+ ?`(y?,y)
)]
(22)
Figure 4: A family of loss functions including as particular cases the ones used in CRFs, structured SVMs, and the
structured perceptron. The hyperparameter ? is the analogue of the inverse temperature in a Gibbs distribution, while
? scales the cost. For any choice of ? > 0 and ? ? 0, the resulting loss function is convex in ?, since, up to a scale
factor, it is the composition of the (convex) log-sum-exp function with an affine map.
Algorithm 1 Aggressive Online Learning
1: Input: {?xi,yi?}mi=1, ?, number of epochs K
2: Initialize ?1 ? 0; set T = mK
3: for t = 1 to T do
4: Receive instance ?xt, yt? and set ?t = ?(yt)
5: Solve Eq. 24 to obtain ??t and L?,?(?t, xt,yt)
6: Compute?L?,?(?t, xt,yt)=F(xt)(??t??t)
7: Compute ?t = min
{
1
?m ,
L?,?(?t;xt,yt)
??L?,?(?t;xt,yt)?2
}
8: Return ?t+1 = ?t ? ?t?L?,?(?t;xt,yt)
9: end for
10: Return the averaged model ?? ? 1T
?T
t=1 ?t.
which trades off conservativeness (stay close to the
most recent solution ?t) and correctness (keep the
loss small). Alg. 1?s lines 7?8 are the result of tak-
ing the first-order Taylor approximation of L around
?t, which yields the lower bound L(?;xt,yt) ?
L(?t;xt,yt) + (? ? ?t)>?L(?t;xt,yt), and plug-
ging that linear approximation into the constraint of
Eq. 23, which gives a simple Euclidean projection
problem (with slack) with a closed-form solution.
The online updating requires evaluating the loss
and computing its gradient. Both quantities can
be computed using the variational expression in
Prop. 1, for any loss L?,?(?;x,y) in Fig. 4.10 Our
only assumption is that the cost function `(y?,y)
can be written as a sum over factor-local costs; let-
ting ? = ?(y) and ?? = ?(y?), this implies
`(y?,y) = p>?? + q for some p and q which are
constant with respect to ??.11 Under this assump-
tion, L?,?(?;x,y) becomes expressible in terms of
the log-partition function of a distribution whose
log-potentials are set to ?(F(x)>? + ?p). From
Eq. 9 and after some algebra, we finally obtain
L?,?(?;x,y) =
10Our description also applies to the (non-differentiable)
hinge loss case, when ? ? ?, if we replace all instances of
?the gradient? in the text by ?a subgradient.?
11For the Hamming cost, this holds with p = 1 ? 2? and
q = 1>?. See Taskar et al (2006) for other examples.
max
???M(Gx)
?>F(x)(????)+
1
?
H(??)+?(p>??+q).
(24)
Let ?? be a maximizer in Eq. 24; from the second
statement of Prop. 1 we obtain ?L?,?(?;x,y) =
F(x)(????). When the inference problem in Eq. 24
is intractable, approximate message-passing algo-
rithms like loopy BP still allow us to obtain approx-
imations of the loss L?,? and its gradient.
For the hinge loss, we arrive precisely at the max-
loss variant of 1-best MIRA (Crammer et al, 2006).
For the logistic loss, we arrive at a new online learn-
ing algorithm for CRFs that resembles stochastic
gradient descent but with an automatic step size that
follows from our variational representation.
Unsupported Features. As datasets grow, so do
the sets of features, creating further computational
challenges. Often only ?supported? features?those
observed in the training data?are included, and
even those are commonly eliminated when their fre-
quencies fall below a threshold. Important infor-
mation may be lost as a result of these expedi-
ent choices. Formally, the supported feature set
is Fsupp ,
?m
i=1 supp?(xi,yi), where suppu ,
{j |uj 6= 0} denotes the support of vector u. Fsupp
is a subset of the complete feature set, comprised of
those features that occur in some candidate output,
Fcomp ,
?m
i=1
?
y?i?Y(xi)
supp?(xi,y?i). Features
in Fcomp\Fsupp are called unsupported.
Sha and Pereira (2003) have shown that training a
CRF-based shallow parser with the complete feature
set may improve performance (over the supported
one), at the cost of 4.6 times more features. De-
pendency parsing has a much higher ratio (around
20 for bilexical word-word features, as estimated in
the Penn Treebank), due to the quadratic or faster
growth of the number of parts, of which only a few
are active in a legal output. We propose a simple
strategy for handling Fcomp efficiently, which can
be applied for those losses in Fig. 4 where ? = ?.
(e.g., the structured SVM and perceptron). Our pro-
cedure is the following: keep an active set F contain-
41
CRF (TURBO PARS. #1) SVM (TURBO PARS. #2) SVM (TURBO #2)
ARC-FACT. SEC. ORD. ARC-FACT. SEC. ORD. |F| |F||Fsupp| +NONPROJ., COMPL.
ARABIC 78.28 79.12 79.04 79.42 6,643,191 2.8 80.02 (-0.14)
BULGARIAN 91.02 91.78 90.84 92.30 13,018,431 2.1 92.88 (+0.34) (?)
CHINESE 90.58 90.87 91.09 91.77 28,271,086 2.1 91.89 (+0.26)
CZECH 86.18 87.72 86.78 88.52 83,264,645 2.3 88.78 (+0.44) (?)
DANISH 89.58 90.08 89.78 90.78 7,900,061 2.3 91.50 (+0.68)
DUTCH 82.91 84.31 82.73 84.17 15,652,800 2.1 84.91 (-0.08)
GERMAN 89.34 90.58 89.04 91.19 49,934,403 2.5 91.49 (+0.32) (?)
JAPANESE 92.90 93.22 93.18 93.38 4,256,857 2.2 93.42 (+0.32)
PORTUGUESE 90.64 91.00 90.56 91.50 16,067,150 2.1 91.87 (-0.04)
SLOVENE 83.03 83.17 83.49 84.35 4,603,295 2.7 85.53 (+0.80)
SPANISH 83.83 85.07 84.19 85.95 11,629,964 2.6 87.04 (+0.50) (?)
SWEDISH 87.81 89.01 88.55 88.99 18,374,160 2.8 89.80 (+0.42)
TURKISH 76.86 76.28 74.79 76.10 6,688,373 2.2 76.62 (+0.62)
ENGLISH NON-PROJ. 90.15 91.08 90.66 91.79 57,615,709 2.5 92.13 (+0.12)
ENGLISH PROJ. 91.23 91.94 91.65 92.91 55,247,093 2.4 93.26 (+0.41) (?)
Table 2: Unlabeled attachment scores, ignoring punctuation. The leftmost columns show the performance of arc-
factored and second-order models for the CRF and SVM losses, after 10 epochs with 1/(?m) = 0.001 (tuned on the
English Non-Proj. dev.-set). The rightmost columns refer to a model to which non-projectivity features were added,
trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated,
the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the
difference w.r.t. a model trained with the supported features only). Entries marked with ? are the highest reported in
the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008),
Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which
achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different).
ing all features that have been instantiated in Alg. 1.
At each round, run lines 4?5 as usual, using only
features in F. Since the other features have not been
used before, they have a zero weight, hence can be
ignored. When ? = ?, the variational problem in
Eq. 24 consists of a MAP computation and the solu-
tion corresponds to one output y?t ? Y(xt). Only the
parts that are active in y?t but not in yt, or vice-versa,
will have features that might receive a nonzero up-
date. Those parts are reexamined for new features
and the active set F is updated accordingly.
6 Experiments
We trained non-projective dependency parsers for
14 languages, using datasets from the CoNLL-X
shared task (Buchholz and Marsi, 2006) and two
datasets for English: one from the CoNLL-2008
shared task (Surdeanu et al, 2008), which contains
non-projective arcs, and another derived from the
Penn Treebank applying the standard head rules of
Yamada and Matsumoto (2003), in which all parse
trees are projective.12 We implemented Alg. 1,
12We used the provided train/test splits for all datasets. For
English, we used the standard test partitions (section 23 of the
Wall Street Journal). We did not exploit the fact that some
datasets only contain projective trees and have unique roots.
which handles any loss function L?,? .13 When ? <
?, Turbo Parser #1 and the loopy BP algorithm of
Smith and Eisner (2008) is used; otherwise, Turbo
Parser #2 is used and the LP relaxation is solved with
CPLEX. In both cases, we employed the same prun-
ing strategy as Martins et al (2009).
Two different feature configurations were first
tried: an arc-factored model and a model with
second-order features (siblings and grandparents).
We used the same arc-factored features as McDon-
ald et al (2005) and second-order features that con-
join words and lemmas (at most two), parts-of-
speech tags, and (if available) morphological infor-
mation; this was the same set of features as in Mar-
tins et al (2009). Table 2 shows the results obtained
in both configurations, for CRF and SVM loss func-
tions. While in the arc-factored case performance is
similar, in second-order models there seems to be a
consistent gain when the SVM loss is used. There
are two possible reasons: first, SVMs take the cost
function into consideration; second, Turbo Parser #2
is less approximate than Turbo Parser #1, since only
the marginal polytope is approximated (the entropy
function is not involved).
13The code is available at http://www.ark.cs.cmu.edu/
TurboParser.
42
? 1 1 1 1 3 5 ?
? 0 (CRF) 1 3 5 1 1 1 (SVM)
ARC-F. 90.15 90.41 90.38 90.53 90.80 90.83 90.66
2 ORD. 91.08 91.85 91.89 91.51 92.04 91.98 91.79
Table 3: Varying ? and ?: neither the CRF nor the
SVM is optimal. Results are UAS on the English Non-
Projective dataset, with ? tuned with dev.-set validation.
The loopy BP algorithm managed to converge for
nearly all sentences (with message damping). The
last three columns show the beneficial effect of un-
supported features for the SVM case (with a more
powerful model with non-projectivity features). For
most languages, unsupported features convey help-
ful information, which can be used with little extra
cost (on average, 2.5 times more features are instan-
tiated). A combination of the techniques discussed
here yields parsers that are in line with very strong
competitors?for example, the parser of Koo and
Collins (2010), which is exact, third-order, and con-
strains the outputs to be projective, does not outper-
form ours on the projective English dataset.14
Finally, Table 3 shows results obtained for differ-
ent settings of ? and ?. Interestingly, we observe
that higher scores are obtained for loss functions that
are ?between? SVMs and CRFs.
7 Related Work
There has been recent work studying efficient com-
putation of messages in combinatorial factors: bi-
partite matchings (Duchi et al, 2007), projective
and non-projective arborescences (Smith and Eis-
ner, 2008), as well as high order factors with count-
based potentials (Tarlow et al, 2010), among others.
Some of our combinatorial factors (OR, OR-WITH-
OUTPUT) and the analogous entropy computations
were never considered, to the best of our knowledge.
Prop. 1 appears in Wainwright and Jordan (2008)
for canonical overcomplete models; we adapt it here
for models with shared features. We rely on the vari-
ational interpretation of loopy BP, due to Yedidia et
al. (2001), to derive the objective being optimized
by Smith and Eisner?s loopy BP parser.
Independently of our work, Koo et al (2010)
14This might be due to the fact that Koo and Collins (2010)
trained with the perceptron algorithm and did not use unsup-
ported features. Experiments plugging the perceptron loss
(? ? ?, ? ? 0) into Alg. 1 yielded worse performance than
with the hinge loss.
recently proposed an efficient dual decomposition
method to solve an LP problem similar (but not
equal) to the one in Eq. 20,15 with excellent pars-
ing performance. Their parser is also an instance
of a turbo parser since it relies on a local approxi-
mation of a marginal polytope. While one can also
use dual decomposition to address our MAP prob-
lem, the fact that our model does not decompose as
nicely as the one in Koo et al (2010) would likely
result in slower convergence.
8 Conclusion
We presented a unified view of two recent approxi-
mate dependency parsers, by stating their underlying
factor graphs and by deriving the variational prob-
lems that they address. We introduced new hard con-
straint factors, along with formulae for their mes-
sages, local belief constraints, and entropies. We
provided an aggressive online algorithm for training
the models with a broad family of losses.
There are several possible directions for future
work. Recent progress in message-passing algo-
rithms yield ?convexified? Bethe approximations
that can be used for marginal inference (Wainwright
et al, 2005), and provably convergent max-product
variants that solve the relaxed LP (Globerson and
Jaakkola, 2008). Other parsing formalisms can be
handled with the inventory of factors shown here?
among them, phrase-structure parsing.
Acknowledgments
The authors would like to thank the reviewers for their
comments, and Kevin Gimpel, David Smith, David Son-
tag, and Terry Koo for helpful discussions. A. M. was
supported by a grant from FCT/ICTI through the CMU-
Portugal Program, and also by Priberam Informa?tica.
N. S. was supported in part by Qatar NRF NPRP-08-485-
1-083. E. X. was supported by AFOSR FA9550010247,
ONR N000140910758, NSF CAREER DBI-0546594,
NSF IIS-0713379, and an Alfred P. Sloan Fellowship.
M. F. and P. A. were supported by the FET programme
(EU FP7), under the SIMBAD project (contract 213250).
15The difference is that the model of Koo et al (2010)
includes features that depend on consecutive siblings?
making it decompose into subproblems amenable to dynamic
programming?while we have factors for all pairs of siblings.
43
References
C. Berrou, A. Glavieux, and P. Thitimajshima. 1993.
Near Shannon limit error-correcting coding and decod-
ing. In Proc. of ICC, volume 93, pages 1064?1070.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. NIPS, 19.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proc. of ACL.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
crfs: Training log-linear models with loss functions.
In Proc. of NAACL.
A. Globerson and T. Jaakkola. 2008. Fixing max-
product: Convergent message passing algorithms for
MAP LP-relaxations. NIPS, 20.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proc. of EMNLP.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 2001.
Factor graphs and the sum-product algorithm. IEEE
Trans. Inf. Theory, 47(2):498?519.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proc. of ACL-IJCNLP.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M. A. T. Figueiredo. 2010a.
Learning structured classifiers with dual coordinate
descent. Technical Report CMU-ML-10-109.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010b. Turbo parsers:
Dependency parsing by approximate variational infer-
ence (extended version).
A. McCallum, K. Schultz, and S. Singh. 2009. Fac-
torie: Probabilistic programming via imperatively de-
fined factor graphs. In NIPS.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. of CoNLL.
R. J. McEliece, D. J. C. MacKay, and J. F. Cheng. 1998.
Turbo decoding as an instance of Pearl?s ?belief prop-
agation? algorithm. IEEE Journal on Selected Areas
in Communications, 16(2).
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of HLT-NAACL.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
D. A. Smith and N. A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
CoNLL.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. JMLR, 8:693?723.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
D. Tarlow, I. E. Givoni, and R. S. Zemel. 2010. HOP-
MAP: Efficient message passing with high order po-
tentials. In Proc. of AISTATS.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
B. Taskar, S. Lacoste-Julien, and M. I. Jordan. 2006.
Structured prediction, dual extragradient and Bregman
projections. JMLR, 7:1627?1653.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proc. of ICML.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
M. J. Wainwright, T.S. Jaakkola, and A.S. Willsky. 2005.
A new class of upper bounds on the log partition func-
tion. IEEE Trans. Inf. Theory, 51(7):2313?2335.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2001. Gen-
eralized belief propagation. In NIPS.
44
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277?1287,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Latent Variable Model for Geographic Lexical Variation
Jacob Eisenstein Brendan O?Connor Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,brendano,nasmith,epxing}@cs.cmu.edu
Abstract
The rapid growth of geotagged social media
raises new computational possibilities for in-
vestigating geographic linguistic variation. In
this paper, we present a multi-level generative
model that reasons jointly about latent topics
and geographical regions. High-level topics
such as ?sports? or ?entertainment? are ren-
dered differently in each geographic region,
revealing topic-specific regional distinctions.
Applied to a new dataset of geotagged mi-
croblogs, our model recovers coherent top-
ics and their regional variants, while identi-
fying geographic areas of linguistic consis-
tency. The model also enables prediction of
an author?s geographic location from raw text,
outperforming both text regression and super-
vised topic models.
1 Introduction
Sociolinguistics and dialectology study how lan-
guage varies across social and regional contexts.
Quantitative research in these fields generally pro-
ceeds by counting the frequency of a handful of
previously-identified linguistic variables: pairs of
phonological, lexical, or morphosyntactic features
that are semantically equivalent, but whose fre-
quency depends on social, geographical, or other
factors (Paolillo, 2002; Chambers, 2009). It is left to
the experimenter to determine which variables will
be considered, and there is no obvious procedure for
drawing inferences from the distribution of multiple
variables. In this paper, we present a method for
identifying geographically-aligned lexical variation
directly from raw text. Our approach takes the form
of a probabilistic graphical model capable of iden-
tifying both geographically-salient terms and coher-
ent linguistic communities.
One challenge in the study of lexical variation is
that term frequencies are influenced by a variety of
factors, such as the topic of discourse. We address
this issue by adding latent variables that allow us to
model topical variation explicitly. We hypothesize
that geography and topic interact, as ?pure? topi-
cal lexical distributions are corrupted by geographi-
cal factors; for example, a sports-related topic will
be rendered differently in New York and Califor-
nia. Each author is imbued with a latent ?region?
indicator, which both selects the regional variant of
each topic, and generates the author?s observed ge-
ographical location. The regional corruption of top-
ics is modeled through a cascade of logistic normal
priors?a general modeling approach which we call
cascading topic models. The resulting system has
multiple capabilities, including: (i) analyzing lexi-
cal variation by both topic and geography; (ii) seg-
menting geographical space into coherent linguistic
communities; (iii) predicting author location based
on text alone.
This research is only possible due to the rapid
growth of social media. Our dataset is derived from
the microblogging website Twitter,1 which permits
users to post short messages to the public. Many
users of Twitter also supply exact geographical co-
ordinates from GPS-enabled devices (e.g., mobile
phones),2 yielding geotagged text data. Text in
computer-mediated communication is often more
vernacular (Tagliamonte and Denis, 2008), and as
such it is more likely to reveal the influence of ge-
ographic factors than text written in a more formal
genre, such as news text (Labov, 1966).
We evaluate our approach both qualitatively and
quantitatively. We investigate the topics and regions
1http://www.twitter.com
2User profiles also contain self-reported location names, but
we do not use that information in this work.
1277
that the model obtains, showing both common-sense
results (place names and sports teams are grouped
appropriately), as well as less-obvious insights about
slang. Quantitatively, we apply our model to predict
the location of unlabeled authors, using text alone.
On this task, our model outperforms several alterna-
tives, including both discriminative text regression
and related latent-variable approaches.
2 Data
The main dataset in this research is gathered from
the microblog website Twitter, via its official API.
We use an archive of messages collected over the
first week of March 2010 from the ?Gardenhose?
sample stream,3 which then consisted of 15% of
all public messages, totaling millions per day. We
aggressively filter this stream, using only messages
that are tagged with physical (latitude, longitude)
coordinate pairs from a mobile client, and whose au-
thors wrote at least 20 messages over this period. We
also filter to include only authors who follow fewer
than 1,000 other people, and have fewer than 1,000
followers. Kwak et al (2010) find dramatic shifts
in behavior among users with social graph connec-
tivity outside of that range; such users may be mar-
keters, celebrities with professional publicists, news
media sources, etc. We also remove messages con-
taining URLs to eliminate bots posting information
such as advertising or weather conditions. For inter-
pretability, we restrict our attention to authors inside
a bounding box around the contiguous U.S. states,
yielding a final sample of about 9,500 users and
380,000 messages, totaling 4.7 million word tokens.
We have made this dataset available online.4
Informal text from mobile phones is challeng-
ing to tokenize; we adapt a publicly available tok-
enizer5 originally developed for Twitter (O?Connor
et al, 2010), which preserves emoticons and blocks
of punctuation and other symbols as tokens. For
each user?s Twitter feed, we combine all messages
into a single ?document.? We remove word types
that appear in fewer than 40 feeds, yielding a vocab-
ulary of 5,216 words. Of these, 1,332 do not appear
in the English, French, or Spanish dictionaries of the
3http://dev.twitter.com/pages/streaming_api
4http://www.ark.cs.cmu.edu/GeoTwitter
5http://tweetmotif.com
spell-checking program aspell.
Every message is tagged with a location, but most
messages from a single individual tend to come from
nearby locations (as they go about their day); for
modeling purposes we use only a single geographic
location for each author, simply taking the location
of the first message in the sample.
The authors in our dataset are fairly heavy Twit-
ter users, posting an average of 40 messages per day
(although we see only 15% of this total). We have
little information about their demographics, though
from the text it seems likely that this user set skews
towards teens and young adults. The dataset cov-
ers each of the 48 contiguous United States and the
District of Columbia.
3 Model
We develop a model that incorporates two sources
of lexical variation: topic and geographical region.
We treat the text and geographic locations as out-
puts from a generative process that incorporates both
topics and regions as latent variables.6 During infer-
ence, we seek to recover the topics and regions that
best explain the observed data.
At the base level of model are ?pure? topics (such
as ?sports?, ?weather?, or ?slang?); these topics are
rendered differently in each region. We call this gen-
eral modeling approach a cascading topic model; we
describe it first in general terms before moving to the
specific application to geographical variation.
3.1 Cascading Topic Models
Cascading topic models generate text from a chain
of random variables. Each element in the chain de-
fines a distribution over words, and acts as the mean
of the distribution over the subsequent element in
the chain. Thus, each element in the chain can be
thought of as introducing some additional corrup-
tion. All words are drawn from the final distribution
in the chain.
At the beginning of the chain are the priors, fol-
lowed by unadulerated base topics, which may then
be corrupted by other factors (such as geography or
time). For example, consider a base ?food? topic
6The region could be observed by using a predefined geo-
graphical decomposition, e.g., political boundaries. However,
such regions may not correspond well to linguistic variation.
1278
that emphasizes words like dinner and delicious;
the corrupted ?food-California? topic would place
weight on these words, but might place extra em-
phasis on other words like sprouts.
The path through the cascade is determined by a
set of indexing variables, which may be hidden or
observed. As in standard latent Dirichlet alocation
(Blei et al, 2003), the base topics are selected by
a per-token hidden variable z. In the geographical
topic model, the next level corresponds to regions,
which are selected by a per-author latent variable r.
Formally, we draw each level of the cascade from
a normal distribution centered on the previous level;
the final multinomial distribution over words is ob-
tained by exponentiating and normalizing. To ensure
tractable inference, we assume that all covariance
matrices are uniform diagonal, i.e., aI with a > 0;
this means we do not model interactions between
words.
3.2 The Geographic Topic Model
The application of cascading topic models to ge-
ographical variation is straightforward. Each doc-
ument corresponds to the entire Twitter feed of a
given author during the time period covered by our
corpus. For each author, the latent variable r cor-
responds to the geographical region of the author,
which is not observed. As described above, r se-
lects a corrupted version of each topic: the kth basic
topic has mean ?k, with uniform diagonal covari-
ance ?2k; for region j, we can draw the regionally-
corrupted topic from the normal distribution, ?jk ?
N(?k, ?
2
kI).
Because ? is normally-distributed, it lies not in
the simplex but in RW . We deterministically com-
pute multinomial parameters ? by exponentiating
and normalizing: ?jk = exp(?jk)/
?
i exp(?
(i)
jk ).
This normalization could introduce identifiability
problems, as there are multiple settings for ? that
maximize P (w|?) (Blei and Lafferty, 2006a). How-
ever, this difficulty is obviated by the priors: given
? and ?2, there is only a single ? that maximizes
P (w|?)P (?|?, ?2); similarly, only a single ?max-
imizes P (?|?)P (?|a, b2).
The observed latitude and longitude, denoted y,
are normally distributed and conditioned on the re-
gion, with mean ?r and precision matrix ?r indexed
by the region r. The region index r is itself drawn
from a single shared multinomial ?. The model is
shown as a plate diagram in Figure 1.
Given a vocabulary size W , the generative story
is as follows:
? Generate base topics: for each topic k < K
? Draw the base topic from a normal distribu-
tion with uniform diagonal covariance: ?k ?
N(a, b2I),
? Draw the regional variance from a Gamma
distribution: ?2k ? G(c, d).
? Generate regional variants: for each region
j < J ,
? Draw the region-topic ?jk from a normal
distribution with uniform diagonal covari-
ance: ?jk ? N(?k, ?
2
kI).
? Convert ?jk into a multinomial
distribution over words by ex-
ponentiating and normalizing:
?jk = exp
(
?jk
)
/
?W
i exp(?
(i)
jk ),
where the denominator sums over the
vocabulary.
? Generate regions: for each region j < J ,
? Draw the spatial mean ?j from a normal dis-
tribution.
? Draw the precision matrix ?j from a Wishart
distribution.
? Draw the distribution over regions ? from a sym-
metric Dirichlet prior, ? ? Dir(??1).
? Generate text and locations: for each document d,
? Draw topic proportions from a symmetric
Dirichlet prior, ? ? Dir(?1).
? Draw the region r from the multinomial dis-
tribution ?.
? Draw the location y from the bivariate Gaus-
sian, y ? N(?r,?r).
? For each word token,
? Draw the topic indicator z ? ?.
? Draw the word token w ? ?rz .
4 Inference
We apply mean-field variational inference: a fully-
factored variational distribution Q is chosen to min-
imize the Kullback-Leibler divergence from the
true distribution. Mean-field variational inference
with conjugate priors is described in detail else-
where (Bishop, 2006; Wainwright and Jordan,
2008); we restrict our focus to the issues that are
unique to the geographic topic model.
1279
? ?
w
z
?
D
Nd
y
r K
?
J
?
?
?
??2
?
?k log of base topic k?s distribution over word types
?2k variance parameter for regional variants of topic k
?jk region j?s variant of base topic ?k
?d author d?s topic proportions
rd author d?s latent region
yd author d?s observed GPS location
?j region j?s spatial center
?j region j?s spatial precision
zn token n?s topic assignment
wn token n?s observed word type
? global prior over author-topic proportions
? global prior over region classes
Figure 1: Plate diagram for the geographic topic model, with a table of all random variables. Priors (besides ?) are
omitted for clarity, and the document indices on z and w are implicit.
We place variational distributions over all latent
variables of interest: ?, z, r,?,?,?, ?2,?, and ?,
updating each of these distributions in turn, until
convergence. The variational distributions over ?
and ? are Dirichlet, and have closed form updates:
each can be set to the sum of the expected counts,
plus a term from the prior (Blei et al, 2003). The
variational distributions q(z) and q(r) are categor-
ical, and can be set proportional to the expected
joint likelihood?to set q(z) we marginalize over r,
and vice versa.7 The updates for the multivariate
Gaussian spatial parameters ? and ? are described
by Penny (2001).
4.1 Regional Word Distributions
The variational region-topic distribution ?jk is nor-
mal, with uniform diagonal covariance for tractabil-
ity. Throughout we will write ?x? to indicate the ex-
pectation of x under the variational distribution Q.
Thus, the vector mean of the distribution q(?jk) is
written ??jk?, while the variance (uniform across i)
of q(?) is written V(?jk).
To update the mean parameter ??jk?, we max-
imize the contribution to the variational bound L
from the relevant terms:
L
[??(i)jk ?]
= ?log p(w|?, z, r)?+?log p(?(i)jk |?
(i)
k , ?
2
k)?,
(1)
7Thanks to the na??ve mean field assumption, we can
marginalize over z by first decomposing across all Nd words
and then summing over q(z).
with the first term representing the likelihood of the
observed words (recall that ? is computed determin-
istically from ?) and the second term corresponding
to the prior. The likelihood term requires the expec-
tation ?log??, but this is somewhat complicated by
the normalizer
?W
i exp(?
(i)), which sums over all
terms in the vocabulary. As in previous work on lo-
gistic normal topic models, we use a Taylor approx-
imation for this term (Blei and Lafferty, 2006a).
The prior on ? is normal, so the contribution from
the second term of the objective (Equation 1) is
? 1
2??2k?
?(?(i)jk ? ?
(i)
k )
2?. We introduce the following
notation for expected counts: N(i, j, k) indicates the
expected count of term i in region j and topic k, and
N(j, k) =
?
iN(i, j, k). After some calculus, we
can write the gradient ?L/???((i))jk ? as
N(i, j, k)?N(j, k)??(i)jk ? ? ??
?2
k ?(??
(i)
jk ? ? ??
(i)
k ?),
(2)
which has an intuitive interpretation. The first two
terms represent the difference in expected counts for
term i under the variational distributions q(z, r) and
q(z, r, ?): this difference goes to zero when ?(i)jk per-
fectly matches N(i, j, k)/N(j, k). The third term
penalizes ?(i)jk for deviating from its prior ?
(i)
k , but
this penalty is proportional to the expected inverse
variance ???2k ?. We apply gradient ascent to maxi-
mize the objective L. A similar set of calculations
gives the gradient for the variance of ?; these are
described in an forthcoming appendix.
1280
4.2 Base Topics
The base topic parameters are?k and ?
2
k; in the vari-
ational distribution, q(?k) is normally distributed
and q(?2k) is Gamma distributed. Note that ?k and
?2k affect only the regional word distributions ?jk.
An advantage of the logistic normal is that the vari-
ational parameters over ?k are available in closed
form,
??(i)k ? =
b2
?J
j ??
(i)
jk ?+ ??
2
k?a
(i)
b2J + ??2k?
V(?k) = (b
?2 + J???2k ?)
?1,
where J indicates the number of regions. The ex-
pectation of the base topic ? incorporates the prior
and the average of the generated region-topics?
these two components are weighted respectively by
the expected variance of the region-topics ??2k? and
the prior topical variance b2. The posterior variance
V(?) is a harmonic combination of the prior vari-
ance b2 and the expected variance of the region top-
ics.
The variational distribution over the region-topic
variance ?2k has Gamma parameters. These param-
eters cannot be updated in closed form, so gradi-
ent optimization is again required. The derivation
of these updates is more involved, and is left for a
forthcoming appendix.
5 Implementation
Variational scheduling and initialization are impor-
tant aspects of any hierarchical generative model,
and are often under-discussed. In our implementa-
tion, the variational updates are scheduled as fol-
lows: given expected counts, we iteratively update
the variational parameters on the region-topics ? and
the base topics?, until convergence. We then update
the geographical parameters ? and ?, as well as the
distribution over regions ?. Finally, for each doc-
ument we iteratively update the variational param-
eters over ?, z, and r until convergence, obtaining
expected counts that are used in the next iteration
of updates for the topics and their regional variants.
We iterate an outer loop over the entire set of updates
until convergence.
We initialize the model in a piecewise fashion.
First we train a Dirichlet process mixture model on
the locations y, using variational inference on the
truncated stick-breaking approximation (Blei and
Jordan, 2006). This automatically selects the num-
ber of regions J , and gives a distribution over each
region indicator rd from geographical information
alone. We then run standard latent Dirichlet aloca-
tion to obtain estimates of z for each token (ignoring
the locations). From this initialization we can com-
pute the first set of expected counts, which are used
to obtain initial estimates of all parameters needed
to begin variational inference in the full model.
The prior a is the expected mean of each topic
?; for each term i, we set a(i) = logN(i) ? logN ,
where N(i) is the total count of i in the corpus and
N =
?
iN(i). The variance prior b
2 is set to 1, and
the prior on ?2 is the Gamma distribution G(2, 200),
encouraging minimal deviation from the base topics.
The symmetric Dirichlet prior on ? is set to 12 , and
the symmetric Dirichlet parameter on ? is updated
from weak hyperpriors (Minka, 2003). Finally, the
geographical model takes priors that are linked to the
data: for each region, the mean is very weakly en-
couraged to be near the overall mean, and the covari-
ance prior is set by the average covariance of clusters
obtained by running K-means.
6 Evaluation
For a quantitative evaluation of the estimated rela-
tionship between text and geography, we assess our
model?s ability to predict the geographic location of
unlabeled authors based on their text alone.8 This
task may also be practically relevant as a step toward
applications for recommending local businesses or
social connections. A randomly-chosen 60% of au-
thors are used for training, 20% for development,
and the remaining 20% for final evaluation.
6.1 Systems
We compare several approaches for predicting au-
thor location; we divide these into latent variable
generative models and discriminative approaches.
8Alternatively, one might evaluate the attributed regional
memberships of the words themselves. While the Dictionary of
American Regional English (Cassidy and Hall, 1985) attempts
a comprehensive list of all regionally-affiliated terms, it is based
on interviews conducted from 1965-1970, and the final volume
(covering Si?Z) is not yet complete.
1281
6.1.1 Latent Variable Models
Geographic Topic Model This is the full version
of our system, as described in this paper. To pre-
dict the unseen location yd, we iterate until con-
vergence on the variational updates for the hidden
topics zd, the topic proportions ?d, and the region
rd. From rd, the location can be estimated as y?d =
arg maxy
?J
j p(y|?j ,?j)q(rd = j). The develop-
ment set is used to tune the number of topics and to
select the best of multiple random initializations.
Mixture of Unigrams A core premise of our ap-
proach is that modeling topical variation will im-
prove our ability to understand geographical varia-
tion. We test this idea by fixing K = 1, running our
system with only a single topic. This is equivalent
to a Bayesian mixture of unigrams in which each au-
thor is assigned a single, regional unigram language
model that generates all of his or her text. The de-
velopment set is used to select the best of multiple
random initializations.
Supervised Latent Dirichlet Allocation In a
more subtle version of the mixture-of-unigrams
model, we model each author as an admixture of re-
gions. Thus, the latent variable attached to each au-
thor is no longer an index, but rather a vector on the
simplex. This model is equivalent to supervised la-
tent Dirichlet alocation (Blei and McAuliffe, 2007):
each topic is associated with equivariant Gaussian
distributions over the latitude and longitude, and
these topics must explain both the text and the ob-
served geographical locations. For unlabeled au-
thors, we estimate latitude and longitude by esti-
mating the topic proportions and then applying the
learned geographical distributions. This is a linear
prediction
f(z?d;a) = (z?
T
da
lat, z?Tda
lon)
for an author?s topic proportions z?d and topic-
geography weights a ? R2K .
6.1.2 Baseline Approaches
Text Regression We perform linear regression
to discriminatively learn the relationship between
words and locations. Using term frequency features
xd for each author, we predict locations with word-
geography weights a ? R2W :
f(xd;a) = (x
T
da
lat, xTda
lon)
Weights are trained to minimize the sum of squared
Euclidean distances, subject to L1 regularization:
?
d
(xTda
lat ? ylatd )
2 + (xTda
lon ? ylond )
2
+ ?lat||a
lat||1 + ?lon||a
lon||1
The minimization problem decouples into two sep-
arate latitude and longitude models, which we fit
using the glmnet elastic net regularized regres-
sion package (Friedman et al, 2010), which ob-
tained good results on other text-based prediction
tasks (Joshi et al, 2010). Regularization parameters
were tuned on the development set. The L1 penalty
outperformed L2 and mixtures of L1 and L2.
Note that for both word-level linear regression
here, and the topic-level linear regression in SLDA,
the choice of squared Euclidean distance dovetails
with our use of spatial Gaussian likelihoods in the
geographic topic models, since optimizing a is
equivalent to maximum likelihood estimation un-
der the assumption that locations are drawn from
equivariant circular Gaussians centered around each
f(xd;a) linear prediction. We experimented with
decorrelating the location dimensions by projecting
yd into the principal component space, but this did
not help text regression.
K-Nearest Neighbors Linear regression is a poor
model for the multimodal density of human popula-
tions. As an alternative baseline, we applied super-
vised K-nearest neighbors to predict the location yd
as the average of the positions of the K most sim-
ilar authors in the training set. We computed term-
frequency inverse-document frequency features and
applied cosine similarity over their first 30 principal
components to find the neighbors. The choices of
principal components, IDF weighting, and neighbor-
hood size K = 20 were tuned on the development
set.
6.2 Metrics
Our principle error metrics are the mean and median
distance between the predicted and true location in
kilometers.9 Because the distance error may be dif-
ficult to interpret, we also report accuracy of classi-
9For convenience, model training and prediction use latitude
and longitude as an unprojected 2D Euclidean space. However,
properly measuring the physical distance between points on the
1282
Regression Classification accuracy (%)
System Mean Dist. (km) Median Dist. (km) Region (4-way) State (49-way)
Geographic topic model 900 494 58 24
Mixture of unigrams 947 644 53 19
Supervised LDA 1055 728 39 4
Text regression 948 712 41 4
K-nearest neighbors 1077 853 37 2
Mean location 1148 1018
Most common class 37 27
Table 1: Location prediction results; lower scores are better on the regression task, higher scores are better on the
classification task. Distances are in kilometers. Mean location and most common class are computed from the test set.
Both the geographic topic model and supervised LDA use the best number of topics from the development set (10 and
5, respectively).
fication by state and by region of the United States.
Our data includes the 48 contiguous states plus the
District of Columbia; the U.S. Census Bureau di-
vides these states into four regions: West, Midwest,
Northeast, and South.10 Note that while major pop-
ulation centers straddle several state lines, most re-
gion boundaries are far from the largest cities, re-
sulting in a clearer analysis.
6.3 Results
As shown in Table 1, the geographic topic model
achieves the strongest performance on all metrics.
All differences in performance between systems
are statistically significant (p < .01) using the
Wilcoxon-Mann-Whitney test for regression error
and the ?2 test for classification accuracy. Figure 2
shows how performance changes as the number of
topics varies.
Note that the geographic topic model and the mix-
ture of unigrams use identical code and parametriza-
tion ? the only difference is that the geographic topic
model accounts for topical variation, while the mix-
ture of unigrams sets K = 1. These results validate
our basic premise that it is important to model the
interaction between topical and geographical varia-
tion.
Text regression and supervised LDA perform es-
pecially poorly on the classification metric. Both
methods make predictions that are averaged across
Earth?s surface requires computing or approximating the great
circle distance ? we use the Haversine formula (Sinnott, 1984).
For the continental U.S., the relationship between degrees and
kilometers is nearly linear, but extending the model to a conti-
nental scale would require a more sophisticated approach.
10http://www.census.gov/geo/www/us_regdiv.pdf
0 5 10 15 20400
500
600
700
800
900
1000
1100
Number of topics
Me
dia
n e
rro
r (k
m)
 
 
Geographic Topic Model
Supervised LDA
Mean location
Figure 2: The effect of varying the number of topics on
the median regression error (lower is better).
each word in the document: in text regression, each
word is directly multiplied by a feature weight; in
supervised LDA the word is associated with a la-
tent topic first, and then multiplied by a weight. For
these models, all words exert an influence on the pre-
dicted location, so uninformative words will draw
the prediction towards the center of the map. This
yields reasonable distance errors but poor classifica-
tion accuracy. We had hoped that K-nearest neigh-
bors would be a better fit for this metric, but its per-
formance is poor at all values of K. Of course it is
always possible to optimize classification accuracy
directly, but such an approach would be incapable
of predicting the exact geographical location, which
is the focus of our evaluation (given that the desired
geographical partition is unknown). Note that the
geographic topic model is also not trained to opti-
mize classification accuracy.
1283
?basketball?
?popular
music?
?daily life? ?emoticons? ?chit chat?
PISTONS KOBE
LAKERS game
DUKE NBA
CAVS STUCKEY
JETS KNICKS
album music
beats artist video
#LAKERS
ITUNES tour
produced vol
tonight shop
weekend getting
going chilling
ready discount
waiting iam
:) haha :d :( ;) :p
xd :/ hahaha
hahah
lol smh jk yea
wyd coo ima
wassup
somethin jp
Boston
+ CELTICS victoryBOSTON
CHARLOTTE
playing daughter
PEARL alive war
comp
BOSTON ;p gna loveee
ese exam suttin
sippin
N. California+
THUNDER
KINGS GIANTS
pimp trees clap
SIMON dl
mountain seee 6am OAKLAND
pues hella koo
SAN fckn
hella flirt hut
iono OAKLAND
New York
+ NETS KNICKS BRONX iam cab oww wasssup nm
Los Angeles+
#KOBE
#LAKERS
AUSTIN
#LAKERS load
HOLLYWOOD
imm MICKEY
TUPAC
omw tacos hr
HOLLYWOOD
af papi raining
th bomb coo
HOLLYWOOD
wyd coo af nada
tacos messin
fasho bomb
Lake Erie
+
CAVS
CLEVELAND
OHIO BUCKS od
COLUMBUS
premiere prod
joint TORONTO
onto designer
CANADA village
burr
stink CHIPOTLE
tipsy
;d blvd BIEBER
hve OHIO
foul WIZ salty
excuses lames
officer lastnight
Table 2: Example base topics (top line) and regional variants. For the base topics, terms are ranked by log-odds
compared to the background distribution. The regional variants show words that are strong compared to both the base
topic and the background. Foreign-language words are shown in italics, while terms that are usually in proper nouns
are shown in SMALL CAPS. See Table 3 for definitions of slang terms; see Section 7 for more explanation and details
on the methodology.
Figure 3: Regional clustering of the training set obtained by one randomly-initialized run of the geographical topic
model. Each point represents one author, and each shape/color combination represents the most likely cluster as-
signment. Ellipses represent the regions? spatial means and covariances. The same model and coloring are shown in
Table 2.
1284
7 Analysis
Our model permits analysis of geographical vari-
ation in the context of topics that help to clarify
the significance of geographically-salient terms. Ta-
ble 2 shows a subset of the results of one randomly-
initialized run, including five hand-chosen topics (of
50 total) and five regions (of 13, as chosen automat-
ically during initialization). Terms were selected by
log-odds comparison. For the base topics we show
the ten strongest terms in each topic as compared to
the background word distribution. For the regional
variants, we show terms that are strong both region-
ally and topically: specifically, we select terms that
are in the top 100 compared to both the background
distribution and to the base topic. The names for the
topics and regions were chosen by the authors.
Nearly all of the terms in column 1 (?basketball?)
refer to sports teams, athletes, and place names?
encouragingly, terms tend to appear in the regions
where their referents reside. Column 2 contains sev-
eral proper nouns, mostly referring to popular mu-
sic figures (including PEARL from the band Pearl
Jam).11 Columns 3?5 are more conversational.
Spanish-language terms (papi, pues, nada, ese) tend
to appear in regions with large Spanish-speaking
populations?it is also telling that these terms ap-
pear in topics with emoticons and slang abbrevia-
tions, which may transcend linguistic barriers. Other
terms refer to people or subjects that may be espe-
cially relevant in certain regions: tacos appears in
the southern California region and cab in the New
York region; TUPAC refers to a rap musician from
Los Angeles, and WIZ refers to a rap musician from
Pittsburgh, not far from the center of the ?Lake Erie?
region.
A large number of slang terms are found to have
strong regional biases, suggesting that slang may
depend on geography more than standard English
does. The terms af and hella display especially
strong regional affinities, appearing in the regional
variants of multiple topics (see Table 3 for defini-
tions). Northern and Southern California use variant
spellings koo and coo to express the same meaning.
11This analysis is from an earlier version of our dataset that
contained some Twitterbots, including one from a Boston-area
radio station. The bots were purged for the evaluation in Sec-
tion 6, though the numerical results are nearly identical.
term definition
af as fuck (very)
coo cool
dl download
fasho for sure
gna going to
hella very
hr hour
iam I am
ima I?m going to
imm I?m
iono I don?t know
lames lame (not cool)
people
term definition
jk just kidding
jp just playing (kid-
ding)
koo cool
lol laugh out loud
nm nothing much
od overdone (very)
omw on my way
smh shake my head
suttin something
wassup what?s up
wyd what are you do-
ing?
Table 3: A glossary of non-standard terms from Ta-
ble 2. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
While research in perceptual dialectology does con-
firm the link of hella to Northern California (Bu-
choltz et al, 2007), we caution that our findings
are merely suggestive, and a more rigorous analysis
must be undertaken before making definitive state-
ments about the regional membership of individual
terms. We view the geographic topic model as an
exploratory tool that may be used to facilitate such
investigations.
Figure 3 shows the regional clustering on the
training set obtained by one run of the model. Each
point represents an author, and the ellipses represent
the bivariate Gaussians for each region. There are
nine compact regions for major metropolitan areas,
two slightly larger regions that encompass Florida
and the area around Lake Erie, and two large re-
gions that partition the country roughly into north
and south.
8 Related Work
The relationship between language and geography
has been a topic of interest to linguists since the
nineteenth century (Johnstone, 2010). An early
work of particular relevance is Kurath?s Word Geog-
raphy of the Eastern United States (1949), in which
he conducted interviews and then mapped the oc-
currence of equivalent word pairs such as stoop and
porch. The essence of this approach?identifying
variable pairs and measuring their frequencies?
remains a dominant methodology in both dialec-
1285
tology (Labov et al, 2006) and sociolinguis-
tics (Tagliamonte, 2006). Within this paradigm,
computational techniques are often applied to post
hoc analysis: logistic regression (Sankoff et al,
2005) and mixed-effects models (Johnson, 2009) are
used to measure the contribution of individual vari-
ables, while hierarchical clustering and multidimen-
sional scaling enable aggregated inference across
multiple variables (Nerbonne, 2009). However, in
all such work it is assumed that the relevant linguis-
tic variables have already been identified?a time-
consuming process involving considerable linguistic
expertise. We view our work as complementary to
this tradition: we work directly from raw text, iden-
tifying both the relevant features and coherent lin-
guistic communities.
An active recent literature concerns geotagged in-
formation on the web, such as search queries (Back-
strom et al, 2008) and tagged images (Crandall et
al., 2009). This research identifies the geographic
distribution of individual queries and tags, but does
not attempt to induce any structural organization of
either the text or geographical space, which is the
focus of our research. More relevant is the work
of Mei et al (2006), in which the distribution over
latent topics in blog posts is conditioned on the ge-
ographical location of the author. This is somewhat
similar to the supervised LDA model that we con-
sider, but their approach assumes that a partitioning
of geographical space into regions is already given.
Methodologically, our cascading topic model is
designed to capture multiple dimensions of variabil-
ity: topics and geography. Mei et al (2007) include
sentiment as a second dimension in a topic model,
using a switching variable so that individual word
tokens may be selected from either the topic or the
sentiment. However, our hypothesis is that individ-
ual word tokens reflect both the topic and the ge-
ographical aspect. Sharing this intuition, Paul and
Girju (2010) build topic-aspect models for the cross
product of topics and aspects. They do not impose
any regularity across multiple aspects of the same
topic, so this approach may not scale when the num-
ber of aspects is large (they consider only two as-
pects). We address this issue using cascading distri-
butions; when the observed data for a given region-
topic pair is low, the model falls back to the base
topic. The use of cascading logistic normal distri-
butions in topic models follows earlier work on dy-
namic topic models (Blei and Lafferty, 2006b; Xing,
2005).
9 Conclusion
This paper presents a model that jointly identifies
words with high regional affinity, geographically-
coherent linguistic regions, and the relationship be-
tween regional and topic variation. The key model-
ing assumption is that regions and topics interact to
shape observed lexical frequencies. We validate this
assumption on a prediction task in which our model
outperforms strong alternatives that do not distin-
guish regional and topical variation.
We see this work as a first step towards a unsuper-
vised methodology for modeling linguistic variation
using raw text. Indeed, in a study of morphosyn-
tactic variation, Szmrecsanyi (2010) finds that by
the most generous measure, geographical factors ac-
count for only 33% of the observed variation. Our
analysis might well improve if non-geographical
factors were considered, including age, race, gen-
der, income and whether a location is urban or ru-
ral. In some regions, estimates of many of these fac-
tors may be obtained by cross-referencing geogra-
phy with demographic data. We hope to explore this
possibility in future work.
Acknowledgments
We would like to thank Amr Ahmed, Jonathan Chang,
Shay Cohen, William Cohen, Ross Curtis, Miro Dud??k,
Scott Kiesling, Seyoung Kim, and the anonymous re-
viewers. This research was enabled by Google?s sup-
port of the Worldly Knowledge project at CMU, AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF IIS-0713379, and an Alfred P. Sloan
Fellowship.
References
L. Backstrom, J. Kleinberg, R. Kumar, and J. Novak.
2008. Spatial variation in search engine queries. In
Proceedings of WWW.
C. M. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
D. M. Blei and M. I. Jordan. 2006. Variational infer-
ence for Dirichlet process mixtures. Bayesian Analy-
sis, 1:121?144.
1286
D. M. Blei and J. Lafferty. 2006a. Correlated topic mod-
els. In NIPS.
D. M. Blei and J. Lafferty. 2006b. Dynamic topic mod-
els. In Proceedings of ICML.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In NIPS.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
M. Bucholtz, N. Bermudez, V. Fung, L. Edwards, and
R. Vargas. 2007. Hella Nor Cal or totally So Cal?
the perceptual dialectology of California. Journal of
English Linguistics, 35(4):325?352.
F. G. Cassidy and J. H. Hall. 1985. Dictionary of Amer-
ican Regional English, volume 1. Harvard University
Press.
J. Chambers. 2009. Sociolinguistic Theory: Linguistic
Variation and its Social Significance. Blackwell.
D. J Crandall, L. Backstrom, D. Huttenlocher, and
J. Kleinberg. 2009. Mapping the world?s photos. In
Proceedings of WWW, page 761770.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. Regular-
ization paths for generalized linear models via coordi-
nate descent. Journal of Statistical Software, 33(1).
D. E. Johnson. 2009. Getting off the GoldVarb standard:
Introducing Rbrul for mixed-effects variable rule anal-
ysis. Language and Linguistics Compass, 3(1):359?
383.
B. Johnstone. 2010. Language and place. In R. Mesthrie
and W. Wolfram, editors, Cambridge Handbook of So-
ciolinguistics. Cambridge University Press.
M. Joshi, D. Das, K. Gimpel, and N. A. Smith. 2010.
Movie reviews and revenues: An experiment in text
regression. In Proceedings of NAACL-HLT.
H. Kurath. 1949. A Word Geography of the Eastern
United States. University of Michigan Press.
H. Kwak, C. Lee, H. Park, and S. Moon. 2010. What
is Twitter, a social network or a news media? In Pro-
ceedings of WWW.
W. Labov, S. Ash, and C. Boberg. 2006. The Atlas of
North American English: Phonetics, Phonology, and
Sound Change. Walter de Gruyter.
W. Labov. 1966. The Social Stratification of English in
New York City. Center for Applied Linguistics.
Q. Mei, C. Liu, H. Su, and C. X Zhai. 2006. A proba-
bilistic approach to spatiotemporal theme pattern min-
ing on weblogs. In Proceedings of WWW, page 542.
Q. Mei, X. Ling, M. Wondra, H. Su, and C. X. Zhai.
2007. Topic sentiment mixture: modeling facets and
opinions in weblogs. In Proceedings of WWW.
T. P. Minka. 2003. Estimating a Dirichlet distribution.
Technical report, Massachusetts Institute of Technol-
ogy.
J. Nerbonne. 2009. Data-driven dialectology. Language
and Linguistics Compass, 3(1).
B. O?Connor, M. Krieger, and D. Ahn. 2010. TweetMo-
tif: Exploratory search and topic summarization for
twitter. In Proceedings of ICWSM.
J. C. Paolillo. 2002. Analyzing Linguistic Variation: Sta-
tistical Models and Methods. CSLI Publications.
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics. In
Proceedings of AAAI.
W. D. Penny. 2001. Variational Bayes for d-dimensional
Gaussian mixture models. Technical report, Univer-
sity College London.
D. Sankoff, S. A. Tagliamonte, and E. Smith. 2005.
Goldvarb X: A variable rule application for Macintosh
and Windows. Technical report, Department of Lin-
guistics, University of Toronto.
R. W. Sinnott. 1984. Virtues of the Haversine. Sky and
Telescope, 68(2).
B. Szmrecsanyi. 2010. Geography is overrated. In
S. Hansen, C. Schwarz, P. Stoeckle, and T. Streck, ed-
itors, Dialectological and Folk Dialectological Con-
cepts of Space. Walter de Gruyter.
S. A. Tagliamonte and D. Denis. 2008. Linguistic ruin?
LOL! Instant messanging and teen language. Ameri-
can Speech, 83.
S. A. Tagliamonte. 2006. Analysing Sociolinguistic Vari-
ation. Cambridge University Press.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
E. P. Xing. 2005. On topic evolution. Technical Report
05-115, Center for Automated Learning and Discov-
ery, Carnegie Mellon University.
1287
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50?61,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Structure Prediction
with Non-Parallel Multilingual Guidance
Shay B. Cohen Dipanjan Das Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,dipanjan,nasmith}@cs.cmu.edu
Abstract
We describe a method for prediction of lin-
guistic structure in a language for which only
unlabeled data is available, using annotated
data from a set of one or more helper lan-
guages. Our approach is based on a model
that locally mixes between supervised mod-
els from the helper languages. Parallel data
is not used, allowing the technique to be ap-
plied even in domains where human-translated
texts are unavailable. We obtain state-of-the-
art performance for two tasks of structure pre-
diction: unsupervised part-of-speech tagging
and unsupervised dependency parsing.
1 Introduction
A major focus of recent NLP research has involved
unsupervised learning of structure such as POS
tag sequences and parse trees (Klein and Manning,
2004; Johnson et al, 2007; Berg-Kirkpatrick et al,
2010; Cohen and Smith, 2010, inter alia). In its
purest form, such research has improved our un-
derstanding of unsupervised learning practically and
formally, and has led to a wide range of new algo-
rithmic ideas. Another strain of research has sought
to exploit resources and tools in some languages (es-
pecially English) to construct similar resources and
tools for other languages, through heuristic ?projec-
tion? (Yarowsky and Ngai, 2001; Xi and Hwa, 2005)
or constraints in learning (Burkett and Klein, 2008;
Smith and Eisner, 2009; Das and Petrov, 2011; Mc-
Donald et al, 2011) or inference (Smith and Smith,
2004). Joint unsupervised learning (Snyder and
Barzilay, 2008; Naseem et al, 2009; Snyder et al,
2009) is yet another research direction that seeks to
learn models for many languages at once, exploiting
linguistic universals and language similarity. The
driving force behind all of this work has been the
hope of building NLP tools for languages that lack
annotated resources.1
In this paper, we present an approach to using
annotated data from one or more languages (helper
languages) to learn models for another language that
lacks annotated data (the target language). Unlike
the previous work mentioned above, our framework
does not rely on parallel data in any form. This is
advantageous because parallel text exists only in a
few text domains (e.g., religious texts, parliamentary
proceedings, and news).
We focus on generative probabilistic models pa-
rameterized by multinomial distributions. We be-
gin with supervised maximum likelihood estimates
for models of the helper languages. In the second
stage, we learn a model for the target language using
unannotated data, maximizing likelihood over inter-
polations of the helper language models? distribu-
tions. The tying is performed at the parameter level,
through coarse, nearly-universal syntactic categories
(POS tags). The resulting model is then used to ini-
tialize learning of the target language?s model using
standard unsupervised parameter estimation.
Some previous multilingual research, such as
Bayesian parameter tying across languages (Co-
hen and Smith, 2009) or models of parameter
1Although the stated objective is often to build systems for
resource-poor languages and domains, for evaluation purposes,
annotated treebank test data figure prominently in this research
(including in this paper).
50
drift down phylogenetic trees (Berg-Kirkpatrick and
Klein, 2010) is comparable, but the practical as-
sumption of supervised helper languages is new to
this work. Naseem et al (2010) used universal
syntactic categories and rules to improve grammar
induction, but their model required expert hand-
written rules as constraints.
Herein, we specifically focus on two problems
in linguistic structure prediction: unsupervised POS
tagging and unsupervised dependency grammar in-
duction. Our experiments demonstrate that the pre-
sented method outperforms strong state-of-the-art
unsupervised baselines for both tasks. Our approach
can be applied to other problems in which a sub-
set of the model parameters can be linked across
languages. We also experiment with unsupervised
learning of dependency structures from words, by
combining our tagger and parser. Our results show
that combining our tagger and parser with joint
inference outperforms pipeline inference, and, in
several cases, even outperforms models built using
gold-standard part-of-speech tags.
2 Overview
For each language `, we assume the presence of a
set of fine-grained POS tags F`, used to annotate the
language?s treebank. Furthermore, we assume that
there is a set of universal, coarse-grained POS tags
C such that, for every language `, there is a determin-
istic mapping from fine-grained to coarse-grained
tags, ?` : F` ? C. Our approach can be summa-
rized using the following steps for a given task:
1. Select a set of L helper languages for which there
exists annotated data ?D1, . . . ,DL?. Here, we use
treebanks in these languages.
2. For all ` ? {1, . . . , L}, convert the examples in
D` by applying ?` to every POS tag in the data,
resulting in D?`. Estimate the parameters of a
probabilistic model using D?`. In this work, such
models are generative probabilistic models based
on multinomial distributions,2 including an HMM
and the dependency model with valence (DMV)
of Klein and Manning (2004). Denote the subset
of parameters that are unlexicalized by ?(`). (Lex-
icalized parameters will be denoted ?(`).)
2In ?4 we also consider a feature-based parametrization.
3. For the target language, define the set of valid un-
lexicalized parameters
? =
{
?
??????k =
L?
`=1
?`,k?(`)k ,
L?
`=1
?`,k = 1,? ? 0
}
,
(1)
for each group of parameters k, and maximize
likelihood over that set, using the target-language
unannotated data U . Because the syntactic cate-
gories referenced by each ?(`) and all models in ?
are in C, the models will be in the same parametric
family. (Figure 1 gives a graphical interpretation
of ?.) Let the resulting model be ?.
4. Transform ? by expanding the coarse-grained
syntactic categories into the target language?s
fine-grained categories. Use the resulting model
to initialize parameter estimation, this time over
fine-grained tags, again using the unannotated
target-language data U . Initialize lexicalized pa-
rameters ? for the target language using standard
methods (e.g., uniform initialization with random
symmetry breaking).
The main idea in the approach is to estimate a
certain model family for one language, while using
supervised models from other languages. The link
between the languages is achieved through coarse-
grained categories, which are now now common-
place (and arguably central to any theory of natural
language syntax). A key novel contribution is the
use of helper languages for initialization, and of un-
supervised learning to learn the contribution of each
helper language to that initialization (step 3). Addi-
tional treatment is required in expanding the coarse-
grained model to the fine-grained one (step 4).
3 Interpolated Multilingual Probabilistic
Context-Free Grammars
Our focus in this paper is on models that consist
of multinomial distributions that have relationships
between them through a generative process such as
a probabilistic context-free grammar (PCFG). More
specifically, we assume that we have a model defin-
ing a probability distribution over observed surface
forms x and derivations y parametrized by ?:
51
(0,1,0)
(0,0,1) (1,0,0)
English
Czech
German
Italian
Figure 1: A simple case of interpolation within the 3-
event probability simplex. The shaded area corresponds
to a convex hull inside the probability simplex, indicating
a mixture of the parameters of the four languages shown
in the figure.
p(x,y | ?) =
K?
k=1
Nk?
i=1
?fk,i(x,y)k,i (2)
= exp
K?
k=1
Nk?
i=1
fk,i(x,y) log ?k,i (3)
where fk,i is a function that ?counts? the number
of times the kth distribution?s ith event occurs in
the derivation. The parameters ? are a collection
of K multinomials ??1, . . . ,?K?, the kth of which
includes Nk events. Letting ?k = ??k,1, . . . , ?k,Nk?,
each ?k,i is a probability, such that ?k,?i, ?k,i ? 0
and ?k,?Nki=1 ?k,i = 1.
3.1 Multilingual Interpolation
Our framework places additional, temporary con-
straints on the parameters ?. More specifically, we
assume that we have L existing, parameter estimates
for the multinomial families from Eq. 3. Each such
estimate ?(`), for 1 ? ` ? L, corresponds to a the
maximum likelihood estimate based on annotated
data for the `th helper language. Then, to create a
model for new language, we define a new set of pa-
rameters ? as:
?k,i =
L?
`=1
?`,k?(`)k,i , (4)
where ? is the set of coefficients that we will now
be interested in estimating (instead of directly esti-
mating ?). Note that for each k,?L`=1 ?`,k = 1 and
?`,k ? 0.
3.2 Grammatical Interpretation
We now give an interpretation of our approach relat-
ing it to PCFGs. We assume familiarity with PCFGs.
For a PCFG ?G,?? we denote the set of nontermi-
nal symbols by N , the set of terminal symbols by
?, and the set of rewrite rules for each nonterminal
A ? N by R(A). Each r ? R(A) has the form
A ? ? where ? ? (N ? ?)?. In addition, there is
a probability attached to each rule ?A?? such that
?A ? N ,??:(A??)?R(A) ?A?? = 1. A PCFG can
be framed as a model using Eq. 3, where ? corre-
spond to K = |N | multinomial distributions, where
each distribution attaches probabilities to rules with
a specific left hand symbol.
We assume that the model we are trying to
estimate (over coarse part-of-speech tags) can be
framed as a PCFG ?G,??. This is indeed the case
for part-of-speech tagging and dependency grammar
induction we experiment with in ?6. In that case,
our approach can be framed for PCFGs as follow-
ing. We assume that there exists L set of parameters
for this PCFG ?(1), . . . ,?(L), each corresponding to
a helper language. We then create a new PCFG G?
with parameters ?? and ? as follows:
1. G? contains all nonterminal and terminal symbols
inG, and none of the rules inG.
2. For each nonterminal A in G, we create a new
nonterminal aA,` for ` ? {1, . . . , L}.
3. For each nonterminal A in G, we create rules
A ? aA,` for ` ? {1, . . . , L} which have proba-
bilities ?A?aA,` .
4. For each rule A? ? inG, we add toG? the rule
aA,` ? ? with
??aA,`?? = ?
(`)
A??. (5)
where ?(`)A?? is the probability associated with
rule A? ? in the `th helper language.
At each point, the derivational process of this
PCFG uses the nonterminal?s specific ? coefficients
52
to choose one of the helper languages. It then se-
lects a rule according to the multinomial from that
language. This step is repeated until a whole deriva-
tion is generated.
This PCFG representation of the approach in ?3
points to a possible generalization. Instead of using
an identical CFG backbone for each language, we
can use a set of PCFGs, ?G(`),?(`)? with an iden-
tical nonterminal set and alphabet, and repeat the
same construction as above, replacing step 4 with
the addition of rules of the form aA,` ? ? for each
rule A ? ? in G(`). Such a construction allows
more syntactic variability in the language we are try-
ing to estimate, originating in the syntax of the var-
ious helper languages. In this paper, we do not use
this generalization, and always use the same PCFG
backbone for all languages.
Note that the interpolated model can still be un-
derstood in terms of the exponential model of Eq. 3.
For a given collection of multinomials and base
models of the form of Eq. 3, we can analogously
define a new log-linear model over a set of ex-
tended derivations. These derivations will now in-
clude L ? K features of the form g`,k(x,y), cor-
responding to a count of the event of choosing the
`th mixture component for multinomial k. In addi-
tion, the feature set fk,i(x,y) will be extended to
a feature set of the form f`,k,i(x,y), analogous to
step 4 in constructed PCFG above. The model pa-
rameterized according to Eq. 4 can be recovered by
marginalizing out the ?g? features. We will refer to
the model with these new set of features as ?the ex-
tended model.?
4 Inference and Parameter Estimation
The main building block commonly required for un-
supervised learning in NLP is that of computing fea-
ture expectations for a given model. These feature
expectations can be used with an algorithm such as
expectation-maximization (where the expectations
are normalized to obtain a new set of multinomial
weights) or with other gradient based log-likelihood
optimization algorithms such as L-BFGS (Liu and
Nocedal, 1989) for feature-rich models.
Estimating Multinomial Distributions Given a
surface form x, a multinomial k and an event i in the
multinomial, ?feature expectation? refers to the cal-
culation of the following quantities (in the extended
model):
E[f`,k,i(x,y)] = ?y p(x,y | ?)f`,k,i(x,y) (6)
E[g`,k(x,y)] = ?y p(x,y | ?)g`,k(x,y) (7)
These feature expectations can usually be computed
using algorithms such as the forward-backward al-
gorithm for hidden Markov models, or more gener-
ally, the inside-outside algorithm for PCFGs. In this
paper, however, the task of estimation is different
than the traditional task. As mentioned in ?2, we are
interested in estimating ? from Eq. 4, while fixing
?(`). Therefore, we are only interested in computing
expectations of the form of Eq. 7.
As explained in ?3.2, any model interpolating
with the ? parameters can be reduced to a new log-
linear model with additional features representing
the mixture coefficients of ?. We can then use the
inside-outside algorithm to obtain the necessary fea-
ture expectations for features of the form g`,k(x,y),
expectations which assist in the estimation of the ?
parameters.
These feature expectations can readily be used
in estimation algorithms such as expectation-
maximization (EM). With EM, the update at itera-
tion t would be:
?(t)`,k =
E[g`,k(x,y)]?
` E[g`,k(x,y)]
, (8)
where the expectations are taken with respect to
?(t?1) and the fixed ?(l) for ` = 1, . . . , L.
Estimating Feature-Rich Directed Models Re-
cently Berg-Kirkpatrick et al (2010) found that
replacing traditional multinomial parameterizations
with locally normalized, feature-based log-linear
models was advantageous. This can be understood
as parameterizing ?:
?k,i =
exp?>h(k, i)?
i?
exp?>h(k, i?)
(9)
where h(k, i) are a set of features looking at event i
in context k. For such a feature-rich model, our mul-
tilingual modeling framework still substitutes ? with
a mixture of supervised multinomials for L helper
languages as in Eq. 4. However, for computational
53
convenience, we also reparametrize the mixture co-
efficients ?:
?`,k =
exp ?`,k?L
`?=1 exp ?`?,k
(10)
Here, each ?`,k is an unconstrained parameter, and
the above ?softmax? transformation ensures that ?
lies within the probability simplex for context k.
This is done so that a gradient-based optimization
method like L-BFGS (Liu and Nocedal, 1989) can
be used to estimate ? without having to worry about
additional simplex constraints. For optimization,
derivatives of the data log-likelihood with respect to
? need to be computed. We calculate the derivatives
following Berg-Kirkpatrick et al (2010, ?3.1), mak-
ing use of feature expectations, calculated exactly as
before.
In addition to these estimation techniques, which
are based on the optimization of the log-likelihood,
we also consider a trivially simple technique for es-
timating ?: setting ?l,k to the uniform weight L?1,
where L is the number of helper languages.
5 Coarse-to-Fine Multinomial Expansion
To expand these multinomials involving coarse-
grained categories into multinomials over fine-
grained categories specific to the target language t,
we do the following:
? Whenever a multinomial conditions on a coarse
category c ? C, we make copies of it for each fine-
grained category in ??1t (c) ? Ft.3 If the multino-
mial does not condition on coarse categories, it is
simply copied.
? Whenever a probability ?i within a multinomial
distribution involves a coarse-grained category c
as an event (i.e., it is on the left side of the condi-
tional bar), we expand the event into |??1t (c)| new
events, one per corresponding fine-grained cate-
gory, each assigned the value ?i|??1t (c)| .
4
3We note that in the models we experiment with, we always
condition on at most one fine-grained category.
4During this expansion process for a coarse event, we tried
adding random noise to ?i|??1t (c)| and renormalizing, to breaksymmetry between the fine events, but that was found to be
harmful in preliminary experiments.
The result of this expansion is a model in the
desired family; we use it to initialize conventional
unsupervised parameter estimation. Lexical param-
eters, if any, do not undergo this expansion pro-
cess, and they are estimated anew in the fine grained
model during unsupervised learning, and are initial-
ized using standard methods.
6 Experiments and Results
In this section, we describe the experiments under-
taken and the results achieved. We first note the
characteristics of the datasets and the universal POS
tags used in multilingual modeling.
6.1 Data
For our experiments, we fixed a set of four helper
languages with relatively large amounts of data,
displaying nontrivial linguistic diversity: Czech
(Slavic), English (West-Germanic), German (West-
Germanic), and Italian (Romance). The datasets are
the CoNLL-X shared task data for Czech and Ger-
man (Buchholz and Marsi, 2006),5 the Penn Tree-
bank for English (Marcus et al, 1993), and the
CoNLL 2007 shared task data for Italian (Monte-
magni et al, 2003). This was the only set of helper
languages we tested; improvements are likely pos-
sible. We leave an exploration of helper language
choice (a subset selection problem) to future re-
search, instead demonstrating that the concept has
merit.
We considered ten target languages: Bulgarian
(Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese
(Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es),
Swedish (Sv), and Turkish (Tr). The data come
from the CoNLL-X and CoNLL 2007 shared tasks
(Buchholz and Marsi, 2006; Nivre et al, 2007). For
all the experiments conducted, we trained models
on the training section of a language?s treebank and
tested on the test set. Table 1 shows the number of
sentences in the treebanks and the size of fine POS
tagsets for each language.
Following standard practice, in unsupervised
grammar induction experiments we remove punctu-
ation and then eliminate sentences from the data of
length greater than 10.
5These are based on the Prague Dependency Treebank
(Hajic?, 1998) and the Tiger treebank (Brants et al, 2002) re-
spectively.
54
Pt Tr Bg Jp El Sv Es Sl Nl Da
Training sentences 9,071 4,997 12,823 17,044 2,705 11,042 3,306 1,534 13,349 5,190
Test sentences 288 623 398 709 197 389 206 402 386 322
Size of POS tagset 22 31 54 80 38 41 47 29 12 25
Table 1: The first two rows show the sizes of the training and test datasets for each language. The third row shows the
number of fine POS tags in each language including punctuations.
6.2 Universal POS Tags
Our coarse-grained, universal POS tag set consists
of the following 12 tags: NOUN, VERB, ADJ
(adjective), ADV (adverb), PRON (pronoun), DET
(determiner), ADP (preposition or postposition),
NUM (numeral), CONJ (conjunction), PRT (parti-
cle), PUNC (punctuation mark) and X (a catch-all
for other categories such as abbreviations or foreign
words). These follow recent work by Das and Petrov
(2011) on unsupervised POS tagging in a multilin-
gual setting with parallel data, and have been de-
scribed in detail by Petrov et al (2011).
While there might be some controversy about
what an appropriate universal tag set should include,
these 12 categories (or a subset) cover the most fre-
quent parts of speech and exist in one form or an-
other in all of the languages that we studied. For
each language in our data, a mapping from the
fine-grained treebank POS tags to these universal
POS tags was constructed manually by Petrov et al
(2011).
6.3 Part-of-Speech Tagging
Our first experimental task is POS tagging, and here
we describe the specific details of the model, train-
ing and inference and the results attained.
6.3.1 Model
The model is a hidden Markov model (HMM),
which has been popular for unsupervised tagging
tasks (Merialdo, 1994; Elworthy, 1994; Smith and
Eisner, 2005; Berg-Kirkpatrick et al, 2010).6 We
use a bigram model and a locally normalized log-
linear parameterization, like Berg-Kirkpatrick et al
(2010). These locally normalized log-linear mod-
els can look at various aspects of the observation x
given a tag y, or the pair of tags in a transition, in-
corporating overlapping features. In basic monolin-
6HMMs can be understood as a special case of PCFGs.
gual experiments, we used the same set of features
as Berg-Kirkpatrick et al (2010). For the transi-
tion log-linear model, Berg-Kirkpatrick et al (2010)
used only a single indicator feature of a tag pair, es-
sentially equating to a traditional multinomial dis-
tribution. For the emission log-linear model, sev-
eral features were used: an indicator feature con-
joining the state y and the word x, a feature checking
whether x contains a digit conjoined with the state y,
another feature indicating whether x contains a hy-
phen conjoined with y, whether the first letter of x is
upper case along with the state y, and finally indica-
tor features corresponding to suffixes up to length 3
present in x conjoined with the state y.
Since only the unlexicalized transition distribu-
tions are common across multiple languages, assum-
ing that they all use a set of universal POS tags, akin
to Eq. 4, we can have a multilingual version of the
transition distributions, by incorporating supervised
helper transition probabilities. Thus, we can write:
?y?y? =
L?
`=1
?`,y?(`)y?y? (11)
We use the above expression to replace the transi-
tion distributions, obtaining a multilingual mixture
version of the model. Here, the transition probabili-
ties ?(`)y?y? for the `th helper language are fixed afterbeing estimated using maximum likelihood estima-
tion on the helper language?s treebank.
6.3.2 Training and Inference
We trained both the basic feature-based HMM
model as well as the multilingual mixture model by
optimizing the following objective function:7
L(?) =
N?
i=1
log
?
y
p(x(i),y | ?)? C???22
7Note that in the objective function, for brevity, we abuse
notation by using ? for both models ? monolingual and multi-
lingual; the latter model is also parameterized by ?.
55
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 45.7 43.6 38.0 60.4 36.7 37.7 31.8 35.9 43.7 36.2 41.0
Mixture+DG 51.5 38.6 35.8 61.7 38.9 39.9 40.5 36.0 50.2 39.9 43.3
DG (B-K et al, 2010) 53.5 27.9 34.7 52.3 35.3 34.4 40.0 33.4 45.4 48.8 40.6
(a)
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform+DG 83.8 50.4 81.3 77.9 80.3 69.0 82.3 82.8 79.3 82.0 76.9
Mixture+DG 84.7 50.0 82.6 79.9 80.3 67.0 83.3 82.8 80.0 82.0 77.3
DG (B-K et al, 2010) 75.4 50.4 80.7 83.4 88.0 61.5 82.3 75.6 79.2 82.3 75.9
(b)
Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con-
structed from the training section of the corresponding treebank. DG (at the bottom) stands for the direct gradient
method of Berg-Kirkpatrick et al (2010) using a monolingual feature-based HMM. ?Mixture+DG? is the model where
multilingual mixture coefficients ? of helper languages are estimated using coarse tags (?4), followed by expansion
(?5), and then initializing DG with the expanded transition parameters. ?Uniform+DG? is the case where ? are set
to 1/4, transitions of helper languages are mixed, expanded, and then DG is initialized with the result. For (a), eval-
uation is performed using one-to-one mapping accuracy. In case of (b), the tag dictionary solves the problem of tag
identification and performance is measured using per word POS accuracy. ?Avg? denotes macro-average across the
ten languages.
Note that this involves marginalizing out all possible
state configurations y for a sentence x, resulting in
a non-convex objective. As described in ?4, we opti-
mized this function using L-BFGS. For the mono-
lingual model, derivatives of the feature weights
took the exact same form as Berg-Kirkpatrick et al
(2010), while for the mixture case, we computed
gradients with respect to ?, the unconstrained pa-
rameters used to express the mixture coefficients ?
(see Eq. 10). The regularization constant C was set
to 1.0 for all experiments, and L-BFGS was run till
convergence.
During training, for the basic monolingual
feature-based HMM model, we initialized all param-
eters using small random real values, sampled from
N (0, 0.01). For estimation of the mixture parame-
ters ? for our multilingual model (step 3 in ?2), we
similarly sampled real values fromN (0, 0.01) as an
initialization point. Moreover, during this stage, the
emission parameters also go through parameter es-
timation, but they are monolingual, and are initial-
ized with real values sampled from N (0, 0.01); as
explained in ?2, coarse universal tags are used both
in the transitions and emissions during multilingual
estimation.
After the mixture parameters ? are estimated, we
compute the mixture probabilities ? using Eq. 10.
Next, for each tag pair y, y?, we compute ?y?y? ,
which are the coarse transition probabilities inter-
polated using ?, given the helper languages. We
then expand these transition probabilities (see ?5) to
result in transition probabilities based on fine tags.
Finally, we train a feature-HMM by initializing its
transition parameters with natural logarithms of the
expanded ? parameters, and the emission parame-
ters using small random real values sampled from
N (0, 0.01). This implies that the lexicalized emis-
sion parameters ? that were previously estimated in
the coarse multilingual model are thrown away and
not used for initialization; instead standard initial-
ization is used.
For inference at the testing stage, we use min-
imum Bayes-risk decoding (or ?posterior decod-
ing?), by choosing the most probable tag for each
word position, given the entire observation x. We
chose this strategy because it usually performs
slightly better than Viterbi decoding (Cohen and
Smith, 2009; Ganchev et al, 2010).
6.3.3 Experimental Setup
For experiments, we considered three configura-
tions, and for each, we implemented two variants of
POS induction, one without any kind of supervision,
and the other with a tag dictionary. Our baseline is
56
the direct gradient approach of Berg-Kirkpatrick et
al. (2010), which is the current state of the art for this
task, outperforming classical HMMs. Because this
model achieves strong performance using straight-
forward MLE, it also serves as the core model within
our approach. This model has also been applied in
a multilingual setting with parallel data (Das and
Petrov, 2011). In this baseline, we set the number
of HMM states to the number of fine-grained tree-
bank tags for the given language.
We test two versions of our model. The first ini-
tializes training of the target language?s POS model
using a uniform mixture of the helper language mod-
els (i.e., each ?`,y = 1L = 14 ), and expansion fromcoarse-grained to fine-grained POS tags as described
in ?5. We call this model ?Uniform+DG.?
The second version estimates the mixture coeffi-
cients to maximize likelihood, then expands the POS
tags (?5), using the result to initialize training of the
final model. We call this model ?Mixture+DG.?
No Tag Dictionary For each of the above configura-
tions, we ran purely unsupervised training without a
tag dictionary, and evaluated using one-to-one map-
ping accuracy constraining at most one HMM state
to map to a unique treebank tag in the test data, us-
ing maximum bipartite matching. This is a variant of
the greedy one-to-one mapping scheme of Haghighi
and Klein (2006).8
With a Tag Dictionary We also ran a second ver-
sion of each experimental configuration, where we
used a tag dictionary to restrict the possible path se-
quences of the HMM during both learning and infer-
ence. This tag dictionary was constructed only from
the training section of a given language?s treebank.
It is widely known that such knowledge improves
the quality of the model, though it is an open debate
whether such knowledge is realistic to assume. For
this experiment we removed punctuation from the
training and test data, enabling direct use within the
dependency grammar induction experiments.
8We also evaluated our approach using the greedy version of
this evaluation metric, and results followed the same trends with
only minor differences. We did not choose the other variant,
many-to-one mapping accuracy, because quite often the metric
mapped several HMM states to one treebank tag, leaving many
treebank tags unaccounted for.
6.3.4 Results
All results for POS induction are shown in Ta-
ble 2. Without a tag dictionary, in eight out of ten
cases, either Uniform+DG or Mixture+DG outper-
forms the monolingual baseline (Table 2a). For six
of these eight languages, the latter model where the
mixture coefficients are learned automatically fares
better than uniform weighting. With a tag dictionary,
the multilingual variants outperform the baseline in
seven out of ten cases, and the learned mixture out-
performs or matches the uniform mixture in five of
those seven (Table 2b).
6.4 Dependency Grammar Induction
We next describe experiments for dependency gram-
mar induction. As the basic grammatical model,
we adopt the dependency model with valence (Klein
and Manning, 2004), which forms the basis for state-
of-the-art results for dependency grammar induc-
tion in various settings (Cohen and Smith, 2009;
Spitkovsky et al, 2010; Gillenwater et al, 2010;
Berg-Kirkpatrick and Klein, 2010). As shown in Ta-
ble 3, DMV obtains much higher accuracy in the su-
pervised setting than the unsupervised setting, sug-
gesting that more can be achieved with this model
family.9 For this reason, and because DMV is eas-
ily interpreted as a PCFG, it is our starting point and
baseline.
We consider four conditions. The independent
variables are (1) whether we use uniform? (all set to
1
4 ) or estimate them using EM (as described in ?4),and (2) whether we simply use the mixture model to
decode the test data, or to initialize EM for the DMV.
The four settings are denoted ?Uniform,? ?Mixture,?
?Uniform+EM,? and ?Mixture+EM.?
The results are given in Table 3. In general, the
use of data from other languages improves perfor-
mance considerably; all of our methods outperform
the Klein and Manning (2004) initializer, and we
achieve state-of-the-art performance for eight out of
ten languages. Uniform and Mixture behave simi-
larly, with a slight advantage to the trained mixture
setting. Using EM to train the mixture coefficients
more often hurts than helps (six languages out of
ten). It is well known that likelihood does not cor-
9Its supervised performance is still far from the supervised
state of the art in dependency parsing.
57
Method Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Uniform 78.6 45.0 75.6 56.3 57.0 74.0 73.2 46.1 50.7 59.2 61.6
Mixture 76.8 45.3 75.5 58.3 59.5 73.2 75.9 46.0 51.1 59.9 62.2
Uniform+EM 78.7 43.9 74.7 59.8 73.0 70.5 75.5 41.3 45.9 51.3 61.5
Mixture+EM 79.8 44.1 72.8 63.9 72.3 68.7 76.7 41.0 46.0 55.2 62.1
EM (K & M, 2004) 42.5 36.3 54.3 43.0 41.0 42.3 38.1 37.0 38.6 41.4 41.4
PR (G et al, ?10) 47.8 53.4 54.0 60.2 - 42.2 62.4 50.3 37.9 44.0 -
Phylo. (B-K & K, ?10) 63.1 - - - - 58.3 63.8 49.6 45.1 41.6 -
Supervised (MLE) 81.7 75.7 83.0 89.2 81.8 83.2 79.0 74.5 64.8 80.8 79.3
Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy
(fraction of parents which are correct). The three existing methods are: our replication of EM with the initializer from
Klein and Manning (2004), denoted ?EM?; reported results from Gillenwater et al (2010) for posterior regularization
(?PR?); and reported results from Berg-Kirkpatrick and Klein (2010), denoted ?Phylo.? ?Supervised (MLE)? are oracle
results of estimating parameters from gold-standard annotated data using maximum likelihood estimation. ?Avg?
denotes macro-average across the ten languages.
Figure 2: Projection of the learned mixture coefficients
through PCA. In green, Japanese. In red, Dutch, Danish
and Swedish. In blue, Bulgarian and Slovene. In ma-
genta, Portuguese and Spanish. In black, Greek. In cyan,
Turkish.
relate with the true accuracy measurement, and so
it is unsurprising that this holds in the constrained
mixture family as well. In future work, a different
parametrization of the mixture coefficients, through
features, or perhaps a Bayesian prior on the weights,
might lead to an objective that better simulates ac-
curacy.
Table 3 shows that even uniform mixture coef-
ficients are sufficient to obtain accuracy which su-
percedes most unsupervised baselines. We were in-
terested in testing whether the coefficients which are
learned actually reflect similarities between the lan-
guages. To do that, we projected the learned vectors
? for each tested language using principal compo-
nent analysis and plotted the result in Figure 2. It
is interesting to note that languages which are closer
phylogenetically tend to appear closer to each other
in the plot.
Our experiments also show that multilingual
learning performs better for dependency grammar
induction than part-of-speech tagging. We believe
that this happens because of the nature of the mod-
els and data we use. The transition matrix in part-
of-speech tagging largely depends on word order in
the various helper languages, which differs greatly.
This means that a mixture of transition matrices will
not necessarily yield a meaningful transition matrix.
However, for dependency grammar, there are certain
universal dependencies which appear in all helper
languages, and therefore, a mixture between multi-
nomials for these dependencies still yields a useful
multinomial.
6.5 Inducing Dependencies from Words
Finally, we combine the models for POS tagging and
grammar induction to perform grammar induction
directly from words, instead of gold-standard POS
tags. Our approach is as follows:
1. With a tag dictionary, learn a fine-grained POS
tagging model unsupervised, using either DG or
Mixture+DG as described in ?6.3 and shown in
Table 2b.
58
Method Tags Pt Tr Bg Jp El Sv Es Sl Nl Da Avg
Joint DG 68.4 52.4 62.4 61.4 63.5 58.2 67.7 47.2 48.3 50.4 57.9
Joint Mixture+DG 62.2 47.4 67.0 69.5 52.2 49.1 69.3 36.8 52.2 50.1 55.6
Pipeline DG 60.0 50.8 57.7 64.2 68.2 57.9 65.8 45.8 49.9 48.9 56.9
Pipeline Mixture+DG 59.8 47.1 62.9 68.6 50.0 47.6 68.1 36.4 51.2 48.3 54.0
Gold-standard tags 79.8 45.3 75.6 63.9 73.0 74.0 76.7 46.1 50.7 59.9 64.5
Table 4: Results for dependency grammar induction over words. ?Joint?/?Pipeline? refers to joint/pipeline decoding
of tags and dependencies as described in the text. See ?6.3 for a description of DG and Mixture+DG. For the induction
of dependencies we use the Mixture+EM setting as described in ?6.4. All tag induction uses a dictionary as specified
in ?6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in
Table 3. ?Avg? denotes macro-average across the ten languages.
2. Apply the fine-grained tagger to the words in the
training data for the dependency parser. We con-
sider two variants: the most probable assignment
of tags to words (denoted ?Pipeline?), and the pos-
terior distribution over tags for each word, repre-
sented as a weighted ?sausage? lattice (denoted
?Joint?). This idea was explored for joint infer-
ence by Cohen and Smith (2007).
3. We apply the Mixture+EM unsupervised parser
learning method from ?6.4 to the automatically
tagged sentences, or the lattices.
4. Given the two models, we infer POS tags on the
test data using DG or Mixture+DG to get a lattice
(Joint) or a sequence (Pipeline) and then parse us-
ing the model from the previous step.10 The re-
sulting dependency trees are evaluated against the
gold standard.
Results are reported in Table 4. In almost all cases,
joint decoding of tags and trees performs better than
the pipeline. Even though our part-of-speech tagger
with multilingual guidance outperforms the com-
pletely unsupervised baseline, there is not always an
advantage of using this multilingually guided part-
of-speech tagger for dependency grammar induc-
tion. For Turkish, Japanese, Slovene and Dutch, our
unsupervised learner from words outperforms unsu-
pervised parsing using gold-standard part-of-speech
tags.
We note that some recent work gives a treatment
to unsupervised parsing (but not of dependencies)
10The decoding method on test data (Joint or Pipeline) was
matched to the training method, though they are orthogonal in
principle.
directly from words (Seginer, 2007). Earlier work
that induced part-of-speech tags and then performed
unsupervised parsing in a pipeline includes Klein
and Manning (2004) and Smith (2006). Headden
et al (2009) described the use of a lexicalized vari-
ant of the DMV model, with the use of gold part-of-
speech tags.
7 Conclusion
We presented an approach to exploiting annotated
data in helper languages to infer part-of-speech tag-
ging and dependency parsing models in a different,
target language, without parallel data. Our approach
performs well in many cases. We also described a
way to do joint decoding of part-of-speech tags and
dependencies which performs better than a pipeline.
Future work might consider exploiting a larger num-
ber of treebanks, and more powerful techniques for
combining models than simple local mixtures.
Acknowledgments
We thank Ryan McDonald and Slav Petrov for help-
ful comments on an early draft of the paper. This re-
search has been funded by NSF grants IIS-0844507 and
IIS-0915187 and by U.S. Army Research Office grant
W911NF-10-1-0533.
References
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic
grammar induction. In Proceedings of ACL.
T. Berg-Kirkpatrick, A. B. Co?te?, J. DeNero, and D. Klein.
2010. Painless unsupervised learning with features. In
Proceedings of NAACL-HLT.
59
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proceedings
of CoNLL.
D. Burkett and D. Klein. 2008. Two languages are bet-
ter than one (for syntactic parsing). In Proceedings of
EMNLP.
S. B. Cohen and N. A. Smith. 2007. Joint morpholog-
ical and syntactic disambiguation. In Proceedings of
EMNLP-CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proceedings of HLT-
NAACL.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proceedings of ACL-HLT.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceedings of ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11:2001?2049.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar
induction. In Proceedings of ACL.
A. Haghighi and D. Klein. 2006. Prototype driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
J. Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevova?. Prague Karolinum, Charles Univer-
sity Press.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of
NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proceedings of ACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45:503?528.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source
transfer of delexicalized dependency parsers. In Pro-
ceedings of EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Compulational Lingustics, 20(2):155?
72.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,
F. Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.
2003. Building the Italian Syntactic-Semantic Tree-
bank. In Building and using Parsed Corpora, Lan-
guage and Speech Series. Kluwer, Dordrecht.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. JAIR, 36.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proceedings of EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceedings
of CoNLL.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. ArXiv:1104.2086.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Proceedings of ACL.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of ACL.
D. A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar features.
In Proceedings of EMNLP.
D. A. Smith and N. A. Smith. 2004. Bilingual parsing
with factored estimation: Using English to parse Ko-
rean. In Proceedings of EMNLP.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proceedings of ACL.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsuper-
vised multilingual grammar induction. In Proceedings
of ACL-IJCNLP.
V. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010. From
baby steps to leapfrog: How ?less is more? in unsuper-
vised dependency parsing. In Proceedings of NAACL.
C. Xi and R. Hwa. 2005. A backoff model for bootstrap-
ping resources for non-English languages. In Proceed-
ings of HLT-EMNLP.
60
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proceedings of NAACL.
61
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 238?249,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Dual Decomposition with Many Overlapping Components
Andre? F. T. Martins?? Noah A. Smith? Pedro M. Q. Aguiar? Ma?rio A. T. Figueiredo?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Sistemas e Robo?tica, Instituto Superior Te?cnico, Lisboa, Portugal
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.pt
Abstract
Dual decomposition has been recently pro-
posed as a way of combining complemen-
tary models, with a boost in predictive power.
However, in cases where lightweight decom-
positions are not readily available (e.g., due to
the presence of rich features or logical con-
straints), the original subgradient algorithm
is inefficient. We sidestep that difficulty by
adopting an augmented Lagrangian method
that accelerates model consensus by regular-
izing towards the averaged votes. We show
how first-order logical constraints can be han-
dled efficiently, even though the correspond-
ing subproblems are no longer combinatorial,
and report experiments in dependency pars-
ing, with state-of-the-art results.
1 Introduction
The last years have witnessed increasingly accurate
models for syntax, semantics, and machine transla-
tion (Chiang, 2007; Finkel et al, 2008; Petrov and
Klein, 2008; Smith and Eisner, 2008; Martins et
al., 2009a; Johansson and Nugues, 2008; Koo et al,
2010). The predictive power of such models stems
from their ability to break locality assumptions. The
resulting combinatorial explosion typically demands
some form of approximate decoding, such as sam-
pling, heuristic search, or variational inference.
In this paper, we focus on parsers built from lin-
ear programming relaxations, the so-called ?turbo
parsers? (Martins et al, 2009a; Martins et al, 2010).
Rush et al (2010) applied dual decomposition as
a way of combining models which alone permit
efficient decoding, but whose combination is in-
tractable. This results in a relaxation of the origi-
nal problem that is elegantly solved with the sub-
gradient algorithm. While this technique has proven
quite effective in parsing (Koo et al, 2010; Auli
and Lopez, 2011) as well as machine translation
(Rush and Collins, 2011), we show here that its
success is strongly tied to the ability of finding a
?good? decomposition, i.e., one involving few over-
lapping components (or slaves). With many compo-
nents, the subgradient algorithm exhibits extremely
slow convergence (cf. Fig. 2). Unfortunately, a
lightweight decomposition is not always at hand, ei-
ther because the problem does not factor in a natural
way, or because one would like to incorporate fea-
tures that cannot be easily absorbed in few tractable
components. Examples include features generated
by statements in first-order logic, features that vio-
late Markov assumptions, or history features such as
the ones employed in transition-based parsers.
To tackle the kind of problems above, we adopt
DD-ADMM (Alg. 1), a recently proposed algorithm
that accelerates dual decomposition (Martins et al,
2011). DD-ADMM retains the modularity of the
subgradient-based method, but it speeds up consen-
sus by regularizing each slave subproblem towards
the averaged votes obtained in the previous round
(cf. Eq. 14). While this yields more involved sub-
problems (with a quadratic term), we show that ex-
act solutions can still be efficiently computed for
all cases of interest, by using sort operations. As
a result, we obtain parsers that can handle very rich
features, do not require specifying a decomposition,
and can be heavily parallelized. We demonstrate the
success of the approach by presenting experiments
in dependency parsing with state-of-the-art results.
2 Background
2.1 Structured Prediction
Let x ? X be an input object (e.g., a sentence), from
which we want to predict a structured output y ?
Y (e.g., a parse tree). The output set Y is assumed
too large for exhaustive search to be tractable. We
assume to have a model that assigns a score f(y) to
each candidate output, based on which we predict
y? = arg max
y?Y
f(y). (1)
238
Designing the model must obey certain practical
considerations. If efficiency is the major concern,
a simple model is usually chosen so that Eq. 1 can
be solved efficiently, at the cost of limited expressive
power. If we care more about accuracy, a model with
richer features and more involved score functions
may be designed. Decoding, however, will be more
expensive, and approximations are often necessary.
A typical source of intractability comes from the
combinatorial explosion inherent in the composition
of two or more tractable models (Bar-Hillel et al,
1964; Tromble and Eisner, 2006). Recently, Rush
et al (2010) have proposed a dual decomposition
framework to address NLP problems in which the
global score decomposes as f(y) = f1(z1)+f2(z2),
where z1 and z2 are two overlapping ?views? of the
output, so that Eq. 1 becomes:
maximize f1(z1) + f2(z2)
w.r.t. z1 ? Y1, z2 ? Y2
s.t. z1 ? z2.
(2)
Above, the notation z1 ? z2 means that z1 and
z2 ?agree on their overlaps,? and an isomorphism
Y ' {?z1, z2? ? Y1?Y2 | z1 ? z2} is assumed. We
next formalize these notions and proceed to compo-
sitions of an arbitrary number of models. Of special
interest is the unexplored setting where this number
is very large and each component very simple.
2.2 Decomposition into Parts
A crucial step in the design of structured predictors
is that of decomposing outputs into parts (Taskar et
al., 2003). We assume the following setup:
Basic parts. We let R be a set of basic parts, such
that each element y ? Y can be identified with a
subset of R. The exact meaning of a ?basic part?
is problem dependent. For example, in dependency
parsing, R can be the set of all possible dependency
arcs (see Fig. 1); in phrase-based parsing, it can be
the set of possible spans; in sequence labeling, it can
be the set of possible labels at each position. Our
only assumption is that we can ?read out? y from
the basic parts it contains. For convenience, we rep-
resent y as a binary vector, y = ?y(r)?r?R, where
y(r) = 1 if part r belongs to y, and 0 otherwise.
Decomposition. We generalize the decomposition
in Eq. 2 by considering sets Y1, . . . ,YS for S ? 2.
Figure 1: Parts used by our parser. Arcs are the ba-
sic parts: any dependency tree can be ?read out? from
the arcs it contains. Consecutive siblings and grandpar-
ent parts introduce horizontal and vertical Markovization
(McDonald et al, 2006; Carreras, 2007). We break the
horizontal Markov assumption via all siblings parts and
the vertical one through parts which indicate a directed
path between two words. Inspired by transition-based
parsers, we also adopt head bigram parts, which look at
the heads attached to consecutive words. Finally, we fol-
low Martins et al (2009a) and have parts which indicate
if an arc is non-projective (i.e., if it spans words that do
not descend from its head).
Each Ys is associated with its own set of parts Rs, in
the same sense as above; we represent the elements
of Ys as binary vectors zs = ?zs(r)?r?Rs . Examples
are vectors indicating a tree structure, a sequence,
or an assignment of variables to a factor, in which
case it may happen that only some binary vectors
are legal. Some parts in Rs are basic, while others
are not. We denote by R?s = Rs ? R the subset of
the ones that are. In addition, we assume that:
? R1, . . . ,RS jointly cover R, i.e., R ? ?Ss=1 Rs;
? Only basic parts may overlap, i.e., Rs ? Rt ?
R, ?s, t ? {1, . . . , S};
? Each zs ? Ys is completely defined by its entries
indexed by elements of R?s, from which we can
guess the ones in Rs \ R?s. This implies that each
y ? Y has a unique decomposition ?z1, . . . , zS?.
Fig. 1 shows several parts used in dependency pars-
ing models; in phrase-based parsing, these could be
spans and production rules anchored in the surface
string; in sequence labeling, they can be unigram,
bigram, and trigram labels.1
1There is a lot of flexibility about how to decompose the
model into S components: each set Rs can correspond to a sin-
239
Global consistency. We want to be able to read
out y ? Y by ?gluing? together the components
?z1, . . . , zS?. This is only meaningful if they are
?globally consistent,? a notion which we make pre-
cise. Two components zs ? Ys and zt ? Yt are said
to be consistent (denoted zs ? zt) if they agree on
their overlaps, i.e., if zs(r) = zt(r), ?r ? Rs ? Rt.
A complete assignment ?z1, . . . , zS? is globally con-
sistent if all pairs of components are consistent. This
is equivalent to the existence of a witness vector
?u(r)?r?R such that zs(r) = u(r), ?s, r ? R?s.
With this setup, assuming that the score function
decomposes as f(z) = ?Ss=1 fs(zs), the decoding
problem (which extends Eq. 2 for S ? 2) becomes:
P : maximize
?S
s=1 fs(zs)w.r.t. zs ? Ys, ?s
?u(r)?r?R ? R|R|,
s.t. zs(r) = u(r), ?s, r ? R?s.
(3)
We call the equality constraints expressed in the last
line the ?agreement constraints.? It is these con-
straints that complicate the problem, which would
otherwise be exactly separable into S subproblems.
The dual decomposition method (Komodakis et al,
2007; Rush et al, 2010) builds an approximation by
dualizing out these constraints, as we describe next.
2.3 Dual Decomposition
We describe dual decomposition in a slightly differ-
ent manner than Rush et al (2010): we will first
build a relaxation of P (called P ?), in which the en-
tire approximation is enclosed. Then, we dualize P ?,
yielding problem D. In the second step, the duality
gap is zero, i.e., P ? and D are equivalent.2
Relaxation. For each s ? {1, . . . , S} we consider
the convex hull of Ys,
Zs =
{ ?
zs?Ys
p(zs)zs
???? p(zs) ? 0,
?
zs?Ys
p(zs) = 1
}
.
(4)
gle factor in a factor graph (Smith and Eisner, 2008), or to a
entire subgraph enclosing several factors (Koo et al, 2010), or
even to a formula in Markov logic (Richardson and Domingos,
2006). In these examples, the basic parts may correspond to
individual variable-value pairs.
2Instead of following the path P ? P ? ? D, Rush et al
(2010) go straight from P to D via a Lagrangian relaxation.
The two formulations are equivalent for linear score functions.
We have that Ys = Zs ? Z|Rs|; hence, problem P
(Eq. 3) is equivalent to one in which each Ys is re-
placed by Zs and the z-variables are constrained to
be integer. By dropping the integer constraints, we
obtain the following relaxed problem:
P ? : maximize
?S
s=1 fs(zs)w.r.t. zs ? Zs, ?s
?u(r)?r?R ? R|R|,
s.t. zs(r) = u(r), ?s, r ? R?s.
(5)
If the score functions fs are convex, P ? becomes a
convex program (unlike P , which is discrete); being
a relaxation, it provides an upper bound of P .
Lagrangian. Introducing a Lagrange multiplier
?s(r) for each agreement constraint in Eq. 5, one
obtains the Lagrangian function
L(z, u, ?) =
?S
s=1
(
fs(zs) +
?
r?R?s ?s(r)zs(r)
)
??r?R
(?
s:r?R?s ?s(r)
)
u(r), (6)
and the dual problem (the master)
D : minimize
?S
s=1 gs(?s)w.r.t. ? = ??1, . . . , ?S?
s.t. ?s:r?R?s ?s(r) = 0, ?r ? R,
(7)
where the gs(?s) are the solution values of the fol-
lowing subproblems (the slaves):
maximize fs(zs) +
?
r?R?s ?s(r)zs(r)w.r.t. zs ? Zs. (8)
We assume that strong duality holds (w.r.t. Eqs. 5?
7), hence we have P ? P ? = D.3
Solving the dual. Why is the dual formulation D
(Eqs. 7?8) more appealing than P ? (Eq. 5)? The an-
swer is that the components 1, . . . , S are now de-
coupled, which makes things easier provided each
slave subproblem (Eq. 8) can be solved efficiently.
In fact, this is always a concern in the mind of
the model?s designer when she chooses a decom-
position (the framework that we describe in ?3,
in some sense, alleviates her from this concern).
If the score functions are linear, i.e., of the form
fs(zs) =
?
r?Rs ?s(r)zs(r) for some vector ?s =
??s(r)?r?Rs , then Eq. 8 becomes a linear program,
for which a solution exists at a vertex of Zs (which
3This is guaranteed if the score functions fs are linear.
240
in turn is an element of Ys). Depending on the struc-
ture of the problem, Eq. 8 may be solved by brute
force, dynamic programming, or specialized combi-
natorial algorithms (Rush et al, 2010; Koo et al,
2010; Rush and Collins, 2011).
Applying the projected subgradient method (Ko-
modakis et al, 2007; Rush et al, 2010) to the mas-
ter problem (Eq. 7) yields a remarkably simple algo-
rithm, which at each round t solves the subproblems
in Eq. 8 for s = 1, . . . , S, and then gathers these
solutions (call them zt+1s ) to compute an ?averaged?
vote for each basic part,
ut+1(r) = 1?(r)
?
s:r?R?s zt+1s (r), (9)
where ?(r) = |{s : r ? Rs}| is the number of com-
ponents which contain part r. An update of the La-
grange variables follows,
?t+1s (r) = ?ts(r)? ?t(zt+1s (r)? ut+1(r)), (10)
where ?t is a stepsize. Intuitively, the algorithm
pushes for a consensus among the slaves (Eq. 9),
via an adjustment of the Lagrange multipliers which
takes into consideration deviations from the aver-
age (Eq. 10). The subgradient method is guaran-
teed to converge to the solution of D (Eq. 7), for
suitably chosen stepsizes (Shor, 1985; Bertsekas et
al., 1999); it also provides a certificate of optimal-
ity in case the relaxation is tight (i.e., P = D) and
the exact solution has been found. However, con-
vergence is slow when S is large (as we will show
in the experimental section), and no certificates are
available when there is a relaxation gap (P < P ?).
In the next section, we describe the DD-ADMM al-
gorithm (Martins et al, 2011), which does not have
these drawbacks and shares a similar simplicity.
3 Alternating Directions Method
There are two reasons why subgradient-based dual
decomposition is not completely satisfying:
? it may take a long time to reach a consensus;
? it puts all its resources in solving the dual problem
D, and does not attempt to make progress in the
primal P ?, which is closer to our main concern.4
4Our main concern is P ; however solving P ? is often a
useful step towards that goal, either because a good rounding
scheme exists, or because one may build tighter relaxations to
approach P (Sontag et al, 2008; Rush and Collins, 2011).
Taking a look back at the relaxed primal problem
P ? (Eq. 5), we see that any primal feasible solution
must satisfy the agreement constraints. This sug-
gests that penalizing violations of these constraints
could speed up consensus.
Augmented Lagrangian. By adding a penalty
term to Eq. 6, we obtain the augmented Lagrangian
function (Hestenes, 1969; Powell, 1969):
A?(z, u, ?) = L(z, u, ?)?
?
2
S?
s=1
?
r?R?s
(zs(r)? u(r))2,
(11)
where the parameter ? ? 0 controls the intensity
of the penalty. Augmented Lagrangian methods
are well-known in the optimization community (see,
e.g., Bertsekas et al (1999), ?4.2). They alternate
updates to the ?-variables, while seeking to maxi-
mize A? with respect to z and u. In our case, how-
ever, this joint maximization poses difficulties, since
the penalty term couples the two variables. The al-
ternating directions method of multipliers (ADMM),
coined by Gabay and Mercier (1976) and Glowinski
and Marroco (1975), sidesteps this issue by perform-
ing alternate maximizations,
zt+1 = arg max
z
A?(z, ut, ?t), (12)
ut+1 = arg max
u
A?(zt+1, u, ?t), (13)
followed by an update of the Lagrange multipliers
as in Eq. 10. Recently, ADMM has attracted inter-
est, being applied in a variety of problems; see the
recent book by Boyd et al (2011) for an overview.
As derived in the App. A, the u-updates in Eq. 13
have a closed form, which is precisely the averag-
ing operation performed by the subgradient method
(Eq. 9). We are left with the problem of comput-
ing the z-updates. Like in the subgradient approach,
the maximization in Eq. 12 can be separated into S
independent slave subproblems, which now take the
form:
maximize fs(zs) +
?
r?R?s ?s(r)zs(r)
??2
?
r?R?s(zs(r)? ut(r))2w.r.t. zs ? Zs(x).
(14)
Comparing Eq. 8 and Eq. 14, we observe that the
only difference is the presence in the latter of a
241
quadratic term which regularizes towards the pre-
vious averaged votes ut(r). Because of this term,
the solution of Eq. 14 for linear score functions may
not be at a vertex (in contrast to the subgradient
method). We devote ?4 to describing exact and effi-
cient ways of solving the problem in Eq. 14 for im-
portant, widely used slaves. Before going into de-
tails, we mention another advantage of ADMM over
the subgradient algorithm: it knows when to stop.
Primal and dual residuals. Recall that the sub-
gradient method provides optimality certificates
when the relaxation is tight (P = P ?) and an ex-
act solution of P has been found. While this is good
enough when tight relaxations are frequent, as in the
settings explored by Rush et al (2010), Koo et al
(2010), and Rush and Collins (2011), it is hard to
know when to stop when a relaxation gap exists.
We would like to have similar guarantees concern-
ing the relaxed primal P ?.5 A general weakness of
subgradient algorithms is that they do not have this
capacity, and so are usually stopped by specifying a
maximum number of iterations. In contrast, ADMM
allows to keep track of primal and dual residuals
(Boyd et al, 2011). This allows providing certifi-
cates not only for the exact solution of P (when the
relaxation is tight), but also to terminate when a near
optimal solution of the relaxed problem P ? has been
found. The primal residual rtP measures the amount
by which the agreement constraints are violated:
rtP =
?S
s=1
?
r?R?s(zts(r)? ut(r))2?
r?R ?(r)
; (15)
the dual residual rtD is the amount by which a dual
optimality condition is violated (see Boyd et al
(2011), p.18, for details). It is computed via:
rtD =
?
r?R ?(r)(ut(r)? ut?1(r))2?
r?R ?(r)
, (16)
Our stopping criterion is thus that these two residu-
als are below a threshold, e.g., 1 ? 10?3. The com-
plete algorithm is depicted as Alg. 1. As stated in
5This problem is more important than it may look. Problems
with many slaves tend to be less exact, hence relaxation gaps
are frequent. Also, when decoding is embedded in training, it is
useful to obtain the fractional solution of the relaxed primal P
(rather than an approximate integer solution). See Kulesza and
Pereira (2007) and Martins et al (2009b) for details.
Algorithm 1 ADMM-based Dual Decomposition
1: input: score functions ?fs(.)?Ss=1, parameters ?, ?,
thresholds P and D.
2: initialize t? 1
3: initialize u1(r)? 0.5 and ?1s(r)? 0, ?s, ?r ? R?s
4: repeat
5: for each s = 1, . . . , S do
6: make a zs-update, yielding zt+1s (Eq. 14)
7: end for
8: make a u-update, yielding ut+1 (Eq. 9)
9: make a ?-update, yielding ?t+1 (Eq. 10)
10: t? t+ 1
11: until rt+1P < P and rt+1D < D (Eqs. 15?16)
12: output: relaxed primal and dual solutions u, z, ?
Martins et al (2011), convergence to the solution of
P ? is guaranteed with a fixed stepsize ?t = ??, with
? ? [1, 1.618] (Glowinski and Le Tallec, 1989, Thm.
4.2). In our experiments, we set ? = 1.5, and adapt
? as described in (Boyd et al, 2011, p.20).6
4 Solving the Subproblems
In this section, we address the slave subproblems of
DD-ADMM (Eq. 14). We show how these subprob-
lems can be solved efficiently for several important
cases that arise in NLP applications. Throughout,
we assume that the score functions fs are linear, i.e.,
they can be written as fs(zs) = ?r?Rs ?s(r)zs(r).This is the case whenever a linear model is used, in
which case ?s(r) = 1?(r)w ? ?(x, r), where w is a
weight vector and ?(x, r) is a feature vector. It is
also the scenario studied in previous work in dual
decomposition (Rush et al, 2010). Under this as-
sumption, and discarding constant terms, the slave
subproblem in Eq. 14 becomes:
max
zs?Zs
?
r?Rs\R?s
?s(r)zs(r)?
?
2
?
r?R?s
(zs(r)? as(r))2.
(17)
where as(r) = ut(r)+??1(?s(r)+?ts(r)). Since Zs
is a polytope, Eq. 17 is a quadratic program, which
can be solved with a general purpose solver. How-
ever, that does not exploit the structure of Zs and is
inefficient when |Rs| is large. We next show that for
many cases, a closed-form solution is available and
6Briefly, we initialize ? = 0.03 and then increase/decrease
? by a factor of 2 whenever the primal residual becomes > 10
times larger/smaller than the dual residual.
242
can be computed inO(|Rs|) time, up to log factors.7
Pairwise Factors. This is the case where RPAIR =
{r1, r2, r12}, where r1 and r2 are basic parts and
r12 is their conjunction, i.e., we have YPAIR =
{?z1, z2, z12? | z12 = z1 ? z2}. This factor is use-
ful to make conjunctions of variables participate in
the score function (see e.g. the grandparent, sibling,
and head bigram parts in Fig. 1). The convex hull
of YPAIR is the polytope ZPAIR = {?z1, z2, z12? ?
[0, 1]3 | z12 ? z1, z12 ? z2, z12 ? z1 + z2 ? 1}, as
shown by Martins et al (2010). In this case, problem
(17) can be written as
max ?12z12 ? ?2 [(z1 ? a1)2 + (z2 ? a2)2]w.r.t. ?z1, z2, z12? ? [0, 1]3
s.t. z12 ? z1, z12 ? z2, z12 ? z1 + z2 ? 1
(18)
and has a closed form solution (see App. B).
Uniqueness Quantification and XOR. Many
problems involve constraining variables to take a
single value: for example, in dependency parsing,
a modifier can only take one head. This can be
expressed as the statement ?!y : Q(y) in first-order
logic,8 or as a one-hot XOR factor in a factor
graph (Smith and Eisner, 2008; Martins et al,
2010). In this case, RXOR = {r1, . . . , rn}, and
YXOR = {?z1, . . . , zn? ? {0, 1}n |
?n
i=1 zi = 1}.
The convex hull of YXOR is ZXOR = {?z1, . . . , zn? ?
[0, 1]n | ?ni=1 zi = 1}. Assume for the sake of
simplicity that all parts in RXOR are basic.9 Up to a
constant, the slave subproblem becomes:
minimize 12
?n
i=1(zi ? ai)2w.r.t. ?z1, . . . , zn? ? [0, 1]n
s.t. ?ni zi = 1.
(19)
This is the problem of projecting onto the probabil-
ity simplex, which can be done in O(n log n) time
via a sort operation (see App. C).10
7This matches the asymptotic time that would be necessary
to solve the corresponding problems in the subgradient method,
for which algorithms are straightforward to derive. The point is
that with ADMM fewer instances of these subproblems need to
be solved, due to faster convergence of the master problem.
8The symbol ?! means ?there is one and only one.?
9A similar derivation can be made otherwise.
10Also common is the need for constraining existence of ?at
most one? element. This can be reduced to uniqueness quantifi-
cation by adding a dummy NULL label.
Existential Quantification and OR. Sometimes,
only existence is required, not necessarily unique-
ness. This can be expressed with disjunctions, ex-
istential quantifiers in first-order logic (?y : Q(y)),
or as a OR factor. In this case, ROR = {r1, . . . , rn},
YOR = {?z1, . . . , zn? ? {0, 1}n |
?n
i=1 zi = 1},
and the convex hull is ZOR = {?z1, . . . , zn? ?
[0, 1]n | ?ni=1 zi ? 1} (see Tab. 1 in Martins et al
(2010)). The slave subproblem becomes:
minimize 12
?n
i=1(zi ? ai)2w.r.t. ?z1, . . . , zn? ? [0, 1]n
s.t. ?ni zi ? 1.
(20)
We derive a procedure in App. D to compute this
projection in O(n log n) runtime, also with a sort.
Negations. The two cases above can be extended
to allow some of their inputs to be negated. By a
change of variables in Eqs. 19?20 it is possible to
reuse the same black box that solves those problems.
The procedure is as follows:
1. For i = 1, . . . , n, set a?i = 1?ai if the ith variable
is negated, and a?i = ai otherwise.
2. Obtain ?z?1, . . . , z?n? as the solution of Eqs. 19 or
20 providing ?a?1, . . . , a?n? as input.
3. For i = 1, . . . , n, set zi = 1?z?i if the ith variable
is negated, and zi = z?i otherwise.
The ability to handle negated variables adds a
great degree of flexibility. From De Morgan?s
laws, we can now handle conjunctions and impli-
cations (since ?ni=1Qi(x) ? R(x) is equivalent to?n
i=1 ?Qi(x) ?R(x)).
Logical Variable Assignments. All previous ex-
amples involve taking a group of existing variables
and defining a constraint. Alternatively, we may
want to define a new variable which is the result of
an operation involving other variables. For exam-
ple, R(x) := ?!y : Q(x, y). This corresponds to the
XOR-WITH-OUTPUT factor in Martins et al (2010).
Interestingly, this can be expressed as a XOR where
R(x) is negated (i.e., either ?R(x) holds or exactly
one y satisfies Q(x, y), but not both).
A more difficult problem is that of the OR-WITH-
OUTPUT factor, expressed by the formula R(x) :=
?y : Q(x, y). We have ROR-OUT = {r0, . . . , rn},
and YOR-OUT = {?z0, . . . , zn? ? {0, 1}n | z0 =
243
# Slaves Runtime Description
Tree ?!h : arc(h,m), m 6= 0 O(n) O(n logn) Each non-root word has a head
flow(h,m, k)? arc(h,m) O(n3) O(1) Only active arcs may carry flow
path(m, d) := ?!h : flow(h,m, d), m 6= 0 O(n2) O(n logn)
path(h, d) := ?!m : flow(h,m, d) O(n2) O(n logn) Paths and flows are consistent
path(0,m) := TRUE, flow(h,m,m) := TRUE (see Martins et al (2010))
All siblings sibl(h,m, s) := arc(h,m) ? arc(h, s) O(n3) O(1) By definition
Grandp. grand(g, h,m) := arc(g, h) ? arc(h,m) O(n3) O(1) By definition
Head Bigram bigram(b, h,m) := arc(b,m? 1) ? arc(h,m), m 6= 0 O(n3) O(1) By definition
Consec. Sibl. lastsibl(h,m,m) := arc(h,m)
?!m ? [h, k] : lastsibl(h,m, k) O(n2) O(n logn) Head automaton model
lastsibl(h,m, k) := lastsibl(h,m, k + 1) (see supplementary material)
? nextsibl(h,m, k + 1) O(n3) O(1)
arc(h,m) := ?!s ? [h,m] : nextsibl(h, s,m) O(n2) O(n logn)
Nonproj. Arc nonproj(h,m) := arc(h,m) ? ?k ? [h,m] : ?path(h, k) O(n2) O(n logn) By definition
Table 1: First-order logic formulae underlying our dependency parser. The basic parts are the predicate variables
arc(h,m) (indicating an arc linking head h to modifier m), path(a, d) (indicating a directed path from ancestor
a to descendant d), nextsibl(h,m, s) (indicating that ?h,m? and ?h, s? are consecutive siblings), nonproj(h,m)
(indicating that ?h,m? is a non-projective arc), as well as the auxiliary variables flow(h,m, d) (indicating that arc
?h,m? carries flow to d), and lastsibl(h,m, k) (indicating that, up to position k, the last seen modifier of h occurred
at position m). The non-basic parts are the pairwise factors sibl(h,m, s), grand(g, h,m), and bigram(b, h,m); as
well as each logical formula. Columns 3?4 indicate the number of parts of each kind, and the time complexity for
solving each subproblem. For a sentence of length n, there are O(n3) parts and the total complexity is O(n3 log n).
?n
i=1 zi}. The convex hull of YOR-OUT is the follow-
ing set: ZOR-OUT = {?z0, . . . , zn? ? [0, 1]n | z0 ??n
i=1 zi, z0 ? zi, ?i = 1, . . . , n} (Martins et al,
2010, Tab.1). The slave subproblem is:
minimize 12
?n
i=0(zi ? ai)2w.r.t. ?z0, . . . , zn? ? [0, 1]n
s.t. z0 ??ni=1 zi; z0 ? zi, ?i = 1, . . . , n.(21)
The problem in Eq. 21 is more involved than the
ones in Eqs. 19?20. Yet, there is still an efficient
procedure with runtime O(n log n) (see App. E).
By using the result above for negated variables, we
are now endowed with a procedure for many other
cases, such that AND-WITH-OUTPUT and formu-
las with universal quantifiers (e.g., R(x) := ?y :
Q(x, y)). Up to a log-factor, the runtimes will be
linear in the number of predicates.
Larger Slaves. The only disadvantage of DD-
ADMM in comparison with the subgradient algo-
rithm is that there is not an obvious way of solving
the subproblem in Eq. 14 exactly for large combi-
natorial factors, such as the TREE constraint in de-
pendency parsing, or a sequence model. Hence, our
method seems to be more suitable for decomposi-
tions which involve ?simple slaves,? even if their
number is large. However, this does not rule out the
possibility of using this method otherwise. Eckstein
and Bertsekas (1992) show that the ADMM algo-
rithm may still converge when the z-updates are in-
exact. Hence the method may still work if the slaves
are solved numerically up to some accuracy. We de-
fer this to future investigation.
5 Experiments: Dependency Parsing
We used 14 datasets with non-projective depen-
dencies from the CoNLL-2006 and CoNLL-2008
shared tasks (Buchholz and Marsi, 2006; Surdeanu
et al, 2008). We also used a projective English
dataset derived from the Penn Treebank by applying
the standard head rules of Yamada and Matsumoto
(2003).11 We did not force the parser to output pro-
jective trees or unique roots for any of the datasets;
everything is learned from the data. We trained by
running 10 iterations of the cost-augmented MIRA
algorithm (Crammer et al, 2006) with LP-relaxed
decoding, as in Martins et al (2009b). Follow-
ing common practice (Charniak and Johnson, 2005;
Carreras et al, 2008), we employed a coarse-to-fine
procedure to prune away unlikely candidate arcs, as
described by Koo and Collins (2010). To ensure
valid parse trees at test time, we rounded fractional
11As usual, we train on sections ?02?21, use ?22 as validation
data, and test on ?23. We ran SVMTool (Gime?nez and Marquez,
2004) to obtain automatic part-of-speech tags for ?22?23.
244
solutions as described in Martins et al (2009a) (yet,
solutions were integral most of the time).
The parts used in our full model are the ones
depicted in Fig. 1. Note that a subgradient-based
method could handle some of those parts efficiently
(arcs, consecutive siblings, grandparents, and head
bigrams) by composing arc-factored models, head
automata, and a sequence labeler. However, no
lightweight decomposition seems possible for incor-
porating parts for all siblings, directed paths, and
non-projective arcs. Tab. 1 shows the first-order
logical formulae that encode the constraints in our
model. Each formula gives rise to a subproblem
which is efficiently solvable (see ?4). By ablating
some of rows of Tab. 1 we recover known methods:
? Resorting to the tree and consecutive sibling for-
mulae gives one of the models in Koo et al
(2010), with the same linear relaxation (a proof
of this fact is included in App. F);
? Resorting to tree, all siblings, grandparent, and
non-projective arcs, recovers a multi-commodity
flow configuration proposed by Martins et al
(2009a); the relaxation is also the same.12
The experimental results are shown in Tab. 2.
For comparison, we include the best published re-
sults for each dataset (at the best of our knowledge),
among transition-based parsers (Nivre et al, 2006;
Huang and Sagae, 2010), graph-based parsers (Mc-
Donald et al, 2006; Koo and Collins, 2010), hybrid
methods (Nivre and McDonald, 2008; Martins et al,
2008), and turbo parsers (Martins et al, 2010; Koo
et al, 2010). Our full model achieved the best re-
ported scores for 7 datasets. The last two columns
show a consistent improvement (with the exceptions
of Chinese and Arabic) when using the full set of
features over a second order model with grandparent
and consecutive siblings, which is our reproduction
of the model of Koo et al (2010).13
12Although Martins et al (2009a) also incorporated consec-
utive siblings in one of their configurations, our constraints are
tighter than theirs. See App. F.
13Note however that the actual results of Koo et al (2010)
are higher than our reproduction, as can be seen in the second
column. The differences are due to the features that were used
and on the way the models were trained. The cause is not search
error: exact decoding with an ILP solver (CPLEX) revealed no
significant difference with respect to our G+CS column. We
leave further analysis for future work.
Best known UAS G+CS Full
Arabic 80.18 [Ma08] 81.12 81.10 (-0.02)
Bulgar. 92.88 [Ma10] 93.04 93.50 (+0.46)
Chinese 91.89 [Ma10] 91.05 90.62 (-0.43)
Czech 88.78 [Ma10] 88.80 89.46 (+0.66)
English 92.57 [Ko10] 92.45 92.68 (+0.23)
Danish 91.78 [Ko10] 91.70 91.86 (+0.16)
Dutch 85.81 [Ko10] 84.77 85.53 (+0.76)
German 91.49 [Ma10] 91.29 91.89 (+0.60)
Japane. 93.42 [Ma10] 93.62 93.72 (+0.10)
Portug. 93.03 [Ko10] 92.05 92.29 (+0.24)
Slovene 86.21 [Ko10] 86.09 86.95 (+0.86)
Spanish 87.04 [Ma10] 85.99 86.74 (+0.75)
Swedish 91.36 [Ko10] 89.94 90.16 (+0.22)
Turkish 77.55 [Ko10] 76.24 76.64 (+0.40)
PTB ?23 93.04 [KC10] 92.19 92.53 (+0.34)
Table 2: Unlabeled attachment scores, excluding punc-
tuation. In the second column, [Ma08] denotes Martins
et al (2008), [KC10] is Koo and Collins (2010), [Ma10]
is Martins et al (2010), and [Ko10] is Koo et al (2010).
In columns 3?4, ?Full? is our full model, and ?G+CS? is
our reproduction of the model of Koo et al (2010), i.e.,
the same as ?Full? but with all features ablated excepted
for grandparents and consecutive siblings.
AF +G+CS +AS +NP Full
PTB ?22 91.02 92.13 92.32 92.36 92.41
PTB ?23 91.36 92.19 92.41 92.50 92.53
Table 3: Feature ablation experiments. AF is an arc-
factored model; +G+CS adds grandparent and consec-
utive siblings; +AS adds all-siblings; +NP adds non-
projective arcs; Full adds the bigram and directed paths.
Feature ablation and error analysis. We con-
ducted a simple ablation study by training several
models on the English PTB with different sets of
features. Tab. 3 shows the results. As expected, per-
formance keeps increasing as we use models with
greater expressive power. We show some concrete
examples in App. G of sentences that the full model
parsed correctly, unlike less expressive models.
Convergence speed and optimality. Fig. 2 com-
pares the performance of DD-ADMM and the sub-
gradient algorithms in the validation section of the
PTB.14 For the second order model, the subgradient
14The learning rate in the subgradient method was set as ?t =
?0/(1+Nincr(t)), as in Koo et al (2010), whereNincr(t) is the
number of dual increases up to the tth iteration, and ?0 is chosen
to maximize dual decrease after 20 iterations (in a per sentence
basis). Those preliminary iterations are not plotted in Fig. 2.
245
method has more slaves than in Koo et al (2010):
it has a slave imposing the TREE constraint (whose
subproblems consists on finding a minimum span-
ning tree) and several for the all-sibling parts, yield-
ing an average number of 310.5 and a maximum
of 4310 slaves. These numbers are still manage-
able, and we observe that a ?good? UAS is achieved
relatively quickly. The ADMM method has many
more slaves due to the multicommodity flow con-
straints (average 1870.8, maximum 65446), yet it
attains optimality sooner, as can be observed in the
right plot. For the full model, the subgradient-based
method becomes extremely slow, and the UAS score
severely degrades (after 1000 iterations it is 2%
less than the one obtained with the ADMM-based
method, with very few instances having been solved
to optimality). The reason is the number of slaves:
in this configuration and dataset the average number
of slaves per instance is 3327.4, and the largest num-
ber is 113207. On the contrary, the ADMM method
keeps a robust performance, with a large fraction of
optimality certificates in early iterations.
Runtime and caching strategies. Despite its suit-
ability to problems with many overlapping compo-
nents, our parser is still 1.6 times slower than Koo
et al (2010) (0.34 against 0.21 sec./sent. in PTB
?23), and is far beyond the speed of transition-based
parsers (e.g., Huang and Sagae (2010) take 0.04
sec./sent. on the same data, although accuracy is
lower, 92.1%). Our implementation, however, is not
fully optimized. We next describe how considerable
speed-ups are achieved by caching the subproblems,
following a strategy similar to Koo et al (2010).
Fig. 3 illustrates the point. After a few iterations,
many variables u(r) see a consensus being achieved
(i.e., ut(r) = zt+1s (r),?s) and enter an idle state:
they are left unchanged by the u-update in Eq. 9,
and so do the Lagrange variables ?t+1s (r) (Eq. 10).
If by iteration t all variables in a subproblem s are
idle, then zt+1s (r) = zts(r), hence the subproblem
does not need to be resolved.15 Fig. 3 shows that
15Even if not all variables are idle in s, caching may still be
useful: note that the z-updates in Eq. 14 tend to be sparse for the
subproblems described in ?4 (these are Euclidean projections
onto polytopes with 0/1 vertices, which tend to hit corners). An-
other trick that may accelerate the algorithm is warm-starting:
since many subproblems involve a sort operation, storing the
sorted indexes may speedup the next round.
200 400 600 800 1000Iterations0
20
40
60
80
100
% ac
tive
Full ADMM% active msgs% active subproblems% active vars
Figure 3: Fraction of active variables, subproblems and
messages along DD-ADMM iterations (full model). The
number of active messages denotes the total number of
variables (active or not) that participate in an active factor.
10-3 10-2 10-1 100 101Time ADMM (sec.)
10-3
10-2
10-1
100
101
Time
 CPLE
X (se
c.)
Elapsed Times
Figure 4: Runtimes of DD-ADMM and CPLEX on PTB
?22 (each point is a sentence). Average runtimes are
0.362 (DD-ADMM) and 0.565 sec./sent. (CPLEX).
many variables and subproblems are left untouched
after the first few rounds.
Finally, Fig. 4 compares the runtimes of our im-
plementation of DD-ADMM with those achieved by
a state-of-the-art LP solver, CPLEX, in its best per-
forming configuration: the simplex algorithm ap-
plied to the dual LP. We observe that DD-ADMM
is faster in some regimes but slower in others. For
short sentences (< 15 words), DD-ADMM tends to
be faster. For longer sentences, CPLEX is quite ef-
fective as it uses good heuristics for the pivot steps
in the simplex algorithm; however, we observed that
it sometimes gets trapped on large problems. Note
also that DD-ADMM is not fully optimized, and that
it is much more amenable to parallelization than the
simplex algorithm, since it is composed of many in-
dependent slaves. This suggests potentially signifi-
cant speed-ups in multi-core environments.
6 Related Work
Riedel and Clarke (2006) first formulated depen-
dency parsing as an integer program, along with
logical constraints. The multicommodity flow for-
246
0 200 400 600 800 1000Iterations85
8687
8889
9091
92
UAS 
(%)
Accuracy
ADMM FullSubgrad FullADMM Sec OrdSubgrad Sec Ord
0 200 400 600 800 1000Iterations0
20
40
60
80
100
Cert
ifica
tes (
%)
Stopping Criteria
ADMM Full (Tol<0.001)ADMM Full (Exact)Subgrad Full (Exact)ADMM Sec Ord (Tol<0.001)ADMM Sec Ord (Exact)Subgrad Sec Ord (Exact)
Figure 2: UAS including punctuation (left) and fraction of optimality certificates (right) accross iterations of the
subgradient and DD-ADMM algorithms, in PTB ?22. ?Full? is our full model; ?Sec Ord? is a second-order model
with grandparents and all siblings, for which the subgradient method uses a coarser decomposition with a TREE factor.
Since subgradient and DD-ADMM are solving the same problems, the solid lines (as the dashed ones) would meet in
the limit, however subgradient converges very slowly for the full model. The right plot shows optimality certificates
for both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of
instances that converged to an accurate solution of P ? (primal and dual residuals < 10?3) and hence can be stopped.
mulation was introduced by Martins et al (2009a),
along with some of the parts considered here. Koo
et al (2010) proposed a subgradient-based dual de-
composition method that elegantly combines head
automata with maximum spanning tree algorithms;
these parsers, as well as the loopy belief propagation
method of Smith and Eisner (2008), are all instances
of turbo parsers (Martins et al, 2010).
DD-ADMM has been proposed and theoretically
analyzed by Martins et al (2011) for problems rep-
resentable as factor graphs. The general ADMM
method has a long-standing history in optimization
(Hestenes, 1969; Powell, 1969; Glowinski and Mar-
roco, 1975; Gabay and Mercier, 1976; Boyd et al,
2011). Other methods have been recently proposed
to accelerate dual decomposition, such as Jojic et al
(2010) and Meshi and Globerson (2011) (the latter
applying ADMM in the dual rather than the primal).
While our paper shows limitations of the sub-
gradient method when there are many overlapping
components, this method may still be advantageous
over ADMM in problems that are nicely decom-
posable, since it often allows reusing existing com-
binatorial machinery. Yet, the scenario we con-
sider here is realistic in NLP, where we often have
to deal with not-lightly-decomposable constrained
problems (e.g., exploiting linguistic knowledge).
7 Conclusion
We have introduced new feature-rich turbo parsers.
Since exact decoding is intractable, we solve an LP
relaxation through a recently proposed consensus al-
gorithm, DD-ADMM, which is suitable for prob-
lems with many overlapping components. We study
the empirical runtime and convergence properties of
DD-ADMM, complementing the theoretical treat-
ment in Martins et al (2011). DD-ADMM com-
pares favourably against the subgradient method in
several aspects: it is faster to reach a consensus, it
has better stopping conditions, and it works better
in non-lightweight decompositions. While its slave
subproblems are more involved, we derived closed-
form solutions for many cases of interest, such as
first-order logic formulas and combinatorial factors.
DD-ADMM may be useful in other frameworks
involving logical constraints, such as the models
for compositional semantics presented by Liang
et al (2011). Non-logical constraints may also
yield efficient subproblems, e.g., the length con-
straints in summarization and compression (Clarke
and Lapata, 2008; Martins and Smith, 2009; Berg-
Kirkpatrick et al, 2011). Finally, DD-ADMM can
be adapted to tighten its relaxations towards exact
decoding, as in Sontag et al (2008) and Rush and
Collins (2011). We defer this for future work.
Acknowledgments
We thank all reviewers for their comments, Eric Xing for
helpful discussions, and Terry Koo and Sasha Rush for
answering questions about their parser and for providing
code. A. M. was supported by a FCT/ICTI grant through
the CMU-Portugal Program, and by Priberam. This
work was partially supported by the FET programme
(EU FP7), under the SIMBAD project (contract 213250).
N. S. was supported by NSF CAREER IIS-1054319.
247
References
M. Auli and A. Lopez. 2011. A Comparison of Loopy
Belief Propagation and Dual Decomposition for Inte-
grated CCG Supertagging and Parsing. In Proc. of
ACL.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On for-
mal properties of simple phrase structure grammars.
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. of ACL.
D. Bertsekas, W. Hager, and O. Mangasarian. 1999.
Nonlinear programming. Athena Scientific.
D.P. Bertsekas, A. Nedic, and A.E. Ozdaglar. 2003. Con-
vex analysis and optimization. Athena Scientific.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.
2011. Distributed Optimization and Statistical Learn-
ing via the Alternating Direction Method of Multipli-
ers. Now Publishers (to appear).
J.P. Boyle and R.L. Dykstra. 1986. A method for find-
ing projections onto the intersections of convex sets in
Hilbert spaces. In Advances in order restricted statis-
tical inference, pages 28?47. Springer Verlag.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In CONLL.
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In CoNLL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
Proc. ACL, pages 173?180. Association for Computa-
tional Linguistics Morristown, NJ, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
computational linguistics, 33(2):201?228.
J. Clarke and M. Lapata. 2008. Global Inference for Sen-
tence Compression An Integer Linear Programming
Approach. JAIR, 31:399?429.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Algo-
rithms. JMLR, 7:551?585.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
2008. Efficient projections onto the L1-ball for learn-
ing in high dimensions. In ICML.
J. Eckstein and D. Bertsekas. 1992. On the Douglas-
Rachford splitting method and the proximal point al-
gorithm for maximal monotone operators. Mathemat-
ical Programming, 55(1):293?318.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proceedings of ACL-08: HLT, pages 959?967.
D. Gabay and B. Mercier. 1976. A dual algorithm for
the solution of nonlinear variational problems via finite
element approximation. Computers and Mathematics
with Applications, 2(1):17?40.
J. Gime?nez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector ma-
chines. In Proc. of LREC.
R. Glowinski and P. Le Tallec. 1989. Augmented La-
grangian and operator-splitting methods in nonlinear
mechanics. Society for Industrial Mathematics.
R. Glowinski and A. Marroco. 1975. Sur
l?approximation, par e?le?ments finis d?ordre un, et la
re?solution, par penalisation-dualite?, d?une classe de
proble`mes de Dirichlet non line?aires. Rev. Franc. Au-
tomat. Inform. Rech. Operat., 9:41?76.
M. Hestenes. 1969. Multiplier and gradient methods.
Jour. Optim. Theory and Applic., 4:302?320.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of ACL,
pages 1077?1086.
R. Johansson and P. Nugues. 2008. Dependency-based
Semantic Role Labeling of PropBank. In EMNLP.
V. Jojic, S. Gould, and D. Koller. 2010. Accelerated dual
decomposition for MAP inference. In ICML.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL, pages 1?11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
A. Kulesza and F. Pereira. 2007. Structured Learning
with Approximate Inference. NIPS.
P. Liang, M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proc.
Association for Computational Linguistics (ACL).
A. F. T. Martins and N. A. Smith. 2009. Summarization
with a joint model for sentence extraction and com-
pression. In NAACL-HLT Workshop on Integer Linear
Programming for NLP.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009a.
Concise integer linear programming formulations for
dependency parsing. In ACL-IJCNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009b.
Polyhedral outer approximations with application to
natural language parsing. In ICML.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate variational infer-
ence. In EMNLP.
248
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011. An Augmented
Lagrangian Approach to Constrained MAP Inference.
In ICML.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In CoNLL.
O. Meshi and A. Globerson. 2011. An Alternating Direc-
tion Method for Dual MAP LP Relaxation. In ECML
PKDD.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
ACL-HLT.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Procs. of CoNLL.
S. Petrov and D. Klein. 2008. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
Proc. of EMNLP.
M. Powell. 1969. A method for nonlinear constraints in
minimization problems. In R. Fletcher, editor, Opti-
mization, pages 283?298. Academic Press.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1):107?136.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In ACL.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming relax-
ations for natural language processing. In EMNLP.
N. Shor. 1985. Minimization methods for non-
differentiable functions. Springer.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and
T Jaakkola. 2008. Tightening LP relaxations for MAP
using message-passing. In UAI.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
R.W. Tromble and J. Eisner. 2006. A fast finite-state
relaxation method for enforcing global constraints on
sequence decoding. In Proc. of NAACL, pages 423?
430.
M. Wainwright and M. Jordan. 2008. Graphical Models,
Exponential Families, and Variational Inference. Now
Publishers.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT.
249
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 474?485,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Quasi-Synchronous Phrase Dependency Grammars
for Machine Translation
Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon Univeristy
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We present a quasi-synchronous dependency
grammar (Smith and Eisner, 2006) for ma-
chine translation in which the leaves of the
tree are phrases rather than words as in pre-
vious work (Gimpel and Smith, 2009). This
formulation allows us to combine structural
components of phrase-based and syntax-based
MT in a single model. We describe a method
of extracting phrase dependencies from paral-
lel text using a target-side dependency parser.
For decoding, we describe a coarse-to-fine ap-
proach based on lattice dependency parsing of
phrase lattices. We demonstrate performance
improvements for Chinese-English and Urdu-
English translation over a phrase-based base-
line. We also investigate the use of unsuper-
vised dependency parsers, reporting encourag-
ing preliminary results.
1 Introduction
Two approaches currently dominate statistical ma-
chine translation (MT) research. Phrase-based mod-
els (Koehn et al, 2003) excel at capturing local
reordering phenomena and memorizing multi-word
translations. Models that employ syntax or syntax-
like representations (Chiang, 2005; Galley et al,
2006; Zollmann and Venugopal, 2006; Huang et al,
2006) handle long-distance reordering better than
phrase-based systems (Auli et al, 2009) but often re-
quire constraints on the formalism or rule extraction
method in order to achieve computational tractabil-
ity. As a result, certain instances of syntactic diver-
gence are more naturally handled by phrase-based
systems (DeNeefe et al, 2007).
In this paper we present a new way of combin-
ing the advantages of phrase-based and syntax-based
MT. We propose a model in which phrases are orga-
nized into a tree structure inspired by dependency
syntax. Instead of standard dependency trees in
which words are vertices, our trees have phrases as
vertices. We describe a simple heuristic to extract
phrase dependencies from an aligned parallel cor-
pus parsed on the target side, and use them to com-
pute target-side tree features. We define additional
string-to-tree features and, if a source-side depen-
dency parser is available, tree-to-tree features to cap-
ture properties of how phrase dependencies interact
with reordering.
To leverage standard phrase-based features along-
side our novel features, we require a formalism
that supports flexible feature combination and effi-
cient decoding. Quasi-synchronous grammar (QG)
provides this backbone (Smith and Eisner, 2006);
we describe a coarse-to-fine approach for decod-
ing within this framework, advancing substantially
over earlier QG machine translation systems (Gim-
pel and Smith, 2009). The decoder involves generat-
ing a phrase lattice (Ueffing et al, 2002) in a coarse
pass using a phrase-based model, followed by lat-
tice dependency parsing of the phrase lattice. This
approach allows us to feasibly explore the combined
search space of segmentations, phrase alignments,
and target phrase dependency trees.
Our experiments demonstrate an average im-
provement of +0.65 BLEU in Chinese-English
translation across three test sets and an improvement
of +0.75 BLEU in Urdu-English translation over
a phrase-based baseline. We also describe experi-
ments in which we replace supervised dependency
parsers with unsupervised parsers, reporting promis-
ing results: using a supervised Chinese parser and
a state-of-the-art unsupervised English parser pro-
vides our best results, giving an averaged gain of
+0.79 BLEU over the baseline. We also discuss how
our model improves translation quality and discuss
future possibilities for combining approaches to ma-
474
chine translation using our framework.
2 Related Work
We previously applied quasi-synchronous grammar
to machine translation (Gimpel and Smith, 2009),
but that system performed translation fundamentally
at the word level. Here we generalize that model to
function on phrases, enabling a tighter coupling be-
tween the phrase segmentation and syntactic struc-
tures. We also present a decoder efficient enough to
scale to large data sets and present performance im-
provements in large-scale experiments over a state-
of-the-art phrase-based baseline.
Aside from QG, there have been many efforts
to use dependency syntax in machine translation.
Quirk et al (2005) used a source-side dependency
parser and projected automatic parses across word
alignments in order to model dependency syntax on
phrase pairs. Shen et al (2008) presented an exten-
sion to Hiero (Chiang, 2005) in which rules have
target-side dependency syntax and therefore enable
the use of a dependency language model.
More recently, researchers have sought the bene-
fits of dependency syntax while preserving the ad-
vantages of phrase-based models, such as efficiency
and coverage. Galley and Manning (2009) loos-
ened standard assumptions about dependency pars-
ing so that the efficient left-to-right decoding pro-
cedure of phrase-based translation could be retained
while a dependency language model is incorporated.
Carreras and Collins (2009) presented a string-to-
dependency system that permits non-projective de-
pendency trees (thereby allowing a larger space of
translations) and use a rule extraction procedure that
includes rules for every phrase in the phrase table.
We take an additional step in this direction by
working with dependency grammars on the phrases
themselves, thereby bringing together the structural
components of phrase-based and dependency-based
MT in a single model. While others have worked
on combining rules from multiple syntax-based sys-
tems (Liu et al, 2009) or using posteriors from mul-
tiple models to score translations (DeNero et al,
2010), we are not aware of any other work that seeks
to directly integrate phrase-based and syntax-based
machine translation at the modeling level.1
1Dymetman and Cancedda (2010) present a formal analy-
3 Model
Given a sentence s and its dependency tree ?s,
we formulate the translation problem as finding the
target sentence t?, the segmentation ?? of s into
phrases, the segmentation ?? of t? into phrases, the
dependency tree ??? on the target phrases ??, and the
one-to-one phrase alignment a? such that
?t?,??,??, ???,a??= argmax
?t,?,?,??,a?
p(t,?,?, ??,a |s, ?s)
We use a linear model (Och and Ney, 2002):
p(t,?,?, ??,a | s, ?s) ?
exp{?>g(s, ?s, t,?,?, ??,a)}
where g is a vector of arbitrary feature functions on
the full set of structures and ? holds corresponding
feature weights. Table 1 summarizes our notation.
In modeling p(t,?,?, ??,a | s, ?s), we make
use of quasi-synchronous grammar (QG; Smith
and Eisner, 2006). Given a source sentence and
its parse, a QG induces a probabilistic monolingual
grammar over sentences ?inspired? by the source
sentence and tree. We denote this grammar byGs,?s ;
its (weighted) language is the set of translations of s.
Quasi-synchronous grammar makes no restric-
tions on the form of the target monolingual gram-
mar, though dependency grammars have been used
in most previous applications of QG (Wang et al,
2007; Das and Smith, 2009; Smith and Eisner,
2009), including previous work in MT (Smith and
Eisner, 2006; Gimpel and Smith, 2009). We pre-
viously presented a word-based machine translation
model based on a quasi-synchronous dependency
grammar. However, it is well-known in the MT com-
munity that translation quality is improved when
larger units are modeled. Therefore, we use a de-
pendency grammar in which the leaves are phrases
rather than words.
We define a phrase dependency grammar as a
model p(?, ??|t) over the joint space of segmen-
tations of a sentence into phrases and dependency
trees on the phrases.2 Phrase dependency grammars
sis of the problem of intersecting phrase-based and hierarchical
translation models, but do not provide experimental results.
2We restrict our attention to projective trees in this paper,
but the generalization to non-projective trees is easily made.
475
s = ?s1, . . . , sn? source language sentence
t = ?t1, . . . , tm? target language sentence, translation of s
? = ??1, . . . , ?n?? segmentation of s into phrases
?i, ?i = ?sj , . . . , sk? s.t. ?1 ? . . . ? ?n? = s
? = ??1, . . . , ?m?? segmentation of t into phrases
?i, ?i = ?tj , . . . , tk? s.t. ?1 ? . . . ? ?m? = t
?s : {1, . . . , n} ? {0, . . . , n} dependency tree on source words s, where ?s(i) is the index of
the parent of word si (0 is the root, $)
?? : {1, . . . ,m?} ? {0, . . . ,m?} dependency tree on target phrases ?, where ??(i) is the index of
the parent of phrase ?i
a : {1, . . . ,m?} ? {1, . . . , n?} one-to-one alignment from phrases in ? to phrases in ?
? = ??,?? parameters of the full model (? = phrase-based, ? = QPDG)
Table 1: Key notation.
have recently been used by Wu et al (2009) for fea-
ture extraction for opinion mining. When used for
translation modeling, they allow us to capture phe-
nomena like local reordering and idiomatic transla-
tions within each phrase as well as long-distance re-
lationships among the phrases in a sentence.
We then define a quasi-synchronous phrase
dependency grammar (QPDG) as a conditional
model p(t,?,?, ??,a | s, ?s) that induces a prob-
abilistic monolingual phrase dependency grammar
over sentences inspired by the source sentence and
(lexical) dependency tree. The source and tar-
get sentences are segmented into phrases and the
phrases are aligned in a one-to-one alignment.
We note that we actually depart here slightly from
the original definition of QG. The alignment variable
in QG links target tree nodes to source tree nodes.
However, we never commit to a source phrase de-
pendency tree, instead using a source lexical depen-
dency tree output by a dependency parser, so our
alignment variable a is a function from target tree
nodes (phrases in ?) to source phrases in ?, which
might not be source tree nodes. The features in our
model may consider a large number of source phrase
dependency trees as long as they are consistent with
?s.
4 Features
Our model contains all of the standard phrase-based
features found in systems like Moses (Koehn et al,
2007), including four phrase table probability fea-
tures, a phrase penalty feature, an n-gram language
model, a distortion cost, six lexicalized reordering
features, and a word penalty feature.
We now describe in detail the additional features
$? said : $? we should
$? said that $? has been
$? is a - us? relations
$? will be $? he said
$? it is cross - strait? relations
$? this is $? pointed out that
$? we must , and? is
the? united states the chinese? government
the? development of $? is the
the two? countries $? said ,
he? said : one - china? principle
$? he said : sino - us? relations
Table 2: Most frequent phrase dependencies with at least
2 words in one of the phrases (dependencies in which one
phrase is entirely punctuation are not shown). $ indicates
the root of the tree.
in our model that are used to score phrase depen-
dency trees. We shall refer to these as QPDG
features and will find it useful later to notation-
ally distinguish their feature weights from those of
the phrase-based model. We use ? for weights of
the standard phrase-based model features and ? for
weights of the QPDG features. We include three cat-
egories of features, differentiated by what pieces of
structure they consider.
4.1 Target Tree Features
We first include features that only consider t, ?,
and ??. These features can be categorized as ?syn-
tactic language model? features (Shen et al, 2008;
Galley and Manning, 2009), though unlike previous
work our features model both the phrase segmenta-
tion and dependency structure. Typically, these sorts
of features are probabilities estimated from a corpus
parsed using a supervised parser. However, there do
not currently exist treebanks with annotated phrase
476
,? made up 0.057
he? made up 0.021
supreme court? made up 0.014
court? made up 0.014
in september 2000? made up 0.014
in september 2000 ,? made up 0.014
made up? of 0.065
made up? . 0.029
made up? , 0.016
made up? mind to 0.01
Table 3: Most probable child phrases for the parent
phrase ?made up? for each direction, sorted by the con-
ditional probability of the child phrase given the parent
phrase and direction.
dependency trees.
Our solution is to use a standard supervised de-
pendency parser and extract phrase dependencies us-
ing bilingual information.3 We begin by obtaining
symmetrized word alignments and extracting phrase
pairs using the standard heuristic from phrase-based
MT (Koehn et al, 2003). Given the set of extracted
phrase pairs for a sentence, denote by W the set of
unique target-side phrases among them. We parse
the target sentence with a dependency parser and, for
each pair of phrases u, v ? W , we extract a phrase
dependency (along with its direction) if u and v do
not overlap and there is at least one lexical depen-
dency between a word in u and a word in v. If there
are lexical dependencies in both directions, we ex-
tract a phrase dependency only for the single longest
one. Since we use a projective dependency parser,
the longest lexical dependency between two phrases
is guaranteed to be unique. Table 2 shows a listing
of the most frequent phrase dependencies extracted
(lexical dependencies are omitted).
We note that during training we never explicitly
commit to any single phrase dependency tree for a
target sentence. Rather, we extract phrase depen-
dencies from all phrase dependency trees consis-
tent with the word alignments and the lexical de-
pendency tree. Thus we treat phrase dependency
trees analogously to phrase segmentations in stan-
dard phrase extraction.
We perform this procedure on all sentence pairs
in the parallel corpus. Given a set of extracted
3For a monolingual task, Wu et al (2009) used a shal-
low parser to convert lexical dependencies from a dependency
parser into phrase dependencies.
phrase dependencies of the form ?u, v, d?, where
u is the head phrase, v is the child phrase, and
d ? {left , right} is the direction, we then estimate
conditional probabilities p(v|u, d) using relative fre-
quency estimation. Table 3 shows the most probable
child phrases for an example parent phrase. To com-
bat data sparseness, we perform the same procedure
with each word replaced by its word cluster ID ob-
tained from Brown clustering (Brown et al, 1992).
We include a feature in the model for the sum of
the scaled log-probabilities of each attachment:
m??
i=1
max
(
0, C + log p(?i|???(i), d(i)
)
(1)
where d(i) = I[??(i)? i > 0] is the direction of the
dependency arc.
Although we use log-probabilities in this feature
function, we first add a constant C to each to ensure
they are all positive.4 The max expression protects
unseen parent-child phrase dependencies from caus-
ing the score to be negative infinity. Our motivation
is a desire for the features to be used to prefer one
derivation over another but not to rule out a deriva-
tion completely if it merely happens to contain a de-
pendency unobserved in the training data.
We also include lexical weighting features simi-
lar to those used in phrase-based MT (Koehn et al,
2003). Whenever we extract a phrase dependency,
we extract the longest lexical dependency contained
within it. For all ?parent, child, direction? lexi-
cal dependency tuples ?x, y, d?, we estimate condi-
tional probabilities plex (y|x, d) from the parsed cor-
pus using relative frequency estimation. Then, for a
phrase dependency with longest lexical dependency
?x, y, d?, we add a feature for plex (y|x, d) to the
model, using a formula similar to Eq. 1. Different
instances of a phrase dependency may have different
lexical dependencies extracted with them. We add
the lexical weight for the most frequent, breaking
ties by choosing the lexical dependency that maxi-
mizes p(y|x, d), as was also done by Koehn et al
(2003).
In all, we include 4 target tree features: one for
phrase dependencies, one for lexical dependencies,
4The reasoning here is that whenever we use a phrase de-
pendency that we have observed in the training data, we want to
boost the score of the translation. If we used log-probabilities,
each observed dependency would incur a penalty.
477
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can you deliver it by tomorrow morning ?
can     you     deliver  it    by     tomorrow morning ?
CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
...
es:it
morgen:tomorrow
liefern:deliver
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
...
fr?h:morning
morgen:tomorrow
morgen:morning
liefern:deliver
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
x y      z
a    b   c d e
x y      z
a    b   c d ex y      z
a    b   c d e
x y      z
a    b   c d e
Figure 1: String-to-tree configurations; each is associated
with a feature that counts its occurrences in a derivation.
and the same features computed from a transformed
version of the corpus in which each word is replaced
by its Brown cluster.
4.2 String-to-Tree Configurations
We consider features that count instances of reorder-
ing configurations involving phrase dependencies.
In addition to the target-side structures, these fea-
tures consider ? and a, though not s or ?s. For ex-
ample, when building a parent-child phrase depen-
dency with the child to the left, one feature value is
incremented if their aligned source-side phrases are
in the same order. This configuration is the leftmost
in Fig. 1; we include features for the other three con-
figurations there as well, for a total of 4 features in
this category.
4.3 Tree-to-Tree Configurations
We include features that consider s, ?, and ?s in ad-
dition to t, ?, and ??. We begin with features for
each of the quasi-synchronous configurations from
Smith and Eisner (2006), adapted to phrase depen-
dency grammars. That is, for a parent-child pair
???(i), i? in ??, we consider the relationship be-
tween a(??(i)) and a(i), the source-side phrases
to which ??(i) and i align. We use the follow-
ing named configurations from Smith and Eisner:
root-root, parent-child, child-parent, grandparent-
grandchild, sibling, and c-command.5 We define a
feature to count instances of each of these configu-
rations, including an additional feature for ?other?
configurations that do not fit into these categories.6
When using a QPDG, there are multiple ways
to compute tree-to-tree configuration features, since
5See Fig. 3 in Smith and Eisner (2006) for illustrations.
6We actually include two versions of each configuration fea-
ture other than ?root-root?: one for the source phrases being in
the same order as the target phrases and one for them being
swapped.
Input: sentence s, dependency parse ?s, coarse
parameters ?M , fine parameters ??,??
Output: translation t
LMERT ? GenerateLattices (s, ?M );
LFB ? FBPrune (LMERT, ?M );
?t,?,?, ??,a? ? QGDEPPARSE(LFB, ??,??);
return t;
Algorithm 1: CoarseToFineDecode
we use a phrase dependency tree for the target side,
a lexical dependency tree for the source side, and
a phrase alignment. We use the following heuristic
approach. Given a pair of source words, one with
index j in source phrase a(??(i)) and the other with
index k in source phrase a(i), we have a parent-
child configuration if ?s(k) = j; if ?s(j) = k, a
child-parent configuration is present. In order for the
grandparent-grandchild configuration to be present,
the intervening parent word must be outside both
phrases. For sibling and other c-command config-
urations, the shared parent or ancestor must also be
outside both phrases.
After obtaining a list of all configurations present
for each pair of words ?j, k?, we fire the feature for
the single configuration corresponding to the max-
imum distance |j ? k|. If no configurations are
present between any pair of words, the ?other? fea-
ture fires. Therefore, only one configuration feature
fires for each phrase dependency attachment.
Finally, we include features that consider the
dependency path distance between phrases in the
source-side dependency tree that are aligned to
parent-child pairs in ??. We include a feature that
sums, for each target phrase i, the inverse of the
minimum undirected path length between each word
in a(i) and each word in ??(a(i)). The minimum
undirected path length is defined as the number of
dependency arcs that must be crossed to travel from
one word to the other in ?s. We use one feature
for undirected path length and one other for directed
path length. If there is no (un)directed path from a
word in a(i) to a word in ??(a(i)), we use? as the
minimum length.
There are 15 features in this category, for a total
of 23 QPDG features.
478
5 Decoding
For a QPDG model, decoding consists of finding
the highest-scoring tuple ?t,?,?, ??,a? for an in-
put sentence s and its parse ?s, i.e., finding the most
probable derivation under the s/?s-specific grammar
Gs,?s . We follow Gimpel and Smith (2009) in con-
structing a lattice to representGs,?s and using lattice
parsing to search for the best derivation, but we con-
struct the lattice differently and employ a coarse-to-
fine strategy (Petrov, 2009) to speed up decoding.
It has become common in recent years for MT re-
searchers to exploit efficient data structures for en-
coding concise representations of the pruned search
space of the model, such as phrase lattices for
phrase-based MT (Ueffing et al, 2002; Macherey
et al, 2008; Tromble et al, 2008). Each edge in
a phrase lattice corresponds to a phrase pair and
each path through the lattice corresponds to a tuple
?t,?,?,a? for the input s. Decoding for a phrase
lattice consists of finding the highest-scoring path,
which is done using dynamic programming. To also
maximize over ??, we perform lattice dependency
parsing, which allows us to search over the space of
tuples ?t,?,?,a, ???. Given the lattice and Gs,?s ,
lattice parsing is a straightforward generalization of
the standard arc-factored dynamic programming al-
gorithm from Eisner (1996).
The lattice parsing algorithm requires O(E2V )
time and O(E2 + V E) space, where E is the num-
ber of edges in the lattice and V is the number of
nodes.7 Typical phrase lattices might easily contain
tens of thousands of nodes and edges, making exact
search prohibitively expensive for all but the small-
est lattices. So, we use approximate search based on
coarse-to-fine decoding. We now discuss each step
of this procedure; an outline is shown as Alg. 1.
Pass 1: Lattice Pruning After generating phrase
lattices using a phrase-based MT system, we prune
lattice edges using forward-backward pruning (Six-
tus and Ortmanns, 1999), which has also been used
in previous work using phrase lattices (Tromble et
al., 2008). This pruning method computes the max-
marginal for each lattice edge, which is the score of
the best full path that uses that edge. Max-marginals
7To prevent confusion, we use the term edge to refer to a
phrase lattice edge and arc to refer to a parent-child dependency
in the phrase dependency tree.
offer the advantage that the best path in the lattice is
preserved during pruning. For each lattice, we use
a grid search to find the most liberal threshold that
leaves fewer than 1000 edges in the resulting lattice.
As complexity is quadratic in E, forcing E to be
less than 1000 improves runtime substantially. Af-
ter pruning, the lattices still contain more than 1016
paths on average and oracle BLEU scores are typi-
cally 12-15 points higher than the model-best paths.
Pass 2: Parent Ranking Given a pruned lattice,
we then remove some candidate dependency arcs
from consideration. It is common in dependency
parsing to use a coarse model to rank the top k par-
ents for each word, and to only consider these during
parsing (Martins et al, 2009; Bergsma and Cherry,
2010). Unlike string parsing, our phrase lattices im-
pose several types of constraints on allowable arcs.
For example, each node in the phrase lattice is an-
notated with a coverage vector?a bit vector indicat-
ing which words in the source sentence have been
translated?which implies a topological ordering of
the nodes. To handle constraints like these, we first
use the Floyd-Warshall algorithm (Floyd, 1962) to
find the best score between every pair of nodes in
the lattice. This algorithm also tells us whether each
edge is reachable from each other edge, allowing
us to immediately prune dependency arcs between
edges that are unreachable from each other.
After eliminating impossible arcs, we turn to
pruning away unlikely ones. In standard (string) de-
pendency parsing, every word is assigned a parent.
In lattice parsing, however, most lattice edges will
not be assigned any parent. Certain lattice edges are
much more likely to be contained within paths, so
we allow some edges to have more candidate parent
edges than others. We introduce hyperparameters
?, ?, and ? to denote, respectively, the minimum,
maximum, and average number of parent edges to
be considered for each lattice edge (? ? ? ? ?).
We rank the full set of E2 arcs according to their
scores (using the QPDG features and their weights
?) and choose the top ?E of these arcs while en-
suring that each edge has at least ? and at most ?
potential parent edges.
This step reduces the time complexity from
O(E2V ) to O(?EV ), where ? < E. In our ex-
periments, we set ? = 300, ? = 100, and ? = 400.
479
Input: tuning set D = ?S, T ?, initial weights ?0 for
coarse model, initial weights ?0 for
additional features in fine model
Output: coarse model learned weights: ?M , fine
model learned weights: ???,???
?M ? MERT (S, T , ?0, 100, MOSES);
LMERT ? GenerateLattices (S, ?M );
LFB ? FBPrune (LMERT, ?M );
???,??? ?
MERT (LFB, T , ??M ,?0?, 200, QGDEPPARSE);return ?M , ???,???;
Algorithm 2: CoarseToFineTrain
Pass 3: Lattice Dependency Parsing After com-
pleting the coarse passes, we parse using bottom-up
dynamic programming based on the agenda algo-
rithm (Nederhof, 2003; Eisner et al, 2005). We only
consider arcs that survived the filtering in Pass 2.
We weight agenda items by the sum of their scores
and the Floyd-Warshall best path scores both from
the start node of the lattice to the beginning of the
item and the end of the item to any final node. This
heuristic helps us to favor exploration of items that
are highly likely under the phrase-based model.
If the score of the partial structure can only get
worse when combining it with other structures (e.g.,
in a PCFG), then the first time that we pop an item
of type GOAL from the agenda, we are guaranteed
to have the best parse. However, in our model, some
features are positive and others negative, making this
property no longer hold; as a result, GOAL items
may be popped out of order from the agenda. There-
fore, we use an approximation, simply popping G
GOAL items from the agenda and then stopping. The
items are sorted by their scores and the best is re-
turned by the decoder (or the k best in the case of
MERT). In our experiments, we set G = 4000.
The combined strategy yields average decoding
times in the range of 30 seconds per sentence, which
is comparable to other syntax-based MT systems.
6 Training
For tuning the coarse and fine parameters, we use
minimum error rate training (MERT; Och, 2003) in
a procedure shown as Alg. 2. We first use MERT to
train parameters for the coarse phrase-based model
used to generate phrase lattices. Then, after gener-
ating the lattices, we prune them and run MERT a
second time to tune parameters of the fine model,
which includes all phrase-based and QPDG param-
eters. The arguments to MERT are a vector of source
sentences (or lattices), a vector of target sentences,
the initial parameter values, the size of the k-best
list, and finally the decoder. We initialize ? to the
default Moses feature weights and for ? we ini-
tialize the two target phrase dependency weights to
0.004, the two lexical dependency weights to 0.001,
and the weights for all configuration features to 0.0.
Our training procedure requires two executions of
MERT, and the second typically takes more itera-
tions to converge (10 to 20 is typical) than the first
due to the use of a larger feature set and increased
possibility for search error due to the enlarged search
space.
7 Experiments
For experimental evaluation, we consider Chinese-
to-English (ZH-EN) and Urdu-to-English (UR-
EN) translation and compare our system to
Moses (Koehn et al, 2007). For ZH-EN, we
used 303k sentence pairs from the FBIS corpus
(LDC2003E14). We segmented the Chinese data
using the Stanford Chinese segmenter in ?CTB?
mode (Chang et al, 2008), giving us 7.9M Chinese
words and 9.4M English words. For UR-EN, we
used parallel data from the NIST MT08 evaluation
consisting of 1.2M Urdu words and 1.1M English
words.
We trained a baseline Moses system using de-
fault settings and features. Word alignment was
performed using GIZA++ (Och and Ney, 2003) in
both directions and the grow-diag-final-and
heuristic was used to symmetrize the alignments.
We used a max phrase length of 7 when extracting
phrases. Trigram language models were estimated
using the SRI language modeling toolkit (Stolcke,
2002) with modified Kneser-Ney smoothing (Chen
and Goodman, 1998). To estimate language models
for each language pair, we used the English side of
the parallel corpus concatenated with 200M words
of randomly-selected sentences from the Gigaword
v4 corpus (excluding the NY Times and LA Times).
We used this baseline Moses system to gener-
ate phrase lattices for our system, so our model in-
cludes all of the Moses features in addition to the
480
MT03 (tune) MT02 MT05 MT06 Average
Moses 33.84 33.35 31.81 28.82 31.33
QPDG (TT) 34.63 (+0.79) 34.10 (+0.75) 32.15 (+0.34) 29.33 (+0.51) 31.86 (+0.53)
QPDG (TT+S2T+T2T) 34.98 (+1.14) 34.26 (+0.91) 32.34 (+0.53) 29.35 (+0.53) 31.98 (+0.65)
Table 4: Chinese-English Results (% BLEU).
QPDG features described in ?4. In our experiments,
we compare our QPDG system (lattice parsing on
each lattice) to the Moses baseline (finding the best
path through each lattice). The conventional wis-
dom holds that hierarchical phrase-based transla-
tion (Chiang, 2005) performs better than phrase-
based translation for language pairs that require
large amounts of reordering, such as ZH-EN and
UR-EN. However, researchers have shown that this
performance gap diminishes when using a larger dis-
tortion limit (Zollmann et al, 2008) and may dis-
appear entirely when using a lexicalized reordering
model (Lopez, 2008; Galley and Manning, 2010).
So, we increase the Moses distortion limit from 6
(the default) to 10 and use Moses? default lexical-
ized reordering model (Koehn et al, 2005).
We parsed the Chinese text using the Stanford
parser (Levy and Manning, 2003) and the English
text using TurboParser (Martins et al, 2009). We
note that computing our features requires parsing the
target (English) side of the parallel text, but not the
source side. We only need to parse the source side
of the tuning and test sets, and the only features that
look at the source-side parse are those from ?4.3.
To obtain Brown clusters for the target tree fea-
tures in ?4.1, we used code from Liang (2005).8
We induced 100 clusters from the English side of
the parallel corpus concatenated with 10M words of
randomly-selected Gigaword sentences. Only words
that appeared at least twice in this data were con-
sidered during clustering. An additional cluster was
created for all other words; this allowed us to use
phrase dependency cluster features even for out-of-
vocabulary words. We used a max phrase length of
7 when extracting phrase dependencies to match the
max phrase length used in phrase extraction. Ap-
proximately 87M unique phrase dependencies were
extracted from the ZH-EN data and 7M from the
UR-EN data.
We tuned the weights of our model using the pro-
8http://www.cs.berkeley.edu/?pliang/
software
Dev (tune) MT09
Moses 24.21 23.56
QPDG (TT+S2T) 24.94 (+0.73) 24.31 (+0.75)
Table 5: Urdu-English Results (% BLEU).
cedure described in ?6. For ZH-EN we used MT03
for tuning and MT02, MT05, and MT06 for test-
ing. For UR-EN we used half of the documents (882
sentence pairs) from the MT08 test set for tuning
(?Dev?) and MT09 for testing. We evaluated trans-
lation output using case-insensitive IBM BLEU (Pa-
pineni et al, 2001).
7.1 Results
Results for ZH-EN and UR-EN translation are
shown in Tables 4 and 5. We show results when us-
ing only the target tree features from ?4.1 (TT), as
well as when adding the string-to-tree features from
?4.2 (S2T) and the tree-to-tree features from ?4.3
(T2T). We note that T2T features are unavailable for
UR-EN because we do not have an Urdu parser. We
find that we can achieve moderate but consistent im-
provements over the baseline Moses system, for an
average increase of 0.65 BLEU points for ZH-EN
and 0.75 for UR-EN.
Fig. 2 shows an example sentence from the MT05
test set alng with its translation output and deriva-
tions produced by Moses and our QPDG system
with the full feature set. This example shows the
kind of improvements that our system makes. In
Chinese, modifiers such as prepositional phrases and
clauses are generally placed in front of the words
they modify, frequently the opposite of English. In
addition, Chinese occasionally uses postpositions
where English uses prepositions. The Chinese sen-
tence in Fig. 2 exhibits both of these, as the preposi-
tional phrase ?after the Palestinian election? appears
before the verb ?strengthen? in the Chinese sen-
tence and ?after? appears as a postposition. Moses
(Fig. 2(a)) does not properly reorder the preposi-
tional phrase, while our system (Fig. 2(b)) properly
handles both reorderings.9 We shall discuss these
9Our system?s derivation is not perfect, in that ?in? is incor-
481
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can you deliver it by tomorrow morning ?
can     you     deliver  it    by     tomorrow morning ?
CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
...
es:it
morgen:tomorrow
liefern:deliver
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
k?nnen:can
k?nnen:may
sie:you
es:it
...
vorbei:by
... ... ...
...
...
k?nnen:can
liefern:deliver
sie:you
sie:it
es:it k?nnen:can
k?nnen:canliefern:deliver
sie:you
es:it
vorbei:by
morgen:tomorrow
morgen:tomorrow
liefern:deliver
es:it
vorbei:by
fr?h:morning
fr?h:early
?:?
morgen:morning
...
fr?h:morning
morgen:tomorrow
morgen:morning
liefern:deliver
vorbei:by
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
$   k?nnen   sie   es  vorbei    leifern    morgen  fr?h  ?
can     you     deliver  it    by     tomorrow morning ?
bush   :   palestinian   presidential election   in   the  united states will   strengthen the peace  efforts
palestine elections strengthen peace effortsafterbush will in:
united
states
bush   : the united states will   strengthen   the   peace   efforts   after   the palestinian   election$
$
bush : us set to boost peace efforts after palestinian election
bush : us to step up peace efforts after palestinian elections
bush : u.s. will enhance peace efforts after palestinian election
us to boost peace efforts after palestinian elections : bush
References
(a)
(b)
(c)
??    :    ?   ?   ?   ????   ?        ?     ??   ??   ??
??    :  ?  ?   ?   ????   ?        ?   ??   ??   ??
Figure 2: (a) Moses translation output along with ?, ?, and a. An English gloss is shown above the Chinese sentence
and above the gloss is shown the dependency parse from the Stanford parser. (b) QPDG system output with additional
structure ??. (c) reference translations.
types of improvements further in ?8.
7.2 Unsupervised Parsing
Our results thus far use supervised parsers for both
Chinese and English, but parsers are only available
for a small fraction of the languages we would like
to translate. Fortunately, unsupervised dependency
grammar induction has improved substantially in re-
cent years due to a flurry of recent research. While
attachment accuracies on standard treebank test sets
are still relatively low, it may be the case that even
though unsupervised parsers do not match treebank
annotations very well, they may perform well when
used for extrinsic applications. We believe that
syntax-based MT offers a compelling platform for
development and extrinsic evaluation of unsuper-
vised parsers.
In this paper, we use the standard dependency
model with valence (DMV; Klein and Manning,
2004). When training is initialized using the out-
put of a simpler, concave dependency model, the
rectly translated and reordered, but the system was nonetheless
able to use it to improve the fluency of the output.
DMV can approach state-of-the-art unsupervised ac-
curacy (Gimpel and Smith, 2011). For English, the
resulting parser achieves 53.1% attachment accu-
racy on Section 23 of the Penn Treebank (Marcus et
al., 1993), which approaches the 55.7% accuracy of
a recent state-of-the-art unsupervised model (Blun-
som and Cohn, 2010). The Chinese parser, ini-
tialized and trained the same way, achieves 44.4%,
which is the highest reported accuracy on the Chi-
nese Treebank (Xue et al, 2004) test set.
Most unsupervised grammar induction models
assume gold standard POS tags and sentences
stripped of punctuation. We use the Stanford tag-
ger (Toutanova et al, 2003) to obtain tags for both
English and Chinese, parse the sentences without
punctuation using the DMV, and then attach punc-
tuation tokens to the root word of the tree in a post-
processing step. For English, the predicted parents
agreed with those of TurboParser for 48.7% of the
tokens in the corpus.
We considered all four scenarios: supervised and
unsupervised English parsing paired with supervised
and unsupervised Chinese parsing. Table 6 shows
482
EN
unsupervised supervised
ZH unsupervised 31.18 (33.76) 31.86 (34.78)supervised 32.12 (34.74) 31.98 (34.98)
Moses 31.33 (33.84)
Table 6: Results when using unsupervised dependency
parsers. Cells contain averaged % BLEU on the three test
sets and % BLEU on tuning data (MT03) in parentheses.
Feature Initial Learned
Left child, same order 9.0 8.9
Left child, swap phrases 1.1 0.0
Right child, same order 7.3 7.3
Right child, swap phrases 1.6 2.3
Root-root 0.4 0.8
Parent-child 4.2 6.1
Child-parent 1.2 0.4
Grandparent-grandchild 1.0 0.2
Sibling 2.4 1.9
C-command 6.1 6.7
Other 1.5 0.9
Table 7: Average feature values across best translations
of sentences in the MT03 tuning set, both before MERT
(column 2) and after (column 3). ?Same? versions of tree-
to-tree configuration features are shown; the rarer ?swap?
features showed a similar trend.
BLEU scores averaged over the three test sets with
tuning data BLEU in parentheses. Surprisingly, we
achieve our best results when using the unsupervised
English parser in place of the supervised one (+0.79
over Moses), while keeping the Chinese parser su-
pervised. Competitive performance is also found
by using the unsupervised Chinese parser and super-
vised English parser (+0.53 over Moses).
However, when using unsupervised parsers for
both languages, performance was below that of
Moses. During tuning for this configuration, we
found that MERT struggled to find good parameter
estimates, typically converging to suboptimal solu-
tions after a small number of iterations. We believe
this is due to the large number of features (37), the
noise in the parse trees, and known instabilities of
MERT. In future work we plan to experiment with
training algorithms that are more stable and that can
handle larger numbers of features.
8 Analysis
To understand what our model learns during MER
training, we computed the feature vectors of the best
derivation for each sentence in the tuning data at
both the start and end of tuning. Table 7 shows
these feature values averaged across all tuning sen-
tences. The first four features are the configurations
from Fig. 1, in order from left to right. From these
rows, we can observe that the model learns to en-
courage swapping when generating right children
and penalize swapping for left children. In addi-
tion to objects, right children in English are often
prepositional phrases, relative clauses, or other mod-
ifiers; as we noted above, Chinese generally places
these modifiers before their heads, requiring reorder-
ing during translation. Here the model appears to be
learning this reordering behavior.
From the second set of features, we see that the
model learns to favor producing dependency trees
that are mostly isomorphic to the source tree, by fa-
voring root-root and parent-child configurations at
the expense of most others.
9 Discussion
In looking at BLEU score differences between the
two systems, the unigram precisions were typically
equal or only slightly different, while precisions for
higher-order n-grams contained the bulk of the im-
provement. This suggests that our system is not
finding substantially better translations for individ-
ual words in the input, but rather is focused on re-
ordering the existing translations. This is not sur-
prising given our choice of features, which focus on
syntactic language modeling and syntax-based re-
ordering. The obvious next step for our framework
is to include bilingual rules that include source syn-
tax (Quirk et al, 2005), target syntax (Shen et al,
2008), and syntax on both sides. Our framework al-
lows integrating together all of these and other types
of structures, with the ultimate goal of combining
the strengths of multiple approaches to translation
in a single model.
Acknowledgments
We thank Chris Dyer and the anonymous reviewers for
helpful comments that improved this paper. This research
was supported in part by the NSF through grant IIS-
0844507, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant number
W911NF-10-1-0533, and Sandia National Laboratories
(fellowship to K. Gimpel).
483
References
M. Auli, A. Lopez, H. Hoang, and P. Koehn. 2009. A
systematic analysis of translation model search spaces.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation.
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proc. of COLING.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In Proc. of EMNLP.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18.
X. Carreras and M. Collins. 2009. Non-projective pars-
ing for statistical machine translation. In Proc. of
EMNLP.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the Third Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proc. of ACL-IJCNLP.
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proc. of EMNLP-CoNLL.
J. DeNero, S. Kumar, C. Chelba, and F. J. Och. 2010.
Model combination for machine translation. In Proc.
of NAACL.
M. Dymetman and N. Cancedda. 2010. Intersecting hi-
erarchical and phrase-based models of translation. for-
mal aspects and algorithms. In Proc. of SSST-4.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
piling Comp Ling: Practical weighted dynamic pro-
gramming and the Dyna language. In Proc. of HLT-
EMNLP.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
R. W. Floyd. 1962. Algorithm 97: Shortest path. Com-
munications of the ACM, 5(6).
M. Galley and C. D. Manning. 2009. Quadratic-time
dependency parsing for machine translation. In Proc.
of ACL-IJCNLP.
M. Galley and C. D. Manning. 2010. Accurate non-
hierarchical phrase-based translation. In Proc. of
NAACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of COLING-ACL.
K. Gimpel and N. A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP.
K. Gimpel and N. A. Smith. 2011. Concavity and initial-
ization for unsupervised dependency grammar induc-
tion. Technical report, Carnegie Mellon University.
L. Huang, K. Knight, and A. Joshi. 2006. Statistical
syntax-directed translation with extended domain of
locality. In Proc. of AMTA.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Edinburgh
system description for the 2005 iwslt speech transla-
tion evaluation. In Proc. of IWSLT.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
R. Levy and C. D. Manning. 2003. Is it harder to parse
chinese, or the chinese treebank? In Proc. of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Y. Liu, H. Mi, Y. Feng, and Q. Liu. 2009. Joint de-
coding with multiple translation models. In Proc. of
ACL-IJCNLP.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proc. of ACL.
M.-J. Nederhof. 2003. Weighted deductive parsing and
knuth?s algorithm. Computational Linguistics, 29(1).
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
484
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL.
L. Shen, J. Xu, and R. Weischedel. 2008. A new string-
to-dependency machine translation algorithm with a
target dependency language model. In Proc. of ACL.
A. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Proc. of
the IEEE Int. Conf. on Acoustics, Speech and Signal
Processing.
D. A. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proc. of HLT-NAACL Workshop on
Statistical Machine Translation.
D. A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous features. In Proc.
of EMNLP.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of HLT-NAACL.
R. Tromble, S. Kumar, F. Och, and W. Macherey. 2008.
Lattice Minimum Bayes-Risk decoding for statistical
machine translation. In EMNLP.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.
Y. Wu, Q. Zhang, X. Huang, and L. Wu. 2009. Phrase
dependency parsing for opinion mining. In Proc. of
EMNLP.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1?30.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proc. of NAACL 2006 Workshop on Statistical Ma-
chine Translation.
A. Zollmann, A. Venugopal, F. J. Och, and J. Ponte.
2008. A systematic comparison of phrase-based, hi-
erarchical and syntax-augmented statistical MT. In
Proc. of COLING.
485
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594?604,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting a Scientific Community?s Response to an Article
Dani Yogatama Michael Heilman Brendan O?Connor Chris Dyer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
1 Introduction
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al, 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al, 2009; Joshi et al, 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
594
0
15
00
4 9
log(# downloads)
# d
oc
s.
0
25
00
0 18
# citations
# d
oc
s.
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
2 Data
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
2.1 NBER
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper?s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (?3) minimize squared er-
rors in the log space. Our download logs begin in
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper?s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
Dataset # Docs. Avg. #
Words
Response
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
Table 1: Descriptive statistics about the datasets.
1999. We use the 8,814 papers from 1999?2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
2.2 ACL
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al, 2009a;
Radev et al, 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
595
3 Model
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a ?ridge? model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a ?maximum entropy? model
(Berger et al, 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
3.1 Generalized Linear Models
Consider a model that predicts a response y given a
vector input x = ?x1, . . . , xd? ? Rd. Our models
are linear functions of x and parameterized by the
vector ?. Given a corpus of M document features,
X , and responses Y , we estimate:
?? = argmin? R(?) + L(?,X, Y ) (1)
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(?,X, Y ) = ?Mi=1(yi ? y?i)2,
where the prediction rule for new documents is:
y? =
?d
j=0 ?jxj . Probabilistically, this equates to an
assumption that ?>x is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
p(yi | xi) =
(
exp?>c xi
) /(?
c??C exp?>c?xi
)
3Preliminary experiments found no consistent benefit from
`1 (?lasso?) models, though we note that `1-regularization leads
to sparse, compact models that may be more interpretable.
where there is a feature vector ?c for each class
c ? C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for ?,
and R(?) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(?,X, Y ) = ??Mi=1 log p(yi | xi).
The prediction rule for a new document is: y? =
argmaxc?C
?d
j=0 ?c,jxj . Generalized linear mod-
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al, 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
3.2 Ridge Regression
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in ?3.3, the penalty R(?) is proportional to the `2-
norm of ?:
R(?) = ????2 = ?
?
j ?2j
where ? is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many ?j close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
?.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient ?j is
drawn i.i.d. from a normal distribution with mean
0 and variance 2??1.
3.3 Time Series Regularization
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RT?d. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt?,j = 0 for all
t? 6= t.
4The linear regression has a bias ?0 that is always active.
The logistic regression also has an unpenalized bias ?c,0 for
each class c. This weight is not regularized.
596
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each ?t,j is independent; even for features that dif-
fered only temporally (e.g., ?t,j and ?t+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, ?6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
R(?) = ?
T?
t=1
d?
j=1
?2t,j+??
T?
t=2
d?
j=0
(?t,j ? ?t?1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant ? by MT to facili-tate tuning of ? for datasets with different numbers
of examples M and numbers of time steps T . The
new parameter, ?, controls the smoothness of the es-
timated coefficients. Setting ? to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large ? parameter
(? ? ?) results in a single ridge regression over all
years since it imposes ?t,j = ?t+1,j for all t ? T .
The partial derivative is:
?R/??t,j = 2??t,j
+ 1{t > 1}2??(?t,j ? ?t?1,j)
+ 1{t < T}2??(?t,j ? ?t+1,j)
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
?
1
?
2
?
3
?
T
Y
1
Y
2
Y
3
Y
T
...
X
1
X
2
X
3
X
T
?,?
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
Probabilistic Interpretation
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be ?j = ??1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix ??1 = ? ?
RT?T :
? = ?
?
??????
1 + ? ?? 0 0 . . .
?? 1 + 2? ?? 0 . . .
0 ?? 1 + 2? ?? . . .
0 0 ?? 1 + 2? . . .
... ... ... ... . . .
?
??????
The form of R(?) follows from noting:
?2 log p(?j ;?, ?) = ?>j ??j + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al, 2008; Hamilton, 1994). The prior dis-
tribution over the sequence ??1,j , . . . , ?T,j? that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, ?j and ?k, are independent.
597
NBER ACL
Response log(#downloads+1) 1{#citations > 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall?s ? Kendall?s ?
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
4 Features
NBER metadata features
? Authors? last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
? NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
? Authors? last names as binary features.
? Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
Text features
? Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in ?5).
? Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a ?null? tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
5 Experiments
For each of the datasets in ?2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
5.1 Extrapolation
The lag between a paper?s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t ? g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t? g, t] as
the ?forecast gap?. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t?, t?g < t? < t. To extrapolate its number of down-
loads, we consider the observed number in [t?, t], and
then estimate the ratio r of downloads that occur in
the first t?t? time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t?, t]
by r?1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
598
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
5.2 Forecasting NBER Downloads
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(?3.2), trained on all past years (?all years?) and on
the single most recent past year (?one year?), respec-
tively. The last model (?time series?) is a GLM with
time series regularization (?3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999?2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999?2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
? ? 5{2,1,...,?5,?6}, and ? ? 5{3,2,...,?1,?2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999?2007 and 1999?2008, respectively. We
treated the last year of the training data (2007 and
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
training
modeling test (unused)
gap dev.
trr taitan tag
training gap test
trr tai tam tao
ddd
ddd
modeling test (unused)
oae
lae
oae
lae
training gap test
trr tamddd
modeling test
oae
lae
tao tar
NBER Experiments
ACL Experiments
training
modeling test (unused)
dev.
toa tro ta ddd
oae
lae
s(u)p)v.(uu)p)vProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1500?1511,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Sparsity in Structured Prediction
Andre? F. T. Martins?? Noah A. Smith? Pedro M. Q. Aguiar? Ma?rio A. T. Figueiredo?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Sistemas e Robo?tica, Instituto Superior Te?cnico, Lisboa, Portugal
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.pt
Abstract
Linear models have enjoyed great success in
structured prediction in NLP. While a lot of
progress has been made on efficient train-
ing with several loss functions, the problem
of endowing learners with a mechanism for
feature selection is still unsolved. Common
approaches employ ad hoc filtering or L1-
regularization; both ignore the structure of the
feature space, preventing practicioners from
encoding structural prior knowledge. We fill
this gap by adopting regularizers that promote
structured sparsity, along with efficient algo-
rithms to handle them. Experiments on three
tasks (chunking, entity recognition, and de-
pendency parsing) show gains in performance,
compactness, and model interpretability.
1 Introduction
Models for structured outputs are in demand across
natural language processing, with applications in in-
formation extraction, parsing, and machine transla-
tion. State-of-the-art models usually involve linear
combinations of features and are trained discrim-
inatively; examples are conditional random fields
(Lafferty et al, 2001), structured support vector
machines (Altun et al, 2003; Taskar et al, 2003;
Tsochantaridis et al, 2004), and the structured per-
ceptron (Collins, 2002a). In all these cases, the un-
derlying optimization problems differ only in the
choice of loss function; choosing among them has
usually a small impact on predictive performance.
In this paper, we are concerned with model se-
lection: which features should be used to define the
prediction score? The fact that models with few
features (?sparse? models) are desirable for several
reasons (compactness, interpretability, good gener-
alization) has stimulated much research work which
has produced a wide variety of methods (Della Pietra
et al, 1997; Guyon and Elisseeff, 2003; McCallum,
2003). Our focus is on methods which embed this
selection into the learning problem via the regular-
ization term. We depart from previous approaches
in that we seek to make decisions jointly about all
candidate features, and we want to promote sparsity
patterns that go beyond the mere cardinality of the
set of features. For example, we want to be able to
select entire feature templates (rather than features
individually), or to make the inclusion of some fea-
tures depend on the inclusion of other features.
We achieve the goal stated above by employ-
ing regularizers which promote structured sparsity.
Such regularizers are able to encode prior knowl-
edge and guide the selection of features by model-
ing the structure of the feature space. Lately, this
type of regularizers has received a lot of attention
in computer vision, signal processing, and compu-
tational biology (Zhao et al, 2009; Kim and Xing,
2010; Jenatton et al, 2009; Obozinski et al, 2010;
Jenatton et al, 2010; Bach et al, 2011). Eisenstein
et al (2011) employed structured sparsity in com-
putational sociolinguistics. However, none of these
works have addressed structured prediction. Here,
we combine these two levels of structure: struc-
ture in the output space, and structure in the feature
space. The result is a framework that allows build-
ing structured predictors with high predictive power,
while reducing manual feature engineering. We ob-
tain models that are interpretable, accurate, and of-
ten much more compact than L2-regularized ones.
Compared with L1-regularized models, ours are of-
ten more accurate and yield faster runtime.
1500
2 Structured Prediction
We address structured prediction problems, which
involve an input set X (e.g., sentences) and an out-
put set Y, assumed large and structured (e.g., tags or
parse trees). We assume that each x ? X has a set
of candidate outputs Y(x) ? Y. We consider linear
models, in which predictions are made according to
y? = arg maxy?Y(x) ? ? ?(x, y), (1)
where ?(x, y) ? RD is a vector of features, and ? ?
RD is the vector of corresponding weights. Let D =
{?xi, yi?}Ni=1 be a training sample. We assume a cost
function is defined such that c(y?, y) is the cost of
predicting y? when the true output is y; our goal is to
learn ? with small expected cost on unseen data. To
achieve this goal, linear models are usually trained
by solving a problem of the form
?? = arg min? ?(?) + 1N
?N
i=1 L(?, xi, yi), (2)
where ? is a regularizer and L is a loss function.
Examples of losses are: the negative conditional log-
likelihood used in CRFs (Lafferty et al, 2001),
LCRF(?, x, y) = ? logP?(y|x), (3)
where P?(y|x) ? exp(? ? ?(x, y)) is a log-linear
model; the margin rescaled loss of structured SVMs
(Taskar et al, 2003; Tsochantaridis et al, 2004),
LSVM(?, x, y) = max
y??Y(x)
? ? ??(y?) + c(y?, y), (4)
where ??(y?) = ?(x, y?)??(x, y); and the loss un-
derlying the structured perceptron (Collins, 2002a),
LSP(?, x, y) = maxy??Y(x) ? ? ??(y?). (5)
Empirical comparison among these loss functions
can be found in the literature (see, e.g., Martins et al,
2010, who also consider interpolations of the losses
above). In practice, it has been observed that the
choice of loss has far less impact than the model de-
sign and choice of features. Hence, in this paper,
we focus our attention on the regularization term in
Eq. 2. We specifically address ways in which this
term can be used to help design the model by pro-
moting structured sparsity. While this has been a
topic of intense research in signal processing and
computational biology (Jenatton et al, 2009; Liu
and Ye, 2010; Bach et al, 2011), it has not yet re-
ceived much attention in the NLP community, where
the choice of regularization for supervised learning
has essentially been limited to the following:
? L2-regularization (Chen and Rosenfeld, 2000):
?L2? (?) , ?2???22 = ?2
?D
d=1 ?2d; (6)
? L1-regularization (Kazama and Tsujii, 2003;
Goodman, 2004):
?L1? (?) , ????1 = ??Dd=1 |?d|. (7)
The latter is known as ?Lasso,? as popularized by
Tibshirani (1996) in the context of sparse regres-
sion. In the two cases above, ? and ? are nonneg-
ative coefficients controlling the intensity of the reg-
ularization. ?L2? usually leads to easier optimizationand robust performance; ?L1? encourages sparser
models, where only a few features receive nonzero
weights; see Gao et al (2007) for an empirical com-
parison. More recently, Petrov and Klein (2008b)
applied L1 regularization for structure learning in
phrase-based parsing; a comparison with L2 appears
in Petrov and Klein (2008a). Elastic nets interpolate
between L1 and L2, having been proposed by Zou
and Hastie (2005) and used by Lavergne et al (2010)
to regularize CRFs.
Neither of the regularizers just described ?looks?
at the structure of the feature space, since they all
treat each dimension independently?we call them
unstructured regularizers, as opposed to the struc-
tured ones that we next describe.
3 Structured Sparsity
We are interested in regularizers that share with ?L1?
the ability to promote sparsity, so that they can be
used for selecting features. In addition, we want to
endow the feature space RD with additional struc-
ture, so that features are not penalized individually
(as in the L1-case) but collectively, encouraging en-
tire groups of features to be discarded. The choice of
groups will allow encoding prior knowledge regard-
ing the kind of sparsity patterns that are intended in
the model. This can be achieved with group-Lasso
regularization, which we next describe.
1501
3.1 The Group Lasso
To capture the structure of the feature space, we
group our D features into M groups G1, . . . , GM ,
where each Gm ? {1, . . . , D}. Ahead, we dis-
cuss meaningful ways of choosing group decompo-
sitions; for now, let us assume a sensible choice is
obvious to the model designer. Denote by ?m =
??d?d?Gm the subvector of those weights that cor-
respond to the features in the m-th group, and let
d1, . . . , dM be nonnegative scalars (one per group).
We consider the following group-Lasso regularizers:
?GLd =
?M
m=1 dm??m?2. (8)
These regularizers were first proposed by Bakin
(1999) and Yuan and Lin (2006) in the context of re-
gression. If d1 = . . . = dM , ?GLd becomes the ?L1
norm of the L2 norms.? Interestingly, this is also
a norm, called the mixed L2,1-norm.1 These regu-
larizers subsume the L1 and L2 cases, which corre-
spond to trivial choices of groups:
? If each group is a singleton, i.e., M = D and
Gd = {?d}, and d1 = . . . = dM = ? , we recover
L1-regularization (cf. Eqs. 7?8).
? If there is a single group spanning all the features,
i.e., M = 1 and G1 = {1, . . . , D}, then the right
hand side of Eq. 8 becomes d1???2. This is equiv-
alent to L2 regularization.2
We next present some non-trivial examples con-
cerning different topologies of G = {G1, . . . , GM}.
Non-overlapping groups. Let us first consider
the case where G is a partition of the feature
space: the groups cover all the features (?mGm =
{1, . . . , D}), and they do not overlap (Ga?Gb = ?,
?a 6= b). Then, ?GLd is termed a non-overlapping
group-Lasso regularizer. It encourages sparsity pat-
terns in which entire groups are discarded. A ju-
dicious choice of groups can lead to very compact
1In the statistics literature, such mixed-norm regularizers,
which group features and then apply a separate norm for each
group, are called composite absolute penalties (Zhao et al,
2009); other norms besides L2,1 can be used, such as L?,1
(Quattoni et al, 2009; Wright et al, 2009; Eisenstein et al,
2011).
2Note that Eqs. 8 and 6 do not become exactly the same: in
Eq. 6, the L2 norm is squared. However it can be shown that
both regularizers lead to identical learning problems (Eq. 2) up
to a transformation of the regularization constant.
models and pinpoint relevant groups of features.
The following examples lie in this category:
? The two cases above (L1 and L2 regularization).
? Label-based groups. In multi-label classification,
where Y = {1, . . . , L}, features are typically de-
signed as conjunctions of input features with la-
bel indicators, i.e., they take the form ?(x, y) =
?(x)? ey, where ?(x) ? RDX , ey ? RL has all
entries zero except the y-th entry, which is 1, and
? denotes the Kronecker product. Hence ?(x, y)
can be reshaped as aDX -by-Lmatrix, and we can
let each group correspond to a row. In this case,
all groups have the same size and we typically set
d1 = . . . = dM . A similar design can be made
for sequence labeling problems, by considering a
similar grouping for the unigram features.3
? Template-based groups. In NLP, features are com-
monly designed via templates. For example, a
template such as w0 ? p0 ? p?1 denotes the word
in the current position (w0) conjoined with its
part-of-speech (p0) and that of the previous word
(p?1). This template encloses many features cor-
responding to different instantiantions of w0, p0,
and p?1. In ?5, we learn feature templates from
the data, by associating each group to a feature
template, and letting that group contain all fea-
tures that are instantiations of this template. Since
groups have different sizes, it is a good idea to
let dm increase with the group size, so that larger
groups pay a larger penalty for being included.
Tree-structured groups. More generally, we may
let the groups in G overlap but be nested, i.e., we may
want them to form a hierarchy (two distinct groups
either have empty intersection or one is contained in
the other). This induces a partial order on G (the set
inclusion relation ?), endowing it with the structure
of a partially ordered set (poset).
A convenient graphical representation of the poset
?G,?? is its Hasse diagram. Each group is a node
in the diagram, and an arc is drawn from group Ga
to group Gb if Gb ? Ga and there is no b? s.t.
Gb ? Gb? ? Ga. When the groups are nested, this
diagram is a forest (a union of directed trees). The
corresponding regularizer enforces sparsity patterns
3The same idea is also used in multitask learning, where
labels correspond to tasks (Caruana, 1997).
1502
where a group of features is only selected if all its
ancestors are also selected.4 Hence, entire subtrees
in the diagram can be pruned away. Examples are:
? The elastic net. The diagram of G has a root node
for G1 = {1, . . . , D} and D leaf nodes, one per
each singleton group (see Fig. 1).
? The sparse group-Lasso. This regularizer was
proposed by Friedman et al (2010):
?SGLd,? (?) =
?M ?
m=1 (dm??m?2 + ?m??m?1) ,
(9)
where the total number of groups is M = M ? +
D, and the components ?1, . . . ,?M ? are non-
overlapping. This regularizer promotes sparsity
at both group and feature levels (i.e., it eliminates
entire groups and sparsifies within each group).
Graph-structured groups. In general, the groups
in G may overlap without being nested. In this case,
the Hasse diagram of G is a directed acyclic graph
(DAG). As in the tree-structured case, a group of
features is only selected if all its ancestors are also
selected. Based on this property, Jenatton et al
(2009) suggested a way of reverse engineering the
groups from the desired sparsity pattern. We next
describe a strategy for coarse-to-fine feature tem-
plate selection that directly builds on that idea.
Suppose that we are given M feature templates
T = {T1, . . . , TM} which are partially ordered ac-
cording to some criterion, such that if Ta  Tb we
would like to include Tb in our model only if Ta
is also included. This criterion could be a measure
of coarseness: we may want to let coarser part-of-
speech features precede finer lexical features, e.g.,
p0 ? p1  w0 ? w1, or conjoined features come af-
ter their elementary parts, e.g., p0  p0 ? p1. The
order does not need to be total, so some templates
may not be comparable (e.g., we may want p0 ? p?1
and p0 ? p1 not to be comparable). To achieve
the sparsity pattern encoded in ?T,?, we choose
G = ?G1, . . . , GM ? as follows: let I(Ta) be the
set of features that are instantiations of template Ta;
then define Ga = ?b:ab I(Tb), for a = 1, . . . ,M .
It is easy to see that ?G,?? and ?T,? are isomorph
posets (their Hasse diagrams have the same shape;
4We say that a group of features Gm is selected if some fea-
ture in Gm (but not necessarily all) has a nonzero weight.
see Fig. 1). The result is a ?coarse-to-fine? regular-
izer, which prefers to select feature templates that
are coarser before zooming into finer features.
3.2 Bayesian Interpretation
The prior knowledge encoded in the group-Lasso
regularizer (Eq. 8) comes with a Bayesian inter-
pretation, as we next describe. In a probabilistic
model (e.g. in the CRF case, where L = LCRF),
the optimization problem in Eq. 2 can be seen as
maximum a posteriori estimation of ?, where the
regularization term ?(?) corresponds to the neg-
ative log of a prior distribution (call it p(?)). It
is well-known that L2-regularization corresponds to
choosing independent zero-mean Gaussian priors,
?d ? N(0, ??1), and that L1-regularization results
from adopting zero-mean Laplacian priors, p(?d) ?
exp(? |?d|).
Figueiredo (2002) provided an alternative inter-
pretation of L1-regularization in terms of a two-
level hierarchical Bayes model, which happens to
generalize to the non-overlapping group-Lasso case,
where ? = ?GLd . As in the L2-case, we also assume
that each parameter receives a zero-mean Gaussian
prior, but now with a group-specific variance ?m,
i.e., ?m ? N(0, ?mI) for m = 1, . . . ,M . This
reflects the fact that some groups should have their
feature weights shrunk more towards zero than oth-
ers. The variances ?m ? 0 are not pre-specified but
rather generated by a one-sided exponential hyper-
prior p(?m|dm) ? exp(?d2m?m/2). It can be shown
that after marginalizing out ?m, we obtain
p(?m|dm) =
? ?
0
p(?m|?m)p(?m|dm)d?m
? exp (?dm??m?) . (10)
Hence, the non-overlapping group-Lasso corre-
sponds to the following two-level hierachical Bayes
model: independently for each m = 1, . . . ,M ,
?m ? Exp(d2m/2), ?m ? N(0, ?mI). (11)
3.3 Prox-operators
Before introducing our learning algorithm for han-
dling group-Lasso regularization, we need to define
the concept of a ?-proximity operator. This is the
function prox? : RD ? RD defined as follows:
prox?(?) = arg min?? 12??? ? ??2 + ?(??). (12)
1503
Figure 1: Hasse diagrams of several group-
based regularizers. For all tree-structured
cases, we use the same plate notation that
is traditionally used in probabilistic graphical
models. The rightmost diagram represents a
coarse-to-fine regularizer: each node is a tem-
plate involving contiguous sequences of words
(w) and POS tags (p); the symbol order ? 
p  w induces a template order (Ta  Tb
iff at each position i [Ta]i  [Tb]i). Digits
below each node are the group indices where
each template belongs.
Proximity operators generalize Euclidean projec-
tions and have many interesting properties; see Bach
et al (2011) for an overview. By requiring zero to be
a subgradient of the objective function in Eq. 12, we
obtain the following closed expression (called soft-
thresholding) for the ?L1? -proximity operator:
[prox?L1? (?)]d =
?
?
?
?d ? ? if ?d > ?
0 if |?d| ? ?
?d + ? if ?d < ?? .
(13)
For the non-overlapping group Lasso case, the prox-
imity operator is given by
[prox?GLd (?)]m =
{
0 if ??m?2 ? dm
??m?2?dm
??m?2 ?m otherwise.
(14)
which can be seen as a generalization of Eq. 13: if
the L2-norm of the m-th group is less than dm, the
entire group is discarded; otherwise it is scaled so
that its L2-norm decreases by an amount of dm.
When groups overlap, the proximity operator
lacks a closed form. When G is tree-structured, it
can still be efficiently computed by a recursive pro-
cedure (Jenatton et al, 2010). When G is not tree-
structured, no specialized procedure is known, and a
convex optimizer is necessary to solve Eq. 12.
4 Online Prox-Grad Algorithm
We now turn our attention to efficient ways of han-
dling group-Lasso regularizers. Several fast and
scalable algorithms having been proposed for train-
ing L1-regularized CRFs, based on quasi-Newton
optimization (Andrew and Gao, 2007), coordinate
descent (Sokolovska et al, 2010; Lavergne et al,
2010), and stochastic gradients (Carpenter, 2008;
Langford et al, 2009; Tsuruoka et al, 2009). The
algorithm that we use in this paper (Alg. 1) extends
the stochastic gradient methods for group-Lasso reg-
ularization; a similar algorithm was used by Martins
et al (2011) for multiple kernel learning.
Alg. 1 addresses the learning problem in Eq. 2 by
alternating between online (sub-)gradient steps with
respect to the loss term, and proximal steps with
respect to the regularizer. Proximal-gradient meth-
ods are very popular in sparse modeling, both in
batch (Liu and Ye, 2010; Bach et al, 2011) and on-
line (Duchi and Singer, 2009; Xiao, 2009) settings.
The reason we have chosen the algorithm of Martins
et al (2011) is that it effectively handles overlap-
ping groups, without the need of evaluating prox?
(which, as seen in ?3.3, can be costly if G is not tree-
structured). To do so, it decomposes ? as
?(?) = ?Jj=1 ?j?j(?) (15)
for some J ? 1, and nonnegative ?1, . . . , ?J ; each
?j-proximal operator is assumed easy to compute.
Such a decomposition always exists: if G does not
have overlapping groups, take J = 1. Otherwise,
find J ? M disjoint sets G1, . . . ,GJ such that?J
j=1 Gj = G and the groups on each Gj are non-
overlapping. The proximal steps are then applied
sequentially, one per each ?j . Overall, Alg. 1 satis-
fies the following important requirements:
? Computational efficiency. Each gradient step at
round t is linear in the number of features that
fire for that instance and independent of the total
number of features D. Each proximal step is lin-
ear in the number of groupsM , and does not need
be to performed every round (as we will see later).
1504
Algorithm 1 Online Sparse Prox-Grad Algorithm
1: input: D, ??j?Jj=1, T , gravity sequence
???jt?Jj=1?Tt=1, stepsize sequence ??t?Tt=1
2: initialize ? = 0
3: for t = 1 to T do
4: take training pair ?xt, yt? ? D
5: ? ? ? ? ?t?L(?;xt, yt) (gradient step)
6: for j = 1 to J do
7: ? = prox?t?jt?j (?) (proximal step)
8: end for
9: end for
10: output: ?
? Memory efficiency. Only a small active set of fea-
tures (those that have nonzero weights) need to
be maintained. Entire groups of features can be
deleted after each proximal step. Furthermore,
only the features which correspond to nonzero en-
tries in the gradient vector need to be inserted in
the active set; for some losses (LSVM and LSP)
many irrelevant features are never instantianted.
? Convergence. With high probability, Alg. 1 pro-
duces an -accurate solution after T ? O(1/2)
rounds, for a suitable choice of stepsizes and hold-
ing ?jt constant, ?jt = ?j (Martins et al, 2011).
This result can be generalized to any sequence
??jt?Tt=1 such that ?j = 1T
?T
t=1 ?jt.
We next describe several algorithmic ingredients
that make Alg. 1 effective in sparse modeling.
Budget-Driven Shrinkage. Alg. 1 requires the
choice of a ?gravity sequence.? We follow Lang-
ford et al (2009) and set ??jt?Jj=1 to zero for all t
which is not a multiple of some prespecified integer
K; this way, proximal steps need only be performed
eachK rounds, yielding a significant speed-up when
the number of groups M is large. A direct adop-
tion of the method of Langford et al (2009) would
set ?jt = K?j for those rounds; however, we have
observed that such a strategy makes the number of
groups vary substantially in early epochs. We use a
different strategy: for each Gj , we specify a budget
of Bj ? 0 groups (this may take into consideration
practical limitations, such as the available memory).
If t is a multiple of K, we set ?jt as follows:
1. If Gj does not have more than Bj nonzero
groups, set ?jt = 0 and do nothing.
2. Otherwise, sort the groups in Gj by decreasing
order of their L2-norms. Check the L2-norms
of the Bj-th and Bj+1-th entries in the list and
set ?jt as the mean of these two divided by ?t.
3. Apply a ?t?jt?j-proximal step using Eq. 14.
At the end of this step, no more than Bj groups
will remain nonzero.5
If the average of the gravity steps converge,
limT?? 1T
?T
t=1 ?jt ? ?j , then the limit points
?j implicitly define the regularizer, via ? =?J
j=1 ?j?j .6 Hence, we have shifted the control of
the amount of regularization to the budget constants
Bj , which unlike the ?j have a clear meaning and
can be chosen under practical considerations.
Space and Time Efficiency. The proximal steps
in Alg. 1 have a scaling effect on each group, which
affects all features belonging to that group (see
Eq. 14). We want to avoid explicitly updating each
feature in the active set, which could be time con-
suming. We mention two strategies that can be used
for the non-overlapping group Lasso case.
? The first strategy is suitable when M is large and
only a few groups ( M ) have features that fire
in each round; this is the case, e.g., of label-based
groups (see ?3.1). It consists of making lazy up-
dates (Carpenter, 2008), i.e., to delay the update
of all features in a group until at least one of
them fires; then apply a cumulative penalty. The
amount of the penalty can be computed if one as-
signs a timestamp to each group.
? The second strategy is suitable when M is small
and some groups are very populated; this is the
typical case of template-based groups (?3.1). Two
operations need to be performed: updating each
feature weight (in the gradient steps), and scaling
entire groups (in the proximal steps). We adapt
a trick due to Shalev-Shwartz et al (2007): repre-
sent the weight vector of them-th group, ?m, by a
5When overlaps exist (e.g. the coarse-to-fine case), we spec-
ify a total pseudo-budget B ignoring the overlaps, which in-
duces budgets B1, . . . , BJ which sum to B. The number of
actually selected groups may be less than B, however, since in
this case some groups can be shrunk more than once. Other
heuristics are possible.
6The convergence assumption can be sidestepped by freez-
ing the ?j after a fixed number of iterations.
1505
triple ??m, cm, ?m? ? R|Gm|?R+?R+, such that
?m = cm?m and ??m?2 = ?m. This representa-
tion allows performing the two operations above
in constant time, and it keeps track of the group
L2-norms, necessary in the proximal updates.
For sufficient amounts of regularization, our al-
gorithm has a low memory footprint. Only features
that, at some point, intervene in the gradient com-
puted in line 5 need to be instantiated; and all fea-
tures that receive zero weights after some proximal
step can be deleted from the model (cf. Fig. 2).
Sparseptron and Debiasing. Although Alg. 1 al-
lows to simultaneously select features and learn the
model parameters, it has been observed in the sparse
modeling literature that Lasso-like regularizers usu-
ally have a strong bias which may harm predictive
performance. A post-processing stage is usually
taken (called debiasing), in which the model is re-
fitted without any regularization and using only the
selected features (Wright et al, 2009). If a final de-
biasing stage is to be performed, Alg. 1 only needs
to worry about feature selection, hence it is appeal-
ing to choose a loss function that makes this pro-
cedure as simple as possible. Examining the input
of Alg. 1, we see that both a gravity and a stepsize
sequence need to be specified. The former can be
taken care of by using budget-driven shrinkage, as
described above. The stepsize sequence can be set
as ?t = ?0/
?
dt/Ne, which ensures convergence,
however ?0 requires tuning. Fortunately, for the
structured perceptron loss LSP (Eq. 5), Alg. 1 is in-
dependent of ?0, up to a scaling of ?, which does not
affect predictions (see Eq. 1).7 We call the instanti-
ation of Alg. 1 with a group-Lasso regularizer and
the loss LSP the sparseptron. Overall, we propose
the following two-stage approach:
1. Run the sparsepton for a few epochs and dis-
card the features with zero weights.
2. Refit the model without any regularization and
using the loss L which one wants to optimize.
7To see why this is the case, note that both gradient and
proximal updates come scaled by ?0; and that the gradient of
the loss is?LSP(?, xt, yt) = ?(xt, y?t)? ?(xt, yt), where y?t
is the prediction under the current model, which is insensitive to
the scaling of ?. This independence on ?0 does not hold when
the loss is LSVM or LCRF.
5 Experiments
We present experiments in three structured predic-
tion tasks for several group choices.
Text Chunking. We use the English dataset pro-
vided in the CoNLL 2000 shared task (Sang and
Buchholz, 2000), which consists of 8,936 training
and 2,012 testing sentences (sections 15?18 and 20
of the WSJ.) The input observations are the token
words and their POS tags; we want to predict the
sequences of IOB tags representing phrase chunks.
We built 96 contextual feature templates as follows:
? Up to 5-grams of POS tags, in windows of 5 to-
kens on the left and 5 tokens on the right;
? Up to 3-grams of words, in windows of 3 tokens
on the left and 3 tokens on the right;
? Up to 2-grams of word shapes, in windows of
2 tokens on the left and 2 tokens on the right.
Each shape replaces characters by their types
(case sensitive letters, digits, and punctuation),
and deletes repeated types?e.g., Confidence
and 2,664,098 are respectively mapped to Aa
and 0,0+,0+ (Collins, 2002b).
We defined unigram features by conjoining these
templates with each of the 22 output labels. An ad-
ditional template was defined to account for label
bigrams?features in this template do not look at the
input string, but only at consecutive pairs of labels.8
We evaluate the ability of group-Lasso regular-
ization to perform feature template selection. To
do that, we ran 5 epochs of the sparseptron algo-
rithm with template-based groups and budget-driven
shrinkage (budgets of 10, 20, 30, 40, and 50 tem-
plates were tried). For each group Gm, we set dm =
log2 |Gm|, which is the average number of bits nec-
essary to encode a feature in that group, if all fea-
tures were equiprobable. We set K = 1000 (the
number of instances between consecutive proximal
steps). Then, we refit the model with 10 iterations
of the max-loss 1-best MIRA algorithm (Crammer
et al, 2006).9 Table 1 compares the F1 scores and
8State-of-the-art models use larger output contexts, such as
label trigrams and 4-grams. We resort to bigram labels as we
are mostly interested in identifying relevant unigram templates.
9This variant optimizes theLSVM loss (Martins et al, 2010).
For the refitting, we used unregularized MIRA. For the baseline
1506
Table 1: Results for
text chunking.
MIRA Group Lasso B = 10 B = 20 B = 30 B = 40 B = 50
F1 (%) 93.10 92.99 93.28 93.59 93.42 93.40
model size (# features) 5,300,396 71,075 158,844 389,065 662,018 891,378
MIRA Lasso C = 0.1 C = 0.5 C = 1 Group-Lasso B = 100 B = 200 B = 300
Spa. dev/test 70.38/74.09 69.19/71.9 70.75/72.38 71.7/74.03 71.79/73.62 72.08/75.05 71.48/73.3
8,598,246 68,565 1,017,769 1,555,683 83,036 354,872 600,646
Dut. dev/test 69.15/71.54 64.07/66.35 66.82/69.42 70.43/71.89 69.48/72.83 71.03/73.33 71.2/72.59
5,727,004 164,960 565,704 953,668 128,320 447,193 889,660
Eng. dev/test 83.95/79.81 80.92/76.95 82.58/78.84 83.38/79.35 85.62/80.26 85.86/81.47 85.03/80.91
8,376,901 232,865 870,587 1,114,016 255,165 953,178 1,719,229
Table 2: Results for named entity recognition. Each cell shows F1 (%) and the number of features.
0 5 10 150
2
4
6 x 10
6
# Epochs
# F
eatu
res
 
 
MIRA
Sparceptron + MIRA (B=30)
Figure 2: Memory footprints of the MIRA and sparsep-
tron algorithms in text chunking. The oscillation in the
first 5 epochs (bottom line) comes from the proximal
steps each K = 1000 rounds. The features are then
frozen and 10 epochs of unregularized MIRA follow.
Overall, the sparseptron requires < 7.5% of the memory
as the MIRA baseline.
the model sizes obtained with the several budgets
against those obtained by running 15 iterations of
MIRA with the original set of features. Note that
the total number of iterations is the same; yet, the
group-Lasso approach has a much smaller memory
footprint (see Fig. 2) and yields much more com-
pact models. The small memory footprint comes
from the fact that Alg. 1 may entertain a large num-
ber of features without ever instantiating all of them.
The predictive power is comparable (although some
choices of budget yield slightly better scores for the
group-Lasso approach).10
Named Entity Recognition. We experiment with
the Spanish, Dutch, and English datasets pro-
vided in the CoNLL 2002/2003 shared tasks (Sang,
2002; Sang and De Meulder, 2003). For Span-
ish, we use the POS tags provided by Car-
(described next), we used L2-regularized MIRA and tuned the
regularization constant with cross-validation.
10We also tried label-based group-Lasso and sparse group-
Lasso (?3.1), with less impressive results (omitted for space).
reras (http://www.lsi.upc.es/?nlp/tools/
nerc/nerc.html); for English, we ignore the syn-
tactic chunk tags provided with the dataset. Hence,
all datasets have the same sort of input observations
(words and POS) and all have 9 output labels. We
use the feature templates described above plus some
additional ones (yielding a total of 452 templates):
? Up to 3-grams of shapes, in windows of size 3;
? For prefix/suffix sizes of 1, 2, 3, up to 3-grams of
word prefixes/suffixes, in windows of size 3;
? Up to 5-grams of case, punctuation, and digit in-
dicators, in windows of size 5.
As before, an additional feature template was de-
fined to account for label bigrams. We do feature
template selection (same setting as before) for bud-
get sizes of 100, 200, and 300. We compare with
both MIRA (using all the features) and the sparsep-
tron with a standard Lasso regularizer ?L1? , for sev-
eral values of C = 1/(?N). Table 2 shows the re-
sults. We observe that template-based group-Lasso
wins both in terms of accuracy and compactness.
Note also that the ability to discard feature tem-
plates (rather than individual features) yields faster
test runtime than models regularized with the stan-
dard Lasso: fewer templates will need to be instan-
tiated, with a speed-up in score computation.
Multilingual Dependency Parsing. We trained
non-projective dependency parsers for 6 languages
using the CoNLL-X shared task datasets (Buchholz
and Marsi, 2006): Arabic, Danish, Dutch, Japanese,
Slovene, and Spanish. We chose the languages with
the smallest datasets, because regularization is more
important when data is scarce. The output to be pre-
dicted from each input sentence is the set of depen-
dency links, which jointly define a spanning tree.
1507
2 4 6 8 10 12
x 106
76.5
77
77.5
78
78.5
Number of Features
UA
S (%
)
Arabic
 
 
0 5 10 15
x 106
89
89.2
89.4
89.6
89.8
90 Danish
0 2 4 6 8
x 106
92
92.5
93
93.5 Japanese
0 2 4 6 8 10
x 106
81
82
83
84 Slovene
0 0.5 1 1.5 2
x 107
82
82.5
83
83.5
84 Spanish
0 5 10 15
x 106
74
74.5
75
75.5
76 Turkish
 
 
Group?LassoGroup?Lasso (C2F)LassoFilter?based (IG)
Figure 3: Comparison between non-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-based
method based on information gain for selecting feature templates in multilingual dependency parsing. The x-axis is
the total number of features at different regularization levels, and the y-axis is the unlabeled attachment score. The
plots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method. The standard
Lasso (which does not select templates, but individual features) is also shown for comparison.
We use arc-factored models, for which exact infer-
ence is tractable (McDonald et al, 2005). We de-
fined M = 684 feature templates for each candi-
date arc by conjoining the words, shapes, lemmas,
and POS of the head and the modifier, as well as
the contextual POS, and the distance and direction
of attachment. We followed the same two-stage
approach as before, and compared with a baseline
which selects feature templates by ranking them ac-
cording to the information gain criterion. This base-
line assigns a score to each template Tm which re-
flects an empirical estimate of the mutual informa-
tion between Tm and the binary variable A that indi-
cates the presence/absence of a dependency link:
IGm ,
?
f?Tm
?
a?{0,1}
P (f, a) log2
P (f, a)
P (f)P (a) , (16)
where P (f, a) is the joint probability of feature f
firing and an arc being active (a = 1) or innactive
(a = 0), and P (f) and P (a) are the corresponding
marginals. All probabilities are estimated from the
empirical counts of events observed in the data.
The results are plotted in Fig. 3, for budget sizes
of 200, 300, and 400. We observe that for all
but one language (Spanish is the exception), non-
overlapping group-Lasso regularization is more ef-
fective at selecting feature templates than the in-
formation gain criterion, and slightly better than
coarse-to-fine group-Lasso. For completeness, we
also display the results obtained with a standard
Lasso regularizer. Table 3 shows what kind of
feature templates were most selected for each lan-
guage. Some interesting patterns can be observed:
morphologically-rich languages with small datasets
(such as Turkish and Slovene) seem to avoid lexi-
cal features, arguably due to potential for overfitting;
in Japanese, contextual POS appear to be specially
relevant. It should be noted, however, that some
of these patterns may be properties of the datasets
rather than of the languages themselves.
6 Related Work
A variant of the online proximal gradient algorithm
used in this paper was proposed by Martins et al
1508
Ara. Dan. Jap. Slo. Spa. Tur.
Bilexical ++ + +
Lex.? POS + +
POS? Lex. ++ + + + +
POS? POS ++ +
Middle POS ++ ++ ++ ++ ++ ++
Shape ++ ++ ++ ++
Direction + + + + +
Distance ++ + + + + +
Table 3: Variation of feature templates that were selected
accross languages. Each line groups together similar tem-
plates, involving lexical, contextual POS, word shape in-
formation, as well as attachment direction and length.
Empty cells denote that very few or none of the templates
in that category was selected; + denotes that some were
selected; ++ denotes that most or all were selected.
(2011), along with a theoretical analysis. The fo-
cus there, however, was multiple kernel learning,
hence overlapping groups were not considered in
their experiments. Budget-driven shrinkage and the
sparseptron are novel techniques, at the best of our
knowledge. Apart from Martins et al (2011), the
only work we are aware of which combines struc-
tured sparsity with structured prediction is Schmidt
and Murphy (2010); however, their goal is to pre-
dict the structure of graphical models, while we
are mostly interested in the structure of the feature
space. Schmidt and Murphy (2010) used to gener-
ative models, while our approach emphasizes dis-
criminative learning.
Mixed norm regularization has been used for a
while in statistics as a means to promote structured
sparsity. Group Lasso is due to Bakin (1999) and
Yuan and Lin (2006), after which a string of variants
and algorithms appeared (Bach, 2008; Zhao et al,
2009; Jenatton et al, 2009; Friedman et al, 2010;
Obozinski et al, 2010). The flat (non-overlapping)
case has tight links with learning formalisms such
as multiple kernel learning (Lanckriet et al, 2004)
and multi-task learning (Caruana, 1997). The tree-
structured case has been addressed by Kim and Xing
(2010), Liu and Ye (2010) and Mairal et al (2010),
along with L?,1 and L2,1 regularization. Graph-
structured groups are discussed in Jenatton et al
(2010), along with a DAG representation. In NLP,
mixed norms have been used recently by Grac?a et al
(2009) in posterior regularization, and by Eisenstein
et al (2011) in a multi-task regression problem.
7 Conclusions
In this paper, we have explored two levels of struc-
ture in NLP problems: structure on the outputs, and
structure on the feature space. We have shown how
the latter can be useful in model design, through the
use of regularizers which promote structured spar-
sity. We propose an online algorithm with mini-
mal memory requirements for exploring large fea-
ture spaces. Our algorithm, which specializes into
the sparseptron, yields a mechanism for selecting
entire groups of features. We apply sparseptron
for selecting feature templates in three structured
prediction tasks, with advantages over filter-based
methods, L1, and L2 regularization in terms of per-
formance, compactness, and model interpretability.
Acknowledgments
We would like to thank all reviewers for their comments,
Eric Xing for helpful discussions, and Slav Petrov for his
comments on a draft version of this paper. A. M. was sup-
ported by a FCT/ICTI grant through the CMU-Portugal
Program, and also by Priberam. This work was partially
supported by the FET programme (EU FP7), under the
SIMBAD project (contract 213250). N. S. was supported
by NSF CAREER IIS-1054319.
References
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In Proc. of
ICML.
G. Andrew and J. Gao. 2007. Scalable training of
L1-regularized log-linear models. In Proc. of ICML.
ACM.
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. 2011.
Convex optimization with sparsity-inducing norms. In
Optimization for Machine Learning. MIT Press.
F. Bach. 2008. Exploring large feature spaces with hier-
archical multiple kernel learning. NIPS, 21.
S. Bakin. 1999. Adaptive regression and model selec-
tion in data mining problems. Ph.D. thesis, Australian
National University.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
B. Carpenter. 2008. Lazy sparse stochastic gradient de-
scent for regularized multinomial logistic regression.
Technical report, Technical report, Alias-i.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
1509
S. F. Chen and R. Rosenfeld. 2000. A survey of
smoothing techniques for maximum entropy models.
IEEE Transactions on Speech and Audio Processing,
8(1):37?50.
M. Collins. 2002a. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
M. Collins. 2002b. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In
Proc. of ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Algo-
rithms. JMLR, 7:551?585.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19:380?393.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2873?2908.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
M.A.T. Figueiredo. 2002. Adaptive sparseness using Jef-
freys? prior. Advances in Neural Information Process-
ing Systems.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. A note
on the group lasso and a sparse group lasso. Unpub-
lished manuscript.
J. Gao, G. Andrew, M. Johnson, and K. Toutanova. 2007.
A comparative study of parameter estimation methods
for statistical natural language processing. In Proc. of
ACL.
J. Goodman. 2004. Exponential priors for maximum en-
tropy models. In Proc. of NAACL.
J. Grac?a, K. Ganchev, B. Taskar, and F. Pereira. 2009.
Posterior vs. parameter sparsity in latent variable mod-
els. Advances in Neural Information Processing Sys-
tems.
I. Guyon and A. Elisseeff. 2003. An introduction to vari-
able and feature selection. Journal of Machine Learn-
ing Research, 3:1157?1182.
R. Jenatton, J.-Y. Audibert, and F. Bach. 2009. Struc-
tured variable selection with sparsity-inducing norms.
Technical report, arXiv:0904.3523.
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. 2010.
Proximal methods for sparse hierarchical dictionary
learning. In Proc. of ICML.
J. Kazama and J. Tsujii. 2003. Evaluation and exten-
sion of maximum entropy models with inequality con-
straints. In Proc. of EMNLP.
S. Kim and E.P. Xing. 2010. Tree-guided group lasso for
multi-task regression with structured sparsity. In Proc.
of ICML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El
Ghaoui, and M. I. Jordan. 2004. Learning the kernel
matrix with semidefinite programming. JMLR, 5:27?
72.
J. Langford, L. Li, and T. Zhang. 2009. Sparse online
learning via truncated gradient. JMLR, 10:777?801.
T. Lavergne, O. Cappe?, and F. Yvon. 2010. Practical
very large scale CRFs. In Proc. of ACL.
J. Liu and J. Ye. 2010. Moreau-Yosida regularization for
grouped tree structure learning. In Advances in Neural
Information Processing Systems.
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. 2010.
Network flow algorithms for structured sparsity. In
Advances in Neural Information Processing Systems.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011. Online learning of
structured predictors with multiple kernels. In Proc. of
AISTATS.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proc. of UAI.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
G. Obozinski, B. Taskar, and M.I. Jordan. 2010. Joint co-
variate selection and joint subspace selection for multi-
ple classification problems. Statistics and Computing,
20(2):231?252.
S. Petrov and D. Klein. 2008a. Discriminative log-linear
grammars with latent variables. Advances in Neural
Information Processing Systems, 20:1153?1160.
S. Petrov and D. Klein. 2008b. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
Proc. of EMNLP.
A. Quattoni, X. Carreras, M. Collins, and T. Darrell.
2009. An efficient projection for l1,? regularization.
In Proc. of ICML.
E.F.T.K. Sang and S. Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proceedings
of CoNLL-2000 and LLL-2000.
E.F.T.K. Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proc. of CoNLL.
E.F.T.K. Sang. 2002. Introduction to the CoNLL-
2002 shared task: Language-independent named entity
recognition. In Proc. of CoNLL.
1510
M. Schmidt and K. Murphy. 2010. Convex structure
learning in log-linear models: Beyond pairwise poten-
tials. In Proc. of AISTATS.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for SVM.
In ICML.
N. Sokolovska, T. Lavergne, O. Cappe?, and F. Yvon.
2010. Efficient learning of sparse conditional random
fields for supervised sequence labelling. IEEE Journal
of Selected Topics in Signal Processing, 4(6):953?964.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society
B., pages 267?288.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In ICML.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL.
S.J. Wright, R. Nowak, and M.A.T. Figueiredo. 2009.
Sparse reconstruction by separable approximation.
IEEE Transactions on Signal Processing, 57(7):2479?
2493.
L. Xiao. 2009. Dual averaging methods for regular-
ized stochastic learning and online optimization. In
Advances in Neural Information Processing Systems.
M. Yuan and Y. Lin. 2006. Model selection and estima-
tion in regression with grouped variables. Journal of
the Royal Statistical Society (B), 68(1):49.
P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hi-
erarchical model selection through composite absolute
penalties. Annals of Statistics, 37(6A):3468?3497.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society Series B (Statistical Methodology),
67(2):301?320.
1511
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1357?1367, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Word Salad: Relating Food Prices and Descriptions
Victor Chahuneau Kevin Gimpel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,kgimpel}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Lily Scherlis
Phillips Academy
Andover, MA 01810, USA
lily.scherlis@gmail.com
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We investigate the use of language in food
writing, specifically on restaurant menus and
in customer reviews. Our approach is to build
predictive models of concrete external vari-
ables, such as restaurant menu prices. We
make use of a dataset of menus and customer
reviews for thousands of restaurants in several
U.S. cities. By focusing on prediction tasks
and doing our analysis at scale, our method-
ology allows quantitative, objective measure-
ments of the words and phrases used to de-
scribe food in restaurants. We also explore
interactions in language use between menu
prices and sentiment as expressed in user re-
views.
1 Introduction
What words might a menu writer use to justify the
high price of a steak? How does describing an item
as chargrilled vs. charbroiled affect its price? When
a customer writes an unfavorable review of a restau-
rant, how is her word choice affected by the restau-
rant?s prices? In this paper, we explore questions
like these that relate restaurant menus, prices, and
customer sentiment. Our goal is to understand how
language is used in the food domain, and we di-
rect our investigation using external variables such
as restaurant menu prices.
We build on a thread of NLP research that seeks
linguistic understanding by predicting real-world
quantities from text data. Recent examples include
prediction of stock volatility (Kogan et al 2009)
and movie revenues (Joshi et al 2010). There, pre-
diction tasks were used for quantitative evaluation
and objective model comparison, while analysis of
learned models gave insight about the social process
behind the data.
We echo this pattern here as we turn our atten-
tion to language use on restaurant menus and in user
restaurant reviews. We use data from a large cor-
pus of restaurant menus and reviews crawled from
the web and formulate several prediction tasks. In
addition to predicting menu prices, we also consider
predicting sentiment along with price.
The relationship between language and senti-
ment is an active area of investigation (Pang and
Lee, 2008). Much of this research has focused on
customer-written reviews of goods and services, and
perspectives have been gained on how sentiment is
expressed in this type of informal text. In addition
to sentiment, however, other variables are reflected
in a reviewer?s choice of words, such as the price of
the item under consideration. In this paper, we take
a step toward joint modeling of multiple variables
in review text, exploring connections between price
and sentiment in restaurant reviews.
Hence this paper contributes an exploratory data
1357
analysis of language used to describe food (by its
purveyors and by its consumers). While our primary
goal is to understand the language used in our cor-
pus, our findings bear relevance to economics and
hospitality research as well. This paper is a step on
the way to the eventual goal of using linguistic anal-
ysis to understand social phenomena like sales and
consumption.
2 Related Work
There are several areas of related work scattered
throughout linguistics, NLP, hospitality research,
and economics.
Freedman and Jurafsky (2011) studied the use of
language in food advertising, specifically the words
on potato chip bags. They argued that, due to
the ubiquity of food writing across cultures, eth-
nic groups, and social classes, studying the use of
language for describing food can provide perspec-
tive on how different socioeconomic groups self-
identify using language and how they are linguisti-
cally targeted. In particular, they showed that price
affects how ?authenticity? is realized in marketing
language, a point we return to in ?5. This is an ex-
ample of how price can affect how an underlying
variable is expressed in language. Among other ex-
plorations in this paper, we consider how price inter-
acts with expression of sentiment in user reviews of
restaurants.
As mentioned above, our work is related to re-
search in predicting real-world quantities using text
data (Koppel and Shtrimberg, 2006; Ghose et al
2007; Lerman et al 2008; Kogan et al 2009; Joshi
et al 2010; Eisenstein et al 2010; Eisenstein et
al., 2011; Yogatama et al 2011). Like much of
this prior work, we aim to learn how language is
used in a specific context while building models that
achieve competitive performance on a quantitative
prediction task.
Along these lines, there is recent interest in ex-
ploring the relationship between product sales and
user-generated text, particularly online product re-
views. For example, Ghose and Ipeirotis (2011)
studied the sales impact of particular properties of
review text, such as readability, the presence of
spelling errors, and the balance between subjective
and objective statements. Archak et al(2011) had a
similar goal but decomposed user reviews into parts
describing particular aspects of the product being
reviewed (Hu and Liu, 2004). Our paper differs
from price modeling based on product reviews in
several ways. We consider a large set of weakly-
related products instead of a homogeneous selection
of a few products, and the reviews in our dataset are
not product-centered but rather describe the overall
experience of visiting a restaurant. Consequently,
menu items are not always mentioned in reviews and
rarely appear with their exact names. This makes it
difficult to directly use review features in a pricing
model for individual menu items.
Menu planning and pricing has been studied for
many years by the culinary and hospitality research
community (Kasavana and Smith, 1982; Kelly et al
1994), often including recommendations for writing
menu item descriptions (Miller and Pavesic, 1996;
McVety et al 2008). Their guidelines frequently
include example menus from successful restaurants,
but typically do not use large corpora of menus or
automated analysis, as we do here. Other work
focused more specifically on particular aspects of
the language used on menus, such as the study by
Zwicky and Zwicky (1980), who made linguistic ob-
servations through manual analysis of a corpus of
200 menus.
Relatedly, Wansink et al(2001; 2005) showed
that the way that menu items are described af-
fects customers? perceptions and purchasing behav-
ior. When menu items are described evocatively,
customers choose them more often and report higher
satisfaction with quality and value, as compared to
when they are given the same items described with
conventional names. Wansink et aldid not use a
corpus, but rather conducted a small-scale experi-
ment in a working cafeteria with customers and col-
lected surveys to analyze consumer reaction. While
our goals are related, our experimental approach is
different, as we use automated analysis of thousands
of restaurant menus and rely on a set of one mil-
lion reviews as a surrogate for observing customer
behavior.
Finally, the connection between products and
prices is also a central issue in economics. How-
ever, the stunning heterogeneity in products makes
empirical work challenging. For example, there are
over 50,000 menu items in New York that include
1358
City # Restaurants # Menu Items # Reviews
train dev. test train dev. test train dev. test
Boston 930 107 113 63,422 8,426 8,409 80,309 10,976 11,511
Chicago 804 98 100 51,480 6,633 6,939 73,251 9,582 10,965
Los Angeles 624 80 68 17,980 2,938 1,592 75,455 13,227 5,716
New York 3,965 473 499 365,518 42,315 45,728 326,801 35,529 37,795
Philadelphia 1,015 129 117 83,818 11,777 9,295 52,275 7,347 5,790
San Francisco 1,908 255 234 103,954 12,871 12,510 499,984 59,378 67,010
Washington, D.C. 773 110 121 47,188 5,957 7,224 71,179 11,852 14,129
Total 10,019 1,252 1,252 733,360 90,917 91,697 1,179,254 147,891 152,916
Table 1: Dataset statistics.
the word chicken. What is the price of chicken? This
is an important practical and daunting matter when
measuring inflation (e.g., Consumer Price Index is
measured with a precisely-defined basket of goods).
Price dispersion across goods and the variation of
the goods is an important area of industrial organi-
zation economic theory. For example, economists
are interested in models of search, add-on pricing,
and obfuscation (Baye et al 2006; Ellison, 2005).
3 Data
We crawled Allmenus.com (www.allmenus.
com) to gather menus for restaurants in seven
U.S. cities: Boston, Chicago, Los Angeles, New
York, Philadelphia, San Francisco, and Washing-
ton, D.C. Each menu includes a list of item names
with optional text descriptions and prices. Most All-
menus restaurant pages contain a link to the cor-
responding page on Yelp (www.yelp.com) with
metadata and user reviews for the restaurant, which
we also collected.
The metadata consist of many fields for each
restaurant, which can be divided into three cate-
gories: location (city, neighborhood, transit stop),
services available (take-out, delivery, wifi, parking,
etc.), and ambience (good for groups, noise level,
attire, etc.). Also, the category of food and a price
range ($ to $$$$, indicating the price of a typical
meal at the restaurant) are indicated. The user re-
views include a star rating on a scale of 1 to 5.
The distribution of prices of individual menu
items is highly skewed, with a mean of $9.22 but
a median of $6.95. On average, a restaurant has
73 items on its menu with a median price of $8.69
and 119 Yelp reviews with a median rating of 3.55
????????
0
100k
200k
300k
400k
500k
  
 
 
 
 
 
 
 
 
star rating
??????? ????????
?????????????
????????????????????????????
???Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 91?101,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Measuring Ideological Proportions in Political Speeches
Yanchuan Sim? Brice D. L. Acree?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ysim,nasmith}@cs.cmu.edu
Justin H. Gross? Noah A. Smith?
?Department of Political Science
University of North Carolina at Chapel Hill
Chapel Hill, NC 27599, USA
{brice.acree,jhgross}@unc.edu
Abstract
We seek to measure political candidates? ideo-
logical positioning from their speeches. To ac-
complish this, we infer ideological cues from
a corpus of political writings annotated with
known ideologies. We then represent the
speeches of U.S. Presidential candidates as se-
quences of cues and lags (filler distinguished
only by its length in words). We apply a
domain-informed Bayesian HMM to infer the
proportions of ideologies each candidate uses
in each campaign. The results are validated
against a set of preregistered, domain expert-
authored hypotheses.
1 Introduction
The artful use of language is central to politics, and
the language of politicians has attracted consider-
able interest among scholars of political commu-
nication and rhetoric (Charteris-Black, 2005; Hart,
2009; Deirmeier et al, 2012; Hart et al, 2013)
and computational linguistics (Thomas et al, 2006;
Fader et al, 2007; Gerrish and Blei, 2011, in-
ter alia). In American politics, candidates for of-
fice give speeches and write books and manifestos
expounding their ideas. Every political season,
however, there are accusations of candidates ?flip-
flopping? on issues, with opinion shows, late-night
comedies, and talk radio hosts replaying clips of
candidates contradicting earlier statements. Pres-
idential candidate Mitt Romney?s own aide infa-
mously proclaimed in 2012: ?I think you hit a reset
button for the fall campaign [i.e., the general elec-
tion]. Everything changes. It?s almost like an Etch-
a-Sketch. You can kind of shake it up and we start
all over again.?
A more general observation, often stated but not
yet, to our knowledge, tested empirically, is that
successful primary candidates ?move to the cen-
ter? before a general election. The expectation fol-
lows directly from long-standing and widely influen-
tial theories of political competition that are collec-
tively referred to in their simplest form as the ?me-
dian voter theorem? (Hotelling, 1929; Black, 1948;
Downs, 1957). Thus it is to be expected that when
a set of voters that are more ideologically concen-
trated are replaced by a set who are more widely
dispersed across the ideological spectrum, as occurs
in the transition between the United States primary
and general elections, that candidates will present
themselves as more moderate in an effort to capture
enough votes to win.
Do political candidates in fact stray ideologically
at opportune moments? More specifically, can we
measure candidates? ideological positions from their
prose at different times? Following much work
on classifying the political ideology expressed by a
piece of text (Laver et al, 2003; Monroe and Maeda,
2004; Hillard et al, 2008), we start from the as-
sumption that a candidate?s choice of words and
phrases reflects a deliberate attempt to signal com-
mon cause with a target audience, and as a broader
strategy, to respond to political competitors. Our
central hypothesis is that, despite candidates? in-
tentional vagueness, differences in position?among
candidates or over time?can be automatically de-
tected and described as proportions of ideologies ex-
pressed in a speech.
In this work, we operationalize ideologies in a
novel empirical way, exploiting political writings
published in explicitly ideological books and mag-
azines (?2).1 The corpus then serves as evidence for
1We consider general positions in terms of broad ideolog-
ical groups that are widely discussed in current political dis-
course (e.g., ?Far Right,? ?Religious Right,? ?Libertarian,??
91
BACKGROUND
LEFT RIGHT
CENTER
PROGRESSIVE
RELIGIOUS LEFT
FAR LEFT
RELIGIOUS RIGHT
CENTER-LEFT
FAR RIGHT
CENTER-RIGHT
LIBERTARIAN
POPULIST
Figure 1: Ideology tree showing the labels for the ide-
ological corpus in ?2.1 (excluding BACKGROUND) and
corresponding to states in the HMM (?3.3).
a probabilistic model that allows us to automatically
infer compact, human-interpretable lexicons of cues
strongly associated with each ideology.
These lexicons are used, in turn, to create a low-
dimensional representation of political speeches: a
speech is a sequence of cues interspersed with lags.
Lags correspond to the lengths of sequences of non-
cue words, which are treated as irrelevant to the in-
ference problem at hand. In other words, a speech is
represented as a series alternating between cues sig-
naling ideological positions and uninteresting filler.
Our main contribution is a probabilistic technique
for inferring proportions of ideologies expressed by
a candidate (?3). The inputs to the model are the
cue-lag representation of a speech and a domain-
specific topology relating ideologies to each other.
The topology tree (shown in Figure 1) encoding
the closeness of different ideologies and, by exten-
sion, the odds of transitioning between them within a
speech. Bayesian inference is used to manage uncer-
tainty about the associations between cues and ide-
ologies, probabilities of traversing each of the tree?s
edges, and other parameters.
We demonstrate the usefulness of the measure-
ment model by showing that it accurately recov-
ers pre-registered beliefs regarding narratives widely
accepted?but not yet tested empirically?about the
2008 and 2012 U.S. Presidential elections (?4).
2 First Stage: Cue Extraction
We first present a data-driven technique for automat-
ically constructing ?cue lexicons? from texts labeled
with ideologies by domain experts.
etc.). Analysis of positions on specific issues is left for future
work.
Total tokens 32,835,190
Total types 138,235
Avg. tokens per book 77,628
Avg. tokens per mag. issue 31,713
Breakdown by ideology: Documents Tokens
LEFT 0 0
FAR LEFT 112 3,334,601
CENTER-LEFT 196 7,396,264
PROGRESSIVE LEFT 138 7,257,723
RELIGIOUS LEFT 7 487,844
CENTER 5 429,480
RIGHT 97 3,282,744
FAR RIGHT 211 7,392,163
LIBERTARIAN RIGHT 88 1,703,343
CENTER-RIGHT 9 702,444
POPULIST RIGHT 5 407,054
RELIGIOUS RIGHT 6 441,530
Table 1: Ideology corpus statistics. Note that some docu-
ments are not labeled with finer-grained ideologies.
2.1 Ideological Corpus
We start with a collection of contemporary political
writings whose authors are perceived as represen-
tative of one particular ideology. Our corpus con-
sists of two types of documents: books and maga-
zines. Books are usually written by a single author,
while each magazine consists of regularly published
issues with collections of articles written by several
authors. A political science domain expert who is
a co-author of this work manually labeled each ele-
ment in a collection of 112 books and 10 magazine
titles2 with one of three coarse ideologies: LEFT,
RIGHT, or CENTER. Documents that were labeled
LEFT and RIGHT were further broken down into
more fine-grained ideologies, shown in Fig. 1.3 Ta-
ble 1 summarizes key details about the ideological
corpus.
In addition to ideology labels, individual chapters
within the books were manually tagged with topics
that the chapter was about. For instance, in Barack
Obama?s book The Audacity of Hope, his chapter
2There are 765 magazine issues, which are published bi-
weekly to quarterly, depending on the magazine. All of a mag-
azine?s issues are labeled with the same ideology.
3We cannot claim that these texts are ?pure? examples of
the ideologies they are labeled with (i.e., they may contain parts
that do not match the label). By finding relatively few terms
strongly associated with texts sharing a label, our model should
be somewhat robust to impurities, focusing on those terms that
are indicative of whatever drew the expert to identify them as
(mostly) sharing an ideology.
92
titled ?Faith? is labeled as RELIGIOUS. Not all
chapters have clearly defined topics, and as such,
these chapters are simply labeled MISC. Maga-
zines are not labeled with topics because each issue
of a magazine generally touches on multiple top-
ics. There are a total of 61 topics; the full list can
be found in the supplementary materials, along with
a table summarizing key details about the corpus,
which contains 32.8 million tokens.
2.2 Cue Discovery Model
We use the ideological corpus to infer ideological
cues: terms that are strongly associated with an ide-
ology. Because our ideologies are organized hierar-
chically, we required a technique that can account
for multiple effects within a single text. We further
require that the sets of cue terms be small, so that
they can be inspected by domain experts. We there-
fore turn to the sparse additive generative (SAGE)
models introduced by Eisenstein et al (2011).
Like other probabilistic language models, SAGE
assigns probability to a text as if it were a bag of
terms. It differs from most language models in pa-
rameterizing the distribution using a generalized lin-
ear model, so that different effects on the log-odds
of terms are additive. In our case, we define the
probability of a term w conditioned on attributes of
the text in which it occurs. These attributes include
both the ideology and its coarsened version (e.g., a
FAR RIGHT book also has the attribute RIGHT).
For simplicity, let A(d) denote the set of attributes
of document d and A =
?
dA(d). The parametric
form of the distribution is given, for term w in doc-
ument d, by:
p(w | A(d);?) =
exp
(
?0w +
?
a?A(d) ?
a
w
)
Z(A(d),?)
Each of the ? weights can be a positive or negative
value influencing the probability of the word, condi-
tioned on various properties of the document. When
we stack an attribute a?s weights into a vector across
all words, we get an ?a vector, understood as an ef-
fect on the term distribution. (We use ? to refer to
the collection of all of these vectors.) The effects in
our model, described in terms of attributes, are:
? ?0, the background (log) frequencies of words,
fixed to the empirical frequencies in the corpus.
Hence the other effects can be understood as de-
viations from this background distribution.
? ?ic , the coarse ideology effect, which takes differ-
ent values for LEFT, RIGHT, and CENTER.
? ?if , the fine ideology effect, which takes different
values for the fine-grained ideologies correspond-
ing to the leaves in Fig. 1.
? ?t, the topic effect, taking different values for
each of the 61 manually assigned topics. We fur-
ther include one effect for each magazine series
(of which there are 10) to account for each maga-
zine?s idiosyncrasies (topical or otherwise).
? ?d, a document-specific effect, which captures id-
iosyncratic usage within a single document.
Note that the effects above are not mutually exclu-
sive, although some effects never appear together
due to constraints imposed by their semantics (e.g.,
no book is labeled both LEFT and RIGHT).
When estimating the parameters of the model (the
? vectors), we impose a sparsity-inducing `1 prior
that forces many weights to zero. The objective is:
max
?
?
d
?
w?d
log p(w | A(d);?)?
?
a?A
?a??a?1
This objective function is convex but requires spe-
cial treatment due to non-differentiability when any
elements are zero; we use the OWL-QN algorithm
to solve it (Andrew and Gao, 2007). To reduce the
complexity of the hyperparameter space (the possi-
ble values of all ?a) and to encourage similar levels
of sparsity across the different effect vectors, we let,
for each ideology attribute a,
?a = ? ? |V(a)| /maxa??A |V(a?)|
where V(a) is the set of term types appearing in
the data with attribute a (i.e., its vocabulary) , and
? is a hyperparameter we can adjust to control the
amount of sparsity in the SAGE vectors. For the
non-ideology effects, we fix ?a = 10 (not tuned).
2.3 Bigram and Trigram Lexicons
After estimating parameters, we are left with sparse
?a for each attribute. We are only interested, how-
ever, in the ideological attributes I ? A. For an
ideological attribute i ? I, we take the terms with
positive elements of this vector to be the cues for
ideology i; call this set L(i) and let L =
?
i?IL(i).
93
Because political texts use a fair amount of multi-
word jargon, we initially represented each document
as a bag of unigrams, bigrams, and trigrams, ignor-
ing the fact that these ?overlap? with each other.4
While this would be inappropriate in language mod-
eling and is inconsistent with our model?s indepen-
dence assumptions among words, it is sensible since
our goal is to identify cues that are statistically asso-
ciated with attributes like ideologies.
Preliminary trials revealed that unigrams tend to
dominate in such a model, since their frequency
counts are so much higher. Further, domain ex-
perts found them harder to interpret out of context
compared to bigrams and trigrams. We therefore in-
cluded only bigrams and trigrams as terms in our cue
discovery model.
2.4 Validation
The term selection method we have described can
be understood as a form of feature selection that
reasons globally about the data and tries to con-
trol for some effects that are not of interest (topic
or document idiosyncrasies). We compared the
approach to two classic, simple methods for fea-
ture selection: ranking based on pointwise mu-
tual information (PMI) and weighted average PMI
(WAPMI) (Schneider, 2005; Cover and Thomas,
2012). Selected features were used to classify the
ideologies of held-out documents from our cor-
pus.5 We evaluated these feature selection methods
within na??ve Bayes classification in a 5-fold cross-
validation setup. We vary ? for the SAGE model
and compare the results to equal-sized sets of terms
selected by PMI and WAPMI. We consider SAGE
with and without topic effects.
Figure 2 visualizes accuracy against the num-
ber of features for each method. Bigrams and
trigrams consistently outperform unigrams (McNe-
mar?s, p < 0.05). Otherwise, there are no sig-
nificant differences in performance except WAPMI
4Generative models that produce the same evidence more
than once are sometimes called ?deficient,? but model defi-
ciency does not necessarily imply that the model is ineffective.
Some of the IBM models for statistical machine translation pro-
vide a classic example (Brown et al, 1993).
5The text was tokenized and stopwords removed. Punctu-
ation, numbers, and web addresses were normalized. Tokens
appearing less than 20 times in training data, or in fewer than 5
documents were removed.
26 27 28 29 210 211 212 213 214 ?0.5
0.55
0.6
0.65
0.7
PMIWAPMI SAGESAGE w/ topics
Figure 2: Plot of average classification accuracy for
5-fold cross validation against the number of features.
Dashed lines refer to using only unigram features, while
solid lines refer to using bigram and trigram features.
with bigrams/trigrams at its highest point. SAGE
with topics is slightly (but not significantly) bet-
ter than without. We conclude that SAGE is a
competitive choice for cue discovery, noting that a
principled way of controlling for topical and doc-
ument effects?offered by SAGE but not the other
methods?may be even more relevant to our task
than classification accuracy.
2.5 Cue Lexicon
We ran SAGE on the the full ideological book cor-
pus, including topic effects, and setting ? = 30, ob-
tained a set of |L| = 8, 483 cue terms. The supple-
mentary materials include top cue terms associated
with various ideologies and a heatmap of similarities
among SAGE vectors.
We conducted a small, relatively informal study
in which seven subjects (including four scholars of
American politics) were asked to match brief de-
scriptions of the classes, including prominent proto-
typical individuals exemplifying each, to cue terms.
About 70% of ideologies were correctly matched
by experts, with relatively few confusions between
LEFT and RIGHT. More details are given in sup-
plementary materials.
3 Second Stage: Cue-Lag Ideological
Proportions
The main contribution of this paper is a technique
for measuring ideology proportions in the prose of
political candidates. We adopt a Bayesian approach
that manages our uncertainty about the cue lexi-
94
con L, the tendencies of political speakers to ?flip-
flop? among ideological types, and the relative ?dis-
tances? among different ideologies. The representa-
tion of a candidate?s ideology as a mixture among
discrete, hierarchically related categories can be dis-
tinguished from continuous representations (?scal-
ing? or ?spatial? models) often used in political sci-
ence, especially to infer positions from Congres-
sional roll-call voting patterns (Poole and Rosen-
thal, 1985; Poole and Rosenthal, 2000; Clinton
et al, 2004). Moreover, the ability to draw in-
ferences about individual policy-makers? ideologies
from their votes on proposed legislation is severely
limited by institutional constraints on the types of
legislation that is actually subject to recorded votes.
3.1 Political Speeches Corpus
We gathered transcribed speeches given by candi-
dates of the two main parties (Democrats and Re-
publicans) during the 2008 and 2012 Presidential
election seasons. Each election season is comprised
of two stages: (i) the primary elections, where can-
didates seek the support of their respective parties to
be nominated as the party?s Presidential candidate,
and (ii) the general elections where the parties? cho-
sen candidates travel across the states to garner sup-
port from all citizens. Each candidate?s speeches are
partitioned into epochs for each election; e.g., those
that occur before the candidate has secured enough
pledged delegates to win the party nomination are
?from the primary.? Table 2 presents a breakdown
of the candidates and speeches in our corpus.
3.2 Cue-Lag Representation
Our measurement model only considers ideological
cues; other terms are treated as filler. We therefore
transform each speech into a cue-lag representation.
The representation is a sequence of alternating
cues (elements from the ideological lexicon L) and
integer ?lags? (counts of non-cue terms falling be-
tween two cues). This will allow us to capture the in-
tuition that a candidate may use longer lags between
evocations of different ideologies, while nearby cues
are likely to be from similar ideologies.
To map a speech into the cue-lag representation,
we simply match all elements of L in the speech and
replace sequences of other words by their lengths.
When a trigram cue strictly includes a bigram cue,
Party Pri?08 Gen?08 Pri?12 Gen?12
Democrats? 167 - - -
Republicans? 50 - 49 -
Obama (D) 78 81 - 99
McCain (R) 9 159 - -
Romney (R) 8 ?(13) 19 19
?Democrats in our corpus are: Joe Biden, Hillary Clinton, John
Edwards, and Bill Richardson in 2008 and Barack Obama in
both 2008 and 2012.
?Republicans in our corpus are: Rudy Giuliani, Mike Huck-
abee, John McCain, and Fred Thompson in 2008, Michelle
Bachmann, Herman Cain, Newt Gingrich, Jon Huntsman, Rick
Perry, and Rick Santorum in 2012, and Ron Paul and Mitt Rom-
ney in both 2008 and 2012.
?For Romney, we have 13 speeches which he gave in the period
2008-2011 (between his withdrawal from the 2008 elections
and before the commencement of the 2012 elections). While
these speeches are not technically part of the regular Presiden-
tial election campaign, they can be seen as his preparation to-
wards the 2012 elections, which is particularly interesting as
Romney has been accused of having inconsistent viewpoints.
Table 2: Breakdown of number of speeches in our polit-
ical speech corpus by epoch. On average, 2,998 tokens,
and 95 cue terms are found in each speech document.
we take only the trigram. When two cues partially
overlap, we treat them as consecutive cue terms and
set the lag to 0. Figure 3 shows an example of our
cue-lag representation.
3.3 CLIP: An Ideology HMM
The model we use to infer ideologies, cue-lag ide-
ological proportions (CLIP), is a hidden Markov
model. Each state corresponds to an ideology
(Fig. 1) or BACKGROUND. The emission from a state
consists of (i) a cue from L and (ii) a lag value. The
high-level generative story for a single speech with
T cue-lag pairs is as follows:
1. Parameters are drawn from conjugate priors
(details in ?3.3.3).
2. Let the initial state be the BACKGROUND
state.
3. For t ? {1, 2, . . . , T}:6
(a) Transition to state St based on the
transition distribution, discussed in ?3.3.1.
This transition is conditioned on the previ-
ous state St?1 and the lag at timestep t?1,
denoted by Lt?1.
6The length of the sequence is assumed to be exogenous, so
that no stop state needs to be defined.
95
Original sentence Just compare this President?s record with Ronald Reagan?s first term. President Reagan also faced
an economic crisis. In fact, in 1982, the unemployment rate peaked at nearly 11 percent. But in the
two years that followed, he delivered a true recovery economic growth and job creation were three
times higher than in the Obama Economy.
Cue-lag representation . . .
6
?? ronald reagan
2
?? presid reagan
3
?? econom crisi
5
?? unemploy rate
17
?? econom growth
1
??
job creation
9
?? . . .
Figure 3: Example of the cue-lag representation.
(b) Emit cue term Wt from the lexicon L
and lag Lt based on the emission distribu-
tion, discussed in ?3.3.2.
We turn next to the transitions and emissions.
3.3.1 Ideology Topology and Transition
Parameterization
CLIP assumes that each cue term uttered by a
politician is generated from a hidden state corre-
sponding to an ideology. The ideologies are orga-
nized into a tree based on their hierarchical relation-
ships; see Fig. 1. In this study, the tree is fixed ac-
cording to our domain knowledge of current Ameri-
can politics; in future work it might be enriched with
greater detail or its structure learned automatically.
The ideology tree is used in defining the transition
distribution in the HMM, but not to directly define
the topology of the HMM. Importantly, each state
may transition to any other state, but the transition
distribution is defined using the graph, so that ide-
ologies that are closer to each other will tend to be
more likely to transition to each other. To transition
between two states si and sj , a walk must be taken
in the tree from vertex si to vertex sj . We emphasize
that the walk corresponds to a single transition?
the speaker does not emit anything from the states
passed through along the path.
A simplified version of our transition distribution,
for exposition, is given as follows:
ptree(sj | si; ?,?)
=
(?
?u,v??Path(si,sj)
(1? ?u)?u,v
)
?sj
Path(si, sj) refers to the sequence of edges in the
tree along the unique path from si to sj . Each of
these edges ?u, v? must be traversed, and the prob-
ability of doing so, conditioned on having already
reached u, is (1??u)?i.e., not stopping in u?times
?u,v?i.e., selecting vertex v from among those that
share an edge with u. Eventually, sj is reached, and
the walk ends, incurring probability ?sj .
In order to capture the intuition that a longer lag
after a cue term should increase the entropy over the
next ideology state, we introduce a restart probabil-
ity, which is conditioned on the length of the most
recent lag, `. The probability of restarting the walk
from the BACKGROUND state is a noisy-OR model
with parameter ?. This gives the transition distribu-
tion:
p(sj | si, `; ?,?, ?) = (1? ?)`+1ptree(sj | si; ?,?)
+ (1? (1? ?)`+1)ptree(sj | sBACKGROUND ; ?,?)
Note that, if ? = 1, there is no Markovian depen-
dency between states (i.e., there is always a restart),
so CLIP reverts to a mixture model.
This approach allows us to parameterize the full
set of |I|2 transitions with O(|I|) parameters.7 Since
the graph is a tree and the walks are not allowed
to backtrack, the only ambiguity in the transition
is due to the restart probability; this distinguishes
CLIP from other algorithms based on random walks
(Brin and Page, 1998; Mihalcea, 2005; Toutanova et
al., 2004; Collins-Thompson and Callan, 2005).
3.3.2 Emission Parameterization
Recall that, at time step t, CLIP emits a cue from
the lexicon L and an integer-valued lag. For each
state s, we let the probability of emitting cue w
be denoted by ?s,w; ?s is a multinomial distribu-
tion over the entire lexicon L. This allows our ap-
proach to handle ambiguous cues that can associate
with more than one ideology, and also to associate a
cue with a different ideology than our cue discovery
method proposed, if the signal from the data is suffi-
ciently strong. We assume each lag to be generated
by a Poisson distribution with global parameter ?.
7More precisely, there are |I| edges (since there are |I| + 1
vertices including BACKGROUND), each with a ?-parameter in
each direction. For a vertex with degree d, however, there are
only d?1 degrees of freedom, so that there are 2|I|?(|I|+1) =
|I|?1 degrees of freedom for ?. There are |I| ?-parameters and
a single ?, for a total of 2|I| degrees of freedom.
96
3.3.3 Inference and Learning
Above we described CLIP?s transitions and emis-
sions. Because our interest is in measuring
proportions?and, as we will see, in comparing
those proportions across speakers and campaign
periods?we require a way to allow variation in pa-
rameters across different conditions. Specifically,
we seek to measure differences in time spent in each
ideology state. This can be captured by allowing
each speaker to have a different ? and ? in each stage
of the campaign. On the other hand, we expect that a
speaker draws from his ideological lexicon similarly
across different epochs?there is a single ? shared
between different epochs.
In order to manage uncertainty about the param-
eters of CLIP, to incorporate prior beliefs based on
our ideology-specific cue lexicons {L(i)}i, and to
allow sharing of statistical strength across condi-
tions, we adopt a Bayesian approach to inference.
This will allow principled exploration of the poste-
rior distribution over the proportions of interest.
We place a symmetric Dirichlet prior on the tree
walk probabilities ?; its parameter is ?. For the
cue emission distribution associated with ideology
i, ?si , we use an informed Dirichlet prior with two
different values, ?cue for cues in L(i), and a smaller
?def for those in L \ L(i).8
Learning proceeds by collapsed Gibbs sampling
for the hidden states and slice sampling (with vague
priors) for the hyperparameters (?, ?, ?, and ?). De-
tails of the sampler are given in the supplementary
materials. At each Gibbs step, we resample the ide-
ology state and restart indicator variable for every
cue term in every speech.
We ran our Gibbs sampler for 75,000 iterations,
discarding the first 25,000 iterations for burn-in, and
collected samples at every 10 iterations. Further, we
perform the slice sampling step at every 5,000 itera-
tions. For each candidate, we collected 5,000 poste-
rior samples which we use to infer his/her ideologi-
cal proportions.
In order to determine the amount of time a candi-
date spends in each ideology, we denote the unit of
time in terms of half the lag before and after each cue
8This implies that a term can, in the posterior distribution,
be associated with an ideology i of whose L(i) it was not a
member. In fact, this occurred frequently in our runs of the
model.
term, i.e., when a candidate draws a cue term from
ideology i during timestep t, we say that he spends
1
2(Lt?1 + Lt) amount of time in ideology i. Aver-
aging over all the samples returned by our sampler
and normalizing it by the length of the documents in
each epoch, we obtain a candidate?s expected ideo-
logical proportions within the epoch.
4 Pre-registered Hypotheses
The traditional way to evaluate a text analysis model
in NLP is, of course, to evaluate its output against
gold-standard judgements by humans. In the case
of recent political speeches, however, we are doubt-
ful that such judgments can be made objectively at
a fine-grained level. While we are confident about
gross categorization of books and magazines in our
ideological corpus (?2.1), many of which are overtly
marked by their ideological assocations, we believe
that human estimates of ideological proportions, or
even association of particular tokens with ideologies
they may evoke, may be overly clouded by the vari-
ation in annotator ideology and domain expertise.
We therefore adopt a different method for evalua-
tion. Before running our model, we identified a set
of hypotheses, which we pre-registered as expec-
tations. These are categorized into groups based on
their strength and relevance to judging the validity of
the model. Strong hypotheses are those that consti-
tute the lowest bar for face validity; if violated, they
suggest a flaw in the model. Moderate hypotheses
are those that match the intuition of domain experts
conducting the research, or extant theory. Violations
suggest more examination is required, and may raise
the possibility that further testing might be pursued
to demonstrate the hypothesis is false. Our 13 prin-
cipal hypotheses are enumerated in Table 3.
5 Evaluation
We compare the posterior proportions inferred by
CLIP with several baselines:
? HMM: rather than ?3.3.1, a fully connected, tra-
ditional transition matrix is used.
? MIX: a mixture model; at each timestep, we al-
ways restart (? = 1). This eliminates Marko-
vian dependencies between ideologies at nearby
timesteps, but still uses the ideology tree in defin-
ing the probabilities of each state through ?.
97
Hypotheses CLIP HMM MIX NORES
Sanity checks (strong):
S1. Republican primary candidates should tend to draw more from RIGHT than
from LEFT.
*12/12 10/13 13/13 12/13
S2. Democratic primary candidates should tend to draw more from LEFT than
from RIGHT.
4/5 5/5 5/5 5/5
S3. In general elections, Democrats should draw more from the LEFT than the
Republicans and vice versa for the RIGHT.
4/4 4/4 3/4 0/4
S total 20/21 19/22 21/22 17/22
Primary hypotheses (strong):
P1. Romney, McCain and other Republicans should almost never draw from FAR
LEFT, and extremely rarely from PROGRESSIVE.
29/32 *21/31 27/32 29/32
P2. Romney should draw more heavily from the RIGHT than Obama in both stages
of the 2012 campaign.
2/2 2/2 1/2 1/2
Primary hypotheses (moderate):
P3. Romney should draw more heavily on words from the LIBERTARIAN,
POPULIST, RELIGIOUS RIGHT, and FAR RIGHT in the primary com-
pared to the general election. In the general election, Romney should draw
more heavily on CENTER, CENTER-RIGHT and LEFT vocabularies.
2/2 2/2 0/2 2/2
P4. Obama should draw more heavily on words from the PROGRESSIVE in the
2008 primary than in the 2008 general election.
0/1 0/1 0/1 1/1
P5. In the 2008 general election, Obama should draw more heavily on the
CENTER, CENTER-LEFT, and RIGHT vocabularies than in the 2008 primary.
1/1 1/1 1/1 1/1
P6. In the 2012 general election, Obama should sample more from the LEFT than
from the RIGHT, and should sample more from the LEFT vocabularies than
Romney.
2/2 2/2 0/2 0/2
P7. McCain should draw more heavily from the FAR RIGHT, POPULIST, and
LIBERTARIAN in the 2008 primary than in the 2008 general election.
0/1 1/1 1/1 1/1
P8. In the general 2008, McCain should draw more heavily from the CENTER,
CENTER-RIGHT, and LEFT vocabularies than in the 2008 primary.
1/1 1/1 1/1 1/1
P9. McCain should draw more heavily from the RIGHT than Obama in both stages
of the campaign.
2/2 2/2 2/2 1/2
P10.Obama and other Democrats should very rarely draw from FAR RIGHT. 6/7 5/7 7/7 4/7
P total 45/51 37/50 40/51 41/51
Table 3: Pre-registered hypotheses used to validate the measurement model; number of statements evaluated correctly
by different models. *Some differences were not significant at p = 0.05 and are not included in the results.
? NORES, where we never restart (? = 0). This
strengthens the Markovian dependencies.
In MIX, there are no temporal effects between cue
terms, although the structure of our ideology tree
encourages the speaker to draw from coarse-grained
ideologies over fine-grained ideologies. On the other
hand, the strong Markovian dependency between
states in NORES would encourage the model to stay
local within the ideology tree. In our experiments,
we will see how that the ideology tree and the ran-
dom treatment of restarting both contribute to our
model?s inferences.
Table 3 presents a summary of which hypothe-
ses the models? inferences are in accordance with.
CLIP is not consistently outperformed by any of the
competing baselines.
Sanity checks (S1?3) CLIP correctly identifies
sixteen LEFT/RIGHT alignments of primary candi-
dates (S1, S2), but is unable to determine one can-
didate?s orientation; it finds Jon Huntsman to spend
roughly equal proportions of speech-time drawing
on LEFT and RIGHT cue terms. Interestingly,
Huntsman, who had served as U.S. Ambassador to
China under Obama, was considered the one mod-
erate in the 2012 Republican field. MIX correctly
identifies all thirteen Republicans, while NORES
places McCain from the 2008 primaries as mostly
LEFT-leaning and HMM misses three of thirteen,
including Perry and Gingrich, who might be deeply
98
disturbed to find that they are misclassified as LEFT-
leaning. As for the Democratic primary candidates
(S2), CLIP?s one questionable finding is that John
Edwards spoke slightly more from the RIGHT than
the LEFT. For the general elections (S3), CLIP and
HMM correctly identify the relative amount of time
spent in LEFT/RIGHT between Obama and his Re-
publican competitors. NORES had the most trou-
ble, missing all four. CLIP finds Obama spend-
ing slightly more time on the RIGHT than on the
LEFT in the 2008 general elections but nevertheless,
Obama is still found to spend more time engaging in
LEFT-speak than McCain.
Name interference When we looked at the cue
terms actually used in the speeches, we found one
systematic issue: the inclusion of candidates? names
as cue terms. Terms mentioning John McCain are
associated with the RIGHT, so that Obama?s men-
tions of his opponent are taken as evidence for
rightward positioning; in total, mentions of McCain
contributed 4% absolute to Obama?s RIGHT ide-
ological proportion. Similarly, barack obama and
presid obama are LEFT cues (though senat obama
is a RIGHT cue). In future work, we believe filtering
candidate names in the first stage will be beneficial.
Strong hypotheses P1 and P2 CLIP and the vari-
ants making use of the ideology tree were in agree-
ment on most of the strong primary hypotheses.
Most of these involved our expectation that the
Republican candidates would rarely draw on FAR
LEFT and PROGRESSIVE LEFT. Our qualitative
hypotheses were not specific about how to quantify
?rare? or ?almost never.? We chose to find a result
inconsistent with a P1 hypothesis any time a Repub-
lican had proportions greater than 5% for either ide-
ology. The notable deviations for CLIP were Fred
Thompson (13% from the PROGRESSIVE LEFT
during the 2008 primary) and Mitt Romney (12%
from the PROGRESSIVE LEFT between the 2008
and 2012 elections, 13% from the FAR LEFT dur-
ing the 2012 general election). This model did no
worse than other variants here and much better than
one: HMM had 10 inconsistencies out of 32 oppor-
tunities, suggesting the importance of the ideology
tree.
M
cC
ai
n
Primaries 2008 General 2008Far LeftReligious (L)Center-Left
Center-Right
Libertarian (R)
Religious (R)
Progressive (L)Left
CenterRight
Populist (R)Far Right
R
om
ne
y
Primaries 2008 2008-2011 Primaries 2012 General 2012Far LeftReligious (L)
CenterCenter-Right
Libertarian (R)Religious (R)
Progressive (L)Left
Center-Left
Right
Populist (R)Far Right
O
ba
m
a
Primaries 2008 General 2008 General 2012Far Left
Religious (L)Left
Center-LeftCenter-Right
Libertarian (R)Populist (R)Religious (R)
Progressive (L)
Center
Right
Far Right
Figure 4: Proportion of time spent in each ideology by
McCain, Romney, and Obama during the 2008 and 2012
Presidential election seasons.
?Etch-a-Sketch? hypotheses Hypotheses P3, P4,
P5, P7, and P8 are all concerned with differences
between the primary and general elections: success-
ful primary candidates are expected to ?move to the
center.? A visualization of CLIP?s proportions for
McCain, Romney, and Obama is shown in Figure 4,
with their speeches grouped together by different
epochs. The model is in agreement with most of
these hypotheses. It did not confirm P4?Obama
appears to CLIP to be more PROGRESSIVE in the
2008 general election than in the primary, though the
difference is small (3%) and may be within the mar-
gin of error. Likewise, in P7, the difference between
McCain drawing from FAR RIGHT, POPULIST
and LIBERTARIAN between the 2008 primary and
general elections is only 2% and highly uncertain,
with a 95% credible interval of 44?50% during the
primary (vs. 47?50% in the general election).
Fine-grained ideologies Fine-grained ideologies
are expected to account for smaller proportions, so
that making predictions about them is quite difficult.
This is especially true for primary elections, where a
broader palette of ideologies is expected to be drawn
from, but we have fewer speeches from each candi-
99
date. CLIP?s inconsistency with P10, for example,
comes from assigning 5.4% of Obama?s 2008 pri-
mary cues to FAR RIGHT.
CLIP?s inferences on the corpus of political
speeches can be browsed at http://www.ark.
cs.cmu.edu/CLIP. We emphasize that CLIP
and its variants are intended to quantify the ideo-
logical content candidates express in speeches, not
necessarily their beliefs (which may not be perfectly
reflected in their words), or even how they are de-
scribed by pundits and analysts (who draw on far
more information than is expressed in speeches).
CLIP?s deviations from the hypotheses are sug-
gestive of potential improvements to cue extraction
(?2), but also of incorrect hypotheses. We expect
future research to explore a richer set of linguistic
cues and attributes beyond ideology (e.g., topics and
framing on various issues). We plan to use CLIP
as a text analysis method to support substantive in-
quiry in political science, such as following trends
in expressed ideology over time.
6 Related Work
As early as the 1960s, there has been research on
modeling ideological beliefs using automated sys-
tems (Abelson and Carroll, 1965; Carbonell, 1978;
Sack, 1994). These early works model ideology at a
sophisticated level, involving the actors, actions and
goals; they require manually constructed knowledge
bases. Poole and Rosenthal (1985) used congres-
sional roll call data to demonstrate the ideological
divide in Congress, and provided a methodology for
measuring ideological positions. Gerrish and Blei
(2011; 2012) augmented the methodology with text
from congressional bills using probabilistic models
to uncover lawmakers? positions on specific polit-
ical issues, putting them on a left-right spectrum,
while Thomas et al (2006) made use of floor de-
bate speeches to predict votes. Likewise, taking ad-
vantage of the proliferation of text today, numer-
ous techniques have been developed to identify top-
ics and perspectives in the media (Gentzkow and
Shapiro, 2005; Lin et al, 2008; Fortuna et al, 2009;
Gentzkow and Shapiro, 2010); determine the polit-
ical leanings of a document or author (Laver et al,
2003; Efron, 2004; Mullen and Malouf, 2006; Fader
et al, 2007); or recognize stances in debates (So-
masundaran and Wiebe, 2009; Anand et al, 2011).
Going beyong lexical indicators, Greene and Resnik
(2009) investigated syntactic features to identify per-
spectives or implicit sentiment.
7 Conclusions
We introduced CLIP, a domain-informed, Bayesian
model of ideological proportions in political lan-
guage. We showed how ideological cues could be
discovered from a lightly labeled corpus of ideolog-
ical writings, then incorporated into CLIP. The re-
sulting inferences are largely consistent with a set
of preregistered hypotheses about candidates in the
2008 and 2012 Presidential elections.
Acknowledgments
For thoughtful feedback on this research, the authors
thank: several anonymous reviewers, Amber Boydstun,
Philip Resnik, members of the ARK group at CMU, and
participants in Princeton University?s Political Methodol-
ogy Colloquium and PolMeth XXX hosted by The Uni-
versity of Virginia. This work was supported in part by an
A?STAR fellowship to Y. Sim, NSF grants IIS-1211201
and IIS-1211277, and Google?s support of the Reading is
Believing project at CMU.
References
Robert P. Abelson and J. Douglas Carroll. 1965. Com-
puter simulation of individual belief systems. Ameri-
can Behavioral Scientist, 8(9):24?30.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: Classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of ICML.
Duncan Black. 1948. On the rationale of group decision-
making. The Journal of Political Economy, 56(1):23?
34.
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1):107?117.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
100
Jaime G. Carbonell. 1978. Politics: Automated ideolog-
ical reasoning. Cognitive Science, 2(1):27?51.
Jonathan Charteris-Black. 2005. Politicians and
Rhetoric: The Persuasive Power of Metaphor.
Palgrave-MacMillan.
Joshua Clinton, Simon Jackman, and Douglas Rivers.
2004. The statistical analysis of roll call data. Ameri-
can Political Science Review, 98(2):355?370.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Query expansion using random walk models. In Pro-
ceedings of CIKM.
Thomas M. Cover and Joy A. Thomas. 2012. Elements
of Information Theory. Wiley-Interscience.
Daniel Deirmeier, Jean-Francois Godbout, Bei Yu, and
Stefan Kaufmann. 2012. Language and ideology
in congress. British Journal of Political Science,
42(1):31?55.
Anthony Downs. 1957. An Economic Theory of Democ-
racy. Harper, New York.
Miles Efron. 2004. Cultural orientation: Classifying
subjective documents by cociation analysis. In AAAI
Fall Symposium on Style and Meaning in Language,
Art, and Music.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Anthony Fader, Dragomir R. Radev, Michael H. Crespin,
Burt L. Monroe, Kevin M. Quinn, and Michael Co-
laresi. 2007. MavenRank: Identifying influential
members of the US senate using lexical centrality. In
Proceedings of EMNLP-CoNLL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statistical
learning methods. In Ashok N. Srivastava and Mehran
Sahami, editors, Text Mining: Classification, Cluster-
ing, and Applications, chapter 2, pages 27?50. Chap-
man & Hall/CRC.
Matthew Gentzkow and Jesse Shapiro. 2005. Media bias
and reputation. Technical report, National Bureau of
Economic Research.
Matthew Gentzkow and Jesse M. Shapiro. 2010. What
drives media slant? evidence from u.s. daily newspa-
pers. Econometrica, 78(1):35?71.
Sean M. Gerrish and David M. Blei. 2011. Predict-
ing legislative roll calls from text. In Proceedings of
ICML.
Sean M. Gerrish and David M. Blei. 2012. How they
vote: Issue-adjusted models of legislative behavior. In
Advances in NIPS 25.
Stephan Greene and Philip Resnik. 2009. More than
words: syntactic packaging and implicit sentiment. In
Proceedings of NAACL.
Roderick P. Hart, Jay P. Childers, and Colene J. Lind.
2013. Political Tone: How Leaders Talk and Why.
University of Chicago Press.
Roderick P. Hart. 2009. Campaign talk: Why elections
are good for us. Princeton University Press.
Dustin Hillard, Stephen Purpura, and John Wilker-
son. 2008. Computer-assisted topic classification for
mixed-methods social science research. Journal of In-
formation Technology & Politics, 4(4):31?46.
Harold Hotelling. 1929. Stability in competition. The
Economic Journal, 39(153):41?57.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. The American Political Science Review,
97(2):311?331.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
EMNLP.
Burt L. Monroe and Ko Maeda. 2004. Talk?s cheap:
Text-based estimation of rhetorical ideal-points. Pre-
sented at the Annual Meeting of the Society for Politi-
cal Methodology.
Tony Mullen and Robert Malouf. 2006. A preliminary
investigation into sentiment analysis of informal polit-
ical discourse. In AAAI Symposium on Computational
Approaches to Analysing Weblogs.
Keith T. Poole and Howard Rosenthal. 1985. A spatial
model for legislative roll call analysis. American Jour-
nal of Political Science, 29(2):357?384.
Keith T. Poole and Howard Rosenthal. 2000. Congress:
A Political-Economic History of Roll Call Voting. Ox-
ford University Press.
Warren Sack. 1994. Actor-role analysis: ideology, point
of view, and the news. Master?s thesis, Massachusetts
Institute of Technology, Cambridge, MA.
Karl-Michael Schneider. 2005. Weighted average point-
wise mutual information for feature selection in text
categorization. In Proceedings of PKDD.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
EMNLP.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In Proceed-
ings of ICML.
101
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1677?1687,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Translating into Morphologically Rich Languages with Synthetic Phrases
Victor Chahuneau Eva Schlinger Noah A. Smith Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,eschling,nasmith,cdyer}@cs.cmu.edu
Abstract
Translation into morphologically rich lan-
guages is an important but recalcitrant prob-
lem in MT. We present a simple and effec-
tive approach that deals with the problem in
two phases. First, a discriminative model is
learned to predict inflections of target words
from rich source-side annotations. Then, this
model is used to create additional sentence-
specific word- and phrase-level translations
that are added to a standard translation model
as ?synthetic? phrases. Our approach re-
lies on morphological analysis of the target
language, but we show that an unsupervised
Bayesian model of morphology can success-
fully be used in place of a supervised analyzer.
We report significant improvements in transla-
tion quality when translating from English to
Russian, Hebrew and Swahili.
1 Introduction
Machine translation into morphologically rich lan-
guages is challenging, due to lexical sparsity and the
large variety of grammatical features expressed with
morphology. In this paper, we introduce a method
that uses target language morphological grammars
(either hand-crafted or learned unsupervisedly) to
address this challenge and demonstrate its effective-
ness at improving translation from English into sev-
eral morphologically rich target languages.
Our approach decomposes the process of produc-
ing a translation for a word (or phrase) into two
steps. First, a meaning-bearing stem is chosen and
then an appropriate inflection is selected using a
feature-rich discriminative model that conditions on
the source context of the word being translated.
Rather than attempting to directly produce full-
sentence translations using such an elementary pro-
cess, we use our model to generate translations of
individual words and short phrases that augment?
on a sentence-by-sentence basis?the inventory of
translation rules obtained using standard translation
rule extraction techniques (Chiang, 2007). We call
these synthetic phrases.
The major advantages of our approach are: (i)
synthesized forms are targeted to a specific transla-
tion context; (ii) multiple, alternative phrases may
be generated with the final choice among rules left
to the global translation model; (iii) virtually no
language-specific engineering is necessary; (iv) any
phrase- or syntax-based decoder can be used with-
out modification; and (v) we can generate forms that
were not attested in the bilingual training data.
The paper is structured as follows. We first
present our ?translate-and-inflect? model for pre-
dicting lexical translations into morphologically rich
languages given a source word and its context (?2).
Our approach requires a morphological grammar to
relate surface forms to underlying ?stem, inflection?
pairs; we discuss how either a standard morpholog-
ical analyzer or a simple Bayesian unsupervised an-
alyzer can be used (?3). After describing an ef-
ficient parameter estimation procedure for the in-
flection model (?4), we employ the translate-and-
inflect model in an MT system. We describe
how we use our model to synthesize translation
options (?5) and then evaluate translation quality
on English?Russian, English?Hebrew, and English?
1677
Swahili translation tasks, finding significant im-
provements in all language pairs (?6). We finally
review related work (?7) and conclude (?8).
2 Translate-and-Inflect Model
The task of the translate-and-inflect model is illus-
trated in Fig. 1 for an English?Russian sentence pair.
The input will be a sentence e in the source language
(in this paper, always English) and any available lin-
guistic analysis of e. The output f will be composed
of (i) a sequence of stems, each denoted ? and (ii)
one morphological inflection pattern for each stem,
denoted ?. When the information is available, a
stem ? is composed of a lemma and an inflectional
class. Throughout, we use ?? to denote the set
of possible morphological inflection patterns for a
given stem ?. ?? might be defined by a grammar;
our models restrict ?? to be the set of inflections
observed anywhere in our monolingual or bilingual
training data as a realization of ?.1
We assume the availability of a deterministic
function that maps a stem ? and morphological in-
flection ? to a target language surface form f . In
some cases, such as our unsupervised approach in
?3.2, this will be a concatenation operation, though
finite-state transducers are traditionally used to de-
fine such relations (?3.1). We abstractly denote this
operation by ?: f = ? ? ?.
Our approach consists in defining a probabilistic
model over target words f . The model assumes in-
dependence between each target word f conditioned
on the source sentence e and its aligned position i in
this sentence.2 This assumption is further relaxed
in ?5 when the model is integrated in the translation
system.
We decompose the probability of generating each
target word f in the following way:
p(f | e, i) =
?
???=f
p(? | ei)
? ?? ?
gen. stem
? p(? | ?, e, i)
? ?? ?
gen. inflection
Here, each stem is generated independently from a
single aligned source word ei, but in practice we
1This prevents the model from generating words that would
be difficult for the language model to reliably score.
2This is the same assumption that Brown et al (1993) make
in, for example, IBM Model 1.
use a standard phrase-based model to generate se-
quences of stems and only the inflection model op-
erates word-by-word. We turn next to the inflection
model.
2.1 Modeling Inflection
In morphologically rich languages, each stem may
be combined with one or more inflectional mor-
phemes to express many different grammatical fea-
tures (e.g., case, definiteness, mood, tense, etc.).
Since the inflectional morphology of a word gen-
erally expresses multiple grammatical features, we
would like a model that naturally incorporates rich,
possibly overlapping features in its representation of
both the input (i.e., conditioning context) and out-
put (i.e., the inflection pattern). We therefore use
the following parametric form to model inflectional
probabilities:
u(?, e, i) = exp
[
?(e, i)>W?(?)+
?(?)>V?(?)
]
,
p(? | ?, e, i) =
u(?, e, i)
?
????? u(?
?, e, i)
. (1)
Here, ? is an m-dimensional source context fea-
ture vector function, ? is an n-dimensional mor-
phology feature vector function, W ? Rm?n and
V ? Rn?n are parameter matrices. As with the
more familiar log-linear parametrization that is writ-
ten with a single feature vector, single weight vec-
tor and single bias vector, this model is linear in its
parameters (it can be understood as working with
a feature space that is the outer product of the two
feature spaces). However, using two feature vectors
allows to define overlapping features of both the in-
put and the output, which is important for modeling
morphology in which output variables are naturally
expressed as bundles of features. The second term
in the sum in u enables correlations among output
features to be modeled independently of input, and
as such can be understood as a generalization of the
bias terms in multi-class logistic regression (on the
diagonal Vii) and interaction terms between output
variables in a conditional random field (off the diag-
onalVij).
1678
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
?:????????_V,+,?:mis2sfm2e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
root
-1 +1
??? ??? ???? ? ? ???? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN         TO   VB      DT     NN  IN  PRP$   NN
nsubj
aux
xcomp
?:????????_V,+,?:mis sfm2e
C50   C473        C28         C8    C275   C37   C43  C82 C94   C331
root
-1 +1
Figure 1: The inflection model predicts a form for the target verb lemma ? =???????? (pytat?sya) based on its
source attempted and the linear and syntactic source context. The correct inflection string for the observed Russian
form in this particular training instance is ? = mis-sfm-e (equivalent to the more traditional morphological string:
+MAIN+IND+PAST+SING+FEM+MEDIAL+PERF).
?
???
???
source aligned word ei
parent word epii with its dependency pii ? i
all children ej | pij = i with their dependency i? j
source words ei?1 and ei+1
?
???
???
?
?
?
token
part-of-speech tag
word cluster
?
?
?
? are ei, epii at the root of the dependency tree?
? number of children, siblings of ei
Figure 2: Source features ?(e, i) extracted from e and its linguistic analysis. pii denotes the parent of the token in
position i in the dependency tree and pii ? i the typed dependency link.
2.2 Source Context Features: ?(e, i)
In order to select the best inflection of a target-
language word, given the source word it translates
and the context of that source word, we seek to ex-
ploit as many features of the context as are avail-
able. Consider the example shown in Fig. 1, where
most of the inflection features of the Russian word
(past tense, singular number, and feminine gender)
can be inferred from the context of the English word
it is aligned to. Indeed, many grammatical functions
expressed morphologically in Russian are expressed
syntactically in English. Fortunately, high-quality
parsers and other linguistic analyzers are available
for English.
On the source side, we apply the following pro-
cessing steps:
? Part-of-speech tagging with a CRF tagger
trained on sections 02?21 of the Penn Tree-
bank.
? Dependency parsing with TurboParser (Mar-
tins et al, 2010), a non-projective dependency
parser trained on the Penn Treebank to produce
basic Stanford dependencies.
? Assignment of tokens to one of 600 Brown
clusters, trained on 8G words of English text.3
We then extract binary features from e using this
information, by considering the aligned source word
ei, its preceding and following words, and its syn-
tactic neighbors. These are detailed in Figure 2.
3 Morphological Grammars and Features
We now describe how to obtain morphological anal-
yses and convert them into feature vectors (?) for
our target languages, Russian, Hebrew, and Swahili,
using supervised and unsupervised methods.
3.1 Supervised Morphology
The state-of-the-art in morphological analysis uses
unweighted morphological transduction rules (usu-
3The entire monolingual data available for the translation
task of the 8th ACL Workshop on Statistical Machine Transla-
tion was used.
1679
ally in the form of an FST) to produce candidate
analyses for each word in a sentence and then sta-
tistical models to disambiguate among the analy-
ses in context (Hakkani-Tu?r et al, 2000; Hajic? et
al., 2001; Smith et al, 2005; Habash and Rambow,
2005, inter alia). While this technique is capable
of producing high quality linguistic analyses, it is
expensive to develop, requiring hand-crafted rule-
based analyzers and annotated corpora to train the
disambiguation models. As a result, such analyzers
are only available for a small number of languages,
and, as a practical matter, each analyzer (which re-
sulted from different development efforts) operates
differently from the others.
We therefore focus on using supervised analysis
for a single target language, Russian. We use the
analysis tool of Sharoff et al (2008) which produces
for each word in context a lemma and a fixed-length
morphological tag encoding the grammatical fea-
tures. We process the target side of the parallel data
with this tool to obtain the information necessary
to extract ?lemma, inflection? pairs, from which we
compute ? and morphological feature vectors ?(?).
Supervised morphology features: ?(?). Since
a positional tag set is used, it is straightforward to
convert each fixed-length tag ? into a feature vector
by defining a binary feature for each key-value pair
(e.g., Tense=past) composing the tag.
3.2 Unsupervised Morphology
Since many languages into which we might want to
translate do not have supervised morphological an-
alyzers, we now turn to the question of how to gen-
erate morphological analyses and features using an
unsupervised analyzer. We hypothesize that perfect
decomposition into rich linguistic structures may not
be required for accurate generation of new inflected
forms. We will test this hypothesis by experimenting
with a simple, unsupervised model of morphology
that segments words into sequences of morphemes,
assuming a (na??ve) concatenative generation process
and a single analysis per type.
Unsupervised morphological segmentation. We
assume that each word can be decomposed into any
number of prefixes, a stem, and any number of suf-
fixes. Formally, we let M represent the set of all
possible morphemes and define a regular grammar
M?MM? (i.e., zero or more prefixes, a stem, and
zero or more suffixes). To infer the decomposition
structure for the words in the target language, we as-
sume that the vocabulary was generated by the fol-
lowing process:
1. Sample morpheme distributions from symmet-
ric Dirichlet distributions: ?p ? Dir|M |(?p)
for prefixes, ?? ? Dir|M |(??) for stems, and
?s ? Dir|M |(?s) for suffixes.
2. Sample length distribution parameters
?p ? Beta(?p, ?p) for prefix sequences
and ?s ? Beta(?s, ?s) for suffix sequences.
3. Sample a vocabulary by creating each word
type w using the following steps:
(a) Sample affix sequence lengths:
lp ? Geometric(?p);
ls ? Geometric(?s).
(b) Sample lp prefixes p1, . . . , plp indepen-
dently from ?p; ls suffixes s1, . . . , sls in-
dependently from ?s; and a stem ? ? ??.
(c) Concatenate prefixes, the stem, and suf-
fixes: w = p1+? ? ?+plp+?+s1+? ? ?+sls .
We use blocked Gibbs sampling to sample seg-
mentations for each word in the training vocabulary.
Because of our particular choice of priors, it possible
to approximately decompose the posterior over the
arcs of a compact finite-state machine. Sampling a
segmentation or obtaining the most likely segmenta-
tion a posteriori then reduces to familiar FST opera-
tions. This model is reminiscent of work on learning
morphology using adaptor grammars (Johnson et al,
2006; Johnson, 2008).
The inferred morphological grammar is very sen-
sitive to the Dirichlet hyperparameters (?p, ?s, ??)
and these are, in turn, sensitive to the number of
types in the vocabulary. Using ?p, ?s  ??  1
tended to recover useful segmentations, but we have
not yet been able to find reliable generic priors for
these values. Therefore, we selected them empiri-
cally to obtain a stem vocabulary size on the parallel
data that is one-to-one with English.4 Future work
4Our default starting point was to use ?p = ?s =
10?6, ?? = 10?4 and then to adjust all parameters by factors
of 10.
1680
Table 1: Corpus statistics.
Parallel Parallel+Monolingual
Sentences EN-tokens TRG-tokens EN-types TRG-types Sentences TRG-tokens TRG-types
Russian 150k 3.5M 3.3M 131k 254k 20M 360M 1,971k
Hebrew 134k 2.7M 2.0M 48k 120k 806k 15M 316k
Swahili 15k 0.3M 0.3M 23k 35k 596k 13M 334k
will involve a more direct method for specifying or
inferring these values.
Unsupervised morphology features: ?(?). For
the unsupervised analyzer, we do not have a map-
ping from morphemes to structured morphological
attributes; however, we can create features from the
affix sequences obtained after morphological seg-
mentation. We produce binary features correspond-
ing to the content of each potential affixation posi-
tion relative to the stem:
prefix      suffix
...-3 -2 -1 STEM +1 +2 +3...
For example, the unsupervised analysis ? =
wa+ki+wa+STEM of the Swahili word wakiwapiga
will produce the following features:
?prefix[?3][wa](?) = 1,
?prefix[?2][ki](?) = 1,
?prefix[?1][wa](?) = 1.
4 Inflection Model Parameter Estimation
To set the parametersW andV of the inflection pre-
diction model (Eq. 1), we use stochastic gradient de-
scent to maximize the conditional log-likelihood of
a training set consisting of pairs of source (English)
sentence contextual features (?) and target word in-
flectional features (?). The training instances are
extracted from the word-aligned parallel corpus with
the English side preprocessed as discussed in ?2.2
and the target side disambiguated as discussed in ?3.
When morphological category information is avail-
able, we train an independent model for each open-
class category (in Russian, nouns, verbs, adjectives,
numerals, adverbs); otherwise a single model is used
for all words (excluding words less than four char-
acters long, which are ignored).
Statistics of the parallel corpora used to train the
inflection model are summarized in Table 1. It is
important to note here that our richly parameterized
model is trained on the full parallel training cor-
pus, not just on a handful of development sentences
(which are typically used to tune MT system param-
eters). Despite this scale, training is simple: the in-
flection model is trained to discriminate among dif-
ferent inflectional paradigms, not over all possible
target language sentences (Blunsom et al, 2008) or
learning from all observable rules (Subotin, 2011).
This makes the training problem relatively tractable:
all experiments in this paper were trained on a sin-
gle processor using a Cython implementation of the
SGD optimizer. For our largest model, trained on
3.3M Russian words, n = 231K ? m = 336 fea-
tures were produced, and 10 SGD iterations were
performed in less than 16 hours.
4.1 Intrinsic Evaluation
Before considering the broader problem of integrat-
ing the inflection model in a machine translation
system, we perform an artificial evaluation to ver-
ify that the model learns sensible source sentence-
target inflection patterns. To do so, we create an
inflection test set as follows. We preprocess the
source (English) sentences exactly as during train-
ing (?2.2), and using the target language morpholog-
ical analyzer, we convert each aligned target word to
?stem, inflection? pairs. We perform word alignment
on the held-out MT development data for each lan-
guage pair (cf. Table 1), exactly as if it were going to
produce training instances, but instead we use them
for testing.
Although the resulting dataset is noisy (e.g., due
to alignment errors), this becomes our intrinsic eval-
uation test set. Using this data, we measure inflec-
tion quality using two measurements:5
5Note that we are not evaluating the stem translation model,
1681
acc. ppl. |??|
S
up
er
vi
se
d
Russian
N 64.1% 3.46 9.16
V 63.7% 3.41 20.12
A 51.5% 6.24 19.56
M 73.0% 2.81 9.14
average 63.1% 3.98 14.49
U
ns
up
. Russian all 71.2% 2.15 4.73
Hebrew all 85.5% 1.49 2.55
Swahili all 78.2% 2.09 11.46
Table 2: Intrinsic evaluation of inflection model (N:
nouns, V: verbs, A: adjectives, M: numerals).
? the accuracy of predicting the inflection given
the source, source context and target stem, and
? the inflection model perplexity on the same set
of test instances.
Additionally, we report the average number of pos-
sible inflections for each stem, an upper bound to the
perplexity that indicates the inherent difficulty of the
task. The results of this evaluation are presented in
Table 2 for the three language pairs considered. We
remark on two patterns in these results. First, per-
plexity is substantially lower than the perplexity of a
uniform model, indicating our model is overall quite
effective at predicting inflections using source con-
text only. Second, in the supervised Russian results,
we see that predicting the inflections of adjectives
is relatively more difficult than for other parts-of-
speech. Since adjectives agree with the nouns they
modify in gender and case, and gender is an idiosyn-
cratic feature of Russian nouns (and therefore not
directly predictable from the English source), this
difficulty is unsurprising.
We can also inspect the weights learned by the
model to assess the effectiveness of the features
in relating source-context structure with target-side
morphology. Such an analysis is presented in Fig. 3.
4.2 Feature Ablation
Our inflection model makes use of numerous fea-
ture types. Table 3 explores the effect of removing
different kinds of (source) features from the model,
evaluated on predicting Russian inflections using
supervised morphological grammars.6 Rows 2?3
just the inflection prediction model.
6The models used in the feature ablation experiment were
trained on fewer examples, resulting in overall lower accuracies
show the effect of removing either linear or depen-
dency context. We see that both are necessary for
good performance; however removing dependency
context substantially degrades performance of the
model (we interpret this result as evidence that Rus-
sian morphological inflection captures grammatical
relationships that would be expressed structurally in
English). The bottom four rows explore the effect
of source language word representation. The results
indicate that lexical features are important for accu-
rate prediction of inflection, and that POS tags and
Brown clusters are likewise important, but they seem
to capture similar information (removing one has lit-
tle impact, but removing both substantially degrades
performance).
Table 3: Feature ablation experiments using supervised
Russian classification experiments.
Features (?(e, i)) acc.
all 54.7%
?linear context 52.7%
?dependency context 44.4%
?POS tags 54.5%
?Brown clusters 54.5%
?POS tags, ?Brown cl. 50.9%
?lexical items 51.2%
5 Synthetic Phrases
We turn now to translation; recall that our translate-
and-inflect model is used to augment the set of rules
available to a conventional statistical machine trans-
lation decoder. We refer to the phrases it produces
as synthetic phrases.
Our baseline system is a standard hierarchical
phrase-based translation model (Chiang, 2007). Fol-
lowing Lopez (2007), the training data is compiled
into an efficient binary representation which allows
extraction of sentence-specific grammars just before
decoding. In our case, this also allows the creation
of synthetic inflected phrases that are produced con-
ditioning on the sentence to translate.
To generate these synthetic phrases with new in-
flections possibly unseen in the parallel training
than seen in Table 2, but the pattern of results is the relevant
datapoint here.
1682
Russian supervised
Verb: 1st Person
child(nsubj)=I child(nsubj)=we
Verb: Future tense
child(aux)=MD child(aux)=will
Noun: Animate
source=animals/victims/...
Noun: Feminine gender
source=obama/economy/...
Noun: Dative case
parent(iobj)
Adjective: Genitive case
grandparent(poss)
Hebrew
Suffix ?? (masculine plural)
parent=NNS after=NNS
Prefix ? (first person sing. + future)
child(nsubj)=I child(aux)='ll
Prefix ? (preposition like/as)
child(prep)=IN parent=as
Suffix ? (possesive mark)
before=my child(poss)=my
Suffix ? (feminine mark)
child(nsubj)=she before=she
Prefix ?? (when)
before=when before=WRB
Swahili
Prefix li (past)
source=VBD source=VBN
Prefix nita (1st person sing. + future)
child(aux) child(nsubj)=I
Prefix ana (3rd person sing. + present)
source=VBZ
Prefix wa (3rd person plural)
before=they child(nsubj)=NNS
Suffix tu (1st person plural)
child(nsubj)=she before=she
Prefix ha (negative tense)
source=no after=not
Figure 3: Examples of highly weighted features learned by the inflection model. We selected a few frequent morpho-
logical features and show their top corresponding source context features.
data, we first construct an additional phrase-based
translation model on the parallel corpus prepro-
cessed to replace inflected surface words with their
stems. We then extract a set of non-gappy phrases
for each sentence (e.g., X ? <attempted,
???????? V>). The target side of each such phrase
is re-inflected, conditioned on the source sentence,
using the inflection model from ?2. Each stem is
given its most likely inflection.7
The original features extracted for the stemmed
phrase are conserved, and the following features
are added to help the decoder select good synthetic
phrases:
? a binary feature indicating that the phrase is
synthetic,
? the log-probability of the inflected forms ac-
cording to our model,
? the count of words that have been inflected,
with a separate feature for each morphological
category in the supervised case.
Finally, these synthetic phrases are combined with
the original translation rules obtained for the base-
line system to produce an extended sentence-specific
grammar which is used as input to the decoder. If a
7Several reviewers asked about what happens when k-best
inflections are added. The results for k ? {2, 4, 8} range from
no effect to an improvement over k = 1 of about 0.2 BLEU
(absolute). We hypothesize that larger values of k could have a
greater impact, perhaps in a more ?global? model of the target
string; however, exploration of this question is beyond the scope
of this paper.
phrase already existing in the standard phrase table
happens to be recreated, both phrases are kept and
will compete with each other with different features
in the decoder.
For example, for the large EN?RU system, 6%
of all the rules used for translation are synthetic
phrases, with 65% of these phrases being entirely
new rules.
6 Translation Experiments
We evaluate our approach in the standard discrim-
inative MT framework. We use cdec (Dyer et al,
2010) as our decoder and perform MIRA training
to learn feature weights of the sentence translation
model (Chiang, 2012). We compare the following
configurations:
? A baseline system, using a 4-gram language
model trained on the entire monolingual and
bilingual data available.
? An enriched system with a class-based n-gram
language model8 trained on the monolingual
data mapped to 600 Brown clusters. Class-
based language modeling is a strong baseline
for scenarios with high out-of-vocabulary rates
but in which large amounts of monolingual
target-language data are available.
? The enriched system further augmented with
our inflected synthetic phrases. We expect the
class-based language model to be especially
8For Swahili and Hebrew, n = 6; for Russian, n = 7.
1683
helpful here and capture some basic agreement
patterns that can be learned more easily on
dense clusters than from plain word sequences.
Detailed corpus statistics are given in Table 1:
? The Russian data consist of the News Com-
mentary parallel corpus and additional mono-
lingual data crawled from news websites.9
? The Hebrew parallel corpus is composed of
transcribed TED talks (Cettolo et al, 2012).
Additional monolingual news data is also used.
? The Swahili parallel corpus was obtained by
crawling the Global Voices project website10
for parallel articles. Additional monolingual
data was taken from the Helsinki Corpus of
Swahili.11
We evaluate translation quality by translating and
measuring the BLEU score of a 2000?3000 sentence-
long evaluation corpus, averaging the results over 3
MIRA runs to control for optimizer instability (Clark
et al, 2011). Table 4 reports the results. For all lan-
guages, using class language models improves over
the baseline. When synthetic phrases are added, sig-
nificant additional improvements are obtained. For
the English?Russian language pair, where both su-
pervised and unsupervised analyses can be obtained,
we notice that expert-crafted morphological analyz-
ers are more efficient at improving translation qual-
ity. Globally, the amount of improvement observed
varies depending on the language; this is most likely
indicative of the quality of unsupervised morpholog-
ical segmentations produced and the kinds of gram-
matical relations expressed morphologically.
Finally, to confirm the effectiveness of our ap-
proach as corpus size increases, we use our tech-
nique on top of a state-of-the art English?Russian
system trained on data from the 8th ACL Work-
shop on Machine Translation (30M words of bilin-
gual text and 410M words of monolingual text). The
setup is identical except for the addition of sparse
9http://www.statmt.org/wmt13/
translation-task.html
10http://sw.globalvoicesonline.org
11http://www.aakkl.helsinki.fi/cameel/
corpus/intro.htm
Table 4: Translation quality (measured by BLEU) aver-
aged over 3 MIRA runs.
EN?RU EN?HE EN?SW
Baseline 14.7?0.1 15.8?0.3 18.3?0.1
+Class LM 15.7?0.1 16.8?0.4 18.7?0.2
+Synthetic
unsupervised 16.2?0.1 17.6?0.1 19.0?0.1
supervised 16.7?0.1 ? ?
rule shape indicator features and bigram cluster fea-
tures. In these large scale conditions, the BLEU score
improves from 18.8 to 19.6 with the addition of word
clusters and reaches 20.0 with synthetic phrases.
Details regarding this system are reported in Ammar
et al (2013).
7 Related Work
Translation into morphologically rich languages is
a widely studied problem and there is a tremen-
dous amount of related work. Our technique of syn-
thesizing translation options to improve generation
of inflected forms is closely related to the factored
translation approach proposed by Koehn and Hoang
(2007); however, an important difference to that
work is that we use a discriminative model that con-
ditions on source context to make ?local? decisions
about what inflections may be used before combin-
ing the phrases into a complete sentence translation.
Combination pre-/post-processing solutions are
also frequently proposed. In these, the tar-
get language is generally transformed from multi-
morphemic surface words into smaller units more
amenable to direct translation, and then a post-
processing step is applied independent of the trans-
lation model. For example, Oflazer and El-Kahlout
(2007) experiment with partial morpheme groupings
to produce novel inflected forms when translating
into Turkish; Al-Haj and Lavie (2010) compare dif-
ferent processing schemes for Arabic. A related but
different approach is to enrich the source language
items with grammatical features (e.g., a source sen-
tence like John saw Mary is preprocessed into, e.g.,
John+subj saw+msubj+fobj Mary+obj) so as
to make the source and target lexicons have simi-
lar morphological contrasts (Avramidis and Koehn,
2008; Yeniterzi and Oflazer, 2010; Chang et al,
1684
2009). In general, this work suffers from the prob-
lem that it is extremely difficult to know a priori
what the right preprocessing is for a given language
pair, data size, and domain.
Several post-processing approaches have relied
on supervised classifiers to predict the optimal com-
plete inflection for an incomplete or lemmatized
translation. Minkov et al (2007) present a method
for predicting the inflection of Russian and Arabic
sentences aligned to English sentences. They train a
sequence model to predict target morphological fea-
tures from the lemmas and the syntactic structures
of both aligned sentences and demonstrate its ability
to recover accurately inflections on reference trans-
lations. Toutanova et al (2008) apply this method
to generate inflections after translation in two differ-
ent ways: by rescoring inflected n-best outputs or by
translating lemmas and re-inflecting them a posteri-
ori. El Kholy and Habash (2012) follow a similar
method and compare different approaches for gen-
erating rich morphology in Arabic after a transla-
tion step. Fraser et al (2012) observe improvements
for translation into German with a similar method.
As in that work, we model morphological features
rather than directly inflected forms. However, that
work may be criticized for providing no mechanism
to translate surface forms directly, even when evi-
dence for a direct translation is available in the par-
allel data.
Unsupervised morphology has begun to play a
role in translation between morphologically com-
plex languages. Stallard et al (2012) show that an
unsupervised approach to Arabic segmentation per-
forms as well as a supervised segmenter for source-
side preprocessing (in terms of English translation
quality). For translation into morphological rich lan-
guages, Clifton and Sarkar (2011) use an unsuper-
vised morphological analyzer to produce morpho-
logical affixes in Finnish, injecting some linguistic
knowledge in the generation process.
Several authors have proposed using conditional
models to predict the probability of phrase transla-
tion in context (Gimpel and Smith, 2008; Chan et
al., 2007; Carpuat and Wu, 2007; Jeong et al, 2010).
Of particular note is the work of Subotin (2011),
who use a conditional model to predict morpholog-
ical features conditioned on rich linguistic features;
however, this latter work also conditions on target
context, which substantially complicates decoding.
Finally, synthetic phrases have been used for
different purposes than generating morphology.
Callison-Burch et al (2006) expanded the cov-
erage of a phrase table by adding synthesized
phrases by paraphrasing source language phrases,
Chen et al (2011) produced ?fabricated? phrases
by paraphrasing both source and target phrases, and
Habash (2009) created new rules to handle out-of-
vocabulary words. In related work, Tsvetkov et al
(2013) used synthetic phrases to improve generation
of (in)definite articles when translating into English
from Russian and Czech, two languages which do
not lexically mark definiteness.
8 Conclusion
We have presented an efficient technique that ex-
ploits morphologically analyzed corpora to produce
new inflections possibly unseen in the bilingual
training data. Our method decomposes into two
simple independent steps involving well-understood
discriminative models.
By relying on source-side context to generate ad-
ditional local translation options and by leaving the
choice of the full sentence translation to the decoder,
we sidestep the difficulty of computing features on
target translations hypotheses. However, many mor-
phological processes (most notably, agreement) are
most best modeled using target language context. To
capture target context effects, we depend on strong
target language models. Therefore, an important
extension of our work is to explore the interaction
of our approach with more sophisticated language
models that more directly model morphology, e.g.,
the models of Bilmes and Kirchhoff (2003), or, alter-
natively, ways to incorporate target language context
in the inflection model.
We also achieve language independence by
exploiting unsupervised morphological segmen-
tations in the absence of linguistically informed
morphological analyses.
Code for replicating the experiments is available from
https://github.com/eschling/morphogen;
further details are available in (Schlinger et al, 2013).
1685
Acknowledgments
This work was supported by the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533. We would
like to thank Kim Spasaro for curating the Swahili devel-
opment and test sets, Yulia Tsvetkov for assistance with
Russian, and the anonymous reviewers for their helpful
comments.
References
Hassan Al-Haj and Alon Lavie. 2010. The im-
pact of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. In Proc. of AMTA.
Waleed Ammar, Victor Chahuneau, Michael Denkowski,
Greg Hanneman, Wang Ling, Austin Matthews, Ken-
ton Murray, Nicola Segall, Yulia Tsvetkov, Alon
Lavie, and Chris Dyer. 2013. The CMU machine
translation systems at WMT 2013: Syntax, synthetic
translation options, and pseudo-references. In Proc. of
WMT.
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proc. of ACL.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. of NAACL.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of EMNLP.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-
ning. 2009. Disambiguating ?DE? for Chinese?
English machine translation. In Proc. of WMT.
Boxing Chen, Roland Kuhn, and George Foster. 2011.
Semantic smoothing and fabrication of phrase pairs for
SMT. In Proc. of IWSLT.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159?1187.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. of ACL.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proc. of ACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc. of
ACL.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
predict or generate: Modeling rich morphology in sta-
tistical machine translation. In Proc. of EAMT.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word-
formation in SMT. In Proc. of EACL.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proc. of WMT.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine transla-
tion. In Proceedings of the 2nd International Confer-
ence on Arabic Language Resources and Tools.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, and
Vladim??r Petkevic?. 2001. Serial combination of rules
and statistics: A case study in Czech tagging. In Proc.
of ACL.
Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proc. of COLING.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and
Chris Quirk. 2010. A discriminative lexicon model
for complex morphology. In Proc. of AMTA.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2006. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
NIPS, pages 641?648.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proc. SIG-
MORPHON.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proc. of EMNLP.
1686
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proc. of EMNLP.
Andre? F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and Ma?rio A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Kemal Oflazer and I?lknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
WMT.
Eva Schlinger, Victor Chahuneau, and Chris Dyer. 2013.
morphogen: Translation into morphologically rich lan-
guages with synthetic phrases. Prague Bulletin of
Mathematical Linguistics, (100).
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a Russian tagset. In Proc. of LREC.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012.
Unsupervised morphology rivals supervised morphol-
ogy for Arabic MT. In Proc. of ACL.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proc. of ACL.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bha-
tia. 2013. Generating English determiners in phrase-
based translation with synthetic translation options. In
Proc. of WMT.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
Proc. of ACL.
1687
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1858?1868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Topics and Positions from Debatepedia
Swapna Gottipati? Minghui Qiu? Yanchuan Sim? Jing Jiang? Noah A. Smith?
?School of Information Systems, Singapore Management University, Singapore
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?{swapnag.2010,minghui.qiu.2010,jingjiang}@smu.edu.sg
?{ysim,nasmith}@cs.cmu.edu
Abstract
We explore Debatepedia, a community-
authored encyclopedia of sociopolitical de-
bates, as evidence for inferring a low-
dimensional, human-interpretable representa-
tion in the domain of issues and positions. We
introduce a generative model positing latent
topics and cross-cutting positions that gives
special treatment to person mentions and opin-
ion words. We evaluate the resulting repre-
sentation?s usefulness in attaching opinionated
documents to arguments and its consistency
with human judgments about positions.
1 Introduction
The social web has evolved into a forum for large
portions of the population to discuss and debate
complex issues of societal importance. Websites like
Debatepedia,1 an online, community-authored ency-
clopedia of debates (?2), seek to organize some of
this exchange into structured information resources
that summarize arguments and link externally to
texts (editorials, blog posts, etc.) that express and
evoke them. Empirical NLP, we propose, has a
role to play in creating a more compact and easily-
interpretable way to understand the opinion space.
In particular, we envision applications to computa-
tional journalism, where there is high demand for
transformation of and pattern discovery in unman-
ageable, unstructured, evolving data (including text)
to inform the public (Cohen et al, 2011).
In this paper, we develop a generative model for
discovering such a representation (?3), using De-
batepedia as a corpus of evidence. We draw in-
spiration from Lin et al (2008) and Ahmed and
1http://dbp.idebate.org
Xing (2010), who used generative models to infer
topics?distributions over words?and other word-
associated variables representing perspectives or
ideologies. We view topics as lexicons, and propose
that grounding a topic model with evidence beyond
bags of words can lead to more lexicon-like repre-
sentations. Specifically, our generative topic model
grounds topics using the hierarchical organization
of arguments within Debatepedia. Further, we use
named entity recognition as a preprocessing step, an
existing sentiment lexicon to construct an informed
prior, and we incorporate a latent, discrete position
variable that cuts across debates.2
We evaluate the model informally and formally
(?4). Subjectively, the model identifies reasonable
topic and perspective terms, and it associates topics
sensibly with important public figures. In quanti-
tative evaluations, we find the model?s representa-
tion superior to topics from vanilla latent Dirichlet
allocation (Blei et al, 2003) and the joint sentiment
topic model (Lin and He, 2009) in matching external
texts to debates. Further, the position variables can
be used to infer the side of an argument within a de-
bate; our model performs with an accuracy of 86%
on position prediction of the debate argument. The
cross-cutting position variable is not especially con-
sistent with human judgments, suggesting that fur-
ther knowledge sources may be required to improve
interpretability across issues.
2 Data
Debatepedia, like Wikipedia, is constructed by vol-
unteer contributors and has a system of community
2This variable might serve to cluster debate sides according
to ?abstract beliefs commonly shared by a group of people,?
sometimes called ideologies (Van Dijk, 1998). We do not claim
that our model infers ideologies (see ?4).
1858
Debate: Gun control; should laws be passed to limit gun ownership further?
Question: Self-defense ? Is self-defense a good reason for gun ownership?
Side: Yes Side: No
Argument: A citizen has a ?right? to guns as a means
to self-defense: Many groups argue that a citizen
should have the ?right? to defend themselves, and that
a gun is frequently the . . .
Argument: The protection of property is not a good
justification for yielding a lethal weapon. While peo-
ple have a right to their property, this should not justify
wielding a lethal . . .
Argument: Gun restrictions and bans disadvantage cit-
izens against armed criminals. Citizens that are not al-
lowed to carry guns are disadvantaged against lawless
criminals that . . .
Argument: Robert F. Drinan, Former Democratic US
Congressman, ?Gun Control: The Good Outweighs
the Evil?, 1976 ? ?These graphic examples of individ-
ual instances of . . .
Question: Economic benefits ? Is gun control economically beneficial?
Side: Yes Side: No
Argument: Lax gun control laws are economically
costly. The Coalition for Gun Control claims that, ?in
Canada, the costs of firearms death and injury alone
have been estimated at . . .
Argument: Gun sports have economic benefits. Field
sports bring money into poor rural economies and pro-
vide a motivation for landowners to value environmen-
tal protection.
Table 1: An example of a Debatepedia debate on the topic ?Gun control.?
moderation. Many of the debate issues covered are
controversial and salient in current public discourse.
Because it is primarily expressed as text, Debatepe-
dia is a corpus of debate topics, but it is organized
hierarchically, with multiple issues in each debate
topic, questions within each issue, and arguments on
two sides of each question. An important feature of
the corpus is the widespread quotation and linking to
external articles on the web, including news stories,
blog postings, wiki pages, and social media forums;
here we use these external articles in evaluation (?4).
Table 1 shows excerpts from a debate page3 from
Debatepedia. Each debate contains ?questions,?
which reflect the different aspects of a debate. In this
particular debate, there are 13 questions (2 shown),
ranging from economic benefits to enforceability to
social impacts. For each question, there are two dis-
tinct sides, each with its own set of supporting argu-
ments. Many of these arguments also contains links
to online articles where the quotes are extracted from
(not shown in Table 1). For example, in the second
argument on the ?No? side, there is an inline link to
the article written by Congressman Drinan.4
Within a debate topic, the sides cut across differ-
ent questions, aligning arguments together. In gen-
3http://dbp.idebate.org/en/index.php/
Debate:_Gun_control
4http://www.saf.org/LawReviews/Drinan1.
html
Debates 1,303
Arguments 33,556
Articles linked by exactly one argument 3,352
Tokens 1,710,814
Types (excluding NE mentions) 59,601
Person named entity mentions 9,496
Table 2: Debatepedia corpus statistics. Types and tokens
include unigrams, bigrams and person named entities.
eral, the questions are phrased so that a consistent
?pro? and ?con? structure is apparent throughout
each debate, aligned to a high-level question (i.e.,
the ?Yes? sides of all the questions are consistent
with the same side of the larger debate). The ex-
ample of Table 1 deviates from this pattern, with the
self-defense ?Yes? arguing ?no? to the high-level de-
bate question?Should laws be passed to limit gun
ownership further??and the economic ?Yes? argu-
ing ?yes? to the high-level question.
Table 2 presents statistics of our corpus.
2.1 Preprocessing
We scraped the Debatepedia website and extracted
the debate, question, argument, and side structure
of the debate topics. We crawled the external
web articles that were linked from the Debatepe-
dia arguments. For the web articles, we extracted
the main text content (ignoring boilerplate elements
such as navigation and advertisments) using Boil-
1859
erpipe (Kohlschu?tter et al, 2010).5 We tokenized
the text and filtered stopwords.6 We considered both
unigrams and bigrams in our model, keeping all uni-
grams and removing bigram types that appeared less
than 5 times in the corpus. Although our modeling
approach ultimately treats texts as bags of terms (un-
igrams and bigrams), one important preprocessing
step was taken to further improve the interpretabil-
ity of the inferred representation: named entity men-
tions of persons. We identified these mentions of
persons using Stanford NER (Finkel et al, 2005)
and treated each person mention as a single token. In
our qualitative analysis of the model (?4.2), we will
show how this special treatment of person mentions
enables the association of well-known individuals
with debate topics. Though not part of our exper-
imental evaluation in this paper, such associations
are, we believe, an interesting direction for future
applications of the model.
3 Model
Our model defines a probability distribution over
terms7 that are observed in the corpus. Each term
occurs in a context defined by the tuple ?d, q, s, a?
(respectively, a debate, a question within the debate,
a side within the debate, and an argument). At each
level of the hierarchy is a different latent variable:
? Each question q within debate d is associated
with a distribution over topics, denoted ?d,q.8
? Each side s of the debate d is associated with a
position, denoted id,s and we posit a global dis-
tribution ? that cuts across different questions
and arguments. In our experiments, there are
two positions, and the two sides of a debate
are constrained to associate with opposing po-
sitions. As illustrated by Table 1, this assump-
5http://code.google.com/p/boilerpipe
6www.ranks.nl/resources/stopwords.html
7Recall that our model includes bigrams. We treat each un-
igram and bigram token (after filtering discussed in ?2.1) as a
separate term.
8In future work, more sharing across questions within a
debate, or more differentiation among the topic distributions
for arguments under a question, might be explored. Wallach
(2006) describes suitable techniques using hierarchical Dirich-
let draws, and Eisenstein et al (2011) suggests the use of sparse
shocks to log-odds at different levels. Here we work on the
assumption that Debatepedia?s questions are the most topically
coherent level, and work with a single topic mixture at this level.
wz y
Nd,q,s,a
Ad,q,s
?
?
Qd
?
?
?tt?
o
i,t?ii ?et
K TKT
?b
?i ?o ?t ?e
?b
i
?
?
Sd
D
Figure 1: Plate diagram. K is the number of positions,
and T is number of topics. The shaded variables are ob-
served and dashed variables are marginalized. ?,?,?
and all ? are fixed hyperparameters (?3.1).
tion is not always correct, though it tends to
hold most of the time.
? Each term wd,q,s,a,n (n is the position index
of the term within an argument) is associated
with one of five functional term types, denoted
yd,q,s,a,n. This variable is latent, except when it
takes the value ?entity? (e) for terms marked as
named entity mentions. When it is not an en-
tity, it takes one of the other four values: ?gen-
eral position? (i), ?topic-specific position? (o),
?topic? (t), or ?background? (b). Thus, every
term w is drawn from one of these 5 types of
bags, and y acts as a switching variable to se-
lect the type of bag.
? For some term types (the ones where y ?
{o, t}), each term wd,q,s,a,n is associated with
one of T discrete topics, as indexed by
zd,q,s,a,n.
Figure 1 illustrates the plate diagram for the
graphical model underlying our approach. The gen-
erative story is given in Figure 2.
3.1 Priors
Typical probabilistic topic models assume a sym-
metric Dirichlet prior over its term distributions or
1860
1. ? topics t, draw topic-term distribution ?tt ? Dirichlet(?t) and topic-entity distribution ?et ? Dirichlet(?e).
2. ? positions i, draw position-term distribution ?ii ? Dirichlet(?i).
3. ? topics t, ? positions i, draw topic-position term distribution ?oi,t ? Dirichlet(?o).
4. Draw background term distribution ?b ? Dirichlet(?b).
5. Draw functional term type distribution ? ? Dirichlet(?).
6. Draw position distribution ? ? Dirichlet(?).
7. ? debates d:
a. Draw id,1, id,2 ? Multinomial(?), assigning each of the two sides to a position.
b. ? questions q in d:
i. Draw topic mixture proportions ?d,q ? Dirichlet(?).
ii. ? arguments a under question q and term positions n in a:
A. Draw topic label zd,q,s,a ? Multinomial(?d,q).
B. Draw functional term type yd,q,s,a ? Multinomial(?).
C. Draw term wd,q,s,a ? Multinomial (?yd,q,s,a | id,1, id,2, zd,q,s,a).
Figure 2: Generative story for our model of Debatepedia.
apply empirical Bayesian techniques to estimate the
hyperparameters. Motivated by past efforts to ex-
ploit prior knowledge (Zhao et al, 2010; Lin and
He, 2009), we use the OpinionFinder sentiment lex-
icon9 (Wilson et al, 2005) to construct ?i and ?o.
Specifically, terms w in the lexicon were given pa-
rameters ?iw = ?ow = 0.01, and other terms were
given ?iw = ?ow = 0.001, capturing our prior belief
that opinion-expressing terms are likely to be used
in expressing positions. 5,451 types were given a
?boost? through this prior.
Information retrieval has long exploited the ob-
servation that a term?s document frequency (i.e., the
number of documents a term occurs in) is inversely
related its usefulness in retrieval (Jones, 1972). We
encode this in ?b, the prior over the background
term distribution, by setting each value to the log-
arithm of the term?s argument frequency.
The other priors were set to be symmetric: ?e =
0.01 (entity topics), ?t = 0.001 (topics), ? =
50/T = 1.25 (topic mixture coefficients), ? = 0.01
(positions), and ? = 0.01 (functional term types).
Preliminary tests showed that final topics are rela-
tively insensitive to the values of the hyperparame-
ters.
3.2 Inference and Parameter Estimation
Exact inference under this model, like most latent-
variable topic models, is intractable. We apply col-
lapsed Gibbs sampling, a standard approach for such
9http://mpqa.cs.pitt.edu/lexicons/subj_
lexicon/
models (Griffiths and Steyvers, 2004).10 The no-
table deviations from typical uses of collapsed Gibbs
sampling are: (i) we jointly sample id,1 and id,2 to
respect the constraint that they differ; and (ii) we
fix the priors, in some cases to be asymmetric, as
discussed in ?3.1. We perform Gibbs sampling for
2,000 iterations over the dataset, discarding the first
500 iterations for burn-in, and averaging over every
10th iteration thereafter to get estimates for our term
distributions.
3.3 T andK
In all experiments, we use T = 40 topics andK = 2
positions. We did not extensively explore different
values for T and K; preliminary exploration sug-
gested that interpretability, gauged informally by the
authors, degraded for higher values of either.
4 Evaluation
Recall that the aim of this work is to infer a low-
dimensional representation of debate text. We esti-
mated our model on the Debatepedia debates (not in-
cluding hyperlinked articles), and conducted several
evaluations of the model, each considering a differ-
ent aspect of the goal. We exploit external articles
hyperlinked from Debatepedia described in ?2 as
supporting texts for arguments, treating each one?s
association to an argument as variable to be pre-
dicted. Firstly, we evaluate our model on the article
associating task. Secondly, we evaluate our model
on the position prediction task. Then, we compare
10Because this technique is well known in NLP, details are
relegated to supplementary material.
1861
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
N
o 
of
 A
rti
cle
s
JS Divergence
LDA
JST
Our Model
Figure 3: The distribution over Jensen-Shannon diver-
gences between a hyperlinked article and the correspond-
ing Debatepedia argument, n = 3, 352.
our model?s positional assignment of arguments to
human annotated clusterings. Finally, we present
qualitative discussion.
4.1 Quantitative Evaluation
4.1.1 Topics
As described in ?2, our corpus includes 3,352 ar-
ticles hyperlinked by Debatepedia arguments.11 Our
model can be used to infer the posterior over top-
ics associated with such an article, and we compare
that distribution to that of the Debatepedia article
that links to it. Calculating the similarity of these
distributions, we get an estimate of how closely our
model can associate text related to a debate with the
specific argument that linked to it. We compare with
LDA (Blei et al, 2003), which ignores sentiment,
and the joint sentiment topic (JST) model (Lin and
He, 2009), an unsupervised model that jointly cap-
tures sentiment and topic.12 Using Jensen-Shannon
divergence, we find that our approach embeds these
pairs significantly closer than LDA and JST (also
trained with 40 topics), under a Wilcoxon signed
rank test (p < 0.001). Figure 3 shows the histogram
of divergences between our model, JST, and LDA.
Associating external articles. More challenging,
of course, is selecting the argument to which an
external article should be associated. We used the
Jensen-Shannon divergence between topic distribu-
tions of articles and arguments to rank the latter,
for each article. The mean reciprocal rank scores
(Voorhees, 1999) for LDA, JST, and our model were
11We consider only those articles linked by a single Debate-
pedia argument.
12JST multiplies topics out by the set of sentiment labels, as-
signing each token to both a topic and a sentment. We use the
OpinionFinder lexicon in JST?s prior in the same way it is used
in our model.
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
5 10 15 20 25 inf
M
R
R
K
LDA
JST
Our Model
Figure 4: Mean reciprocal ranks for the association task.
0.1272, 0.1421, and 0.1507, respectively; the differ-
ence is significant (Wilcoxon signed rank test, p <
0.001). We found the same pattern for MRR@k,
k ? {5, 10, 15, 20, 25,?}, as shown in Figure 4.
It is likely possible to engineer more accurate
models for attaching articles to arguments, but the
attachment task is our aim only insofar as it con-
tributes to an overall assessment of an inferred rep-
resentation?s quality.
4.1.2 Positions
Positional distance by topic. We next consider
the JS divergences of position term distributions by
topic; for each topic t, we consider the divergence
between inferred values for ?o1,t and ?o2,t. Figure 5
shows these measurements sorted from most to least
different; these might be taken as evidence for which
issue areas? arguments are more lexically distin-
guishable by side, perhaps indicating less common
ground in discourse or (more speculatively) greater
controversy. For example, our model suggests that
debates relating to topics like presidential politics,
foreign policy, teachers, women?s health, religion,
and Israel/Palestine are more heated (within the De-
batepedia community at the time the debates took
place) than those about the minimum wage, Iran as
a nuclear threat, or immigration.
Predicting positions for arguments. We tested
our model?s ability to infer the positions of argu-
ments. In this experiment (only), we held out 3,000
arguments during parameter estimation. The held-
out arguments were selected so that every debate
side maintained at least one argument whose in-
ferred side could serve as the correct answer for the
held-out argument. We then inferred i for each held-
out argument from debate d and side s, given the
parameters, and compared it with the value of id,s
inferred during parameter estimation. The model
achieved 86% accuracy (Table 3 shows the confu-
1862
sion matrix). Note that JST does not provide a base-
line for comparison, since it does not capture debate
sides.
i = 1 i = 2
i? = 1 1,272 216
i? = 2 199 1,313
Table 3: Confusion matrix for position prediction on
held-out arguments.
Predicting positions for external articles. We
can also use the model to predict the position
adopted in an external text. For articles linked from
within Debatepedia, we have a gold standard: from
which side of a debate was it linked? After using
the model to infer a position variable for such a text,
we can check whether the inferred position variable
matches that of the argument that links to it. Table 4
shows that our model does not successfully com-
plete this task, assigning about 60% of both kinds
of articles i = 1.
i = 1 i = 2
i? = 1 1,042 623
i? = 2 1,043 644
Table 4: Confusion matrix for position prediction on hy-
perlinked articles.
Genre. We manually labeled 500 of these articles
into six genre categories. We had two annotators for
this task (Cohen?s ? = 0.856). These categories,
in increasing order of average Jensen-Shannon di-
vergence, are: blogs, editorials, wiki pages, news,
other, and government. Figure 6 shows the results.
While the only difference between the first and last
groups are surprising by chance, we are encouraged
by our model?s suggestion that blogs and editori-
als may be more ?Debatepedia argument-like? than
news and government articles.
Note that our model is learned only from text
within Debatepedia; it does not observe the text of
external linked articles. Future work might incorpo-
rate this text as additional evidence in order to cap-
ture effects on language stemming from the interac-
tion of position and genre.
0.1 0.2 0.3 0.4 0.5comment, minimum, wage, poverty, capitalism
nuclear, weapons, iran, states, threatparty, vote, republican, political, voters
energy, gas, power, fuel, windtax, economic, trade, cost, percent
immigration, cameras, police, immigrants, crimepeople, dont, time, lot, make
food, consumers, products, calorie, informationdeath, crime, punishment, penalty, justice
marijuana, drug, drugs, alcohol, agemarriage, gay, mars, space, moon
rights, law, people, individual, amendmentsouth, kosovo, independence, state, republic
human, rights, animals, life, animalchildren, child, sex, parents, sexual
school, schools, students, education, publicchina, tibet, chinese, people, tibetan
global, emissions, climate, carbon, warminginternational, court, war, crimes, icc
english, language, violence, people, videoorleans, euthanasia, city, suicide, priests
speech, corporations, corporate, public, moneyhealth, care, insurance, public, private
circumcision, men, sexual, circumcised, foreskininformation, torture, science, evidence, wikipedia
companies, market, industry, business, bailoutlaw, workers, union, rights, legal
college, cloning, game, football, incesttimes, york, ban, june, january
countries, eu, european, international, statesoil, water, production, ethanol, environmental
military, war, iraq, forces, marcheconomy, financial, spending, economic, government
government, social, governments, state, programsisrael, gaza, hamas, israeli, palestinian
women, religious, abortion, god, lifeteachers, pay, test, left, merit
peace, state, west, united, actionunited, states, president, administration, foreign
president, washington, obama, american, america
Figure 5: Jensen-Shannon divergences between topic-
specific positional term distributions, for each topic. Top-
ics are labeled by their most frequent terms from ?t.
4.1.3 Comparison to Human Judgments of
Positions
We compared our model?s inferred positions to
human judgments. For each of the 11 topics in Ta-
ble 8, we selected two associated debates with more
arguments than average (24.99). The debates were
provided to each of three human annotators,13 who
13All were native English-speaking American graduate stu-
dents not otherwise involved in this research. Each is known
by the authors to have basic literacy with issues and debates in
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
blog(12) edit(14) wiki(11) news(33) other(18) gov(12)
JS
 D
ive
rge
nc
e S
co
re
Article type(% of articles)
Figure 6: Position prediction on 500 hyperlinked articles
by genre.
1863
?Israel-Palestine? ?Same-sex marriage? ?Drugs? ?Healthcare? ?Death penalty? ?Abortion?
i1
pre emptive same sex hands free single payer anti death pro choice
israeli palestinian long term performance enhancing so called non violent pro life
open and shut second class in depth self sustaining african american non muslim
i2
two state opposite sex long term government run semi automatic would be
long term well intentioned high speed government approved high profile full time
self destructive day time short term high risk hate crime late term
a. Our model: topic-specific position bigrams associated with six selected topics.
?
war large illegal support death power
assault possibility abuse force penalty limit
disproportionate problems high threat murder civil
+
peace civil disease care power care
independence rights nature universal clean suicide
self-determination affirmative potential uninsured waste death
b. JST: sentiments associated with six selected topics manually aligned to our model?s topics.
Table 6: Terms associated with selected topics. The labels and alignments between the two models? topics were
assigned manually. (a.) Our model: topic-specific position bigrams which are ranked by comparing the log odds
conditioned on the position and topic: log ?oi1,t,w? log ?oi2,t,w. We show the top three terms for each position (b.) JST:we show the top three terms for each sentiment (negative and positive).
A1 (11) A2 (5) A3 (16)
Model (2) 3.21 2.58 3.45
A1 (11) 2.15 2.15
A2 (5) 2.63
Table 5: Variation of information scores for each pairing
of annotators and model.
were instructed to group the 44 sides of the debates.
The instructions stated:
Our goal is to see what you think about how
the different sides of different debates can be
lined up. You might find it convenient to
think of these in terms of political philoso-
phies, contemporary political party platforms,
or something else. Any of these is fine; we
want you to tell us the grouping you find most
reasonable.
All three annotators (hereafter denoted A1, A2, and
A3) used fairly involved labeling schemes; the an-
notators used 37, 30, and 16 unique labels, respec-
tively.14 A1 used keyword lists to label items; we
coarsened his labels manually by removing or merg-
ing less common keywords (resulting in: Republi-
can, Democrat, science/environment, nanny, politi-
cal reform, fiscal liberal, fiscal conservative, liber-
tarian, Israel, Palestine, and one unlabeled side).
A2 provided a coarse annotation along with each
American politics.
14In a small number of cases, an annotator declined to label
a side. Each unlabeled item received its own cluster.
fine-grained one (liberal, conservative, ?, and two
unlabeled sides). We used 100 samples from our
Gibbs sampler to estimate posteriors for each id,s;
these were always 99% or more in agreement, so we
mapped each debate side into its single most proba-
ble cluster. Recall that the two sides of each debate
must be in different clusters.
Table 5 shows the variation of information mea-
sure (Meila, 2003) for each pairing among the three
annotators and our model. The model agrees with
A2?s coarse clustering most closely, and in fact is
closer to A2?s clustering than A2 is to A3?s; it also
agrees with A2?s coarse clustering better than A2?s
coarse and fine clusterings agree (3.36, not shown
in the table). This is promising, but we do not
have confidence that the positional dimension is be-
ing captured especially well in this model; for those
debate-sides labeled liberal or conservative by A2,
the best match of our two positions was still only in
agreement only about 60% of the time, and agree-
ment with each human annotator is within the inter-
val of what would be expected if each debate?s sides
were assigned uniformly at random to positions.15
Remarks. Within debates and within topics, the
model uses the position variable to distinguish sides
well. For external text, the model performs well
on articles such as blogs and editorials but on oth-
ers the positional categories do not seem meaning-
15This was determined using a Monte Carlo simulation with
1,000 samples.
1864
Topic i = 1 i = 2
None (?i) vice president, c sections, twenty four, cross pressures,
pre dates, anti ballistic, cost effectiveness, anti land-
mine, court appointed, child poverty
cross examination, under runs, hand outs, half million,
non christians, break down, counter argument, seventy
five, co workers, run up
?Israel-
Palestine?
pre emptive, israeli palestinian, open and shut, first
time, hamas controlled, democratically elected
two state, long term, self destructive, secretary general,
right wing, all out, near daily, short term
?Same-sex
marriage?
same sex, long term, second class, blankenhorn rauch,
wrong headed, self denial, left handed
opposite sex, well intentioned, day time, planet wide,
day night, child rearing, low earth, one way, one third
?Drugs? hands free, performance enhancing, in depth, hand
held, best kept, non pharmaceutical, anti marijuana
long term, high speed, short term, peer reviewed, alco-
hol related, mind altering, inner city, long lasting
?Healthcare? single payer, so called, self sustaining, public private,
for profit, long run, high cost, multi payer
government run, government approved, high risk, two
tier, government appointed, low cost, set up
?Death
penalty?
anti death, non violent, african american, self help, cut
and cover, heavy handed, dp equivalent
semi automatic, high profile, hate crime, assault
weapons, military style, high dollar, self protective
?Abortion? pro choice, pro life, non muslim, well educated, anti
abortion, much needed, church state, birth control
would be, full time, late term, judeo christian, life
style, day to day, non christian, child bearing
Table 7: General position (first row) and topic-specific position bigrams associated with six selected topics.
Topic Terms Person entity mentions
?Israel-
Palestine?
israel, gaza, hamas, israeli, pales-
tinian
Benjamin Netanyahu, Al Jazeera, Mavi Marmara, Nicholas Kristoff,
Steven R. David
?Same-sex
marriage?
marriage, gay, mars, space, moon Buzz Aldrin, Andrew Sullivan, Moon Base, Scott Bidstrup, Ted Olson
?Drugs? marijuana, drug, drugs, alcohol, age Four Loko, Evo Morales, Toni Meyer, Sean Flynn, Robert Hahn
?Healthcare? health, care, insurance, public, pri-
vate
Kent Conrad, Paul Hsieh, Paul Krugman, Ezra Klein, Jacob Hacker
?Death
penalty?
death, crime, punishment, penalty,
justice
Adam Bedau, Thomas R. Eddlem, Jeff Jacoby, John Baer, Peter Bronson
?Abortion? women, religious, abortion, god, life Ronald Reagan, John Paul II, Sara Malkani, Mother Teresa, Marcella
Alsan
Table 8: For 6 selected topics (labels assigned manually), top terms (?t) and person entities (?e). Bigrams were
included but did not rank in the top five for these topics. The model has conflated debates relating to same-sex
marriage with the space program.
ful, perhaps due to the less argumentative nature
of other kinds of articles. Noting the vast litera-
ture focusing on ideological positions expressed in
text, we believe this failure suggests (i) that broad-
based positions that hold across many topics may
require richer textual representations (see, e.g., the
?syntactic priming? of Greene and Resnik, 2009),
or (ii) that an alternative representation of positions,
such as the spatial models favored by political sci-
entists (Poole and Rosenthal, 1991), may be more
discoverable. Aside from those issues, a stronger
theory of positions may be required. Such a the-
ory could be encoded in a more informative prior or
weaker independence assumptions across debates.
Finally, exploiting explicitly ideological texts along-
side the moderated arguments of Debatepedia might
also help to identify textual associations with gen-
eral positions (Sim et al, 2013). We leave these di-
rections to future work.
4.2 Qualitative Analysis
Of the T = 40 topics our model inferred, we subjec-
tively judged 37 to be coherent; a glimpse of each is
given in Figure 5. We manually selected six of the
most interpretable topics for further evaluation.
As a generative modeling approach, our model
was designed for the purpose of reducing the dimen-
sionality of the sociopolitical debate space, as evi-
denced by Debatepedia. It is like other topic models
in this regard, but we believe that some effects of our
design choices are noteworthy. Table 6 compares the
positional bigrams of our model to the sentiments in-
ferred by JST. We observe the benefit of our model
in identifying terms associated with positions on so-
cial issues, while JST selects more general sentiment
terms.
1865
Table 7 shows bigrams most strongly associated
with general position distributions ?i and selected
topic-position distributions ?o.16 We see the poten-
tial benefit of multiword expressions. Although we
have used frequent bigrams as a poor man?s approx-
imation to multiword expression analysis, we find
the topic-specific positions terms to be subjectively
evocative. While somewhat internally coherent, we
do not observe consistent alignment across topics,
and the general distributions ?i are not suggestive.
The separation of personal name mentions into
their own distributions, shown for some topics in
Table 8, gives a distinctive characterization of top-
ics based on relevant personalities. Subjectively, the
top individuals are relevant to the subject matter as-
sociated with each topic (though the topics are not
always pure; same-sex marriage and the space pro-
gram are merged, for example).
5 Related Work
Insofar as debates are subjective, our study is related
to opinion mining. Subjective text classification
(Wiebe and Riloff, 2005) leads to opinion mining
tasks such as opinion extraction (Dave et al, 2003),
positive and negative polarity classification (Pang et
al., 2002), sentiment target detection (Hu and Liu,
2004; Ganapathibhotla and Liu, 2008), and feature-
opinion extraction (Wu et al, 2009). The above
studies are conducted mostly on product reviews, a
domain with a simpler opinion landscape and more
concrete rationales for those opinions, compared to
sociopolitical debates.
Generative topic models have been successfully
implemented in opinion mining tasks such as feature
identification (Titov and McDonald, 2008), entity-
topic extraction (Newman et al, 2006), mining con-
tentious expressions and interactions (Mukherjee
and Liu, 2012) and specific aspect-opinion word ex-
traction from labeled data (Zhao et al, 2010). Most
relevant to this research is work on feature-sentiment
extraction (Lin and He, 2009; Mei et al, 2007). Mei
et al (2007) built on PLSI, which is problematic
for generalizing beyond the training sample. The
JST model of Lin and He (2009) is an LDA-based
topic model in which each word token is assigned
both a sentiment and a topic; they exploited a sen-
16For more topics, please refer to the supplementary notes.
timent lexicon in the prior distribution. Our model
is closely related, but introduces a switching vari-
able that assigns some tokens to positions, some to
topics, and some to both. Unlike Lin and He?s senti-
ments, our model?s positions are associated with the
two sides of a debate, and we incorporate topics at
the level of questions within debates.
Some studies have specifically analyzed con-
trastive viewpoints or stances in general discussion
text.Agrawal et al (2003) used graph mining based
method to classify authors in to opposite camps for
a given topic. Paul et al (2010) developed an unsu-
pervised method for summarizing contrastive opin-
ions from customer reviews. Abu-Jbara et al (2012)
and Dasigi et al (2012) developed techniques to ad-
dress the problem of automatically detecting sub-
groups of people holding similar stances in a dis-
cussion thread.
Several prior studies have considered debates.
Cabrio and Villata (2012) developed a system based
on argumentation theory which recognizes the en-
tailment and contradiction relationships between
two texts. Awadallah et al (2011) used a debate
corpus as a seed for extracting person-opinion-topic
tuples from news and other web documents and in
later work classified the quotations to specific top-
ics and polarity using language models (Awadal-
lah et al, 2012). Somasundaran and Wiebe (2009)
and Anand et al (2011) were interested in ideolog-
ical content in debates, relying on discourse struc-
ture and leveraging sentiment lexicons to recognize
stances.
Closer to the methodology we describe, Lin et
al. (2008) presented a statistical model for politi-
cal discourse that incorporates both topics and ide-
ologies; they used debates on the Israeli-Palestinian
conflict. Fortuna et al (2009) showed that it is pos-
sible to isolate a subset of terms from media content
that are informative of a news organization?s bias to-
wards a particular issue. Ahmed and Xing (2010) in-
troduced multi-level latent Dirichlet alocation, and
Eisenstein et al (2011) introduced sparse additive
generative models, both conceived as extensions to
well-established probabilistic modeling techniques
(Blei et al, 2003); these were applied to debates
and political blog datasets. Our approach builds on
these models (especially the switching variables of
Ahmed and Xing). We go farther in jointly modeling
1866
text across many debates evidenced by the structure
of Debatepedia, thus grounding our models more
solidly in familiar sociopolitical issues, and in mak-
ing extensive use of existing NLP resources.
6 Conclusion
Using text from Debatepedia, we inferred topics and
position term lexicons in the domain of sociopoliti-
cal debates. Our approach brings together tools from
information extraction and sentiment analysis into a
latent-variable topic model and exploits the hierar-
chical structure of the dataset. Our qualitative and
quantitative evaluations show the model?s strengths
and weaknesses.
Acknowledgments
The authors thank several anonymous reviewers,
Justin Gross, David Kaufer, and members of the
ARK group at CMU for helpful feedback on this
work and gratefully acknowledge the assistance of
the annotators. This research is supported by the
Singapore National Research Foundation under its
International Research Centre@Singapore Funding
Initiative and administered by the IDM Programme
Office, by an A?STAR fellowship to Y.S., and by
Google?s support of the Reading is Believing project
at CMU.
References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW
?03.
Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of EMNLP.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2011. OpinioNetIt: Understanding the
opinions-people network for politically controversial
topics. In Proceedings of CIKM.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2012. PolariCQ: Polarity classification of
political quotations. In Proceedings of CIKM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings of
ACL.
Sarah Cohen, James T. Hamilton, and Fred Turner. 2011.
Computational journalism. Communications of the
ACM, 54(10):66?71.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: a pilot study of implicit attitude using
latent textual semantics. In Proceedings of ACL.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statis-
tical learning methods. In Ashok N . Srivastava and
Mehran Sahami, editors, Text Mining: Classification,
Clustering, and Applications, pages 27?50. Chapman
& Hall/CRC.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
COLING.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of HLT-NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of CIKM.
Karen Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of documentation, 28(1):11?21.
Christian Kohlschu?tter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of WSDM.
1867
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceedings of
CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW.
Marina Meila. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, Learning Theory and Kernel
Machines, volume 2777 of Lecture Notes in Computer
Science, pages 173?187. Springer.
Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceedings
of KDD.
David Newman, Chaitanya Chemudugunta, and Padhraic
Smyth. 2006. Statistical entity-topic models. In Pro-
ceedings of KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of EMNLP.
Keith Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, pages 118?178.
Yanchuan Sim, Brice Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological propor-
tions in political speeches. In Proceedings of EMNLP.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Teun A. Van Dijk. 1998. Ideology: A Multidisciplinary
Approach. Sage Publications Limited.
Ellen M. Voorhees. 1999. The trec-8 question answering
track report. In Proceedings of TREC.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of ICML.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT-EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming
Li. 2010. Jointly modeling aspects and opinions with
a maxent-lda hybrid. In Proceedings of EMNLP.
1868
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001?1012,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Dependency Parser for Tweets
Lingpeng Kong Nathan Schneider Swabha Swayamdipta
Archna Bhatia Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{lingpenk,nschneid,swabha,archna,cdyer,nasmith}@cs.cmu.edu
Abstract
We describe a new dependency parser for
English tweets, TWEEBOPARSER. The
parser builds on several contributions: new
syntactic annotations for a corpus of tweets
(TWEEBANK), with conventions informed
by the domain; adaptations to a statistical
parsing algorithm; and a new approach to
exploiting out-of-domain Penn Treebank
data. Our experiments show that the parser
achieves over 80% unlabeled attachment
accuracy on our new, high-quality test set
and measure the benefit of our contribu-
tions.
Our dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
1 Introduction
In contrast to the edited, standardized language of
traditional publications such as news reports, social
media text closely represents language as it is used
by people in their everyday lives. These informal
texts, which account for ever larger proportions of
written content, are of considerable interest to re-
searchers, with applications such as sentiment anal-
ysis (Greene and Resnik, 2009; Kouloumpis et al.,
2011). However, their often nonstandard content
makes them challenging for traditional NLP tools.
Among the tools currently available for tweets are
a POS tagger (Gimpel et al., 2011; Owoputi et al.,
2013) and a named entity recognizer (Ritter et al.,
2011)?but not a parser.
Important steps have been taken. The English
Web Treebank (Bies et al., 2012) represents an
annotation effort on web text?which likely lies
somewhere between newspaper text and social me-
dia messages in formality and care of editing?that
was sufficient to support a shared task (Petrov and
McDonald, 2012). Foster et al. (2011b) annotated
a small test set of tweets to evaluate parsers trained
on the Penn Treebank (Marcus et al., 1993), aug-
mented using semi-supervision and in-domain data.
Others, such as Soni et al. (2014), have used exist-
ing Penn Treebank?trained models on tweets.
In this work, we argue that the Penn Treebank
approach to annotation?while well-matched to
edited genres like newswire?is poorly suited to
more informal genres. Our starting point is that
rapid, small-scale annotation efforts performed
by imperfectly-trained annotators should provide
enough evidence to train an effective parser. We
see this starting point as a necessity, given observa-
tions about the rapidly changing nature of tweets
(Eisenstein, 2013), the attested difficulties of do-
main adaptation for parsing (Dredze et al., 2007),
and the expense of creating Penn Treebank?style
annotations (Marcus et al., 1993).
This paper presents TWEEBOPARSER, the first
syntactic dependency parser designed explicitly for
English tweets. We developed this parser follow-
ing current best practices in empirical NLP: we
annotate a corpus (TWEEBANK) and train the pa-
rameters of a statistical parsing algorithm. Our
research contributions include:
? a survey of key challenges posed by syntactic
analysis of tweets (by humans or machines) and
decisions motivated by those challenges and by
our limited annotation-resource scenario (?2);
? our annotation process and quantitative mea-
sures of the quality of the annotations (?3);
? adaptations to a statistical dependency parsing
algorithm to make it fully compatible with the
above, and also to exploit information from out-
of-domain data cheaply and without a strong
commitment (?4); and
? an experimental analysis of the parser?s unla-
beled attachment accuracy?which surpasses
80%?and contributions of various important
components (?5).
The dataset and parser can be found at http://www.
ark.cs.cmu.edu/TweetNLP.
1001
2 Annotation Challenges
Before describing our annotated corpus of tweets
(?3), we illustrate some of the challenges of syn-
tactic analysis they present. These challenges moti-
vate an approach to annotation that diverges signif-
icantly from conventional approaches to treebank-
ing. Figure 1 presents a single example illustrating
four of these: token selection, multiword expres-
sions, multiple roots, and structure within noun
phrases. We discuss each in turn.
2.1 Token Selection
Many elements in tweets have no syntactic function.
These include, in many cases, hashtags, URLs, and
emoticons. For example:
RT @justinbieber : now Hailee get a twitter
The retweet discourse marker, username, and colon
should not, we argue, be included in the syntactic
analysis. By contrast, consider:
Got #college admissions questions ? Ask them
tonight during #CampusChat I?m looking
forward to advice from @collegevisit
http://bit.ly/cchOTk
Here, both the hashtags and the at-mentioned user-
name are syntactically part of the utterances, while
the punctuation and the hyperlink are not. In the
example of Figure 1, the unselected tokens include
several punctuation tokens and the final token #be-
lieber, which marks the topic of the tweet.
Typically, dependency parsing evaluations ig-
nore punctuation token attachment (Buchholz and
Marsi, 2006), and we believe it is a waste of an-
notator (and parser) time to decide how to attach
punctuation and other non-syntactic tokens. Ma
et al. (2014) recently proposed to treat punctua-
tion as context features rather than dependents, and
found that this led to state-of-the-art performance
in a transition-based parser. A small adaptation
to our graph-based parsing approach, described in
?4.2, allows a similar treatment.
Our approach to annotation (?3) forces annota-
tors to explicitly select tokens that have a syntactic
function. 75.6% tokens were selected by the anno-
tators. Against the annotators? gold standard, we
found that a simple rule-based filter for usernames,
hashtags, punctuation, and retweet tokens achieves
95.2% (with gold-standard POS tags) and 95.1%
(with automatic POS tags) average accuracy in the
task of selecting tokens with a syntactic function
in a ten-fold cross-validation experiment. To take
context into account, we developed a first-order
sequence model and found that it achieves 97.4%
average accuracy (again, ten-fold cross-validated)
with either gold-standard or automatic POS tags.
Features include POS; shape features that recog-
nize the retweet marker, hashtags, usernames, and
hyperlinks; capitalization; and a binary feature
for tokens that include punctuation. We trained
the model using the structured perceptron (Collins,
2002).
2.2 Multiword Expressions
We consider multiword expressions (MWEs) of
two kinds. The first, proper names, have been
widely modeled for information extraction pur-
poses, and even incorporated into parsing (Finkel
and Manning, 2009). (An example found in Fig-
ure 1 is LA Times.) The second, lexical idioms,
have been a ?pain in the neck? for many years (Sag
et al., 2002) and have recently received shallow
treatment in NLP (Baldwin and Kim, 2010; Con-
stant and Sigogne, 2011; Schneider et al., 2014).
Constant et al. (2012), Green et al. (2012), Candito
and Constant (2014), and Le Roux et al. (2014)
considered MWEs in parsing. Figure 1 provides
LA Times and All the Rage as examples.
Penn Treebank?style syntactic analysis (and de-
pendency representations derived from it) does
not give first-class treatment to this phenomenon,
though there is precedent for marking multiword
lexical units and certain kinds of idiomatic relation-
ships (Haji
?
c et al., 2012; Abeill? et al., 2003).
1
We argue that internal analysis of MWEs is not
critical for many downstream applications, and
therefore annotators should not expend energy on
developing and respecting conventions (or mak-
ing arbitrary decisions) within syntactically opaque
or idiosyncratic units. We therefore allow annota-
tors to decide to group words as explicit MWEs,
including: proper names (Justin Bieber, World
Series), noncompositional or entrenched nominal
compounds (belly button, grilled cheese), connec-
tives (as well as), prepositions (out of), adverbials
(so far), and idioms (giving up, make sure).
From an annotator?s perspective, a MWE func-
tions as a single node in the dependency parse,
with no internal structure. For idioms whose in-
ternal syntax is easily characterized, the parse can
be used to capture compositional structure, an at-
1
The popular Stanford typed dependencies (de Marneffe
and Manning, 2008) scheme includes a special dependency
type for multiwords, though this is only applied to a small list.
1002
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
? #belieber
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
Figure 1: Parse tree for a (constructed) example illustrating annotation challenges discussed in ?2. Colors highlight token
selection (gray; ?2.1), multiword expressions (blue; ?2.2), multiple roots (red; ?2.3), coordination (dotted arcs, green; ?3.2), and
noun phrase internal structure (orange; ?2.4). The internal structure of multiword expressions (dashed arcs below the sentence)
was predicted automatically by a parser, as described in ?2.2.
tractive property from the perspective of semantic
processing.
To allow training a fairly conventional statisti-
cal dependency parser from these annotations, we
find it expedient to apply an automatic conversion
to the MWE annotations, in the spirit of Johnson
(1998). We apply an existing dependency parser,
the first-order TurboParser (Martins et al., 2009)
trained on the Penn Treebank, to parse each MWE
independently, assigning structures like those for
LA Times and All the Rage in Figure 1. Arcs
involving the MWE in the annotation are then re-
connected to the MWE-internal root, so that the re-
sulting tree respects the original tokenization. The
MWE-internal arcs are given a special label so that
the transformation can be reversed and MWEs re-
constructed from parser output.
2.3 Multiple Roots
For news text such as that found in the Penn Tree-
bank, sentence segmentation is generally consid-
ered a very easy task (Reynar and Ratnaparkhi,
1997). Tweets, however, often contain multiple
sentences or fragments, which we call ?utterances,?
each with its own syntactic root disconnected from
the others. The selected tokens in Figure 1 com-
prise four utterances.
Our approach to annotation allows multiple ut-
terances to emerge directly from the connectedness
properties of the graph implied by an annotator?s
decisions. Our parser allows multiple attachments
to the ?wall? symbol, so that multi-rooted analyses
can be predicted.
2.4 Noun Phrase Internal Structure
A potentially important drawback of deriving de-
pendency structures from phrase-structure annota-
tions, as is typically done using the Penn Treebank,
is that flat annotations lead to loss of information.
This is especially notable for noun phrases in the
Penn Treebank (Vadas and Curran, 2007). Consider
Teen Pop Star Heartthrob in Figure 1; Penn Tree-
bank conventions would label this as a single NP
with four NN children and no internal structure. De-
pendency conversion tools would likely attach the
first three words in the NP to Heartthrob. Direct de-
pendency annotation (rather than phrase-structure
annotation followed by automatic conversion) al-
lows a richer treatment of such structures, which is
potentially important for semantic analysis (Vecchi
et al., 2013).
3 A Twitter Dependency Corpus
In this section, we describe the TWEEBANK cor-
pus, highlighting data selection (?3.1), the annota-
tion process (?3.2), important convention choices
(?3.3), and measures of quality (?3.4).
3.1 Data Selection
We added manual dependency parses to 929 tweets
(12,318 tokens) drawn from the POS-tagged Twit-
ter corpus of Owoputi et al. (2013), which are tok-
enized and contain manually annotated POS tags.
Owoputi et al.?s data consists of two parts. The
first, originally annotated by Gimpel et al. (2011),
consists of tweets sampled from a particular day,
October 27, 2010?this is known as OCT27. Due
to concerns about overfitting to phenomena specific
to that day (e.g., tweets about a particular sports
game), Owoputi et al. (2013) created a new set of
547 tweets (DAILY547) consisting of one random
English tweet per day from January 2011 through
June 2012.
Our corpus is drawn roughly equally from
OCT27 and DAILY547.
2
Despite its obvious tem-
poral skew, there is no reason to believe this sample
is otherwise biased; our experiments in ?5 suggest
that this property is important.
3.2 Annotation
Unlike a typical treebanking project, which may
take years and involve thousands of person-hours
of work by linguists, most of TWEEBANK was built
in a day by two dozen annotators, most of whom
had only cursory training in the annotation scheme.
2
This results from a long-term goal to fully annotate both.
1003
(1) RT @FRIENDSHlP : Friendship is love without
kissing ...
Friendship > is < love < without < kissing
(2) bieber is an alien ! :O he went down to earth .
bieber > is** < alien < an
he > [went down]** < to < earth
(3) RT @YourFavWhiteGuy : Helppp meeeee . I?mmm
meltiiinngggg ? http://twitpic.com/316cjg
Helppp** < meeeee
I?mmm** < meltiiinngggg
Figure 2: Examples of GFL annotations from the corpus.
Our annotators used the Graph Fragment Lan-
guage (GFL), a text-based notation that facilitates
keyboard entry of parses (Schneider et al., 2013). A
Python Flask web application allows the annotator
to validate and visualize each parse (Mordowanec
et al., 2014). Some examples are shown in Fig-
ure 2. Note that all of the challenges in ?2 are
handled easily by GFL notation: ?retweet? infor-
mation, punctuation, and a URL are not selected by
virtue of their exclusion from the GFL expression;
in (2) went down is annotated as a MWE using
GFL?s square bracket notation; in (3) the tokens
are grouped into two utterances whose roots are
marked by the ** symbol.
Schneider et al.?s GFL offers some additional fea-
tures, only some of which we made use of in this
project. One important feature allows an annotator
to leave the parse underspecified in some ways. We
allowed our annotators to make use of this feature;
however, we excluded from our training and test-
ing data any parse that was incomplete (i.e., any
parse that contained multiple disconnected frag-
ments with no explicit root, excluding unselected
tokens). Learning to parse from incomplete anno-
tations is a fascinating topic explored in the past
(Hwa, 2001; Pereira and Schabes, 1992) and, in the
case of tweets, left for future work.
An important feature of GFL that we did use is
special notation for coordination structures. For
the coordination structure in Figure 1, for example,
the notation is:
$a :: {? want} :: {&}
where $a creates a new node in the parse tree as it is
visualized for the annotator, and this new node at-
taches to the syntactic parent of the conjoined struc-
ture, avoiding the classic forced choice between
coordinator and conjunct as parent. For learning to
parse, we transform GFL?s coordination structures
into specially-labeled dependency parses collaps-
ing nodes like $a with the coordinator and labeling
the attachments specially for postprocessing, fol-
lowing Schneider et al. (2013). In our evaluation
(?5), these are treated like other attachments.
3.3 Annotation Conventions
A wide range of dependency conventions are in use;
in many cases these are conversion conventions
specifying how dependency trees can be derived
from phrase-structure trees. For English, the most
popular are due to Yamada and Matsumoto (2003)
and de Marneffe and Manning (2008), known as
?Yamada-Matsumoto? (YM) and ?Stanford? depen-
dencies, respectively. The main differences be-
tween them are in whether the auxiliary is the par-
ent of the main verb (or vice versa) and whether the
preposition or its argument heads a prepositional
phrase (Elming et al., 2013).
A full discussion of our annotation conventions
is out of scope. We largely followed the conven-
tions suggested by Schneider et al. (2013), which in
turn are close to those of YM. Auxiliary verbs are
parents of main verbs, and prepositions are parents
of their arguments. The key differences from YM
are in coordination structures (discussed in ?3.2;
YM makes the first conjunct the head) and posses-
sive structures, in which the possessor is the child
of the clitic, which is the child of the semantic head,
e.g., the > king > ?s > horses.
3.4 Intrinsic Quality
Our approach to developing this initial corpus of
syntactically annotated tweets was informed by an
aversion to making the perfect the enemy of the
good; that is, we sought enough data of sufficient
quality to build a usable parser within a relatively
short amount of time. If our research goals had
been to develop a replicable process for annotation,
more training and more quality control would have
been called for. Under our budgeted time and anno-
tator resources, this overhead was simply too costly.
Nonetheless, we performed a few analyses that give
a general picture of the quality of the annotations.
Inter-annotator agreement. 170 of the tweets
were annotated by multiple users. By the softCom-
Prec measure (Schneider et al., 2013),
3
the agree-
ment rate on dependencies is above 90%.
Expert linguistic judgment. A linguist co-
author examined a stratified sample (balanced
3
softComPrec is a generalization of attachment accuracy
that handles unselected tokens and MWEs.
1004
across annotators) of 60 annotations and rated their
quality on a 5-point scale. 30 annotations were
deemed to have ?no obvious errors,? 15 only minor
errors, 3 a major error (i.e., clear violation of an-
notation guidelines),
4
4 a major error and at least
one minor error, and 8 as containing multiple major
errors. Thus, 75% are judged as having no major
errors. We found this encouraging, considering that
this sample is skewed in favor of people who anno-
tated less (including many of the less experienced
and/or lower-proficiency annotators).
Pairwise ranking. For 170 of the doubly anno-
tated tweets, an experienced annotator examined
whether one or the other was markedly better. In
100 cases the two annotations were of comparable
quality (neither was obviously better) and did not
contain any obvious major errors. In only 7 pairs
did both of the annotations contain a serious error.
Qualitatively, we found several unsurprising
sources of error or disagreement, including em-
bedded/subordinate clauses, subject-auxiliary in-
version, predeterminers, and adverbial modifiers
following a modal/auxiliary verb and a main verb.
Clarification of the conventions, or even explicit
rule-based checking in the validation step, might
lead to quality improvements in further annotation
efforts.
4 Parsing Algorithm
For parsing, we start with TurboParser, which is
open-source and has been found to perform well on
a range of parsing problems in different languages
(Martins et al., 2013; Kong and Smith, 2014). The
underlying model allows for flexible incorporation
of new features and changes to specification in the
output space. We briefly review the key ideas in
TurboParser (?4.1), then describe decoder modifi-
cations required for our problem (?4.2). We then
discuss features we added to TurboParser (?4.3).
4.1 TurboParser
Let an input sentence be denoted by x and the set
of possible dependency parses for x be denoted by
Y
x
. A generic linear scoring function based on a
4
What we deemed major errors included, for example,
an incorrect dependency relation between an auxiliary verb
and the main verb (like ima > [have to]). Minor errors
included an incorrect attachment between two modifiers of
the same head, as in the > only > [grocery store]?the
correct annotation would have two attachments to a single
head, i.e. the > [grocery store] < only (or equivalent).
feature vector representation g is used in parsing
algorithms that seek to find:
parse
?(x) = argmax
y?Y
x
w
?
g(x,y) (1)
The score is parameterized by a vector w of
weights, which are learned from data (most com-
monly using MIRA, McDonald et al., 2005a).
The decomposition of the features into local
?parts? is a critical choice affecting the computa-
tional difficulty of solving Eq. 1. The most aggres-
sive decomposition leads to an ?arc-factored? or
?first-order? model, which permits exact, efficient
solution of Eq. 1 using spanning tree algorithms
(McDonald et al., 2005b) or, with a projectivity
constraint, dynamic programming (Eisner, 1996).
Second- and third-order models have also been
introduced, typically relying on approximations,
since less-local features increase the computational
cost, sometimes to the point of NP-hardness (Mc-
Donald and Satta, 2007). TurboParser attacks the
parsing problem using a compact integer linear pro-
gramming (ILP) representation of Eq. 1 (Martins
et al., 2009), then employing alternating directions
dual decomposition (AD
3
; Martins et al., 2011).
This enables inclusion of second-order features
(e.g., on a word with its sibling or grandparent;
Carreras, 2007) and third-order features (e.g., a
word with its parent, grandparent, and a sibling, or
with its parent and two siblings; Koo and Collins,
2010).
For a collection of (possibly overlapping) parts
for input x, S
x
(which includes the union of all
parts of all trees in Y
x
), we will use the following
notation. Let
g(x,y) = ?
s?S
x
f
s
(x,y), (2)
where f
s
only considers part s and is nonzero only
if s is present in y. In the ILP framework, each s
has a corresponding binary variable z
s
indicating
whether part s is included in the output. A col-
lection of constraints relating z
s
define the set of
feasible vectors z that correspond to valid outputs
and enfore agreement between parts that overlap.
Many different versions of these constraints have
been studied (Riedel and Clarke, 2006; Smith and
Eisner, 2008; Martins et al., 2009, 2010).
A key attraction of TurboParser is that many
overlapping parts can be handled, making use of
separate combinatorial algorithms for efficiently
handling subsets of constraints. For example, the
constraints that force z to encode a valid tree can
be exploited within the framework by making calls
1005
to classic arborescence algorithms (Chu and Liu,
1965; Edmonds, 1967). As a result, when describ-
ing modifications to TurboParser, we need only to
explain additional constraints and features imposed
on parts.
4.2 Adapted Parse Parts
The first collection of parts we adapt are simple
arcs, each consisting of an ordered pair of indices
of words in x; arc(p,c) corresponds to the attach-
ment of x
c
as a child of x
p
(iff z
arc(p,c) = 1). Our rep-
resentation explicitly excludes some tokens from
being part of the syntactic analysis (?2.1); to han-
dle this, we constrain z
arc(i, j) = 0 whenever xi or x j
is excluded.
The implication is that excluded tokens are still
?visible? to feature functions that involve other
edges. For example, some conventional first-order
features consider the tokens occurring between a
parent and child. Even if a token plays no syntactic
role of its own, it might still be informative about
the syntactic relationships among other tokens. We
note three alternative methods:
1. We might remove all unselected tokens from
x before running the parser. In ?5.6 we find
this method to fare 1.7?2.3% worse than our
modified decoding algorithm.
2. We might remove unselected tokens but use
them to define new features, so that they still
serve as evidence. This is the approach taken
by Ma et al. (2014) for punctuation. We judge
our simple modification to the decoding algo-
rithm to be more expedient, and leave the trans-
lation of existing context-word features into that
framework for future exploration.
3. We might incorporate the token selection deci-
sions into the parser, performing joint inference
for selection and parsing. The AD
3
algorithm
within TurboParser is well-suited to this kind
of extension: z-variables for each token?s se-
lection could be added, and similar scores to
those of our token selection sequence model
(?2.1) could be integrated into parsing. Given,
however, that the sequence model achieves over
97% accuracy, and that perfect token selection
would gain only 0.1?1% in parsing accuracy (re-
ported in ?5.5), we leave this option for future
work as well.
For first-order models, the above change is all
that is necessary. For second- and third-order
models, TurboParser makes use of head automata,
in particular ?grand-sibling head automata? that
assign scores to word tuples of x
g
, its child x
p
,
and two of x
p
?s adjacent children, x
c
and x
?
c
(Koo
et al., 2010). The second-order models in our
experiments include parts for sibling(p,c,c?) and
grandparent(p,c,g) and use the grand-sibling head
automaton to reason about these together. Au-
tomata for an unselected x
p
or x
g
, and transitions
that consider unselected tokens as children, are
eliminated. In order to allow the scores to depend
on unselected tokens between x
c
and x
?
c
, we added
the binned counts of unselected tokens (mostly
punctuation) joint with the word form and POS
tag of x
p
and the POS tag of x
c
and x
?
c
as features
scored in the sibling(p,c,c?) part. The changes dis-
cussed above comprise the totality of adaptations
we made to the TurboParser algorithm; we refer to
them as ?parsing adaptations? in the experiments.
4.3 Additional Features
Brown clusters. Owoputi et al. (2013) found that
Brown et al. (1992) clusters served as excellent fea-
tures in Twitter POS tagging. Others have found
them useful in parsing (Koo et al., 2008) and other
tasks (Turian et al., 2010). We therefore follow
Koo et al. in incorporating Brown clusters as fea-
tures, making use of the publicly available Twitter
clusters from Owoputi et al.
5
We use 4 and 6 bit
cluster representations to create features wherever
POS tags are used, and full bit strings to create
features wherever words were used.
Penn Treebank features. A potential danger of
our choice to ?start from scratch? in developing
a dependency parser for Twitter is that the result-
ing annotation conventions, data, and desired out-
put are very different from dependency parses de-
rived from the Penn Treebank. Indeed, Foster et al.
(2011a) took a very different approach, applying
Penn Treebank conventions in annotation of a test
dataset for evaluation of a parser trained using Penn
Treebank trees. In ?5.4, we replicate, for depen-
dencies, their finding that a Penn Treebank?trained
parser is hard to beat on their dataset, which was
not designed to be topically representative of En-
glish Twitter. When we turn to a more realistic
dataset like ours, we find the performance of the
Penn Treebank?trained parser to be poor.
Nonetheless, it is hard to ignore such a large
amount of high-quality syntactic data. We there-
5http://www.ark.cs.cmu.edu/TweetNLP/clusters/
50mpaths2
1006
fore opted for a simple, stacking-inspired incor-
poration of Penn Treebank information into our
model.
6
We define a feature on every candidate arc
whose value is the (quantized) score of the same arc
under a first-order model trained on the Penn Tree-
bank converted using head rules that are as close
as possible to our conventions (discussed in more
detail in ?5.1). This lets a Penn Treebank model
literally ?weigh in? on the parse for a tweet, and
lets the learning algorithm determine how much
consideration it deserves.
5 Experiments
Our experiments quantify the contributions of vari-
ous components of our approach.
5.1 Setup
We consider two test sets. The first, TEST-NEW,
consists of 201 tweets from our corpus annotated
by the most experienced of our annotators (one
of whom is a co-author of this work). Given very
limited data, we believe using the highest quality
data for measuring performance, and lower-quality
data for training, is a sensibly realistic choice.
Our second test set, TEST-FOSTER, is the dataset
annotated by Foster et al. (2011b), which consists
of 250 sentences. Recall that their corpus was
annotated with phrase structures according to Penn
Treebank conventions. Conversion to match our
annotation conventions was carried out as follows:
1. We used the PennConverter tool with head rule
options selected to approximate our annotation
conventions as closely as possible.
7
2. An experienced annotator manually modified
the automatically converted trees by:
(a) Performing token selection (?2.1) to remove
the tokens which have no syntactic function.
(b) Grouping MWEs (?2.2). Here, most of the
MWEs are named entities such as Manch-
ester United.
(c) Attaching the roots of the utterance in tweets
to the ?wall? symbol (?2.3).
8
6
Stacking is a machine learning method where the predic-
tions of one model are used to create features for another. The
second model may be from a different family. Stacking has
been found successful for dependency parsing by Nivre and
McDonald (2008) and Martins et al. (2008). Johansson (2013)
describes further advances that use path features.
7http://nlp.cs.lth.se/software/treebank_
converter; run with -rightBranching=false
-coordStructure=prague -prepAsHead=true
-posAsHead=true -subAsHead=true -imAsHead=true
-whAsHead=false.
8
This was infrequent; their annotations split most multi-
TRAIN TEST-NEW TEST-FOSTER
tweets 717 201 < 250?
unique tweets 569 201 < 250?
tokens 9,310 2,839 2,841
selected tokens 7,015 2,158 2,366
types 3,566 1,461 1,230
utterances 1,473 429 337
multi-root tweets 398 123 60
MWEs 387 78 109
Table 1: Statistics of our datasets. (A tweet with k annotations
in the training set is counted k times for the totals of tokens,
utterances, etc.).
?
TEST-FOSTER contains 250 manually split
sentences. The number of tweets should be smaller but is not
recoverable from the data release.
(d) Recovering the internal structure of the noun
phrases.
(e) Fixing a difference in conventions with re-
spect to subject-auxiliary inversion.
9
We consider two training sets. TRAIN-NEW con-
sists of the remaining 717 tweets from our corpus
(?3) annotated by the rest of the annotators. Some
of these tweets have annotations from multiple an-
notators; 11 annotations for tweets that also oc-
curred in TEST-NEW were excluded. TRAIN-PTB
is the conventional training set from the Penn Tree-
bank (?2?21). The PennConverter tool was used
to extract dependencies, with head rule options se-
lected to approximate our annotation conventions
as closely as possible (see footnote 7). The result-
ing annotations lack the same attention to noun
phrase?internal structure (?2.4) and handle subject-
auxiliary inversions differently than our data. Part-
of-speech tags were coarsened to be compatible
with the Twitter POS tags, using the mappings spec-
ified by Gimpel et al. (2011).
Statistics for the in-domain datasets are given in
Table 1. As we can see in the table, more than half
of the tweets in our corpus have multiple utterances.
The out-of-vocabulary rate for our TRAIN/TEST-
NEW split is 33.7% by token and 62.5% by type;
for TRAIN/TEST-FOSTER it is 41.4% and 64.6%
respectively. These are much higher than the 2.5%
and 13.2% in the standard Penn Treebank split.
All evaluations here are on unlabeled attachment
F
1
scores.
10
Our parser provides labels for coordi-
nation structures and MWEs (?2), but we do not
present detailed evaluations of those due to space
constraints.
utterance tweets into separate sentence-instances.
9
For example, in the sentence Is he driving, we attached
he to driving while PennConverter attaches it to Is.
10
Because of token selection, precision and recall may not
be equal.
1007
5.2 Preprocessing
Because some of the tweets in our test set were
also in the training set of Owoputi et al. (2013),
we retrained their POS tagger on all the annotated
data they have minus the 201 tweets in our test
set. Its tagging accuracy was 92.8% and 88.7% on
TEST-NEW and TEST-FOSTER, respectively. The
token selection model (?2.1) achieves 97.4% on
TEST-NEW with gold or automatic POS tagging;
and on TEST-FOSTER, 99.0% and 99.5% with gold
and automatic POS tagging, respectively.
As noted in ?4.3, Penn Treebank features were
developed using a first-order TurboParser trained
on TRAIN-PTB; Brown clusters were included in
computing these Penn Treebank features if they
were available in the parser to which the features
(i.e. Brown clusters) were added.
5.3 Main Parser
The second-order TurboParser described in ?4,
trained on TRAIN-NEW (default hyperparameter
values), achieves 80.9% unlabeled attachment ac-
curacy on TEST-NEW and 76.1% on TEST-FOSTER.
The experiments consider variations on this main
approach, which is the version released as TWEE-
BOPARSER.
The discrepancy between the two test sets is
easily explained: as noted in ?3.1, the dataset
from which our tweets are drawn was designed
to be representative of English on Twitter. Fos-
ter et al. (2011b) selected tweets from Berming-
ham and Smeaton?s (2010) corpus, which uses fifty
predefined topics like politics, business, sports,
and entertainment?in short, topics not unlike
those found in the Penn Treebank. Relative to
the Penn Treebank training set, the by-type out-
of-vocabulary rates are 45.2% for TEST-NEW and
only 21.6% for TEST-FOSTER (cf. 13.2% for the
Penn Treebank test set).
Another mismatch is in the handling of utter-
ances. In our corpus, utterance segmentation
emerges from multi-rooted annotations (?2.3). Fos-
ter et al. (2011b) manually split each tweet into
utterances and treat those as separate instances in
their corpus, so that our model trained on often
multi-rooted tweets from TRAIN is being tested
only on single-rooted utterances.
5.4 Experiment: Which Training Set?
We consider the direct use of TRAIN-PTB instead
of TRAIN-NEW. Table 2 reports the results on both
Unlabeled Attachment F
1
(%)
mod. POS POS as-is
TEST-NEW
Baseline 73.0 73.5
+ Brown 73.7 73.3
+ Brown & PA 72.9 73.1
TEST-FOSTER
Baseline 76.3 75.2
+ Brown 75.5 76.7
+ Brown & PA 76.9 77.0
Table 2: Performance of second-order TurboParser trained on
TRAIN-PTB, with various preprocessing options. The main
parser (?5.3) achieves 80.9% and 76.1% on the two test sets,
respectively; see ?5.4 for discussion.
test sets, with various options. ?Baseline? is off-
the-shelf second-order TurboParser. We consider
augmenting it with Brown cluster features (?4.3;
?+ Brown?) and then also with the parsing adapta-
tions of ?4.2 (?+ Brown & PA?). Another choice
is whether to modify the POS tags at test time; the
modified version (?mod. POS?) maps at-mentions
to pronoun, and hashtags and URLs to noun.
We note that comparing these scores to our main
parser (?5.3) conflates three very important inde-
pendent variables: the amount of training data
(39,832 Penn Treebank sentences vs. 1,473 Twitter
utterances), the annotation method, and the source
of the data. However, we are encouraged that, on
what we believe is the superior test set (TEST-NEW),
our overall approach obtains a 7.8% gain with an
order of magnitude less annotated data.
5.5 Experiment: Effect of Preprocessing
Table 3 (second block, italicized) shows the per-
formance of the main parser on both test sets with
gold-standard and automatic POS tagging and to-
ken selection. On TEST-NEW, with either gold-
standard POS tags or gold-standard token selection,
performance increases by 1.1%; with both, it in-
creases by 2.3%. On TEST-FOSTER, token selec-
tion matters much less, but POS tagging accounts
for a drop of more than 6%. This is consistent with
Foster et al.?s finding: using a fine-grained Penn
Treebank?trained POS tagger (achieving around
84% accuracy on Twitter), they saw 5?8% improve-
ment in unlabeled dependency attachment accuracy
using gold-standard POS tags.
5.6 Experiment: Ablations
We ablated each key element of our main parser?
PTB features, Brown features, second order fea-
tures and decoding, and the parsing adaptations of
1008
0.76
0.78
0.8
0.82
Unla
beled
 Atta
chme
nt F 1
 
 
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
First?OrderSecond?Order
(a) TEST-NEW
0.7
0.72
0.74
0.76
Unla
beled
 Atta
chme
nt F 1
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
 
 First?OrderSecond?Order
(b) TEST-FOSTER
Figure 3: Feature ablations; these charts present the same scores shown in Table 3 and more variants of the first-order model.
Unlabeled Attachment F
1
(%)
TEST-NEW TEST-FOSTER
Main parser 80.9 76.1
Gold POS and TS 83.2 82.8
Gold POS, automatic TS 82.0 82.3
Automatic POS, gold TS 82.0 76.2
Single ablations:
? PTB 80.2 72.6
? Brown 81.2 75.4
? 2nd order 80.1 75.6
? PA 79.2 73.7
Double ablations:
? PTB, ? Brown 79.5 72.8
? PTB, ? 2nd order 78.5 72.2
? PTB, ? PA 77.4 69.6
? Brown, ? 2nd order 80.7 74.5
? Brown, ? PA 78.2 73.7
? 2nd order, ? PA 77.7 73.5
Baselines:
Second order 76.5 70.4
First order 76.1 70.4
Table 3: Effects of gold-standard POS tagging and token
selection (TS; ?5.5) and of feature ablation (?5.6). The ?base-
lines? are TurboParser without the parsing adaptations in ?4.2
and without Penn Treebank or Brown features. The best result
in each column is bolded. See also Figure 3.
?4.2?as well as each pair of these. These condi-
tions use automatic POS tags and token selection.
The ?? PA? condition, which ablates parsing adap-
tations, is accomplished by deleting punctuation
(in training and test data) and parsing using Turbo-
Parser?s existing algorithm.
Results are shown in Table 3. Further results
with first- and second-order TurboParsers are plot-
ted in Figure 3. Notably, a 2?3% gain is obtained by
modifying the parsing algorithm, and our stacking-
inspired use of Penn Treebank data contributes in
both cases, quite a lot on TEST-FOSTER (unsur-
prisingly given that test set?s similarity to the Penn
Treebank). More surprisingly, we find that Brown
cluster features do not consistently improve perfor-
mance, at least not as instantiated here, with our
small training set.
6 Conclusion
We described TWEEBOPARSER, a dependency
parser for English tweets that achieves over 80%
unlabeled attachment score on a new, high-quality
test set. This is on par with state-of-the-art re-
ported results for news text in Turkish (77.6%;
Koo et al., 2010) and Arabic (81.1%; Martins
et al., 2011). Our contributions include impor-
tant steps taken to build the parser: a considera-
tion of the challenges of parsing tweets that in-
formed our annotation process, the resulting new
TWEEBANK corpus, adaptations to a statistical
parsing algorithm, a new approach to exploiting
data in a better-resourced domain (the Penn Tree-
bank), and experimental analysis of the decisions
we made. The dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgments
The authors thank the anonymous reviewers
and Andr? Martins, Yanchuan Sim, Wang Ling,
Michael Mordowanec, and Alexander Rush for
helpful feedback, as well as the annotators Waleed
Ammar, Jason Baldridge, David Bamman, Dallas
Card, Shay Cohen, Jesse Dodge, Jeffrey Flanigan,
Dan Garrette, Lori Levin, Wang Ling, Bill Mc-
Dowell, Michael Mordowanec, Brendan O?Connor,
Rohan Ramanath, Yanchuan Sim, Liang Sun, Sam
Thomson, and Dani Yogatama. This research was
supported in part by the U. S. Army Research Lab-
oratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533 and by
NSF grants IIS-1054319 and IIS-1352440.
1009
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Treebanks,
pages 165?187. Springer.
Timonthy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Handbook of Natural Lan-
guage Processing, Second Edition. CRC Press, Tay-
lor and Francis Group.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proc. of CIKM.
Ann Bies, Justin Mott, Colin Warner, and Seth
Kulick. 2012. English Web Treebank. Techni-
cal Report LDC2012T13, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T13.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie Candito and Matthieu Constant. 2014. Strate-
gies for contiguous multiword expression analysis
and dependency parsing. In Proc. of ACL.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proc. of
EMNLP-CoNLL.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest
arborescence of a directed graph. Scientia Sinica,
14(10):1396.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to
the Real World.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate mul-
tiword expression recognition and parsing. In Proc.
of ACL.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Proc. of COLING Workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proc. of EMNLP-
CoNLL.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(233-240):160.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL-HLT.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, H?ctor Mart?nez Alonso, and
Anders S?gaard. 2013. Down-stream effects of tree-
to-dependency conversions. In Proc. of NAACL-
HLT.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc
of ACL-HLT.
Jennifer Foster, ?zlem ?etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011a.
#hardtoparse: POS tagging and parsing the Twitter-
verse. In Proc. of AAAI Workshop on Analyzing Mi-
crotext.
Jennifer Foster, ?zlem ?etino?glu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011b. From news to comment:
resources and benchmarks for parsing the language
of Web 2.0. In Proc. of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2012. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Stephan Greene and Philip Resnik. 2009. Syntac-
tic packaging and implicit sentiment. In Proc. of
NAACL.
1010
Jan Haji?c, Eva Haji?cov?, Jarmila Panevov?, Petr Sgall,
Silvie Cinkov?, Eva Fu?c?kov?, Marie Mikulov?,
Petr Pajas, Jan Popelka, Ji?r? Semeck`y, Jana
?indlerov?, Jan ?t?ep?nek, Josef Toman, Zde?nka
Ure?ov?, and Zden?ek ?abokrtsk?. 2012. Prague
Czech-English Dependency Treebank 2.0. Techni-
cal Report LDC2012T08, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T08.
Rebecca Hwa. 2001. Learning Probabilistic Lexical-
ized Grammars for Natural Language Processing.
Ph.D. thesis, Harvard University.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In Proc. of NAACL-HLT.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Lingpeng Kong and Noah A. Smith. 2014. An empiri-
cal comparison of parsing methods for Stanford de-
pendencies. ArXiv:1404.4314.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In
Proc. of ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual de-
composition for parsing with non-projective head au-
tomata. In Proc. of EMNLP.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. of ICWSM.
Joseph Le Roux, Matthieu Constant, and Antoine
Rozenknop. 2014. Syntactic parsing and compound
recognition via dual decomposition: application to
French. In Proc. of COLING.
Ji Ma, Yue Zhang, and Jingbo Zhu. 2014. Punctua-
tion processing for projective dependency parsing.
In Proc. of ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional linguistics, 19(2):313?330.
Andr? F.T. Martins, Miguel Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proc. of ACL.
Andr? F.T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, Pedro M.Q.
Aguiar, and M?rio A.T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proc. of ACL-
IJCNLP.
Andr? F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and M?rio A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji
?
c. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proc. of IWPT.
Michael T. Mordowanec, Nathan Schneider, Chris.
Dyer, and Noah A. Smith. 2014. Simplified depen-
dency annotations with GFL-Web. In Proc. of ACL,
demonstration track.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL-HLT.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL-HLT.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proc. of ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. of ANLP.
1011
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In Proc. of EMNLP.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Proc. of
CICLing.
Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. Transactions of the Association for Compu-
tational Linguistics, 2:193?206.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of EMNLP.
Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob
Eisenstein. 2014. Modeling factuality judgments in
social media text. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proc. of
ACL.
Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-
roni. 2013. Studying the recursive behaviour of
adjectival modification with compositional distribu-
tional semantics. In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
1012
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162?173,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Recall-Oriented Learning of Named Entities in Arabic Wikipedia
Behrang Mohit? Nathan Schneider? Rishav Bhowmick? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?P.O. Box 24866, Doha, Qatar ?Pittsburgh, PA 15213, USA
{behrang@,nschneid@cs.,rishavb@qatar.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the problem of NER in Arabic
Wikipedia, a semisupervised domain adap-
tation setting for which we have no labeled
training data in the target domain. To fa-
cilitate evaluation, we obtain annotations
for articles in four topical groups, allow-
ing annotators to identify domain-specific
entity types in addition to standard cate-
gories. Standard supervised learning on
newswire text leads to poor target-domain
recall. We train a sequence model and show
that a simple modification to the online
learner?a loss function encouraging it to
?arrogantly? favor recall over precision?
substantially improves recall and F1. We
then adapt our model with self-training
on unlabeled target-domain data; enforc-
ing the same recall-oriented bias in the self-
training stage yields marginal gains.1
1 Introduction
This paper considers named entity recognition
(NER) in text that is different from most past re-
search on NER. Specifically, we consider Arabic
Wikipedia articles with diverse topics beyond the
commonly-used news domain. These data chal-
lenge past approaches in two ways:
First, Arabic is a morphologically rich lan-
guage (Habash, 2010). Named entities are ref-
erenced using complex syntactic constructions
(cf. English NEs, which are primarily sequences
of proper nouns). The Arabic script suppresses
most vowels, increasing lexical ambiguity, and
lacks capitalization, a key clue for English NER.
Second, much research has focused on the use
of news text for system building and evaluation.
Wikipedia articles are not news, belonging instead
to a wide range of domains that are not clearly
1The annotated dataset and a supplementary document
with additional details of this work can be found at:
http://www.ark.cs.cmu.edu/AQMAR
delineated. One hallmark of this divergence be-
tween Wikipedia and the news domain is a dif-
ference in the distributions of named entities. In-
deed, the classic named entity types (person, or-
ganization, location) may not be the most apt for
articles in other domains (e.g., scientific or social
topics). On the other hand, Wikipedia is a large
dataset, inviting semisupervised approaches.
In this paper, we describe advances on the prob-
lem of NER in Arabic Wikipedia. The techniques
are general and make use of well-understood
building blocks. Our contributions are:
? A small corpus of articles annotated in a new
scheme that provides more freedom for annota-
tors to adapt NE analysis to new domains;
? An ?arrogant? learning approach designed to
boost recall in supervised training as well as
self-training; and
? An empirical evaluation of this technique as ap-
plied to a well-established discriminative NER
model and feature set.
Experiments show consistent gains on the chal-
lenging problem of identifying named entities in
Arabic Wikipedia text.
2 Arabic Wikipedia NE Annotation
Most of the effort in NER has been fo-
cused around a small set of domains and
general-purpose entity classes relevant to those
domains?especially the categories PER(SON),
ORG(ANIZATION), and LOC(ATION) (POL),
which are highly prominent in news text. Ara-
bic is no exception: the publicly available NER
corpora?ACE (Walker et al 2006), ANER (Be-
najiba et al 2008), and OntoNotes (Hovy et al
2006)?all are in the news domain.2 However,
2OntoNotes contains news-related text. ACE includes
some text from blogs. In addition to the POL classes, both
corpora include additional NE classes such as facility, event,
product, vehicle, etc. These entities are infrequent and may
not be comprehensive enough to cover the larger set of pos-
162
History Science Sports Technology
dev: Damascus Atom Rau?l Gonza?les Linux
Imam Hussein Shrine Nuclear power Real Madrid Solaris
test: Crusades Enrico Fermi 2004 Summer Olympics Computer
Islamic Golden Age Light Christiano Ronaldo Computer Software
Islamic History Periodic Table Football Internet
Ibn Tolun Mosque Physics Portugal football team Richard Stallman
Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System
Claudio Filippone (PER) 	??J. ?J

	
? ?K
X???; Linux (SOFTWARE) ??
	
JJ
?; Spanish
League (CHAMPIONSHIPS) ?


	
GAJ.?B@ ?

P?Y?@; proton (PARTICLE) 	??K?QK. ; nuclear
radiation (GENERIC-MISC) ?


??
	
J? @ ?A? ?B@; Real Zaragoza (ORG)

???

Q??? ?AK
P
Table 1: Translated titles
of Arabic Wikipedia arti-
cles in our development
and test sets, and some
NEs with standard and
article-specific classes.
Additionally, Prussia and
Amman were reserved
for training annotators,
and Gulf War for esti-
mating inter-annotator
agreement.
appropriate entity classes will vary widely by do-
main; occurrence rates for entity classes are quite
different in news text vs. Wikipedia, for instance
(Balasuriya et al 2009). This is abundantly
clear in technical and scientific discourse, where
much of the terminology is domain-specific, but it
holds elsewhere. Non-POL entities in the history
domain, for instance, include important events
(wars, famines) and cultural movements (roman-
ticism). Ignoring such domain-critical entities
likely limits the usefulness of the NE analysis.
Recognizing this limitation, some work on
NER has sought to codify more robust invento-
ries of general-purpose entity types (Sekine et al
2002; Weischedel and Brunstein, 2005; Grouin
et al 2011) or to enumerate domain-specific
types (Settles, 2004; Yao et al 2003). Coarse,
general-purpose categories have also been used
for semantic tagging of nouns and verbs (Cia-
ramita and Johnson, 2003). Yet as the number
of classes or domains grows, rigorously docu-
menting and organizing the classes?even for a
single language?requires intensive effort. Ide-
ally, an NER system would refine the traditional
classes (Hovy et al 2011) or identify new entity
classes when they arise in new domains, adapting
to new data. For this reason, we believe it is valu-
able to consider NER systems that identify (but
do not necessarily label) entity mentions, and also
to consider annotation schemes that allow annota-
tors more freedom in defining entity classes.
Our aim in creating an annotated dataset is to
provide a testbed for evaluation of new NER mod-
els. We will use these data as development and
sible NEs (Sekine et al 2002). Nezda et al(2006) anno-
tated and evaluated an Arabic NE corpus with an extended
set of 18 classes (including temporal and numeric entities);
this corpus has not been released publicly.
testing examples, but not as training data. In ?4
we will discuss our semisupervised approach to
learning, which leverages ACE and ANER data
as an annotated training corpus.
2.1 Annotation Strategy
We conducted a small annotation project on Ara-
bic Wikipedia articles. Two college-educated na-
tive Arabic speakers annotated about 3,000 sen-
tences from 31 articles. We identified four top-
ical areas of interest?history, technology, sci-
ence, and sports?and browsed these topics un-
til we had found 31 articles that we deemed sat-
isfactory on the basis of length (at least 1,000
words), cross-lingual linkages (associated articles
in English, German, and Chinese3), and subjec-
tive judgments of quality. The list of these arti-
cles along with sample NEs are presented in ta-
ble 1. These articles were then preprocessed to
extract main article text (eliminating tables, lists,
info-boxes, captions, etc.) for annotation.
Our approach follows ACE guidelines (LDC,
2005) in identifying NE boundaries and choos-
ing POL tags. In addition to this traditional form
of annotation, annotators were encouraged to ar-
ticulate one to three salient, article-specific en-
tity categories per article. For example, names
of particles (e.g., proton) are highly salient in the
Atom article. Annotators were asked to read the
entire article first, and then to decide which non-
traditional classes of entities would be important
in the context of article. In some cases, annotators
reported using heuristics (such as being proper
3These three languages have the most articles on
Wikipedia. Associated articles here are those that have been
manually hyperlinked from the Arabic page as cross-lingual
correspondences. They are not translations, but if the associ-
ations are accurate, these articles should be topically similar
to the Arabic page that links to them.
163
Token position agreement rate 92.6% Cohen?s ?: 0.86
Token agreement rate 88.3% Cohen?s ?: 0.86
Token F1 between annotators 91.0%
Entity boundary match F1 94.0%
Entity category match F1 87.4%
Table 2: Inter-annotator agreement measurements.
nouns or having an English translation which is
conventionally capitalized) to help guide their de-
termination of non-canonical entities and entity
classes. Annotators produced written descriptions
of their classes, including example instances.
This scheme was chosen for its flexibility: in
contrast to a scenario with a fixed ontology, anno-
tators required minimal training beyond the POL
conventions, and did not have to worry about
delineating custom categories precisely enough
that they would extend straightforwardly to other
topics or domains. Of course, we expect inter-
annotator variability to be greater for these open-
ended classification criteria.
2.2 Annotation Quality Evaluation
During annotation, two articles (Prussia and Am-
man) were reserved for training annotators on
the task. Once they were accustomed to anno-
tation, both independently annotated a third ar-
ticle. We used this 4,750-word article (Gulf War,

?J

	
K A

J? @ i. J
?
	
m?'@ H. Qk) to measure inter-annotator
agreement. Table 2 provides scores for token-
level agreement measures and entity-level F1 be-
tween the two annotated versions of the article.4
These measures indicate strong agreement for
locating and categorizing NEs both at the token
and chunk levels. Closer examination of agree-
ment scores shows that PER and MIS classes have
the lowest rates of agreement. That the mis-
cellaneous class, used for infrequent or article-
specific NEs, receives poor agreement is unsur-
prising. The low agreement on the PER class
seems to be due to the use of titles and descriptive
terms in personal names. Despite explicit guide-
lines to exclude the titles, annotators disagreed on
the inclusion of descriptors that disambiguate the
NE (e.g., the father in H.

B@ ??K. h. Qk. : George
Bush, the father).
4The position and boundary measures ignore the distinc-
tions between the POLM classes. To avoid artificial inflation
of the token and token position agreement rates, we exclude
the 81% of tokens tagged by both annotators as not belong-
ing to an entity.
History: Gulf War, Prussia, Damascus, Crusades
WAR CONFLICT ? ? ?
Science: Atom, Periodic table
THEORY ? CHEMICAL ? ?
NAME ROMAN ? PARTICLE ? ?
Sports: Football, Rau?l Gonza?les
SPORT ? CHAMPIONSHIP ?
AWARD ? NAME ROMAN ?
Technology: Computer, Richard Stallman
COMPUTER VARIETY ? SOFTWARE ?
COMPONENT ?
Table 3: Custom NE categories suggested by one or
both annotators for 10 articles. Article titles are trans-
lated from Arabic. ? indicates that both annotators vol-
unteered a category for an article; ? indicates that only
one annotator suggested the category. Annotators were
not given a predetermined set of possible categories;
rather, category matches between annotators were de-
termined by post hoc analysis. NAME ROMAN indi-
cates an NE rendered in Roman characters.
2.3 Validating Category Intuitions
To investigate the variability between annotators
with respect to custom category intuitions, we
asked our two annotators to independently read
10 of the articles in the data (scattered across our
four focus domains) and suggest up to 3 custom
categories for each. We assigned short names to
these suggestions, seen in table 3. In 13 cases,
both annotators suggested a category for an article
that was essentially the same (?); three such cat-
egories spanned multiple articles. In three cases
a category was suggested by only one annotator
(?).5 Thus, we see that our annotators were gen-
erally, but not entirely, consistent with each other
in their creation of custom categories. Further, al-
most all of our article-specific categories corre-
spond to classes in the extended NE taxonomy of
(Sekine et al 2002), which speaks to the reason-
ableness of both sets of categories?and by exten-
sion, our open-ended annotation process.
Our annotation of named entities outside of the
traditional POL classes creates a useful resource
for entity detection and recognition in new do-
mains. Even the ability to detect non-canonical
types of NEs should help applications such as QA
and MT (Toral et al 2005; Babych and Hart-
ley, 2003). Possible avenues for future work
include annotating and projecting non-canonical
5When it came to tagging NEs, one of the two annota-
tors was assigned to each article. Custom categories only
suggested by the other annotator were ignored.
164
NEs from English articles to their Arabic coun-
terparts (Hassan et al 2007), automatically clus-
tering non-canonical types of entities into article-
specific or cross-article classes (cf. Frietag, 2004),
or using non-canonical classes to improve the
(author-specified) article categories in Wikipedia.
Hereafter, we merge all article-specific cate-
gories with the generic MIS category. The pro-
portion of entity mentions that are tagged as MIS,
while varying to a large extent by document, is
a major indication of the gulf between the news
data (<10%) and the Wikipedia data (53% for the
development set, 37% for the test set).
Below, we aim to develop entity detection mod-
els that generalize beyond the traditional POL en-
tities. We do not address here the challenges of
automatically classifying entities or inferring non-
canonical groupings.
3 Data
Table 4 summarizes the various corpora used in
this work.6 Our NE-annotated Wikipedia sub-
corpus, described above, consists of several Ara-
bic Wikipedia articles from four focus domains.7
We do not use these for supervised training data;
they serve only as development and test data. A
larger set of Arabic Wikipedia articles, selected
on the basis of quality heuristics, serves as unla-
beled data for semisupervised learning.
Our out-of-domain labeled NE data is drawn
from the ANER (Benajiba et al 2007) and
ACE-2005 (Walker et al 2006) newswire cor-
pora. Entity types in this data are POL cate-
gories (PER, ORG, LOC) and MIS. Portions of the
ACE corpus were held out as development and
test data; the remainder is used in training.
4 Models
Our starting point for statistical NER is a feature-
based linear model over sequences, trained using
the structured perceptron (Collins, 2002).8
In addition to lexical and morphological9 fea-
6Additional details appear in the supplement.
7We downloaded a snapshot of Arabic Wikipedia
(http://ar.wikipedia.org) on 8/29/2009 and pre-
processed the articles to extract main body text and metadata
using the mwlib package for Python (PediaPress, 2010).
8A more leisurely discussion of the structured percep-
tron and its connection to empirical risk minimization can
be found in the supplementary document.
9We obtain morphological analyses from the MADA tool
(Habash and Rambow, 2005; Roth et al 2008).
Training words NEs
ACE+ANER 212,839 15,796
Wikipedia (unlabeled, 397 docs) 1,110,546 ?
Development
ACE 7,776 638
Wikipedia (4 domains, 8 docs) 21,203 2,073
Test
ACE 7,789 621
Wikipedia (4 domains, 20 docs) 52,650 3,781
Table 4: Number of words (entity mentions) in data sets.
tures known to work well for Arabic NER (Be-
najiba et al 2008; Abdul-Hamid and Darwish,
2010), we incorporate some additional features
enabled by Wikipedia. We do not employ a
gazetteer, as the construction of a broad-domain
gazetteer is a significant undertaking orthogo-
nal to the challenges of a new text domain like
Wikipedia.10 A descriptive list of our features is
available in the supplementary document.
We use a first-order structured perceptron; none
of our features consider more than a pair of con-
secutive BIO labels at a time. The model enforces
the constraint that NE sequences must begin with
B (so the bigram ?O, I? is disallowed).
Training this model on ACE and ANER data
achieves performance comparable to the state of
the art (F1-measure11 above 69%), but fares much
worse on our Wikipedia test set (F1-measure
around 47%); details are given in ?5.
4.1 Recall-Oriented Perceptron
By augmenting the perceptron?s online update
with a cost function term, we can incorporate a
task-dependent notion of error into the objective,
as with structured SVMs (Taskar et al 2004;
Tsochantaridis et al 2005). Let c(y,y?) denote
a measure of error when y is the correct label se-
quence but y? is predicted. For observed sequence
x and feature weights (model parameters) w, the
structured hinge loss is `hinge(x,y,w) =
max
y?
(
w>g(x,y?) + c(y,y?)
)
?w>g(x,y)
(1)
The maximization problem inside the parentheses
is known as cost-augmented decoding. If c fac-
10A gazetteer ought to yield further improvements in line
with previous findings in NER (Ratinov and Roth, 2009).
11Though optimizing NER systems for F1 has been called
into question (Manning, 2006), no alternative metric has
achieved widespread acceptance in the community.
165
tors similarly to the feature function g(x,y), then
we can increase penalties for y that have more
local mistakes. This raises the learner?s aware-
ness about how it will be evaluated. Incorporat-
ing cost-augmented decoding into the perceptron
leads to this decoding step:
y? ? arg max
y?
(
w>g(x,y?) + c(y,y?)
)
, (2)
which amounts to performing stochastic subgradi-
ent ascent on an objective function with the Eq. 1
loss (Ratliff et al 2006).
In this framework, cost functions can be for-
mulated to distinguish between different types of
errors made during training. For a tag sequence
y = ?y1, y2, . . . , yM ?, Gimpel and Smith (2010b)
define word-local cost functions that differently
penalize precision errors (i.e., yi = O ? y?i 6= O
for the ith word), recall errors (yi 6= O? y?i = O),
and entity class/position errors (other cases where
yi 6= y?i). As will be shown below, a key problem
in cross-domain NER is poor recall, so we will
penalize recall errors more severely:
c(y,y?) =
M?
i=1
?
?
?
0 if yi = y?i
? if yi 6= O ? y?i = O
1 otherwise
(3)
for a penalty parameter ? > 1. We call our learner
the ?recall-oriented? perceptron (ROP).
We note that Minkov et al(2006) similarly ex-
plored the recall vs. precision tradeoff in NER.
Their technique was to directly tune the weight
of a single feature?the feature marking O (non-
entity tokens); a lower weight for this feature will
incur a greater penalty for predicting O. Below
we demonstrate that our method, which is less
coarse, is more successful in our setting.12
In our experiments we will show that injecting
?arrogance? into the learner via the recall-oriented
loss function substantially improves recall, espe-
cially for non-POL entities (?5.3).
4.2 Self-Training and Semisupervised
Learning
As we will show experimentally, the differences
between news text and Wikipedia text call for do-
main adaptation. In the case of Arabic Wikipedia,
12The distinction between the techniques is that our cost
function adjusts the whole model in order to perform better
at recall on the training data.
Input: labeled data ??x(n),y(n)??Nn=1; unlabeled
data ?x?(j)?Jj=1; supervised learner L;
number of iterations T ?
Output: w
w? L(??x(n),y(n)??Nn=1)
for t = 1 to T ? do
for j = 1 to J do
y?(j) ? arg maxy w
>g(x?(j),y)
w? L(??x(n),y(n)??Nn=1 ? ??x?
(j), y?(j)??Jj=1)
Algorithm 1: Self-training.
there is no available labeled training data. Yet
the available unlabeled data is vast, so we turn to
semisupervised learning.
Here we adapt self-training, a simple tech-
nique that leverages a supervised learner (like the
perceptron) to perform semisupervised learning
(Clark et al 2003; Mihalcea, 2004; McClosky
et al 2006). In our version, a model is trained
on the labeled data, then used to label the un-
labeled target data. We iterate between training
on the hypothetically-labeled target data plus the
original labeled set, and relabeling the target data;
see Algorithm 1. Before self-training, we remove
sentences hypothesized not to contain any named
entity mentions, which we found avoids further
encouragement of the model toward low recall.
5 Experiments
We investigate two questions in the context of
NER for Arabic Wikipedia:
? Loss function: Does integrating a cost func-
tion into our learning algorithm, as we have
done in the recall-oriented perceptron (?4.1),
improve recall and overall performance on
Wikipedia data?
? Semisupervised learning for domain adap-
tation: Can our models benefit from large
amounts of unlabeled Wikipedia data, in addi-
tion to the (out-of-domain) labeled data? We
experiment with a self-training phase following
the fully supervised learning phase.
We report experiments for the possible combi-
nations of the above ideas. These are summarized
in table 5. Note that the recall-oriented percep-
tron can be used for the supervised learning phase,
for the self-training phase, or both. This leaves us
with the following combinations:
? reg/none (baseline): regular supervised learner.
? ROP/none: recall-oriented supervised learner.
166
Figure 1: Tuning the recall-oriented cost parame-
ter for different learning settings. We optimized
for development set F1, choosing penalty ? = 200
for recall-oriented supervised learning (in the plot,
ROP/*?this is regardless of whether a stage of
self-training will follow); ? = 100 for recall-
oriented self-training following recall-oriented su-
pervised learning (ROP/ROP); and ? = 3200 for
recall-oriented self-training following regular super-
vised learning (reg/ROP).
? reg/reg: standard self-training setup.
? ROP/reg: recall-oriented supervised learner, fol-
lowed by standard self-training.
? reg/ROP: regular supervised model as the initial la-
beler for recall-oriented self-training.
? ROP/ROP (the ?double ROP? condition): recall-
oriented supervised model as the initial labeler for
recall-oriented self-training. Note that the two
ROPs can use different cost parameters.
For evaluating our models we consider the
named entity detection task, i.e., recognizing
which spans of words constitute entities. This
is measured by per-entity precision, recall, and
F1.13 To measure statistical significance of differ-
ences between models we use Gimpel and Smith?s
(2010) implementation of the paired bootstrap re-
sampler of (Koehn, 2004), taking 10,000 samples
for each comparison.
5.1 Baseline
Our baseline is the perceptron, trained on the
POL entity boundaries in the ACE+ANER cor-
pus (reg/none).14 Development data was used to
select the number of iterations (10). We per-
formed 3-fold cross-validation on the ACE data
and found wide variance in the in-domain entity
detection performance of this model:
P R F1
fold 1 70.43 63.08 66.55
fold 2 87.48 81.13 84.18
fold 3 65.09 51.13 57.27
average 74.33 65.11 69.33
(Fold 1 corresponds to the ACE test set described
in table 4.) We also trained the model to perform
POL detection and classification, achieving nearly
identical results in the 3-way cross-validation of
ACE data. From these data we conclude that our
13Only entity spans that exactly match the gold spans are
counted as correct. We calculated these scores with the
conlleval.pl script from the CoNLL 2003 shared task.
14In keeping with prior work, we ignore non-POL cate-
gories for the ACE evaluation.
baseline is on par with the state of the art for Ara-
bic NER on ACE news text (Abdul-Hamid and
Darwish, 2010).15
Here is the performance of the baseline entity
detection model on our 20-article test set:16
P R F1
technology 60.42 20.26 30.35
science 64.96 25.73 36.86
history 63.09 35.58 45.50
sports 71.66 59.94 65.28
overall 66.30 35.91 46.59
Unsurprisingly, performance on Wikipedia data
varies widely across article domains and is much
lower than in-domain performance. Precision
scores fall between 60% and 72% for all domains,
but recall in most cases is far worse. Miscella-
neous class recall, in particular, suffers badly (un-
der 10%)?which partially accounts for the poor
recall in science and technology articles (they
have by far the highest proportion of MIS entities).
5.2 Self-Training
Following Clark et al(2003), we applied self-
training as described in Algorithm 1, with the
perceptron as the supervised learner. Our unla-
beled data consists of 397 Arabic Wikipedia ar-
ticles (1 million words) selected at random from
all articles exceeding a simple length threshold
(1,000 words); see table 4. We used only one iter-
ation (T ? = 1), as experiments on development
data showed no benefit from additional rounds.
Several rounds of self-training hurt performance,
15Abdul-Hamid and Darwish report as their best result a
macroaveraged F1-score of 76. As they do not specify which
data they used for their held-out test set, we cannot perform
a direct comparison. However, our feature set is nearly a
superset of their best feature set, and their result lies well
within the range of results seen in our cross-validation folds.
16Our Wikipedia evaluations use models trained on
POLM entity boundaries in ACE. Per-domain and overall
scores are microaverages across articles.
167
SELF-TRAINING
SUPERVISED none reg ROP
reg 66.3 35.9 46.59 66.7 35.6 46.41 59.2 40.3 47.97
ROP 60.9 44.7 51.59 59.8 46.2 52.11 58.0 47.4 52.16
Table 5: Entity detection precision, recall, and F1 for each learning setting, microaveraged across the 24 articles
in our Wikipedia test set. Rows differ in the supervised learning condition on the ACE+ANER data (regular
vs. recall-oriented perceptron). Columns indicate whether this supervised learning phase was followed by self-
training on unlabeled Wikipedia data, and if so which version of the perceptron was used for self-training.
baseline
entities words recall
PER 1081 1743 49.95
ORG 286 637 23.92
LOC 1019 1413 61.43
MIS 1395 2176 9.30
overall 3781 5969 35.91
Figure 2: Recall improve-
ment over baseline in the test
set by gold NER category,
counts for those categories in
the data, and recall scores for
our baseline model. Markers
in the plot indicate different
experimental settings corre-
sponding to cells in table 5.
an effect attested in earlier research (Curran et al
2007) and sometimes known as ?semantic drift.?
Results are shown in table 5. We find that stan-
dard self-training (the middle column) has very
little impact on performance.17 Why is this the
case? We venture that poor baseline recall and the
domain variability within Wikipedia are to blame.
5.3 Recall-Oriented Learning
The recall-oriented bias can be introduced in ei-
ther or both of the stages of our semisupervised
learning framework: in the supervised learn-
ing phase, modifying the objective of our base-
line (?5.1); and within the self-training algorithm
(?5.2).18 As noted in ?4.1, the aim of this ap-
proach is to discourage recall errors (false nega-
tives), which are the chief difficulty for the news
text?trained model in the new domain. We se-
lected the value of the false positive penalty for
cost-augmented decoding, ?, using the develop-
ment data (figure 1).
The results in table 5 demonstrate improve-
ments due to the recall-oriented bias in both
stages of learning.19 When used in the super-
17In neither case does regular self-training produce a sig-
nificantly different F1 score than no self-training.
18Standard Viterbi decoding was used to label the data
within the self-training algorithm; note that cost-augmented
decoding only makes sense in learning, not as a prediction
technique, since it deliberately introduces errors relative to a
correct output that must be provided.
19In terms of F1, the worst of the 3 models with the ROP
supervised learner significantly outperforms the best model
with the regular supervised learner (p < 0.005). The im-
vised phase (bottom left cell), the recall gains
are substantial?nearly 9% over the baseline. In-
tegrating this bias within self-training (last col-
umn of the table) produces a more modest im-
provement (less than 3%) relative to the base-
line. In both cases, the improvements to recall
more than compensate for the amount of degra-
dation to precision. This trend is robust: wher-
ever the recall-oriented perceptron is added, we
observe improvements in both recall and F1. Per-
haps surprisingly, these gains are somewhat addi-
tive: using the ROP in both learning phases gives
a small (though not always significant) gain over
alternatives (standard supervised perceptron, no
self-training, or self-training with a standard per-
ceptron). In fact, when the standard supervised
learner is used, recall-oriented self-training suc-
ceeds despite the ineffectiveness of standard self-
training.
Performance breakdowns by (gold) class, fig-
ure 2, and domain, figure 3, further attest to the
robustness of the overall results. The most dra-
matic gains are in miscellaneous class recall?
each form of the recall bias produces an improve-
ment, and using this bias in both the supervised
and self-training phases is clearly most success-
ful for miscellaneous entities. Correspondingly,
the technology and science domains (in which this
class dominates?83% and 61% of mentions, ver-
provements due to self-training are marginal, however: ROP
self-training produces a significant gain only following reg-
ular supervised learning (p < 0.05).
168
Figure 3: Supervised
learner precision vs.
recall as evaluated
on Wikipedia test
data in different
topical domains. The
regular perceptron
(baseline model) is
contrasted with ROP.
No self-training is
applied.
sus 6% and 12% for history and sports, respec-
tively) receive the biggest boost. Still, the gaps
between domains are not entirely removed.
Most improvements relate to the reduction of
false negatives, which fall into three groups:
(a) entities occurring infrequently or partially
in the labeled training data (e.g. uranium);
(b) domain-specific entities sharing lexical or con-
textual features with the POL entities (e.g. Linux,
titanium); and (c) words with Latin characters,
common in the science and technology domains.
(a) and (b) are mostly transliterations into Arabic.
An alternative?and simpler?approach to
controlling the precision-recall tradeoff is the
Minkov et al(2006) strategy of tuning a single
feature weight subsequent to learning (see ?4.1
above). We performed an oracle experiment to
determine how this compares to recall-oriented
learning in our setting. An oracle trained with
the method of Minkov et aloutperforms the three
models in table 5 that use the regular perceptron
for the supervised phase of learning, but under-
performs the supervised ROP conditions.20
Overall, we find that incorporating the recall-
oriented bias in learning is fruitful for adapting to
Wikipedia because the gains in recall outpace the
damage to precision.
6 Discussion
To our knowledge, this work is the first sugges-
tion that substantively modifying the supervised
learning criterion in a resource-rich domain can
reap benefits in subsequent semisupervised appli-
cation in a new domain. Past work has looked
20Tuning the O feature weight to optimize for F1 on our
test set, we found that oracle precision would be 66.2, recall
would be 39.0, and F1 would be 49.1. The F1 score of our
best model is nearly 3 points higher than the Minkov et al
style oracle, and over 4 points higher than the non-oracle
version where the development set is used for tuning.
at regularization (Chelba and Acero, 2006) and
feature design (Daume? III, 2007); we alter the
loss function. Not surprisingly, the double-ROP
approach harms performance on the original do-
main (on ACE data, we achieve 55.41% F1, far
below the standard perceptron). Yet we observe
that models can be prepared for adaptation even
before a learner is exposed a new domain, sacri-
ficing performance in the original domain.
The recall-oriented bias is not merely encour-
aging the learner to identify entities already seen
in training. As recall increases, so does the num-
ber of new entity types recovered by the model:
of the 2,070 NE types in the test data that were
never seen in training, only 450 were ever found
by the baseline, versus 588 in the reg/ROP condi-
tion, 632 in the ROP/none condition, and 717 in
the double-ROP condition.
We note finally that our method is a simple
extension to the standard structured perceptron;
cost-augmented inference is often no more ex-
pensive than traditional inference, and the algo-
rithmic change is equivalent to adding one addi-
tional feature. Our recall-oriented cost function
is parameterized by a single value, ?; recall is
highly sensitive to the choice of this value (fig-
ure 1 shows how we tuned it on development
data), and thus we anticipate that, in general, such
tuning will be essential to leveraging the benefits
of arrogance.
7 Related Work
Our approach draws on insights from work in
the areas of NER, domain adaptation, NLP with
Wikipedia, and semisupervised learning. As all
are broad areas of research, we highlight only the
most relevant contributions here.
Research in Arabic NER has been focused on
compiling and optimizing the gazetteers and fea-
169
ture sets for standard sequential modeling algo-
rithms (Benajiba et al 2008; Farber et al 2008;
Shaalan and Raza, 2008; Abdul-Hamid and Dar-
wish, 2010). We make use of features identi-
fied in this prior work to construct a strong base-
line system. We are unaware of any Arabic NER
work that has addressed diverse text domains like
Wikipedia. Both the English and Arabic ver-
sions of Wikipedia have been used, however, as
resources in service of traditional NER (Kazama
and Torisawa, 2007; Benajiba et al 2008). Attia
et al(2010) heuristically induce a mapping be-
tween Arabic Wikipedia and Arabic WordNet to
construct Arabic NE gazetteers.
Balasuriya et al(2009) highlight the substan-
tial divergence between entities appearing in En-
glish Wikipedia versus traditional corpora, and
the effects of this divergence on NER perfor-
mance. There is evidence that models trained
on Wikipedia data generalize and perform well
on corpora with narrower domains. Nothman
et al(2009) and Balasuriya et al(2009) show
that NER models trained on both automatically
and manually annotated Wikipedia corpora per-
form reasonably well on news corpora. The re-
verse scenario does not hold for models trained
on news text, a result we also observe in Arabic
NER. Other work has gone beyond the entity de-
tection problem: Florian et al(2004) addition-
ally predict within-document entity coreference
for Arabic, Chinese, and English ACE text, while
Cucerzan (2007) aims to resolve every mention
detected in English Wikipedia pages to a canoni-
cal article devoted to the entity in question.
The domain and topic diversity of NEs has been
studied in the framework of domain adaptation
research. A group of these methods use self-
training and select the most informative features
and training instances to adapt a source domain
learner to the new target domain. Wu et al(2009)
bootstrap the NER leaner with a subset of unla-
beled instances that bridge the source and target
domains. Jiang and Zhai (2006) and Daume? III
(2007) make use of some labeled target-domain
data to tune or augment the features of the source
model towards the target domain. Here, in con-
trast, we use labeled target-domain data only for
tuning and evaluation. Another important dis-
tinction is that domain variation in this prior
work is restricted to topically-related corpora (e.g.
newswire vs. broadcast news), whereas in our
work, major topical differences distinguish the
training and test corpora?and consequently, their
salient NE classes. In these respects our NER
setting is closer to that of Florian et al(2010),
who recognize English entities in noisy text, (Sur-
deanu et al 2011), which concerns information
extraction in a topically distinct target domain,
and (Dalton et al 2011), which addresses English
NER in noisy and topically divergent text.
Self-training (Clark et al 2003; Mihalcea,
2004; McClosky et al 2006) is widely used
in NLP and has inspired related techniques that
learn from automatically labeled data (Liang et
al., 2008; Petrov et al 2010). Our self-training
procedure differs from some others in that we use
all of the automatically labeled examples, rather
than filtering them based on a confidence score.
Cost functions have been used in non-
structured classification settings to penalize cer-
tain types of errors more than others (Chan and
Stolfo, 1998; Domingos, 1999; Kiddon and Brun,
2011). The goal of optimizing our structured NER
model for recall is quite similar to the scenario ex-
plored by Minkov et al(2006), as noted above.
8 Conclusion
We explored the problem of learning an NER
model suited to domains for which no labeled
training data are available. A loss function to en-
courage recall over precision during supervised
discriminative learning substantially improves re-
call and overall entity detection performance, es-
pecially when combined with a semisupervised
learning regimen incorporating the same bias.
We have also developed a small corpus of Ara-
bic Wikipedia articles via a flexible entity an-
notation scheme spanning four topical domains
(publicly available at http://www.ark.cs.
cmu.edu/AQMAR).
Acknowledgments
We thank Mariem Fekih Zguir and Reham Al Tamime
for assistance with annotation, Michael Heilman for
his tagger implementation, and Nizar Habash and col-
leagues for the MADA toolkit. We thank members of
the ARK group at CMU, Hal Daume?, and anonymous
reviewers for their valuable suggestions. This publica-
tion was made possible by grant NPRP-08-485-1-083
from the Qatar National Research Fund (a member of
the Qatar Foundation). The statements made herein
are solely the responsibility of the authors.
170
References
Ahmed Abdul-Hamid and Kareem Darwish. 2010.
Simplified feature set for Arabic named entity
recognition. In Proceedings of the 2010 Named En-
tities Workshop, pages 110?115, Uppsala, Sweden,
July. Association for Computational Linguistics.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-
ica Monachini, and Josef van Genabith. 2010.
An automatically built named entity lexicon for
Arabic. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, Mike Rosner, and Daniel Tapias, ed-
itors, Proceedings of the Seventh Conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Bogdan Babych and Anthony Hartley. 2003. Im-
proving machine translation quality with automatic
named entity recognition. In Proceedings of the 7th
International EAMT Workshop on MT and Other
Language Technology Tools, EAMT ?03.
Dominic Balasuriya, Nicky Ringland, Joel Nothman,
Tara Murphy, and James R. Curran. 2009. Named
entity recognition in Wikipedia. In Proceedings
of the 2009 Workshop on The People?s Web Meets
NLP: Collaboratively Constructed Semantic Re-
sources, pages 10?18, Suntec, Singapore, August.
Association for Computational Linguistics.
Yassine Benajiba, Paolo Rosso, and Jose? Miguel
Bened??Ruiz. 2007. ANERsys: an Arabic named
entity recognition system based on maximum en-
tropy. In Alexander Gelbukh, editor, Proceedings
of CICLing, pages 143?153, Mexico City, Mexio.
Springer.
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic named entity recognition using optimized
feature sets. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 284?293, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Philip K. Chan and Salvatore J. Stolfo. 1998. To-
ward scalable learning with non-uniform class and
cost distributions: a case study in credit card fraud
detection. In Proceedings of the Fourth Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 164?168, New York City, New
York, USA, August. AAAI Press.
Ciprian Chelba and Alex Acero. 2006. Adaptation of
maximum entropy capitalizer: Little data can help
a lot. Computer Speech and Language, 20(4):382?
399.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
168?175.
Stephen Clark, James Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 49?55.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1?
8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual
Exclusion Bootstrapping. In Proceedings of PA-
CLING, 2007.
Jeffrey Dalton, James Allan, and David A. Smith.
2011. Passage retrieval for incorporating global
evidence in sequence labeling. In Proceedings of
the 20th ACM International Conference on Infor-
mation and Knowledge Management (CIKM ?11),
pages 355?364, Glasgow, Scotland, UK, October.
ACM.
Hal Daume? III. 2007. Frustratingly easy domain
adaptation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 256?263, Prague, Czech Republic,
June. Association for Computational Linguistics.
Pedro Domingos. 1999. MetaCost: a general method
for making classifiers cost-sensitive. Proceedings
of the Fifth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining,
pages 155?164.
Benjamin Farber, Dayne Freitag, Nizar Habash, and
Owen Rambow. 2008. Improving NER in Arabic
using a morphological tagger. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, and Daniel Tapias,
editors, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), pages
2509?2514, Marrakech, Morocco, May. European
Language Resources Association (ELRA).
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A
statistical model for multilingual entity detection
and tracking. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, Proceedings of the Hu-
man Language Technology Conference of the North
171
American Chapter of the Association for Compu-
tational Linguistics: HLT-NAACL 2004, page 18,
Boston, Massachusetts, USA, May. Association for
Computational Linguistics.
Radu Florian, John Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection ro-
bustness to noisy input. In Proceedings of EMNLP
2010, pages 335?345, Cambridge, MA, October.
Association for Computational Linguistics.
Dayne Freitag. 2004. Trained named entity recog-
nition using distributional clusters. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 262?269, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2010a. Softmax-
margin CRFs: Training log-linear models with loss
functions. In Proceedings of the Human Language
Technologies Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 733?736, Los Angeles, California,
USA, June.
Kevin Gimpel and Noah A. Smith. 2010b.
Softmax-margin training for structured log-
linear models. Technical Report CMU-LTI-
10-008, Carnegie Mellon University. http:
//www.lti.cs.cmu.edu/research/
reports/2010/cmulti10008.pdf.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Karn Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: from guidelines to evaluation,
an overview. In Proceedings of the 5th Linguis-
tic Annotation Workshop, pages 92?100, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Nizar Habash and Owen Rambow. 2005. Arabic to-
kenization, part-of-speech tagging and morpholog-
ical disambiguation in one fell swoop. In Proceed-
ings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
573?580, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan and Claypool Pub-
lishers.
Ahmed Hassan, Haytham Fahmy, and Hany Hassan.
2007. Improving named entity translation by ex-
ploiting comparable and parallel corpora. In Pro-
ceedings of the Conference on Recent Advances
in Natural Language Processing (RANLP ?07),
Borovets, Bulgaria.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the Human Language Technology Conference of
the NAACL (HLT-NAACL), pages 57?60, New York
City, USA, June. Association for Computational
Linguistics.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Peas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1466?1475, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2006. Exploit-
ing domain structure for named entity recognition.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL (HLT-NAACL), pages
74?81, New York City, USA, June. Association for
Computational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2007.
Exploiting Wikipedia as external knowledge for
named entity recognition. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 698?707, Prague, Czech Republic,
June. Association for Computational Linguistics.
Chloe Kiddon and Yuriy Brun. 2011. That?s what
she said: double entendre identification. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 89?94, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
LDC. 2005. ACE (Automatic Content Extraction)
Arabic annotation guidelines for entities, version
5.3.3. Linguistic Data Consortium, Philadelphia.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for fea-
tures. In Proceedings of the 25th International Con-
ference on Machine Learning (ICML), pages 592?
599, Helsinki, Finland.
Chris Manning. 2006. Doing named entity recogni-
tion? Don?t optimize for F1. http://nlpers.
blogspot.com/2006/08/doing-named-
entity-recognition-dont.html.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In HLT-NAACL
2004 Workshop: Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004),
Boston, Massachusetts, USA.
172
Einat Minkov, Richard Wang, Anthony Tomasic, and
William Cohen. 2006. NER systems that suit user?s
preferences: adjusting the recall-precision trade-off
for entity extraction. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 93?96,
New York City, USA, June. Association for Com-
putational Linguistics.
Luke Nezda, Andrew Hickl, John Lehmann, and Sar-
mad Fayyaz. 2006. What in the world is a Shahab?
Wide coverage named entity recognition for Arabic.
In Proccedings of LREC, pages 41?46.
Joel Nothman, Tara Murphy, and James R. Curran.
2009. Analysing Wikipedia and gold-standard cor-
pora for NER training. In Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL 2009),
pages 612?620, Athens, Greece, March. Associa-
tion for Computational Linguistics.
PediaPress. 2010. mwlib. http://code.
pediapress.com/wiki/wiki/mwlib.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705?713, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. Subgradient methods for maxi-
mum margin structured learning. In ICML Work-
shop on Learning in Structured Output Spaces,
Pittsburgh, Pennsylvania, USA.
Ryan Roth, Owen Rambow, Nizar Habash, Mona
Diab, and Cynthia Rudin. 2008. Arabic morpho-
logical tagging, diacritization, and lemmatization
using lexeme models and feature ranking. In Pro-
ceedings of ACL-08: HLT, pages 117?120, Colum-
bus, Ohio, June. Association for Computational
Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Pro-
ceedings of LREC.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Nigel Collier, Patrick Ruch, and Adeline
Nazarenko, editors, COLING 2004 International
Joint workshop on Natural Language Processing in
Biomedicine and its Applications (NLPBA/BioNLP)
2004, pages 107?110, Geneva, Switzerland, Au-
gust. COLING.
Khaled Shaalan and Hafsa Raza. 2008. Arabic
named entity recognition from diverse text types. In
Advances in Natural Language Processing, pages
440?451. Springer.
Mihai Surdeanu, David McClosky, Mason R. Smith,
Andrey Gusev, and Christopher D. Manning. 2011.
Customizing an information extraction system to
a new domain. In Proceedings of the ACL 2011
Workshop on Relational Models of Semantics, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004. Max-margin Markov networks. In Sebastian
Thrun, Lawrence Saul, and Bernhard Scho?lkopf,
editors, Advances in Neural Information Processing
Systems 16. MIT Press.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question an-
swering using named entity recognition. Natu-
ral Language Processing and Information Systems,
3513/2005:181?191.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484, September.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multi-
lingual training corpus. LDC2006T06, Linguistic
Data Consortium, Philadelphia.
Ralph Weischedel and Ada Brunstein. 2005.
BBN pronoun coreference and entity type cor-
pus. LDC2005T33, Linguistic Data Consortium,
Philadelphia.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1523?1532, Singapore, August.
Association for Computational Linguistics.
Tianfang Yao, Wei Ding, and Gregor Erbach. 2003.
CHINERS: a Chinese named entity recognition sys-
tem for the sports domain. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing, pages 55?62, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
173
Weighted and Probabilistic Context-Free
Grammars Are Equally Expressive
Noah A. Smith?
Carnegie Mellon University
Mark Johnson??
Brown University
This article studies the relationship between weighted context-free grammars (WCFGs), where
each production is associated with a positive real-valued weight, and probabilistic context-free
grammars (PCFGs), where the weights of the productions associated with a nonterminal are
constrained to sum to one. Because the class of WCFGs properly includes the PCFGs, one
might expect that WCFGs can describe distributions that PCFGs cannot. However, Z. Chi
(1999, Computational Linguistics, 25(1):131?160) and S. P. Abney, D. A. McAllester, and
P. Pereira (1999, In Proceedings of the 37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 542?549, College Park, MD) proved that every WCFG distribution
is equivalent to some PCFG distribution. We extend their results to conditional distributions,
and show that every WCFG conditional distribution of parses given strings is also the condi-
tional distribution defined by some PCFG, even when the WCFG?s partition function diverges.
This shows that any parsing or labeling accuracy improvement from conditional estimation of
WCFGs or conditional random fields (CRFs) over joint estimation of PCFGs or hidden Markov
models (HMMs) is due to the estimation procedure rather than the change in model class,
because PCFGs and HMMs are exactly as expressive as WCFGs and chain-structured CRFs,
respectively.
1. Introduction
In recent years the field of computational linguistics has turned to machine learning
to aid in the development of accurate tools for language processing. A widely used
example, applied to parsing and tagging tasks of various kinds, is a weighted grammar.
Adding weights to a formal grammar allows disambiguation (more generally, ranking
of analyses) and can lead to more efficient parsing. Machine learning comes in when we
wish to choose those weights empirically.
The predominant approach for many years was to select a probabilistic model?
such as a hidden Markov model (HMM) or probabilistic context-free grammar
(PCFG)?that defined a distribution over the structures allowed by a grammar. Given a
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15217, USA.
E-mail: nasmith@cs.cmu.edu.
?? Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912, USA.
E-mail: Mark Johnson@brown.edu.
Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for
publication: 30 March 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
treebank, maximum likelihood estimation can be applied to learn the probability values
in the model.
More recently, new machine learning methods have been developed or ex-
tended to handle models of grammatical structure. Notably, conditional estimation
(Ratnaparkhi, Roukos, and Ward 1994; Johnson et al 1999; Lafferty, McCallum, and
Pereira 2001), maximum margin estimation (Taskar et al 2004), and unsupervised con-
trastive estimation (Smith and Eisner 2005) have been applied to structured models.
Weighted grammars learned in this way differ in two important ways from traditional,
generative models. First, the weights can be any positive value; they need not sum
to one. Second, features can ?overlap,? and it can be difficult to design a generative
model that uses such features. The benefits of new features and discriminative training
methods are widely documented and recognized.
This article focuses specifically on the first of these differences. It compares the
expressive power of weighted context-free grammars (WCFGs), where each rule is
associated with a positive weight, to that of the corresponding PCFGs, that is, with
the same rules but where the weights of the rules expanding a nonterminal must sum
to one.
One might expect that because normalization removes one or more degrees of free-
dom, unnormalized models should be more expressive than normalized, probabilistic
models. Perhaps counterintuitively, previous work has shown that the classes of proba-
bility distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, and
Pereira 1999; Chi 1999).
However, this result does not completely settle the question about the expressive
power of WCFGs and PCFGs. As we show herein, a WCFG can define a conditional
distribution from strings to trees even if it does not define a probability distribution
over trees. Because these conditional distributions are what are used in classification
tasks and related tasks such as parsing, we need to know the relationship between
the classes of conditional distributions defined by WCFGs and PCFGs. In fact we
extend the results of Chi and of Abney et al, and show that WCFGs and PCFGs both
define the same class of conditional distribution. Moreover, we present an algorithm
for converting an arbitrary WCFG that defines a conditional distribution over trees
given strings but possibly without a finite partition function into a PCFG with the
same rules as the WCFG and that defines the same conditional distribution over trees
given strings.
This means that maximum conditional likelihood WCFGs are non-identifiable, be-
cause there are an infinite number of rule weights all of which maximize the conditional
likelihood.
2. Weighted CFGs
A CFG G is a tuple ?N, S,?, R? where N is a finite set of nonterminal symbols, S ? N is
the start symbol, ? is a finite set of terminal symbols (disjoint from N), and R is a set of
production rules of the form X ? ? where X ? N and ? ? (N ? ?). A WCFG associates
a positive number called the weight with each rule in R.1 We denote by ?X?? the weight
attached to the rule X ? ?, and the vector of rule weights by ? = {?A?? : A ? ? ? R}.
A weighted grammar is the pair G? = ?G,??.
1 Assigning a weight of zero to a rule equates to excluding it from R.
478
Smith and Johnson Weighted and Probabilistic CFGs
Unless otherwise specified, we assume a fixed underlying context-free grammar G.
Let ?(G) be the set of (finite) trees that G generates. For any ? ? ?(G), the score s?(?) of
? is defined as follows:
s?(?) =
?
(X??)?R
(?X??)
f (X??;?) (1)
where f (X ? ?; ?) is the number of times X ? ? is used in the derivation of the tree ?.
The partition function Z(?) is the sum of the scores of the trees in ?(G).
Z(?) =
?
???(G)
s?(?)
Because we have imposed no constraints on?, the partition function need not equal
one; indeed, as we show subsequently the partition function need not even exist. If Z(?)
is finite then we say that the WCFG is convergent, and we can define a Gibbs probability
distribution over ?(G) by dividing by Z(?):
P?(?) =
s?(?)
Z(?)
A probabilistic CFG, or PCFG, is a WCFG in which the sum of the weights of the
rules expanding each nonterminal is one:
?X ? N,
?
(X??)?R
?X?? = 1 (2)
It is easy to show that if G? is a PCFG then Z(?) ? 1. A tight PCFG is a PCFG G?
for which Z(?) = 1. Necessary conditions and sufficient conditions for a PCFG to be
tight are given in several places, including Booth and Thompson (1973) and Wetherell
(1980).
We now describe the results of Chi (1999) and Abney, McAllester, and Pereira (1999).
Let G = {G?} denote the set of the WCFGs based on the CFG G (i.e., the WCFGs in G
all have the same underlying grammar G but differ in their rule weight vectors ?).
Let GZ<? be the subset of G for which the partition function Z(?) is finite, and let
GZ=? = G \ GZ<? be the subset of G with an infinite partition function. Further let GPCFG
denote the set of PCFGs based on G. In general, GPCFG is a proper subset of GZ<?, that
is, every PCFG is also a WCFG, but because there are weight vectors ? that don?t obey
Equation 2, not all WCFGs are PCFGs.
However, this does not mean that WCFGs are more expressive than PCFGs. As
noted above, the WCFGs GZ<? define Gibbs distributions. Again, for a fixed G, let
PZ<? be the probability distributions over the trees ?(G) defined by the WCFGs GZ<?
and let PPCFG be the probability distributions defined by the PCFGs GPCFG. Chi (Propo-
sition 4) and Abney, McAllester, and Pereira (Lemma 5) showed that PZ<? = PPCFG,
namely, that every WCFG probability distribution is in fact generated by some PCFG.
There is no ?PZ=?? because there is no finite normalizing term Z(?) for such WCFGs.
479
Computational Linguistics Volume 33, Number 4
2.1 Chi?s Algorithm for Converting WCFGs to Equivalent PCFGs
Chi (1999) describes an algorithm for converting a WCFG to an equivalent PCFG. Let
G? be a WCFG in GZ<?. If X ? N is a nonterminal, let ?X(G) be the set of trees rooted
in X that can be built using G. Then define:
ZX(?) =
?
???X (G)
s?(?)
For simplicity, let Zt(?) = 1 for all t ? ?. Chi demonstrated that G? ? GZ<? implies
that ZX(?) is finite for all X ? N ? ?.
For every rule X ? ? in R define:
??X?? =
?X??
|?|
?
i=1
Z?i (?)
ZX(?)
where ?i is the ith element of ? and |?| is the length of ?. Chi proved that G?? is a PCFG
and that P?? (?) = s?(?)/Z(?) for all trees ? ? ?(G).
Chi did not describe how to compute the nonterminal-specific partition functions
ZX(?). The ZX(?) are related by equations of the form
ZX(?) =
?
?:X???R
?X??
|?|
?
i=1
Z?i (?)
which constitute a set of nonlinear polynomial equations in ZX(?). Although a numeri-
cal solver might be employed to find the ZX(?), we have found that in practice iterative
propagation of weights following the method described by Stolcke (1995, Section 4.7.1)
converges quickly when Z(?) is finite.
3. Classifiers and Conditional Distributions
A common application of weighted grammars is parsing. One way to select a parse tree
for a sentence x is to choose the maximum weighted parse that is consistent with the
observation x:
??(x) = argmax
???(G):y(?)=x
s?(?) (3)
where y(?) is the yield of ?. Other decision criteria exist, including minimum-loss de-
coding and re-ranked n-best decoding. All of these classifiers use some kind of dynamic
programming algorithm to optimize over trees, and they also exploit the conditional
distribution of trees given sentence observations. A WCFG defines such a conditional
distribution as follows:
P?(? | x) =
s?(?)
?
????(G):y(?? )=x s?(??)
=
s?(?)
Zx(?)
(4)
480
Smith and Johnson Weighted and Probabilistic CFGs
where Zx(?) is the sum of scores for all parses of x. Note that Equation (4) will be
ill-defined when Zx(?) diverges. Because Zx(?) is constant for a given x, solving Equa-
tion (3) is equivalent to choosing ? to maximize P?(? | x).
We turn now to classes of these conditional distribution families. Let CZ<? (CPCFG)
be the class of conditional distribution families that can be expressed by grammars in
GZ<? (GPCFG, respectively). It should be clear that, because PZ<? = PPCFG, CZ<? = CPCFG
since a conditional family is derived by normalizing a joint distribution by its marginals.
We now define another subset of G. Let GZn<? contain every WCFG G? = ?G,??
such that, for all n ? 0,
Zn(?) =
?
???(G):|y(?)|=n
s?(?) < ? (5)
(Note that, to be fully rigorous, we should quantify n in GZn<?, writing ?G?nZn(?)<?.?
We use the abbreviated form to keep the notation crisp.) For any G? ? GZn<?, it also
follows that, for any x ? L(G), Zx(?) < ?; the converse holds as well.
It follows that any WCFG in GZn<? can be used to construct a conditional dis-
tribution of trees given the sentence, for any sentence x ? L(G). To do so, we only
need to normalize s?(?) by Zx(?) (Equation (4)). Let GZn=? contain the WCFGs where
some Zn(?) diverge; this is a subset of GZ=?.2 To see that GZ=? ? GZn<? = ?, consider
Example 1.
Example 1
?A?A A = 1, ?A?a = 1
This grammar produces binary structures over strings in a+. Every such tree receives
score 1. Because there are infinitely many trees, Z(?) diverges. But for any fixed string
an, the number of parse trees is finite. This grammar defines a uniform conditional
distribution over all binary trees, given the string.
For a grammar G? to be in GZn<?, it is sufficient that, for every nonterminal X ? N,
the sum of scores of all cyclic derivations X ?+ X be finite. Conservatively, this can be
forced by eliminating epsilon rules and unary rules or cycles altogether, or by requiring
the sum of cyclic derivations for every nonterminal X to sum to strictly less than one.
Example 2 gives a grammar in GZn=? with a unary cyclic derivation that does not
?dampen.?
Example 2
?A?A A = 1, ?A?A = 1, ?A?a = 1
For any given an, there are infinitely many equally weighted parse trees, so even the
set of trees for an cannot be normalized into a distribution (Zn(?) =?). Generally
speaking, if there exists a string x ? L(G) such that the set of trees that derive x is not
2 Here, full rigor would require quantification of n, writing ?G?nZn (?)=?.?
481
Computational Linguistics Volume 33, Number 4
finite (i.e., there is no finite bound on the number of derivations for strings in L(G); the
grammar in Example 2 is a simple example), then GZn<? and GZ<? are separable.3
For a given CFG G, a conditional distribution over trees given strings is a function
?? ? (?(G) ? [0, 1]). Our notation for the set of conditional distributions that can be
expressed by GZn<? is CZn<?. Note that there is no ?CZn=?? because an infinite Zn(?)
implies an infinite Z(x) for some sentence x and therefore an ill-formed conditional fam-
ily. Indeed, it is difficult to imagine a scenario in computational linguistics in which non-
dampening cyclic derivations (WCFGs in GZn=?) are desirable, because no linguistic
explanations depend crucially on arbitrary lengthening of cyclic derivations.
We now state our main theorem.
Theorem 1
For a given CFG G, CZn<? = CZ<?.
Proof
Suppose we are given weights ? for G such that G? ? GZn<?. We will show that the
sequence Z1(?), Z2(?), ... is bounded by an exponential function of n, then describe
a transformation on ? resulting in a new grammar G?? that is in GZ<? and defines
the same family of conditional distributions (i.e., ?? ? ?(G),?x ? L(G), P?(? | x) =
P?? (? | x)).
First we prove that for all n ? 1 there exists some c such that Zn(?) ? cn. Given G?,
we construct G??? in CNF that preserves the total score for any x ? L(G). The existence
of G??? was demonstrated by Goodman (1998, Section 2.6), who gives an algorithm for
constructing the value-preserving weighted grammar G??? from G?.
Note that G? = ?N?, S,?, R??, containing possibly more nonterminals and rules than G.
The set of (finite) trees ?
(
G?
)
is different from ?(G); the new trees must be binary and
may include new nonterminals.
Next, collapse the nonterminals in N? into one nonterminal, S. The resulting gram-
mar is G??? = ??{S}, S,?, R??, ???. R? contains the rule S ? S S and rules of the form S ? a
for a ? ?. The weights of these rules are
??S?S S = ? = max(1,
?
(X?Y Z)?R?
??X?Y Z) (6)
??S?a = ? = max(1,
?
(X?b)?R?
??X?b) (7)
The grammar G??? will allow every tree allowed by G??? (modulo labels on nonterminal
nodes, which are now all S). It may allow some additional trees. The score of a tree
under G??? will be at least as great as the sum of scores of all structurally equivalent trees
under G???, because ? and ? are defined to be large enough to absorb all such scores. It
follows that, for all x ? L(G):
s??(x) ? s??(x) = s?(x) (8)
3 We are grateful to an anonymous reviewer for pointing this out, and an even stronger point: for a given
G, G and GZn<? have a nonempty set-difference if and only if G has infinite ambiguity (some x ? L(G)
has infinitely many parse trees).
482
Smith and Johnson Weighted and Probabilistic CFGs
Summing over all trees of any given yield length n, we have
Zn(??) ? Zn(??) = Zn(?) (9)
G? generates all possible binary trees (with internal nodes undifferentiated) over
a given sentence x in L(G). Every tree generated by G? with yield length n will have
the same score: ?n?1?n, because every binary tree with n terminals has exactly n ? 1
nonterminals. Each tree corresponds to a way of bracketing n items, so the total number
of parse trees generated by G? for a string of length n is the number of different ways
of bracketing a sequence of n items. The total number of unlabeled binary bracketings
of an n-length sequence is the nth Catalan number Cn (Graham, Knuth, and Patashnik
1994), which in turn is bounded above by 4n (Vardi 1991). The total number of strings of
length n is |?|n. Therefore
Zn(??) = Cn|?|n?n?1?n ? 4n|?|n?n?1?n ? (4|?|??)n (10)
We now transform the original weights ? as follows. For every rule (X ? ?) ? R,
let
??X?? ?
?X??
(8|?|??)t(?) (11)
where t(?) is the number of ? symbols appearing in ?. This transformation results in
every n-length sentence having its score divided by (8|?|??)n. The relative scores of
trees with the same yield are unaffected, because they are all scaled equally. Therefore
G?? defines the same conditional distribution over trees given sentences as G?, which
implies that G? and G?? have the same highest scoring parses. Note that any sufficiently
large value could stand in for 8|?|?? to both (a) preserve the conditional distribution
and (b) force Zn(?) to converge. We have not found the minimum such value, but 8|?|??
is sufficiently large.
The sequence of Zn(?) now converges:
Zn(??) ?
Zn(?)
(8|?|??)n ?
(
1
2
)n
(12)
Hence Z(??) =
??
n=0 Zn(?
?) ? 2 and G?? ? GZ<?. 
Corollary 1
Given a CFG G, CZn<? = CPCFG.
Proof
By Theorem 1, CZn<? = CZ<?. We know that PZ<? = PPCFG, from which it follows that
CZ<? = CPCFG. Hence CZn<? = CPCFG. To convert a WCFG in CZn<? into a PCFG, first
apply the transformation in the proof of Theorem 1 to get a convergent WCFG, then
apply Chi?s method (our Section 2.1). 
483
Computational Linguistics Volume 33, Number 4
Figure 1
A graphical depiction of the primary result of this article. Given a fixed set of productions, G is
the set of WCFGs with exactly those productions (i.e., they vary only in the production weights),
GZ<? is the subset of G that defines (joint) probability distributions over trees (i.e., that have a
finite partition function Z) and PZ<? is the set of probability distributions defined by grammars
in GZ<?. Chi (1999) and Abney, McAllester, and Pereira (1999) proved that PZ<? is
the same as PPCFG, the set of probability distributions defined by the PCFG GPCFG with the same
productions as G. Thus even though the set of WCFGs properly includes the set of PCFGs,
WCFGs define exactly the same probability distributions over trees as PCFGs. This article
extends these results to conditional distributions over trees conditioned on their strings. Even
though the set GZn<? of WCFGs that define conditional distributions may be larger than GZ<?
and properly includes GPCFG, the set of conditional distributions CZn<? defined by GZn<? is
equal to the set of conditional distributions CPCFG defined by PCFGs. Our proof is constructive:
we give an algorithm which takes as input a WCFG G ? GZn<? and returns a PCFG which
defines the same conditional distribution over trees given strings as G.
Figure 1 presents the main result graphically in the context of earlier results.
4. HMMs and Related Models
Hidden Markov models (HMMs) are a special case of PCFGs. The structures they
produce are labeled sequences, which are equivalent to right-branching trees. We can
write an HMM as a PCFG with restricted types of rules. We will refer to the unweighted,
finite-state grammars that HMMs stochasticize as ?right-linear grammars.? Rather than
using the production rule notation of PCFGs, we will use more traditional HMM nota-
tion and refer to states (interchangeable with nonterminals) and paths (interchangeable
with parse trees).
In the rest of the article we distinguish between HMMs, which are probabilistic
finite-state automata locally normalized just like a PCFG, and chain-structured Markov
random fields (MRFs; Section 4.1), in which moves or transitions are associated with
positive weights and which are globally normalized like a WCFG.4 We also distinguish
two different types of dependency structures in these automata. Abusing the standard
terminology somewhat, in a Mealy automaton arcs are labeled with output or terminal
symbols, whereas in a Moore automaton the states emit terminal symbols.5
4 We admit that these names are somewhat misleading, because as we will show, chain-structured MRFs
also have the Markov property and define the same joint and conditional distributions as HMMs.
5 In formal language theory both Mealy and Moore machines are finite-state transducers (Mealy 1955;
Moore 1956); we ignore the input symbols here.
484
Smith and Johnson Weighted and Probabilistic CFGs
A Mealy HMM defines a probability distribution over pairs ?x,??, where x is a
length-n sequence ?x1, x2, ..., xn? ? ?n and ? = ??0,?1,?2, ...,?n? ? Nn+1 is a state (or
nonterminal) path. The distribution is given by
PHMM(x,?) =
(
n
?
i=1
p(xi,?i | ?i?1)
)
p(STOP | ?n) (13)
?0 is assumed, for simplicity, to be constant and known; we also assume that every
state transition emits a symbol (no 
 arcs), an assumption made in typical tagging and
chunking applications of HMMs. We can convert a Mealy HMM to a PCFG by including,
for every tuple ?x,?,?? (x ? ? and ?,? ? N) such that p(x,? | ?) > 0, the rule ? ? x ?,
with the same probability as the corresponding HMM transition. For every ? such that
p(STOP | ?), we include the rule ? ? 
, with probability p(STOP | ?).
A Moore HMM factors the distribution p(x,? | ?) into p(x | ?) ? p(? | ?). A Moore
HMM can be converted to a PCFG by adding a new nonterminal ?? for every state ?
and including the rules ? ? ?? (with probability p(? | ?)) and ?? ? x ? (with probability
p(x | ?)). Stop probabilities are added as in the Mealy case. For a fixed number of states,
Moore HMMs are less probabilistically expressive than Mealy HMMs, though we can
convert between the two with a change in the number of states.
We consider Mealy HMMs primarily from here on. If we wish to define the distri-
bution over paths given words, we conditionalize
PHMM(? | x) =
(
?n
i=1 p(xi,?i | ?i?1)
)
p(STOP | ?n)
?
???Nn+1
(
?n
i=1 p(xi,?
?
i | ??i?1)
)
p(STOP | ??n)
(14)
This is how scores are assigned when selecting the best path given a sequence.
For a grammar G that is right-linear, we can therefore talk about the set of HMM
(right-linear) grammars GHMM, the set of probability distributions PHMM defined by those
grammars, and CHMM, the set of conditional distributions over state paths (trees) that they
define.6
4.1 Mealy Markov Random Fields
When the probabilities in Mealy HMMs are replaced by arbitrary positive weights, the
production rules can be seen as features in a Gibbs distribution. The resulting model
is a type of MRF with a chain structure; these have recently become popular in natural
language processing (Lafferty, McCallum, and Pereira 2001). Lafferty et al?s formulation
defined a conditional distribution over paths given sequences by normalizing for each
sequence x:
PCMRF(? | x) =
(
n
?
i=1
??i?1,xi,?i
)
??n,STOP
Zx(?)
(15)
6 Of course, the right-linear grammar is a CFG, so we could also use the notation GPCFG, PPCFG, and CPCFG.
485
Computational Linguistics Volume 33, Number 4
Using a single normalizing term Z(?), we can also define a joint distribution over
states and paths:
PCMRF(x,?) =
(
n
?
i=1
??i?1,xi,?i
)
??n,STOP
Z(?)
(16)
Let G = {G?} denote the set of weighted grammars based on the unweighted right-
linear grammar G. We call these weighted grammars ?Mealy MRFs.? As in the WCFG
case, we can add the constraint Zn(?) < ? (for all n), giving the class GZn<?.
Recall that, in the WCFG case, the move from G to GZn<? had to do with cyclic
derivations. The analogous move in the right-linear grammar case involves 
 emis-
sions (production rules of the form X ? Y). If, as in typical applications of finite-state
models to natural language processing, there are no rules of the form X ? Y, then
GZn<? is empty and GZn<? = G. Our formulae, in fact, assume that there are no 

emissions.
Because Mealy MRFs are a special case of WCFGs, Theorem 1 applies to them.
This means that any random field using Mealy HMM features (Mealy MRF) such that
?n, Zn(?) < ? can be transformed into a Mealy HMM that defines the same conditional
distribution of tags given words.7
Corollary 2
For a given right-linear grammar G, CHMM = CZ<? = CZn<?.
Lafferty, McCallum, and Pereira?s conditional random fields are typically trained to
optimize a different objective function than HMMs (conditional likelihood and joint
likelihood, respectively). Our result shows that optimizing either objective on the set
of Mealy HMMs as opposed to Mealy MRFs will achieve the same result, modulo
imperfections in the numerical search for parameter values.
4.2 Maximum-Entropy Markov Models
While HMMs and chain MRFs represent the same set of conditional distributions, we
can show that the maximum-entropy Markov models (MEMMs) of McCallum, Freitag,
and Pereira (2000) represent a strictly smaller class of distributions.
An MEMM is a similar model with a different event structure. It defines the distri-
bution over paths given words as:
PMEMM(? | x) =
n
?
i=1
p(?i | ?i?1, xi) (17)
Unlike an HMM, the MEMM does not define a distribution over output sequences x.
The name ?maximum entropy Markov model? comes from the fact that the conditional
7 What if we allow additional features? It can be shown that, as long as the vocabulary ? is finite and
known, we can convert any such MRF with potential functions on state transitions and emissions into
an HMM functioning equivalently as a classifier. If ? is not fully known, then we cannot sum over all
emissions from each state, and we cannot use Chi?s method (Section 2.1) to convert to a PCFG (HMM).
486
Smith and Johnson Weighted and Probabilistic CFGs
distributions p(? | ?, x) typically have a log-linear form, rather than a multinomial form,
and are trained to maximize entropy.
Lemma 1
For every MEMM, there is a Mealy MRF that represents the same conditional distribu-
tion over paths given symbols.
Proof
By definition, the features of the MRF include triples ??i?1, xi,?i?. Assign to the
weight ??i,xj,?k the value PMEMM(?i | ?k, xj). Assign to ??i,STOP the value 1. In computing
PCMRF(? | x) (Equation (15)), the normalizing term for each x will be equal to 1. 
MEMMs, like HMMs, are defined by locally normalized conditional multinomial
distributions. This has computational advantages (no potentially infinite Z(?) terms to
compute). However, the set of conditional distributions of labels given terminals that
can be expressed by MEMMs is strictly smaller than those expressible by HMMs (and
by extension, Mealy MRFs).
Theorem 2
For a given right-linear grammar G, CMEMM ? CHMM.
Proof
We give an example of a Mealy HMM whose conditional distribution over paths (trees)
given sentences cannot be represented by an MEMM. We thank Michael Collins for
pointing out to us the existence of examples like this one. Define a Mealy HMM with
three states named 0, 1, and 2, over an alphabet {a, b, c}, as follows. State 0 is the start
state.
Example 3
Under this model, PHMM(0, 1, 1 | a, b) = PHMM(0, 2, 2 | a, c) = 1. These conditional dis-
tributions cannot both be met by any MEMM. To see why, consider
p(1 | 0, a) ? p(1 | 1, b) = p(2 | 0, a) ? p(2 | 2, c) = 1
This implies that
p(1 | 0, a) = p(1 | 1, b) = p(2 | 0, a) = p(2 | 2, c) = 1
487
Computational Linguistics Volume 33, Number 4
But it is impossible for p(1 | 0, a) = p(2 | 0, a) = 1. This holds regardless of the form of
the distribution p(? | ?, x) (e.g., multinomial or log-linear).
Because P(0, 1, 1 | a, b) = P(0, 2, 2 | a, c) cannot be met by any MEMM, there are
distributions in the family allowed by HMMs that cannot be expressed as MEMMs,
and the latter are less expressive. 
It is important to note that this result applies to Mealy HMMs; our result compares
models with the same dependencies among random variables. If the HMM?s distribu-
tion p(xi,?i | ?i?1) is factored into p(xi | ?i) ? p(?i | ?i?1) (i.e., it is a Moore HMM), then
there may exist an MEMM with the same number of states that can represent some
distributions that the Moore HMM cannot.8
One can also imagine MEMMs in which p(?i | ?i?1, xi, ...) is conditioned on more
surrounding context (xi?1 or xi+1, or the entire sequence x, for example). Conditioning
on more context can be done by increasing the order of the Markov model?all of
our models so far have been first-order, with a memory of only the previous state.
Our result can be extended to include higher-order MEMMs. Suppose we allow the
MEMM to ?look ahead? n words, factoring its distribution into p(?i | ?i?1, xi, xi+1, ...,
xi+n).
Corollary 3
A first-order Mealy HMM can represent some classifiers that no MEMM with finite
lookahead can represent.
Proof
Consider again Example 3. Note that, for all m ? 1, it sets
PHMM(0,
m 1?s
? ?? ?
1, ..., 1 | amb) = 1
PHMM(0, 2, ..., 2
? ?? ?
m 2?s
| amc) = 1
Suppose we wish to capture this in an MEMM with n symbols of look-ahead. Letting
m = n + 1,
p(1 | 0, an+1) ? p(1 | 1, anb) ?
n
?
i=1
p(1 | 1, an?ib) = 1
p(2 | 0, an+1) ? p(2 | 2, anc) ?
n
?
i=1
p(2 | 2, an?ic) = 1
The same issue arises as in the proof of Theorem 2: it cannot be that p(1 | 0, an+1) =
p(2 | 0, an+1) = 1, and so this MEMM does not exist. Note that even if we allow the
8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the
distribution.
488
Smith and Johnson Weighted and Probabilistic CFGs
MEMM to ?look back? and condition on earlier symbols (or states), it cannot represent
the distribution in Example 3. 
Generally speaking, this limitation of MEMMs has nothing to do with the estima-
tion procedure (we have committed to no estimation procedure in particular) but rather
with the conditional structure of the model. That some model structures work better
than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning
(2002). Our result?that the class of distributions allowed by MEMMs is a strict subset
of those allowed by Mealy HMMs?makes this unsurprising.
5. Practical Implications
Our result is that weighted generalizations of classical probabilistic grammars (PCFGs
and HMMs) are no more powerful than the probabilistic models. This means that, inso-
far as log-linear models for NLP tasks like tagging and parsing are more successful
than their probabilistic cousins, it is due to either (a) additional features added to
the model, (b) improved estimation procedures (e.g., maximum conditional likelihood
estimation or contrastive estimation), or both. (Note that the choice of estimation proce-
dure (b) is in principle orthogonal to the choice of model, and conditional estimation
should not be conflated with log-linear modeling.) For a given estimation criterion,
weighted CFGs, and Mealy MRFs, in particular, cannot be expected to behave any
differently than PCFGs and HMMs, respectively, unless they are augmented with more
features.
6. Related Work
Abney, McAllester, and Pereira (1999) addressed the relationship between PCFGs and
probabilistic models based on push-down automaton operations (e.g., the structured
language model of Chelba and Jelinek, 1998). They proved that, although the conversion
may not be simple (indeed, a blow-up in the automaton?s size may be incurred), given
G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down
automata are weakly equivalent. Importantly, the standard conversion of a CFG into a
shift-reduce PDA, when applied in the stochastic case, does not always preserve the prob-
ability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further
work on the relationship between weighted CFGs and weighted PDAs is described in
Nederhof and Satta (2004).
MacKay (1996) proved that linear Boltzmann chains (a class of weighted models
that is essentially the same as Moore MRFs) express the same set of distributions as
Moore HMMs, under the condition that the Boltzmann chain has a single specific end
state. MacKay avoided the divergence problem by defining the Boltzmann chain always
to condition on the length of the sequence; he tacitly requires all of his models to be in
GZn<?. We have suggested a more applicable notion of model equivalence (equivalence
of the conditional distribution) and our Theorem 1 generalizes to context-free models.
7. Conclusion
We have shown that weighted CFGs that define finite scores for all sentences in their
languages have no greater expressivity than PCFGs, when used to define distributions
489
Computational Linguistics Volume 33, Number 4
over trees given sentences. This implies that the standard Mealy MRF formalism is
no more powerful than Mealy HMMs, for instance. We have also related ?maximum
entropy Markov models? to Mealy Markov random fields, showing that the former is a
strictly less expressive weighted formalism.
Acknowledgments
This work was supported by a Fannie and
John Hertz Foundation fellowship to
N. Smith at Johns Hopkins University. The
views expressed are not necessarily endorsed
by the sponsors. We are grateful to three
anonymous reviewers for feedback that
improved the article, to Michael Collins for
encouraging exploration of this matter and
helpful comments on a draft, and to Jason
Eisner and Dan Klein for insightful
conversations. Any errors are the sole
responsibility of the authors.
References
Abney, Steven P., David A. McAllester,
and Fernando Pereira. 1999. Relating
probabilistic grammars and automata.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics,
pages 542?549, College Park, MD.
Booth, Taylor L. and Richard A. Thompson.
1973. Applying probability measures to
abstract languages. IEEE Transactions on
Computers, 22(5):442?450.
Chelba, Ciprian and Frederick Jelinek.
1998. Exploiting syntactic structure for
language modeling. In Proceedings
of the 36th Annual Meeting of the
Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 325?331,
Montreal, Canada.
Chi, Zhiyi. 1999. Statistical properties of
probabilistic context-free grammars.
Computational Linguistics, 25(1):131?160.
Goodman, Joshua T. 1998. Parsing Inside-Out.
Ph.D. thesis, Harvard University,
Cambridge, MA.
Graham, Ronald L., Donald E. Knuth,
and Oren Patashnik. 1994. Concrete
Mathematics. Addison-Wesley,
Reading, MA.
Johnson, Mark. 2001. Joint and conditional
estimation of tagging and parsing models.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics,
pages 314?321, Toulouse, France.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
?unification-based? grammars.
In Proceedings of the 37th Annual
Conference of the Association for
Computational Linguistics, pages 535?541,
College Park, MD.
Klein, Dan and Christopher D. Manning.
2002. Conditional structure versus
conditional estimation in NLP models.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 9?16, Philadelphia, PA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the 18th International
Conference on Machine Learning,
pages 282?289, Williamstown, MA.
MacKay, David J. C. 1996. Equivalence of
linear Boltzmann chains and hidden
Markov models. Neural Computation,
8(1):178?181.
McCallum, Andrew, Dayne Freitag, and
Fernando Pereira. 2000. Maximum
entropy Markov models for information
extraction and segmentation. In
Proceedings of the 17th International
Conference on Machine Learning,
pages 591?598, Palo Alto, CA.
Mealy, G. H. 1955. A method for
synthesizing sequential circuits.
Bell System Technology Journal,
34:1045?1079.
Moore, Edward F. 1956. Gedanken-
experiments on sequential machines.
In Automata Studies, number 34 in
Annals of Mathematics Studies.
Princeton University Press, Princeton,
NJ, pages 129?153.
Nederhof, Mark-Jan and Giorgio Satta.
2004. Probabilistic parsing strategies.
In Proceedings of the 42nd Annual
Meeting of the Association for
Computational Linguistics, pages 543?550,
Barcelona, Spain.
Ratnaparkhi, Adwait, Salim Roukos,
and R. Todd Ward. 1994. A maximum
entropy model for parsing. In Proceedings
of the International Conference on Spoken
Language Processing, pages 803?806,
Yokohama, Japan.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
490
Smith and Johnson Weighted and Probabilistic CFGs
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics, pages 354?362, Ann Arbor, MI.
Stolcke, Andreas. 1995. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Computational Linguistics,
21(2):165?201.
Taskar, Ben, Dan Klein, Michael Collins,
Daphne Koller, and Christopher Manning.
2004. Max-margin parsing. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing, pages 1?8,
Barcelona, Spain.
Vardi, Ilan. 1991. Computational Recreations in
Mathematica. Addison-Wesley, Redwood
City, CA.
Wetherell, C. S. 1980. Probabilistic languages:
A review and some open questions.
Computing Surveys, 12:361?379.
491

Empirical Risk Minimization for
Probabilistic Grammars: Sample
Complexity and Hardness of Learning
Shay B. Cohen?
Columbia University
Noah A. Smith??
Carnegie Mellon University
Probabilistic grammars are generative statistical models that are useful for compositional and
sequential structures. They are used ubiquitously in computational linguistics. We present a
framework, reminiscent of structural risk minimization, for empirical risk minimization of prob-
abilistic grammars using the log-loss. We derive sample complexity bounds in this framework
that apply both to the supervised setting and the unsupervised setting. By making assumptions
about the underlying distribution that are appropriate for natural language scenarios, we are able
to derive distribution-dependent sample complexity bounds for probabilistic grammars. We also
give simple algorithms for carrying out empirical risk minimization using this framework in both
the supervised and unsupervised settings. In the unsupervised case, we show that the problem of
minimizing empirical risk is NP-hard. We therefore suggest an approximate algorithm, similar
to expectation-maximization, to minimize the empirical risk.
1. Introduction
Learning from data is central to contemporary computational linguistics. It is in com-
mon in such learning to estimate a model in a parametric family using the maximum
likelihood principle. This principle applies in the supervised case (i.e., using anno-
tated data) as well as semisupervised and unsupervised settings (i.e., using unan-
notated data). Probabilistic grammars constitute a range of such parametric families
we can estimate (e.g., hidden Markov models, probabilistic context-free grammars).
These parametric families are used in diverse NLP problems ranging from syntactic
and morphological processing to applications like information extraction, question
answering, and machine translation.
Estimation of probabilistic grammars, in many cases, indeed starts with the prin-
ciple of maximum likelihood estimation (MLE). In the supervised case, and with
traditional parametrizations based on multinomial distributions, MLE amounts to
? Department of Computer Science, Columbia University, New York, NY 10027, United States.
E-mail: scohen@cs.columbia.edu. This research was completed while the first author was at Carnegie
Mellon University.
?? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States.
E-mail: nasmith@cs.cmu.edu.
Submission received: 1 November 2010; revised submission received: 21 June 2011; accepted for publication:
3 August 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
normalization of rule frequencies as they are observed in data. In the unsupervised case,
on the other hand, algorithms such as expectation-maximization are available. MLE is
attractive because it offers statistical consistency if some conditions are met (i.e., if the
data are distributed according to a distribution in the family, then we will discover the
correct parameters if sufficient data is available). In addition, under some conditions it
is also an unbiased estimator.
An issue that has been far less explored in the computational linguistics literature
is the sample complexity of MLE. Here, we are interested in quantifying the number of
samples required to accurately learn a probabilistic grammar either in a supervised
or in an unsupervised way. If bounds on the requisite number of samples (known as
?sample complexity bounds?) are sufficiently tight, then they may offer guidance to
learner performance, given various amounts of data and a wide range of parametric
families. Being able to reason analytically about the amount of data to annotate, and
the relative gains in moving to a more restricted parametric family, could offer practical
advantages to language engineers.
We note that grammar learning has been studied in formal settings as a problem of
grammatical inference?learning the structure of a grammar or an automaton (Angluin
1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008,
among others). Our setting in this article is different. We assume that we have a fixed
grammar, and our goal is to estimate its parameters. This approach has shown great
empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005)
and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and
Manning 2004; Cohen and Smith 2010a) settings. There has also been some discus-
sion of sample complexity bounds for statistical parsing models, in a distribution-free
setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis
of natural language, as it has to account for pathological cases of distributions that
generate data.
We develop a framework for deriving sample complexity bounds using the max-
imum likelihood principle for probabilistic grammars in a distribution-dependent
setting. Distribution dependency is introduced here by making empirically justified
assumptions about the distributions that generate the data. Our framework uses and
significantly extends ideas that have been introduced for deriving sample complexity
bounds for probabilistic graphical models (Dasgupta 1997). Maximum likelihood esti-
mation is put in the empirical risk minimization framework (Vapnik 1998) with the loss
function being the log-loss. Following that, we develop a set of learning theoretic tools
to explore rates of estimation convergence for probabilistic grammars. We also develop
algorithms for performing empirical risk minimization.
Much research has been devoted to the problem of learning finite state automata
(which can be thought of as a class of grammars) in the Probably Approximately Correct
setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989;
Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting:
Error is measured as the probability mass of strings that are not identified correctly by
the learned finite state automaton, instead of measuring KL divergence between the
automaton and the true distribution. In addition, in many cases, there is also a focus on
the distribution-free setting. To the best of our knowledge, it is still an open problem
whether finite state automata are learnable in the distribution-dependent setting when
measuring the error as the fraction of misidentified strings. Other work (Ron 1995; Ron,
Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also gives
treatment to probabilistic automata with an error measure which is more suitable for
the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance.
480
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
These also focus on learning the structure of finite state machines. As mentioned earlier,
in our setting we assume that the grammar is fixed, and that our goal is to estimate its
parameters.
We note an important connection to an earlier study about the learnability of
probabilistic automata and hidden Markov models by Abe and Warmuth (1992). In
that study, the authors provided positive results for the sample complexity for learning
probabilistic automata?they showed that a polynomial sample is sufficient for MLE.
We demonstrate positive results for the more general class of probabilistic grammars
which goes beyond probabilistic automata. Abe and Warmuth also showed that the
problem of finding or even approximating the maximum likelihood solution for a two-
state probabilistic automaton with an alphabet of an arbitrary size is hard. Even though
these results extend to probabilistic grammars to some extent, we provide a novel proof
that illustrates the NP-hardness of identifying the maximum likelihood solution for
probabilistic grammars in the specific framework of ?proper approximations? that we
define in this article. Whereas Abe and Warmuth show that the problem of maximum
likelihood maximization for two-state HMMs is not approximable within a certain
factor in time polynomial in the alphabet and the length of the observed sequence, we
show that there is no polynomial algorithm (in the length of the observed strings) that
identifies the maximum likelihood estimator in our framework. In our reduction, from
3-SAT to the problem of maximum likelihood estimation, the alphabet used is binary
and the grammar size is proportional to the length of the formula. In Abe andWarmuth,
the alphabet size varies, and the number of states is two.
This article proceeds as follows. In Section 2 we review the background necessary
from Vapnik?s (1988) empirical risk minimization framework. This framework is re-
duced to maximum likelihood estimation when a specific loss function is used: the log-
loss.1 There are some shortcomings in using the empirical risk minimization framework
in its simplest form. In its simplest form, the ERM framework is distribution-free, which
means that we make no assumptions about the distribution that generated the data.
Naively attempting to apply the ERM framework to probabilistic grammars in the
distribution-free setting does not lead to the desired sample complexity bounds. The
reason for this is that the log-loss diverges whenever small probabilities are allocated in
the learned hypothesis to structures or strings that have a rather large probability in the
probability distribution that generates the data. With a distribution-free assumption,
therefore, we would have to give treatment to distributions that are unlikely to be
true for natural language data (e.g., where some extremely long sentences are very
probable).
To correct for this, we move to an analysis in a distribution-dependent setting, by
presenting a set of assumptions about the distribution that generates the data. In Sec-
tion 3 we discuss probabilistic grammars in a general way and introduce assumptions
about the true distribution that are reasonable when our data come from natural lan-
guage examples. It is important to note that this distribution need not be a probabilistic
grammar.
The next stepwe take, in Section 4, is approximating the set of probabilistic grammars
over which we maximize likelihood. This is again required in order to overcome the
divergence of the log-loss for probabilities that are very small. Our approximations are
1 It is important to remember that minimizing the log-loss does not equate to minimizing the error of a
linguistic analyzer or natural language processing application. In this article we focus on the log-loss
case because we believe that probabilistic models of language phenomena have inherent usefulness
as explanatory tools in computational linguistics, aside from their use in systems.
481
Computational Linguistics Volume 38, Number 3
based on bounded approximations that have been used for deriving sample complexity
bounds for graphical models in a distribution-free setting (Dasgupta 1997).
Our approximations have two important properties: They are, by themselves, prob-
abilistic grammars from the family we are interested in estimating, and they become a
tighter approximation around the family of probabilistic grammars we are interested in
estimating as more samples are available.
Moving to the distribution-dependent setting and defining proper approximations
enables us to derive sample complexity bounds. In Section 5 we present the sample
complexity results for both the supervised and unsupervised cases. A question that
lingers at this point is whether it is computationally feasible to maximize likelihood
in our framework even when given enough samples.
In Section 6, we describe algorithms we use to estimate probabilistic grammars
in our framework, when given access to the required number of samples. We show
that in the supervised case, we can indeed maximize likelihood in our approximation
framework using a simple algorithm. For the unsupervised case, however, we show that
maximizing likelihood is NP-hard. This fact is related to a notion known in the learning
theory literature as inherent unpredictability (Kearns and Vazirani 1994): Accurate
learning is computationally hard evenwith enough samples. To overcome this difficulty,
we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977)
to approximately maximize likelihood (or minimize log-loss) in the unsupervised case
with proper approximations.
In Section 7 we discuss some related ideas. These include the failure of an alter-
native kind of distributional assumption and connections to regularization by maxi-
mum a posteriori estimation with Dirichlet priors. Longer proofs are included in the
appendices. A table of notation that is used throughout is included as Table D.1 in
Appendix D.
This article builds on two earlier papers. In Cohen and Smith (2010b) we presented
the main sample complexity results described here; the present article includes signifi-
cant extensions, a deeper analysis of our distributional assumptions, and a discussion of
variants of these assumptions, as well as related work, such as that about the Tsybakov
noise condition. In Cohen and Smith (2010c) we proved NP-hardness for unsupervised
parameter estimation of probalistic context-free grammars (PCFGs) (without approxi-
mate families). The present article uses a similar type of proof to achieve results adapted
to empirical risk minimization in our approximation framework.
2. Empirical Risk Minimization and Maximum Likelihood Estimation
We begin by introducing some notation. We seek to construct a predictive model that
maps inputs from space X to outputs from space Z. In this work, X is a set of strings
using some alphabet ? (X ? ??), and Z is a set of derivations allowed by a grammar
(e.g., a context-free grammar). We assume the existence of an unknown joint probability
distribution p(x, z) over X? Z. (For the most part, we will be discussing discrete input
and output spaces. This means that p will denote a probability mass function.) We are
interested in estimating the distribution p from examples, either in a supervised setting,
where we are provided with examples of the form (x, z) ? X? Z, or in the unsupervised
setting, where we are provided only with examples of the form x ? X. We first consider
the supervised setting and return to the unsupervised setting in Section 5. We will use
q to denote the estimated distribution.
482
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
In order to estimate p as accurately as possible using q(x, z), we are interested in
minimizing the log-loss, that is, in finding qopt, from a fixed family of distributions Q
(also called ?the concept space?), such that
qopt = argmin
q?Q
Ep
[
? log q
]
= argmin
q?Q
?
?
x,z
p(x, z) log q(x, z) (1)
Note that if p ? Q, then this quantity achieves theminimumwhen qopt = p, in which case
the value of the log-loss is the entropy of p. Indeed, more generally, this optimization is
equivalent to finding q such that it minimizes the KL divergence from p to q.
Because p is unknown, we cannot hope to minimize the log-loss directly. Given a
set of examples (x1, z1), . . . , (xn, zn), however, there is a natural candidate, the empirical
distribution p?n, for use in Equation (1) instead of p, defined as:
p?n(x, z) = n
?1
n
?
i=1
I {(x, z) = (xi, zi)}
where I {(x, z) = (xi, zi)} is 1 if (x, z) = (xi, zi) and 0 otherwise.2 We then set up the
problem as the problem of empirical risk minimization (ERM), that is, trying to find q
such that
q? = argmin
q?Q
Ep?n
[
? log q
]
(2)
= argmin
q?Q
?n?1
n
?
i=1
log q(xi, zi)
= argmax
q?Q
n?1
n
?
i=1
log q(xi, zi) (3)
Equation (3) immediately shows that minimizing empirical risk using the log-loss is
equivalent to the maximizing likelihood, which is a common statistical principle used
for estimating a probabilistic grammar in computational linguistics (Charniak 1993;
Manning and Schu?tze 1999).3
As mentioned earlier, our goal is to estimate the probability distribution p while
quantifying how accurate our estimate is. One way to quantify the estimation accuracy
is by bounding the excess risk, which is defined as
Ep(q;Q) = Ep(q)  Ep
[
? log q
]
?min
q??Q
Ep
[
? log q?
]
(4)
We are interested in bounding the excess risk for q?, Ep(q
?). The excess risk is
reduced to KL divergence between p and q if p ? Q, because in this case the quantity
minq??Q E
[
? log q?
]
is minimized with q? = p, and equals the entropy of p. In a typical
2 We note that p?n itself is a random variable, because it depends on the sample drawn from p.
3 We note that being able to attain the minimum through an hypothesis q? is not necessarily possible in
the general case. In our instantiations of ERM for probabilistic grammars, however, the minimum can be
attained. In fact, in the unsupervised case the minimum can be attained by more than a single hypothesis.
In these cases, q? is arbitrarily chosen to be one of these minimizers.
483
Computational Linguistics Volume 38, Number 3
case, where we do not necessarily have p ? Q, then the excess risk of q is bounded from
above by the KL divergence between p and q.
We can bound the excess risk by showing the double-sided convergence of the
empirical process Rn(Q), defined as follows:
Rn(Q)  sup
q?Q
?
?Ep?n
[
? log q
]
? Ep
[
? log q
]
?
?? 0 (5)
as n? ?. For any  > 0, if, for large enough n it holds that
sup
q?Q
?
?Ep?n
[
? log q
]
? Ep
[
? log q
]
?
? <  (6)
(with high probability), then we can ?sandwich? the following quantities:
Ep
[
? log qopt
]
? Ep
[
? log q?
]
(7)
? Ep?n
[
? log q?
]
+ 
? Ep?n
[
? log qopt
]
+ 
? Ep
[
? log qopt
]
+ 2 (8)
where the inequalities come from the fact that qopt minimizes the expected risk
Ep
[
? log q
]
for q ? Q, and q? minimizes the empirical risk Ep?n
[
? log q
]
for q ? Q. The
consequence of Equations (7) and (8) is that the expected risk of q? is at most 2 away
from the expected risk of qopt, and as a result, we find the excess risk Ep(q
?), for large
enough n, is smaller than 2. Intuitively, this means that, under a large sample, q? does
not give much worse results than qopt under the criterion of the log-loss.
Unfortunately, the regularity conditions which are required for the convergence of
Rn(Q) do not hold because the log-loss can be unbounded. This means that a modifi-
cation is required for the empirical process in a way that will actually guarantee some
kind of convergence. We give a treatment to this in the next section.
We note that all discussion of convergence in this section has been about conver-
gence in probability. For example, we want Equation (6) to hold with high probability?
for most samples of size n. We will make this notion more rigorous in Section 2.2.
2.1 Empirical Risk Minimization and Structural Risk Minimization Methods
It has been noted in the literature (Vapnik 1998; Koltchinskii 2006) that often the class Q
is too complex for empirical risk minimization using a fixed number of data points.
It is therefore desirable in these cases to create a family of subclasses {Q? | ? ? A}
that have increasing complexity. The more data we have, the more complex our Q?
can be for empirical risk minimization. Structural risk minimization (Vapnik 1998) and
the method of sieves (Grenander 1981) are examples of methods that adopt such an
approach. Structural risk minimization, for example, can be represented in many cases
as a penalization of the empirical risk method, using a regularization term.
In our case, the level of ?complexity? is related to allocation of small probabilities to
derivations in the grammar by a distribution q ? Q. The basic problem is this: Whenever
we have a derivation with a small probability, the log-loss becomes very large (in
absolute value), and this makes it hard to show the convergence of the empirical process
484
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Rn(Q). Because grammars can define probability distributions over infinitely many
discrete outcomes, probabilities can be arbitrarily small and log-loss can be arbitrarily
large.
To solve this issuewith the complexity ofQ, we define in Section 4 a series of approx-
imations {Qn | n ? N} for probabilistic grammars such that
?
n Qn = Q. Our framework
for empirical riskminimization is then set up tominimize the empirical risk with respect
to Qn, where n is the number of samples we draw for the learner:
q?n = argmin
q?Qn
Ep?n
[
? log q
]
(9)
We are then interested in the convergence of the empirical process
Rn(Qn) = sup
q?Qn
?
?Ep?n
[
? log q
]
? Ep
[
? log q
]
?
? (10)
In Section 4 we show that the minimizer q?n is an asymptotic empirical risk minimizer
(in our specific framework), which means that Ep
[
? log q?n
]
? Ep
[
? log q?
]
. Because
we have
?
n Qn = Q, the implication of having asymptotic empirical risk minimization
is that we have Ep(q
?
n ;Qn) ? Ep(q
?;Q).
2.2 Sample Complexity Bounds
Knowing that we are interested in the convergence of Rn(Qn) = supq?Qn |Ep?n
[
? log q
]
?
Ep
[
? log q
]
|, a natural question to ask is: ?At what rate does this empirical process
converge??
Because the quantity Rn(Qn) is a random variable, we need to give a probabilistic
treatment to its convergence. More specifically, we ask the question that is typically
asked when learnability is considered (Vapnik 1998): ?How many samples n are re-
quired so that with probability 1? ? we have Rn(Qn) < ?? Bounds on this number
of samples are also called ?sample complexity bounds,? and in a distribution-free
setting they are described as a function N(, ?,Q), independent of the distribution p that
generates the data.
A complete distribution-free setting is not appropriate for analyzing natural lan-
guage. This setting poses technical difficulties with the convergence ofRn(Qn) and needs
to take into account pathological cases that can be ruled out in natural language data.
Instead, we will make assumptions about p, parametrize these assumptions in several
ways, and then calculate sample complexity bounds of the form N(, ?,Q, p), where the
dependence on the distribution is expressed as dependence on the parameters in the
assumptions about p.
The learning setting, then, can be described as follows. The user decides on a level
of accuracy () which the learning algorithm has to reach with confidence (1? ?). Then,
N(, ?,Q, p) samples are drawn from p and presented to the learning algorithm. The
learning algorithm then returns an hypothesis according to Equation (9).
3. Probabilistic Grammars
We begin this section by discussing the family of probabilistic grammars. A probabilistic
grammar defines a probability distribution over a certain kind of structured object (a
derivation of the underlying symbolic grammar) explained step-by-step as a stochastic
485
Computational Linguistics Volume 38, Number 3
process. Hidden Markov models (HMMs), for example, can be understood as a random
walk through a probabilistic finite-state network, with an output symbol sampled at
each state. PCFGs generate phrase-structure trees by recursively rewriting nonterminal
symbols as sequences of ?child? symbols (each itself either a nonterminal symbol or a
terminal symbol analogous to the emissions of an HMM).
Each step or emission of an HMM and each rewriting operation of a PCFG is
conditionally independent of the others given a single structural element (one HMM
or PCFG state); this Markov property permits efficient inference over derivations given
a string.
In general, a probabilistic grammar ?G,?? defines the joint probability of a string x
and a grammatical derivation z:
q(x, z | ?,G) =
K
?
k=1
Nk
?
i=1
?
?k,i(x,z)
k,i
= exp
K
?
k=1
Nk
?
i=1
?k,i(x, z) log?k,i (11)
where ?k,i is a function that ?counts? the number of times the kth distribution?s
ith event occurs in the derivation. The parameters ? are a collection of K multi-
nomials ??1, . . . ,?K?, the kth of which includes Nk competing events. If we let ?k =
??k,1, . . . ,?k,Nk?, each ?k,i is a probability, such that
?k,?i, ?k,i ? 0
?k,
Nk
?
i=1
?k,i = 1
We denote by ?G this parameter space for ?. The grammar G dictates the support
of q in Equation (11). As is often the case in probabilistic modeling, there are differ-
ent ways to carve up the random variables. We can think of x and z as correlated
structure variables (often x is known if z is known), or the derivation event counts
?(x, z) = ??k,i(x, z)?1?k?K,1?i?Nk as an integer-vector random variable. In this article,
we assume that x is always a deterministic function of z, so we use the distribution
p(z) interchangeably with p(x, z).
Note that there may be many derivations z for a given string x?perhaps even
infinitely many in some kinds of grammars. For HMMs, there are three kinds of multi-
nomials: a starting state multinomial, a transitionmultinomial per state and an emission
multinomial per state. In that case K = 2s+ 1, where s is the number of states. The value
of Nk depends on whether the kth multinomial is the starting state multinomial (in
which case Nk = s), transition multinomial (Nk = s), or emission multinomial (Nk = t,
with t being the number of symbols in the HMM). For PCFGs, each multinomial
among the K multinomials corresponds to a set of Nk context-free rules headed by
the same nonterminal. The parameter ?k,i is then the probability of the ith rule for the
kth nonterminal.
We assume that G denotes a fixed grammar, such as a context-free or regular gram-
mar. We let N =
?K
k=1Nk denote the total number of derivation event types. We use
D(G) to denote the set of all possible derivations of G. We define Dx(G) = {z ? D(G) |
yield(z) = x}. We use deg(G) to denote the ?degree? of G, i.e., deg(G) = maxk Nk. We
let |x| denote the length of the string x, and |z| =
?K
k=1
?Nk
i=1 ?k,i(z) denote the ?length?
(number of event tokens) of the derivation z.
486
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Going back to the notation in Section 2, Q would be a collection of probabilistic
grammars, parametrized by ?, and q would be a specific probabilistic grammar with
a specific ?. We therefore treat the problem of ERM with probabilistic grammars as the
problem of parameter estimation?identifying ? from complete data or incomplete data
(strings x are visible but the derivations z are not). We can also view parameter esti-
mation as the identification of a hypothesis from the concept space Q = H(G) = {h?(z) |
? ? ?G} (where h? is a distribution of the form of Equation [11]) or, equivalently, from
negated log-concept space F(G) = {? log h?(z) | ? ? ?G}. For simplicity of notation, we
assume that there is a fixed grammar G and use H to refer to H(G) and F to refer
to F(G).
3.1 Distributional Assumptions about Language
In this section, we describe a parametrization of assumptions we make about the dis-
tribution p(x, z), the distribution that generates derivations from D(G) (note that p does
not have to be a probabilistic grammar). We first describe empirical evidence about the
decay of the frequency of long strings x.
Figure 1 shows the frequency of sentence length for treebanks in various lan-
guages.4 The trend in the plots clearly shows that in the extended tail of the curve, all
languages have an exponential decay of probabilities as a function of sentence length. To
test this, we performed a simple regression of frequencies using an exponential curve.
We estimated each curve for each language using a curve of the form f (l; c,?) = cl?.
This estimation was done by minimizing squared error between the frequency ver-
sus sentence length curve and the approximate version of this curve. The data points
used for the approximation are (li, pi), where li denotes sentence length and pi denotes
frequency, selected from the extended tail of the distribution. Extended tail here refers
to all points with length longer than l1, where l1 is the length with the highest frequency
in the treebank. The goal of focusing on the tail is to avoid approximating the head
of the curve, which is actually a monotonically increasing function. We plotted the
approximate curve together with a length versus frequency curve for new syntactic
data. It can be seen (Figure 1) that the approximation is rather accurate in these corpora.
As a consequence of this observation, we make a few assumptions about G and
p(x, z):
 Derivation length proportional to sentence length: There is an ? ? 1 such
that, for all z, |z| ? ?|yield(z)|. Further, |z| ? |x|. (This prohibits unary
cycles.)
 Exponential decay of derivations: There is a constant r < 1 and a constant
L ? 0 such that p(z) ? Lr|z|. Note that the assumption here is about the
frequency of length of separate derivations, and not the aggregated
frequency of all sentences of a certain length (cf. the discussion above
referring to Figure 1).
4 Treebanks offer samples of cleanly segmented sentences. It is important to note that the distributions
estimated may not generalize well to samples from other domains in these languages. Our argument
is that the family of the estimated curve is reasonable, not that we can correctly estimate the curve?s
parameters.
487
Computational Linguistics Volume 38, Number 3
Figure 1
A plot of the tail of frequency vs. sentence length in treebanks for English, German, Bulgarian,
Turkish, Spanish, and Chinese. Red lines denote data from the treebank, blue lines denote an
approximation which uses an exponential function of the form f (l; c,?) = cl? (the blue line uses
data which is different from the data used to estimate the curve parameters, c and ?). The
parameters (c,?) are (0.19, 0.92) for English, (0.06, 0.94) for German, (0.26, 0.89) for Bulgarian,
(0.26, 0.83) for Turkish, (0.11, 0.93) for Spanish, and (0.03, 0.97) for Chinese. Squared errors are
0.0005, 0.0003, 0.0007, 0.0003, 0.001, and 0.002 for English, German, Bulgarian, Turkish, Spanish,
and Chinese, respectively.
 Exponential decay of strings: Let ?(k) = |{z ? D(G) | |z| = k}| be the
number derivations of length k in G. We assume that ?(k) is an increasing
function, and complete it such that it is defined over positive numbers by
taking ?(t)  ?(t). Taking r as before, we assume there exists a constant
q < 1, such that ?2(k)rk ? qk (and as a consequence, ?(k)rk ? qk). This
implies that the number of derivations of length kmay be exponentially
large (e.g., as with many PCFGs), but is bounded by (q/r)k.
488
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
 Bounded expectations of rules: There is a B < ? such that Ep
[
?k,i(z)
]
? B
for all k and i.
These assumptions must hold for any pwhose support consists of a finite set. These
assumptions also hold in many cases when p itself is a probabilistic grammar. Also, we
note that the last requirement of bounded expectations is optional, and it can be inferred
from the rest of the requirements: B = L/(1? q)2. We make this requirement explicit for
simplicity of notation later. We denote the family of distributions that satisfy all of these
requirements by P(?,L, r, q,B,G).
There are other cases in the literature of language learning where additional as-
sumptions are made on the learned family of models in order to obtain positive learn-
ability results. For example, Clark and Thollard (2004) put a bound on the expected
length of strings generated from any state of probabilistic finite state automata, which
resembles the exponential decay of strings we have for p in this article.
An immediate consequence of these assumptions is that the entropy of p is finite
and bounded by a quantity that depends on L, r and q.5 Bounding entropy of labels
(derivations) given inputs (sentences) is a common way to quantify the noise in a
distribution. Here, both the sentential entropy (Hs(p) = ?
?
x p(x) log p(x)) is bounded
as well as the derivational entropy (Hd(p) = ?
?
x,z p(x, z) log p(x, z)). This is stated in the
following result.
Proposition 1
Let p ? P(?,L, r, q,B,G) be a distribution. Then, we have
Hs(p) ? Hd(p) ? ? log L+
L log r
(1? q)2
log 1r +
(1+ log L)/ log 1r 
e ?
(?
1+ log L
log 1r
?)
Proof
First note that Hs(p) ? Hd(p) holds by the data processing inequality (Cover and
Thomas 1991) because the sentential probability distribution p(x) is a coarser version
of the derivational probability distribution p(x, z). Now, consider p(x, z). For simplicity
of notation, we use p(z) instead of p(x, z). The yield of z, x, is a function of z, and therefore
can be omitted from the distribution. It holds that
Hd(p) = ?
?
z
p(z) log p(z)
= ?
?
z?Z1
p(z) log p(z)?
?
z?Z2
p(z) log p(z)
= Hd(p,Z1)+Hd(p,Z2)
where Z1 = {z | p(z) > 1/e} and Z2 = {z | p(z) ? 1/e}. Note that the function ?? log?
reaches its maximum for ? = 1/e. We therefore have
Hd(p,Z1) ?
|Z1|
e
5 For simplicity and consistency with the log-loss, we measure entropy in nats, which means we use the
natural logarithm when computing entropy.
489
Computational Linguistics Volume 38, Number 3
We give a bound on |Z1|, the number of ?high probability? derivations. Because we have
p(x, z) ? Lr|z|, we can find the maximum length of a derivation that has a probability of
more than 1/e (and hence, it may appear in Z1) by solving 1/e ? Lr|z| for |z|, which leads
to |z| ? log(1/eL)/ log r. Therefore, there are at most
?(1+logL)/ log 1r 
k=1
?(k) derivations in
|Z1| and therefore we have
|Z1| ?
?
(1+ log L)/ log 1r
?
?
(?
(1+ logL)/ log 1r
?)
Hd(p,Z1) ?
?
(1+ log L)/ log 1r
?
e ?
(?
(1+ logL)/ log 1r
?)
(12)
where we use the monotonicity of ?. Consider Hd(p,Z2) (the ?low probability? deriva-
tions). We have:
Hd(p,Z2) ? ?
?
z?Z2
Lr|z| log
(
Lr|z|
)
? ? log L?
(
L log r
)
?
z?Z2
|z|r|z|
? ? log L?
(
L log r
)
?
?
k=1
?(k)krk
? ? log L?
(
L log r
)
?
?
k=1
kqk (13)
= ? log L+
L log r
(1? q)2
log 1q (14)
where Equation (13) holds from the assumptions about p. Putting Equation (12) and
Equation (14) together, we obtain the result. 
We note that another common way to quantify the noise in a distribution is through
the notion of Tsybakov noise (Tsybakov 2004; Koltchinskii 2006). We discuss this further
in Section 7.1, where we show that Tsybakov noise is too permissive, and probabilistic
grammars do not satisfy its conditions.
3.2 Limiting the Degree of the Grammar
When approximating a family of probabilistic grammars, it is much more convenient
when the degree of the grammar is limited. In this article, we limit the degree of the
grammar by making the assumption that all Nk ? 2. This assumption may seem, at first
glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence,
other formalisms), this assumption does not limit the total generative capacity that we
can have across all context-free grammars.
We first show that any context-free grammar with arbitrary degree can be mapped
to a corresponding grammar with all Nk ? 2 that generates derivations equivalent to
derivations in the original grammar. Such a grammar is also called a ?covering gram-
mar? (Nijholt 1980; Leermakers 1989). Let G be a CFG. Let A be the kth nonterminal.
Consider the rules A? ?i for i ? Nk where A appears on the left side. For each rule
490
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Figure 2
Example of a context-free grammar and its equivalent binarized form.
A? ?i, i < Nk, we create a new nonterminal in G
? such that Ai has two rewrite rules:
Ai ? ?i and Ai ? Ai+1. In addition, we create rules A? A1 and ANk ? ?Nk . Figure 2
demonstrates an example of this transformation on a small context-free grammar.
It is easy to verify that the resulting grammar G? has an equivalent capacity to
the original CFG, G. A simple transformation that converts each derivation in the
new grammar to a derivation in the old grammar would involve collapsing any path
of nonterminals added to G? (i.e., all Ai for nonterminal A) so that we end up with
nonterminals from the original grammar only. Similarly, any derivation in G can be
converted to a derivation in G? by adding new nonterminals through unary application
of rules of the form Ai ? Ai+1. Given a derivation z in G, we denote by ?G
?G? (z) the
corresponding derivation in G? after adding the new non-terminals Ai to z. Throughout
this article, we will refer to the normalized form of G? as a ?binary normal form.?6
Note that K?, the number of multinomials in the binary normal form, is a func-
tion of both the number of nonterminals in the original grammar and the number of
rules in that grammar. More specifically, we have that K? =
?K
k=1Nk + K. To make the
equivalence complete, we need to show that any probabilistic context-free grammar can
be translated to a PCFG with maxk Nk ? 2 such that the two PCFGs induce the same
equivalent distributions over derivations.
Utility Lemma 1
Let ai ? [0, 1], i ? {1, . . . ,N} such that
?
i ai = 1. Define b1 = a1, c1 = 1? a1, bi =
(
ai
ai?1
)
(
bi?1
ci?1
)
, and ci = 1? bi for i ? 2. Then ai =
?
?
i?1
?
j=1
cj
?
? bi.
See Appendix A for the proof of Utility Lemma 1.
Theorem 1
Let ?G,?? be a probabilistic context-free grammar. Let G? be the binarizing transforma-
tion of G as defined earlier. Then, there exists ?? for G? such that for any z ? D(G) we
have p(z | ?,G) = p(?G 
?G? (z) | ?
?,G?).
6 We note that this notion of binarization is different from previous types of binarization appearing in
computational linguistics for grammars. Typically in previous work about binarized grammars such as
CFGs, the grammars are constrained to have at most two nonterminals in the right side in Chomsky
normal form. Another form of binarization for linear context-free rewriting systems is restriction of the
fan-out of the rules to two (Go?mez-Rodr??guez and Satta 2009; Gildea 2010). We, however, limit the
number of rules for each nonterminal (or more generally, the number of elements in each multinomial).
491
Computational Linguistics Volume 38, Number 3
Proof
For the grammar G, index the set {1, ...,K} with nonterminals ranging from A1 to AK.
DefineG? as before.We need to define ??. Index themultinomials inG? by (k, i), each hav-
ing two events. Let ?(k,i),1 = ?k,i, ?(k,i),2 = 1? ?k,i for i = 1 and set ?k,i,1 = ?k,i/?(k,i?1),2,
and ?(k,i?1),2 = 1? ?(k,i?1),2.
?G?,?? is a weighted context-free grammar such that the ?(k,i),1 corresponds to the
ith event in the k multinomial of the original grammar. Let z be a derivation in G and
z? = ?G 
?G? (z). Then, from Utility Lemma 1 and the construction of g
?, we have that:
p(z | ?,G) =
K
?
k=1
Nk
?
i=1
?
?k,i(z)
k,i
=
K
?
k=1
Nk
?
i=1
?k,i(z)
?
l=1
?k,i
=
K
?
k=1
Nk
?
i=1
?k,i(z)
?
l=1
?
?
i?1
?
j=1
?(k,j),2
?
??(k,i),1
=
K
?
k=1
Nk
?
i=1
?
?
i?1
?
j=1
?
?k,i(z)
(k,j),2
?
??
?k,i(z)
(k,i),1
=
K
?
k=1
Nk
?
j=1
2
?
i=1
?
?k,j(z
? )
(k,j),i
= p(z? | ?,G?)
From Chi (1999), we know that the weighted grammar ?G?,?? can be converted to
a probabilistic context-free grammar ?G?,???, through a construction of ?? based on ?,
such that p(z? | ?,G?) = p(z? | ??,G?). 
The proof for Theorem 1 gives a construction the parameters ?? ofG? such that ?G,??
is equivalent to ?G?,???. The construction of ?? can also be reversed: Given ?? for G?, we
can construct ? for G so that again we have equivalence between ?G,?? and ?G?,???.
In this section, we focused on presenting parametrized, empirically justified distri-
butional assumptions about language data that will make the analysis in later sections
moremanageable.We showed that these assumptions bound the amount of entropy as a
function of the assumption parameters. We also made an assumption about the structure
of the grammar family, and showed that it entails no loss of generality for CFGs. Many
other formalisms can follow similar arguments to show that the structural assumption
is justified for them as well.
4. Proper Approximations
In order to follow the empirical risk minimization described in Section 2.1, we have
to define a series of approximations for F, which we denote by the log-concept spaces
F1,F2, . . . . We also have to replace two-sided uniform convergence (Equation [6]) with
convergence on the sequence of concept spaces we defined (Equation [10]). The concept
spaces in the sequence vary as a function of the number of samples we have. We next
492
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
construct the sequence of concept spaces, and in Section 5 we return to the learning
model. Our approximations are based on the concept of bounded approximations (Abe,
Takeuchi, and Warmuth 1991; Dasgupta 1997), which were originally designed for
graphical models.7 A bounded approximation is a subset of a concept space which is
controlled by a parameter that determines its tightness. Here we use this idea to define
a series of subsets of the original concept space F as approximations, while having two
asymptotic properties that control the series? tightness.
Let Fm (for m ? {1, 2, . . .}) be a sequence of concept spaces. We consider three
properties of elements of this sequence, which should hold for m > M for a fixedM.
The first is containment in F:
Fm ? F
The second property is boundedness:
?Km ? 0,?f ? Fm, E
[
| f | ? I {| f | ? Km}
]
? bound(m)
where bound is a non-increasing function such that bound(m) ??
m??
0. This states that
the expected values of functions from Fm on values larger than some Km is small.
This is required to obtain uniform convergence results in the revised empirical risk
minimization model from Section 2.1. Note that Km can grow arbitrarily large.
The third property is tightness:
?Cm ? F ? Fm, p
?
?
?
f?F
{z | Cm( f )(z)? f (z) ? tail(m)}
?
? ? tail(m)
where tail is a non-increasing function such that tail(m) ??
m??
0, and Cm denotes an
operator that maps functions in F to Fm. This ensures that our approximation actually
converges to the original concept space F. We will show in Section 4.3 that this is
actually a well-motivated characterization of convergence for probabilistic grammars
in the supervised setting.
We say that the sequence Fm properly approximates F if there exist tail(m), bound(m),
and Cm such that, for all m larger than some M, containment, boundedness, and tight-
ness all hold.
In a good approximation, Km would increase at a fast rate as a function of m and
tail(m) and bound(m) decrease quickly as a function ofm. As wewill see in Section 5, we
cannot have an arbitrarily fast convergence rate (by, for example, taking a subsequence
of Fm), because the size of Km has a great effect on the number of samples required to
obtain accurate estimation.
7 There are other ways to manage the unboundedness of KL divergence in the language learning literature.
Clark and Thollard (2004), for example, decompose the KL divergence between probabilistic finite-state
automata into several terms according to a decomposition of Carrasco (1997) and then bound each term
separately.
493
Computational Linguistics Volume 38, Number 3
Table 1
Example of a PCFG where there is more than a single way to approximate it by truncation with
? = 0.1, because it has more than two rules. Any value of ? ? [0,?] will lead to a different
approximation.
Rule ? General ? = 0 ? = 0.01 ? = 0.005
S? NP VP 0.09 0.01 0.1 0.1 0.1
S? NP 0.11 0.11? ? 0.11 0.1 0.105
S? VP 0.8 0.8? ?+ ? 0.79 0.8 0.795
4.1 Constructing Proper Approximations for Probabilistic Grammars
Wenow focus on constructing proper approximations for probabilistic grammarswhose
degree is limited to 2. Proper approximations could, in principle, be used with losses
other than the log-loss, though their main use is for unbounded losses. Starting from
this point in the article, we focus on using such proper approximations with the
log-loss.
We construct Fm. For each f ? F we define a transformation T( f,?) that shifts every
binomial parameter ?k = ??k,1,?k,2? in the probabilistic grammar by at most ?:
??k,1,?k,2? ?
?
?
?
??, 1? ?? if ?k,1 < ?
?1? ?, ?? if ?k,1 > 1? ?
??k,1, ?k,2? otherwise
Note that T( f,?) ? F for any ? ? 1/2. Fix a constant s > 1.8 We denote by T(?,?) the
same transformation on ? (which outputs the new shifted parameters) and we denote
by ?G(?) = ?(?) the set {T(?,?) | ? ? ?G}. For each m ? N, define Fm = {T( f,m?s) |
f ? F}.
When considering our approach to approximate a probabilistic grammar by in-
creasing its parameter probabilities to be over a certain threshold, it becomes clear
why we are required to limit the grammar to have only two rules and why we are
required to use the normal from Section 3.2 with grammars of degree 2. Consider the
PCFG rules in Table 1. There are different ways to move probability mass to the rule
with small probability. This leads to a problem with identifability of the approximation:
How does one decide how to reallocate probability to the small probability rules? By
binarizing the grammar in advance, we arrive at a single way to reallocate mass when
required (i.e., move mass from the high-probability rule to the low-probability rule).
This leads to a simpler proof for sample complexity bounds and a single bound (rather
than different bounds depending on different smoothing operators). We note, however,
that the choices made in binarizing the grammar imply a particular way of smoothing
the probability across the original rules.
We now describe how this construction of approximations satisfies the proper-
ties mentioned in Section 4, specifically, the boundedness property and the tightness
property.
8 By varying swe get a family of approximations. The larger s is, the tighter the approximation is. Also,
the larger s is, as we see later, the looser our sample complexity bound will be.
494
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Proposition 2
Let p ? P(?,L, r, q,B,G) and let Fm be as defined earlier. There exists a constant ? =
?(L, q, p,N) > 0 such that Fm has the boundedness property with Km = sN log
3m and
bound(m) = m
?? logm.
See Appendix A for the proof of Proposition 2.
Next, Fm is tight with respect to F with tail(m) =
N log2m
ms ? 1 .
Proposition 3
Let p ? P(?,L, r, q,B,G) and let Fm as defined earlier. There exists anM such that for any
m > Mwe have
p
?
?
?
f?F
{z | Cm( f )(z)? f (z) ? tail(m)}
?
? ? tail(m)
for tail(m) =
N log2m
ms ? 1 and Cm( f ) = T( f,m
?s).
See Appendix A for the proof of Proposition 3.
We now have proper approximations for probabilistic grammars. These approx-
imations are defined as a series of probabilistic grammars, related to the family of
probabilistic grammars we are interested in estimating. They consist of three prop-
erties: containment (they are a subset of the family of probabilistic grammars we
are interested in estimating), boundedness (their log-loss does not diverge to infinity
quickly), and they are tight (there is a small probability mass at which they are not tight
approximations).
4.2 Coupling Bounded Approximations with Number of Samples
At this point, the number of samples n is decoupled from the bounded approximation
(Fm) that we choose for grammar estimation. To couple between these two, we need
to define m as a function of the number of samples, m(n). As mentioned earlier, there
is a clear trade-off between choosing a fast rate for m(n) (such as m(n) = nk for some
k > 1) and a slower rate (such as m(n) = logn). The faster the rate is, the tighter the
family of approximations that we use for n samples. If the rate is too fast, however,
then Km grows quickly as well. In that case, because our sample complexity bounds are
increasing functions of such Km, the bounds will degrade.
To balance the trade-off, we choose m(n) = n. As we see later, this gives sample
complexity bounds which are asymptotically interesting for both the supervised and
unsupervised case.
495
Computational Linguistics Volume 38, Number 3
4.3 Asymptotic Empirical Risk Minimization
It would be compelling to determine whether the empirical risk minimizer over Fn is
an asymptotic empirical risk minimizer. This would mean that the risk of the empirical
risk minimizer over Fn converges to the risk of the maximum likelihood estimate. As a
conclusion to this section about proper approximations, we motivate the three re-
quirements that we posed on proper approximations by showing that this is indeed
true. We now unify n, the number of samples, and m, the index of the approxima-
tion of the concept space F. Let f ?n be the minimizer of the empirical risk over F,
( f ?n = argminf?F Ep?n
[
f
]
) and let gn be the minimizer of the empirical risk over Fn
(gn = argminf?Fn Ep?n
[
f
]
).
Let D = {z1, ..., zn} be a sample from p(z). The operator (gn =) argminf?Fn Ep?n [ f ] is
an asymptotic empirical risk minimizer if E
[
Ep?n
[
gn
]
? Ep?n [ f
?
n ]
]
? 0 as n? ? (Shalev-
Shwartz et al 2009). Then, we have the following
Lemma 1
Denote by Z,n the set
?
f?F{z | Cn( f )(z)? f (z) ? }. Denote by A,n the event ?one of
zi ? D is in Z,n.? If Fn properly approximates F, then:
E
[
Ep?n
[
gn
]
? Ep?n
[
f ?n
]
]
(15)
?
?
?
?
E
[
Ep?n
[
Cn( f
?
n )
]
| A,n
]
?
?
?
p(A,n)+
?
?
?
E
[
Ep?n
[
f ?n
]
| A,n
]
?
?
?
p(A,n)+ tail(n)
where the expectations are taken with respect to the data set D.
See Appendix A for the proof of Lemma 1.
Proposition 4
Let D = {z1, ..., zn} be a sample of derivations from G. Then gn = argminf?Fn Ep?n
[
f
]
is
an asymptotic empirical risk minimizer.
Proof
Let f0 ? F be the concept that puts uniform weights over ?, namely, ?k = ? 12 ,
1
2 ? for all k.
Note that
|E
[
Ep?n
[
f ?n
]
| A,n
]
|p(A,n)
? |E
[
Ep?n
[
f0
]
| A,n
]
|p(A,n) =
log 2
n
?n
l=1
?
k,i E[?k,i(zl) | A,n]p(A,n)
496
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Let Aj,,n for j ? {1, . . . ,n} be the event ?zj ? Z,n?. Then A,n =
?
j Aj,,n. We have
that
E[?k,i(zl) | A,n]p(A,n) ?
?
j
?
zl
p(zl,Aj,,n)|zl|
?
?
j =l
?
zl
p(zl)p(Aj,,n)|zl|+
?
zl
p(zl,Al,,n)|zl| (16)
?
?
?
?
j =l
p(Aj,,n)
?
?B+ E[?k,i(z) | z ? Z,n]p(z ? Z,n)
? (n? 1)Bp(z ? Z,n)+ E[?k,i(z) | z ? Z,n]p(z ? Z,n)
where Equation (16) comes from zl being independent. Also, B is the constant from
Section 3.1. Therefore, we have:
1
n
n
?
l=1
?
k,i
E[?k,i(zl) | A,n]p(A,n)
?
?
k,i
(
E[?k,i(z) | z ? Z,n]p(z ? Z,n)+ (n? 1)Bp(z ? Z,n)
)
From the construction of our proper approximations (Proposition 3), we know that only
derivations of length log2 n or greater can be in Z,n. Therefore
E[?k,i | Z,n]p(Z,n) ?
?
z:|z|>log2 n
p(z)?k,i(z) ?
?
?
l>log2 n
L?(l)rll ? ?qlog
2 n = o(1)
where ? > 0 is a constant. Similarly, we have p(z ? Z,n) = o(n?1). This means that
|E[Ep?n[? log?f
?
n ] | A,n]|p(A,n) ??
n??
0. In addition, it can be shown that |E[Ep?n[Cn( f
?
n ) |
A,n]|p(A,n) ??
n??
0 using the same proof technique we used here, while relying on the
fact that Cn( f
?
n ) ? Fn, and therefore Cn( f
?
n )(z) ? sN|z| log n. 
5. Sample Complexity Bounds
Equipped with the framework of proper approximations as described previously, we
now give our main sample complexity results for probabilistic grammars. These results
hinge on the convergence of supf?Fn |Ep?n
[
f
]
? Ep
[
f
]
|. Indeed, proper approximations
replace the use of F in these convergence results. The rate of this convergence can be
fast, if the covering numbers for Fn do not grow too fast.
5.1 Covering Numbers and Bounds on Covering Numbers
We next give a brief overview of covering numbers. A cover provides a way to reduce
a class of functions to a much smaller (finite, in fact) representative class such that each
function in the original class is represented using a function in the smaller class. Let G
497
Computational Linguistics Volume 38, Number 3
be a class of functions. Let d(f, g) be a distance measure between two functions f, g from
G. An -cover is a subset of G, denoted by G?, such that for every f ? G there exists an
f ? ? G? such that d( f, f ?) < . The covering number N(,G, d) is the size of the smallest
-cover of G for the distance measure d.
We are interested in a specific distancemeasure which is dependent on the empirical
distribution p?n that describes the data z1, ..., zn. Let f, g ? G. We will use
dp?n ( f, g) = Ep?n
[
| f ? g|
]
=
?
z?D(G)
| f (z)? g(z)| p?n(z)
= 1n
?n
i=1
| f (zi)? g(zi)|
Instead of using N(,G, dp?n ) directly, we bound this quantity with N(,G) = supp?n
N(,G, dp?n ), where we consider all possible samples (yielding p?n). The following is the
key result regarding the connection between covering numbers and the double-sided
convergence of the empirical process supf?Fn |Ep?n
[
f
]
? Ep
[
f
]
| as n? ?. This result
is a general-purpose result that has been used frequently to prove the convergence of
empirical processes of the type we discuss in this article.
Lemma 2
Let Fn be a permissible class
9 of functions such that for every f ? Fn we have E[| f | ?
I {| f | ? Kn}] ? bound(n). Let Ftruncated,n = {f ? I {f ? Kn} | f ? Fm}, namely, the set of
functions from Fn after being truncated by Kn. Then for  > 0 we have
p
(
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| > 2
)
? 8N(/8,Ftruncated,n) exp
(
? 1
128
n2/K2n
)
+ bound(n)/
provided n ? K2n/4
2 and bound(n) < .
See Pollard (1984; Chapter 2, pages 30?31) for the proof of Lemma 2. See also Ap-
pendix A.
Covering numbers are rather complex combinatorial quantities which are hard
to compute directly. Fortunately, they can be bounded using the pseudo-dimension
(Anthony and Bartlett 1999), a generalization of the Vapnik-Chervonenkis (VC)
dimension for real functions. In the case of our ?binomialized? probabilistic grammars,
the pseudo-dimension of Fn is bounded by N, because we have Fn ? F, and the
functions in F are linear with N parameters. Hence, Ftruncated,n also has pseudo-
dimension that is at most N. We then have the following.
9 The ?permissible class? requirement is a mild regularity condition regarding measurability that holds for
proper approximations. We refer the reader to Pollard (1984) for more details.
498
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Lemma 3
(From Pollard [1984] and Haussler [1992].) Let Fn be the proper approximations for
probabilistic grammars, for any 0 <  < Kn we have:
N(,Ftruncated,n) < 2
(
2eKn
 log
2eKn

)N
5.2 Supervised Case
We turn to give an analysis for the supervised case. This analysis is mostly described as a
preparation for the unsupervised case. In general, the families of probabilistic grammars
we give a treatment to are parametric families, and the maximum likelihood estimator
for these families is a consistent estimator in the supervised case. In the unsupervised
case, however, lack of identifiability prevents us from getting these traditional consis-
tency results. Also, the traditional results about the consistency of MLE are based on the
assumption that the sample is generated from the parametric family we are trying to
estimate. This is not the case in our analysis, where the distribution that generates the
data does not have to be a probabilistic grammar.
Lemmas 2 and 3 can be combined to get the following sample complexity result.
Theorem 2
LetG be a grammar. Let p ? P(?,L, r, q,B,G) (Section 3.1). Let Fn be a proper approxima-
tion for the corresponding family of probabilistic grammars. Let z1, . . . , zn be a sample
of derivations. Then there exists a constant ?(L, q, p,N) and constantM such that for any
0 < ? < 1 and 0 <  < Kn and any n > M and if
n ? max
{
128K2n
2
(
2N log(16eKn/)+ log
32
?
)
,
log 4/?+ log 1/
?(L, q, p,N)
}
then we have
P
(
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| ? 2
)
? 1? ?
where Kn = sN log
3 n.
Proof Sketch
?(L, q, p,N) is the constant from Proposition 2. The main idea in the proof is to solve for
n in the following two inequalities (based on Equation [17] [see the following]) while
relying on Lemma 3:
8N(/8,Ftruncated,n) exp
(
? 1
128
n2/K2n
)
? ?/2
bound(n)/ ? ?/2

499
Computational Linguistics Volume 38, Number 3
Theorem 2 gives little intuition about the number of samples required for accurate
estimation of a grammar because it considers the ?additive? setting: The empirical risk
is within  from the expected risk. More specifically, it is not clear how we should pick
 for the log-loss, because the log-loss can obtain arbitrary values.
We turn now to converting the additive bound in Theorem 2 to a multiplicative
bound. Multiplicative bounds can be more informative than additive bounds when the
range of the values that the log-loss can obtain is not known a priori. It is important
to note that the two views are equivalent (i.e., it is possible to convert a multiplicative
bound to an additive bound and vice versa). Let ? ? (0, 1) and choose  = ?Kn. Then,
substituting this  in Theorem 2, we get that if
n ? max
{
128
?2
(
2N log 16e? + log
32
?
)
,
log 4/?+ log 1/?
?(L, q, p,N)
}
then, with probability 1? ?,
sup
f?Fn
?
?
?
?
?
1?
Ep?n
[
f
]
Ep
[
f
]
?
?
?
?
?
?
?? 2sN log3(n)
H(p)
(17)
where H(p) is the Shannon entropy of p. This stems from the fact that Ep
[
f
]
? H(p) for
any f . This means that if we are interested in computing a sample complexity bound
such that the ratio between the empirical risk and the expected risk (for log-loss) is
close to 1 with high probability, we need to pick up ? such that the righthand side of
Equation (17) is smaller than the desired accuracy level (between 0 and 1). Note that
Equation (17) is an oracle inequality?it requires knowing the entropy of p or some
upper bound on it.
5.3 Unsupervised Case
In the unsupervised setting, we have n yields of derivations from the grammar, x1, ..., xn,
and our goal again is to identify grammar parameters ? from these yields. Our concept
classes are now the sets of log marginalized distributions from Fn. For each f? ? Fn, we
define f ?? as
f ??(x) = ? log
?
z?Dx(G)
exp(?f?(z)) = ? log
?
z?Dx(G)
exp
?
?
K
?
k=1
Nk
?
i=1
?i,k(z)?i,k
?
?
We denote the set of { f ??} by F
?
n. Analogously, we define F
?. Note that we also need to
define the operator C?n( f
?) as a first step towards defining F?n as proper approximations
(for F?) in the unsupervised setting. Let f ? ? F?. Let f be the concept in F such that
f ?(x) =
?
z f (x, z). Then we define C
?
n( f
?)(x) =
?
z Cn( f )(x, z).
It does not immediately follow that F?n is a proper approximation for F
?. It is not
hard to show that the boundedness property is satisfied with the same Kn and the same
form of bound(n) as in Proposition 2 (we would have 
?
bound(m) = m
??? logm for some
??(L, q, p,N) = ?? > 0). This relies on the property of bounded derivation length of p (see
Appendix A, Proposition 7). The following result shows that we have tightness as well.
500
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Utility Lemma 2
For ai, bi ? 0, if ? log
?
i ai + log
?
i bi ?  then there exists an i such that ? log ai +
log bi ? .
Proposition 5
There exists anM such that for any n > Mwe have
p
?
?
?
f ??F?
{x | C?n(f
?)(x)? f ?(x) ? tail(n)}
?
? ? tail(n)
for tail(n) =
N log2 n
ns ? 1 and the operator C
?
n( f ) as defined earlier.
Proof Sketch
From Utility Lemma 2 we have
p
?
?
?
f ??F?
{x | C?n( f
?)(x)? f ?(x) ? tail(n)}
?
? ? p
?
?
?
f?F
{x | ?zCn( f )(z)? f (z) ? tail(n)}
?
?
Define X(n) to be all x such that there exists a z with yield(z) = x and |z| ? log2 n.
From the proof of Proposition 3 and the requirements on p, we know that there exists
an ? ? 1 such that
p
(
?
f?F{x | ?z s.t.Cn( f )(z)? f (z) ? tail(n)}
)
?
?
x?X(n)
p(x)
?
?
x:|x|?log2 n/?
p(x) ?
?
?
k=log2 n/?
L?(k)rk ? tail(n)
where the last inequality happens for some n larger than a fixedM. 
Computing either the covering number or the pseudo-dimension of F?n is a hard
task, because the function in the classes includes the ?log-sum-exp.? Dasgupta (1997)
overcomes this problem for Bayesian networks with fixed structure by giving a bound
on the covering number for (his respective) F? which depends on the covering number
of F.
Unfortunately, we cannot fully adopt this approach, because the derivations of
a probabilistic grammar can be arbitrarily large. Instead, we present the following
proposition, which is based on the ?Hidden Variable Rule? from Dasgupta (1997). This
proposition shows that the covering number of F? (or more accurately, its bounded
approximations) can be bounded in terms of the covering number of the bounded
501
Computational Linguistics Volume 38, Number 3
approximations of F, and the constants which control the underlying distribution p
mentioned in Section 3.
Utility Lemma 3
For any two positive-valued sequences (a1, . . . , an) and (b1, . . . , bn) we have that
?
i | log ai/bi| ? | log (
?
ai/
?
bi) |.
Proposition 6 (Hidden Variable Rule for Probabilistic Grammars)
Let m =
log
4Kn
(1? q)
log 1q
. Then, N(,F?truncated,n) ? N
(

2?(m)
,Ftruncated,n
)
.
Proof
Let Z(m) = {z | |z| ? m} be the subset of derivations of length shorter than m. Consider
f, f0 ? Ftruncated,n. Let f ? and f ?0 be the corresponding functions in F
?
truncated,n. Then, for any
distribution p,
dp( f ?, f ?0) =
?
x
| f ?(x)? f ?0(x)| p(x) ?
?
x
?
z
| f (x, z)? f0(x, z)| p(x)
=
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+
?
x
?
z/?Z(m)
| f (x, z)? f0(x, z)| p(x)
?
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+
?
x
?
z/?Z(m)
2Knp(x) (18)
?
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+ 2Kn
?
x : |x|?m
|Dx(G)|p(x)
?
?
x
?
z?Z(m)
| f (x, z)? f0(x, z)| p(x)+ 2Kn
?
?
k=m
?2(k)rk
? dp
?
( f, f0)|Z(m)|+ 2Kn
qm
1? q
where p?(x, z) is a probability distribution that uniformly divides the probability mass
p(x) across all derivations for the specific x, that is:
p?(x, z) =
p(x)
|Dx(G)|
The inequality in Equation (18) stems from Utility Lemma 3.
Set m to be the quantity that appears in the proposition to get the necessary result
( f ? and f are arbitrary functions in F?truncated,n and Ftruncated,n respectively. Then consider
f ?0 and f0 to be functions from the respective covers.). 
For the unsupervised case, then, we get the following sample complexity result.
502
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Theorem 3
Let G be a grammar. Let F?n be a proper approximation for the corresponding family of
probabilistic grammars. Let p(x, z) be a distribution over derivations which satisfies the
requirements in Section 3.1. Let x1, . . . , xn be a sample of strings from p(x). Then there
exists a constant ??(L, q, p,N) and constant M such that for any 0 < ? < 1, 0 <  < Kn,
any n > M, and if
n ? max
{
128K2n
2
(
2N log
(
32eKn?(m)

)
+ log 32
?
)
,
log 4/?+ log 1/
??(L, q, p,N)
}
(19)
where m =
log
4Kn
(1? q)
log 1q
, we have that
p
(
sup
f?F?n
|Ep?n
[
f
]
? Ep
[
f
]
| ? 2
)
? 1? ?
where Kn = sN log
3 n.
Theorem 3 states that the number of samples we require in order to accurately esti-
mate a probabilistic grammar from unparsed strings depends on the level of ambiguity
in the grammar, represented as ?(m). We note that this dependence is polynomial, and
we consider this a positive result for unsupervised learning of grammars. More specif-
ically, if ? is an exponential function (such as the case with PCFGs), when compared to
the supervised learning, there is an extra multiplicative factor in the sample complexity
in the unsupervised setting that behaves like O(log log
Kn
 ).
We note that the following Equation (20) can again be reduced to a multiplicative
case, similarly to the way we described it for the supervised case. Setting  = ?Kn (? ?
(0, 1)), we get the following requirement on n:
n ? max
{
128
?2
(
2N log
(
32e? t(?)
?
)
+ log 32
?
)
,
log 4/?+ log 1/
??(L, q, p,N)
}
(20)
where t(?) =
log 4
?(1? q)
log 1q
.
6. Algorithms for Empirical Risk Minimization
We turn now to describing algorithms and their properties for minimizing empirical
risk using the framework described in Section 4.
6.1 Supervised Case
ERM with proper approximations leads to simple algorithms for estimating the proba-
bilities of a probabilistic grammar in the supervised setting. Given an  > 0 and a ? > 0,
we draw n examples according to Theorem 2. We then set ? = n?s. To minimize the
log-loss with respect to these n examples, we use the proper approximation Fn.
503
Computational Linguistics Volume 38, Number 3
Note that the value of the empirical log-loss for a probabilistic grammar param-
etrized by ? is
Ep?n
[
? log h(x, z | ?)
]
= ?
?
x,z
p?n(x, z) log h(x, z | ?)
= ?
?
x,z
p?n(x, z)
K
?
k=1
Nk
?
i=1
?k,i(x, z) log(?k,i)
= ?
K
?
k=1
Nk
?
i=1
log(?k,i)Ep?n
[
?k,i
]
Because we make the assumption that deg(G) ? 2 (Section 3.2), we have
Ep?n
[
? log h(x, z | ?)
]
= ?
K
?
k=1
(
log(?k,1)Ep?n
[
?k,1
]
+ log(1? ?k,1)Ep?n
[
?k,2
])
(21)
To minimize the log-loss with respect to Fn, we need to minimize Equation (21) under
the constraint that ? ? ?k,i ? 1? ? and ?k1 + ?k,2 = 1. It can be shown that the solution
for this optimization problem is
?k,i = min
?
?
?
1? ?,max
?
?
?
?,
?
?
n
?
j=1
??j,k,i
?
?
/
?
?
n
?
j=1
2
?
i?=1
??j,k,i?
?
?
?
?
?
?
?
?
(22)
where ??j,k,i is the number of times that ?k,i fires in Example j. (We include a full
derivation of this result in Appendix B.) The interpretation of Equation (22) is simple:
We count the number of times a rule appears in the samples and then normalize this
value by the total number of times rules associated with the same multinomial appear
in the samples. This frequency count is the maximum likelihood solution with respect
to the full hypothesis class H (Corazza and Satta 2006; see Appendix B). Because we
constrain ourselves to obtain a value away from 0 or 1 by a margin of ?, we need to
truncate this solution, as done in Equation (22).
This truncation to amargin ? can be thought of as a smoothing factor that enables us
to compute sample complexity bounds. We explore this connection to smoothing with
a Dirichlet prior in a Maximum a posteriori (MAP) Bayesian setting in Section 7.2.
6.2 Unsupervised Case
Similarly to the supervised case, minimizing the empirical log-loss in the unsupervised
setting requires minimizing (with respect to ?) the following:
Ep?n
[
? log h(x | ?)
]
= ?
?
x
p?n(x) log
?
z
h(x, z | ?) (23)
with the constraint that ? ? ?k,i ? 1? ? (i.e., ? ? ?(?)) where ? = n?s. This is done
after drawing n examples according to Theorem 3.
504
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
6.2.1 Hardness of ERM with Proper Approximations. It turns out that minimizing Equa-
tion (23) under the specified constraints is actually an NP-hard problem when G is a
PCFG. This result follows using a similar proof to the one in Cohen and Smith (2010c)
for the hardness of Viterbi training and maximizing log-likelihood for PCFGs. We turn
to giving the full derivation of this hardness result for PCFGs and the modification
required for adapting the results from Cohen and Smith to the case of having an
arbitrary ?margin constraint.
In order to show an NP-hardness result, we need to ?convert? the problem of the
maximization of Equation (23) to a decision problem. We do so by stating the following
decision problem.
Problem 1 (Unsupervised Minimization of the Log-Loss with Margin)
Input: A binarized context-free grammar G, a set of sentences x1, . . . , xn, a value ? ?
[0, 12 ), and a value ? ? [0, 1].
Output: 1 if there exists ? ? ?(?) (and hence, h ? H(G)) such that
?
?
x
p?n(x) log
?
z
h(x, z | ?) ? ? log(?) (24)
and 0 otherwise.
We will show the hardness result both when ? is not restricted at all as well as when
we allow ? > 0. The proof of the hardness result is achieved by reducing the problem
3-SAT (Sipser 2006), known to be NP-complete, to Problem 1. The problem 3-SAT is
defined as follows:
Problem 2 (3-SAT)
Input: A formula ? =
?m
i=1 (ai ? bi ? ci) in conjunctive normal form, such that each
clause has three literals.
Output: 1 if there is a satisfying assignment for ?, and 0 otherwise.
Given an instance of the 3-SAT problem, the reduction will, in polynomial time,
create a grammar and a single string such that solving Problem 1 for this grammar and
string will yield a solution for the instance of the 3-SAT problem.
Let ? =
?m
i=1 (ai ? bi ? ci) be an instance of the 3-SAT problem, where ai, bi, and
ci are literals over the set of variables {Y1, . . . ,YN} (a literal refers to a variable Yj or
its negation, Y?j). Let Cj be the jth clause in ?, such that Cj = aj ? bj ? cj. We define the
following CFG G? and string to parse s?:
1. The terminals of G? are the binary digits ? = {0, 1}.
2. We create N nonterminals VYr , r ? {1, . . . ,N} and rules VYr ? 0 and
VYr ? 1.
3. We create N nonterminals VY?r , r ? {1, . . . ,N} and rules VY?r ? 0 and
VY?r ? 1.
4. We create UYr,1 ? VYrVY?r and UYr,0 ? VY?rVYr .
5. We create the rule S1 ? A1. For each j ? {2, . . . ,m}, we create a rule
Sj ? Sj?1Aj where Sj is a new nonterminal indexed by ?j 
?j
i=1 Ci
and Aj is also a new nonterminal indexed by j ? {1, . . . ,m}.
505
Computational Linguistics Volume 38, Number 3
6. Let Cj = aj ? bj ? cj be clause j in ?. Let Y(aj) be the variable that aj
mentions. Let (y1, y2, y3) be a satisfying assignment for Cj where yk ? {0, 1}
and is the value of Y(aj), Y(bj), and Y(cj), respectively, for k ? {1, 2, 3}. For
each such clause-satisfying assignment, we add the rule
Aj ? UY(aj ),y1UY(bj ),y2UY(cj ),y3
For each Aj, we would have at most seven rules of this form, because one
rule will be logically inconsistent with aj ? bj ? cj.
7. The grammar?s start symbol is Sn.
8. The string to parse is s? = (10)
3m, that is, 3m consecutive occurrences of
the string 10.
A parse of the string s? using G? will be used to get an assignment by setting
Yr = 0 if the rule VYr ? 0 or VY?r ? 1 is used in the derivation of the parse tree, and 1
otherwise. Notice that at this point we do not exclude ?contradictions? that come from
the parse tree, such as VY3 ? 0 used in the tree together with VY3 ? 1 or VY?3 ? 0. To
maintain the restriction on the degree of grammars, we convertG? to the binary normal
form described in Section 3.2. The following lemma gives a condition under which the
assignment is consistent (so that contradictions do not occur in the parse tree).
Lemma 4
Let ? be an instance of the 3-SAT problem, and let G? be a probabilistic CFG based on
the given grammar with weights ??. If the (multiplicative) weight of the Viterbi parse
(i.e., the highest scoring parse according to the PCFG) of s? is 1, then the assignment
extracted from the parse tree is consistent.
Proof
Because the probability of the Viterbi parse is 1, all rules of the form {VYr ,VY?r} ? {0, 1}
which appear in the parse tree have probability 1 as well. There are two possible types
of inconsistencies. We show that neither exists in the Viterbi parse:
1. For any r, an appearance of both rules of the form VYr ? 0 and VYr ? 1
cannot occur because all rules that appear in the Viterbi parse tree have
probability 1.
2. For any r, an appearance of rules of the form VYr ? 1 and VY?r ? 1 cannot
occur, because whenever we have an appearance of the rule VYr ? 0, we
have an adjacent appearance of the rule VY?r ? 1 (because we parse
substrings of the form 10), and then we again use the fact that all rules in
the parse tree have probability 1. The case of VYr ? 0 and VY?r ? 0 is
handled analogously.
Thus, both possible inconsistencies are ruled out, resulting in a consistent assignment.

Figure 3 gives an example of an application of the reduction.
506
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Figure 3
An example of a Viterbi parse tree which represents a satisfying assignment for
? = (Y1 ? Y2 ? Y?4) ? (Y?1 ? Y?2 ? Y3). In ??, all rules appearing in the parse tree have
probability 1. The extracted assignment would be Y1 = 0,Y2 = 1,Y3 = 1,Y4 = 0.
Note that there is no usage of two different rules for a single nonterminal.
Lemma 5
Define ? and G? as before. There exists ?? such that the Viterbi parse of s? is 1 if and
only if ? is satisfiable. Moreover, the satisfying assignment is the one extracted from the
parse tree with weight 1 of s? under ??.
Proof
(=?) Assume that there is a satisfying assignment. Each clause Cj = aj ? bj ? cj is sat-
isfied using a tuple (y1, y2, y3), which assigns values for Y(aj), Y(bj), and Y(cj). This
assignment corresponds to the following rule:
Aj ? UY(aj ),y1UY(bj ),y2UY(cj ),y3
Set its probability to 1, and set al other rules of Aj to 0. In addition, for each r, if
Yr = y, set the probabilities of the rules VYr ? y and VY?r ? 1? y to 1 and VY?r ? y and
VYr ? 1? y to 0. The rest of the weights for Sj ? Sj?1Aj are set to 1. This assignment of
rule probabilities results in a Viterbi parse of weight 1.
(?=) Assume that the Viterbi parse has probability 1. From Lemma 4, we know that we
can extract a consistent assignment from the Viterbi parse. In addition, for each clause
Cj we have a rule
Aj ? UY(aj ),y1UY(bj ),y2UY(cj ),y3
that is assigned probability 1, for some (y1, y2, y3). One can verify that (y1, y2, y3) are
the values of the assignment for the corresponding variables in clause Cj, and that
they satisfy this clause. This means that each clause is satisfied by the assignment we
extracted. 
We are now ready to prove the following result.
Theorem 4
Problem 1 is NP-hard when either requiring ? > 0 or when fixing ? = 0.
507
Computational Linguistics Volume 38, Number 3
Proof
We first describe the reduction for the case of ? = 0. In Problem 1, set ? = 0, ? = 1,
G = G?, ? = 0, and x1 = s?. If ? is satisfiable, then the left side of Equation (24) can get
value 0, by setting the rule probabilities according to Lemma 5, hence we would return
1 as the result of running Problem 1.
If ? is unsatisfiable, then we would still get value 0 only if L(G) = {s?}. If G? gen-
erates a single derivation for (10)3m, then we actually do have a satisfying assignment
from Lemma 4. Otherwise (more than a single derivation), the optimal ? would have
to give fractional probabilities to rules of the form VYr ? {0, 1} (or VY?r ? {0, 1}). In
that case, it is no longer true that (10)3m is the only generated sentence, and this is a
contradiction to getting value 0 for Problem 1.
We next show that Problem 1 is NP-hard even if we require ? > 0. Let ? < 120m .
Set ? = ?, and the rest of the inputs to Problem 1 the same as before. Assume that ?
is satisfiable. Let ? be the rule probabilities from Equation (5) after being shifted with a
margin of ?. Then, because there is a derivation that uses only rules that have probability
1? ?, we have
h(x1 | T(?,?),G?) =
?
z
p(x1, z | T(?,?),G?)
? (1? ?)10m
> ?
because the size of the parse tree for (10)3m is at most 10m (using the binarized G?)
and assuming ? = ? < (1? ?)10m. This inequality indeed holds whenever ? < 120m .
Therefore, we have ? log h(x1 | ?) > ? log?. Problem 1 would return 0 in this case.
Now, assume that ? is not satisfiable. That means that any parse tree for the string
(10)3m would have to contain two different rules headed by the same non-terminal. This
means that
h(x1 | T(?,?),G?) =
?
z
p(x1, z | T(?,?),G?)
? ?
and therefore ? log h(x1 | T(?,?)) ? ? log?, and Problem 1 would return 1. 
6.2.2 An Expectation-Maximization Algorithm. Instead of solving the optimization prob-
lem implied by Equation (21), we propose a rather simple modification to the
expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to ap-
proximate the optimal solution?this algorithm finds a local maximum for the max-
imum likelihood problem using proper approximations. The modified algorithm is
given in Algorithm 1.
The modification from the usual expectation-maximization algorithm is done in the
M-step: Instead of using the expected value of the sufficient statistics by counting and
normalizing, we truncate the values by ?. It can be shown that if ?(0) ? ?(?), then the
likelihood is guaranteed to increase (and hence, the log-loss is guaranteed to decrease)
after each iteration of the algorithm.
508
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Algorithm 1: Expectation-Maximization Algorithmwith Proper Approximations.
Input: grammar G in binary normal form, initial parameters ?(0),  > 0, ? > 0,
s > 1
Output: learned parameters ?
draw x = ?x1, ..., xn? from p following Theorem 3; t? 1 ;
? ? n?s;
repeat
// E
?(t?1)
[
?k,i(z) | xj
]
denotes the expected counts of event i in multinomial k
under the distribution p?n(x)p(z | x,?
(t?1))
Compute for each training example j ? {1, . . . ,n}, for each event i ? {1, 2} in each
multinomial k ? {1, . . . ,K}: ??j,k,i ? E?(t?1)
[
?k,i(z) | xj
]
;
Set ?
(t)
i,k
= min{1? ?,max{?,
(
?n
j=1 ??j,k,i
)
/
(
?n
j=1
?2
i?=1 ??j,k,i?
)
}};
t? t+ 1;
until convergence;
return ?(t)
The reason for this likelihood increase stems from the fact that the M-step solves
the optimization problem of minimizing the log-loss (with respect to ? ? ?(?)) when
the posterior calculate at the E-step as the base distribution is used. This means that the
M-step minimizes (in iteration t): Er
[
? log h(x, z | ?(t) )
]
where the expectation is taken
with respect to the distribution r(x, z) = p?n(x)p(z | x,?
(t?1)). With this notion in mind,
the likelihood increase after each iteration follows from principles similar to those
described in Bishop (2006) for the EM algorithm.
7. Discussion
Our framework can be specialized to improve the two main criteria which have a trade-
off: the tightness of the proper approximation and the sample complexity. For example,
we can improve the tightness of our proper approximations by taking a subsequence
of Fn. This will make the sample complexity bound degrade, however, because Kn will
grow faster. Table 2 shows the trade-offs between parameters in our model and the
effectiveness of learning.
We note that the sample complexity bounds that we give in this article give
insight about the asymptotic behavior of grammar estimation, but are not necessarily
Table 2
Trade-off between quantities in our learning model and effectiveness of different criteria. Kn is
the constant that satisfies the boundedness property (Theorems 2 and 3) and s is a fixed constant
larger than 1 (Section 4.1).
criterion as Kn increases . . . as s increases . . .
tightness of proper approximation improves improves
sample complexity bound degrades degrades
509
Computational Linguistics Volume 38, Number 3
sufficiently tight to be used in practice. It still remains an open problem to obtain
sample complexity bounds which are sufficiently tight in this respect. For a discussion
about the connection of grammar learning in theory and practice, we refer the reader
to Clark and Lappin (2010).
It is also important to note that MLE is not the only option for estimating finite
state probabilistic grammars. There has been some recent advances in learning finite
state models (HMMs and finite state transducers) by using spectral analysis of matrices
which consist of quantities estimated from observations only (Hsu, Kakade, and Zhang
2009; Balle, Quattoni, and Carreras 2011), based on the observable operator models of
Jaeger (1999). These algorithms are not prone to local minima, and converge to the
correct model as the number of samples increases, but require some assumptions about
the underlying model that generates the data.
7.1 Tsybakov Noise
In this article, we chose to introduce assumptions about distributions that generate
natural language data. The choice of these assumptions was motivated by observations
about properties shared among treebanks. The main consequence of making these
assumptions is bounding the amount of noise in the distribution (i.e., the amount of
variation in probabilities across labels given a fixed input).
There are other ways to restrict the noise in a distribution. One condition for such
noise restriction, which has received considerable recent attention in the statistical liter-
ature, is the Tsybakov noise condition (Tsybakov 2004; Koltchinskii 2006). Showing that
a distribution satisfies the Tsybakov noise condition enables the use of techniques (e.g.,
from Koltchinskii 2006) for deriving distribution-dependent sample complexity bounds
that depend on the parameters of the noise. It is therefore of interest to see whether
Tsybakov noise holds under the assumptions presented in Section 3.1. We show that
this is not the case, and that Tsybakov noise is too permissive. In fact, we show that p
can be a probabilistic grammar itself (and hence, satisfy the assumptions in Section 3.1),
and still not satisfy the Tsybakov noise conditions.
Tsybakov noise was originally introduced for classification problems (Tsybakov
2004), and was later extended to more general settings, such as the one we are facing in
this article (Koltchinskii 2006). We now explain the definition of Tsybakov noise in our
context.
Let C > 0 and ? ? 1. We say that a distribution p(x, z) satisfies the (C,?) Tsybakov
noise condition if for any  > 0 and h, g ? H such that h, g ? {h? | Ep(h?,H) ? }, we
have
dist(g, h) 
?
?
?
?Ep
[
(
log g
log h
)2
]
? C1/? (25)
This interpretation of Tsybakov noise implies that the diameter of the set of functions
from the concept class that has small excess risk should shrink to 0 at the rate in
Equation (25). Distribution-dependent bounds from Koltchinskii (2006) are monotone
with respect to the diameter of this set of functions, and therefore demonstrating that it
goes to 0 enables sharper derivations of sample complexity bounds.
510
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
We turn now to illustrating that the Tsybakov condition does not hold for proba-
bilistic grammars in most cases. Let G be a probabilistic grammar. Define A = AG(?) as
a matrix such that
(AG(?))(k,i),(k?,i? ) 
E
[
?k,i ??k?,i?
]
E[?k,i]E[?k?,i? ]
Theorem 5
Let G be a grammar with K ? 2 and degree 2. Assume that p is ?G,??? for some ??, such
that ??1,1 = ?
?
2,1 = ? and that c1 ? c2. If AG(?
?) is positive definite, then p does not satisfy
the Tsybakov noise condition for any (C,?), where C > 0 and ? ? 1.
See Appendix C for the proof of Theorem 5.
In Appendix C we show that AG(?) is positive semi-definite for any choice of ?.
The main intuition behind the proof is that given a probabilistic grammar p, we can
construct an hypothesis h such that the KL divergence between p and h is small, but
dist(p, h) is lower-bounded and is not close to 0.
We conclude that probabilistic grammars, as generative distributions of data, do
not generally satisfy the Tsybakov noise condition. This motivates an alternative choice
of assumptions that could lead to better understanding of rates of convergences and
bounds on the excess risk. Section 3.1 states such assumptions which were also justified
empirically.
7.2 Comparison to Dirichlet Maximum A Posteriori Solutions
The transformation T(?,?) from Section 4.1 can be thought of as a smoother for the
probabilities ?: It ensures that the probability of each rule is at least ? (and as a result,
the probabilities of all rules cannot exceed 1? ?). Adding pseudo-counts to frequency
counts is also a common way to smooth probabilities in models based on multinomial
distributions, including probabilistic grammars (Manning and Schu?tze 1999). These
pseudo-counts can be framed as a maximum a posteriori (MAP) alternative to the
maximum likelihood problem, with the choice of Bayesian prior over the parameters in
the form of a Dirichlet distribution. In comparison to our framework, with (symmetric)
Dirichlet smoothing, instead of truncating the probabilities with a margin ? we would
set the probability of each rule (in the supervised setting) to
??k,i =
?n
j=1 ??j,k,i + ?? 1
?n
j=1 ??j,k,1 +
?n
j=1 ??j,k,2 + 2(?? 1)
(26)
for i = 1, 2, where ??k,i are the counts in the data of event i in multinomial k for Example j.
Dirichlet smoothing can be formulated as the result of adding a symmetric Dirichlet
prior over the parameters ?k,i with hyperparameter ?. Then Equation (26) is the mode
of the posterior after observing ??k,i appearances of event i in multinomial k.
The effect of Dirichlet smoothing becomes weaker as we have more samples,
because the frequency counts ??j,k,i become dominant in both the numerator and the
denominator when there are more data. In this sense, the prior?s effect on learning
diminishes as we use more data. A similar effect occurs in our framework: ? = n?s
where n is the number of samples?the more samples we have, the more we trust the
511
Computational Linguistics Volume 38, Number 3
counts in the data to be reliable. There is a subtle difference, however. With the Dirichlet
MAP solution, the smoothing is less dominant only if the counts of the features are large,
regardless of the number of samples we have. With our framework, smoothing depends
only on the number of samples we have. These two scenarios are related, of course: The
more samples we have, the more likely it is that the counts of the events will grow large.
7.3 Other Derivations of Sample Complexity Bounds
In this section, we discuss other possible solutions to the problem of deriving sample
complexity bounds for probabilistic grammars.
7.3.1 Using Talagrand?s Inequality. Our bounds are based on VC theory together with
classical results for empirical processes (Pollard 1984). There have been some recent
developments to the derivation of rates of convergence in statistical learning theory
(Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), most
prominently through the use of Talagrand?s inequality (Talagrand 1994), which is a
concentration of measure inequality, in the spirit of Lemma 2.
The bounds achieved with Talagrand?s inequality are also distribution-dependent,
and are based on the diameter of the -minimal set?the set of hypotheses which have
an excess risk smaller than . We saw in Section 7.1 that the diameter of the -minimal
set does not follow the Tsybakov noise condition, but it is perhaps possible to find
meaningful bounds for it, in which case we may be able to get tighter bounds using
Talagrand?s inequality. We note that it may be possible to obtain data-dependent bounds
for the diameter of the -minimal set, following Koltchinskii (2006), by calculating the
diameter of the -minimal set using p?n.
7.3.2 Simpler Bounds for the Supervised Case.As noted in Section 6.1, minimizing empirical
risk with the log-loss leads to a simple frequency count for calculating the estimated
parameters of the grammar. In Corazza and Satta (2006), it has been also noted that to
minimize the non-empirical risk, it is necessary to set the parameters of the grammar to
the normalized expected count of the features.
This means that we can get bounds on the deviation of a certain parameter from
the optimal parameter by applying modifications to rather simple inequalities such
as Hoeffding?s inequality, which determines the probability of the average of a set of
i.i.d. random variables deviating from its mean. The modification would require us
to split the event space into two cases: one in which the count of some features is
larger than some fixed value (which will happen with small probability because of the
bounded expectation of features), and one in which they are all smaller than that fixed
value. Handling these two cases separately is necessary because Hoeffding?s inequality
requires that the count of the rules is bounded.
The bound on the deviation from the mean of the parameters (the true probability)
can potentially lead to a bound on the excess risk in the supervised case. This formula-
tion of the problem would not generalize to the unsupervised case, however, where the
empirical risk minimization does not amount to simple frequency count.
7.4 Open Problems
We conclude the discussion with some directions for further exploration and future
work.
512
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
7.4.1 Sample Complexity Bounds with Semi-Supervised Learning. Our bounds focus on the
supervised case and the unsupervised case. There is a trivial extension to the semi-
supervised case. Consider the objective function to be the sum of the likelihood for the
labeled data together with the marginalized likelihood of the unlabeled data (this sum
could be a weighted sum). Then, use the sample complexity bounds for each summand
to derive a sample complexity bound on this sum.
It would be more interesting to extend our results to frameworks such as the one
described by Balcan and Blum (2010). In that case, our discussion of sample complexity
would attempt to identify how unannotated data can reduce the space of candidate
probabilistic grammars to a smaller set, after which we can use the annotated data
to estimate the final grammar. This reduction of the space is accomplished through a
notion of compatibility, a type of fitness that the learner believes the estimated grammar
should have given the distribution that generates the data. The key challenge in the
case of probabilistic grammars would be to properly define this compatibility notion
such that it fits the log-loss. If this is achieved, then similar machinery to that described
in this paper (with proper approximations) can be followed to derive semi-supervised
sample complexity bounds for probabilistic grammars.
7.4.2 Sharper Bounds for the Pseudo-Dimension of Probabilistic Grammars. The pseudo-
dimension of a probabilistic grammar with the log-loss is bounded by the number of
parameters in the grammar, because the logarithm of a distribution generated by a
probabilistic grammar is a linear function. Typically the set of counts for the feature
vectors of a probabilistic grammar resides in a subspace of a dimension which is smaller
than the full dimension specified by the number of parameters, however. The reason for
this is that there are usually relationships (which are often linear) between the elements
in the feature counts. For example, with HMMs, the total feature count for emissions
should equal the total feature count for transitions. With PCFGs, the total number of
times that nonterminal rules fire equals the total number of times that features with
that nonerminal in the right-hand side fired, again reducing the pseudo-dimension. An
open problem that remains is characterization of the exact value pseudo-dimension for
a given grammar, determined by consideration of various properties of that grammar.
We conjecture, however, that a lower bound on the pseudo-dimension would be rather
close to the full dimension of the grammar (the number of parameters).
It is interesting to note that there has been some work to identify the VC dimension
and pseudo-dimension for certain types of grammars. Bane, Riggle, and Sonderegger
(2010), for example, calculated the VC dimension for constraint-based grammars.
Ishigami and Tani (1993, 1997) computed the VC dimension for finite state automata
with various properties.
7.5 Conclusion
We presented a framework for performing empirical risk minimization for probabilis-
tic grammars, in which sample complexity bounds, for the supervised case and the
unsupervised case, can be derived. Our framework is based on the idea of bounded
approximations used in the past to derive sample complexity bounds for graphical
models.
Our framework required assumptions about the probability distribution that gener-
ates sentences or derivations in the language of the given grammar. These assumptions
were tested using corpora, and found to fit the data well.
513
Computational Linguistics Volume 38, Number 3
We also discussed algorithms that can be used for minimizing empirical risk in
our framework, given enough samples. We showed that directly trying to minimize
empirical risk in the unsupervised case is NP-hard, and suggested an approximation
based on an expectation-maximization algorithm.
Appendix A. Proofs
We include in this appendix proofs for several results in the article.
Utility Lemma 1
Let ai ? [0, 1], i ? {1, . . . ,N} such that
?
i ai = 1. Define b1 = a1, c1 = 1? a1, bi =
(
ai
ai?1
)
(
bi?1
ci?1
)
, and ci = 1? bi for i ? 2. Then ai =
?
?
i?1
?
j=1
cj
?
? bi.
Proof
Proof by induction on i ? {1, . . . ,N}. Clearly, the statement holds for i = 1. Assume it
holds for arbitrary i < N. Then:
ai+1 =
(
ai
ai
)
ai+1 =
?
?
?
?
i?1
?
j=1
cj
?
? bi
?
?
ai+1
ai
=
?
?
?
?
i?1
?
j=1
cj
?
? bi
?
?
cibi+1
bi
=
?
?
i
?
j=1
cj
?
? bi+1
and this completes the proof. 
Lemma 1
Denote by Z,n the set
?
f?F{z | Cn( f )(z)? f (z) ? }. Denote by A,n the event ?one of
zi ? D is in Z,n.? If Fn properly approximates F, then:
E
[
Ep?n
[
gn
]
? Ep?n
[
f ?n
]
]
(A.1)
?
?
?
?
E
[
Ep?n
[
Cn( f
?
n )
]
| A,n
]
?
?
?
p(A,n)+
?
?
?
E
[
Ep?n
[
f ?n
]
| A,n
]
?
?
?
p(A,n)+ tail(n)
where the expectations are taken with respect to the data set D.
Proof
Consider the following:
E
[
Ep?n
[
gn
]
? Ep?n
[
f ?n
]
]
= E
[
Ep?n
[
gn
]
? Ep?n
[
Cn( f
?
n )
]
+ Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
]
= E
[
Ep?n
[
gn
]
? Ep?n
[
Cn( f
?
n )
]
]
+ E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
]
514
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Note first that E
[
Ep?n
[
gn
]
? Ep?n
[
Cn( f
?
n )
]]
? 0, by the definition of gn as the mini-
mizer of the empirical risk. We next bound E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]]
. We know from
the requirement of proper approximation that we have
E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
]
= E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
| A,n
]
p(A,n)
+ E
[
Ep?n
[
Cn( f
?
n )
]
? Ep?n
[
f ?n
]
| ?A,n
]
(1? p(A,n))
? |E
[
Ep?n
[
Cn( f
?
n )
]
| A,n
]
|p(A,n)+ |E
[
Ep?n
[
f ?n
]
| A,n
]
|p(A,n)+ tail(n)
and that equals the right side of Equation (Appendix A.1). 
Proposition 2
Let p ? P(?,L, r, q,B,G) and let Fm be as defined earlier. There exists a constant ? =
?(L, q, p,N) > 0 such that Fm has the boundedness property with Km = sN log
3m and
bound(m) = m
?? logm.
Proof
Let f ? Fm. Let Z(m) = {z | |z| ? log
2m}. Then, for all z ? Z(m) we have | f (z)| =
?
?
i,k?(k, i) log?k,i ?
?
i,k?(k, i)(p logm) ? sN log
3m = Km, where the first inequality
follows from f ? Fm (?k,i ? m?s) and the second from |z| ? log
2m. In addition, from the
requirements on p we have
E
[
| f | ? I {| f | ? Km}
]
?
(
sN log3m
)
?
?
?
?
k>log2m
L?(k)rkk
?
? ?
(
? log3m
)
?
(
qlog
2m
)
for ? = sNL
(1? q)2
. Finally, for ?(L, q, p,N)  log?+ 1+ log 1q = ? > 0 and if m > 1 then
(
? log3m
)(
qlog
2m
)
? m?? logm. 
Utility Lemma 4
(From [Dasgupta 1997].) Let a ? [0, 1] and let b = a if a ? [?, 1? ?], b = ? if a ? ?,
and b = 1? ? if a ? 1? ?. Then for any  ? 1/2 such that ? ? /(1+ ) we have
log a/b ? .
Proposition 3
Let p ? P(?,L, r, q,B,G) and let Fm as defined earlier. There exists anM such that for any
m > Mwe have
p
?
?
?
f?F
{z | Cm( f )(z)? f (z) ? tail(m)}
?
? ? tail(m)
for tail(m) =
N log2m
ms ? 1 and Cm( f ) = T( f,m
?s).
515
Computational Linguistics Volume 38, Number 3
Proof
Let Z(m) be the set of derivations of size bigger than log2m. Let f ? F. Define f ? =
T(f,m?s). For any z /? Z(m) we have that
f ?(z)? f (z) = ?
K
?
k=1
(
?k,1(z) log?k,1 + ?k,2(z) log?k,2 ? ?k,1(z) log?
?
k,1 ? ?k,1(z) log?
?
k,2
)
?
K
?
k=1
log2m
(
max{0, log(??k,1/?k,1)}+max{0, log(?
?
k,2/?k,2)}
)
(A.2)
Without loss of generality, assume tail(n)/N log
2m ? 1/2. Let ? =
tail(m)/N log
2m
1+ tail(m)/N log
2m
=
1/ms. From Utility Lemma 4 we have that log(??k,i/?k,i) ? tail(m)/N logm. Plug this
into Equation A.2 (N = 2K) to get that for all z /? Z(m) we have f ?(z)? f (z) ? tail(m).
It remains to show that the measure p(Z(m)) ? tail(m). Note that
?
z?Z(m) p(z) ?
?
k>log2m
L?(k)rk ? L
?
k>log2m
qk = Lqlog
2m/(1? q) < tail(m) for m > M where M is
fixed. 
Proposition 7
There exists a ??(L, p, q,N) > 0 such that F?m has the boundedness property with Km =
sN log3m and bound(m) = m
??? logm.
Proof
From the requirement of p, we know that for any x we have a z such that yield(z) = x
and |z| ? ?|x|. Therefore, if we let X(m) = {x | |x| ? log2m/?}, then we have for any
f ? F?m and x ? X(m) that f (x) ? sN log
3m = Km (similarly to the proof of Proposition 2).
Denote by f1(x, z) the function in Fm such that f (x) = ? log
?
z exp(?f1(x, z)).
In addition, from the requirements on p and the definition of Km we have
E
[
| f | ? I {| f | ? Km}
]
=
?
x
p(x)f (x)I { f ? Km}
=
?
x:|x|>log2m/?
p(x)f (x)
?
?
x:|x|>log2m/?
p(x)f1(x, z(x))
516
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
where z(x) is some derivation for x. We have
?
x:|x|>log2m/?
p(x)f1(x, z(x)) ?
?
x:|x|?log2m/?
?
z?Dx(G)
p(x, z)f1(x, z(x))
? sN logm
?
x:|x|>log2m/?
?
z
p(x, z)|z(x)|
? sN logm
?
k>log2m
?(k)rkk
? sN logm
?
k>log2m
qkk ? ? logmqlog
2m
for some constant ? > 0. Finally, for some ??(L, p, q,N) = ?? > 0 and some constant M,
if m > M then ? logm
(
qlog
2m
)
? m??
? logm. 
Utility Lemma 2
For ai, bi ? 0, if ? log
?
i ai + log
?
i bi ?  then there exists an i such that ? log ai +
log bi ? .
Proof
Assume ? log ai + log bi <  for all i. Then, bi/ai < e, therefore
?
i bi/
?
i ai < e
, there-
fore ? log
?
i ai + log
?
i bi <  which is a contradiction to ? log
?
i ai + log
?
i bi ? .

The next lemma is the main concentation of measure result that we use. Its proof
requires some simple modification to the proof given for Theorem 24 in Pollard (1984,
pages 30?31).
Lemma 2
Let Fn be a permissible class of functions such that for every f ? Fn we have E[| f | ?
I {| f | ? Kn}] ? bound(n). Let Ftruncated,n = { f ? I { f ? Kn} | f ? Fm}, that is, the set of
functions from Fn after being truncated by Kn. Then for  > 0 we have
p
(
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| > 2
)
? 8N(/8,Ftruncated,n) exp
(
? 1
128
n2/K2n
)
+ bound(n)/
provided n ? K2n/4
2 and bound(n) < .
Proof
First note that
sup
f?Fn
|Ep?n
[
f
]
? Ep
[
f
]
| ? sup
f?Fn
|Ep?n
[
f I {| f | ? Kn}
]
? Ep
[
f I {| f | ? Kn}
]
|
+ sup
f?Fn
Ep?n
[
| f |I {| f | ? Kn}
]
+ sup
f?Fn
Ep
[
| f |I {| f | ? Kn}
]
517
Computational Linguistics Volume 38, Number 3
We have supf?Fn Ep
[
| f |I {| f | ? Kn}
]
? bound(n) < , and also, from Markov in-
equality, we have
P(sup
f?Fn
Ep?n
[
| f |I {| f | ? Kn}
]
> ) ? bound(n)/
At this point, we can follow the proof of Theorem 24 in Pollard (1984), and its
extension on pages 30?31 to get Lemma 2, using the shifted set of functions Ftruncated,n.

Appendix B. Minimizing Log-Loss for Probabilistic Grammars
Central to our algorithms for minimizing the log-loss (both in the supervised case and
the unsupervised case) is a convex optimization problem of the form
min
?
K
?
k=1
ck,1 log?k,1 + ck,2 log?k,2
such that ?k ? {1, . . . ,K} :
?k,1 + ?k,2 = 1
? ? ?k,1 ? 1? ?
? ? ?k,2 ? 1? ?
for constants ck,i which depend on p?n or some other intermediate distribution in the
case of the expectation-maximization algorithm and ? which is a margin determined
by the number of samples. This minimization problem can be decomposed into several
optimization problems, one for each k, each having the following form:
max
?
c1?1 + c2?2 (B.1)
such that exp(?1)+ exp(?2) = 1 (B.2)
? ? ?1 ? 1? ? (B.3)
? ? ?2 ? 1? ? (B.4)
where ci ? 0 and 1/2 > ? ? 0. Ignore for a moment the constraints ? ? ?i ? 1? ?. In
that case, this can be thought of as a regular maximum likelihood estimation problem,
so ?i = ci/(c1 + c2). We give a derivation of this result in this simple case for completion.
We use Lagranian multipliers to solve this problem. Let F(?1,?2) = c1?1 + c2?2. Define
the Lagrangian:
g(?) = inf
?
L(?,?)
= inf
?
c1?1 + c2?2 + ?(exp(?1)+ exp(?2)? 1)
518
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Taking the derivative of the term we minimize in the Lagrangian, we have
?L
??i
= ci + ? exp(?i)
Setting the derivatives to 0 for minimization, we have
g(?) = c1 log(?c1/?)+ c2 log(?c2/?)+ ?(?c1/?? c2/?? 1) (B.5)
g(?) is the objective function of the dual problem of Equation (B.1)?Equation (B.2).
We would like to minimize Equation (B.5) with respect to ?. The derivative of g(?) is
?g
??
= ?c1/?? c2/?? 1
hence when equating the derivative of g(?) to 0, we get ? = ?(c1 + c2), and therefore
the solution is ??i = log (ci/(c1 + c2)). We need to verify that the solution to the dual
problem indeed gets the optimal value for the primal. Because the primal problem is
convex, it is sufficient to verify that the Karush-Kuhn-Tucker (KKT) conditions hold
(Boyd and Vandenberghe 2004). Indeed, we have
?F
??i
(??)+ ? ?h
??i
(??) = ci ? (c1 + c2)?
ci
c1 + c2
= 0
where h(?)  exp(?)+ exp(?)? 1 stands for the equality constraint. The rest of the
KKT conditions trivially hold, therefore ?? is the optimal solution for Equations (B.1)?
(B.2).
Note that if 1? ? < ci/(c1 + c2) < ?, then this is the solution even when again
adding the constraints in Equation (B.3) and (B.4). When c1/(c1 + c2) < ?, then the
solution is ??1 = ? and ?
?
2 = 1? ?. Similarly, when c2/(c1 + c2) < ? then the solution is
??2 = ? and ?
?
1 = 1? ?. We describe why this is true for the first case. The second case
follows very similarly. Assume c1/(c1 + c2) < ?. We want to show that for any choice of
? ? [0, 1] such that ? > ?we have
c1 log?+ c2 log(1? ?) ? c1 log?+ c2 log(1? ?)
Divide both sides of the inequality by c1 + c2 and we get that we need to show that
c1
c1 + c2
log(?/?)+
c2
c1 + c2
log
(
1? ?
1? ?
)
? 0
Becausewehave? > ?, andwealsohave c1/(c1 + c2) < ?, it is sufficient to show that
? log(?/?)+ (1? ?) log
(
1? ?
1? ?
)
? 0 (B.6)
Equation (B.6) is precisely the definition of the KL divergence between the distribu-
tion of a coinwith probability ? of heads and the distribution of a coinwith probability?
519
Computational Linguistics Volume 38, Number 3
of heads, and therefore the right side in Equation (B.6) is positive, and we get what
we need.
Appendix C. Counterexample to Tsybakov Noise (Proofs)
Lemma 6
A = AG(?) is positive semi-definite for any probabilistic grammar ?G,??.
Proof
Let dk,i be a collection of constants. Define the random variable:
R(z) =
?
i,k
dk,i
E
[
?k,i
]?k,i(z)
We have that
E
[
R2
]
=
?
i,i?
?
k,k?
A(k,i),(k?,i? )dk,idk?,i?
which is always larger or equal to 0. Therefore, A is positive semi-definite. 
Lemma 7
Let 0 < ? < 1/2, c1, c2 ? 0. Let ?,C > 0. Also, assume that c1 ? c2. For any  > 0, define:
a = ?
(
exp
(
C1/? + /2
c1
))
= ?1?
b = ?
(
exp
(
?C1/? + /2
c2
))
= ?2?
t() = c1
(
1? ?
1? a
)
+ c2
(
1? ?
1? b
)
? (c1 + c2) exp(/2)
Then, for small enough , we have t() ? 0.
Proof
We have that t() ? 0 if
ac2 + bc1 ? ?
(c1 + c2)(1? a)(1? b)
1? ? exp(/2)+ c1 + c2
= (c1 + c2)
(
1?
(1? a)(1? b)
(1? ?) exp(?/2)
)
(C.1)
First, show that
(1? a)(1? b)
(1? ?) exp(?/2)
? 1? ? (C.2)
520
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
which happens if (after substituting a = ?1?, b = ?2?)
? ? (?1 + ?2 ? 2)/(1? ?1?2)
Note we have ?1?2 > 1 because c1 ? c2. In addition, we have ?1 + ?2 ? 2 ? 0 for small
enough  (can be shown by taking the derivative, with respect to  of?1 + ?2 ? 2, which
is always positive for small enough , and in addition, noticing that the value of ?1 +
?2 ? 2 is 0 when  = 0.) Therefore, Equation (C.2) is true.
Substituting Equation (C.2) in Equation (C.1), we have that t() ? 0 if
ac2 + bc1 ? (c1 + c2)?
which is equivalent to
c2?1 + c1?2 ? c1 + c2 (C.3)
Taking again the derivative of the left side of Equation (C.3), we have that it is an
increasing function of  (if c1 ? c2), and in addition at  = 0 it obtains the value c1 + c2.
Therefore, Equation (C.3) holds, and therefore t() ? 0 for small enough . 
Theorem 5
Let G be a grammar with K ? 2 and degree 2. Assume that p is ?G,??? for some ??, such
that ??1,1 = ?
?
2,1 = ? and that c1 ? c2. If AG(?
?) is positive definite, then p does not satisfy
the Tsybakov noise condition for any (C,?), where C > 0 and ? ? 1.
Proof
Define ? to be the eigenvalue of AG(?) with the smallest value (? is positive). Also,
define v(?) to be a vector indexed by k, i such that
vk,i(?) = E
[
?k,i
]
log
??k,i
?k,i
.
Simple algebra shows that for any h ? H(G) (and the fact that p ? H(G)), we have
Ep(h) = DKL(p?h) =
K
?
k=1
(
Ep
[
?k,1
]
log
??k,1
?k,1
+ Ep
[
?k,1
]
log
(
1? ??k,1
1? ?k,1
))
For a C > 0 and ? ? 1, define ? = C1/?. Let  < ?. First, we construct an h such
that DKL(p?h) < + /2 but dist(p, h) > C1/? as  ? 0. The construction follows.
Parametrize h by ? such that ? is identical to ?? except for k = 1, 2, in which case we
have
?1,1 = ?
?
1,1
(
exp
(
?+ /2
c1
))
= ?
(
exp
(
?+ /2
c1
))
(C.4)
?2,1 = ?
?
2,1
(
exp
(
??+ /2
c2
))
= ?
(
exp
(
??+ /2
c2
))
(C.5)
521
Computational Linguistics Volume 38, Number 3
Note that ? ? ?1,1 ? 1/2 and ?2,1 < ?. Then, we have that
DKL(p?h) =
K
?
k=1
(
Ep
[
?k,1
]
log
??k,1
?k,1
+ Ep
[
?k,1
]
log
(
1? ??k,1
1? ?k,1
))
= + c1 log
1? ??k,1
1? ?1,1
+ c2 log
1? ??k,2
1? ?2,1
= + c1 log
1? ?
1? ?1,1
+ c2 log
1? ?
1? ?2,1
We also have
c1 log
1? ?
1? ?1,1
+ c2 log
1? ?
1? ?2,1
? 0 (C.6)
if
c1 ?
1? ?
1? ?1,1
+ c2 ?
1? ?
1? ?2,1
? c1 + c2 (C.7)
(This can be shown by dividing Equation [C.6] by c1 + c2 and then using the concavity of
the logarithm function.) From Lemma 7, we have that Equation (C.7) holds. Therefore,
DKL(p?h) ? 2
Now, consider the following, which can be shown through algebraic manipulation:
dist(p, h) = E
[
(
log
p
h
)2
]
=
?
k,k?
?
i,i?
E
[
?k,i ??k?,i?
]
(
log
??k,i
?k,i
)(
log
??k?,i?
?k?,i?
)
Then, additional algebraic simplification shows that
E
[
(
log
p
h
)2
]
= v(?)Av(?)
A fact from linear algebra states that
v(?)Av(?) ? ?||v(?)||22
where ? is the smallest eigenvalue in A. From the construction of ? and Equation (C.4)?
(C.5), we have that ||v(?)||22 > ?
2. Therefore,
E
[
(
log
p
h
)2
]
? ??2
which means dist(p, h) ?
?
?C1/?. Therefore, p does not satisfy the Tsybakov noise
condition with parameters (D,?) for any D > 0. 
522
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Appendix D. Notation
Table D.1 gives a table of notation for symbols used throughout this article.
Table 1
Table of notation symbols used in this article.
Symbol Description 1st Mention
E
R
M
X Instance space (natural language sentences) Sec. 2
Z Output space (grammar derivations) Sec. 2
p Distribution generating the data Sec. 2
Q Concept space, a family of distributions Sec. 2
q An estimated distribution Sec. 2
qopt Risk minimizer Eq. 1
n Number of available samples Sec. 2
p?n Empirical distribution Sec. 2
q? Empirical risk minimizer Eq. 2
Ep(q;Q) Excess risk Eq. 4
Rn(Q) Empirical process for the log-loss Eq. 5
G
ra
m
m
a
rs
G Grammar (for example, CFG rules) Sec. 3
? Probabilistic grammar parameters Sec. 3
K Number of multinomials in the probabilistic grammar Eq. 11
Nk Size of the kth multinomial of the probabilistic grammar Eq. 11
N
?K
k=1Nk Sec. 3
x Sentence in the language of the grammar Sec. 3
z Derivation in the grammar Sec. 3
?k,i(x, z) Count of the ith event firing in the kth multinomial in x and z Eq. 11
?G Parameter space for a given probabilistic grammar G Eq. 11
? Parameters for a probabilistic grammar Eq. 11
deg(G) The degree of G, maxk Nk Sec. 3
Dx(G) The set of derivations for string x Sec. 3
H,H(G) Concept space, a set of probabilistic grammars Sec. 3
F,F(G) Negated log-concept space, {? log h | h ? H(G)} Sec. 3
L Constant determining distributional assumption Sec. 3.1
q Constant determining distributional assumption Sec. 3.1
r Constant determining distributional assumption Sec. 3.1
P
ro
p
e
r
A
p
p
ro
x
im
a
ti
o
n
s
Fn Element n in a proper approximation (contained in F) Sec. 4
tail(n) Convergence rate for the boundedness property Sec. 4
bound(n) Convergence rate for the tightness property Sec. 4
Cn( f ) A map for f ? F to f ? ? Fn Sec. 4
T(?,?) Parameters ?with shifted probabilities Sec. 4.1
T( f,?) f ? F with shifted probabilities Sec. 4.1
?G(?) Set of parameters {T(?,?) | ? ? ?G} for a given G Sec. 4.1
s A constant larger than 1 on which boundedness property
depends
Sec. 4.1
?(L, q, p,N) A constant on which sample complexity depends for the su-
pervised case
Prop. 2
F?n Element n in a proper approximation (contained in F) Sec. 4
C?n( f ) A map for f ? F to f
? ? Fn Sec. 4
?tail(n) Convergence rate for the soundness property Sec. 4
?bound(n) Convergence rate for the tightness property Sec. 4
??(L, q, p,N) A constant on which sample complexity depends for the
unsupervised case
Sec. 5.3
523
Computational Linguistics Volume 38, Number 3
Acknowledgments
The authors thank the anonymous reviewers
for their comments and Avrim Blum, Steve
Hanneke, Mark Johnson, John Lafferty, Dan
Roth, and Eric Xing for useful conversations.
This research was supported by National
Science Foundation grant IIS-0915187.
References
Abe, N., J. Takeuchi, and M. Warmuth.
1991. Polynomial learnability
of probabilistic concepts with
respect to the Kullback-Leiber
divergence. In Proceedings of the
Conference on Learning Theory,
pages 277?289.
Abe, N. and M. Warmuth. 1992. On
the computational complexity of
approximating distributions by
probabilistic automata.Machine
Learning, 2:205?260.
Angluin, D. 1987. Learning regular sets from
queries and counterexamples. Information
and Computation, 75:87?106.
Anthony, M. and P. L. Bartlett. 1999.
Neural Network Learning: Theoretical
Foundations. Cambridge
University Press.
Balcan, M. and A. Blum. 2010.
A discriminative model for semi-
supervised learning. Journal of the
Association for Computing Machinery,
57(3):1?46.
Balle, B., A. Quattoni, and X. Carreras.
2011. A spectral learning algorithm for
finite state transducers. In Proceedings
of the European Conference on Machine
Learning/the Principles and Practice of
Knowledge Discovery in Databases,
pages 156?171.
Bane, M., J. Riggle, and M. Sonderegger.
2010. The VC dimension of
constraint-based grammars.
Lingua, 120(5):1194?1208.
Bartlett, P., O. Bousquet, and S. Mendelson.
2005. Local Rademacher complexities.
Annals of Statistics, 33(4):1497?1537.
Bishop, C. M. 2006. Pattern Recognition and
Machine Learning. Springer, Berlin.
Boyd, S. and L. Vandenberghe. 2004.
Convex Optimization. Cambridge
University Press.
Carrasco, R. 1997. Accurate computation
of the relative entropy between
stochastic regular grammars.
Theoretical Informatics and Applications,
31(5):437?444.
Carroll, G. and E. Charniak. 1992. Two
experiments on learning probabilistic
dependency grammars from corpora.
Technical report, Brown University,
Providence, RI.
Charniak, E. 1993. Statistical Language
Learning. MIT Press, Cambridge, MA.
Charniak, E. and M. Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the Association for Computational Linguistics,
pages 173?180.
Chi, Z. 1999. Statistical properties of
probabilistic context-free grammars.
Computational Linguistics, 25(1):131?160.
Clark, A., R. Eyraud, and A. Habrard. 2008.
A polynomial algorithm for the inference
of context free languages. In Proceedings of
the International Colloquium on Grammatical
Inference, pages 29?42.
Clark, A. and S. Lappin. 2010. Unsupervised
learning and grammar induction.
In Alexander Clark, Chris Fox, and
Shalom Lappin, editors, The Handbook
of Computational Linguistics and Natural
Language Processing. Wiley-Blackwell,
London, pages 197?220.
Clark, A. and F. Thollard. 2004.
PAC-learnability of probabilistic
deterministic finite state automata.
Journal of Machine Learning Research,
5:473?497.
Cohen, S. B. and N. A. Smith. 2010a.
Covariance in unsupervised learning of
probabilistic grammars. Journal of Machine
Learning Research, 11:3017?3051.
Cohen, S. B. and N. A. Smith. 2010b.
Empirical risk minimization with
approximations of probabilistic
grammars. In Proceedings of the
Advances in Neural Information
Processing Systems, pages 424?432.
Cohen, S. B. and N. A. Smith. 2010c. Viterbi
training for PCFGs: Hardness results and
competitiveness of uniform initialization.
In Proceedings of the Association for
Computational Linguistics, pages 1502?1511.
Collins, M. 2003. Head-driven statistical
models for natural language processing.
Computational Linguistics, 29:589?637.
Collins, M. 2004. Parameter estimation for
statistical parsing models: Theory and
practice of distribution-free methods.
In H. Bunt, J. Carroll, and G. Satta, Text,
Speech and Language Technology (New
Developments in Parsing Technology).
Kluwer, Dordrecht, pages 19?55.
Corazza, A. and G. Satta. 2006. Cross-entropy
and estimation of probabilistic context-free
524
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
grammars. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics, pages 335?342.
Cover, T. M. and J. A. Thomas. 1991.
Elements of Information Theory. Wiley,
London.
Dasgupta, S. 1997. The sample complexity
of learning fixed-structure bayesian
networks.Machine Learning,
29(2?3):165?180.
de la Higuera, C. 2005. A bibliographical
study of grammatical inference. Pattern
Recognition, 38:1332?1348.
Dempster, A., N. Laird, and D. Rubin. 1977.
Maximum likelihood estimation from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B,
39:1?38.
Gildea, D. 2010. Optimal parsing strategies
for linear context-free rewriting systems.
In Proceedings of the North American Chapter
of the Association for Computational
Linguistics, pages 769?776.
Go?mez-Rodr??guez, C. and G. Satta.
2009. An optimal-time binarization
algorithm for linear context-free
rewriting systems with fan-out two.
In Proceedings of the Association for
Computational Linguistics-International
Joint Conference on Natural Language
Processing, pages 985?993.
Grenander, U. 1981. Abstract Inference. Wiley,
New York.
Haussler, D. 1992. Decision-theoretic
generalizations of the PAC model
for neural net and other learning
applications. Information and
Computation, 100:78?150.
Hsu, D., S. M. Kakade, and T. Zhang.
2009. A spectral algorithm for
learning hidden Markov models.
In Proceedings of the Conference on
Learning Theory.
Ishigami, Y. and S. Tani. 1993. The
VC-dimensions of finite automata
with n states. In Proceedings of
Algorithmic Learning Theory,
pages 328?341.
Ishigami, Y. and S. Tani. 1997.
VC-dimensions of finite automata and
commutative finite automata with k letters
and n states. Applied Mathematics,
74(3):229?240.
Jaeger, H. 1999. Observable operator models
for discrete stochastic time series. Neural
Computation, 12:1371?1398.
Kearns, M. and L. Valiant. 1989.
Cryptographic limitations on learning
Boolean formulae and finite automata.
In Proceedings of the 21st Association
for Computing Machinery Symposium
on the Theory of Computing,
pages 433?444.
Kearns, M. J. and U. V. Vazirani. 1994.
An Introduction to Computational
Learning Theory. MIT Press,
Cambridge, MA.
Klein, D. and C. D. Manning. 2004.
Corpus-based induction of syntactic
structure: Models of dependency and
constituency. In Proceedings of the
Association for Computational Linguistics,
pages 478?487.
Koltchinskii, V. 2006. Local Rademacher
complexities and oracle inequalities
in risk minimization. The Annals of
Statistics, 34(6):2593?2656.
Leermakers, R. 1989. How to cover a
grammar. In Proceedings of the Association
for Computational Linguistics,
pages 135?142.
Manning, C. D. and H. Schu?tze. 1999.
Foundations of Statistical Natural
Language Processing. MIT Press,
Cambridge, MA.
Massart, P. 2000. Some applications of
concentration inequalities to statistics.
Annales de la Faculte? des Sciences de
Toulouse, IX(2):245?303.
Nijholt, A. 1980. Context-Free Grammars:
Covers, Normal Forms, and Parsing
(volume 93 of Lecture Notes in
Computer Science). Springer-Verlag,
Berlin.
Palmer, N. and P. W. Goldberg. 2007.
PAC-learnability of probabilistic
deterministic finite state automata
in terms of variation distance.
In Proceedings of Algorithmic Learning
Theory, pages 157?170.
Pereira, F. C. N. and Y. Schabes. 1992.
Inside-outside reestimation from partially
bracketed corpora. In Proceedings of the
Association for Computational Linguistics,
pages 128?135.
Pitt, L. 1989. Inductive inference, DFAs, and
computational complexity. Analogical and
Inductive Inference, 397:18?44.
Pollard, D. 1984. Convergence of Stochastic
Processes. Springer-Verlag, New York.
Ron, D. 1995. Automata Learning and Its
Applications. Ph.D. thesis, Hebrew
University of Jerusalem.
Ron, D., Y. Singer, and N. Tishby. 1998.
On the learnability and usage of acyclic
probabilistic finite automata. Journal
of Computer and System Sciences,
56(2):133?152.
525
Computational Linguistics Volume 38, Number 3
Shalev-Shwartz, S., O. Shamir, K. Sridharan,
and N. Srebro. 2009. Learnability and
stability in the general learning setting.
In Proceedings of the Conference on
Learning Theory.
Sipser, M. 2006. Introduction to the Theory of
Computation, Second Edition. Thomson
Course Technology, Boston, MA.
Talagrand, M. 1994. Sharper bounds for
Gaussian and empirical processes.
Annals of Probability, 22:28?76.
Terwijn, S. A. 2002. On the learnability of
hidden Markov models. In P. Adriaans,
H. Fernow, & M. van Zaane. Grammatical
Inference: Algorithms and Applications
(Lecture Notes in Computer Science).
Springer, Berlin, pages 344?348.
Tsybakov, A. 2004. Optimal aggregation of
classifiers in statistical learning. The Annals
of Statistics, 32(1):135?166.
Vapnik, V. N. 1998. Statistical Learning Theory.
Wiley-Interscience, New York.
526
Frame-Semantic Parsing
Dipanjan Das?
Google Inc.
Desai Chen??
Massachusetts Institute of Technology
Andre? F. T. Martins?
Priberam Labs
Instituto de Telecomunicac?o?es
Nathan Schneider?
Carnegie Mellon University
Noah A. Smith?
Carnegie Mellon University
Frame semantics is a linguistic theory that has been instantiated for English in the FrameNet
lexicon. We solve the problem of frame-semantic parsing using a two-stage statistical model
that takes lexical targets (i.e., content words and phrases) in their sentential contexts and
predicts frame-semantic structures. Given a target in context, the first stage disambiguates it to a
semantic frame. This model uses latent variables and semi-supervised learning to improve frame
disambiguation for targets unseen at training time. The second stage finds the target?s locally
expressed semantic arguments. At inference time, a fast exact dual decomposition algorithm
collectively predicts all the arguments of a frame at once in order to respect declaratively stated
linguistic constraints, resulting in qualitatively better structures than na??ve local predictors.
Both components are feature-based and discriminatively trained on a small set of annotated
frame-semantic parses. On the SemEval 2007 benchmark data set, the approach, along with a
heuristic identifier of frame-evoking targets, outperforms the prior state of the art by significant
margins. Additionally, we present experiments on the much larger FrameNet 1.5 data set. We
have released our frame-semantic parser as open-source software.
? Google Inc., New York, NY 10011. E-mail: dipanjand@google.com.
?? Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology,
Cambridge, MA 02139. E-mail: desaic@csail.mit.edu.
? Alameda D. Afonso Henriques, 41 - 2.? Andar, 1000-123, Lisboa, Portugal. E-mail: atm@priberam.pt.
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nschneid@cs.cmu.edu.
? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nasmith@cs.cmu.edu.
Submission received: 4 May 2012; revised submission received: 10 November 2012; accepted for publication:
22 December 2012.
doi:10.1162/COLI a 00163
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing consider-
able information about lexical and predicate-argument semantics in English. Grounded
in the theory of frame semantics (Fillmore 1982), it suggests?but does not formally
define?a semantic representation that blends representations familiar from word-sense
disambiguation (Ide and Ve?ronis 1998) and semantic role labeling (SRL; Gildea and
Jurafsky 2002). Given the limited size of available resources, accurately producing
richly structured frame-semantic structures with high coverage will require data-driven
techniques beyond simple supervised classification, such as latent variable modeling,
semi-supervised learning, and joint inference.
In this article, we present a computational and statistical model for frame-semantic
parsing, the problem of extracting from text semantic predicate-argument structures
such as those shown in Figure 1. We aim to predict a frame-semantic representation
with two statistical models rather than a collection of local classifiers, unlike earlier ap-
proaches (Baker, Ellsworth, and Erk 2007). We use a probabilistic framework that cleanly
integrates the FrameNet lexicon and limited available training data. The probabilistic
framework we adopt is highly amenable to future extension through new features, more
relaxed independence assumptions, and additional semi-supervised models.
Carefully constructed lexical resources and annotated data sets from FrameNet,
detailed in Section 3, form the basis of the frame structure prediction task. We de-
compose this task into three subproblems: target identification (Section 4), in which
frame-evoking predicates are marked in the sentence; frame identification (Section 5),
in which the evoked frame is selected for each predicate; and argument identification
(Section 6), in which arguments to each frame are identified and labeled with a role from
that frame. Experiments demonstrating favorable performance to the previous state of
the art on SemEval 2007 and FrameNet data sets are described in each section. Some
novel aspects of our approach include a latent-variable model (Section 5.2) and a semi-
supervised extension of the predicate lexicon (Section 5.5) to facilitate disambiguation of
words not in the FrameNet lexicon; a unified model for finding and labeling arguments
Figure 1
An example sentence from the annotations released as part of FrameNet 1.5 with three targets
marked in bold. Note that this annotation is partial because not all potential targets have been
annotated with predicate-argument structures. Each target has its evoked semantic frame
marked above it, enclosed in a distinct shape or border style. For each frame, its semantic roles
are shown enclosed within the same shape or border style, and the spans fulfilling the roles are
connected to the latter using dotted lines. For example, manner evokes the CONDUCT frame, and
has the AGENT and MANNER roles fulfilled by Austria and most un-Viennese, respectively.
10
Das et al. Frame-Semantic Parsing
(Section 6) that diverges from prior work in semantic role labeling; and an exact dual
decomposition algorithm (Section 7) that collectively predicts all the arguments of a
frame together, thereby incorporating linguistic constraints in a principled fashion.
Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Represen-
tations)1 achieves the best published results to date on the SemEval 2007 frame-semantic
structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present
results on newly released data with FrameNet 1.5, the latest edition of the lexicon.
Some of the material presented in this article has appeared in previously published
conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011)
described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a
sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the
dual decomposition algorithm for constrained joint argument identification. We present
here a synthesis of those results and several additional details:
1. The set of features used in the two statistical models for frame identification and
argument identification.
2. Details of a greedy beam search algorithm for argument identification that avoids
illegal argument overlap.
3. Error analysis pertaining to the dual decomposition argument identification algo-
rithm, in contrast with the beam search algorithm.
4. Results on full frame-semantic parsing using graph-based semi-supervised learn-
ing with sparsity-inducing penalties; this expands the small FrameNet predicate
lexicon, enabling us to handle unknown predicates.
Our primary contributions are the use of efficient structured prediction tech-
niques suited to shallow semantic parsing problems, novel methods in semi-supervised
learning that improve the lexical coverage of our parser, and making frame-semantic
structures a viable computational semantic representation usable in other language
technologies. To set the stage, we next consider related work in the automatic prediction
of predicate-argument semantic structures.
2. Related Work
In this section, we will focus on previous scientific work relevant to the problem of
frame-semantic parsing. First, we will briefly discuss work done on PropBank-style
semantic role labeling, following which we will concentrate on the more relevant prob-
lem of frame-semantic structure extraction. Next, we review previous work that has
used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior
work on joint structure prediction relevant to frame-semantic parsing.
2.1 Semantic Role Labeling
Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, there
has been a great deal of computational work using predicate-argument structures
for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed
by CoNLL shared tasks on semantic role labeling (Carreras and Ma`rquez 2004,
2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank.
PropBank annotations are closely tied to syntax, because the data set consists of the
1 See http://www.ark.cs.cmu.edu/SEMAFOR.
11
Computational Linguistics Volume 40, Number 1
(a)
(b)
Figure 2
(a) A phrase-structure tree taken from the Penn Treebank and annotated with PropBank
predicate-argument structures. The verbs created and pushed serve as predicates in this
sentence. Dotted arrows connect each predicate to its semantic arguments (bracketed phrases).
(b) A partial depiction of frame-semantic structures for the same sentence. The words in bold
are targets, which instantiate a (lemmatized and part-of-speech?tagged) lexical unit and evoke
a semantic frame. Every frame annotation is shown enclosed in a distint shape or border style,
and its argument labels are shown together on the same vertical tier below the sentence.
See text for explanation of abbreviations.
phrase-structure syntax trees from the Wall Street Journal section of the Penn Treebank
(Marcus, Marcinkiewicz, and Santorini 1993) annotated with predicate-argument
structures for verbs. In Figure 2(a), the syntax tree for the sentence is marked with
various semantic roles. The two main verbs in the sentence, created and pushed, are
the predicates. For the former, the constituent more than 1.2 million jobs serves as the
semantic role ARG1 and the constituent In that time serves as the role ARGM-TMP. Similarly
for the latter verb, roles ARG1, ARG2, ARGM-DIR, and ARGM-TMP are shown in the figure.
PropBank defines core roles ARG0 through ARG5, which receive different interpretations
for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal)
and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation
therefore has a small number of roles, and the training data set comprises some
40,000 sentences, thus making the semantic role labeling task an attractive one from the
perspective of machine learning.
There are many instances of influential work on semantic role labeling using
PropBank conventions. Pradhan et al. (2004) present a system that uses support vector
machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic
roles, followed by classification of the identified arguments to role names via a collection
of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses inte-
ger linear programming for inference and uses several global constraints to find the best
12
Das et al. Frame-Semantic Parsing
suited predicate-argument structures. Joint modeling for semantic role labeling with
discriminative log-linear models is presented by Toutanova, Haghighi, and Manning
(2005), where global features looking at all arguments of a particular verb together are
incorporated into a dynamic programming and reranking framework. The Computa-
tional Linguistics special issue on semantic role labeling (Ma`rquez et al. 2008) includes
other interesting papers on the topic, leveraging the PropBank conventions for labeling
shallow semantic structures. Recently, there have been initiatives to predict syntactic
dependencies as well as PropBank-style predicate-argument structures together using
one joint model (Surdeanu et al. 2008; Hajic? et al. 2009).
Here, we focus on the related problem of frame-semantic parsing. Note from the
annotated semantic roles for the two verbs in the sentence of Figure 2(a) that it is
unclear what the core roles ARG1 or ARG2 represent linguistically. To better understand
the roles? meaning for a given verb, one has to refer to a verb-specific file provided along
with the PropBank corpus. Although collapsing these verb-specific core roles into tags
ARG0-ARG5 leads to a small set of classes to be learned from a reasonable sized corpus,
analysis shows that the roles ARG2?ARG5 serve many different purposes for different
verbs. Yi, Loper, and Palmer (2007) point out that these four roles are highly overloaded
and inconsistent, and they mapped them to VerbNet (Schuler 2005) thematic roles to
get improvements on the SRL task. Recently, Bauer and Rambow (2011) presented
a method to improve the syntactic subcategorization patterns for FrameNet lexical
units using VerbNet. Instead of working with PropBank, we focus on shallow semantic
parsing of sentences in the paradigm of frame semantics (Fillmore 1982), to which we
turn next.
2.2 Frame-Semantic Parsing
The FrameNet lexicon (Fillmore, Johnson, and Petruck 2003) contains rich linguistic
information about lexical items and predicate-argument structures. A semantic frame
present in this lexicon includes a list of lexical units, which are associated words
and phrases that can potentially evoke it in a natural language utterance. Each frame
in the lexicon also enumerates several roles corresponding to facets of the scenario
represented by the frame. In a frame-analyzed sentence, predicates evoking frames
are known as targets, and a word or phrase filling a role is known as an argument.
Figure 2(b) shows frame-semantic annotations for the same sentence as in Figure 2(a).
(In the figure, for example, the CARDINAL NUMBERS frame, ?M? denotes the role Multiplier
and ?E? denotes the role Entity.) Note that the verbs created and pushed evoke the frames
INTENTIONALLY CREATE and CAUSE CHANGE POSITION ON A SCALE, respectively. The correspond-
ing lexical units2 from the FrameNet lexicon, create.V and push.V, are also shown.
The PropBank analysis in Figure 2(a) also has annotations for these two verbs. While
PropBank labels the roles of these verbs with its limited set of tags, the frame-
semantic parse labels each frame?s arguments with frame-specific roles shown in the
figure, making it immediately clear what those arguments mean. For example, for the
INTENTIONALLY CREATE frame, more than 1.2 million jobs is the Created entity, and In that time is
the Time when the jobs were created. FrameNet also allows non-verbal words and phrases
to evoke semantic frames: in this sentence, million evokes the frame CARDINAL NUMBERS
and doubles as its Number argument, with 1.2 as Multiplier, jobs as the Entity being quantified,
and more than as the Precision of the quantity expression.
2 See Section 5.1 for a detailed description of lexical units.
13
Computational Linguistics Volume 40, Number 1
EVENT
Place
Time
Event
TRANSITIVE_ACTION
Agent
Patient
Event
Cause
Place
TimeOBJECTIVE_INFLUENCE
Dependent_entity
uencing_situation
Place
Time
uencing_entity
CAUSE_TO_MAKE_NOISE
Agent
Sound_maker
Cause
Place
Time
MAKE_NOISE
Noisy_event
Sound
Sound_source
Place
Time
cough.v, gobble.v, 
hiss.v, ring.v, yodel.v, ...
blare.v, honk.v, play.v, 
ring.v, toot.v, ...?
affect.v, effect.n, 
impact.n, impact.v, ...
event.n, happen.v, 
occur.v, take place.v, ...
Inheritance relation Causative_of relation
Excludes relation
Purpose
Figure 3
Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame,
from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are
unfilled bars. No particular significance is ascribed to the ordering of a frame?s roles in its
lexicon entry (the selection and ordering of roles above is for illustrative convenience).
CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here.
Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) con-
tains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs,
and prepositions among its lexical units. Finally, FrameNet frames organize predicates
according to semantic principles, both by allowing related terms to evoke a common
frame (e.g., push.V, raise.V, and growth.N for CAUSE CHANGE POSITION ON A SCALE) and by
defining frames and their roles within a hierarchy (see Figure 3). PropBank does not
explicitly encode relationships among predicates.
Most early work on frame-semantic parsing has made use of the exemplar sentences
in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame
and its arguments. Gildea and Jurafsky (2002) presented a discriminative model for
arguments given the frame; Thompson, Levy, and Manning (2003) used a generative
model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first
used maximum entropy models to find and label arguments given the frame. Shi and
Mihalcea (2004) developed a rule-based system to predict frames and their arguments
in text, and Erk and Pado? (2006) introduced the Shalmaneser tool, which uses naive
Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti
2006, for instance) have used SVMs. Most of this work was done on an older, smaller
version of FrameNet, containing around 300 frames and fewer than 500 unique semantic
roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared
task data set, and also the newer FrameNet 1.5,3 which lists 877 frames and 1,068 role
types?thus handling many more labels, and resulting in richer frame-semantic parses.
Recent work in frame-semantic parsing?in which sentences may contain multiple
frames which need to be recognized along with their arguments?was undertaken
as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth,
and Erk 2007). This task leveraged FrameNet 1.3, and also released a small corpus
3 Available at http://framenet.icsi.berkeley.edu as of 19 January 2013.
14
Das et al. Frame-Semantic Parsing
containing a little more than 2,000 sentences with full text annotations. The LTH system
of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the
best performance in the SemEval 2007 task in terms of full frame-semantic parsing.
Johansson and Nugues broke down the task as identifying targets that could evoke
frames in a sentence, identifying the correct semantic frame for a target, and finally
determining the arguments that fill the semantic roles of a frame. They used a series
of SVMs to classify the frames for a given target, associating unseen lexical items to
frames and identifying and classifying token spans as various semantic roles. Both
the full text annotation corpus as well as the FrameNet exemplar sentences were
used to train their models. Unlike Johansson and Nugues, we use only the full text
annotated sentences as training data, model the whole problem with only two statis-
tical models, and obtain significantly better overall parsing scores. We also model the
argument identification problem using a joint structure prediction model and use semi-
supervised learning to improve predicate coverage. We also present experiments on
recently released FrameNet 1.5 data.
In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) in-
vestigated various uses of FrameNet?s taxonomic relations for learning generalizations
over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features
for the subtask of argument identification. Another line of work has sought to extend
the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea
2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries
and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado
and Lapata 2005; Fu?rstenau and Lapata 2009b). Others have explored the application
of frame-semantic structures to tasks such as information extraction (Moschitti,
Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt
and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu
2004; Shen and Lapata 2007), and paraphrase recognition (Pado? and Erk 2005).
2.3 Semi-Supervised Methods
Although there has been a significant amount of work in supervised shallow semantic
parsing using both PropBank- and FrameNet-style representations, a few improve-
ments over vanilla supervised methods using unlabeled data are notable. Fu?rstenau and
Lapata (2009b) present a method of projecting predicate-argument structures from some
seed examples to unlabeled sentences, and use a linear program formulation to find
the best alignment explaining the projection. Next, the projected information as well
as the seeds are used to train statistical model(s) for SRL. The authors ran experiments
using a set of randomly chosen verbs from the exemplar sentences of FrameNet and
found improvements over supervised methods. In an extension to this work, Fu?rstenau
and Lapata (2009a) present a method for finding examples for unseen verbs using a
graph alignment method; this method represents sentences and their syntactic analysis
as graphs and graph alignment is used to project annotations from seed examples to
unlabeled sentences. This alignment problem is again modeled as a linear program.
Fu?rstenau and Lapata (2012) present an detailed expansion of the aforementioned
papers. Although this line of work presents a novel direction in the area of SRL, the
published approach does not yet deal with non-verbal predicates and does not evaluate
the presented methods on the full text annotations of the FrameNet releases.
Deschacht and Moens (2009) present a technique of incorporating additional infor-
mation from unlabeled data by using a latent words language model. Latent variables
are used to model the underlying representation of words, and parameters of this model
15
Computational Linguistics Volume 40, Number 1
are estimated using standard unsupervised methods. Next, the latent information is
used as features for an SRL model. Improvements over supervised SRL techniques
are observed with the augmentation of these extra features. The authors also compare
their method with the aforementioned two methods of Fu?rstenau and Lapata (2009a,
2009b) and show relative improvements. Experiments are performed on the CoNLL
2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conven-
tions and only labels verbal and nominal predicates?in contrast to our work, which
includes most lexicosyntactic categories. A similar approach is presented by Weston,
Ratle, and Collobert (2008), who use neural embeddings of words, which are eventu-
ally used for SRL; improvements over state-of-the-art PropBank-style SRL systems are
observed.
Recently, there has been related work in unsupervised semantic role labeling (Lang
and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic
roles automatically from unannotated data. This line of work may be useful in discov-
ering new semantic frames and roles, but here we stick to the concrete representation
provided in FrameNet, without seeking to expand its inventory of semantic types. We
present a new semi-supervised technique to expand the set of lexical items with the
potential semantic frames that they could evoke; we use a graph-based semi-supervised
learning framework to achieve this goal (Section 5.5).
2.4 Joint Inference and Shallow Semantic Parsing
Most high-performance SRL systems that use conventions from PropBank (Kingsbury
and Palmer 2002) and NomBank (Meyers et al. 2004) utilize joint inference for seman-
tic role labeling (Ma`rquez et al. 2008). To our knowledge, the separate line of work
investigating frame-semantic parsing has not previously dealt with joint inference. A
common trait in prior work, both in PropBank and FrameNet conventions, has been
the use of a two-stage model that identifies arguments first, then labels them, often
using dynamic programming or integer linear programs (ILPs); we treat both problems
together here.4
Recent work in natural language processing (NLP) problems has focused on ILP for-
mulations for complex structure prediction tasks like dependency parsing (Riedel and
Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth
and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work
in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush
et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian
relaxation) as a way of exploiting the structure of the problem and existing combina-
torial algorithms. The method allows the combination of models that are individually
tractable, but not jointly tractable, by solving a relaxation of the original problem. Since
then, dual decomposition has been used to build more accurate models for dependency
parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing
(Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and
Macherey 2011; Rush and Collins 2011).
Recently, Martins et al. (2011b) showed that the success of subgradient-based dual
decomposition strongly relies on breaking down the original problem into a ?good?
4 In prior work, there are exceptions where identification and classification of arguments have been treated
in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on
semantic role labeling (Carreras and Ma`rquez 2004).
16
Das et al. Frame-Semantic Parsing
decomposition, that is, one with few overlapping components. This leaves out many
declarative constrained problems, for which such a good decomposition is not readily
available. For those, Martins et al. proposed the Alternating Directions Dual Decom-
position (AD3) algorithm, which retains the modularity of previous methods, but can
handle thousands of small overlapping components. We adopt that algorithm as it
perfectly suits the problem of argument identification, as we observe in Section 7.5 We
also contribute an exact branch-and-bound technique wrapped around AD3.
Before delving into the details of our modeling framework, we describe in detail the
structure of the FrameNet lexicon and the data sets used to train our models.
3. Resources and Task
We consider frame-semantic parsing resources consisting of a lexicon and annotated
sentences with frame-semantic structures, evaluation strategies, and previous baselines.
3.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manually identified general-purpose semantic
frames for English.6 Listed in the lexicon with each frame are a set of lemmas (with
parts of speech) that can denote the frame or some aspect of it?these are called lexical
units (LUs). In a sentence, word or phrase tokens that evoke a frame are known as
targets. The set of LUs listed for a frame in FrameNet may not be exhaustive; we may
see a target in new data that does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame elements, or roles, corresponding
to different aspects of the concept represented by the frame, such as participants,
props, and attributes. We use the term argument to refer to a sequence of word tokens
annotated as filling a frame role. Figure 1 shows an example sentence from the training
data with annotated targets, LUs, frames, and role-argument pairs. The FrameNet
lexicon also provides information about relations between frames and between roles
(e.g., INHERITANCE). Figure 3 shows a subset of the relations between five frames and
their roles.
Accompanying most frame definitions in the FrameNet lexicon is a set of lexico-
graphic exemplar sentences (primarily from the British National Corpus) annotated
for that frame. Typically chosen to illustrate variation in argument realization pat-
terns for the frame in question, these sentences only contain annotations for a single
frame.
In preliminary experiments, we found that using exemplar sentences directly to
train our models hurt performance as evaluated on SemEval 2007 data, which formed
a benchmark for comparison with previous state of the art. This was a noteworthy
observation, given that the number of exemplar sentences is an order of magnitude
larger than the number of sentences in training data that we consider in our experiments
(Section 3.2). This is presumably because the exemplars are not representative as a
sample, do not have complete annotations, and are not from a domain similar to the
5 AD3 was previously referred to as ?DD-ADMM,? in reference to the use of dual decomposition with the
alternating directions method of multipliers.
6 Like the SemEval 2007 participants, we used FrameNet 1.3 and also the newer version of the lexicon,
FrameNet 1.5 (http://framenet.icsi.berkeley.edu).
17
Computational Linguistics Volume 40, Number 1
Table 1
Salient statistics of the data sets used in our experiments. There is a significant overlap between
the two data sets.
SemEval 2007 Data FrameNet 1.5 Release
count count
Exemplar sentences 139,439 154,607
Frame labels (types) 665 877
Role labels (types) 720 1,068
Sentences in training data 2,198 3,256
Targets in training data 11,195 19,582
Sentences in test data 120 2,420
Targets in test data 1,059 4,458
Unseen targets in test data 210 144
test data. Instead, we make use of these exemplars in the construction of features
(Section 5.2).
3.2 Data
In our experiments on frame-semantic parsing, we use two sets of data:
1. SemEval 2007 data: In benchmark experiments for comparison with previous
state of the art, we use a data set that was released as part of the SemEval 2007
shared task on frame-semantic structure extraction (Baker, Ellsworth, and Erk 2007).
Full text annotations in this data set consisted of a few thousand sentences con-
taining multiple targets, each annotated with a frame and its arguments. The then-
current version of the lexicon (FrameNet 1.3) was used for the shared task as the
inventory of frames, roles, and lexical units (Figure 3 illustrates a small portion
of the lexicon). In addition to the frame hierarchy, FrameNet 1.3 also contained
139,439 exemplar sentences containing one target each. Statistics of the data used
for the SemEval 2007 shared task are given in the first column of Table 1. A total
of 665 frame types and 720 role types appear in the exemplars and the training
portion of the data. We adopted the same training and test split as the SemEval
2007 shared task; however, we removed four documents from the training set7 for
development. Table 2 shows some additional information about the SemEval data
set; the variety of lexicosyntactic categories of targets stands in marked contrast
with the PropBank-style SRL data and task.
2. FrameNet 1.5 release: A more recent version of the FrameNet lexicon was released
in 2010.8 We also test our statistical models (only frame identification and argu-
ment identification) on this data set to get an estimate of how much improvement
additional data can provide. Details of this data set are shown in the second col-
umn of Table 1. Of the 78 documents in this release with full text annotations, we
selected 55 (19,582 targets) for training and held out the remaining 23 (4,458 tar-
gets) for testing. There are fewer target annotations per sentence in the test set than
7 These were: StephanopoulousCrimes, Iran Biological, NorthKorea Introduction, and WMDNews 042106.
8 Released on 15 September 2010, and downloadable from http://framenet.icsi.berkeley.edu as of
13 February 2013. In our experiments, we used a version downloaded on 22 September 2010.
18
Das et al. Frame-Semantic Parsing
Table 2
Breakdown of targets and arguments in the SemEval 2007 training set in terms of part of speech.
The target POS is based on the LU annotation for the frame instance. For arguments, this reflects
the part of speech of the head word (estimated from an automatic dependency parse); the
percentage is out of all overt arguments.
targets arguments
count % count %
Noun 5,155 52 Noun 9,439 55
Verb 2,785 28 Preposition or
complementizerAdjective 1,411 14 2,553 15
Preposition 296 3 Adjective 1,744 10
Adverb 103 1 Verb 1,156 7
Number 63 1 Pronoun 736 4
Conjunction 8 Adverb 373 2
Article 3 Other 1,047 6
9,824 17,048
the training set.9 Das and Smith (2011, supplementary material) give the names
of the test documents for fair replication of our work. We also randomly selected
4,462 targets from the training data for development of the argument identification
model (Section 6.1).
Preprocessing. We preprocessed sentences in our data set with a standard set of anno-
tations: POS tags from MXPOST (Ratnaparkhi 1996) and dependency parses from the
MST parser (McDonald, Crammer, and Pereira 2005); manual syntactic parses are not
available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum
1998) for lemmatization. Our models treat these pieces of information as observations.
We also labeled each verb in the data as having ACTIVE or PASSIVE voice, using code
from the SRL system described by Johansson and Nugues (2008).
3.3 Task and Evaluation Methodology
Automatic annotations of frame-semantic structure can be broken into three parts:
(1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the
lexicon, evoked by each target; and (3) the arguments, or spans of words that serve
to fill roles defined by each evoked frame. These correspond to the three subtasks
in our parser, each described and evaluated in turn: target identification (Section 4),
frame identification (Section 5, not unlike word-sense disambiguation), and argument
identification (Section 6, essentially the same as semantic role labeling).
The standard evaluation script from the SemEval 2007 shared task calculates pre-
cision, recall, and F1-measure for frames and arguments; it also provides a score that
gives partial credit for hypothesizing a frame related to the correct one. We present
9 For creating the splits, we first included the documents that had incomplete annotations as mentioned in
the initial FrameNet 1.5 data release in the test set; because we do not evaluate target identification for
this version of data, the small number of targets per sentence does not matter. After these documents
were put into the test set, we randomly selected 55 remaining documents for training, and picked the
rest for additional testing. The final test set contains a total of 23 documents. When these documents
are annotated in their entirety, the test set will become larger and the training set will be unaltered.
19
Computational Linguistics Volume 40, Number 1
precision, recall, and F1-measure microaveraged across the test documents, report labels-
only matching scores (spans must match exactly), and do not use named entity labels.10
More details can be found in the task description paper from SemEval 2007 (Baker,
Ellsworth, and Erk 2007). For our experiments, statistical significance is measured using
a reimplementation of Dan Bikel?s randomized parsing evaluation comparator, a strat-
ified shuffling test whose original implementation11 is accompanied by the following
description (quoted verbatim, with explanations of our use of the test given in square
brackets):
The null hypothesis is that the two models that produced the observed results are the
same, such that for each test instance [here, a set of predicate-argument structures for a
sentence], the two observed scores are equally likely. This null hypothesis is tested by
randomly shuffling individual sentences? scores between the two models and then
re-computing the evaluation metrics [precision, recall or F1 score in our case]. If the
difference in a particular metric after a shuffling is equal to or greater than the original
observed difference in that metric, then a counter for that metric is incremented. Ideally,
one would perform all 2n shuffles, where n is the number of test cases (sentences), but
given that this is often prohibitively expensive, the default number of iterations is
10,000 [we use independently sampled 10,000 shuffles]. After all iterations, the
likelihood of incorrectly rejecting the null [hypothesis, i.e., the p-value] is simply
(nc + 1)/(nt + 1), where nc is the number of random differences greater than the
original observed difference, and nt is the total number of iterations.
3.4 Baseline
A strong baseline for frame-semantic parsing is the system presented by Johansson and
Nugues (2007, hereafter J&N?07), the best system in the SemEval 2007 shared task. That
system is based on a collection of SVMs. They used a set of rules for target identification
which we describe in Appendix A. For frame identification, they used an SVM classifier
to disambiguate frames for known frame-evoking words. They used WordNet synsets
to extend the vocabulary of frame-evoking words to cover unknown words, and then
used a collection of separate SVM classifiers?one for each frame?to predict a single
evoked frame for each occurrence of a word in the extended set.
J&N?07 followed Xue and Palmer (2004) in dividing the argument identification
problem into two subtasks: First, they classified candidate spans as to whether they
were arguments or not; then they assigned roles to those that were identified as ar-
guments. Both phases used SVMs. Thus, their formulation of the problem involves
a multitude of independently trained classifiers that share no information?whereas
ours uses two log-linear models, each with a single set of parameters shared across all
contexts, to find a full frame-semantic parse.
We compare our models with J&N?07 using the benchmark data set from SemEval
2007. However, because we are not aware of any other work using the FrameNet 1.5 full
text annotations, we report our results on that data set without comparison to any other
system.
10 For microaveraging, we concatenated all sentences of the test documents and measured precision and
recall over the concatenation. Macroaveraging, on the other hand, would mean calculating these metrics
for each document, then averaging them. Microaveraging treats every frame or argument as a unit,
regardless of the length of the document in which it occurs.
11 See http://www.cis.upenn.edu/dbikel/software.html#comparator.
20
Das et al. Frame-Semantic Parsing
4. Target Identification
Target identification is the problem of deciding which word tokens (or word token
sequences) evoke frames in a given sentence. In other semantic role labeling schemes
(e.g., PropBank), simple part-of-speech criteria typically distinguish targets from non-
targets. But in frame semantics, verbs, nouns, adjectives, and even prepositions can
evoke frames under certain conditions. One complication is that semantically impov-
erished support predicates (such as make in make a request) do not evoke frames in the
context of a frame-evoking, syntactically dependent noun (request). Furthermore, only
temporal, locative, and directional senses of prepositions evoke frames.12
Preliminary experiments using a statistical method for target identification gave
unsatisfactory results; instead, we followed J&N?07 in using a small set of rules to
identify targets. First, we created a master list of all the morphological variants of
targets that appear in the exemplar sentences and a given training set. For a sentence in
new data, we considered as candidate targets only those substrings that appear in this
master list. We also did not attempt to capture discontinuous frame targets: for example,
we treat there would have been as a single span even though the corresponding LU is
there be.V.13
Next, we pruned the candidate target set by applying a series of rules identical
to the ones described by Johansson and Nugues (2007, see Appendix A), with two
exceptions. First, they identified locative, temporal, and directional prepositions using
a dependency parser so as to retain them as valid LUs. In contrast, we pruned all types
of prepositions because we found them to hurt our performance on the development
set due to errors in syntactic parsing. In a second departure from their target extraction
rules, we did not remove the candidate targets that had been tagged as support verbs
for some other target. Note that we used a conservative white list that filters out targets
whose morphological variants were not seen either in the lexicon or the training data.14
Therefore, when this conservative process of automatic target identification is used, our
system loses the capability to predict frames for completely unseen LUs, despite the fact
that our powerful frame identification model (Section 5) can accurately label frames for
new LUs.15
Results. Table 3 shows results on target identification tested on the SemEval 2007 test
set; our system gains 3 F1 points over the baseline. This is statistically significant with
p < 0.01. Our results are also significant in terms of precision (p < 0.05) and recall (p <
0.01). There are 85 distinct LUs for which the baseline fails to identify the correct target
while our system succeeds. A considerable proportion of these units have more than
12 Note that there have been dedicated shared tasks to determine relationships between nominals (Girju
et al. 2007) and word-sense disambiguation of prepositions (Litkowski and Hargraves 2007), but we do
not build specific models for predicates of these categories.
13 There are 629 multiword LUs in the lexicon, and they correspond to 4.8% of the targets in the training
set; among them are screw up.V, shoot the breeze.V, and weapon of mass destruction.N. In the SemEval 2007
training data, there are just 99 discontinuous multiword targets (1% of all targets).
14 This conservative approach violates theoretical linguistic assumptions about frame-evoking targets as
governed by frame semantics. It also goes against the spirit of using linguistic constraints to improve
the separate subtask of argument identification (see Section 7); however, due to varying distributions
of target annotations, high empirical error in identifying locative, temporal, and directional prepositions,
and support verbs, we resorted to this aggressive filtering heuristic to avoid making too many target
identification mistakes.
15 To predict frames and roles for new and unseen LUs, SEMAFOR provides the user with an option to
mark those LUs in the input.
21
Computational Linguistics Volume 40, Number 1
Table 3
Target identification results for our system and the baseline on the SemEval?07 data set. Scores in
bold denote significant improvements over the baseline (p < 0.05).
TARGET IDENTIFICATION P R F1
Our technique (?4) 89.92 70.79 79.21
Baseline: J&N?07 87.87 67.11 76.10
one token (e.g., chemical and biological weapon.N, ballistic missile.N), which J&N?07 do not
model. The baseline also does not label variants of there be.V (e.g., there are and there has
been), which we correctly label as targets. Some examples of other single token LUs that
the baseline fails to identify are names of months, LUs that belong to the ORIGIN frame
(e.g., iranian.A), and directions (e.g., north.A or north-south.A).16
5. Frame Identification
Given targets, our parser next identifies their frames, using a statistical model.
5.1 Lexical Units
FrameNet specifies a great deal of structural information both within and among
frames. For frame identification we make use of frame-evoking lexical units, the (lem-
matized and POS-tagged) words and phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING frame are 10 LUs, including boast.N,
boast.V, boastful.A, brag.V, and braggart.N. Of course, due to polysemy and homonymy,
the same LU may be associated with multiple frames; for example, gobble.V is listed
under both the INGESTION and MAKE NOISE frames. We thus term gobble.V an ambiguous
LU. All targets in the exemplar sentences, our training data, and most in our test data,
correspond to known LUs. (See Section 5.4 for statistics of unknown LUs in the test sets.)
To incorporate frame-evoking expressions found in the training data but not the
lexicon?and to avoid the possibility of lemmatization errors?our frame identification
model will incorporate, via a latent variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of (unlemmatized and automati-
cally POS-tagged) targets found in the exemplar sentences of the lexicon and/or the
sentences in our training set. Let Lf ? L be the subset of these targets annotated as
evoking a particular frame f .17 Let Ll and Llf denote the lemmatized versions of L and
Lf , respectively. Then, we write boasted.VBD ? LBRAGGING and boast.VBD ? LlBRAGGINGto
indicate that this inflected verb boasted and its lemma boast have been seen to evoke the
BRAGGING frame. Significantly, however, another target, such as toot your own horn, might
be used elsewhere to evoke this frame. We thus face the additional hurdle of predicting
frames for unknown words.
16 We do not evaluate the target identification module on the FrameNet 1.5 data set; we instead ran
controlled experiments on those data to measure performance of the statistical frame identification and
argument identification subtasks, assuming that the correct targets were given. Moreover, as discussed
in Section 3.2, the target annotations on the FrameNet 1.5 test set were fewer in number in comparison
to the training set, resulting in a mismatch of target distributions between train and test settings.
17 For example, on average, there are 34 targets per frame in the SemEval 2007 data set; the average frame
ambiguity of each target in L is 1.17.
22
Das et al. Frame-Semantic Parsing
In producing full text annotations for the SemEval 2007 data set, annotators created
several domain-critical frames that were not already present in version 1.3 of the lexicon.
For our experiments we omit frames attested in neither the training data nor the exem-
plar sentences from the lexicon.18 This leaves a total of 665 frames for the SemEval 2007
data set and a total of 877 frames for the FrameNet 1.5 data set.
5.2 Model
For a given sentence x with frame-evoking targets t, let ti denote the ith target (a word
sequence).19 Let tli denote its lemma. We seek a list f = ?f1, . . . , fm? of frames, one per
target. In our model, the set of candidate frames for ti is defined to include every frame
f such that tli ? Llf ?or if tli ? Ll, then every known frame (the latter condition applies
for 4.7% of the annotated targets in the SemEval 2007 development set). In both cases,
we let Fi be the set of candidate frames for the ith target in x. We denote the entire set
of frames in the lexicon as F .
To allow frame identification for targets whose lemmas were seen in neither the
exemplars nor the training data, our model includes an additional variable, i. This
variable ranges over the seen targets in Lfi , which can be thought of as prototypes
for the expression of the frame. Importantly, frames are predicted, but prototypes are
summed over via the latent variable. The prediction rule requires a probabilistic model
over frames for a target:
fi ? argmax
f?Fi
?
?Lf
p?(f,  | ti, x) (1)
We model the probability of a frame f and the prototype unit , given the target and the
sentence x as:
p?(f,  | ti, x) =
exp?g(f, , ti, x)
?
f ??F
?
??Lf?
exp?g(f ?, ?, ti, x)
(2)
This is a conditional log-linear model: for f ? Fi and  ? Lf , where ? are the model
weights, and g is a vector-valued feature function. This discriminative formulation is
very flexible, allowing for a variety of (possibly overlapping) features; for example, a
feature might relate a frame type to a prototype, represent a lexical-semantic relation-
ship between a prototype and a target, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better coverage during frame identifica-
tion (Burchardt, Erk, and Frank 2005; Johansson and Nugues 2007, e.g., by expanding
the set of targets using synsets), and others have sought to extend the lexicon itself.
We differ in our use of a latent variable to incorporate lexical-semantic features in a
discriminative model, relating known lexical units to unknown words that may evoke
frames. Here we are able to take advantage of the large inventory of partially annotated
18 Automatically predicting new frames is a challenge not yet attempted to our knowledge (including here).
Note that the scoring metric (Section 3.3) gives partial credit for related frames (e.g., a more general frame
from the lexicon).
19 Here each ti is a word sequence ?wu, . . . , wv?, 1 ? u ? v ? n, though in principle targets can be
noncontiguous.
23
Computational Linguistics Volume 40, Number 1
Table 4
Features used for frame identification (Equation (2)). All also incorporate f , the frame being
scored.  = ?w,?? consists of the words and POS tags20 of a target seen in an exemplar or
training sentence as evoking f . The features with starred bullets were also used by Johansson
and Nugues (2007).
? the POS of the parent of the head word of ti
?? the set of syntactic dependencies of the head word21 of ti
?? if the head word of ti is a verb, then the set of dependency labels of its children
? the dependency label on the edge connecting the head of ti and its parent
? the sequence of words in the prototype, w
? the lemmatized sequence of words in the prototype
? the lemmatized sequence of words in the prototype and their part-of-speech tags ?
? WordNet relation22 ? holds between  and ti
? WordNet relation22 ? holds between  and ti, and the prototype is 
? WordNet relation22 ? holds between  and ti, the POS tag sequence of  is ?, and the POS
tag sequence of ti is ?t
exemplar sentences. Note that this model makes an independence assumption: Each
frame is predicted independently of all others in the document. In this way the model
is similar to J&N?07. However, ours is a single conditional model that shares features
and weights across all targets, frames, and prototypes, whereas the approach of J&N?07
consists of many separately trained models. Moreover, our model is unique in that it
uses a latent variable to smooth over frames for unknown or ambiguous LUs.
Frame identification features depend on the preprocessed sentence x, the prototype
 and its WordNet lexical-semantic relationship with the target ti, and of course the
frame f . Our model uses binary features, which are detailed in Table 4.
5.3 Parameter Estimation
Given a training data set (either SemEval 2007 data set or the FrameNet 1.5 full text
annotations), which is of the form ??x(j), t(j), f(j),A(j)??Nj=1, we discriminatively train the
frame identification model by maximizing the training data log-likelihood:23
max
?
N
?
j=1
mj
?
i=1
log
?
?L
f ( j)i
p?( f
(j)
i ,  | t
(j)
i , x
(j) ) (3)
In Equation (3), mj denotes the number of frames in a sentence indexed by j. Note
that the training problem is non-convex because of the summed-out prototype latent
20 POS tags are found automatically during preprocessing.
21 If the target is not a subtree in the parse, we consider the words that have parents outside the span,
and apply three heuristic rules to select the head: (1) choose the first word if it is a verb; (2) choose the
last word if the first word is an adjective; (3) if the target contains the word of, and the first word is a
noun, we choose it. If none of these hold, choose the last word with an external parent to be the head.
22 These are: IDENTICAL-WORD, SYNONYM, ANTONYM (including extended and indirect antonyms),
HYPERNYM, HYPONYM, DERIVED FORM, MORPHOLOGICAL VARIANT (e.g., plural form), VERB
GROUP, ENTAILMENT, ENTAILED-BY, SEE-ALSO, CAUSAL RELATION, and NO RELATION.
23 We found no benefit on either development data set from using an L2 regularizer (zero-mean
Gaussian prior).
24
Das et al. Frame-Semantic Parsing
Table 5
Frame identification results on both the SemEval 2007 data set and the FrameNet 1.5 release.
Precision, recall, and F1 were evaluated under exact and partial frame matching; see Section 3.3.
Bold indicates best results on the SemEval 2007 data, which are also statistically significant with
respect to the baseline (p < 0.05).
FRAME IDENTIFICATION (?5.2) exact matching partial matchingP R F1 P R F1
SemEval 2007 Data
gold targets 60.21 60.21 60.21 74.21 74.21 74.21
automatic targets (?4) 69.75 54.91 61.44 77.51 61.03 68.29
J&N?07 targets 65.34 49.91 56.59 74.30 56.74 64.34
Baseline: J&N?07 66.22 50.57 57.34 73.86 56.41 63.97
FrameNet 1.5 Release
gold targets 82.97 82.97 82.97 90.51 90.51 90.51
? unsupported features 80.30 80.30 80.30 88.91 88.91 88.91
& ? latent variable 75.54 75.54 75.54 85.92 85.92 85.92
variable  for each frame. To calculate the objective function, we need to cope with a
sum over frames and prototypes for each target (see Equation (2)), often an expensive
operation. We locally optimize the function using a distributed implementation of L-
BFGS.24 This is the most expensive model that we train: With 100 parallelized CPUs
using MapReduce (Dean and Ghemawat 2008), training takes several hours.25 Decoding
takes only a few minutes on one CPU for the test set.
5.4 Supervised Results
SemEval 2007 Data. On the SemEval 2007 data set, we evaluate the performance of
our frame identification model given gold-standard targets and automatically identified
targets (Section 4); see Table 5. Together, our target and frame identification outperform
the baseline by 4 F1 points. To compare the frame identification stage in isolation with
that of J&N?07, we ran our frame identification model with the targets identified by their
system as input. With partial matching, our model achieves a relative improvement of
0.6% F1 over J&N?07, as shown in the third row of Table 5 (though this is not significant).
Note that for exact matching, the F1 score of the automatic targets setting is better than
the gold target setting. This is due to the fact that there are many unseen predicates in
the test set on which the frame identification model performs poorly; however, for the
automatic targets that are mostly seen in the lexicon and training data, the model gets
high precision, resulting in better overall F1 score.
Our frame identification model thus performs on par with the previous state of the
art for this task, and offers several advantages over J&N?s formulation of the problem:
It requires only a single model, learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand the vocabulary of frame-evoking
words, and is probabilistic, which can facilitate global reasoning.
24 We do not experiment with the initialization of model parameters during this non-convex optimization
process; all parameters are initialized to 0.0 before running the optimizer. However, in future work,
experiments can be conducted with different random initialization points to seek non-local optima.
25 In later experiments, we used another implementation with 128 parallel cores in a multi-core MPI setup
(Gropp, Lusk, and Skjellum 1994), where training took several hours.
25
Computational Linguistics Volume 40, Number 1
In the SemEval 2007 data set, for gold-standard targets, 210 out of 1,059 lemmas
were not present in the white list that we used for target identification (see Section 4).
Our model correctly identifies the frames for 4 of these 210 lemmas. For 44 of these
lemmas, the evaluation script assigns a score of 0.5 or more, suggesting that our model
predicts a closely related frame. Finally, for 190 of the 210 lemmas, a positive score is
assigned by the evaluation script. This suggests that the hidden variable model helps
in identifying related (but rarely exact) frames for unseen targets, and explains why
under exact?but not partial?frame matching, the F1 score using automatic targets is
commensurate with the score for oracle targets.26
For automatically identified targets, the F1 score falls because the model fails
to predict frames for unseen lemmas. However, our model outperforms J&N?07 by
4 F1 points. The partial frame matching F1 score of our model represents a significant
improvement over the baseline (p < 0.01). The precision and recall measures are
significant as well (p < 0.05 and p < 0.01, respectively). However, because targets
identified by J&N?07 and frames classified by our frame identification model resulted
in scores on par with the baseline, we note that the significant results follow due to
better target identification. Note from the results that the automatic target identification
model shows an increase in precision, at the expense of recall. This is because the white
list for target identification restricts the model to predict frames only for known LUs.
If we label the subset of test set with already seen LUs (seen only in the training set,
excluding the exemplars) with their corresponding most frequent frame, we achieve
an exact match accuracy between 52.9% and 91.2%, depending on the accuracy of the
unseen LUs (these bounds assume, respectively, that they are all incorrectly labeled or
all correctly labeled).
FrameNet 1.5 Release. The bottom three rows of Table 5 show results on the full text
annotation test set of the FrameNet 1.5 release. Because the number of annotations
nearly doubled, we see large improvements in frame identification accuracy. Note that
we only evaluate with gold targets as input to frame identification. (As mentioned in
Section 3.2, some documents in the test set have not been annotated for all targets, so
evaluating automatic target identification would not be informative.) We found that
50.1% of the targets in the test set were ambiguous (i.e., they associated with more than
one frame either in FrameNet or our training data). On these targets, the exact frame
identification accuracy is 73.10% and the partial accuracy is 85.77%, which indicates that
the frame identification model is robust to target ambiguity. On this data set, the most
frequent frame baseline achieves an exact match accuracy between 74.0% and 88.1%,
depending on the accuracy of the unseen LUs.
We conducted further experiments with ablation of the latent variable in our frame
identification model. Recall that the decoding objective used to choose a frame by
marginalizing over a latent variable , whose values range over targets known to
associate with the frame f being considered (see Equations (1) and (2)) in training. How
much do the prototypes, captured by the latent variable, contribute to performance?
Instead of treating  as a marginalized latent variable, we can fix its value to the observed
target.
26 J&N?07 did not report frame identification results for oracle targets; thus directly comparing the frame
identification models is difficult.
26
Das et al. Frame-Semantic Parsing
An immediate effect of this choice is a blow-up in the number of features; we
must instantiate features (see Table 4) for all 4,194 unique targets observed in training.
Because each of these features needs to be associated with all 877 frames in the partition
function of Equation (2), the result is an 80-fold blowup of the feature space (the latent
variable model had 465,317 features). Such a model is not computationally feasible in
our engineering framework, so we considered a model using only features observed to
fire at some point in the training data (called ?supported? features),27 resulting in only
72,058 supported features. In Table 5, we see a significant performance drop (on both
exact and partial matching accuracy) with this latent variable?free model, compared
both with our latent variable model with all features and with only supported features
(of which there are 165,200). This establishes that the latent variable in our frame
identification model helps in terms of accuracy, and lets us use a moderately sized
feature set incorporating helpful unsupported features.
Finally, in our test set, we found that 144 out of the 4,458 annotated targets were un-
seen, and our full frame identification model only labeled 23.1% of the frames correctly
for those unseen targets; in terms of partial match accuracy, the model achieved a score
of 46.6%. This, along with the results on the SemEval 2007 unseen targets, shows that
there is substantial opportunity for improvement when unseen targets are presented to
the system. We address this issue next.
5.5 Semi-Supervised Lexicon Expansion
We next address the poor performance of our frame identification model on targets that
were unseen as LUs in FrameNet or as instances in training data, and briefly describe
a technique for expanding the set of lexical units with potential semantic frames that
they can associate with. These experiments were carried out on the FrameNet 1.5 data
only. We use a semi-supervised learning (SSL) technique that uses a graph constructed
from labeled and unlabeled data. The widely used graph-based SSL framework?see
Bengio, Delalleau, and Le Roux (2006) and Zhu (2008) for introductory material on this
topic?has been shown to perform better than several other semi-supervised algorithms
on benchmark data sets (Chapelle, Scho?lkopf, and Zien 2006, chapter 21). The method
constructs a graph where a small portion of vertices correspond to labeled instances,
and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting
the similarity between the pair. Traditionally, Markov random walks (Szummer and
Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness
properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and
Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from
the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class
generalizations of graph-propagation algorithms suitable for NLP applications, where
each graph vertex can assume one or more out of many possible labels (Subramanya and
Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to
natural language types (not tokens) and undirected edges between them are weighted
using a similarity metric. Recently, this set-up has been used to learn soft labels on
natural language types (say, word n-grams or in our case, syntactically disambiguated
27 The use of unsupported features (i.e., those that can fire for an analysis in the partition function but not
observed to fire in the training data) has been observed to give performance improvements in NLP
problems; see, for example, Sha and Pereira (2003) and Martins et al. (2010).
27
Computational Linguistics Volume 40, Number 1
predicates) from seed data, resulting in large but noisy lexicons, which are used to
constrain structured prediction models. Applications have ranged from domain adap-
tation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised
learning of POS taggers by using bilingual graph-based projections (Das and Petrov
2011).
We describe our approach to graph construction, propagation for lexicon expansion,
and the use of the result to impose constraints on frame identification.
5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each
vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag.
We use two resources for graph construction. First, we take all the words and phrases
present in a dependency-based thesaurus constructed using syntactic cooccurrence
statistics (Lin 1998), and aggregate words and phrases that share the same lemma and
coarse POS tag. To construct this resource, Lin used a corpus containing 64 million
words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic
contexts were used to find similar lexical items for a given word or phrase. Lin sepa-
rately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the
thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs
present in FrameNet data.
The second component of graph construction comes from FrameNet itself. We
scanned the exemplar sentences in FrameNet 1.5 and the training section of the full
text annotations and gathered a distribution over frames for each LU appearing in
FrameNet data. For a pair of LUs, we measured the Euclidean distance between their
frame distributions. This distance was next converted to a similarity score and inter-
polated with the similarity score from Lin?s dependency thesaurus. We omit further
details about the interpolation and refer the reader to full details given in Das and Smith
(2011).
For each LU, we create a vertex and link it to the K nearest neighbor LUs under the
interpolated similarity metric. The resulting graph has 64,480 vertices, 9,263 of which
are labeled seeds from FrameNet 1.5 and 55,217 of which are unlabeled. Each vertex has
a possible set of labels corresponding to the 877 frames defined in the lexicon. Figure 4
shows an excerpt from the constructed graph.
Figure 4
Excerpt from our constructed graph over LUs. Green LUs are observed in the FrameNet 1.5 data.
Above/below them are shown the most frequently observed frame that these LUs associate
with. The black LUs are unobserved and graph propagation produces a distribution over most
likely frames that they could evoke as target instances.
28
Das et al. Frame-Semantic Parsing
5.5.2 Propagation by Optimization. Once the graph is constructed, the 9,263 seed ver-
tices with supervised frame distributions are used to propagate the semantic frame
information via their nearest neighbors to all vertices. Here we discuss two graph-
based SSL objective functions. Das and Smith (2012) compare several other graph-based
SSL algorithms for this problem; we refer the interested reader to that paper. Let V
denote the set of all vertices in our graph, V? ? V be the set of seed vertices, and F
denote the set of all frames. Let N (v) denote the set of neighbors of vertex v ? V. Let
q = {q1, q2, . . . , q|V|} be the set of frame distributions, one per vertex. For each seed
vertex v ? V?, we have a supervised frame distribution q?v. All edges in the graph are
weighted according to the aforementioned interpolated similarity score, denoted wuv
for the edge adjacent to vertices u and v. We find q by solving:
NGF-2 : arg min
q, s.t. q?0,
?v?V,?qv?1=1
?
v?V?
?q?v ? qv?22 + ?
?
v?V,u?N (v)
wuv?qv ? qu?22 + ?
?
v?V
?qv ? 1|F|?22
(4)
We call the objective in Equation (4) NGF-2 because it uses normalized probability dis-
tributions at each vertex and is a Gaussian field; it also utilizes a uniform 2 penalty?the
third term in the objective function. This is a multiclass generalization of the quadratic
cost criterion (Bengio, Delalleau, and Le Roux 2006), also used by Subramanya, Petrov,
and Pereira (2010) and Das and Petrov (2011). Our second graph objective function is as
follows:
UJSF-1,2 : arg min
q, s.t. q?0
?
v?V?
DJS(q?v?qv) + ?
?
v?V,u?N (v)
wuvDJS(qv?qu) + ?
?
v?V
?qv?21 (5)
We call it UJSF-1,2 because it uses unnormalized probability measures at each vertex
and is a Jensen-Shannon field, utilizing pairwise Jensen-Shannon divergences (Lin 1991;
Burbea and Rao 2006) and a sparse 1,2 penalty (Kowalski and Torre?sani 2009) as the
third term. Das and Smith (2012) proposed the objective function in Equation (5). It seeks
at each graph vertex a sparse measure, as we expect in a lexicon (i.e., few frames have
nonzero probability for a given target). These two graph objectives can be optimized
by iterative updates, whose details we omit in this article; more information about the
motivation behind using the 1,2 penalty in the UJSF-1,2 objective, the optimization
procedure, and an empirical comparison of these and other objectives on another NLP
task can be found in Das and Smith (2012).
5.5.3 Constraints for Frame Identification. Once a graph-based SSL objective function is
minimized, we arrive at the optimal set of frame distributions q?, which we use to
constrain our frame identification inference rule, expressed in Equation (1). In that
rule, ti is the ith target in a sentence x, and fi is the corresponding evoked frame. We
now add a constraint to that rule. Recall from Section 5.2 that for targets with known
lemmatized forms, Fi was defined to be the set of frames that associate with lemma tli
in the supervised data. For unknown lemmas, Fi was defined to be all the frames in the
lexicon. If the LU corresponding to ti is present in the graph, let it be the vertex vi. For
such targets ti covered by the graph, we redefine Fi as:
Fi = {f : f ? M-best frames under q?vi} (6)
29
Computational Linguistics Volume 40, Number 1
Table 6
Exact and partial frame identification accuracy on the FrameNet 1.5 data set with the size of
lexicon (in terms of non-zero frame components in the truncated frame distributions) used for
frame identification, given gold targets. The supervised model is compared with alternatives in
Table 5. Bold indicates best results. UJSF-1,2 produces statistically significant results (p < 0.001)
for all metrics with respect to the supervised baseline for both the unseen LUs as well as the
whole test set. Although the NGF-2 and UJSF-1,2 models are statistically indistinguishable,
it is noteworthy that the UJSF-1,2 objective produces a much smaller lexicon.
UNKNOWN TARGETS ALL TARGETS Graph
exact partial exact partial Lexicon
frame matching frame matching frame matching frame matching Size
Supervised 23.08 46.62 82.97 90.51 ?
Self-training 18.88 42.67 82.27 90.02 ?
NGF-2 39.86 62.35 83.51 91.02 128,960
UJSF-1,2 42.67 65.29 83.60 91.12 45,544
For targets ti in test data whose LUs are not present in the graph (and hence in
supervised data), Fi is the set of all frames. Note that in this semi-supervised extension
of our frame identification inference procedure, we introduced several hyperparam-
eters, namely, ?, ?, K (the number of nearest neighbors for each vertex included in
the graph) and M (the number of highest scoring frames per vertex according to the
induced frame distribution). We choose these hyperparameters using cross-validation
by tuning the frame identification accuracy on unseen targets. (Different values of the
first three hyperparameters were chosen for the different graph objectives and we omit
their values here for brevity; M turned out to be 2 for all models.)
Table 6 shows frame identification accuracy, both using exact match as well as
partial match. Performance is shown on the portion of the test set containing unknown
LUs, as well as the whole test set. The final column presents lexicon size in terms
of the set of truncated frame distributions (filtered according to the top M frames in
qv for a vertex v) for all the LUs in a graph. For comparison with a semi-supervised
baseline, we consider a self-trained system. For this system, we used the supervised
frame identification system to label 70,000 sentences from the English Gigaword corpus
with frame-semantic parses. For finding targets in a raw sentence, we used a relaxed
target identification scheme, where we marked as potential frame-evoking units all
targets seen in the lexicon and all other words which were not prepositions, particles,
proper nouns, foreign words, or WH-words. We appended these automatic annotations
to the training data, resulting in 711,401 frame annotations, more than 36 times the
annotated data. These data were next used to train a frame identification model.28 This
set-up is very similar to that of Bejan (2009) who used self-training to improve frame
identification. In our setting, however, self-training hurts relative to the fully supervised
approach (Table 6).
Note that for the unknown part of the test set the graph-based objectives outperform
both the supervised model as well as the self-training baseline by a margin of ?20%
28 We ran self-training with smaller amounts of data, but found no significant difference with the results
achieved with 711,401 frame annotations. As we observe in Table 6, in our case, self-training performs
worse than the supervised model, and we do not hope to improve with even more data.
30
Das et al. Frame-Semantic Parsing
absolute. The best model is UJSF-1,2, and its performance is significantly better than
the supervised model (p < 0.01). It also produces a smaller lexicon (using the sparsity-
inducing penalty) than NGF-2, requiring less memory during frame identification
inference. The small footprint can be attributed to the removal of LUs for which all
frame components were zero (qi = 0). The improvements of the graph-based objectives
over the supervised and the self-trained models are modest for the whole test set,
but the best model still has statistically significant improvements over the supervised
model (p < 0.01).
6. Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of targets t = ?t1, . . . , tm?, and a list of evoked
frames f = ?f1, . . . , fm? corresponding to each target, argument identification is the task
of choosing which of each fi?s roles are filled, and by which parts of x. This task is most
similar to the problem of semantic role labeling, but uses a richer set of frame-specific
labels than PropBank annotations.
6.1 Model
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s roles (named frame element types) observed
in an exemplar sentence and/or our training set. A subset of each frame?s roles are
marked as core roles; these roles are conceptually and/or syntactically necessary for
any given use of the frame, though they need not be overt in every sentence involving
the frame. These are roughly analogous to the core arguments ARG0?ARG5 in PropBank.
Non-core roles?analogous to the various ARGM-* in PropBank?loosely correspond to
syntactic adjuncts, and carry broadly applicable information such as the time, place,
or purpose of an event. The lexicon imposes some additional structure on roles, in-
cluding relations to other roles in the same or related frames, and semantic types with
respect to a small ontology (marking, for instance, that the entity filling the protag-
onist role must be sentient for frames of cognition). Figure 3 illustrates some of the
structural elements comprising the frame lexicon by considering the CAUSE TO MAKE NOISE
frame.
We identify a set S of spans that are candidates for filling any role r ? Rfi . In
principle, S could contain any subsequence of x, but in this work we only consider
the set of contiguous spans that (a) contain a single word or (b) comprise a valid subtree
of a word and all its descendants in the dependency parse produced by the MST parser.
This covers approximately 80% of arguments in the development data for both data
sets.
The empty span, denoted ?, is also included in S , since some roles are not explicitly
filled; in the SemEval 2007 development data, the average number of roles an evoked
frame defines is 6.7, but the average number of overt arguments is only 1.7.29 In
29 In the annotated data, each core role is filled with one of three types of null instantiations indicating how
the role is conveyed implicitly. For instance, the imperative construction implicitly designates a role as
filled by the addressee, and the corresponding filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiation. The interested reader may refer to Chen
et al. (2010), who handle the different types of null instantions during argument identification.
31
Computational Linguistics Volume 40, Number 1
training, if a labeled argument is not a subtree of the dependency parse, we add its
span to S .30
Let Ai denote the mapping of roles in Rfi to spans in S . Our model makes a
prediction for each Ai(rk) (for all roles rk ? Rfi ) using:
Ai(rk) ? argmax
s?S
p?(s | rk, fi, ti, x) (7)
We use a conditional log-linear model over spans for each role of each evoked frame:
p?(Ai(rk) = s | fi, ti, x) =
exp?h(s, rk, fi, ti, x)
?
s??S
exp?h(s?, rk, fi, ti, x)
(8)
Note that our model chooses the span for each role separately from the other roles
and ignores all frames except the frame the role belongs to. Our model departs
from the traditional SRL literature by modeling the argument identification problem
in a single stage, rather than first classifying token spans as arguments and then
labeling them. A constraint implicit in our formulation restricts each role to have at
most one overt argument, which is consistent with 96.5% of the role instances in the
SemEval 2007 training data and 96.4% of the role instances in the FrameNet 1.5 full text
annotations.
Out of the overt argument spans in the training data, 12% are duplicates, having
been used by some previous frame in the sentence (supposing some arbitrary ordering
of frames). Our role-filling model, unlike a sentence-global argument detection-and-
classification approach,31 permits this sort of argument sharing among frames. Word
tokens belong to an average of 1.6 argument spans, including the quarter of words that
do not belong to any argument.
Appending together the local inference decisions from Equation (7) gives us the best
mapping A?t for target t. Features for our log-linear model (Equation (8)) depend on the
preprocessed sentence x; the target t; a role r of frame f ; and a candidate argument span
s ? S .32 For features using the head word of the target t or a candidate argument span
s, we use the heuristic described in footnote 21 for selecting the head of non-subtree
spans.
Table 7 lists the feature templates used in our model. Every feature template has
a version that does not take into account the role being filled (so as to incorporate
overall biases). The  symbol indicates that the feature template also has a variant that
is conjoined with r, the name of the role being filled; and  indicates that the feature
30 Here is an example in the FrameNet 1.5 training data where this occurs. In the sentence: As capital of
Europe?s most explosive economy, Dublin seems to be changing before your very eyes, the word economy
evokes the ECONOMY frame with the phrase most explosive fulfilling the Descriptor role. However,
in the dependency parse for the sentence the phrase is not a subtree because both words in the frame
attach to the word economy. Future work may consider better heuristics to select potential arguments
from the dependency parses to recover more gold arguments than what the current work achieves.
31 J&N?07, like us, identify arguments for each target.
32 In this section we use t, f , and r without subscripts because the features only consider a single role of a
single target?s frame.
32
Das et al. Frame-Semantic Parsing
Table 7
Features used for argument identification. Section 6.1 describes the meanings of the different
circles attached to each feature.
Features with both null and non-null variants: These features come in two flavors:
if the argument is null, then one version fires; if it is overt (non-null), then another
version fires.
 some word in t has lemma ?  some word in t has POS ?
 some word in t has lemma ?, and the
sentence uses PASSIVE voice
 some word in t has lemma ?, and the
sentence uses ACTIVE voice
 the head of t has subcategorization
sequence ? = ??1, ?2, . . . ?
 some syntactic dependent of the head of t
has dependency type ?
 the head of t has c syntactic dependents  bias feature (always fires)
Span content features: apply to overt argument candidates.
 POS tag ? occurs for some word in s  the head word of s has POS ?
 the first word of s has POS ?  |s|, the number of words in the span
 the last word of s has POS ?  the first word of s has lemma ?
 the head word of s has syntactic
dependency type ?
 the first word of s: ws1 , and its POS tag ?s1 ,
if ?s1 is a closed-class POS
 ws2 and its closed-class POS tag ?s2 ,
provided that |s| ? 2
 the syntactic dependency type ?s1 of the
first word with respect to its head
 the head word of s has lemma ?  ?s2 , provided that |s| ? 2
 the last word of s: ws|s| has lemma ?  ?s|s| , provided that |s| ? 3
 ws|s| , and its closed-class POS tag ?s|s| ,
provided that |s| ? 3
 lemma ? is realized in some word in s
 lemma ? is realized in some word in s, the
voice denoted in the span, s?s position
with respect to t (BEFORE, AFTER, or
OVERLAPPING)
 lemma ? is realized in some word in s,
the voice denoted in the span (ACTIVE
or PASSIVE)
Syntactic features: apply to overt argument candidates.
 dependency path: sequence of labeled,
directed edges from the head word of s to
the head word of t
 length of the dependency path
Span context POS features: for overt candidates, up to 6 of these features will be active.
 a word with POS ? occurs up to 3 words
before the first word of s
 a word with POS ? occurs up to 3 words
after the last word of s
Ordering features: apply to overt argument candidates.
 the position of s with respect to the span
of t: BEFORE, AFTER, or OVERLAPPING (i.e.
there is at least one word shared by s and t)
 target-argument crossing: there is at least
one word shared by s and t, at least one
word in s that is not in t, and at least one
word in t that is not in s
 linear word distance between the nearest
word of s and the nearest word of t,
provided s and t do not overlap
 linear word distance between the middle
word of s and the middle word of t,
provided s and t do not overlap
template additionally has a variant that is conjoined with both r and f , the name of the
frame.33 The role-name-only variants provide for smoothing over frames for common
types of roles such as Time and Place; see Matsubayashi, Okazaki, and Tsujii (2009) for
a detailed analysis of the effects of using role features at varying levels of granularity.
Certain features in our model rely on closed-class POS tags, which are defined to be
all Penn Treebank tags except for CD and tags that start with V, N, J, or R. Finally, the
33 That is, the  symbol subsumes , which in turn subsumes .
33
Computational Linguistics Volume 40, Number 1
features that encode a count or a number are binned into groups: (??,?20], [?19,?10],
[?9,?5], ?4, ?3, ?2, ?1, 0, 1, 2, 3, 4, [5, 9], [10, 19], [20,?).
6.2 Parameter Estimation
We train the argument identification model by:
max
?
N
?
j=1
mj
?
i=1
|R
f (j)i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i , x
(j) ) ? C ???22 (9)
Here, N is the number of data points (sentences) in the training set, and m is the number
of frame annotations per sentence. This objective function is concave. For experiments
with the SemEval 2007 data, we trained the model using stochastic gradient ascent
(Bottou 2004) with no Gaussian regularization (C = 0).34 Early stopping was done by
tuning on the development set, and the best results were obtained with a batch size of 2
and 23 passes through the data.
On the FrameNet 1.5 release, we trained this model using L-BFGS (Liu and Nocedal
1989) and ran it for 1,000 iterations. C was tuned on the development data, and we
obtained best results for C = 1.0. We did not use stochastic gradient descent for this
data set as the number of training instances increased and parallelization of L-BFGS
on a multicore setup implementing MPI (Gropp, Lusk, and Skjellum 1994) gave faster
training speeds.
6.3 Decoding with Beam Search
Naive prediction of roles using Equation (7) may result in overlap among arguments
filling different roles of a frame, because the argument identification model fills each role
independently of the others. We want to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans.35 Toutanova, Haghighi, and Manning
(2005) presented a dynamic programming algorithm to prevent overlapping arguments
for SRL; however, their approach used an orthogonal view to the argument identi-
fication stage, wherein they labeled phrase-structure tree constituents with semantic
roles. That formulation admitted a dynamic programming approach; our formulation
of finding the best argument span for each role does not.
To eliminate illegal overlap, we adopt the beam search technique detailed in
Algorithm 1. The algorithm produces a set of k-best hypotheses for a frame instance?s
full set of role-span pairs, but uses an approximation in order to avoid scoring an
exponential number of hypotheses. After determining which roles are most likely not
explicitly filled, it considers each of the other roles in turn: In each iteration, hypotheses
incorporating a subset of roles are extended with high-scoring spans for the next role,
always maintaining k alternatives. We set k=10,000 as the beam width.36
34 This was the setting used by Das et al. (2010) and we kept it unchanged.
35 On rare occasions a frame annotation may include a secondary frame element layer, allowing arguments to
be shared among multiple roles in the frame; see Ruppenhofer et al. (2006) for details. The evaluation for
this task only considers the primary layer, which is guaranteed to have disjoint arguments.
36 We show the effect of varying beam widths in Table 9, where we present performance of an exact
algorithm for argument identification.
34
Das et al. Frame-Semantic Parsing
Algorithm 1 Joint decoding of frame fi?s arguments via beam search. topk(S , p?, rj)
extracts the k most probable spans from S , under p?, for role rj. extend(D0:(j?1),S ?)
extends each span vector in D0:(j?1) with the most probable non-overlapping span from
S ?, resulting in k best extensions overall.
Require: k > 0, Rfi , S , the distribution p? from Equation 8 for each role rj ? Rfi
Ensure: A?i, a high-scoring mapping of roles of fi to spans with no token overlap among
the spans
1: Calculate Ai according to Equation 7
2: ?r ? Rfi such that Ai(r) = ?, let A?i(r) ? ?
3: R+fi ? {r : r ? Rfi ,Ai(r) = ?}
4: n ? |R+fi |
5: Arbitrarily order R+fi as {r1, r2, . . . rn}
6: Let D0:j = ?D0:j1 , . . . , D
0:j
k ? refer to the k-best list of vectors of compatible filler spans
for roles r1 through rj
7: Initialize D0:0 to be empty
8: for j = 1 to n do
9: D0:j ? extend(D0:(j?1), topk(S , p?, rj))
10: end for
11: ?j ? {1, . . . , n}, A?i(rj) ? D0:n1 [j]
12: return A?i
6.4 Results
Performance of the argument identification model is presented in Table 8 for both data
sets in consideration. We analyze them here.
SemEval 2007 Data: For the SemEval data set, the table shows how performance
varies given different types of input: correct targets and correct frames, correct targets
but automatically identified frames, and ultimately, no oracle input (the full frame
parsing scenario). Rows 1?2 isolate the argument identification task from the frame
identification task. Given gold targets and frames, our argument identification model
(without beam search) gets an F1 score of 68.09%; when beam search is applied, this
increases to 68.46%, with a noticeable increase in precision. Note that an estimated 19%
of correct arguments are excluded because they are neither single words nor complete
subtrees (see Section 6.1) of the automatic dependency parses.37
Qualitatively, the problem of candidate span recall seems to be largely due to
syntactic parse errors.38 Although our performance is limited by errors when using
the syntactic parse to determine candidate spans, it could still improve; this suggests
37 We found that using all constituents from the 10-best syntactic parses would improve oracle recall of
spans in the development set by just a couple of percentage points, at the computational cost of a larger
pool of candidate arguments per role.
38 Note that, because of our labels-only evaluation scheme (Section 3.3), arguments missing a word or
containing an extra word receive no credit. In fact, of the frame roles correctly predicted as having an
overt span, the correct span was predicted 66% of the time, while 10% of the time the predicted starting
and ending boundaries of the span were off by a total of one or two words.
35
Computational Linguistics Volume 40, Number 1
Ta
b
le
8
A
rg
u
m
en
ti
d
en
ti
fi
ca
ti
on
re
su
lt
s
on
bo
th
th
e
Se
m
E
va
l?0
7
d
at
a
as
w
el
la
s
th
e
fu
ll
te
xt
an
no
ta
ti
on
s
of
Fr
am
eN
et
1.
5.
Fo
r
d
ec
od
in
g,
be
am
an
d
na
iv
e
in
d
ic
at
e
w
he
th
er
th
e
ap
p
ro
xi
m
at
e
jo
in
td
ec
od
in
g
al
go
ri
th
m
ha
s
be
en
u
se
d
or
lo
ca
li
nd
ep
en
d
en
td
ec
is
io
ns
ha
ve
be
en
m
ad
e
fo
r
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
,
re
sp
ec
ti
ve
ly
.O
n
th
e
Se
m
E
va
l2
00
7
d
at
a,
fo
r
fu
ll
p
ar
si
ng
(a
u
to
m
at
ic
ta
rg
et
,f
ra
m
e,
an
d
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
),
b
ol
d
sc
or
es
in
d
ic
at
e
be
st
re
su
lt
s,
w
hi
ch
ar
e
al
so
si
gn
ifi
ca
nt
im
p
ro
ve
m
en
ts
re
la
ti
ve
to
th
e
ba
se
lin
e
(p
<
0.
05
).
O
n
th
e
Fr
am
eN
et
1.
5
d
at
a
se
t,
b
ol
d
sc
or
es
in
d
ic
at
e
be
st
re
su
lt
s
on
au
to
m
at
ic
fr
am
e
an
d
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
?
th
is
is
ac
hi
ev
ed
by
th
e
fr
am
e
id
en
ti
fi
ca
ti
on
m
od
el
th
at
u
se
s
th
e
U
JS
F-
 1
,2
gr
ap
h-
ob
je
ct
iv
e
an
d
au
to
m
at
ic
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
u
si
ng
be
am
se
ar
ch
.T
hi
s
re
su
lt
is
st
at
is
ti
ca
lly
si
gn
ifi
ca
nt
ov
er
th
e
su
p
er
vi
se
d
re
su
lt
s
sh
ow
n
in
ro
w
9
(p
<
0.
00
1)
.I
n
te
rm
s
of
p
re
ci
si
on
an
d
F 1
sc
or
e
m
ea
su
re
d
w
it
h
p
ar
ti
al
fr
am
e
m
at
ch
in
g,
th
e
re
su
lt
s
w
it
h
th
e
U
JS
F-
 1
,2
m
od
el
is
st
at
is
ti
ca
lly
si
gn
ifi
ca
nt
ov
er
th
e
N
G
F-
 2
m
od
el
(p
<
0.
05
).
Fo
r
re
ca
ll
w
it
h
p
ar
ti
al
fr
am
e
m
at
ch
in
g,
an
d
fo
r
al
lt
he
th
re
e
m
et
ri
cs
w
it
h
ex
ac
tf
ra
m
e
m
at
ch
in
g,
th
e
re
su
lt
s
w
it
h
th
e
tw
o
gr
ap
h
ob
je
ct
iv
es
ar
e
st
at
is
ti
ca
lly
in
d
is
ti
ng
u
is
ha
bl
e.
N
ot
e
th
at
ce
rt
ai
n
p
ar
ti
al
m
at
ch
re
su
lt
s
ar
e
m
is
si
ng
be
ca
u
se
in
th
os
e
se
tt
in
gs
,g
ol
d
fr
am
es
ha
ve
be
en
u
se
d
fo
r
ar
gu
m
en
ti
d
en
ti
fi
ca
ti
on
.
A
R
G
U
M
E
N
T
ID
E
N
T
IF
IC
A
T
IO
N
ta
rg
et
s
fr
am
es
de
co
di
ng
ex
ac
tm
at
ch
in
g
p
ar
ti
al
m
at
ch
in
g
P
R
F 1
P
R
F 1
S
em
E
va
l?
07
D
at
a
A
rg
u
m
en
t
id
en
ti
fi
ca
ti
on
(f
u
ll)
go
ld
go
ld
na
iv
e
77
.4
3
60
.7
6
68
.0
9
1
go
ld
go
ld
be
am
78
.7
1
60
.5
7
68
.4
6
2
P
ar
si
ng
(o
ra
cl
e
ta
rg
et
s)
go
ld
su
p
er
vi
se
d
(?
5.
2)
be
am
49
.6
8
42
.8
2
46
.0
0
57
.8
5
49
.8
6
53
.5
6
3
P
ar
si
ng
(f
u
ll)
au
to
su
p
er
vi
se
d
(?
5.
2)
be
am
58
.0
8
38
.7
6
46
.4
9
62
.7
6
41
.8
9
50
.2
4
4
P
ar
si
ng
(J
&
N
?0
7
ta
rg
et
s
an
d
fr
am
es
)
au
to
su
p
er
vi
se
d
(?
3.
4)
be
am
56
.2
6
36
.6
3
44
.3
7
60
.9
8
39
.7
0
48
.0
9
5
B
as
el
in
e:
J&
N
?0
7
au
to
su
p
er
vi
se
d
(?
3.
4)
N
/
A
51
.5
9
35
.4
4
42
.0
1
56
.0
1
38
.4
8
45
.6
2
6
Fr
am
eN
et
1.
5
R
el
ea
se
A
rg
u
m
en
t
id
en
ti
fi
ca
ti
on
(f
u
ll)
go
ld
go
ld
na
iv
e
82
.0
0
76
.3
6
79
.0
8
7
go
ld
go
ld
be
am
83
.8
3
76
.2
8
79
.8
8
8
P
ar
si
ng
(o
ra
cl
e
ta
rg
et
s)
go
ld
su
p
er
vi
se
d
(?
5.
2)
be
am
67
.8
1
60
.6
8
64
.0
5
72
.4
7
64
.8
5
68
.4
5
9
go
ld
SS
L
(N
G
F-
 2
,?
5.
5)
be
am
68
.2
2
61
.0
4
64
.4
3
72
.8
7
65
.2
0
68
.8
2
10
go
ld
SS
L
(U
JS
F-
 1
,2
,?
5.
5)
be
am
68
.3
3
61
.1
4
64
.5
4
72
.9
8
65
.3
0
68
.9
3
11
36
Das et al. Frame-Semantic Parsing
that the model has trouble discriminating between good and bad arguments, and that
additional feature engineering or jointly decoding arguments of a sentence?s frames
may be beneficial.
Rows 3?4 show the effect of automatic supervised frame identification on overall
frame parsing performance. There is a 22% absolute decrease in F1 (18% when partial
credit is given for related frames), suggesting that improved frame identification
or joint prediction of frames and arguments is likely to have a sizeable impact on
overall performance. Rows 4?6 compare our full model (target, frame, and argument
identification) with the baseline, showing significant improvement of more than 4.4
F1 points for both exact and partial frame matching. As with frame identification, we
compared the argument identification stage with that of J&N?07 in isolation, using the
automatically identified targets and frames from the latter as input to our model. As
shown in row 5, with partial frame matching, this gave us an F1 score of 48.1% on the
test set?significantly better (p < 0.05) than 45.6%, the full parsing result from J&N?07
(row 6 in Table 8). This indicates that our argument identification model?which uses a
single discriminative model with a large number of features for role filling (rather than
argument labeling)?is more accurate than the previous state of the art.
FrameNet 1.5 Release: Rows 7?12 show results on the newer data set, which is part
of the FrameNet 1.5 release. As in the frame identification results of Table 5, we do not
show results using predicted targets, as we only test the performance of the statistical
models. First, we observe that for results with gold frames, the F1 score is 79.08%
with naive decoding, which is significantly higher than the SemEval counterpart. This
indicates that increased training data greatly improves performance on the task. We also
observe that beam search improves precision by nearly 2%, while getting rid of overlap-
ping arguments. When both model frames and model arguments are used, we get an
F1 score of 68.45%, which is encouraging in comparison to the best results we achieved
on the SemEval 2007 data set. Semi-supervised lexicon expansion for frame identifi-
cation further improves parsing performance. We observe the best results when the
UJSF-1,2 graph objective is used for frame identification, significantly outperforming
the fully supervised model on parsing (p < 0.001) for all evaluation metrics. The im-
provements with SSL can be explained by noting that frame identification performance
goes up when the graph objectives are used, which carries over to argument iden-
tification. Figure 5 shows an example where the graph-based model UJSF-1,2 corrects
an error made by the fully supervised model for the unseen LU discrepancy.N, both for
frame identification and full frame-semantic parsing.
7. Collective Argument Identification with Constraints
The argument identification strategy described in the previous section does not capture
some facets of semantic knowledge represented declaratively in FrameNet. In this
section, we present an approach that exploits such knowledge in a principled, unified,
and intuitive way. In prior NLP research using FrameNet, these interactions have been
largely ignored, though they have the potential to improve the quality and consistency
of semantic analysis. The beam search technique (Algorithm 1) handles one kind of
constraint: avoiding argument overlaps. It is, however, approximate and cannot handle
other forms of constraints.
Here, we present an algorithm that exactly identifies the best full collection of argu-
ments of a target given its semantic frame. Although we work within the conventions of
37
Computational Linguistics Volume 40, Number 1
Figure 5
(a) Output of the supervised frame-semantic parsing model, with beam search for argument
identification, given the target discrepancies. The output is incorrect. (b) Output using the
constrained frame identification model that takes into account the graph-based frame
distributions over unknown predicates. In this particular example, the UJSF-1,2 graph
objective is used. This output matches the gold annotation. The LU discrepancy.N is unseen
in supervised FrameNet data.
FrameNet, our approach is generalizable to other SRL frameworks. We model argument
identification as constrained optimization, where the constraints come from expert
knowledge encoded in FrameNet. Following prior work on PropBank-style SRL that
dealt with similar constrained problems (Punyakanok et al. 2004; Punyakanok, Roth,
and Yih 2008, inter alia), we incorporate this declarative knowledge in an integer linear
program.
Because general-purpose ILP solvers are proprietary and do not fully exploit the
structure of the problem, we turn to a class of optimization techniques called dual
decomposition (Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010; Martins et al.
2011a). We derive a modular, extensible, parallelizable approach in which semantic con-
straints map not just to declarative components in the algorithm, but also to procedural
ones, in the form of ?workers.? Although dual decomposition algorithms only solve
a relaxation of the original problem, we make our approach exact by wrapping the
algorithm in a branch-and-bound search procedure. 39
We experimentally find that our algorithm achieves accuracy comparable to the
results presented in Table 8, while respecting all imposed linguistic constraints. In
comparison with beam search, which violates many of these constraints, the presented
exact decoder is slower, but it decodes nine times faster than CPLEX, a state-of-the-art,
proprietary, general-purpose exact ILP solver.40
39 Open-source code in C++ implementing the AD3 algorithm can be found at
http://www.ark.cs.cmu.edu/AD3.
40 See http://www-01.ibm.com/software/integration/optimization/cplex-optimizer.
38
Das et al. Frame-Semantic Parsing
7.1 Joint Inference
Here, we take a declarative approach to modeling argument identification using an ILP
and relate our formulation to prior work in shallow semantic parsing. We show how
knowledge specified in a linguistic resource (FrameNet in our case) can be used to
derive the constraints in our ILP. Finally, we draw connections of our specification to
graphical models, a popular formalism in artificial intelligence and machine learning,
and describe how the constraints can be treated as factors in a factor graph.
7.1.1 Declarative Specification. Let us simplify notation by considering a given target t and
not considering its index in a sentence x; let the semantic frame it evokes be f . To solely
evaluate argument identification, we assume that the semantic frame f is given, which is
traditionally the case in controlled experiments used to evaluate SRL systems (Ma`rquez
et al. 2008). Let the set of roles associated with the frame f be Rf . In sentence x, the set
of candidate spans of words that might fill each role is enumerated, usually following
an overgenerating heuristic, which is described in Section 6.1; as before, we call this set
of spans S . As before, this set also includes the null span ?; connecting it to a role r ? Rf
denotes that the role is not overt. Our approach assumes a scoring function that gives
a strength of association between roles and candidate spans. For each role r ? Rf and
span s ? S , this score is parameterized as:
c(r, s) = ?h(s, r, f, t, x), (10)
where ? are model weights and h is a feature function that looks at the target t,
the evoked frame f , sentence x, and its syntactic analysis, along with r and s. This
scoring function is identical in form to the numerator?s exponent in the log-linear model
described in Equation (8). The SRL literature provides many feature functions of this
form and many ways to use machine learning to acquire ?. Our presented method does
not make any assumptions about the score except that it has the form in Equation (10).
We define a vector z of binary variables zr,s ? {0, 1} for every role and span pair. We
have that: z ? {0, 1}d, where d = |Rf | ? |S|. zr,s = 1 means that role r is filled by span s.
Given the binary z vector, it is straightforward to recover the collection of arguments
by checking which components zr,s have an assignment of 1; we use this strategy to
find arguments, as described in Section 7.3 (strategies 4 and 6). The joint argument
identification task can be represented as a constrained optimization problem:
maximize
?
r?Rf
?
s?S
c(r, s) ? zr,s
with respect to z ? {0, 1}d
such that Az ? b (11)
In the last line, A is a k ? d matrix and b is a vector of length k. Thus, Az ? b is a set of
k inequalities representing constraints that are imposed on the mapping between roles
and spans; these are motivated on linguistic grounds and are described next. 41
41 Note that equality constraints a ? z = b can be transformed into double-side inequalities a ? z ? b and
?a ? z ? ?b.
39
Computational Linguistics Volume 40, Number 1
Uniqueness. Each role r is filled by at most one span in S . This constraint can be
expressed by:
?r ? Rf ,
?
s?S
zr,s = 1 (12)
There are O(|Rf |) such constraints. Note that because S contains the null span ?, non-
overt roles are also captured using the above constraints. Such a constraint is used
extensively in prior literature (Punyakanok, Roth, and Yih 2008, Section 3.4.1).
Overlap. SRL systems commonly constrain roles to be filled by non-overlapping spans.
For example, Toutanova, Haghighi, and Manning (2005) used dynamic programming
over a phrase structure tree to prevent overlaps between arguments, and Punyakanok,
Roth, and Yih (2008) used constraints in an ILP to respect this requirement. Inspired by
the latter, we require that each input sentence position of x be covered by at most one
argument of t. We define:
G(i) = {s | s ? S , s covers position i in x} (13)
We can define our overlap constraints in terms of G as follows, for every sentence
position i:
?i ? {1, . . . , |x|},
?
r?Rf
?
s?G(i)
zr,s ? 1 (14)
This gives us O(|x|) constraints. It is worth noting that this constraint aims to achieve the
same effect as beam search, as described in Section 6.3, which tries to avoid argument
overlap greedily.
Pairwise ?Exclusions.? For many target classes, there are pairs of roles forbidden to
appear together in the analysis of a single target token. Consider the following two
sentences:
(1) A blackberry
Entity 1
resembles a loganberry
Entity 2
.
(2) Most berries
Entities
resemble each other.
Consider the uninflected target resemble in both sentences, evoking the same
meaning. In Example (1), two roles?which we call Entity 1 and Entity 2?describe two
entities that are similar to each other. In the second sentence, a phrase fulfills a third
role, called Entities, that collectively denotes some objects that are similar. It is clear that
the roles Entity 1 and Entities cannot be overt for the same target at once, because the latter
already captures the function of the former; a similar argument holds for the Entity 2 and
Entities roles. We call this phenomenon the ?excludes? relationship. Let us define a set of
pairs from Rf that have this relationship:
Exclf = {(ri, rj) | ri and rj exclude each other}
40
Das et al. Frame-Semantic Parsing
Using the given set, we define the constraint:
?(ri, rj) ? Exclf , zri,? + zrj,? ? 1 (15)
If both roles are overt in a parse, this constraint will be violated, contravening the
?excludes? relationship specified between the pair of roles. If neither or only one of
the roles is overt, the constraint is satisfied. The total number of such constraints is
O(|Exclf |), which is the number of pairwise ?excludes? relationships of a given frame.
Pairwise ?Requirements.? The sentence in Example (1) illustrates another kind of con-
straint. The target resemble cannot have only one of Entity 1 and Entity 2 as roles in text.
For example,
(3) * A blackberry
Entity 1
resembles.
Enforcing the overtness of two roles sharing this ?requires? relationship is straight-
forward. We define the following set for a frame f :
Reqf = {(ri, rj) | ri and rj require each other}
This leads to constraints of the form
?(ri, rj) ? Reqf , zri,? ? zrj,? = 0 (16)
If one role is overt (or absent), the other must be as well. A related constraint has
been used previously in the SRL literature, enforcing joint overtness relationships be-
tween core arguments and referential arguments (Punyakanok, Roth, and Yih 2008,
Section 3.4.1), which are formally similar to our example.42
7.1.2 Integer Linear Program and Relaxation. Plugging the constraints in Equations 12,
14, 15, and 16 into the last line of Equation (11), we have the argument identification
problem expressed as an ILP, since the indicator variables z are binary. Here, apart from
the ILP formulation, we will consider the following relaxation of Equation (11), which
replaces the binary constraint z ? {0, 1}d by a unit interval constraint z ? [0, 1]d, yielding
a linear program:
maximize
?
r?Rf
?
s?S
c(r, s) ? zr,s
with respect to z ? [0, 1]d
such that Az ? b. (17)
42 We noticed that, in the annotated data, in some cases, the ?requires? constraint is violated by the
FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence
containing the target, but is rather instantiated in an earlier sentence (Gerber and Chai 2010). We apply
the hard constraint in Equation (16), though extending our algorithm to seek arguments outside the
sentence is straightforward. For preliminary work extending SEMAFOR this way, see Chen et al. (2010).
41
Computational Linguistics Volume 40, Number 1
There are several LP and ILP solvers available, and a great deal of effort has been
spent by the optimization community to devise efficient generic solvers. An example
is CPLEX, a state-of-the-art solver for mixed integer programming that we use as a
baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17).
Like many of the best implementations, CPLEX is proprietary.
7.1.3 Linguistic Constraints from FrameNet. Although enforcing the four different sets of
constraints is intuitive from a general linguistic perspective, we ground their use in
definitive linguistic information present in the FrameNet lexicon. From the annotated
data in the FrameNet 1.5 release, we gathered that only 3.6% of the time is a role
instantiated multiple times by different spans in a sentence. This justifies the uniqueness
constraint enforced by Equation (12). Use of such a constraint is also consistent with
prior work in frame-semantic parsing (Johansson and Nugues 2007). Similarly, we
found that in the annotations, no arguments overlapped with each other for a given
target. Hence, the overlap constraints in Equation (14) are also justified.
Our third and fourth sets of constraints, presented in Equations (15) and (16), come
from FrameNet, too. Examples (1) and (2) are instances where the target resemble
evokes the SIMILARITY frame, which is defined in FrameNet as:
Two or more distinct entities, which may be concrete or abstract objects or types, are
characterized as being similar to each other. Depending on figure/ground relations, the
entities may be expressed in two distinct frame elements and constituents, Entity 1 and
Entity 2, or jointly as a single frame element and constituent, Entities.
For this frame, the lexicon lists several roles other than the three we have already
observed, such as Dimension (the dimension along which the entities are similar), Differ-
entiating fact (a fact that reveals how the concerned entities are similar or different), and
so forth. Along with the roles, FrameNet also declares the ?excludes? and ?requires?
relationships noted in our discussion in Section 7.1.1. The case of the SIMILARITY frame
is not unique; in Figure 1, the frame COLLABORATION, evoked by the target partners, also
has two roles Partner 1 and Partner 2 that share the ?requires? relationship. In fact, out
of 877 frames in FrameNet 1.5, 204 frames have at least a pair of roles for which the
?excludes? relationship holds, and 54 list at least a pair of roles that share the ?requires?
relationship.
7.1.4 Constraints as Factors in a Graphical Model. The LP in Equation (17) can be repre-
sented as a maximum a posteriori inference problem in an undirected graphical model.
In the factor graph, each component (zr,s) of the vector z corresponds to a binary
variable, and each instantiation of a constraint in Equations (12), (14), (15), and (16)
corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such
a representation to impose constraints in a dependency parsing problem; the latter
discussed the equivalence of linear programs and factor graphs for representing dis-
crete optimization problems. All of our constraints take standard factor forms we can
describe using the terminology of Smith and Eisner and Martins et al. The uniqueness
constraint in Equation (12) corresponds to an XOR factor, while the overlap constraint
in Equation (14) corresponds to an ATMOSTONE factor. The constraints in Equation (15)
enforcing the ?excludes? relationship can be represented with an OR factor. Finally,
each ?requires? constraints in Equation (16) is equivalent to an XORWITHOUTPUT
factor.
42
Das et al. Frame-Semantic Parsing
In the following section, we describe how we arrive at solutions for the LP in
Equation (17) using dual decomposition, and how we adapt it to efficiently recover the
exact solution of the ILP (Equation (11)), without the need of an off-the-shelf ILP solver.
7.2 ?Augmented? Dual Decomposition
Dual decomposition methods address complex optimization problems in the dual, by
dividing them into simple worker problems (subproblems), which are repeatedly solved
until a consensus is reached. The simplest technique relies on the subgradient algorithm
(Komodakis, Paragios, and Tziritas 2007; Rush et al. 2010); as an alternative, Martins
et al. (2011a, 2011b) proposed an augmented Lagrangian technique, which is more
suitable when there are many small components ?commonly the case in declarative
constrained problems, like the one at hand. Here, we present a brief overview of the
latter, which is called AD3.
Let us start by establishing some notation. Let m ? {1, . . . , M} index a factor, and
denote by i(m) the vector of indices of variables linked to that factor. (Recall that each
factor represents the instantiation of a constraint.) We introduce a new set of variables,
u ? Rd, called the ?witness? vector. We split the vector z into M overlapping pieces
z1, . . . , zM, where each zm ? [0, 1]|i(m)|, and add M constraints zm = ui(m) to impose that
all the pieces must agree with the witness (and therefore with each other). Each of the M
constraints described in Section 7.1 can be encoded with its own matrix Am and vector
bm (which jointly define A and b in Equation (17)). For convenience, we denote by c ? Rd
the score vector, whose components are c(r, s), for each r ? Rf and s ? S (Equation (10)),
and define the following scores for the mth subproblem:
cm(r, s) = ?(r, s)?1c(r, s), ?(r, s) ? i(m)
where ?(r, s) is the number of constraints that involve role r and span s. Note that
according to this definition, c ? z =
?M
m=1 cm ? zm. We can rewrite the LP in Equation (17)
in the following equivalent form:
maximize
M
?
m=1
cm ? zm
with respect to u ? Rd, zm ? [0, 1]i(m), ?m
such that Amzm ? bm, ?m
zm = ui(m), ?m (18)
We introduce Lagrange multipliers ?m for the equality constraints in the last line.
The AD3 algorithm is depicted as Algorithm 2. Like dual decomposition approaches,
it repeatedly performs a broadcast operation (the zm-updates, which can be done in
parallel, one constraint per ?worker?) and a gather operation (the u- and ?-updates).
Each u-operation can be seen as an averaged voting which takes into consideration each
worker?s results.
Like in the subgradient method, the ?-updates can be regarded as price adjust-
ments, which will affect the next round of zm-updates. The only difference with respect
to the subgradient method (Rush et al. 2010) is that each subproblem involved in a
zm-update also has a quadratic penalty that penalizes deviations from the previous
43
Computational Linguistics Volume 40, Number 1
Algorithm 2 AD3 for Argument Identification
Require: role-span matching scores c := ?c(r, s)?r,s, structural constraints ?Am, bm?Mm=1,
penalty ? > 0
1: initialize t ? 1
2: initialize u1 uniformly (i.e., u(r, s) = 0.5, ?r, s)
3: initialize each ?1m = 0, ?m ? {1, . . . , M}
4: repeat
5: for each m = 1, . . . , M do
6: make a zm-update by finding the best scoring analysis for the mth constraint,
with penalties for deviating from the consensus u:
z(t+1)m ? argmax
Amztm?bm
(cm + ?tm) ? zm ?
?
2?zm ? u
t
i(m)?2 (19)
7: end for
8: make a u-update by updating the consensus solution, averaging z1, . . . , zm:
u(t+1)(r, s) ? 1
?(r, s)
?
m:(r,s)?i(m)
z(t+1)m (r, s)
9: make a ?-update:
?(t+1)m ? ?tm ? ?(z(t+1)m ? u(t+1)i(m) ), ?m
10: t ? t + 1
11: until convergence
Ensure: relaxed primal solution u? and dual solution ??. If u? is integer, it will encode
an assignment of spans to roles. Otherwise, it will provide an upper bound of the
true optimum.
average voting; it is this term that accelerates consensus and therefore convergence.
Martins et al. (2011b) also provide stopping criteria for the iterative updates using
primal and dual residuals that measure convergence; we refer the reader to that paper
for details.
A key attraction of this algorithm is that all the components of the declarative
specification remain intact in the procedural form. Each worker corresponds exactly
to one constraint in the ILP, which corresponds to one linguistic constraint. There is no
need to work out when, during the procedure, each constraint might have an effect, as
in beam search.
7.2.1 Solving the Subproblems. In a different application, Martins et al. (2011b, Section 4)
showed how to solve each zm-subproblem associated with the XOR, XORWITHOUTPUT
and OR factors in runtime O(|i(m)| log |i(m)|). The only subproblem that remains is that
of the ATMOSTONE factor; a solution with the same runtime is given in Appendix B.
7.2.2 Exact Decoding. It is worth recalling that AD3, like other dual decomposition
algorithms, solves a relaxation of the actual problem. Although we have observed that
the relaxation is often tight (cf. Section 7.3), this is not always the case. Specifically, a
fractional solution may be obtained, which is not interpretable as an argument, and
therefore it is desirable to have a strategy to recover the exact solution. Two observations
44
Das et al. Frame-Semantic Parsing
are noteworthy. First, the optimal value of the relaxed problem (Equation (17)) provides
an upper bound to the original problem (Equation (11)). This is because Equation (11)
has the additional integer constraint on the variables. In particular, any feasible dual
point provides an upper bound to the original problem?s optimal value. Second, dur-
ing execution of the AD3 algorithm, we always keep track of a sequence of feasible
dual points. Therefore, each iteration constructs tighter and tighter upper bounds.
With this machinery, we have all that is necessary for implementing a branch-and-
bound search that finds the exact solution of the ILP. The procedure works recursively
as follows:
1. Initialize L = ?? (our best value so far).
2. Run Algorithm 2. If the solution u? is integer, return u? and set L to the objec-
tive value. If along the execution we obtain an upper bound less than L, then
Algorithm 2 can be safely stopped and return ?infeasible??this is the bound part.
Otherwise (if u? is fractional) go to step 3.
3. Find the ?most fractional? component of u? (call it u?j ) and branch: constrain uj = 0
and go to step 2, eventually obtaining an integer solution u?0 or infeasibility; and
then constrain uj = 1 and do the same, obtaining u?1 . Return the u
? ? {u?0 , u?1} that
yields the largest objective value.
Although this procedure may have worst-case exponential runtime, we found it empir-
ically to rapidly obtain the exact solution in all test cases.
7.3 Results with Collective Argument Identification
We present experiments only on argument identification in this section, as our goal is
to exhibit the importance of incorporating the various linguistic constraints during our
inference procedure. We present results on the full text annotations of FrameNet 1.5, and
do not experiment on the SemEval 2007 benchmark, as we have already established our
constraint-agnostic models as state-of-the-art. The model weights ? used in the scoring
function c were learned as in Section 6.1 (i.e., by training a logistic regression model to
maximize conditional log-likelihood). The AD3 parameter ? was initialized to 0.1, and
we followed Martins et al. (2011b) in dynamically adjusting it to keep a balance between
the primal and dual residuals.
We compare the following algorithms to demonstrate the efficacy of our collective
argument identification approach:43
1. Naive: This strategy selects the best span for each role r according to the score
function c(r, s), independently of all other roles?the decoding rule formalized in
Equation (7) of Section 6.1. It ignores all constraints except ?uniqueness.?
2. Beam: This strategy employs greedy beam search to eliminate overlaps between
predicted arguments, as described in Algorithm 1. Note that it does not try to
respect the ?excludes? and ?requires? constraints between pairs of roles. The
default size of the beam in Section 1 was a safe 10,000; this resulted in extremely
slow decoding times. For time comparison, we tried beam sizes of 100 and 2 (the
latter being the smallest size that achieves the same F1 score on the FrameNet 1.5
dev set).
43 The first two strategies correspond to rows 7 and 9, respectively, of Table 8.
45
Computational Linguistics Volume 40, Number 1
Table 9
Comparison of decoding strategies in Section 7.3 on the data set released with the FrameNet 1.5
Release, given gold frames. We evaluate in terms of precision, recall, and F1 score on our test
set containing 4,458 targets. We also compute the number of constraint violations each model
makes: the three values are the numbers of overlapping arguments and violations of the
?requires? and ?excludes? constraints of Section 7.1. Finally, decoding time (without feature
computation steps) on the whole test set is shown in the last column averaged over five runs.
ARGUMENT IDENTIFICATION
Method P R F1 Violations Time (s)
naive 82.00 76.36 79.08 441 45 15 1.26 ? 0.01
beam = 2 83.68 76.22 79.78 0 49 0 2.74 ? 0.10
beam = 100 83.83 76.28 79.88 0 50 1 29.00 ? 0.25
beam = 10, 000 83.83 76.28 79.88 0 50 1 440.67 ? 5.53
CPLEX, LP 83.80 76.16 79.80 0 1 0 32.67 ? 1.29
CPLEX, exact 83.78 76.17 79.79 0 0 0 43.12 ? 1.26
AD3, LP 83.77 76.17 79.79 2 2 0 4.17 ? 0.01
AD3, exact 83.78 76.17 79.79 0 0 0 4.78 ? 0.04
3. CPLEX, LP: This uses CPLEX to solve the relaxed LP in Equation (17). To han-
dle fractional z, for each role r, we choose the best span s? such that s? =
argmaxs?Sr zr,s, breaking ties arbitrarily.
4. CPLEX, exact: This tackles the actual ILP (Equation (11)) with CPLEX.
5. AD3, LP: The relaxed problem is solved using AD3. We choose a span for each role
as in strategy 3.
6. AD3, exact: This couples AD3 with branch-and-bound search to get the exact
integer solution.
Table 9 shows performance of these decoding strategies on the test set. We report
precision, recall, and F1 scores. As with experiments in previous sections, we use the
evaluation script from SemEval 2007 shared task. Because these scores do not penalize
constraint violations, we also report the number of overlap, ?excludes,? and ?requires?
constraints that were violated in the test set. Finally, we tabulate each setting?s decoding
time in seconds on the whole test set averaged over five runs.44 The naive model
is very fast but suffers degradation in precision and violates one constraint roughly
per nine targets. The decoding strategy of Section 6.1 used a default beam size of
10,000, which is extremely slow; a faster version of beam size 100 results in the same
precision and recall values, but is 15 times faster on our test set. Beam size 2 results
in slightly worse precision and recall values, but is even faster. All of these, however,
result in many constraint violations. Strategies involving CPLEX and AD3 perform
similarly to each other and to beam search on precision and recall, but eliminate most
or all of the constraint violations. With respect to precision and recall, exact AD3 and
beam search with a width of 10,000 were found to be statistically indistinguishable
(p > 0.01). The decoding strategy with beam size 2 is 11?16 times faster than the
44 Experiments were conducted on a 64-bit machine with two 2.6-GHz dual-core CPUs (i.e., four processors
in all) and a total of 8 GB of RAM. The workers in AD3 were not parallelized, whereas CPLEX
automatically parallelized execution.
46
Das et al. Frame-Semantic Parsing
.
(a) Gold annotation.
.
(b) Beam search output.
Figure 6
An example from the test set where (a) exhibits the gold annotation for a target that evokes
the COLLABORATION frame, with the Partners role filled by the span international. (b) shows
the prediction made by the beam search decoding scheme (beam = 10,000), where it marks
international with the Partner 1 role, violating the ?requires? constraint; FrameNet notes that this
role should be present with the Partner 2 role. AD3 is conservative and predicts no role?it is
penalized by the evaluation script, but does not produce output that violates
linguistic constraints.
CPLEX strategies, but is only twice as fast as AD3, and results in significantly more
constraint violations. The exact algorithms are slower than the LP versions, but com-
pared with CPLEX, AD3 is significantly faster and has a narrower gap between its
exact and LP versions. We found that relaxation was tight 99.8% of the time on the test
examples.
The example in Figure 1 is taken from our test set, and shows an instance where two
roles, Partner 1 and Partner 2, share the ?requires? relationship; for this example, the beam
search decoder misses the Partner 2 role, which is a violation, while our AD3 decoder
identifies both arguments correctly. Note that beam search makes plenty of linguistic
violations. We found that beam search, when violating many ?requires? constraints,
often finds one role in the pair, which increases its recall. AD3 is sometimes more
conservative in such cases, predicting neither role. Figure 6 shows such an example
where beam search finds one role (Partner 1) while AD3 is more conservative and predicts
no roles. Figure 7 shows another example contrasting the output of beam search and
AD3 where the former predicts two roles sharing an ?excludes? relationship; AD3 does
not violate this constraint and tries to predict a more consistent argument set. Overall,
we found it interesting that imposing the constraints did not have much effect on
standard measures of accuracy.
Table 9 only shows results with gold frames. We ran the exact version of AD3 with
automatic frames as well. When the semi-supervised graph objective UJSF-1,2 is used
for frame identification, the performance with AD3 is only a bit worse in comparison
with beam search (row 11 in Table 8) when frame and argument identification are
evaluated together. We get a precision of 72.92, a recall of 65.22 and an F1 score of 68.86
(partial frame matching). Again, all linguistic constraints are respected, unlike beam
search.
47
Computational Linguistics Volume 40, Number 1
8. Conclusion
We have presented an approach to rich frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic models trained on full text annota-
tions released along with the FrameNet lexicon, and expedient heuristics. The frame
identification model uses latent variables in order to generalize to predicates unseen
in either the FrameNet lexicon or training data, and our results show that, quite often,
this model chooses a frame closely related to the gold-standard annotation. We also
presented an extension of this model that uses graph-based semi-supervised learning
to better generalize to new predicates; this achieves significant improvements over the
fully supervised approach. Our argument identification model, trained using maximum
conditional log-likelihood, unifies the traditionally separate steps of detecting and
(a) Gold annotation.
(b) Beam search output.
(c) AD3 output.
Figure 7
An example from the test set where (a) exhibits the gold annotation for a target that evokes
the DISCUSSION frame, with the Interlocutor 1 role filled by the span neighbors. (b) shows
the prediction made by the beam search decoding scheme (beam = 10,000), where it marks
The next morning his households and neighbors with the Interlocutors role, which violates
the ?excludes? constraint with respect to the Interlocutor 2 role. In (c), AD3 marks the wrong
span as the Interlocutor 1 role, but it does not violate the constraint. Both beam and
AD3 inference miss the Topic role.
48
Das et al. Frame-Semantic Parsing
labeling arguments. Our system achieves improvements over the previous state of the
art on the SemEval 2007 benchmark data set at each stage of processing and collectively.
We also report stronger results on the more recent, larger FrameNet 1.5 release.
We applied the AD3 algorithm to collective prediction of a target?s arguments,
incorporating declarative linguistic knowledge as constraints. It outperforms the naive
local decoding scheme that is oblivious to the constraints. Furthermore, it is significantly
faster than a decoder employing a state-of-the-art proprietary solver; it is only twice as
slow as beam search (our chosen decoding method for comparison with the state of
the art), which is inexact and does not respect all linguistic constraints. This method is
easily amenable to the inclusion of additional constraints.
From our results, we observed that in comparison to the SemEval 2007 data
set, frame-semantic parsing performance significantly increases when we use the
FrameNet 1.5 release; this suggests that the increase in the number of full text anno-
tations and the size of the FrameNet lexicon is beneficial. We believe that with more
annotations in the future (say, in the range of the number of PropBank annotations), our
frame-semantic parser can reach even better accuracy, making it more useful for NLP
applications that require semantic analysis.
There are several open problems to be addressed. Firstly, we could further im-
prove the coverage of the frame-semantic parser by improving our semi-supervised
learning approach; two possibilities are custom metric learning approaches (Dhillon,
Talukdar, and Crammer 2010) that suit the frame identification problem in graph-based
SSL, and sparse word representations (Turian, Ratinov, and Bengio 2010) as features
in frame identification. The argument identification model might also benefit from
semi-supervised learning. Further feature engineering and improved preprocessing,
including tokenization into lexical units, improved syntactic parsing, and the use of
external knowledge bases, is expected to improve the system?s accuracy. Finally, the
FrameNet lexicon does not contain exhaustive semantic knowledge. Automatic frame
and role induction is an exciting direction of future research that could further enhance
our methods of automatic frame-semantic parsing. The parser described in this article
is available for download at http://www.ark.cs.cmu.edu/SEMAFOR.
Appendix
A. Target Identification Heuristics from J&N?07
We describe here the filtering rules that Johansson and Nugues (2007) used for identify-
ing frame evoking targets in their SemEval 2007 shared task paper. They built a filtering
component based on heuristics that removed words that appear in certain contexts, and
kept the remaining ones.45 These are:
 have was retained only if had an object,
 be was retained only if it was preceded by there,
 will was removed in its modal sense,
 of course and in particular were removed,
45 Although not explicitly mentioned in the paper, we believe that these rules were applied on a white list of
potential targets seen in FrameNet and the SemEval 2007 training data.
49
Computational Linguistics Volume 40, Number 1
 the prepositions above, against, at, below, beside, by, in, on, over, and under
were removed unless their head was marked as locative,
 after and before were removed unless their head was marked as temporal,
 into, to, and through were removed unless their head was marked as
direction,
 as, for, so, and with were always removed,
 because the only sense of the word of was the frame PARTITIVE, it was
removed unless it was preceded by only, member, one, most, many, some, few,
part, majority, minority, proportion, half, third, quarter, all, or none, or it was
followed by all, group, them, or us,
 all targets marked as support verbs for some other target were removed.
Note that J&N?07 used a syntactic parser that provided dependency labels correspond-
ing to locative, temporal, and directional arguments, which our syntactic parser of
choice (the MST parser) does not provide.
B. Solving ATMOSTONE subproblems in AD3
The ATMOSTONE subproblem can be transformed into that of projecting a point
(a1, . . . , ak) onto the set
Sm =
{
zm ? [0, 1]|i(m)| |
?|i(m)|
j=1 zm,j ? 1
}
This projection can be computed as follows:
1. Clip each aj into the interval [0, 1] (i.e., set a?j = min{max{aj, 0}, 1}). If the result
satisfies
?k
j=1 a
?
j ? 1, then return (a?1, . . . , a?k).
2. Otherwise project (a1, . . . , ak) onto the probability simplex:
{
zm ? [0, 1]|i(m)| |
?|i(m)|
j=1 zm,j = 1
}
This is precisely the XOR subproblem and can be solved in time O(|i(m)|
log |i(m)|).
The proof of this procedure?s correctness follows from the proof in Appendix B of
Martins et al. (2011b).
Acknowledgments
We thank Collin Baker, Katrin Erk, Richard
Johansson, and Nils Reiter for software, data,
evaluation scripts, and methodological
details. We thank the reviewers of this and
the earlier papers, Alan Black, Ric Crabbe,
Michael Ellsworth, Rebecca Hwa, Dan Klein,
Russell Lee-Goldman, Slav Petrov, Dan Roth,
Josef Ruppenhofer, Amarnag Subramanya,
Partha Talukdar, and members of the ARK
group for helpful comments. This work was
supported by DARPA grant NBCH-1080004,
NSF grants IIS-0836431 and IIS-0915187,
Qatar National Research Foundation grant
NPRP 08-485-1-083, Google?s support of the
Worldly Knowledge Project at CMU,
computational resources provided by Yahoo,
and TeraGrid resources provided by the
Pittsburgh Supercomputing Center under
grant TG-DBS110003.
50
Das et al. Frame-Semantic Parsing
References
Auli, Michael and Adam Lopez. 2011.
A comparison of loopy belief propagation
and dual decomposition for integrated
CCG supertagging and parsing.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 470?480, Portland, OR.
Baker, Collin, Michael Ellsworth, and
Katrin Erk. 2007. SemEval-2007 task 19:
Frame semantic structure extraction.
In Proceedings of the Fourth International
Workshop on Semantic Evaluations,
pages 99?104, Prague.
Baluja, Shumeet, Rohan Seth, D. Sivakumar,
Yushi Jing, Jay Yagnik, Shankar Kumar,
Deepak Ravichandran, and Mohamed
Aly. 2008. Video suggestion and discovery
for YouTube: taking random walks
through the view graph. In Proceedings
of the 17th International Conference on
the World Wide Web, pages 895?904,
Beijing.
Bauer, Daniel and Owen Rambow. 2011.
Increasing coverage of syntactic
subcategorization patterns in FrameNet
using VerbNet. In Proceedings of the 2011
IEEE Fifth International Conference on
Semantic Computing, pages 181?184,
Washington, DC.
Bejan, Cosmin A. 2009. Learning Event
Structures From Text. Ph.D. thesis, The
University of Texas at Dallas.
Bengio, Yoshua, Olivier Delalleau, and
Nicolas Le Roux. 2006. Label propagation
and quadratic criterion. In Olivier
Chapelle, Bernhard Scho?lkopf, and
Alexander Zien, editors, Semi-Supervised
Learning. MIT Press, Cambridge, MA,
pages 193?216.
Boas, Hans C. 2002. Bilingual FrameNet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation, pages 1,364?1,371, Las Palmas.
Bottou, Le?on. 2004. Stochastic learning.
In Olivier Bousquet and Ulrike von
Luxburg, editors, Advanced Lectures on
Machine Learning, Lecture Notes in
Artificial Intelligence, LNAI 3176.
Springer Verlag, Berlin, pages 146?168.
Burbea, Jacob and Calyampudi R. Rao. 2006.
On the convexity of some divergence
measures based on entropy functions.
IEEE Transactions on Information Theory,
28(3):489?495.
Burchardt, Aljoscha, Katrin Erk, and
Anette Frank. 2005. A WordNet detour
to FrameNet. In Bernhard Fisseni,
Hans-Christian Schmitz, Bernhard
Schro?der, and Petra Wagner, editors,
Sprachtechnologie, mobile Kommunikation
und linguistische Resourcen, volume 8 of
Computer Studies in Language and Speech.
Peter Lang, Frankfurt am Main,
pages 408?421.
Burchardt, Aljoscha and Anette Frank. 2006.
Approaching textual entailment with LFG
and FrameNet frames. In Proceedings of the
Second PASCAL RTE Challenge Workshop,
pages 92?97, Venice.
Burchardt, Aljoscha, Marco Pennacchiotti,
Stefan Thater, and Manfred Pinkal. 2009.
Assessing the impact of frame semantics
on textual entailment. Natural Language
Engineering, 15(4):527?550.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning, pages 89?97,
Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning, pages 152?164,
Ann Arbor, MI.
Chang, Yin-Wen and Michael Collins.
2011. Exact decoding of phrase-based
translation models through Lagrangian
relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 26?37,
Edinburgh.
Chapelle, Olivier, Bernhard Scho?lkopf,
and Alexander Zien, editors. 2006.
Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Chen, Desai, Nathan Schneider, Dipanjan
Das, and Noah A. Smith. 2010. SEMAFOR:
Frame argument resolution with
log-linear models. In Proceedings of the
5th International Workshop on Semantic
Evaluation, pages 264?267, Upssala.
Corduneanu, Adrian and Tommi Jaakkola.
2003. On information regularization.
In Proceedings of the Nineteenth Conference
on Uncertainty in Artificial Intelligence,
pages 151?158, Acapulco.
Das, Dipanjan, Andre? F. T. Martins, and
Noah A. Smith. 2012. An exact dual
decomposition algorithm for shallow
semantic parsing with constraints.
In Proceedings of the First Joint Conference
on Lexical and Computational Semantics,
pages 209?217, Montre?al.
51
Computational Linguistics Volume 40, Number 1
Das, Dipanjan and Slav Petrov. 2011.
Unsupervised part-of-speech tagging
with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 600?609, Portland, OR.
Das, Dipanjan, Nathan Schneider, Desai
Chen, and Noah A. Smith. 2010.
Probabilistic frame-semantic parsing.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 948?956,
Los Angeles, CA.
Das, Dipanjan and Noah A. Smith. 2011.
Semi-supervised frame-semantic parsing
for unknown predicates. In Proceedings of
the 49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 1,435?1,444,
Portland, OR.
Das, Dipanjan and Noah A. Smith. 2012.
Graph-based lexicon expansion with
sparsity-inducing penalties. In Proceedings
of the Human Language Technologies
Conference of the North American Chapter
of the Association for Computational
Linguistics, pages 677?687, Montre?al.
Dean, Jeffrey and Sanjay Ghemawat. 2008.
MapReduce: Simplified data processing
on large clusters. Communications of the
ACM, 51(1):107?113.
DeNero, John and Klaus Macherey. 2011.
Model-based aligner combination using
dual decomposition. In Proceedings of
the 49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 420?429,
Portland, OR.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the Latent Words Language
Model. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 21?29, Singapore.
Dhillon, Paramveer S., Partha Pratim
Talukdar, and Koby Crammer. 2010.
Learning better data representation
using inference-driven metric
learning. In Proceedings of the ACL
2010 Conference Short Papers,
pages 377?381, Uppsala.
Erk, Katrin and Sebastian Pado?. 2006.
Shalmaneser?a toolchain for shallow
semantic parsing. In Proceedings of the
Fifth International Conference on Language
Resources and Evaluation, pages 527?532,
Genoa.
Fellbaum, Christiane, editor. 1998. WordNet:
an electronic lexical database. MIT Press,
Cambridge, MA.
Fillmore, Charles J. 1982. Frame semantics.
In Linguistics in the Morning Calm.
Hanshin Publishing Co., Seoul,
pages 111?137.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16.3:235?250.
Fleischman, Michael, Namhee Kwon, and
Eduard Hovy. 2003. Maximum entropy
models for FrameNet classification.
In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language
Processing, pages 49?56, Sapporo.
Fung, Pascale and Benfeng Chen. 2004.
BiFrameNet: Bilingual frame semantics
resource construction by cross-lingual
induction. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 931?937, Geneva.
Fu?rstenau, Hagen and Mirella Lapata. 2009a.
Graph alignment for semi-supervised
semantic role labeling. In Proceedings of the
2009 Conference on Empirical Methods in
Natural Language Processing, pages 11?20,
Singapore.
Fu?rstenau, Hagen and Mirella Lapata. 2009b.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Fu?rstenau, Hagen and Mirella Lapata. 2012.
Semi-supervised semantic role labeling
via structural alignment. Computational
Linguistics, 38(1):135?171.
Gerber, Matthew and Joyce Chai. 2010.
Beyond NomBank: A study of implicit
arguments for nominal predicates. In
Proceedings of ACL, pages 1,583?1,592,
Uppsala.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Girju, Roxana, Preslav Nakov, Vivi Nastase,
Stan Szpakowicz, Peter Turney, and Deniz
Yuret. 2007. SemEval-2007 task 04:
Classification of semantic relations
between nominals. In Proceedings of the
Fourth International Workshop on Semantic
Evaluations, pages 13?18, Prague.
Giuglea, Ana-Maria and Alessandro
Moschitti. 2006. Shallow semantic
parsing based on FrameNet, VerbNet
and PropBank. In Proceedings of the
17th European Conference on Artificial
Intelligence, pages 563?567, Amsterdam.
52
Das et al. Frame-Semantic Parsing
Gropp, W., E. Lusk, and A. Skjellum. 1994.
Using MPI: Portable Parallel Programming
with the Message-Passing Interface.
MIT Press, Cambridge, MA.
Hajic?, Jan, Massimiliano Ciaramita,
Richard Johansson, Daisuke Kawahara,
Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai
Surdeanu, Nianwen Xue, and Yi Zhang.
2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in
multiple languages. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning, pages 1?18,
Boulder, CO.
Ide, Nancy and Jean Ve?ronis. 1998.
Introduction to the special issue on word
sense disambiguation: The state of the art.
Computational Linguistics, 24(1):2?40.
Johansson, Richard and Pierre Nugues.
2007. LTH: Semantic structure extraction
using nonprojective dependency trees.
In Proceedings of the 4th International
Workshop on Semantic Evaluations,
pages 227?230, Prague.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based semantic role labeling
of PropBank. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 69?78,
Honolulu, HI.
Kingsbury, Paul and Martha Palmer. 2002.
From TreeBank to PropBank. In Proceedings
of the 3rd International Conference on
Language Resources and Evaluation,
pages 1,989?1,993, Las Palmas.
Komodakis, Nikos, Nikos Paragios, and
Georgios Tziritas. 2007. MRF optimization
via dual decomposition: Message-passing
revisited. In Eleventh International
Conference on Computer Vision, pages 1?8,
Rio de Janeiro.
Koo, Terry, Alexander M. Rush, Michael
Collins, Tommi Jaakkola, and David
Sontag. 2010. Dual decomposition for
parsing with non-projective head
automata. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,288?1,298,
Cambridge, MA.
Kowalski, Matthieu and Bruno Torre?sani.
2009. Sparsity and persistence: Mixed
norms provide simple signal models with
dependent coefficients. Signal, Image and
Video Processing, 3:251?264.
Lang, Joel and Mirella Lapata. 2010.
Unsupervised induction of semantic roles.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 939?947,
Los Angeles, CA.
Lang, Joel and Mirella Lapata. 2011.
Unsupervised semantic role induction
with graph partitioning. In Proceedings
of the 2011 Conference on Empirical
Methods in Natural Language Processing,
pages 1320?1331, Edinburgh.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 112?120,
Columbus, OH.
Lin, Dekang. 1994. Principar?an efficient,
broad-coverage, principle-based parser.
In Proceedings of the 15th Conference on
Computational Linguistics, pages 482?488,
Kyoto.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 768?774, Montreal.
Lin, Jianhua. 1991. Divergence measures
based on the Shannon entropy. IEEE
Transactions on Information Theory,
37(1):145?151.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory BFGS method for
large scale optimization. Mathematical
Programming, 45(3):503?528.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: the Penn treebank. Computational
Linguistics, 19(2):313?330, June.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: an introduction to
the special issue. Computational Linguistics,
34(2):145?159, June.
Martins, Andre? F. T., Mario A. T. Figueiredo,
Pedro M. Q. Aguiar, Noah A. Smith, and
Eric P. Xing. 2011a. An augmented
Lagrangian approach to constrained MAP
inference. In Proceedings of the 28th
International Conference on Machine
Learning, pages 169?176, Bellevue, WA.
Martins, Andre? F. T., Noah A. Smith, Pedro
M. Q. Aguiar, and Mario A. T. Figueiredo.
53
Computational Linguistics Volume 40, Number 1
2011b. Dual decomposition with many
overlapping components. In Proceedings
of the 2011 Conference on Empirical
Methods in Natural Language Processing,
pages 238?249, Edinburgh.
Martins, Andre? F. T., Noah A. Smith, and
Eric P. Xing. 2009. Concise integer
linear programming formulations for
dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 342?350,
Suntec.
Martins, Andre? F. T., Noah A. Smith, Eric P.
Xing, Mario A. T. Figueiredo, and Pedro
M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate
variational inference. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 34?44,
Cambridge, MA.
Matsubayashi, Yuichiroh, Naoaki Okazaki,
and Jun?ichi Tsujii. 2009. A comparative
study on generalization of semantic roles
in FrameNet. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
Association for Computational Linguistics and
the 4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 19?27, Suntec.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 91?98, Ann Arbor, MI.
Meyers, Adam, Ruth Reeves, Catherine
Macleod, Rachel Szekely, Veronika
Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project:
An interim report. In Proceedings of the
NAACL-HLT Workshop on Frontiers
in Corpus Annotation, pages 24?31,
Boston, MA.
Moschitti, Alessandro, Paul Morarescu, and
Sanda M. Harabagiu. 2003. Open-domain
information extraction via automatic
semantic labeling. In Ingrid Russell and
Susan M. Haller, editors, Proceedings of the
Sixteenth International Florida Artificial
Intelligence Research Society Conference,
pages 397?401, St. Augustine, FL.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings of
the 20th International Conference on
Computational Linguistics, Geneva.
Pado?, Sebastian and Katrin Erk. 2005.
To cause or not to cause: cross-lingual
semantic matching for paraphrase
modelling. In Proceedings of the
Cross-Language Knowledge Induction
Workshop, Cluj-Napoca.
Pado, Sebastian and Mirella Lapata. 2005.
Cross-linguistic projection of role-semantic
information. In Proceedings of Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 859?866,
Vancouver.
Pennacchiotti, Marco, Diego De Cao, Roberto
Basili, Danilo Croce, and Michael Roth.
2008. Automatic induction of FrameNet
lexical units. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 457?465,
Honolulu, HI.
Pradhan, Sameer S., Wayne H. Ward,
Kadri Hacioglu, James H. Martin, and
Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines.
In Proceedings of the Human Language
Technologies Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 233?240,
Boston, MA.
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2008. The importance of syntactic
parsing and inference in semantic role
labeling. Computational Linguistics,
34(2):257?287.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings of the 20th
International Conference on Computational
Linguistics, pages 1,346?1,352, Geneva.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the 1996 Empirical
Methods in Natural Language Processing,
pages 133?142, Copenhagen.
Riedel, Sebastian and James Clarke. 2006.
Incremental integer linear programming
for non-projective dependency parsing.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 129?137, Sydney.
Roth, Dan and Wen-tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of the Eighth Conference on
Computational Natural Language Learning,
pages 1?8, Boston, MA.
Ruppenhofer, Josef, Michael Ellsworth,
Miriam R. L. Petruck, Christopher R.
54
Das et al. Frame-Semantic Parsing
Johnson, and Jan Scheffczyk. 2006.
FrameNet II: extended theory and
practice. International Computer Science
Institute, Berkeley, CA.
Rush, Alexander M. and Michael Collins.
2011. Exact decoding of syntactic
translation models through Lagrangian
relaxation. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 72?82, Portland, OR.
Rush, Alexander M, David Sontag, Michael
Collins, and Tommi Jaakkola. 2010.
On dual decomposition and linear
programming relaxations for natural
language processing. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 1?11,
Cambridge, MA.
Schuler, Karin K. 2005. VerbNet: a
broad-coverage, comprehensive verb lexicon.
Ph.D. thesis, University of Pennsylvania.
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields.
In Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 134?141, Edmonton.
Shen, Dan and Mirella Lapata. 2007. Using
semantic roles to improve question
answering. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 12?21,
Prague.
Shi, Lei and Rada Mihalcea. 2004. An
algorithm for open text semantic parsing.
In Proceedings of Workshop on Robust
Methods in Analysis of Natural Language
Data, pages 59?67, Geneva.
Shi, Lei and Rada Mihalcea. 2005. Putting
pieces together: Combining FrameNet,
VerbNet and WordNet for robust
semantic parsing. In Proceedings of the 6th
International Conference on Computational
Linguistics and Intelligent Text Processing,
pages 100?111, Mexico City.
Smith, David A. and Jason Eisner. 2008.
Dependency parsing by belief
propagation. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 145?156,
Honolulu, HI.
Subramanya, Amarnag and Jeff Bilmes.
2008. Soft-supervised learning for text
classification. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 1,090?1,099,
Honolulu, HI.
Subramanya, Amarnag, Slav Petrov, and
Fernando Pereira. 2010. Efficient
graph-based semi-supervised learning of
structured tagging models. In Proceedings
of the 2010 Conference on Empirical Methods
in Natural Language Processing,
pages 167?176, Cambridge, MA.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting on Association for
Computational Linguistics, pages 8?15,
Sapporo.
Surdeanu, Mihai, Richard Johansson,
Adam Meyers, Llu??s Ma`rquez, and Joakim
Nivre. 2008. The CoNLL 2008 shared task
on joint parsing of syntactic and semantic
dependencies. In Proceedings of the Twelfth
Conference on Computational Natural
Language Learning, pages 159?177,
Manchester.
Szummer, Martin and Tommi Jaakkola.
2001. Partially labeled classification with
Markov random walks. In Advances in
Neural Information Processing Systems 14,
pages 945?952, Vancouver.
Talukdar, Partha Pratim and Koby Crammer.
2009. New regularized algorithms for
transductive learning. In Proceedings of the
European Conference on Machine Learning
and Knowledge Discovery in Databases,
pages 442?457, Bled.
Thompson, Cynthia A., Roger Levy,
and Christopher D. Manning. 2003.
A generative model for semantic role
labeling. In Proceedings of the European
Conference on Machine Learning,
pages 397?408, Cavtat-Dubrovnik.
Titov, Ivan and Alexandre Klementiev. 2012.
A Bayesian approach to unsupervised
semantic role induction. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 12?22, Avignon.
Toutanova, Kristina, Aria Haghighi,
and Christopher Manning. 2005.
Joint learning improves semantic
role labeling. In Proceedings of the
43rd Annual Meeting of the Association
for Computational Linguistics,
pages 589?596, Ann Arbor, MI.
Turian, Joseph, Lev-Arie Ratinov,
and Yoshua Bengio. 2010. Word
representations: A simple and general
method for semi-supervised learning.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 384?394, Uppsala.
55
Computational Linguistics Volume 40, Number 1
Weston, Jason, Fre?de?ric Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing, pages 88?94,
Barcelona.
Yi, Szu-ting, Edward Loper, and Martha
Palmer. 2007. Can semantic roles
generalize across genres? In Proceedings of
the Human Language Technologies Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 548?555, Rochester, NY.
Zhu, Xiaojin. 2008. Semi-supervised
learning literature survey. Available at
http://pages.cs.wisc.edu/?jerryzhu/
pub/ssl survey.pdf. Last Accessed
July 2013.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In Proceedings of the
20th International Conference on Machine
Learning, pages 912?919, Washington, DC.
56
Phrase Dependency Machine
Translation with Quasi-Synchronous
Tree-to-Tree Features
Kevin Gimpel?
Toyota Technological Institute
at Chicago
Noah A. Smith??
Carnegie Mellon University
Recent research has shown clear improvement in translation quality by exploiting linguistic
syntax for either the source or target language. However, when using syntax for both languages
(?tree-to-tree? translation), there is evidence that syntactic divergence can hamper the extraction
of useful rules (Ding and Palmer 2005). Smith and Eisner (2006) introduced quasi-synchronous
grammar, a formalism that treats non-isomorphic structure softly using features rather than
hard constraints. Although a natural fit for translation modeling, its flexibility has proved
challenging for building real-world systems. In this article, we present a tree-to-tree machine
translation system inspired by quasi-synchronous grammar. The core of our approach is a new
model that combines phrases and dependency syntax, integrating the advantages of phrase-based
and syntax-based translation. We report statistically significant improvements over a phrase-
based baseline on five of seven test sets across four language pairs. We also present encouraging
preliminary results on the use of unsupervised dependency parsing for syntax-based machine
translation.
1. Introduction
Building translation systems for many language pairs requires addressing a wide range
of translation divergence phenomena. Several researchers have studied divergence be-
tween languages in corpora and found it to be considerable, even for closely related
languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; S?gaard
and Kuhn 2009). To address this, many have incorporated linguistic syntax into trans-
lation model design. The statistical natural language processing (NLP) community
has developed automatic parsers that can produce syntactic analyses for sentences in
several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al.
? Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu.
?? School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: nasmith@cs.cmu.edu.
Submission received: 10 November 2012; revised submission received: 12 May 2013; accepted for publication:
23 June 2013.
doi:10.1162/COLI a 00175
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
2007). The availability of these parsers, and gains in their accuracy, triggered research
interest in syntax-based statistical machine translation (Yamada and Knight 2001).
Syntax-based translation models are diverse, using different grammatical for-
malisms and features. Some use a parse tree for the source sentence (?tree-to-string?),
others produce a parse when generating the target sentence (?string-to-tree?), and
others combine both (?tree-to-tree?). We focus on the final category in this article.
Tree-to-tree translation has proved to be a difficult modeling problem, as initial at-
tempts at it underperformed systems that used no syntax at all (Cowan, Kuc?erova?,
and Collins 2006; Ambati and Lavie 2008; Liu, Lu?, and Liu 2009). Subsequent re-
search showed that substantial performance gains can be achieved if hard constraints?
specifically, isomorphism between a source sentence?s parse and the parse of its
translation?are relaxed (Liu, Lu?, and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong
2011; Hanneman and Lavie 2011). This suggests that constraints must be handled
with care.
Yet the classic approach to tree-to-tree translation imposes hard constraints through
the use of synchronous grammars developed for programming language compilation
(Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously:
one in the source language and one in the target language. A single derivation is
used for both strings, which limits the divergence phenomena that can be captured.
As a result, researchers have developed synchronous grammars with larger rules
that, rule-internally, capture more phenomena, typically at increased computational
expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005).
We take a different approach. We take inspiration from a family of formalisms
called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous
grammar, QG assumes the entire input sentence and some syntactic parse of it are
provided and fixed. QG then defines a monolingual grammar whose language is a
set of translations inspired by the input sentence and tree. The productions in this
monolingual grammar generate a piece of the translation?s tree and align it to a piece of
the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible between
the two trees. A weighted QG uses feature functions to softly penalize or encourage
particular types of syntactic divergence.
In this article, we present a statistical tree-to-tree machine translation system in-
spired by quasi-synchronous grammar. We exploit the flexibility of QG to develop a
new syntactic translation model that seeks to combine the benefits of both phrase-based
and syntax-based translation. Our model organizes phrases into a tree structure inspired
by dependency syntax (Tesnie`re 1959). Instead of standard dependency trees in which
words are vertices, our trees have phrases as vertices. The result captures phenomena
like local reordering and idiomatic translations within phrases, as well as long-distance
relationships among the phrases in a sentence. We use the term phrase dependency tree
when referring to this type of dependency tree; phrase dependencies have also been
used by Wu et al. (2009) for opinion mining and previously for machine translation by
Hunter and Resnik (2010). Because we combine phrase dependencies with features from
quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase
dependency (QPD) translation model.
Our tree-to-tree approach requires parsers for both the source and target languages.
For two of the language pairs we consider (Chinese?English and German?English),
treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank;
Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical
parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and
Xing 2009). We also want to apply our model to languages that do not have tree-
350
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The
NLP community has developed a range of statistical algorithms for building unsuper-
vised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010;
Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require
only raw, unannotated text in the language of interest, making them ideal for use in
translation.
Unsupervised shallow syntactic analysis has been used successfully for translation
modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech
tags could be used to label the hierarchical translation rules of Chiang (2005) to match
the performance of a system that uses supervised full syntactic parses. We take addi-
tional steps in this direction, leveraging state-of-the-art unsupervised models for full
syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and
Smith 2012a) to obtain improvements in translation quality. We find that replacing a
supervised parser for Chinese with an unsupervised one has no effect on performance,
and using an unsupervised English parser only hurts slightly. We use unsupervised
parsing to apply our full model to Urdu?English and English?Malagasy translation,
reporting statistically significant improvements over our baselines. These initial results
offer promise for researchers to apply syntactic translation models to the thousands of
languages for which we do not have manually annotated corpora, and naturally suggest
future research directions.
The rest of this article is laid out as follows. In Section 2, we discuss quasi-
synchronous grammar and dependency syntax and motivate our modeling choices.
We present our translation model in Section 3, describe how we extract rules in Sec-
tion 4, and list our feature functions in Section 5. Decoding algorithms are given in
Section 6. We present experiments measuring our system?s performance on translation
tasks involving four language pairs and several test sets in Section 7. We find statistically
significant improvements over a strong phrase-based baseline on five out of seven test
sets across four language pairs. We also perform a human evaluation to study how our
system improves translation quality. This article is a significantly expanded version of
Gimpel and Smith (2011), containing additional features, a new decoding algorithm,
and a more thorough experimental evaluation. It presents key material from Gimpel
(2012), to which readers seeking further details are referred.
2. Background and Motivation
We begin by laying groundwork for the rest of the article. We define notation in
Section 2.1. Section 2.2 discusses how synchronous and quasi-synchronous grammar
handle syntactic divergence. In Section 2.3, we introduce dependency syntax and review
prior work that has used it for machine translation. Section 2.4 presents two examples
of syntactic divergence that motivate the model we develop in Section 3.
2.1 Notation
We use boldface for vectors and we denote individual elements in vectors using sub-
scripts; for example, the source and target sentences are denoted x = ?x1, . . . , xn? and
y = ?y1, . . . , ym?. We denote sequences of elements in vectors using subscripts and su-
perscripts; for example, the sequence from source word i to source word j (inclusive)
is denoted xji, and therefore x
i
i = ?xi?. We denote the set containing the first k positive
integers as [k]. This notation is summarized in Table 1.
351
Computational Linguistics Volume 40, Number 2
Table 1
Notation used in this article.
i, j, k, l integers
x,y vectors
xi entry i in vector x
xji sequence from entry i to entry j (inclusive) in vector x
[i] the set containing the first i positive integers
|x| length of vector x
2.2 Synchronous and Quasi-Synchronous Grammars
To model syntactic transformations, researchers have developed powerful grammat-
ical formalisms, many of which are variations of synchronous grammars. The most
widely used is synchronous context-free grammar (Wu 1997; Gildea 2003; Chiang 2005;
Melamed 2003), an extension of context-free grammar to a bilingual setting where two
strings are generated simultaneously with a single derivation. Synchronous context-free
grammars are computationally attractive but researchers have shown that they cannot
handle certain phenomena in manually aligned parallel data (Wellington, Waxmonsky,
and Melamed 2006; S?gaard and Kuhn 2009). Figure 1 shows two such examples of
word alignment patterns in German?English data. These patterns were called ?cross-
serial discontinuous translation units? (CDTUs) by S?gaard and Kuhn (2009). CDTUs
cannot even be handled by the more sophisticated synchronous formalisms given by
Eisner (2003) and Ding and Palmer (2005). CDTUs can be handled by synchronous
tree adjoining grammar (STAG; Shieber and Schabes 1990), but STAG comes with sub-
stantially heftier computational requirements. Furthermore, S?gaard and Kuhn (2009)
found examples in parallel data that even STAG cannot handle.
Smith and Eisner (2006) noted that these limitations of synchronous grammars
result from an emphasis on generating the two strings. However, for many real-world
applications, such as translation, one of the sentences is provided. The model only needs
to score translations of the given source sentence, not provide a generative account for
sentence pairs. Smith and Eisner proposed an alternative to synchronous grammar?
quasi-synchronous grammar (QG)?that exploits this fact for increased flexibility in
translation modeling. A QG assumes the source sentence and a parse are given and
scores possible translations of the source sentence along with their parses. That is, a
quasi-synchronous grammar is a monolingual grammar that derives strings in the target
language. The strings? derivations are scored using feature functions on an alignment
from nodes in the target tree to nodes in the source tree. The quasi-synchronous depen-
dency grammars of Smith and Eisner (2006) and Gimpel and Smith (2009b) can generate
the translations in Figure 1, as can phrase-based models like Moses (Koehn et al. 2007)
and the phrase dependency model we present in Section 3.
wir  durchleben  keine  wiederholung  des  jahres  1938  . 
          we  are  not  living  a  replay  of  1938  . 
wir  wollen  keinen  .
we  do  not  want  one  . 
Figure 1
Examples of word alignment patterns in German?English that require the increased expressive
power of synchronous tree adjoining grammar.
352
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Quasi-synchronous grammar, like synchronous grammar, can in principle be in-
stantiated for a wide range of formalisms. Dependency syntax (which we discuss in
Section 2.3) has been used in most previous applications of QG, including word align-
ment (Smith and Eisner 2006) and machine translation (Gimpel and Smith 2009b). Aside
from translation, QG has been used for a variety of applications involving relationships
among sentences, including question answering (Wang, Smith, and Mitamura 2007),
paraphrase identification (Das and Smith 2009), parser projection and adaptation (Smith
and Eisner 2009), title generation (Woodsend, Feng, and Lapata 2010), sentence sim-
plification (Woodsend and Lapata 2011), information retrieval (Park, Croft, and Smith
2011), and supervised parsing from multiple treebanks with different annotation
conventions (Li, Liu, and Che 2012).
2.3 Dependency Syntax and Machine Translation
Many syntactic theories have been applied to translation modeling, but we focus in
this article on dependency syntax (Tesnie`re 1959). Dependency syntax is a lightweight
formalism that builds trees consisting of a set of directed arcs from words to their
syntactic heads (also called ?parents?). Examples of dependency trees are shown in
Figure 2. Each word has exactly one parent, and $ is a special ?wall? symbol that is
located at position 0 in the sentence and acts as parent to words that have no other
parent in the sentence. Formally, a dependency tree on an m-word sentence y is a
function ?y : {1, . . . , m} ? {0, . . . , m} where ?y (i) is the index of the parent of word
yi. If ?y (i) = 0, we say word yi is a root of the tree. The function ?y is not permitted
to have cycles. We restrict our attention to projective dependency trees in this article.
Projective dependency trees are informally defined as having no crossing arcs when all
dependencies are drawn on one side of the sentence. See Ku?bler, McDonald, and Nivre
(2009) for formal definitions of these terms.
Researchers have shown that dependency trees are better preserved when pro-
jecting across word alignments than phrase structure trees (Fox 2002). This makes
dependency syntax appealing for translation modeling, but to date there are not many
tree-to-tree translation models that use dependency syntax on both sides. One exception
is the system of Ding and Palmer (2005), who used a synchronous tree substitution
grammar designed for dependency syntax, capturing non-isomorphic structure within
rules using elementary trees. Another is the system of Riezler and Maxwell III (2006),
who used lexical-functional dependency trees on both sides and also include phrase
translation rules. Relatedly, Quirk, Menezes, and Cherry (2005) used a source-side
dependency parser and projected automatic parses across word alignments in order
to model dependency syntax on phrase pairs.
$  konnten  sie  es  ?bersetzen  ?
$  could  you  translate  it  ?
Figure 2
Examples of dependency trees with word alignment. Arrows are drawn from children to
parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a
special ?wall? symbol that serves as the parent of all root words in the tree (i.e., those with
no other parent).
353
Computational Linguistics Volume 40, Number 2
But most who have used dependency syntax have done so either on the source side
in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or
the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and
Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al.
2010). Others have added features derived from source dependency parses to phrase-
based or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao,
Koehn, and Birch 2011).
2.4 Motivating Examples
Although Fox (2002) found that dependencies are more often preserved across hand-
aligned bitext than constituents, there are still several concerns when using dependency
syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for
small data sets and few language pairs, so in practice we must deal with the noise in
automatic word aligners and parsers. Second, not all dependencies are preserved in
hand-aligned data, so we would need to be able to handle non-isomorphic structure
even if we did have perfect tools. The model we present in Section 3 avoids isomor-
phism constraints from synchronous grammar and encourages dependency preserva-
tion across languages by using dependencies on phrases?flat multi-word units?rather
than words.
To motivate these choices, we now give two frequently occurring examples of de-
pendency tree-to-tree divergence in German?English data.1 We consider the German?
English parallel corpus used in our experiments (and described in Appendix A). We
parsed the English side using TurboParser (Martins et al. 2010), a state-of-the-art depen-
dency parser. TurboParser was trained on the Penn Treebank (Marcus, Santorini, and
Marcinkiewicz 1993) converted to dependencies using the Yamada-Matsumoto head
rules (Yamada and Matsumoto 2003). We parsed the German side using the factored
model in the Stanford parser (Rafferty and Manning 2008), which is trained from
the NEGRA phrase-structure treebank (Skut et al. 1997). The Stanford parser?s source
code defines a set of head rules for converting the phrase-structure parse output to
dependencies.2
The first example is shown in Figure 3. The bold words illustrate a ?sibling? rela-
tionship, meaning that the source words aligned to the parent and child in the English
sentence have the same parent on the German side. Many sibling configurations appear
when the English dependency is DET?N within a PP. By convention, the NEGRA
treebank uses flat structures for PPs like ?P DET N? rather than using a separate NP
for DET N. When the parser converts this to a dependency tree, the DET and N are made
children of the P. In English dependency parsing, due to the Penn Treebank conventions,
the DET is made a child of the N, which is a child of the P. There are many other instances
like this one that frequently lie within PPs, like the?us and recent?years. However, if
we tokenized the us as a phrase and also den usa, then both would be children of the
preposition, and the dependency would be preserved.
1 The study that uncovered these examples is detailed in Gimpel (2012). It gives evidence of frequent
non-isomorphic dependency structure between German and English with automatic word aligners
and parsers.
2 These rules use comparable conventions to the Yamada-Matsumoto head rules for English (modulo
the differences in languages and tag/label sets): finite verbs are sentence roots, adpositions are heads
of adpositional phrases, nouns are heads of noun phrases, and so forth.
354
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
$  auch  die mietm?rkte  in den  usa  sind  flexibler
$  rental  markets  are  also  more  flexible  in  the  us
Figure 3
Example of a sentence pair containing a frequently-observed ?sibling? relationship in
German?English data: in the the?us dependency, the aligned German words are siblings
in the source dependency tree. This occurs due to differences in treebank and head rule
conventions between the two data sets. The German parser produces flat PPs with little
internal structure, so when the dependency tree is generated, each word in the PP attaches
to the P, the head of the phrase.
The second example is shown in Figure 4, which gives an example of a
?grandparent-grandchild? relationship. In the English dependency until?recently, the
aligned source words are in a grandparent relationship in the source sentence?s depen-
dency tree. We note, however, that if vor kurzem is tokenized as a phrase, then we might
let the entire phrase be the child of bis, preserving the dependency across languages.
By considering phrasal structure and dependencies among phrases, we can reduce
some of the syntactic divergence in real-world data. The model we develop in the next
section is based on this idea.
3. Model
In the previous section we noted two examples in which flattening dependency tree
structure into ?phrasal dependencies? could improve dependency preservation be-
tween German and English. This idea is compatible with the well-known principle that
translation quality is improved when larger units are modeled within translation rules.
For example, improvements were found by moving from word-based models to so-
called phrase-based translation models. Modern phrase-based translation systems are
typified by the Moses system (Koehn et al. 2007), based on the approach presented by
Koehn, Och, and Marcu (2003). Phrase-based models excel at capturing local reordering
phenomena and memorizing multi-word translations.
$  bis vor kurzem hielten sich beide seiten an diesen stillschweigenden vertrag
$  until recently , both sides adhered to this tacit contract
Figure 4
Example of a sentence pair containing a frequently-observed ?grandparent-grandchild?
relationship in German?English data: the English parent and child words in the until?recently
dependency are aligned to German words in a grandparent-grandchild relationship.
355
Computational Linguistics Volume 40, Number 2
On the other hand, models that use rules employing syntax (Yamada and Knight
2001) or syntax-like representations (Chiang 2005) handle long-distance reordering
better than phrase-based systems (Birch, Blunsom, and Osborne 2009), and therefore
perform better for certain language pairs (Zollmann et al. 2008). In order to better handle
syntactic divergence and obtain the benefits of these two types of models, we use rules
that combine phrases and syntax. In particular, our rules use dependencies between
phrases rather than words; we call them phrase dependencies. When adding in source
syntax, we eschew the constraints of synchronous grammar in favor of the feature-based
approach of quasi-synchronous grammar. So we call our model a quasi-synchronous
phrase dependency (QPD) translation model.
In Section 3.1, we define phrase dependency trees and in Section 3.2 we present
our model. We discuss rule extraction in Section 4 and define the feature functions in
the model in Section 5. Decoding is discussed in Section 6 and an empirical evaluation
is given in Section 7. Key definitions used throughout this section and the remaining
sections are listed in Table 2.
3.1 Phrase Dependencies
In Section 2.3 we defined dependency trees. Now we provide an analogous definition
for phrase dependency trees. We first define a segmentation of a sentence into phrases.
Given a sentence y, where m = |y|, we define a phrase? as a word sequence ykj , for j and
Table 2
Key definitions for our model.
x = ?x1, . . . , xn? source language sentence
y = ?y1, . . . , ym? target language sentence, translation of x
pi = xkj source-sentence phrase: subsequence of words in the source
sentence x, i.e., 1 ? j ? k ? n; the number of words in pi is |pi|
? = ykj target-sentence phrase: subsequence of words in the target
sentence y, i.e., 1 ? j ? k ? m
pi = ?pi1, . . . ,pin?? segmentation of x into phrases such that for i ? [n
?], pii = xkj is
a source-sentence phrase and pi1 ? . . . ? pin? = x
? = ??1, . . . ,?n?? segmentation of y into phrases such that for i ? [n
?], ?i = ykj is
a target-sentence phrase and ?1 ? . . . ??n? = y
b : {1, . . . , n?} ? {1, . . . , n?} one-to-one alignment (bijection) from phrases in ? to phrases
in pi; for all i ? [n?], if b(i) = j, then pij is a subsequence of x and
?i is a subsequence of y
?x : {1, . . . , n} ? {0, . . . , n} dependency tree on source words x, where ?x (i) is the index
of the parent of word xi (0 is the wall symbol $)
?? : {1, . . . , n?} ? {0, . . . , n?} dependency tree on target phrases ?, where ??(i) is the index
of the parent of phrase ?i (0 is the wall symbol $)
h = ?h?,h??? vector of feature functions; h? holds the Moses feature func-
tions and h?? holds the QPD feature functions
? = ???,???? vector of feature weights for h
356
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
k such that 1 ? j ? k ? m. The number of words in phrase ? is denoted |?|. We define a
phrase segmentation of y as ? = ??1, . . . ,?n?? such that for i ? [n?], ?i = ykj is a phrase
and ?1 ? . . . ??n? = y, where ? denotes string concatenation.
Given a phrase segmentation ?, we define a phrase dependency tree as a function
?? : [n?]? {0} ? [n?] where ??(i) is the index of the parent of phrase?i. If ??(i) = 0, we
say phrase ?i is the root of the phrase dependency tree; we require there to be exactly
one root phrase. As with dependency trees, ?? cannot have cycles.3 To distinguish
phrase dependency trees from the ordinary dependency trees defined in Section 2.3,
we will sometimes refer to the latter as ?lexical dependency trees.?
Phrase dependency trees have also been used by Wu et al. (2009) to extract features
for opinion mining and a similar formalism was used previously for machine translation
by Hunter and Resnik (2010). Phrase dependencies allow us to capture phenomena like
local reordering and idiomatic translations within each phrase as well as longer-distance
relationships among the phrases in a sentence.
3.2 Quasi-Synchronous Phrase Dependency Translation
Let X denote the set of all strings in a source language and, for a particular x ? X , let Yx
denote the set of its possible translations (correct and incorrect) in the target language.
Given a sentence x and its lexical dependency tree ?x , we formulate the translation
problem as finding the target sentence y?, the phrase segmentation pi? of x, the phrase
segmentation ?? of y?, the phrase dependency tree ??? on the target phrases ?
?, and the
one-to-one phrase alignment b? such that
?y?,pi?,??, ???,b
?? = argmax
?y,pi,?,??,b?
? ? h(x, ?x ,y,pi,?, ??,b) (1)
where h is a vector of feature functions and ? is a vector of feature weights. The source-
language dependency parse ?x is optional and can be omitted if no source dependency
parser is available. If ?x is provided, we include tree-to-tree configurational features
from QG, which are described in Section 5.3. Hence we call the model defined in
Equation (1) a quasi-synchronous phrase dependency (QPD) translation model.
Our model extends the phrase-based translation model of Koehn, Och, and Marcu
(2003). The phrase segmentation variables ? and the one-to-one phrase alignment b :
[n?]? [n?] are taken directly from phrase-based translation. For all i ? [n?], if b(i) = j,
then pij is a subvector of x and ?i is a subvector of y. If ?x is not given and the features
ignore ??, then the remaining variables (x, y, pi, ?, and b) are defined in the same way
as in phrase-based models.
Computational tractability requires that the feature functions h decompose across
?parts? of the output structures in the model. The feature functions that look only at
the phrase-based variables (x, y, pi, ?, and b) are identical to the features used in the
Moses phrase-based system (Koehn et al. 2007), so they decompose in the same way
as in Moses.4 For clarity, we partition the features and weights into two parts, namely,
? = ???,???? and h = ?h?,h???, where ?? are the weights for the phrase-based features h?
3 Further, we restrict our attention to projective phrase dependency trees in this article. We conjecture that
non-projective trees may be a better fit for translation modeling (Carreras and Collins 2009; Galley and
Manning 2009), particularly for certain language pairs, but we leave their exploration for future work.
4 A more detailed discussion of how the Moses features decompose can be found in Gimpel (2012).
357
Computational Linguistics Volume 40, Number 2
and ??? are the weights for the QPD features h??. So we rewrite the right-hand side of
Equation (1) as the following:
argmax
?y,pi,?,??,b?
?? ? h?(x,y,pi,?,b) + ??? ? h??(x, ?x ,y,pi,?, ??,b) (2)
Furthermore, we assume an additive decomposition across individual phrase depen-
dencies in the phrase dependency tree ??, allowing us to rewrite Equation (2) as
argmax
?y,pi,?,??,b?
?? ? h?(x,y,pi,?,b)
+
n??
i=1
??? ? f (x, ?x , i, ??(i),?i,???(i),b(i),b(??(i)),pib(i),pib(??(i))) (3)
where we introduce new notation f to represent the feature vector that operates on a
single phrase dependency at a time in the ?arc-factored? decomposition of h??. Each
feature in f can look at the entirety of x and ?x because they are inputs, but can only
look at a single target-side phrase dependency ??i,???(i)? at a time (along with their
aligned source phrases pib(i) and pib(??(i)) and the indices).
Example. Figure 5 shows an example. The inputs to the model are a segmented Chi-
nese sentence and its lexical dependency tree. We used the Stanford Chinese word
segmenter (Chang, Galley, and Manning 2008) to segment the Chinese data and the
Stanford parser (Levy and Manning 2003) to get Chinese dependency trees. The outputs
references: annan to hold talks with us , russia and eu over situation in middle east
annan will discuss middle east situation with u.s. , russia and european union
annan to discuss mideast situation with us , russia and eu
annan to meeting the us , russia and eu to discuss middle east crisis
annan  will  hold talks  with  the united states  , russia and  the european union  to discuss  the middle east  situation$
$
Figure 5
Example output of our model for Chinese?English translation. The word-segmented Chinese
sentence and dependency tree are inputs. Our model?s outputs include the English translation,
phrase segmentations for each sentence (a box surrounds each phrase), a one-to-one alignment
between the English and Chinese phrases, and a projective dependency tree on the English
phrases. Note that the Chinese dependency tree is on words whereas the English dependency
tree is on phrases.
358
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
of the model include a segmentation of the Chinese sentence into phrases, the English
translation, its segmentation into phrases, a projective dependency tree on the English
phrases, and a one-to-one alignment between the English phrases and Chinese phrases.
Four reference translations are also shown. In this example, the model correctly moved
the phrase hold talks and also noted its connection to to discuss by making the latter a
phrasal dependent.
4. Rule Extraction
In typical statistical machine translation (SMT) models, the space of allowable trans-
lations is constrained by a set of rules. Informally, a rule consumes part of the input
text and emits text in the output language. Building an SMT system typically requires
collecting a massive set of rules from parallel text, a process called rule extraction.
For phrase-based translation, these rules are phrase pairs and the translation space
is constrained by the phrase pairs in the phrase table.5 In our model, even though
we have additional structure (i.e., the phrase dependency tree ??), we do not want
to enforce any additional constraints on the search space. That is, the space of valid
translations is still constrained solely by a standard phrase table. We allow ?? to be
any projective phrase dependency tree on ?, so the structure of ?? merely affects how
translations are scored, not what translations are permitted. We made this decision
because we did not want to reduce the coverage of phrase-based models, which is one
of their strengths. Rather, we wanted to better score their translations.6
So, even though our phrase dependency rules do not consume parts of the input,
we still speak in terms of ?rule extraction? because our procedure is similar to rule
extraction in other systems and we define feature functions on our rules in a standard
way. In particular, we use the extracted rule instances to compute relative frequency
estimates for many of the features presented in Section 5.
The rest of this section is organized as follows. In Section 4.1 we describe how we
extract rules that only look at target-side words and syntactic structure. In Section 4.2
we extract rules that also look at the source sentence, but not its syntax. (Although our
system uses unlexicalized features based on source-side syntax, they do not derive from
rules; we turn to features in Section 5). This lets us avoid the computational expense of
parsing the source side of the parallel training corpus.
4.1 Target-Tree Rules
We first extract rules that only consider the target side: y, ?, and ??. These rules can be
used as the basis for ?dependency language model? features (Shen, Xu, and Weischedel
2008; Galley and Manning 2009; Zhang 2009), though unlike previous work, our features
model both the phrase segmentation and dependency structure. Typically, these sorts
of features are relative frequencies from a corpus parsed using a supervised parser.
However, there do not currently exist treebanks with annotated phrase dependency
5 It is common to add ?identity? phrase pairs for unknown words to allow them to pass through
untranslated.
6 This strategy can also lead to limitations. Because we do not expand the search space beyond what is
licensed by the phrase table, we are limited by the ability of the underlying phrase-based model to
provide us with a good search space.
359
Computational Linguistics Volume 40, Number 2
trees. Our solution is to use a standard lexical dependency parser and extract phrase
dependencies using bilingual information.7 Essentially, we combine phrases from the
standard phrase extraction pipeline with selected lexical dependencies from the output
of a dependency parser.
We first give an overview of our approach and then describe it more formally. We
begin by obtaining word alignments and extracting phrase pairs using the standard
heuristic approach of Koehn, Och, and Marcu (2003). We then parse the target sentence
with a projective dependency parser to obtain a projective dependency tree ?y for a
sentence y. Note that ?y is a tree on words, not phrases (cf. ??). For each pair of target-
side phrases in the phrase pairs from phrase extraction, we extract a phrase dependency
(along with its direction) if the phrases do not overlap and there is at least one lexical
dependency between them. If there is only a dependency in one direction, we extract
a single phrase dependency with that direction. If there are lexical dependencies in
both directions, we extract a phrase dependency only for the single longest lexical
dependency, and in its direction. Because we use a projective dependency parser, the
longest lexical dependency between two phrases is guaranteed to be unique. If a phrase
contains a root word in ?y , we extract a phrase dependency with the wall symbol as
its head.
We now present the procedure more formally. Given word-aligned sentence pairs,
we extract phrase pairs that are p-consistent with (i.e., do not violate) the word align-
ments. Let R denote a relation between the two sets [n] and [m], where n = |x| and
m = |y|. If a pair (i, j) belongs to R for some i ? [n] and j ? [m], then we say that xi is
aligned to yj. We define new notation R here instead of using b because R allows many-
to-many word alignments, which are typically used for phrase extraction.8 A phrase
pair ?xji,y
l
k? is p-consistent with R if, for all u such that i ? u ? j, and all v such that (u, v)
belongs to R, it is the case that k ? v ? l. So far this is identical to the phrase extraction
pipeline used in Moses.
Given word alignments R and a dependency tree ?y on y, we extract (target-side)
phrase dependencies. We say a phrase dependency ?yji,y
l
k?with y
l
k as the parent phrase
is d-consistent with ?y and R if:
1. ?xj
?
i? ,x
l?
k? such that ?x
j?
i? ,y
j
i? and ?x
l?
k? ,y
l
k? are p-consistent with R
2. yji and y
l
k do not overlap: (1 ? i ? j < k ? l ? m) ? (1 ? k ? l < i ? j ? m)
3. the longest lexical dependency from yji to y
l
k is longer than the longest from
ylk to y
j
i: maxu:i?u?j,k??y (u)?l
|?y (u)? u| > max
v:k?v?l,i??y (v)?j
|?y (v)? v|
The final condition also implies that there is a lexical dependency from a word in yji to a
word in ylk: ?u, i ? u ? j, such that k ? ?y (u) ? l.
7 Other ways of getting phrase dependencies are possible. For example, for a monolingual task, Wu et al.
(2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase
dependencies.
8 Many-to-many word alignments can be obtained from certain alignment models or, more frequently, by
using heuristics to combine alignments from one-to-many and many-to-one alignments (Koehn, Och, and
Marcu 2003).
360
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
We also need to extract root phrase dependencies. We say a root phrase dependency
?yji, $? is d-consistent with ?y and R if:
1. ?xj
?
i? such that ?x
j?
i? ,y
j
i? is p-consistent with R
2. ?u, i ? u ? j, such that ?y (u) = 0
We extract all phrase dependencies that are d-consistent with the word alignments
and target-side lexical dependency trees. We note that while extracting phrase depen-
dencies we never explicitly commit to any single phrase dependency tree for a target
sentence. Rather, we extract phrase dependencies from all phrase dependency trees
compatible with the word alignments and the lexical dependency tree. Thus we treat
phrase dependency trees analogously to phrase segmentations in phrase extraction.
When actually extracting phrase dependencies, we record additional information
from the sentence pairs in which we found them. Specifically, for d-consistent phrase
dependencies ?yji,y
l
k? (where y
l
k is the parent), we extract tuples of the following form:
?yji,y
l
k, yu? , y?y (u? ), I
[
j < k
]
? (4)
where I [P] is the indicator function that returns 1 if P evaluates to true and 0 otherwise.
The index u? is chosen to make ?yu? , y?y (u? )? the longest lexical dependency within the
phrase dependency:
u? = argmax
u:i?u?j,k??y (u)?l
|?y (u)? u| (5)
This lexical dependency is recorded for use in back-off features, analogous to the lexical
weighting in phrase-based models. The fifth field in Equation (4) holds the direction of
the phrase dependency, which is also the direction of the longest lexical dependency.
Root phrase dependencies use k = l = 0 in the parent phrase and designate $ as y0. The
direction of root phrase dependencies is inconsequential and can remain as I
[
j < k
]
.
4.1.1 Examples. What do typical phrase dependencies look like? Tables 3 and 4
show some of the most frequent examples of root phrases and parent-child phrase
dependencies extracted by this technique on our German?English (DE?EN) corpus.
The English side of the parallel corpus was parsed using TurboParser (Martins et al.
2010). Naturally, there are many phrase dependencies with a single word in each
phrase, but because these are very similar to lists of frequent lexical dependencies in a
parsed corpus, we have only shown dependencies with phrases containing more than
one word.
Root phrases (Table 3) frequently contain a subject along with a verb (it is, i would
like, etc.), though the lexical root is typically a verb or auxiliary. These are examples of
how we can get syntactic information for phrases that typically would not correspond
to constituents in phrase structure trees.
Table 4 shows frequent phrase dependencies from the same corpus; because this
corpus is mostly European Parliamentary proceedings, certain formulaic and domain-
specific phrases appear with large counts. When phrases attach to each other, they
typically behave like their heads. For example, in the phrase dependency of the?union,
the word union is the child phrase because of the is behaving like of . There is likely also a
361
Computational Linguistics Volume 40, Number 2
Table 3
Top 60 most frequent root phrases in DE?EN data with at least two words, shown with their
counts. Shown in bold are the actual root words in the lexical dependency trees from which these
phrases were extracted; these are extracted along with the phrases and used for back-off features.
35,265 it is 6,210 i think 4,843 would be 2,918 thank you
13,751 this is 6,115 is that 4,289 we will 2,816 it will
12,763 is a 6,105 is not 4,262 i believe that 2,788 is to
11,831 we have 6,019 , it is 4,018 is also 2,737 it is a
11,551 would like 5,975 believe that 3,910 that is why 2,736 it has
11,245 we must 5,818 will be 3,838 i would like to 2,730 they are
11,243 is the 5,706 we need 3,775 would like to 2,611 we can
11,015 i would like 5,628 there are 3,505 hope that 2,580 i think that
10,008 there is 5,495 should like 3,427 is an 2,551 i will
8,983 i am 5,453 i should like 3,239 , i would like 2,483 does not
8,019 we are 5,227 i hope 3,130 i hope that 2,482 debate is
7,722 that is 5,150 , is 3,101 need to 2,445 i can
6,883 i would 5,110 we should 3,059 it was 2,438 want to
6,443 i have 5,010 has been 3,021 have been 2,416 must be
6,328 i believe 4,917 do not 2,937 think that 2,405 this is a
dependency from the to union whenever the longer phrase dependency is extracted, but
due to our practice of following the longest lexical dependency in deciding the direction,
of?union is favored over the?union.
We note that even though these phrase dependencies only contain words from the
target language (English), the presence and counts of the phrase dependencies will
Table 4
Most frequent phrase dependencies in DE?EN data, shown with their counts and attachment
directions. Child phrases point to their parents. To focus on interesting phrase dependencies,
we only show those in which one phrase has at least two tokens and neither phrase is entirely
punctuation. The words forming the longest lexical dependency in each extracted phrase
dependency are shown in bold; these are used for back-off features.
30,064 mr? president , 4,582 i believe? that
19,931 the? european union 4,516 , which? is
18,819 the european? union 4,347 that? will be
12,318 i? would like 4,297 the fact? that
11,990 the?member states 4,289 it is? important
8,169 it is? that 4,232 one? of the
7,779 the? european parliament 4,215 of the? commission
7,762 madam? president , 3,932 it is? not
7,448 the european? parliament 3,793 i? would like to
6,897 of the? union 3,761 in the? union
6,196 mr? president , i 3,752 in?member states
6,188 i? should like 3,673 president? ladies and gentlemen ,
6,087 that the? is 3,673 is? that the
5,478 i? believe that 3,667 president ,? ladies and gentlemen ,
5,283 of the? european union 3,602 i hope? that
5,268 that? and that 3,531 we? need to
4,956 of the european? union 3,495 the? fact that
4,902 , and? is 3,494 that the? commission
4,798 the? united states 3,462 i? do not
4,607 ) mr? president , 3,446 , the? commission
4,592 , it? is 3,421 that the? will
362
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
depend on the source language through the word alignments. For example, when of
the union is expressed in German, the preposition will often be dropped and the definite
article chosen to express genitive case. In our corpus, the most common translation of
the English union is the German noun union, which is feminine. The genitive feminine
definite article is der and, indeed, we find in the phrase table that the translation of of
the union with highest probability is der union.9 Thus the dominance of the phrase depen-
dency of the?union (6,897 occurrences) as compared with of?the union (142 occurrences)
is caused by the German translation.
4.1.2 Word Clusters. When trying to compute feature functions for dependencies between
long phrases, we expect to face problems of data sparseness. Long phrases do not occur
very often, so pairs of long phrases will occur less often still. One way to address this is
to also extract rules that use part-of-speech (POS) tags in place of words. However, since
words can have multiple POS tags, we would then need to infer POS tags for the words
in order to determine which rule is applicable. So we instead use hard word clusters,
which provide a deterministic mapping from words to cluster identifiers. Furthermore,
certain types of hard word clusters, such as Brown clusters (Brown et al. 1992), have
been shown to correspond well to POS tag categories (Christodoulopoulos, Goldwater,
and Steedman 2010). We chose Brown clusters for this reason.
Brown clustering uses a bigram hidden Markov model (HMM) in which states
are hard cluster labels and observations are words. The emission distributions are
constrained such that each word has a nonzero emission probability from at most
one cluster label. Clusters can be obtained efficiently through a greedy algorithm that
approximately maximizes the HMM?s log-likelihood by alternately proposing new
clusters and merging existing ones. This procedure actually produces a hierarchical
clustering, but we discard the hierarchy information and simply use unique IDs for
each cluster. The number of clusters is specified as an input to the algorithm; we used
100 clusters for all experiments in this article. Additional details on cluster generation
for our data sets are provided in Appendix B.
Given Brown clusters, we extract tuples like those above in which we replace each
word by its Brown cluster ID:
?clust(yji), clust(y
l
k), clust(yu? ), clust(y?y (u? )), I
[
j < k
]
? (6)
where clust() is a function that takes a sequence of words and replaces each by
its Brown cluster ID. The index u? is defined as in Equation (5). Examples of fre-
quent Brown cluster phrase dependencies, including root dependencies, are shown in
Table 5.
4.2 String-to-Tree Rules
Our simplest probability features use the information in these tuples, but we also extract
tuples with more information to support richer features. In particular, we record aligned
9 The phrase table probability of the German der union given the English of the union is 0.64. The next
most-probable German phrase is der europa?ischen union, with probability 0.03.
363
Computational Linguistics Volume 40, Number 2
Table 5
Most frequent Brown cluster phrase dependencies extracted from DE?EN data, shown with
their counts. As in Table 4, we only show those in which one phrase has at least two tokens and
neither phrase is entirely punctuation. Each cluster is shown as a set of words large enough to
cover 95% of the token counts in the cluster, up to a maximum of four words. It is characteristic
of Brown clustering that very frequent tokens (e.g., function words) often receive their own
clusters.
47,137 {mr, mrs, madam, mr.}? {president, president-in-office, van, barroso} ,
35,656 $? it is
29,775 the? {time, way, right, question} of
28,199 the? {european, soviet} {union, parliament, globalisation}
27,373 $? i {say, believe, think, know}
26,480 the? {state, development, group, security} of
26,388 the {european, soviet}? {union, parliament, globalisation}
24,726 {one, part, number, behalf}? of the
22,536 of the? {people, countries, members, citizens}
21,449 {state, development, group, security} {and, or}? {state, development, group, security}
21,007 $? {we, they} {should, must, cannot, shall}
20,933 {state, development, group, security}? {and, or} {state, development, group, security}
20,919 the? {one, part, number, behalf} of
20,897 of the? {report, committee, issue, agreement}
20,081 the? {economic, political, international, national} {policy, years, rights, market}
19,209 the? {report, committee, issue, agreement} of
18,535 {people, countries, members, citizens}? of the
18,523 $? {say, believe, think, know} that
18,510 {time, way, right, question}? of the
18,232 the? {member, united} {states, nations}
18,157 {one, part, number, behalf} of? {people, countries, members, citizens}
17,950 {people, countries, members, citizens} {and, or}? {people, countries, members, citizens}
17,643 {state, development, group, security}? of the
17,539 the? {people, countries, members, citizens} of
17,457 the? {economic, political, international, national} {state, development, group, security}
16,608 to {take, make, see, help}? {people, countries, members, citizens}
16,163 the {time, way, right, question}? of
15,517 the? {economic, political, international, national} {people, countries, members, citizens}
15,292 in the? {report, committee, issue, agreement}
15,257 a? {new, good, u.s., common} {report, committee, issue, agreement}
15,223 the {state, development, group, security}? of
15,217 {people, countries, members, citizens}? {and, or} {people, countries, members, citizens}
15,214 it is? {important, clear, necessary, concerned}
14,977 i? {say, believe, think, know} that
14,697 $? is {important, clear, necessary, concerned}
14,582 i {say, believe, think, know}? that
14,399 {should, must, cannot, shall}? be {made, taken, put, set}
14,146 $? this is
14,089 a {new, good, u.s., common}? {report, committee, issue, agreement}
14,047 {europe, china, today, women}? {and, or} {europe, china, today, women}
13,599 {made, taken, put, set}? {by, from, into, between} the
13,190 the? {new, good, u.s., common} {report, committee, issue, agreement}
13,089 the? {new, good, u.s., common} {people, countries, members, citizens}
13,035 $? {we, they} have
13,034 {economic, political, international, national}? {and, or} {economic, political, international,. . .}
13,013 $? is a
12,713 $? {need, want, needs, wish} to
12,399 $? i {say, believe, think, know} that
12,387 the? {time, way, right, question} of the
12,319 i?would {like, according, relating}
12,217 in the? {eu, world, government, country}
12,125 the? {economic, political, international, national} {report, committee, issue, agreement}
11,979 of? {economic, political, international, national} {state, development, group, security}
11,955 the {report, committee, issue, agreement}? of
11,838 $? {we, they} {are, were}
11,551 $?would {like, according, relating}
11,537 the? {people, countries, members, citizens} of the
364
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
source phrases and details about reordering and the presence of gaps between phrases.
That is, for d-consistent phrase dependencies ?yji,y
l
k?, we extract tuples
?yji,y
l
k,x
j?
i? ,x
l?
k? ,
I
[
j < k
]
,
I
[
I
[
j < k)
]
= I
[
j? < k?
]]
,
I
[
(j + 1 = k) ? (l + 1 = i)
]
,
I
[
(j? + 1 = k?) ? (l? + 1 = i?)
]
? (7)
for all i?, j?, k?, and l? such that the phrase pairs ?xj
?
i? ,y
j
i? and ?x
l?
k? ,y
l
k? are p-consistent
with R, and such that xj
?
i? does not overlap with x
l?
k? .
10 Again, I [P] is the indicator
function that returns 1 if P evaluates to true and 0 otherwise. That is, we include the
two target phrases, their aligned source phrases, the direction of the target attachment,
the orientation between the source and target phrases (whether the two target phrases
are in the same order as their aligned source phrases or swapped), whether a gap is
present between the two target phrases, and finally whether a gap is present between
the two source phrases. When ylk = $, all of the additional fields are irrelevant except
the aligned source phrase xj
?
i? .
We now note some examples of the phenomena that we can model with these richer
tuples. A common cause of reordering in German-to-English translation relates to verbs.
Figure 6 shows two examples of frequently extracted phrase dependencies that model
verb movement. Figure 6(a) gives an example of how German reorders the finite verb
to the end of a dependent clause, whereas English keeps it next to the subject. The
extracted rule, shown below the sentence pair, only applies when intervening words
appear on the German side and no intervening words appear on the English side. This
is indicated by the presence (absence) of an ellipsis on the German (English) side of the
rule.
Figure 6(b) shows an example of how German moves an infinitive (danken, ?to
thank?) to the end of an independent clause when a modal verb (mo?chte, ?would like?)
is present. The ellipses on both sides indicate that other words must be present between
both the source and target phrase pairs. We note that this rule says nothing about what
fills the gap. In particular, the gap-filling material does not have to be translationally
equivalent, and indeed in the given sentence pair it is not. As opposed to rules in
hierarchical phrase-based models (Chiang 2005), which typically specify translationally
equivalent substructures, this rule simply models the reordering and long-distance
movement of the infinitive. Much prior work has found phrase pairs with gaps to
be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley
and Manning 2010), and we extract tuples as in Equation (7) so that we can model
such structures, even though we do not directly model gap-filling like hierarchical
models and other models based on synchronous context-free grammar (Zollmann and
Venugopal 2006, inter alia).
10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from
the previous section, which are extracted even when the source phrases overlap.
365
Computational Linguistics Volume 40, Number 2
ich meine deshalb  , dass es  eine frage der geeigneten methodik  ist  .
i think  that it   is  consequently a question of the appropriate methodologies .
dass es   ...
(a)
(b)
ist
that it  is  
abschlie?end m?chte ich herrn langen herzlich  f?r seinen  bericht  danken  ,...
finally , mr president , i would like  to thank  mr langen warmly  for his  report ,...
f?r seinen    ... danken
to thank     ...  for his  
Figure 6
Examples of illustrative sentence pairs and frequently extracted rules that model verb
movement between German and English. An ellipsis indicates that there must be material
between the two phrases for the rule to apply. (a) Example of movement of the finite verb
to the end of a dependent clause. (b) Example of movement of an infinitive to the end of an
independent clause following a modal verb (mo?chte, ?would like?). Discussion of the features
used to score these string-to-tree rules is given in Section 5.2.
The tuples described here are used to compute all of the lexicalized phrase depen-
dency features in our model. We extract each tuple with a count of 1 each time it is
observed, aggregate the counts across all sentence pairs in the parallel corpus, and use
the counts to compute the statistical features we present in the next section. We also
have structural features that consider string-to-tree and tree-to-tree configurations, but
these do not require any rule extraction. In the next section we describe the full set of
features in our model.
5. Features
Our model extends the phrase-based translation model of Moses (Koehn et al. 2007), so
we include all of its features in our model. These include four phrase table probability
features, a phrase penalty feature, an n-gram language model, a distortion cost, six
lexicalized reordering features, and a word penalty feature. These features are contained
in h? in Equation (3), reproduced here:
argmax
?y,pi,?,??,b?
?? ? h?(x,y,pi,?,b)
+
n??
i=1
??? ? f (x, ?x , i, ??(i),?i,???(i),b(i),b(??(i)),pib(i),pib(??(i))) (8)
366
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
We now describe in detail the additional features f that are used to score phrase de-
pendency trees. Each operates on a single phrase dependency and takes the arguments
?x, ?x , c, d,?c,?d, c?, d?,pic? ,pid??, which are, in order, the source sentence (x), the source
dependency tree (?x), the target child phrase index (c), the target parent phrase index
(d), the target child phrase (?c), the target parent phrase (?d), the index of the source
phrase aligned to the target child (c?), the index of the source phrase aligned to the
target parent (d?), the child-aligned source phrase (pic? ), and the parent-aligned source
phrase (pid? ).
Like the phrase probability features in Moses, many of our feature functions are
conditional probabilities computed using relative frequency estimation given the full
collection of extracted tuples. That is, for a tuple ??,??, the conditional probability of
field ? given field ? is estimated as
p?(? | ?) =
#{??,??}
?
?? #{??,?
??}
(9)
where #{??,??} denotes the count of the tuple ??,?? in the multiset of extracted tuples.
We use the notation p? in the following to indicate that relative frequency estimates are
being used.11
5.1 Target-Tree Features
We first include features that only consider the target-side words and phrase depen-
dency tree; these are computed based on the rules extracted in Section 4.1. The first
feature is the sum of the scaled log-probabilities of each phrase dependency attachment
in ??:
fpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? ) = max
(
0, C + log p?(?c | ?d, dir(c, d))
)
(10)
where dir(c, d) is defined
dir(c, d) =
?
??
??
root if d = 0
left if d > c
right otherwise
(11)
and returns the direction of the attachment for head index d and child index c, that is,
the direction in which the child resides; root indicates that phrase c is the root.
Although we use log-probabilities in this feature function, we add a constant C,
chosen to ensure the feature value is never negative. The reasoning here is that when-
ever we use a phrase dependency that we have observed in the training data, we
want to boost the score of the translation. If we used log-probabilities, each observed
dependency would incur a penalty. The max expression prevents unseen parent-child
phrase dependencies from causing the score to be negative infinity. Our motivation is
a desire for the features to prefer one derivation over another but not to rule out a
derivation completely if it merely happens to contain an unseen phrase dependency.
11 Note that, as is standard across many SMT models, all frequencies here are counts of extraction events.
They are not counts of derivation or translation events, since many competing rules may be extracted from
each training instance.
367
Computational Linguistics Volume 40, Number 2
Because we will use this same practice for all other probability features, we intro-
duce some shorthand for simplicity of presentation. We first redefine this feature:
fpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? ) =
max
(
0, Cpdep + log gpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? )
)
(12)
where
gpdep(x, ?x , c, d,?c,?d, c
?, d?,pic? ,pid? ) = p?(?c | ?d, dir(c, d)) (13)
In what follows, we will restrict our attention to defining the g-style functions for
probability features, and assume that there is always a corresponding f that has the same
subscript and takes the same inputs, as in Equation (12). Furthermore, when presenting
the remaining features, we will suppress the arguments of each for clarity; all take the
same arguments as fpdep and gpdep.
We will assume C is chosen appropriately for each g based on the minimum log-
probability for the feature. For example,
Cpdep = 0.01? min
?,??,r
log p?(? | ??, r) (14)
that is, the minimum log-probability is found, negated, and a small positive value
(0.01) is added to ensure the feature is greater than zero. This ensures that, if a phrase
dependency has been seen, its contribution is at least 0.01.
To counteract data sparseness, we include other features that are less specific than
gpdep. First, we include a version of this feature with words replaced by Brown clusters:
gpdep
clust
= p?(clust(?c) | clust(?d), dir(c, d)) (15)
We also include lexical weighting features similar to those used in phrase-based ma-
chine translation (Koehn, Och, and Marcu 2003). These use the longest lexical depen-
dencies extracted during rule extraction. First, for all ?child, parent, direction? lexical
dependency tuples ?y, y?, r? in the parsed target side of the parallel corpus, we estimate
conditional probabilities p?lex(y | y?, r) using relative frequency estimation.
Then, assuming the given phrase dependency ??c,?d? has longest child-parent
lexical dependency ?y, y?? for direction dir(c, d), we include the feature:
gldep = p?lex(y | y
?, dir(c, d)) (16)
We include an analogous feature with words replaced by Brown clusters. Different
instances of a phrase dependency may have different lexical dependencies extracted
with them. We only use the lexical weight for the most frequent, breaking ties by
choosing the lexical dependency that maximizes p?lex(y | y?, r), as was done similarly by
Koehn, Och, and Marcu (2003).
So far we described four features that consider y, ?, and ??: one for phrase de-
pendencies, one for lexical dependencies, and the same two features computed on
a transformed version of the corpus in which each word is replaced by its Brown
cluster ID.
368
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
5.2 String-to-Tree Features
We next discuss features that consider properties of the source sentence x, its phrase
segmentation pi, and the phrase alignment b, in addition to y,?, and ??. However, these
features still do not depend on the source tree ?x , so they can be included even when a
parser for the source language is not available. We will discuss features that use ?x in
Section 5.3.
These features are similar to the previously defined gpdep, but condition on addi-
tional pieces of structure. All features condition on direction. The first pair of features
condition on the source phrase (pic? ) aligned to the child phrase (?c) in the target phrase
dependency (??c,?d?):
gpdep
child
= p?(?c | ?d, dir(c, d),pic? ) (17)
gpdep
child
clust
= p?(?c | clust(?d), dir(c, d),pic? ) (18)
In the second feature, we condition on word clusters for the parent phrase ?d, but on
words in the aligned source phrase pic? . Because Brown clusters often correspond to
syntactic clusters, even at times resembling part-of-speech tags (Christodoulopoulos,
Goldwater, and Steedman 2010), it did not seem logical to model translation probabili-
ties between source- and target-language word clusters. This is why we did not include
a feature like the above with word clusters for ?c and pic? . Our use of these clusters is a
simple kind of backoff or smoothing that allows some sharing across specific phrases,
since statistics on phrase pairs are expected to be sparse.
The next set of features includes those that condition on the orientation between
the source- and target-side phrases. The ori function returns the orientation of the
aligned source phrases in a target phrase dependency attachment, namely, whether the
aligned source phrases are in the same order as the target phrases (?same?) or if they
are in the opposite order (?swap?):
ori(c, d, c?, d?) =
?
??
??
root if d = 0
same if dir(c, d) = dir(c?, d?)
swap otherwise
(19)
Given this definition of ori, we define the following features that condition on orienta-
tion (in addition to other fields):
gpdep
orient
= p?(?c | ?d, dir(c, d), ori(c, d, c
?, d?)) (20)
gpdep
orient
clust
= p?(clust(?c) | clust(?d), dir(c, d), ori(c, d, c
?, d?)) (21)
gpdep
child
orient
= p?(?c | ?d, dir(c, d),pic? , ori(c, d, c
?, d?)) (22)
gpdep
child
orient
clust
= p?(?c | clust(?d), dir(c, d),pic? , ori(c, d, c
?, d?)) (23)
369
Computational Linguistics Volume 40, Number 2
where the last two features condition on the aligned child phrase pic? in addition to the
direction and orientation.
We next give features that condition on the presence of gaps between the child and
parent target phrases and gaps between the aligned phrases on the source side. The
gap(c, d) function indicates whether there is a gap between the phrases indexed by c
and d:
gap(c, d) =
?
??
??
root if d = 0
yes if |d? c| ? 1
no otherwise
(24)
Given this gap function, we define the following features:
gpdep
orient
gap
= p?(?c | ?d, dir(c, d), ori(c, d, c
?, d?), gap(c, d), gap(c?, d?)) (25)
gpdep
orient
gap
clust
= p?(clust(?c) | clust(?d), dir(c, d), ori(c, d, c
?, d?), gap(c, d), gap(c?, d?)) (26)
All the features mentioned so far have the child phrase on the left-hand side of the
conditioning bar. We now present features that have both the child and parent phrases
on the left-hand side:
gpdep
pc
= p?(?c,?d, dir(c, d) | pic? ,pid? ) (27)
gpdep
pc
orient
= p?(?c,?d, dir(c, d) | pic? ,pid? , ori(c, d, c
?, d?)) (28)
gpdep
pc
orient
gap
= p?(?c,?d, dir(c, d), gap(c, d) | pic? ,pid? , ori(c, d, c
?, d?), gap(c?, d?)) (29)
These last features score larger rules composed of two phrase pairs from the phrase
table. Including direction, orientation, and gaps enables us to model longer-distance re-
orderings; we showed some examples of such frequently extracted phrase dependencies
in Section 4.2.
In all, we introduced 11 features in this section, giving us a total of 15 so far. For
the feature ablation experiments in Section 7, we will partition these features into two
parts: We refer to the six features with subscript clust as CLUST and the other nine
as WORD.
5.2.1 String-to-Tree Configurations (CFG). We now present features that count instances of
local reordering configurations involving phrase dependencies. We refer to the features
described in this section and the next section as CFG. These features consider the target
segmentation ?, the target phrase dependency tree ??, and the phrase alignment b,
but not the target words y or the source words x, segmentation pi, or dependency
tree ?x .
370
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Our first set of features only looks at configurations involving direction and orien-
tation. The first feature value is incremented if the child is to the left and the aligned
source-side phrases are in the same order:
fleft
same
= I
[
dir(c, d) = left ? ori(c, d, c?, d?) = same
]
(30)
Another feature fires if the aligned source phrases are in the opposite order:
fleft
swap
= I
[
dir(c, d) = left ? ori(c, d, c?, d?) = swap
]
(31)
Analogous features are used when the child is to the right of the parent:
fright
same
= I
[
dir(c, d) = right ? ori(c, d, c?, d?) = same
]
(32)
fright
swap
= I
[
dir(c, d) = right ? ori(c, d, c?, d?) = swap
]
(33)
These four configuration features are shown in order in the leftmost column in
Figure 7. They are agnostic as to the presence of gaps between the two target phrases
and between the two source phrases. We include 16 features that add gap information
to these four coarse configurations, as shown in the remainder of the table. Four gap
configurations are possible, constructed from one binary variable indicating the pres-
ence or absence of a source gap paired with a binary variable indicating the presence or
absence of a target gap. We replicate the four coarse features for each gap configuration,
giving us a total of 20 string-to-tree configuration features, all shown in Figure 7.
?
?
no gaps source gap target gap source andtarget gaps
coarse configurations
(only direction
and orientation)
xi xj xk xi xj xk xlxi xj xk xlxi xj xk
xlxi xj xk xlxi xj xk
xlxi xj xk xlxi xj xk
xlxi xj xk xlxi xj xk
xi xj xk
xi xj xk xi xj xk
xi xj xk xi xj xk
xi xj xk xi xj xk
?xi xj xk
?xi xj xk
?xi xj xk
yk?yi? yj? yk?yi? yj? yk?yi? yj? yl? yk?yi? yj? yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yl? yk?yi? yj?
yk?yi? yj? yk?yi? yj?
yk?yi? yj? yk?yi? yj?
yk?yi? yj? yk?yi? yj?
? yk?yi? yj?
? yk?yi? yj?
? yk?yi? yj?
Figure 7
String-to-tree configurations; each is associated with a feature that counts its occurrences in
a derivation.
371
Computational Linguistics Volume 40, Number 2
5.2.2 Dependency Length Features. Related to the string-to-tree configurations are features
that score source- and target-side lengths (i.e., number of words crossed) of target-side
phrase dependencies. These lengths can also be useful for hard constraints to speed up
inference; we return to this in Section 6. These features and constraints are similar to
those used in vine grammar (Eisner and Smith 2005).
We first include a feature that counts the number of source-side words between the
aligned source phrases in each attachment in ??. Letting pic? = x
j?
i? and pid? = x
l?
k? :
fsrc
vine
= I
[
dir(c?, d?) = left
] (
k? ? (j? + 1)
)
+ I
[
dir(c?, d?) = right
] (
i? ? (l? + 1)
)
(34)
Although this feature requires the segmentation of the source sentence in order to
determine the number of source words crossed, the actual identities of those words are
not needed, so the feature does not depend on x. We would expect this feature?s weight
to be negative for most language pairs, encouraging closeness in the source sentence of
phrases aligned to each phrase dependency in the target.
We would like to use a similar feature for target-side dependency lengths, for
example, where ?c = y
j
i and ?d = x
l
k:
I [dir(c, d) = left]
(
k? (j + 1)
)
+ I [dir(c, d) = right] (i? (l + 1)) (35)
However, such a feature could require looking at the entire phrase segmentation being
generated to score a single phrase dependency (e.g., if ??(1) = n?). Using this feature
would prevent us from being able to use dynamic programming for decoding (we
discuss our approach to decoding in Section 6). Instead, we use a feature that considers
bounds on the number of target words crossed by each phrase dependency. In particular,
the feature sums the maximum number of target words that could be crossed by a
particular phrase dependency. We will discuss how this feature is computed when we
discuss decoding in Section 6.
We use CFG to refer to the set containing the 20 string-to-tree configuration features
and the 2 string-to-tree dependency length features. Adding these 22 features to the 15
from Sections 5.1 and 5.2 gives us 37 QPD features so far.
5.3 Tree-to-Tree Features (TREETOTREE)
The last two sets of features consider the source-side dependency tree ?x in addition
to x, pi, b, y, ?, and ??. These are the only features that use source and target syntax
simultaneously. We use TREETOTREE to refer to these features.
5.3.1 Quasi-Synchronous Tree-to-Tree Configurations. We begin with features based on
the quasi-synchronous configurations from Smith and Eisner (2006), shown for lexical
dependency trees in Figure 8. For a child-parent dependency on the target side, these
configurations consider the relationship between the aligned source words. For exam-
ple, if the aligned source words form a child-parent dependency in the source tree, then
we have a ?parent-child? configuration. There is also an ?other? category for those that
do not fit any of the named categories.
However, for our model we need to score configurations involving phrase depen-
dencies. That is, for a child-parent phrase dependency ??c,?d? in ??, we consider the
relationship between pic? and pid? , the source-side phrases to which ?c and ?d align.
372
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
xi   ...   xj
yk   ...   yl
parent-child
xh  ...  xi   ...  xj
yk   ...   yl
grandparent-grandchild c-command
xh  ...  xi   ...  xj
yk   ...   yl
siblings
xg  ...  xh   ...  xi  ...  xj
yk   ...   yl
$   ...   xi
$   ...   yk
root-root
xi   ...   xj
yk   ...   yl
child-parent
xi
yk   ...   yl
same-node
Figure 8
Quasi-synchronous tree-to-tree configurations from Smith and Eisner (2006). There are
additional configurations involving NULL alignments and an ?other? category for those that do
not fit into any of the named categories.
There are several options for computing configuration features for our model, since we
use a phrase dependency tree for the target sentence, a lexical dependency tree for the
source sentence, and a phrase alignment.
We use a heuristic approach. First we find the full set of configurations that are
present between any word in one source phrase and any word in the other source
phrase. That is, given a pair of source words, one with index j in source phrase d?
and the other with index k in source phrase c?, we have a parent-child configura-
tion if ?x (k) = j; if ?x (j) = k, a child-parent configuration is present. In order for the
grandparent-grandchild configuration to be present, the intervening parent word must
be outside both phrases. For sibling configurations, the shared parent must also be
outside both phrases. In lieu of standard (non-sibling) c-command relationships, we
define a modified c-command category as follows. We first find the highest ancestors of
words j and k that are still in their respective phrases. Of these two ancestors, if neither
is an ancestor of the other and if they are not siblings, then the ?c-command? feature
fires.
After obtaining a list of all configurations present for each pair of words ?j, k?, we fire
the feature for the single configuration corresponding to the maximum distance |j? k|.
If no configurations are present between any pair of words, the ?other? feature fires.
Therefore, only one configuration feature fires for each extracted phrase dependency
attachment.
For the six configurations other than ?root-root,? we actually include multiple
instances of each configuration feature: one set includes direction (6? 2 = 12 features),
another set includes orientation (12 features), and the final set includes both source- and
target-side gap information (24 features). There are therefore 49 features in this category
(including the single ?root-root? feature).
5.3.2 Tree-to-Tree Dependency Path Length Features. Finally, we include features that con-
sider the dependency path length between the source phrases aligned to the target
phrases in each phrase dependency. The features in Section 5.2.2 considered distance
along the source sentence (the number of words crossed). Now we add features that
consider distance along the source tree (the number of lexical dependency arcs crossed).
We expect the learned weights for these features to encourage short dependency path
lengths on the source side.
373
Computational Linguistics Volume 40, Number 2
We first include a feature that sums, for each target phrase i, the inverse of the
minimum undirected path length between each word in pic? = x
j?
i? and each word in
pid? = xl
?
k? :
fundir
path
=
j??
j=i?
l??
k=k?
1
minUndirPathLen(x, ?x , j, k)
(36)
where minUndirPathLen(x, ?x , j, k) returns the shortest undirected dependency path
length from xj to xk in ?x . The shortest undirected path length is defined as the number
of dependency arcs that must be crossed to travel from one word to the other along the
arcs in ?x .
Assuming an analogous function minDirPathLen(x, ?x , j, k) that computes the mini-
mum directed dependency path length, we also include the following feature:
fdir
path
=
j??
j=i?
l??
k=k?
1
minDirPathLen(x, ?x , j, k)
(37)
If there is no directed path from xj to xk, minDirPathLen returns?.
Adding these two features gives us a total of 88 QPD features. Along with the 14
phrase-based features there are a total of 102 features in our model.
6. Decoding
For our model, decoding consists of solving Equation (1)?that is, finding the highest-
scoring tuple ?y,pi,?, ??,b? for an input sentence x and its parse ?x . This is a challenging
search problem, because it is at least as hard as the search problem for phrase-based
models, which is intractable (Koehn, Och, and Marcu 2003). Because of this we use a
coarse-to-fine strategy for decoding (Charniak and Johnson 2005; Petrov 2009). Coarse-
to-fine inference is a general term for procedures that make two (or more) passes over
the search space, pruning the space with each pass. Typically, feature complexity is
increased in each pass, as richer features can often be computed more easily in the
smaller search space.
One simple coarse-to-fine procedure for our model would start by generating a
k-best list of derivations using a phrase-based decoder. This ?coarse model? would
account for all of the phrase-based features. Then we could parse each derivation to
incorporate the QPD features and rerank the k-best list with the modified scores; this is
the ?fine model.? The advantage of this approach is its simplicity, but other research has
shown that k-best lists for structured prediction tend to have very little diversity (Huang
2008), and we expect even less diversity in cases like machine translation where latent
variables are almost always present. Instead, we generate a phrase lattice (Ueffing, Och,
and Ney 2002) in a coarse pass and perform lattice dependency parsing as the fine pass.
The remainder of this section is laid out as follows. We begin by reviewing phrase
lattices in Section 6.1. In Section 6.2 we present our basic lattice dependency parsing
algorithm. We give three ways to speed it up in Section 6.3; one enables a more judicious
search without affecting the search space, and the other two prune the search space
in different ways. In Section 6.4, we discuss how decoding affects learning of the
feature weights ?, and we describe the structured support vector machine reranking
formulation from Yadollahpour, Batra, and Shakhnarovich (2013) that we use. We close
374
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
konnten / could
konnten / could
konnten sie / could you
sie / you
sie / you
es ?bersetzen / translate it
sie es   ?bersetzen / you translate it
?bersetzen /
translate
?bersetzen /
translate
es / it
es / it
es / it
? / ?
? / ?
... you
... could
... could
source:  konnten sie es ?bersetzen ?
reference:  could you translate it ?
Figure 9
Example phrase lattice for the source sentence shown. Each node contains an n-gram history for
computing n-gram language model features and a coverage vector representing the source
words that have been translated so far. For clarity, the n-gram history (n = 2) and coverage
vector are only shown for three nodes.
in Section 6.5 with a brief discussion of how this decoder differs from earlier versions
published in Gimpel and Smith (2009b, 2011).
6.1 Phrase Lattices
The most common decoding strategy for phrase-based models is to use beam
search (Koehn, Och, and Marcu 2003). The search is performed by choosing phrase
pairs from the phrase table and applying them to translate source phrases into the target
language. Coverage vectors are maintained during decoding to track which words have
been translated so far. They are used to enforce the constraint that each source word
appear in exactly one phrase pair.
It is often convenient to build a packed representation of the (pruned) search space
explored during decoding. For phrase-based models, this representation takes the form
of a phrase lattice (Ueffing, Och, and Ney 2002), a finite-state acceptor in which each
path corresponds to a derivation. Figure 9 shows an example. The source sentence and
a reference translation are shown at the top of the figure. Each path from the start node
on the left to a final node corresponds to a complete output in the model?s output space.
Each lattice edge corresponds to a phrase pair used in the output. All paths leading to a
given node in the lattice must agree in the set of source words that have been translated
thus far. So, every node in the lattice is annotated with the coverage vector of all paths
that end there. This is shown for three of the nodes in the figure.
The lattice is constructed such that all features in the model are locally computable
on individual lattice edges. To make n-gram language model features local, all paths
leading to a given node must end in the same n? 1 words.12 In the example, there
are two nodes with equivalent coverage vectors that are separated because they end in
12 In practice, this state replication can be reduced by exploiting sparsity in the language model (Li and
Khudanpur 2008).
375
Computational Linguistics Volume 40, Number 2
different words (you vs. could). Decoders like Moses can output phrase lattices like these;
the lattice simply encodes the paths explored during the beam search.
6.2 Lattice Dependency Parsing
Each path in a phrase lattice corresponds to a tuple ?y,pi,?,b? for the input x. To also
maximize over ??, we perform lattice dependency parsing, which allows us to search
over the space of tuples ?y,pi,?,b, ???. Lattice parsing jointly maximizes over paths
through a lattice and parse structures on those paths.
Because we use an arc-factored phrase dependency model (Equation (3)), the lattice
dependency parsing algorithm we use is a straightforward generalization of the arc-
factored dynamic programming algorithm from Eisner (1996). The algorithm is shown
in Figure 10. It is shown as a set of recursive equations in which shapes are used in
place of function names and shape indices are used in place of function arguments.
The equations ground out in functions edgeScore and arcScore that score individual
lattice edges and phrase dependency arcs, respectively.13 A semiring-generic format is
used; for decoding, the semiring ?plus? operator (?) would be defined as max and the
semiring ?times? operator (?) would be defined as +. The entry point when executing
the algorithm is to build GOAL, which in turn requires building the other structures.
We use a simple top?down implementation with memoization. Our style of spec-
ifying dynamic programming algorithms is similar to weighted deduction, but ad-
ditionally specifies indices and ranges of iteration, which are useful for a top?down
implementation. Top?down dynamic programming avoids the overhead of maintaining
a priority queue that is required by bottom?up agenda algorithms (Nederhof 2003;
Eisner, Goldlust, and Smith 2005).
The disadvantage of top?down dynamic programming is that wasted work can be
done; structures can be built that are never used in any full parse. This problem appears
when parsing with context-free grammars, and so the CKY algorithm works bottom?
up, starting with the smallest constituents and incrementally building larger ones. This
is because context-free grammars may contain rules with only non-terminals. Top?
down execution may consider the application of such rules in sequence, producing long
derivations of non-terminals that never ?ground out? in any symbols in the string. A
dependency model, on the other hand, always works directly on words when building
items, so a top?down implementation can avoid wasted effort.
However, this situation changes with lattice dependency parsing. It is possible for
a top?down lattice dependency parser to consider some dependencies that are never
used in a full parse. We address this issue in the next section.
6.3 Computational Complexity and Speeding Up Decoding
The lattice parsing algorithm requires O(E2V) time and O(E2 + VE) space, where E
is the number of edges in the lattice and V is the number of nodes. Typical phrase
lattices might easily contain tens of thousands of nodes and edges, making exact search
prohibitively expensive for all but the smallest lattices. So we use three techniques to
speed up decoding: (1) avoiding construction of items that are inconsequential (i.e.,
13 To prevent confusion, we use the term edge to refer to a phrase lattice edge and arc to refer to a
dependency attachment in a dependency tree.
376
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Figure 10
Lattice dependency parsing using an arc-factored dependency model. Lone indices like p and
i denote nodes in the lattice, and an ordered pair like (i, j) denotes the lattice edge from node i
to node j. START is the single start node in the lattice and FINAL is a set of final nodes. We use
edgeScore(i, j) to denote the model score of crossing lattice edge (i, j), which only includes
the phrase-based features h?. We use arcScore((i, j), (l, m)) to denote the score of building the
dependency arc from lattice edge (i, j) to its parent (l, m); arcScore only includes the QPD
features h??.
that could never be contained in a full parse), (2) pruning the lattices, and (3) limiting
the maximum length of a phrase dependency.
6.3.1 Avoiding Construction of Inconsequential Items. By design, our phrase lattices impose
several types of natural constraints on allowable dependency arcs. For example, each
node in the phrase lattice is annotated with a coverage vector?a bit vector indicating
377
Computational Linguistics Volume 40, Number 2
which words in the source sentence have been translated?which implies a topological
ordering of the nodes. Once a word in the source sentence has been covered (i.e.,
translated), it cannot be uncovered later. This can tell us whether certain nodes are
unreachable from other nodes. For example, for a three-word source sentence, there
cannot exist a directed path from a node with coverage vector ?0, 1, 0? to a node with
coverage vector ?0, 0, 1?. However, there may or may not be a path from a node with
vector ?0, 1, 0? to one with ?0, 1, 1?.
Generally, we need an efficient way to determine, for any two nodes in the lattice,
whether there exists a path from one to the other. If there is no path, we can avoid
wasting time figuring out the best way to build items that would end at the two nodes.
To discover this, we use an all-pairs shortest paths algorithm to find the score of the best
path between each pair of nodes in the lattice. The algorithm also tells us whether each
edge is reachable from each other edge, allowing us to avoid drawing dependencies
that will never ground out in a lattice path. We use the Floyd-Warshall algorithm (Floyd
1962). This adds some initial overhead to decoding, but in preliminary experiments we
found that it saves more time than it costs. We actually run a modified version of the
algorithm that computes the length (in words) of the longest path between any two
nodes. If the maximum length between two nodes is?, the nodes are unreachable from
each other. Before we build an item in the algorithm in Figure 10, we check reachability
of the item endpoints and only proceed if one can reach the other.
We modified the algorithm to output maximum lengths because we use the max-
imum lengths to compute the target-side vine grammar features and constraints, as
mentioned in Section 5.2.2. In particular we use a feature ftgt
vine
that is a target-side
analog to fsrc
vine
but using the Floyd-Warshall maximum path lengths in place of the actual
lengths.
6.3.2 Lattice Pruning. To reduce phrase lattice sizes, we prune lattice edges using
forward?backward pruning (Sixtus and Ortmanns 1999), which has also been used by
Tromble et al. (2008). This pruning method computes the max-marginal for each lattice
edge, which is the score of the best full path that uses that edge, then prunes edges
whose max-marginal is below a certain fraction of the best path score in the lattice. Max-
marginals have been used for other coarse-to-fine learning frameworks (Weiss, Sapp,
and Taskar 2010) and offer the advantage that the best path in the lattice is preserved
during pruning.
We only use the score contribution from the phrase-based features when computing
these max-marginals. For each lattice, we use a grid search to find the most liberal
threshold that leaves fewer than 2,000 edges in the resulting lattice. As complexity is
quadratic in E, forcing E to be less than 2,000 improves runtime substantially. After
pruning, the lattices contain more than 1016 paths on average and oracle BLEU scores
are typically 10?15 points higher than the model-best paths.
6.3.3 Maximum Dependency Lengths. We can easily adapt our vine grammar features
to function as hard constraints on allowable dependency trees, as originally done by
Eisner and Smith (2005) for monolingual dependency parsing. We use two simple
constraints on the maximum length of a phrase dependency used during translation.
One constrains the number of source words that are crossed from one aligned source
phrase to the other aligned source phrase by the phrase dependency. The other con-
strains the maximum number of target-side words crossed by any path from one
target phrase to the other target phrase in a phrase dependency. During translation,
we never build items that would require using dependency arcs that violate these
378
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
constraints. In Section 7 we discuss the values we used in our primary experiments
and also compare translation quality and decoding speed for several values of these
hyperparameters.
6.4 Interaction with Learning
The use of a coarse-to-fine decoding procedure affects how we learn the parameters
of our model. We use two separate versions of the phrase-based feature weights: one
for lattice generation and one for lattice dependency parsing. This is common with
coarse-to-fine strategies?separate instances of coarser parameters are required for each
subsequent pass. We first learn parameters for the coarse phrase-based model used
to generate phrase lattices. Then, after generating the lattices, we prune them (Sec-
tion 6.3.2) and use a second round of tuning to learn parameters of the fine model, which
includes all phrase-based and QPD feature weights. We initialized the phrase-based
feature weights using the default Moses weights. For the QPD features, we initialized
the phrase dependency probability feature weights to 0.002 and the weights for all other
features to 0.
For tuning, we need the k-best outputs, for which efficient dynamic programming
algorithms are available. We use Algorithm 3 from Huang and Chiang (2005), which
lazily finds the k best derivations efficiently. In preliminary testing, we found that the
k-best lists tended to be dominated by repeated translations with different derivations,
so we used the technique presented by Huang, Knight, and Joshi (2006), which finds a
unique k-best list, returning the highest-scoring derivation for each of k unique transla-
tions. This modification requires the maintenance of additional data structures to store
all of the previously found string yields for each item built during parsing. This incurs
additional overhead but allows us to obtain a far more diverse k-best list given a fixed
time and memory budget.
For the first round of tuning, we use RAMPION (Gimpel and Smith 2012b), which
performs competitively with minimum error rate training (Och 2003) but is more stable.
For training the fine model, however, we found that RAMPION did not lead to substan-
tial improvements over the output of the coarse phrase-based model alone. We found
better performance by using a fine learner designed for the k-best reranking setting, in
particular the structured support vector machine reranker described by Yadollahpour,
Batra, and Shakhnarovich (2013). Though we are doing lattice reranking rather than
k-best reranking, the learning problem for our fine model is similar to that for k-best
reranking in that the decoder is exact (i.e., there is no pruning that could lead to different
patterns of search error as the parameters change). That is, phrase lattice generation and
pruning (described in Section 6.3.2) only depend on the coarse phrase-based feature
weights and the maximum dependency length constraints (described in Section 6.3.3);
they do not depend on the fine model parameters.
We now briefly describe how we learn parameters for the fine model via lattice
reranking. For simplicity, we will only write the source sentence x and its translation
y when describing the reranker and omit the additional input and output variables
?x ,pi,?, ??, and b, but they are always present and used for computing features. We
assume a tuning set with N source sentences: {xi}Ni=1. Let Y
R
i be the set of reference
translations for source sentence xi. Let Yi = {y
(1)
i . . .y
(k)
i } denote the set of k candidate
translations (outputs of our lattice dependency parsing decoder) for xi. Let y?i denote
the highest-quality translation in the set, that is, y?i = argminy?Yi `(Y
R
i ,y), where `(Y
R
i ,y)
is the negated BLEU+1 score (Lin and Och 2004) of y evaluated against references YRi .
379
Computational Linguistics Volume 40, Number 2
We use the following cost function for sentence i and candidate translation y:
L(YRi ,y) = `(Y
R
i ,y)? `(Y
R
i ,y
?
i ) (38)
that is, the negated BLEU+1 score of translation yi relative to that of the best translation
(y?i ) in the set.
Yadollahpour, Batra, and Shakhnarovich (2013) formulate the reranking learning
problem as an L2-regularized slack-rescaled structured support vector machine (SSVM;
Tsochantaridis et al. 2005). The feature weights ? for the fine model are learned by
solving the following quadratic program:
min
?,?i
||?||22 + ?
?
i?[N]
?i (39a)
s.t. ?>
(
h(xi,y
?
i )? h(xi,y)
)
? 1?
?i
L(YRi ,y)
(39b)
?i ? 0, ?y ? Yi \ y
?
i , (39c)
In Equation (39b), the violation in the margin ?i is scaled by the cost of the translation.
Thus if in addition to y?i there are other good solutions in the set, the margin for such
solutions will not be tightly enforced. On the other hand, the margin between y?i and
bad solutions will be very strictly enforced. Equation (39) is solved via the 1-slack
cutting-plane algorithm of Joachims, Finley, and Yu (2009).14 During the execution of
the cutting-plane algorithm, we compute the tuning set BLEU score with all param-
eter vector values that are considered. At convergence we return the parameters that
led to the highest tuning BLEU score. This helps to bridge the discrepancy between
our use of sentence-level BLEU+1 in the loss function and corpus BLEU for final
evaluation.
We alternate between generating k-best lists using our lattice parser and solving
Equation (39) on the fixed lists, each time pooling all previous iterations? lists. We repeat
until the parameters do not change, up to a maximum of 15 iterations. We used k-best
lists of size 150 and a fixed, untuned value of ? = 0.1 for all experiments.
6.5 Comparison to Earlier Work
The decoder described above represents some advances over those presented in earlier
papers. Our original decoder was designed for a lexical dependency model; we used
lattice dependency parsing on lattices in which each edge contained a single source-
target word pair (Gimpel and Smith 2009b). Inference was approximated using cube
decoding (Gimpel and Smith 2009a), an algorithm that incorporates non-local features
in a way similar to cube pruning (Chiang 2007). After developing our QPD model,
we moved to phrase lattices but still approximated inference using an agenda algo-
rithm (Nederhof 2003; Eisner, Goldlust, and Smith 2005) with pre-pruning of depen-
dency edges in a coarse pass (Gimpel and Smith 2011).
14 We used OOQP (Gertz and Wright 2003) to solve the quadratic program in the inner loop, which uses
HSL, a collection of Fortran codes for large-scale scientific computation (www.hsl.rl.ac.uk).
380
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
All decoders used lattice dependency parsing, but our current decoder uses an exact
algorithm once two simple approximations are made: the pruning of the lattice and the
use of maximum dependency length constraints. Hyperparameters control the severity
of these two approximations and the use of an exact parsing algorithm allows us to
measure their effects on runtime and accuracy.
7. Experiments and Analysis
We now present experimental results using our QPD model. Because our model extends
phrase-based translation models with features on source- and target-side syntactic
structures, we can conduct experiments that simulate phrase-based, string-to-tree, and
tree-to-tree translation, merely by specifying which feature sets to include. This suggests
an additional benefit of using a quasi-synchronous approach for machine translation. By
using features rather than constraints, we can simulate a range of translation systems
in a single framework, allowing clean experimental comparisons among modeling
strategies and combining strengths of diverse approaches.
We describe our experimental setup in Section 7.1 and present our main results in
Section 7.2. We measure the impact of using unsupervised parsing in Section 7.2.1 and
include feature ablation experiments in Section 7.2.2. We present the results of a manual
evaluation in Section 7.3 and give examples. We conclude in Section 7.4 with a runtime
analysis of our decoder and show the impact of decoding constraints on speed and
translation quality.
7.1 Experimental Setup
In this section we describe details common to the experiments reported in this sec-
tion. Details about decoding and learning were described in Section 6. Full details
about language pairs, data sets, and baseline systems are given in Appendix A and
Appendix B. We repeat important details here. We use case-insensitive IBM BLEU
(Papineni et al. 2002) for evaluation. To measure significance, we use a paired bootstrap
(Koehn 2004) with 100,000 samples (p ? 0.05).
7.1.1 Language Pairs. We consider German?English (DE?EN), Chinese?English
(ZH?EN), Urdu?English (UR?EN), and English?Malagasy (EN?MG) translation.
These four languages exhibit a range of syntactic divergence from English. They also
vary in the availability of resources like parallel data, monolingual target-language data,
and treebanks. It is standard practice to evaluate unsupervised parsers on languages
that do actually have treebanks, which are used for evaluation. We consider this case
as well, comparing supervised parsers for English and Chinese to our unsupervised
parsers, but we also want to evaluate our ability to exploit unsupervised parsing for
languages that have small or nonexistent treebanks, hence our inclusion of Urdu and
Malagasy.
7.1.2 Baselines. We compare our model to several baselines:
r Moses, RAMPION, S = 200: This is a standard Moses phrase-based system,
trained with RAMPION. The Moses default stack size S of 200 was used
during tuning and testing. This is the result one would obtain with an
off-the-shelf Moses phrase-based system on these data sets (and trained
using RAMPION).
381
Computational Linguistics Volume 40, Number 2
r Moses, RAMPION, S = 500: This baseline trains a model in the same way as
the previous using S = 200, but then uses a larger stack size (S = 500)
when decoding the test sets. This larger stack size was used for generating
phrase lattices for lattice reranking, so it provides a more appropriate
baseline for comparing to our model.
r Moses, SSVM reranking: Using phrase lattices generated with the
preceding configuration, this baseline uses the SSVM reranker from
Section 6.4 on the phrase lattices with only the Moses phrase-based features,
that is, without any QPD features. This baseline helps to separate out the
gains achieved through SSVM reranking and the addition of QPD features.
r Hiero, RAMPION: This is a standard hierarchical phrase-based
system (Chiang 2007), as implemented in the Moses toolkit and trained
using RAMPION.
We see the three Moses systems as our primary baselines because Moses was used
to generate phrase lattices for our system. Our model adds new syntactic structures and
features to Moses, but because our decoder use Moses? phrase lattices, our approach
can be viewed as rescoring Moses? search space. There are pros and cons to this choice.
It lets us build on a strong baseline rather than building a system from scratch. Also, by
comparing the third baseline (?Moses, SSVM reranking?) to our model, we are able to
cleanly measure the contribution of our QPD features. However, Hiero has been shown
to perform better than phrase-based systems for certain language pairs (Chiang 2007;
Zollmann et al. 2008; Birch, Blunsom, and Osborne 2009), and in these cases Hiero
proves to be a strong baseline for our model to beat as well. We note that our QPD
features could also be used to rescore Hiero?s search space to potentially yield further
improvements, but we leave this to future work.
7.1.3 Parsers. Our full QPD model requires parsers for both source and target languages.
For each language pair, the target-language parser is only used to parse the target side
of the parallel corpus and the source-language parser is only used to parse the source
side of the tuning and test sets.
We have access to supervised parsers for Chinese, German, and English, which we
used for our experiments. In particular, we used the Stanford parser (Levy and Manning
2003; Rafferty and Manning 2008) for Chinese and German and TurboParser (Martins
et al. 2010) for English (see Appendix A for details). The Stanford parser is fundamen-
tally a phrase-structure parser and generates dependency trees via head rules, but we
chose it for our experiments for its ease of use and compatibility with the tokenization
we used, particularly the Chinese segmentation which we obtained from the Stanford
Chinese segmenter.15
For Urdu and Malagasy, we turn to unsupervised parsing. To measure the impact
of using unsupervised parsers, we also performed experiments in which we replaced
supervised parsers for Chinese and English with unsupervised counterparts. We now
describe how we trained unsupervised parsers for these four languages.
15 More dependency parsers have been made available by the research community since we began this
research and would be natural choices for further experimentation, such as ParZu (Sennrich et al. 2009)
for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and
DuDuPlus (Chen et al. 2012) for Chinese.
382
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
The most common approach to unsupervised parsing is to train models on sen-
tences from treebanks (without using the annotated trees, of course) along with their
gold standard POS tags. This practice must be changed if we wish to use unsupervised
parsing for machine translation, because we do not have gold standard POS tags for
our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using
automatic POS tags for dependency grammar induction can work as well as or better
than gold standard POS tags. For syntax-based translation, Zollmann and Vogel (2011)
showed that unsupervised tags could work as well as those from a supervised POS
tagger.
For Urdu and Malagasy, we use fully unsupervised POS tagging, using the ap-
proach from Berg-Kirkpatrick et al. (2010) with 40 tags. We use the ?direct gradient?
version optimized by L-BFGS (Liu and Nocedal 1989). For Chinese and English, we
use the gold standard POS tags from their respective treebanks for training the parser,
then use the Stanford POS tagger (Toutanova et al. 2003) to tag the parallel data, tuning,
and test sets. As our dependency parsing model, we use the dependency model with
valence (Klein and Manning 2004) initialized with a convex initializer (Gimpel and
Smith 2012a). The training procedure is described in Gimpel (2012). Our Chinese and
English unsupervised parsers are roughly 30 percentage points worse than supervised
parsers in dependency attachment accuracy on standard treebank test sets.
We also compared the supervised and unsupervised parsers to a uniform-at-random
parser. Well-known algorithms exist for sampling derivations under a context-free
grammar for a sentence (Johnson, Griffiths, and Goldwater 2007). These algorithms can
be used to sample projective dependency trees by representing a projective dependency
grammar using a context-free grammar (Smith 2006; Johnson 2007). We used cdec (Dyer
et al. 2010) to sample projective dependency trees uniformly at random for each
sentence.16
We only compared the random parser for source-side parsing. Swapping parsers for
the target language requires parsing the target side of the parallel corpus, rerunning
rule extraction and feature computation with the new parses, and finally re-tuning to
learn new feature weights. By contrast, changing the source-side parser only requires
re-parsing the source side of the tuning and test sets and re-tuning.
7.2 Results
We now present our main results, shown in Tables 6?9. We see that enlarging the
search space results in gains in BLEU, as Moses with stack size 500 typically out-
performs Moses with stack size 200. For DE?EN (Table 6), SSVM reranking im-
proves performance even without adding any more features, pushing the numbers
close to that of Hiero; and adding our QPD features does not provide any additional
improvement.
For the other language pairs, however, we do see significant gains over the Moses
baselines. For ZH?EN (Table 7), we see an average gain of 0.5 BLEU over the best
Moses baseline when using target syntactic features (TGTTREE), and a total average gain
of 0.7 BLEU with the full QPD model (TGTTREE + TREETOTREE). The QPD numbers
still lag behind the Hiero results on average, but are statistically indistinguishable from
Hiero on two of the three test sets. Our QPD features are able to mostly close the
16 We thank Chris Dyer for implementing this feature in cdec for us.
383
Computational Linguistics Volume 40, Number 2
Table 6
%BLEU on tune and test sets for DE?EN translation, comparing the baselines to our QPD model
with target syntactic features (TGTTREE) and then also with source syntax (+ TREETOTREE).
Here, merely using the additional round of tuning with the SSVM reranker improves the BLEU
score to 19.9, which is statistically indistinguishable from the two QPD feature sets. Differences
between Hiero and the three 19.9 numbers are at the border of statistical significance; the first
two are statistically indistinguishable from Hiero but the third is different at p = 0.04.
German?English
model notes tune test
Moses RAMPION, S = 200 16.2 19.0
RAMPION, S = 500 16.2 19.2
SSVM reranking 16.9 19.9
QPD TGTTREE 17.2 19.9
TGTTREE + TREETOTREE 17.1 19.9
Hiero RAMPION 17.1 20.1
Table 7
%BLEU on tune and test sets for ZH?EN translation, showing the contribution of feature sets in
our QPD model. Both QPD models are significantly better than the best Moses numbers on test
sets 1 and 2, but not on test set 3. The full QPD model is significantly better than the version with
only TGTTREE features on test set 1 but statistically indistinguishable on the other two test sets.
Hiero is significantly better than the full QPD model on test set 2 but not on the other two.
Chinese?English
model notes tune test 1 test 2 test 3 test avg.
Moses RAMPION, S = 200 36.0 35.5 34.3 31.3 33.7
RAMPION, S = 500 36.2 36.1 34.6 31.8 34.2
SSVM reranking 36.3 36.1 34.6 31.8 34.2
QPD TGTTREE 37.1 36.8 35.3 32.0 34.7
TGTTREE + TREETOTREE 37.3 37.2 35.5 31.9 34.9
Hiero RAMPION 37.3 37.4 36.1 32.1 35.2
performance gap between Moses and Hiero, suggesting that the Moses search space
(and even our heavily pruned Moses phrase lattices) has the potential for significant
improvements when using the right features.
Results for UR?EN translation are shown in Table 8. Here we only have a super-
vised parser for English, so the TREETOTREE features are incorporated using our unsu-
pervised Urdu parser. All QPD results are significantly better than all Moses baseline
results, but there is no significant difference between the two QPD feature sets. This
may be due to our use of unsupervised parsing; perhaps the Urdu parses are too noisy
for us to see any benefit from the TREETOTREE features. In Section 7.2.1 we measure the
impact of using unsupervised parsing for ZH?EN translation. Hiero still significantly
outperforms the QPD model, although we have halfway closed the gap between Moses
and Hiero.
384
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table 8
%BLEU on tune and test sets for UR?EN translation, using our unsupervised Urdu parser to
incorporate source syntactic features. The two QPD rows are statistically indistinguishable on
both test sets. Both are significantly better than all Moses results, but Hiero is significantly better
than all others.
Urdu?English
model notes tune test 1 test 2 test avg.
Moses RAMPION, S = 200 24.6 24.6 24.5 24.5
RAMPION, S = 500 24.7 24.8 24.9 24.8
SSVM reranking 24.9 24.4 24.7 24.6
QPD TGTTREE 25.8 25.4 25.5 25.4
TGTTREE + (unsup.) TREETOTREE 25.8 25.4 25.6 25.5
Hiero RAMPION 25.7 26.4 26.7 26.6
Table 9
%BLEU on tune and test sets for EN?MG translation, using a supervised English parser and an
unsupervised Malagasy parser. The 15.6 BLEU reached by the full QPD model is statistically
significantly better than all other results on the test set. All other test set numbers are statistically
indistinguishable.
English?Malagasy
model notes tune test
Moses RAMPION, S = 200 17.6 15.1
RAMPION, S = 500 17.8 15.1
SSVM reranking 17.8 15.1
QPD (unsup.) TGTTREE 17.6 15.2
(unsup.) TGTTREE + TREETOTREE 17.9 15.6
Hiero RAMPION 17.4 15.0
For EN?MG translation (Table 9), we see significant gains in BLEU over both
Moses and Hiero when using the full QPD model.17 We used unsupervised parsing
to incorporate TGTTREE features but we only see a statistically significant improvement
when we add TREETOTREE features, which use a supervised English parser.
7.2.1 Impact of Unsupervised Parsing. Table 10 shows results when comparing parsers
for ZH?EN translation. We pair supervised and unsupervised parsers for English and
Chinese. The final row shows the Moses BLEU scores for comparison.
17 For the EN?MG experiments, we modified our initialization procedure for the QPD feature weights.
When using the same initialization as the other language pairs (setting QPD probability feature weights
to 0.002 and all other QPD weights to 0), we found that SSVM reranking did not find any higher BLEU
score in the initial k-best lists than the 1-best translations for all sentences. So we multiplied the initial
QPD weights by 10 in an effort to inject more diversity in the initial k-best lists.
385
Computational Linguistics Volume 40, Number 2
Table 10
%BLEU on tune and test sets when comparing parsers for ZH?EN translation. QPD uses all
features, including TGTTREE and TREETOTREE. The table first pairs supervised English parsing
with supervised, unsupervised, and random Chinese parsing, then pairs unsupervised English
parsing with supervised and unsupervised Chinese parsing. ? = significantly better than
sup/sup, ? = significantly worse than sup/sup.
EN parser ZH parser tune test 1 test 2 test 3 avg. test
%BLEU %BLEU %BLEU %BLEU %BLEU
Q
P
D sup.
sup. 37.3 37.2 35.5 31.9 34.9
unsup. 37.2 37.0 35.8? 31.8 34.9
random 37.1 36.5? 35.2 31.6? 34.4
unsup. sup. 37.2 37.1 35.3 31.7
? 34.7
unsup. 37.2 36.8? 35.3 31.5? 34.5
Moses, RAMPION, S = 500 36.2 36.1? 34.6? 31.8 34.2
When using supervised English parsing, we find that using our unsupervised
Chinese parser in place of the Stanford parser leads to the same average test set BLEU
score. When instead using random Chinese parses, we see a significant drop on two of
the three test sets and an average decrease of 0.5 BLEU. When pairing unsupervised
English parsing with supervised Chinese parsing, we see an average drop of just 0.2
BLEU compared to the fully supervised case. When both parsers are unsupervised,
BLEU scores drop further but are still above the best Moses baseline on average.
One idea that we have not explored is to parse our parallel corpus using each
parser (unsupervised and supervised), then extract rules consistent with any of the
parses. This might give us some of the benefits of forest-based rule extraction, which has
frequently been shown to improve translation quality (Liu et al. 2007; Mi, Huang, and
Liu 2008; Mi and Huang 2008). Similarly, because we train systems for several language
pairs, we could pool the rules extracted from all parallel corpora for computing target-
syntactic features. For example, adding the English phrase dependency rules from the
DE?EN corpus could improve performance of our ZH?EN and UR?EN systems.
Moving beyond translation, we could use the pool of extracted rules from all systems
(and using all parsers) to build monolingual phrase dependency parsers for use in other
applications (Wu et al. 2009).
7.2.2 Feature Ablation. We performed feature ablation experiments for UR?EN transla-
tion, shown in Table 11. Starting with TGTTREE features, which consist of word (WORD),
cluster (CLUST), and configuration (CFG) feature sets, we alternately removed each of
the three. We find only a small (and statistically insignificant) drop in BLEU when
omitting word features, but a larger drop when omitting word cluster features. This
may be due to the small size of our training data for UR?EN (approximately 1 million
words of parallel text). With limited training data, it is not surprising that unlexicalized
features like the cluster and configuration features would show a stronger effect than
the lexicalized features.
7.3 Human Evaluation
We focused on UR?EN and ZH?EN translation for our manual evaluation, as these
language pairs showed the largest gains in BLEU when using our QPD model. We
386
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table 11
Feature ablation experiments for UR?EN translation with string-to-tree features, showing the
drop in BLEU when separately removing word (WORD), cluster (CLUST), and configuration
(CFG) feature sets. ? = significantly worse than TGTTREE. Removing word features causes no
significant difference. Removing cluster features results in a significant difference on both test
sets, and removing configuration features results in a significant difference on test 2 only.
Urdu?English
model notes tune test 1 test 2 test avg. (?)
Moses SSVM reranking 24.9 24.4 24.7 24.6
QPD TGTTREE = WORD + CLUST + CFG 25.8 25.4 25.5 25.4
TGTTREE ? WORD 25.6 25.0 25.5 25.2 (?0.2)
TGTTREE ? CLUST 25.4 24.8? 24.9? 24.9 (?0.5)
TGTTREE ? CFG 25.1 25.1 25.0? 25.0 (?0.4)
began by performing a human evaluation using Amazon Mechanical Turk (MTurk)
in order to validate the BLEU differences against human preference judgments and to
identify translations that were consistently judged better under each model for follow-
up manual evaluation.
7.3.1 Procedure. We first removed sentences with unknown words, as we feared they
would only confuse judges.18 We then randomly selected 500 sentences from UR?EN
test 2 and 500 from the concatenation of ZH?EN test 1 and test 2. For each of the 1,000
sentences, we chose a single reference translation from among the four references to
show to judges.19 All text was detokenized. Judges were shown the reference transla-
tion, the translation from the Moses system with SSVM reranking, and the translation
from our QPD system with the full feature set. We randomized the order in which the
two machine translations were presented. Judges were asked to select which translation
was closer in meaning to the reference; alternatively, they could indicate that they were
of the same quality. We obtained judgments like these from three judges for each of the
1,000 sentences.
7.3.2 Results and Analysis. Table 12 shows the results of our MTurk experiments. If a
sentence was judged to be translated better by one system more often than the other,
it was counted as a victory for that system. The QPD translations for 40?43% of the
sentences were preferred over Moses, but for 28?33% of the sentences, the reverse was
true.
We can use these judgments to study when and how our system improves over
Moses, and also when Moses still performs better. For a follow-up manual evaluation,
we looked at ZH?EN sentences for which all three judges selected either Moses or
the QPD model; these should be the clearest examples of success for each system. In
18 Although this filtering step may introduce bias, we confirmed that the system differences in BLEU were
similar whether looking at sentences with unknown words, those without unknown words, or all
sentences.
19 For UR?EN test 2 and ZH?EN test 2, we chose the first reference set from the four provided. For
ZH?EN test 1, we instead chose the second reference set because its average length was closer to the
average across the four reference sets.
387
Computational Linguistics Volume 40, Number 2
Table 12
Results of human evaluation performed via Amazon Mechanical Turk. The percentages
represent the portion of sentences for which one system had more preference judgments
than the other system. If a sentence had an equal number of judgments for the two systems,
it was counted in the final row (?neither preferred?).
% of sentences preferred
ZH?EN UR?EN
Moses, SSVM reranking 33.4% 28.6%
QPD, TGTTREE + TREETOTREE 40.6% 42.8%
neither preferred 26.0% 28.6%
looking at these sentences, we attempted to categorize the primary reasons why all three
judges would have preferred one system?s output over the other. We began with two
broad categories of improvement: word choice and word order. We divided word choice
improvements into two subcategories: those involving verbs and those involving words
other than verbs. The reason we made this distinction is because some differences in
non-verb translation are not as crucial for understanding a sentence as differences in
verb translation or word order. Anecdotally, we observed that when one sentence has
a better verb translation and the other has a better preposition translation, judges tend
to prefer the translation with the better verb. We noted some sentences that fit multiple
categories, but in our analysis we chose a single category that we deemed to be the most
important factor in the judges? decisions.
Of the 26 sentences for which Moses output was preferred unanimously, we agreed
with the consensus on 25 and found that 19 of these improved due to better word
choice, most frequently (13 out of 19) for words other than verbs. Only 6 of the 25
were determined to be preferred due to word order. The top section of Table 13 shows
representative examples when Moses? translations were unanimously preferred. Moses
handles prepositions and other function words better than the QPD model in these
examples. This may occur due to the reliance of phrase-based systems upon strong
n-gram language models to ensure local fluency. The QPD model uses all of Moses?
features, including the same n-gram language model, but it adds many other features
that score longer-distance word order and may be overwhelming the n-gram model in
certain cases.
For the 44 sentences for which QPD output was unanimously preferred, we agreed
with the judges on 42. Of these, we found that 15 had improved word order, 14 had
improvements in verb word choice, and 13 had improved word choice for non-verbs.
So the QPD model?s improvements were due to word order on 36% of unanimous
sentences, compared with Moses? 24%, suggesting that the QPD model?s strength is
in improving word order. The lower section of Table 13 shows representative examples.
Consider the final example in the table. The Moses translation has better local fluency,
but mixes words across clauses, confusing the meaning. The QPD translation has two
local disfluencies (?because law-abiding citizen hopes to? and ?don?t need to fear will
attack?), but has no erroneous word reordering across clause boundaries.
7.4 Decoding Speed
So far we have reported BLEU scores for various feature sets and parsers, but we
have not discussed decoding speed. BLEU improvements may not be worth substantial
388
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
reductions in translation speed. In this section we report decoding speeds and BLEU
scores for UR?EN translation as pruning thresholds are varied. Our lattice dependency
parsing decoding algorithm is exact, but two pruning stages precede lattice parsing, as
discussed in Section 6.3: (1) pruning the phrase lattices based on the phrase-based model
Table 13
Examples from manual evaluation of ZH?EN output, selected from sentences with unanimous
preference judgments for Moses (top) and our QPD model (bottom). R = reference; M = Moses,
SSVM reranking; Q = QPD, TGTTREE + TREETOTREE.
Moses preferred:
R the purpose of cheney?s trip is to win supports of middle east countries in case of a u.s.
attack on iraq.
M cheney?s trip is to lobby the middle east countries to support the us attack on iraq.
Q cheney?s trip was to lobby the middle east countries support us attacks on iraq.
R he said, "after this conference, our position is clear to all."
M he said: "this time after the talks, our position is well known."
Q he said: "this round of talks, our position is well known."
R ukraine condemned us for interfering its internal affairs.
M ukraine condemned the united states for interfering in its internal affairs.
Q ukraine condemned the united states of interfering in its internal affairs.
R former serbian interior minister, vlajko stojiljkovic, attempts to commit suicide.
M yugoslavia, a former serbian interior minister tried to commit suicide.
Q interior minister tried to commit suicide before the serbian republic of yugoslavia.
R arafat died in a french hospital of an unknown illness on november 11.
M arafat died in a french hospital on nov. 11, due to an unidentified disease.
Q arafat died on nov. 11 in a french hospital, an unidentified disease.
QPD preferred:
R bush: nato must forge a new relationship with russia
M bush said that the establishment of a new type of relationship between russia and nato
must
Q bush says nato must build new relations with russia
R that would portend increased flows of glacial ice from antarctica and a rise in global sea
levels, endangering low-lying land areas.
M this bodes well for the antarctica ice flow will increase, the global sea level will rise in the
lowlands, will cause harm to the region.
Q this indicates that the antarctica?s ice flow will increase, global sea level will rise, and will
cause harm to the lowlands region.
R this is the first sandstorm in north korea since spring this year.
M this is the first since the spring of this year the sandstorms.
Q this is the first sandstorm of the korean experience since spring this year.
R "we really have to tackle the problem of street-crime. law-abiding citizens want to feel safe
when they walk on the street. they want a peaceful life and untroubled by attack," he said.
M he said: "we really should rectify the order because of law-abiding citizens, in the hope of
peace and security on the streets and on the living, not to be afraid of attacks."
Q he said: "we really should rectify the order in the streets, because law-abiding citizen hopes
to secure a safe life on the streets, and don?t need to fear will attack."
389
Computational Linguistics Volume 40, Number 2
scores, and (2) pruning the search space deterministically based on source- and target-
side limits on dependency lengths. In this section, we measure the impact of the latter
type of pruning only.20
We vary maximum dependency lengths and we report BLEU scores and decoding
speeds. We find that we can set these limits to be relatively strict and get similar BLEU
scores in less time. In all previous experiments, we used a source-side limit ?x of 15
and a target-side limit ?y of 20. That is, all target-side phrase dependencies may cover
a maximum of 20 words in the target sentence, and the number of words between the
aligned source phrases can be at most 15. We often use a larger value of ?y because
it is constraining an upper bound on the number of words crossed in the translation,
whereas?x constraints the exact number of source words crossed by a dependency (see
Section 6.3.3 for details).
For timing experiments, we ran a single decoding thread on a Sun Fire X2200 M2 x64
server with two 2.6-GHz dual-core CPUs. Decoding during tuning is time-consuming,
because we generate unique 150-best lists for each iteration, so we only use two max
dependency length settings for tuning. But given trained models, finding the 1-best
output on the test data is much faster. So we experimented with more pruning settings
for decoding. Table 14 shows our results. The upper table reminds us of the baseline
BLEU scores. The lower table shows what happens when we train with two pruning
settings: (?x = 10,?y = 15) and (?x = 15,?y = 20), and test with many others.
The times reported only include the time required for running the Floyd-Warshall
algorithm on the lattice and performing lattice dependency parsing. We use the Moses
decoder for lattice generation; this typically takes only slightly longer than ordinary
decoding, which is generally in the range of a couple seconds per sentence, depending
on how the phrase table and language model are accessed. The average time required
to run the Floyd-Warshall algorithm on the lattices is approximately 0.8 seconds per
sentence, so it begins to dominate the total time as the pruning thresholds go below
(5, 5). The earlier numbers in this section used (?x = 15,?y = 20) for both tuning and
testing, which causes test-time decoding to take approximately 6 seconds per sentence,
as shown in the table. We can see that we can use stricter constraints during test-
time decoding only (e.g., (?x = 5,?y = 10)) and speed this up by a factor of 3 while
only losing 0.1 BLEU. The only severe drops in BLEU appear when using thresholds
below (5, 5).
8. Conclusion and Future Work
We presented a new approach to machine translation that combines phrases, depen-
dency syntax, and quasi-synchronous tree-to-tree relationships. We introduced several
categories of features for dependency-based translation, including string-to-tree and
tree-to-tree features. We proposed lattice dependency parsing to solve the decoding
problem and presented ways to speed up the search and prune the search space. We
presented experimental results on seven test sets across four language pairs, finding
statistically significant improvements over strong phrase-based baselines on five of the
seven. Manual inspection reveals improvement in the translation of verbs, an important
component in preserving the meaning of the source text. We showed that unsupervised
20 Ideally we could also measure the impact of pruning the phrase lattices to various sizes, but this would
require the time-consuming process of filtering our phrase dependency tables for each lattice size, so we
have not yet tested the effect of this pruning systematically.
390
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table 14
%BLEU on tune and test sets for UR?EN translation, comparing several settings for maximum
dependency lengths in the decoder (?x is for the source side and?y is for the target side). The
upper table shows Moses BLEU scores for comparison. The lower table compares two max
dependency length settings during tuning, and several for decoding on the test sets, showing
both BLEU scores and average decoding times per sentence. See text for discussion.
tune
%BLEU
test 1
%BLEU
test 2
%BLEU
avg. test
%BLEU
Moses, SSVM reranking 24.9 24.4 24.7 24.6
tune
(?x ,?y )
tune
%BLEU
test
(?x ,?y )
test 1
%BLEU
test 2
%BLEU
avg. test
%BLEU
time
(sec./sent.)
Q
P
D
(10, 15) 25.9
(3, 3) 21.7 22.3 22.0 1.11
(3, 5) 23.2 23.5 23.3 1.28
(5, 5) 24.9 24.7 24.8 1.41
(5, 10) 25.2 25.0 25.1 2.09
(10, 10) 25.4 25.3 25.3 3.01
(10, 15) 25.3 25.4 25.4 4.00
(15, 20) 25.5 25.5 25.5 6.15
(20, 20) 25.6 25.5 25.6 7.18
(20, 25) 25.5 25.5 25.5 7.83
(15, 20) 25.8
(3, 3) 22.2 22.8 22.5 1.11
(3, 5) 23.2 24.0 23.6 1.28
(5, 5) 25.2 25.2 25.2 1.41
(5, 10) 25.3 25.4 25.4 2.08
(10, 10) 25.2 25.5 25.4 3.01
(10, 15) 25.4 25.6 25.5 4.02
(15, 20) 25.4 25.6 25.5 6.07
(20, 20) 25.4 25.6 25.5 7.16
(20, 25) 25.4 25.6 25.5 7.96
dependency parsing can be used effectively within a tree-to-tree translation system,
enabling the use of our system for low-resource languages like Urdu and Malagasy. This
result offers promise for researchers to apply syntactic translation models to languages
for which we do not have manually annotated corpora.
There are many directions for future work. Unsupervised learning of syntax can be
improved if parallel text is available and we have a parser for one of the languages: The
parallel text can be word-aligned and the annotations can be projected across the word
alignments (Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentoswki 2001). The
projected parses can be improved by applying manually written rules (Hwa et al. 2005)
or modeling the noisy projection process (Ganchev, Gillenwater, and Taskar 2009; Smith
and Eisner 2009). If we do not have parsers for either language, grammar induction
models have been developed to exploit parallel text without using any annotations
on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also
available for grammar induction using treebanks in different languages that are not
built on parallel data (Cohen, Das, and Smith 2011).
Researchers have recently begun to target learning of parsers specifically for ap-
plications like machine translation. Hall et al. (2011) developed a framework to train
supervised parsers for use in particular applications by optimizing arbitrary evaluation
metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering
391
Computational Linguistics Volume 40, Number 2
in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised
learning of syntactic structure in parallel text to target reordering phenomena.
In addition, we may not need full monolingual syntactic parses to obtain the
benefits of syntax-based translation modeling. Indeed, the widely used hierarchical
phrase-based model of Chiang (2005) induces a synchronous grammar from parallel
text without any linguistic annotations. Zollmann and Vogel (2011) and Zollmann (2011)
showed that using a supervised POS tagger to label these synchronous rules can im-
prove performance up to the level of a model that uses a supervised full syntactic parser.
They further showed that unsupervised POS taggers could be effectively used in place
of supervised taggers. These results suggest that it may be fruitful to explore the use of
simpler annotation tools such as POS taggers, whether supervised or unsupervised, in
order to apply syntax-based translation to new language pairs.
Appendix A. Language Pairs
We consider four language pairs in this article, two for which large amounts of par-
allel data are available and two involving low-resource languages. The large-data
language pairs we consider are Chinese?English (ZH?EN) and German?English
(DE?EN). The two low-resource language pairs are Urdu?English (UR?EN) and
English?Malagasy (EN?MG).
For all language pairs, English text was parsed using TurboParser version 0.1
(Martins et al. 2010). We used a second-order model with sibling and grandparent
features that was trained to maximize conditional log-likelihood.
The following sections describe the procedures used to prepare the data for each
language pair. The line and token counts are summarized in Tables A.1?A.3.
Chinese?English. For ZH?EN, we used 303k sentence pairs from the FBIS corpus
(LDC2003E14). We segmented the Chinese data using the Stanford Chinese seg-
menter (Chang, Galley, and Manning 2008) in ?CTB? mode, giving us 7.9M Chinese
Table A.1
Statistics of data used for rule extraction and feature computation.
lines source tokens target tokens
ZH?EN 302,996 7,984,637 9,350,506
DE?EN 1,010,466 23,154,642 24,044,528
UR?EN 165,159 1,169,367 1,083,807
EN?MG 83,670 1,500,922 1,686,022
Table A.2
Statistics of data used for tuning. The numbers of target tokens are averages across four
reference translations for ZH?EN and UR?EN, rounded to the nearest token.
lines source tokens target tokens
ZH?EN 919 24,152 28,870
DE?EN 1,300 29,791 31,318
UR?EN 882 18,004 16,606
EN?MG 1,359 28,408 32,682
392
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Table A.3
Test data statistics. The numbers of target tokens are averages across four reference translations
for ZH?EN and UR?EN, rounded to the nearest token.
test 1 test 2 test 3
lines source target lines source target lines source targettokens tokens tokens tokens tokens tokens
ZH?EN 878 22,708 26,877 1,082 29,956 35,227 1,664 38,787 48,169
DE?EN 2,525 62,699 65,595 N/A N/A
UR?EN 883 21,659 19,575 1,792 42,082 39,889 N/A
EN?MG 1,133 24,362 28,301 N/A N/A
tokens and 9.4M English tokens. For tuning and testing, we used MT03 (?tune?), MT02
(?test 1?), MT05 (?test 2?), and MT06 (?test 3?). The Chinese text was parsed using the
Stanford parser (Levy and Manning 2003).
German?English. We started with the Europarl corpus provided for the WMT12 shared
task. We tokenized both sides, filtered sentences with more than 50 words, and down-
cased the text. We then discarded every other sentence, beginning with the second,
leaving half of the corpus remaining. We did this to speed our experiment cycle. The
corpus still has about 850k sentence pairs. We did the same processing with the news
commentary corpus, but did not discard half of the sentences. There were about 150k
news commentary sentences, giving us a total of about 1M lines of DE?EN parallel
training data. For tuning, we used the first 1,300 sentences from the 2008 2,051-sentence
test set (?tune?). For testing, we used the 2009 test set (?test 1?). The tuning/test sets are
from the newswire domain. The German text was parsed using the factored model in
the Stanford parser (Rafferty and Manning 2008).
Urdu?English. For UR?EN, we used parallel data from the NIST MT08 evaluation.
Although there are 165,159 lines of parallel data, there are many dictionary and
otherwise short entries, so it is close to an order of magnitude smaller than ZH?EN.
We used half of the documents (882 sentences) from the MT08 test set for tuning
(?tune?). We used the remaining half for one test set (?test 1?) and MT09 as a second
test set (?test 2?). The Urdu text was parsed using an unsupervised dependency parser
as described in Section 7.1.3.
English?Malagasy. For EN?MG translation, we used data obtained from the Global
Voices weblogging community (http://globalvoicesonline.org), prepared by Victor
Chahuneau.21 We used release 12.06 along with its recommended training, development
(tuning), and test set. Like Urdu, the Malagasy text was parsed using an unsupervised
dependency parser as described in Section 7.1.3.
Appendix B. Experimental Details
Appendix A contains details about the data sets used in our experiments. Other experi-
mental details are given here.
21 The data are publicly available at http://www.ark.cs.cmu.edu/global-voices/.
393
Computational Linguistics Volume 40, Number 2
Translation Models. For phrase-based models, we used the Moses machine translation
toolkit (Koehn et al. 2007). We mostly used default settings and features, includ-
ing the default lexicalized reordering model. Word alignment was performed using
GIZA++ (Och and Ney 2003) in both directions, the grow-diag-final-and heuristic
was used to symmetrize the alignments, and a max phrase length of 7 was used for
phrase extraction. The only exception to the defaults was setting the distortion limit to
10 in all experiments.
Language Models. Language models were trained using the target side of the parallel
corpus in each case augmented with 24,760,743 lines (601,052,087 tokens) of randomly
selected sentences from the Gigaword v4 corpus (excluding the New York Times and Los
Angeles Times). The minimum count cutoff for unigrams, bigrams, and trigrams was one
and the cutoff for fourgrams and fivegrams was three. Language models were estimated
using the SRI Language Modeling toolkit (Stolcke 2002) with modified Kneser-Ney
smoothing (Chen and Goodman 1998). Language model inference was performed using
KenLM (Heafield 2011) within Moses.
For EN?MG, we estimated a 5-gram language model using only the target side
of the parallel corpus, which contained 89,107 lines with 2,031,814 tokens. We did not
use any additional Malagasy data for estimating the EN?MG language models in
order to explore a scenario in which target-language text is limited or expensive to
obtain.
Word Clustering. Brown clusters (Brown et al. 1992) were generated using code provided
by Liang (2005). For each language pair, 100 word clusters were generated for the target
language. The implementation allows the use of a token count cutoff, which causes the
algorithm to only cluster words appearing more times than the cutoff. When the clusters
are used, all words with counts below the cutoff are assigned a special ?unknown word?
cluster. So in practice, if a clustering with 100 clusters is generated, there are 101 clusters
used when the clusters are applied.
For ZH?EN, DE?EN, and UR?EN, the target side of the parallel data was used
along with 412,000 lines of randomly selected Gigaword data comprising 10,001,839
words. This data was a subset of the Gigaword data used for language modeling. The
count cutoff was 2. For EN?MG, only the target side of the parallel corpus was used.
The count cutoff was 1. In all cases, the data was tokenized and downcased prior to
cluster generation.
Acknowledgments
We thank the anonymous reviewers as well
as Dhruv Batra, Jaime Carbonell, David
Chiang, Shay Cohen, Dipanjan Das, Chris
Dyer, Jason Eisner, Alon Lavie, Andre?
Martins, Greg Shakhnarovich, David Smith,
Stephan Vogel, and Eric Xing. This research
was supported in part by the National
Science Foundation through grant
IIS-0844507, the U.S. Army Research
Laboratory and the U.S. Army Research
Office under contract/grant number
W911NF-10-1-0533, and Sandia National
Laboratories (fellowship to K. Gimpel).
References
Aho, A. V. and J. D. Ullman. 1969. Syntax
directed translations and the pushdown
assembler. Journal of Computer and System
Sciences, 3(1):37?56.
Ambati, V. and A. Lavie. 2008. Improving
syntax driven translation models
by re-structuring divergent and
non-isomorphic parse tree structures. In
Proceedings of the Eighth Conference of the
Association for Machine Translation in the
Americas, pages 235?244, Waikiki, HI.
Berg-Kirkpatrick, T., A. Bouchard-Co?te?,
J. DeNero, and D. Klein. 2010. Painless
394
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
unsupervised learning with features.
In Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL), pages 582?590,
Los Angeles, CA.
Birch, A., P. Blunsom, and M. Osborne. 2009.
A quantitative analysis of reordering
phenomena. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 197?205, Athens.
Blunsom, P. and T. Cohn. 2010. Unsupervised
induction of tree substitution grammars
for dependency parsing. In Proceedings
of the 2010 Conference on Empirical
Methods in Natural Language Processing,
pages 1,204?1,213, Cambridge, MA.
Bohnet, B. 2010. Top accuracy and
fast dependency parsing is not a
contradiction. In Proceedings of the 23rd
International Conference on Computational
Linguistics (Coling 2010), pages 89?97,
Beijing.
Brown, P. F., P. V. deSouza, R. L. Mercer,
V. J. Della Pietra, and J. C. Lai. 1992.
Class-based n-gram models of natural
language. Computational Linguistics,
18(4):467?479.
Buchholz, S. and E. Marsi. 2006. CoNLL-X
shared task on multilingual dependency
parsing. In Proceedings of the Tenth
Conference on Computational Natural
Language Learning (CoNLL-X),
pages 149?164, New York City.
Carreras, X. and M. Collins. 2009.
Non-projective parsing for statistical
machine translation. In Proceedings
of the 2009 Conference on Empirical
Methods in Natural Language Processing,
pages 200?209, Singapore.
Chang, P., M. Galley, and C. Manning. 2008.
Optimizing Chinese word segmentation
for machine translation performance.
In Proceedings of the Third Workshop
on Statistical Machine Translation,
pages 224?232, Columbus, OH.
Charniak, E. and M. Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, MI.
Chen, S. and J. Goodman. 1998. An empirical
study of smoothing techniques for
language modeling. Technical report 10-98,
Harvard University.
Chen, W., J. Kazama, K. Uchimoto, and
K. Torisawa. 2012. Exploiting subtrees in
auto-parsed data to improve dependency
parsing. Computational Intelligence,
28(3):426?451.
Chiang, D. 2005. A hierarchical phrase-based
model for statistical machine translation.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL?05), pages 263?270, Ann Arbor, MI.
Chiang, D. 2007. Hierarchical phrase-based
translation. Computational Linguistics,
33(2):201?228.
Chiang, D. 2010. Learning to translate with
source and target syntax. In Proceedings
of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 1,443?1,452, Uppsala.
Christodoulopoulos, C., S. Goldwater, and
M. Steedman. 2010. Two decades of
unsupervised POS induction: How far
have we come? In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 575?584,
Cambridge, MA.
Cohen, S. B. 2011. Computational Learning of
Probabilistic Grammars in the Unsupervised
Setting. Ph.D. thesis, Carnegie Mellon
University, Pittsburgh, PA.
Cohen, S. B., D. Das, and N. A. Smith. 2011.
Unsupervised structure prediction with
non-parallel multilingual guidance.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 50?61, Edinburgh.
Cowan, B., I. Kuc?erova?, and M. Collins. 2006.
A discriminative model for tree-to-tree
translation. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 232?241,
Sydney.
Crego, J. M. and F. Yvon. 2009. Gappy
translation units under left-to-right SMT
decoding. In Proceedings of the Meeting of the
European Association for Machine Translation
(EAMT), pages 66?73, Barcelona.
Das, D. and N. A. Smith. 2009. Paraphrase
identification as probabilistic
quasi-synchronous recognition.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 468?476, Suntec.
DeNero, J. and J. Uszkoreit. 2011. Inducing
sentence structure from parallel corpora
for reordering. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 193?203,
Edinburgh.
Ding, Y. and M. Palmer. 2005. Machine
translation using probabilistic
395
Computational Linguistics Volume 40, Number 2
synchronous dependency insertion
grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 541?548,
Ann Arbor, MI.
Dorr, B. J. 1994. Machine translation
divergences: a formal description and
proposed solution. Computational
Linguistics, 20(4):597?633.
Dyer, C., A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan,
V. Eidelman, and P. Resnik. 2010. cdec:
A decoder, alignment, and learning
framework for finite-state and context-free
translation models. In Proceedings of the
ACL 2010 System Demonstrations,
pages 7?12, Uppsala.
Eisner, J. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
Conference on Computational Linguistics
(COLING-96), pages 340?345, Copenhagen.
Eisner, J. 2003. Learning non-isomorphic tree
mappings for machine translation.
In Proceedings of ACL, pages 205?208,
Sapporo.
Eisner, J., E. Goldlust, and N. A. Smith. 2005.
Compiling Comp Ling: Practical weighted
dynamic programming and the Dyna
language. In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 281?290, Vancouver.
Eisner, J. and N. A. Smith. 2005. Parsing with
soft and hard constraints on dependency
length. In Proceedings of IWPT,
pages 30?41, Vancouver.
Floyd, R. W. 1962. Algorithm 97: Shortest
path. Communications of the ACM, 5(6):345.
Fox, H. J. 2002. Phrasal cohesion and
statistical machine translation.
In Proceedings of the 2002 Conference
on Empirical Methods in Natural
Language Processing, pages 304?311,
Philadelphia, PA.
Galley, M. and C. D. Manning. 2009.
Quadratic-time dependency parsing for
machine translation. In Proceedings of the
Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 773?781, Suntec.
Galley, M. and C. D. Manning. 2010.
Accurate non-hierarchical phrase-based
translation. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 966?974, Los Angeles, CA.
Ganchev, K., J. Gillenwater, and B. Taskar.
2009. Dependency grammar induction via
bitext projection constraints. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 369?377,
Suntec.
Gao, Y., P. Koehn, and A. Birch. 2011. Soft
dependency constraints for reordering in
hierarchical phrase-based translation.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 857?868, Edinburgh.
Gertz, E. M. and S. J. Wright. 2003.
Object-oriented software for quadratic
programming. ACM Transactions on
Mathematical Software, 29(1):58?81.
Gildea, D. 2003. Loosely tree-based
alignment for machine translation.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics, pages 80?87, Sapporo.
Gimpel, K. 2012. Discriminative Feature-Rich
Modeling for Syntax-Based Machine
Translation. Ph.D. thesis, Carnegie
Mellon University, Pittsburgh, PA.
Gimpel, K. and N. A. Smith. 2008. Rich
source-side context for statistical machine
translation. In Proceedings of the Third
Workshop on Statistical Machine Translation,
pages 9?17, Columbus, OH.
Gimpel, K. and N. A. Smith. 2009a. Cube
summing, approximate inference with
non-local features, and dynamic
programming without semirings. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009),
pages 318?326, Athens.
Gimpel, K. and N. A. Smith. 2009b.
Feature-rich translation by
quasi-synchronous lattice parsing. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 219?228, Singapore.
Gimpel, K. and N. A. Smith. 2011.
Quasi-synchronous phrase dependency
grammars for machine translation. In
Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 474?485, Edinburgh.
Gimpel, K. and N. A. Smith. 2012a.
Concavity and initialization for
unsupervised dependency parsing.
In Proceedings of the 2012 Conference of the
North American Chapter of the Association
for Computational Linguistics: Human
Language Technologies, pages 577?581,
Montre?al.
396
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Gimpel, K. and N. A. Smith. 2012b.
Structured ramp loss minimization for
machine translation. In Proceedings of the
2012 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies,
pages 221?231, Montre?al.
Hall, K., R. McDonald, J. Katz-Brown, and
M. Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple
objectives. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 1,489?1,499,
Edinburgh.
Hanneman, G. and A. Lavie. 2011.
Automatic category label coarsening for
syntax-based machine translation. In
Proceedings of Fifth Workshop on Syntax,
Semantics and Structure in Statistical
Translation, pages 98?106, Portland, OR.
Heafield, K. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of
the Sixth Workshop on Statistical Machine
Translation, pages 187?197, Edinburgh.
Huang, L. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL-08: HLT,
pages 586?594, Columbus, OH.
Huang, L. and D. Chiang. 2005. Better
k-best parsing. In Proceedings of the Ninth
International Workshop on Parsing
Technology, pages 53?64, Vancouver.
Huang, L., K. Knight, and A. Joshi. 2006.
Statistical syntax-directed translation with
extended domain of locality. In Proceedings
of the Association for Machine Translation in
the Americas, pages 66?73, Cambridge, MA.
Hunter, T. and P. Resnik. 2010. Exploiting
syntactic relationships in a phrase-based
decoder: An exploration. Machine
Translation, 24(2):123?140.
Hwa, R., P. Resnik, A. Weinberg, C. Cabezas,
and O. Kolak. 2005. Bootstrapping parsers
via syntactic projection across parallel
texts. Journal of Natural Language
Engineering, 11(3):311?25.
Joachims, T., T. Finley, and Chun-Nam Yu.
2009. Cutting-plane training of structural
SVMs. Machine Learning, 77(1):27?59.
Johnson, M. 2007. Transforming projective
bilexical dependency grammars into
efficiently-parsable CFGs with unfold-fold.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 168?175, Prague.
Johnson, M., T. Griffiths, and S. Goldwater.
2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human
Language Technologies 2007: The Conference
of the North American Chapter of the
Association for Computational Linguistics;
Proceedings of the Main Conference,
pages 139?146, Rochester, NY.
Katz-Brown, J., S. Petrov, R. McDonald,
F. Och, D. Talbot, H. Ichikawa, M. Seno,
and H. Kazawa. 2011. Training a parser
for machine translation reordering.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 183?192, Edinburgh.
Klein, D. and C. D. Manning. 2002. A
generative constituent-context model
for improved grammar induction.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 128?135, Philadelphia, PA.
Klein, D. and C. D. Manning. 2003. Fast exact
inference with a factored model for natural
language parsing. In Advances in Neural
Information Processing Systems 15 (NIPS),
pages 3?10, Vancouver.
Klein, D. and C. D. Manning. 2004.
Corpus-based induction of syntactic
structure: Models of dependency and
constituency. In Proceedings of the 42nd
Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume,
pages 478?485, Barcelona.
Koehn, P. 2004. Statistical significance tests
for machine translation evaluation.
In Proceedings of EMNLP 2004,
pages 388?395, Barcelona.
Koehn, P., H. Hoang, A. Birch,
C. Callison-Burch, M. Federico,
N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague.
Koehn, P., F. J. Och, and D. Marcu. 2003.
Statistical phrase-based translation.
In Proceedings of HLT-NAACL,
pages 48?54, Edmonton.
Ku?bler, S., R. McDonald, and J. Nivre. 2009.
Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan
& Claypool.
Kuhn, J. 2004. Experiments in parallel-text
based grammar induction. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main
Volume, pages 470?477, Barcelona.
Levy, R. and C. D. Manning. 2003. Is it harder
to parse Chinese, or the Chinese treebank?
In Proceedings of the 41st Annual Meeting of
397
Computational Linguistics Volume 40, Number 2
the Association for Computational Linguistics,
pages 439?446, Sapporo.
Li, Z. and S. Khudanpur. 2008. A scalable
decoder for parsing-based machine
translation with equivalent language
model state maintenance. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax
and Structure in Statistical Translation
(SSST-2), pages 10?18, Columbus, OH.
Li, Z., T. Liu, and W. Che. 2012. Exploiting
multiple treebanks for parsing with
quasi-synchronous grammars. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics
(Volume 1: Long Papers), pages 675?684,
Jeju Island.
Liang, P. 2005. Semi-supervised learning
for natural language. Master?s thesis,
Massachusetts Institute of Technology,
Cambridge, MA.
Lin, C. and F. J. Och. 2004. Orange: A method
for evaluating automatic evaluation
metrics for machine translation. In
Proceedings of Coling 2004, pages 501?507,
Geneva.
Lin, D. 2004. A path-based transfer model for
machine translation. In Proceedings of
Coling 2004, pages 625?630, Geneva.
Liu, D. C. and J. Nocedal. 1989. On the
limited memory BFGS method for large
scale optimization. Mathematical
Programming, 45:503?528.
Liu, Y., Y. Huang, Q. Liu, and S. Lin. 2007.
Forest-to-string statistical translation rules.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 704?711, Prague.
Liu, Y., Y. Lu?, and Q. Liu. 2009. Improving
tree-to-tree translation with packed forests.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 558?566, Suntec.
Marcus, M. P., B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19:313?330.
Martins, A. F. T., N. A. Smith, and E. P. Xing.
2009. Concise integer linear programming
formulations for dependency parsing.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural
Language Processing of the AFNLP,
pages 342?350, Suntec.
Martins, A. F. T., N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M. A. T. Figueiredo.
2010. Turbo parsers: Dependency parsing
by approximate variational inference.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing, pages 34?44, Cambridge, MA.
Melamed, I. D. 2003. Multitext grammars
and synchronous parsers. In Proceedings of
the 2003 Conference of the North American
Chapter of the Association for Computational
Linguistics on Human Language Technology -
Volume 1, pages 79?86, Edmonton.
Mi, H. and L. Huang. 2008. Forest-based
translation rule extraction. In Proceedings
of the 2008 Conference on Empirical
Methods in Natural Language Processing,
pages 206?214, Honolulu, HI.
Mi, H., L. Huang, and Q. Liu. 2008.
Forest-based translation. In Proceedings
of ACL-08: HLT, pages 192?199,
Columbus, OH.
Naseem, T., H. Chen, R. Barzilay, and
M. Johnson. 2010. Using universal
linguistic knowledge to guide grammar
induction. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,234?1,244,
Cambridge, MA.
Nederhof, M.-J. 2003. Weighted deductive
parsing and Knuth?s algorithm.
Computational Linguistics, 29(1):135?143.
Nivre, J., J. Hall, S. Ku?bler, R. McDonald,
J. Nilsson, S. Riedel, and D. Yuret.
2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings
of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932,
Prague.
Och, F. J. 2003. Minimum error rate training
in statistical machine translation.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics, pages 160?167, Sapporo.
Och, F. J. and H. Ney. 2003. A systematic
comparison of various statistical
alignment models. Computational
Linguistics, 29(1):19?51.
Papineni, K., S. Roukos, T. Ward, and
W. J. Zhu. 2002. BLEU: A method for
automatic evaluation of machine
translation. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 311?318,
Philadelphia, PA.
Park, J. H., W. B. Croft, and D. A. Smith.
2011. A quasi-synchronous dependence
model for information retrieval. In
Proceedings of the 20th ACM International
Conference on Information and Knowledge
Management, pages 17?26, Glasgow.
398
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
Petrov, S. 2009. Coarse-to-Fine Natural
Language Processing. Ph.D. thesis,
University of California at Berkeley.
Quirk, C., A. Menezes, and C. Cherry.
2005. Dependency treelet translation:
Syntactically informed phrasal SMT.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 271?279,
Ann Arbor, MI.
Rafferty, A. N. and C. D. Manning. 2008.
Parsing three German treebanks:
Lexicalized and unlexicalized baselines.
In Proceedings of the Workshop on Parsing
German, pages 40?46, Columbus, OH.
Riezler, S. and J. T. Maxwell III. 2006.
Grammatical machine translation.
In Proceedings of the Human Language
Technology Conference of the NAACL,
Main Conference, pages 248?255,
New York City.
Seeker, W. and J. Kuhn. 2012. Making ellipses
explicit in dependency conversion for a
German treebank. In Proceedings of the
Eight International Conference on Language
Resources and Evaluation (LREC?12),
pages 3,132?3,139, Istanbul.
Sennrich, R., G. Schneider, M. Volk, and
M. Warin. 2009. A new hybrid dependency
parser for German. In Proceedings of the
German Society for Computational Linguistics
and Language Technology (GSCL),
pages 115?124, Potsdam.
Shen, L., J. Xu, and R. Weischedel. 2008.
A new string-to-dependency machine
translation algorithm with a target
dependency language model.
In Proceedings of ACL-08: HLT,
pages 577?585, Columbus, OH.
Shieber, S. M. and Y. Schabes. 1990.
Synchronous tree-adjoining grammars.
In Proceedings of the 13th International
Conference on Computational Linguistics,
pages 253?258, Helsinki.
Simard, M., N. Cancedda, B. Cavestro,
M. Dymetman, E. Gaussier, C. Goutte,
K. Yamada, P. Langlais, and A. Mauser. 2005.
Translating with non-contiguous phrases.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 755?762, Vancouver.
Sixtus, A. and S. Ortmanns. 1999.
High quality word graphs using
forward-backward pruning. In Proceedings
of the IEEE International Conference
Acoustics, Speech, and Signal Processing,
1999 - Volume 02, pages 593?596,
Phoenix, AZ.
Skut, W., B. Krenn, T. Brants, and
H. Uszkoreit. 1997. An annotation
scheme for free word order languages.
In Proceedings of the Fifth Conference on
Applied Natural Language Processing,
pages 88?95, Washington, DC.
Smith, D. A. and J. Eisner. 2006.
Quasi-synchronous grammars:
Alignment by soft projection of syntactic
dependencies. In Proceedings of the
Workshop on Statistical Machine Translation,
pages 23?30, New York City.
Smith, D. A. and J. Eisner. 2009. Parser
adaptation and projection with
quasi-synchronous grammar features.
In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 822?831, Singapore.
Smith, N. A. 2006. Novel Estimation Methods
for Unsupervised Discovery of Latent
Structure in Natural Language Text.
Ph.D. thesis, Johns Hopkins University,
Baltimore, MD.
Snyder, B., T. Naseem, and R. Barzilay. 2009.
Unsupervised multilingual grammar
induction. In Proceedings of the Joint
Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 73?81, Suntec.
S?gaard, A. and J. Kuhn. 2009. Empirical
lower bounds on alignment error rates
in syntax-based machine translation.
In Proceedings of the Third Workshop on
Syntax and Structure in Statistical
Translation (SSST-3) at NAACL HLT 2009,
pages 19?27, Boulder, CO.
Spitkovsky, V. I., H. Alshawi, A. X. Chang,
and D. Jurafsky. 2011. Unsupervised
dependency parsing without gold
part-of-speech tags. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 1,281?1,290, Edinburgh.
Spitkovsky, V. I., H. Alshawi, and D. Jurafsky.
2010. From baby steps to leapfrog:
How ?less is More? in unsupervised
dependency parsing. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 751?759, Los Angeles, CA.
Stolcke, A. 2002. SRILM?An extensible
language modeling toolkit. In Proceedings
of the 7th International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Su, J., Y. Liu, H. Mi, H. Zhao, Y. Lu?, and
Q. Liu. 2010. Dependency-based
399
Computational Linguistics Volume 40, Number 2
bracketing transduction grammar for
statistical machine translation. In Coling
2010: Posters, pages 1,185?1,193, Beijing.
Tesnie`re, L. 1959. E?le?ment de Syntaxe
Structurale. Klincksieck.
Toutanova, K., D. Klein, C. D. Manning, and
Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the
North American Chapter of the Association
for Computational Linguistics on Human
Language Technology - Volume 1,
pages 173?180, Edmonton.
Tromble, R., S. Kumar, F. Och, and
W. Macherey. 2008. Lattice minimum
Bayes-risk decoding for statistical machine
translation. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 620?629,
Honolulu, HI.
Tsochantaridis, I., T. Joachims, T. Hofmann,
and Y. Altun. 2005. Large margin methods
for structured and interdependent output
variables. Journal of Machine Learning
Research, 6:1453?1484.
Tu, Z., Y. Liu, Y. Hwang, Q. Liu, and S. Lin.
2010. Dependency forest for statistical
machine translation. In Proceedings
of the 23rd International Conference on
Computational Linguistics (Coling 2010),
pages 1,092?1,100, Beijing.
Ueffing, N., F. J. Och, and H. Ney. 2002.
Generation of word graphs in statistical
machine translation. In Proceedings of the
ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10,
pages 156?163, Philadelphia, PA.
Wang, M., N. A. Smith, and T. Mitamura.
2007. What is the Jeopardy model? A
quasi-synchronous grammar for QA.
In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL),
pages 22?32, Prague.
Weiss, D., B. Sapp, and B. Taskar. 2010.
Sidestepping intractable inference
with structured ensemble cascades.
In Advances in Neural Information
Processing Systems (NIPS),
pages 2,415?2,423, Vancouver.
Wellington, B., S. Waxmonsky, and
I. D. Melamed. 2006. Empirical lower
bounds on the complexity of translational
equivalence. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 977?984, Sydney.
Woodsend, K., Y. Feng, and M. Lapata. 2010.
Title generation with quasi-synchronous
grammar. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 513?523,
Cambridge, MA.
Woodsend, K. and M. Lapata. 2011.
Learning to simplify sentences with
quasi-synchronous grammar and integer
programming. In Proceedings of the 2011
Conference on Empirical Methods in Natural
Language Processing, pages 409?420,
Edinburgh.
Wu, D. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
Wu, Y., Q. Zhang, X. Huang, and L. Wu. 2009.
Phrase dependency parsing for opinion
mining. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 1,533?1,541, Singapore.
Xie, J., H. Mi, and Q. Liu. 2011. A novel
dependency-to-string model for statistical
machine translation. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 216?226, Edinburgh.
Xiong, D., Q. Liu, and S. Lin. 2007.
A dependency treelet string
correspondence model for statistical
machine translation. In Proceedings of the
Second Workshop on Statistical Machine
Translation, pages 40?47, Prague.
Yadollahpour, P., D. Batra, and
G. Shakhnarovich. 2013. Discriminative
re-ranking of diverse segmentations.
In Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition
(CVPR), pages 1,923?1,930, Portland, OR.
Yamada, H. and Y. Matsumoto. 2003.
Statistical dependency analysis with
support vector machines. In Proceedings
of IWPT, pages 195?206, Nancy, France.
Yamada, K. and K. Knight. 2001.
A syntax-based statistical translation
model. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 523?530, Toulouse.
Yarowsky, D. and G. Ngai. 2001. Inducing
multilingual POS taggers and NP
bracketers via robust projection across
aligned corpora. In Proceedings of the
Second Meeting of the North American
Chapter of the Association for Computational
Linguistics on Language Technologies,
pages 1?8, Pittsburgh, PA.
Yarowsky, D., G. Ngai, and R. Wicentoswki.
2001. Inducing multilingual text analysis
400
Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features
tools via robust projection across
aligned corpora. In Proceedings of the First
International Conference on Human
Language Technology Research, pages 1?8,
San Diego, CA.
Zhang, J., F. Zhai, and C. Zong. 2011.
Augmenting string-to-tree translation
models with fuzzy use of source-side
syntax. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language
Processing, pages 204?215, Edinburgh.
Zhang, Y. 2009. Structured Language Models
for Statistical Machine Translation. Ph.D.
thesis, Carnegie Mellon University,
Pittsburgh, PA.
Zollmann, A. 2011. Learning
Multiple-Nonterminal Synchronous
Grammars for Statistical Machine Translation.
Ph.D. thesis, Carnegie Mellon University,
Pittsburgh, PA.
Zollmann, A. and A. Venugopal. 2006.
Syntax augmented machine translation
via chart parsing. In Proceedings on the
Workshop on Statistical Machine Translation,
pages 138?141, New York City.
Zollmann, A., A. Venugopal, F. J. Och, and
J. Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and
syntax-augmented statistical MT.
In Proceedings of the 22nd International
Conference on Computational Linguistics
(Coling 2008), pages 1,145?1,152,
Manchester.
Zollmann, A. and S. Vogel. 2011.
A word-class approach to labeling
PSCFG rules for machine translation.
In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies, pages 1?11,
Portland, OR.
401
402
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 293?296,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Movie Reviews and Revenues: An Experiment in Text Regression?
Mahesh Joshi Dipanjan Das Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{maheshj,dipanjan,kgimpel,nasmith}@cs.cmu.edu
Abstract
We consider the problem of predicting a
movie?s opening weekend revenue. Previous
work on this problem has used metadata about
a movie?e.g., its genre, MPAA rating, and
cast?with very limited work making use of
text about the movie. In this paper, we use
the text of film critics? reviews from several
sources to predict opening weekend revenue.
We describe a new dataset pairing movie re-
views with metadata and revenue data, and
show that review text can substitute for meta-
data, and even improve over it, for prediction.
1 Introduction
Predicting gross revenue for movies is a problem
that has been studied in economics, marketing,
statistics, and forecasting. Apart from the economic
value of such predictions, we view the forecasting
problem as an application of NLP. In this paper, we
use the text of critics? reviews to predict opening
weekend revenue. We also consider metadata for
each movie that has been shown to be successful for
similar prediction tasks in previous work.
There is a large body of prior work aimed at pre-
dicting gross revenue of movies (Simonoff and Spar-
row, 2000; Sharda and Delen, 2006; inter alia). Cer-
tain information is used in nearly all prior work on
these tasks, such as the movie?s genre, MPAA rating,
running time, release date, the number of screens on
which the movie debuted, and the presence of partic-
ular actors or actresses in the cast. Most prior text-
based work has used automatic text analysis tools,
deriving a small number of aggregate statistics. For
example, Mishne and Glance (2006) applied sen-
timent analysis techniques to pre-release and post-
release blog posts about movies and showed higher
?We appreciate reviewer feedback and technical advice
from Brendan O?Connor. This work was supported by NSF IIS-
0803482, NSF IIS-0844507, and DARPA NBCH-1080004.
correlation between actual revenue and sentiment-
based metrics, as compared to mention counts of the
movie. (They did not frame the task as a revenue
prediction problem.) Zhang and Skiena (2009) used
a news aggregation system to identify entities and
obtain domain-specific sentiment for each entity in
several domains. They used the aggregate sentiment
scores and mention counts of each movie in news
articles as predictors.
While there has been substantial prior work on
using critics? reviews, to our knowledge all of this
work has used polarity of the review or the number
of stars given to it by a critic, rather than the review
text directly (Terry et al, 2005).
Our task is related to sentiment analysis (Pang et
al., 2002) on movie reviews. The key difference is
that our goal is to predict a future real-valued quan-
tity, restricting us from using any post-release text
data such as user reviews. Further, the most im-
portant clues about revenue may have little to do
with whether the reviewer liked the movie, but rather
what the reviewer found worth mentioning. This pa-
per is more in the tradition of Ghose et al (2007) and
Kogan et al (2009), who used text regression to di-
rectly quantify review ?value? and make predictions
about future financial variables, respectively.
Our aim in using the full text is to identify partic-
ular words and phrases that predict the movie-going
tendencies of the public. We can also perform syn-
tactic and semantic analysis on the text to identify
richer constructions that are good predictors. Fur-
thermore, since we consider multiple reviews for
each movie, we can compare these features across
reviews to observe how they differ both in frequency
and predictive performance across different media
outlets and individual critics.
In this paper, we use linear regression from text
and non-text (meta) features to directly predict gross
revenue aggregated over the opening weekend, and
the same averaged per screen.
293
Domain train dev test total
Austin Chronicle 306 94 62 462
Boston Globe 461 154 116 731
LA Times 610 2 13 625
Entertainment Weekly 644 208 187 1039
New York Times 878 273 224 1375
Variety 927 297 230 1454
Village Voice 953 245 198 1396
# movies 1147 317 254 1718
Table 1: Total number of reviews from each domain for
the training, development and test sets.
2 Data
We gathered data for movies released in 2005?2009.
For these movies, we obtained metadata and a list
of hyperlinks to movie reviews by crawling Meta-
Critic (www.metacritic.com). The metadata
include the name of the movie, its production house,
the set of genres it belongs to, the scriptwriter(s),
the director(s), the country of origin, the primary
actors and actresses starring in the movie, the re-
lease date, its MPAA rating, and its running time.
From The Numbers (www.the-numbers.com),
we retrieved each movie?s production budget, open-
ing weekend gross revenue, and the number of
screens on which it played during its opening week-
end. Only movies found on both MetaCritic and The
Numbers were included.
Next we chose seven review websites that most
frequently appeared in the review lists for movies at
Metacritic, and obtained the text of the reviews by
scraping the raw HTML. The sites chosen were the
Austin Chronicle, the Boston Globe, the LA Times,
Entertainment Weekly, the New York Times, Vari-
ety, and the Village Voice. We only chose those
reviews that appeared on or before the release date
of the movie (to ensure that revenue information is
not present in the review), arriving at a set of 1718
movies with at least one review. We partitioned this
set of movies temporally into training (2005?2007),
development (2008) and test (2009) sets. Not all
movies had reviews at all sites (see Table 1).
3 Predictive Task
We consider two response variables, both in
U.S. dollars: the total revenue generated by a movie
during its release weekend, and the per screen rev-
enue during the release weekend. We evaluate these
predictions using (1) mean absolute error (MAE) in
U.S. dollars and (2) Pearson?s correlation between
the actual and predicted revenue.
We use linear regression to directly predict the
opening weekend gross earnings, denoted y, based
on features x extracted from the movie metadata
and/or the text of the reviews. That is, given an input
feature vector x ? Rp, we predict an output y? ? R
using a linear model: y? = ?0 + x>?. To learn val-
ues for the parameters ? = ??0,??, the standard
approach is to minimize the sum of squared errors
for a training set containing n pairs ?xi, yi? where
xi ? Rp and yi ? R for 1 ? i ? n:
?? = argmin
?=(?0,?)
1
2n
n?
i=1
(
yi ? (?0 + x>i ?)
)2
+?P (?)
A penalty term P (?) is included in the objective for
regularization. Classical solutions use an `2 or `1
norm, known respectively as ridge and lasso regres-
sion. Introduced recently is a mixture of the two,
called the elastic net (Zou and Hastie, 2005):
P (?) =
?p
j=1
(
1
2(1? ?)?
2
j + ?|?j |
)
where ? ? (0, 1) determines the trade-off be-
tween `1 and `2 regularization. For our experi-
ments we used the elastic net and specifically the
glmnet package which contains an implementa-
tion of an efficient coordinate ascent procedure for
training (Friedman et al, 2008).
We tune the ? and ? parameters on our develop-
ment set and select the model with the ??, ?? com-
bination that yields minimum MAE on the develop-
ment set.
4 Experiments
We compare predictors based on metadata, predic-
tors based on text, and predictors that use both kinds
of information. Results for two simple baselines of
predicting the training set mean and median are re-
ported in Table 2 (Pearson?s correlation is undefined
since the standard deviation is zero).
4.1 Metadata Features
We considered seven types of metadata features, and
evaluated their performance by adding them to our
pool of features in the following order: whether the
294
film is of U.S. origin, running time (in minutes), the
logarithm of its budget, # opening screens, genre
(e.g., Action, Comedy) and MPAA rating (e.g., G,
PG, PG-13), whether the movie opened on a holiday
weekend or in summer months, total count as well as
of presence of individual Oscar-winning actors and
directors and high-grossing actors. For the first task
of predicting the total opening weekend revenue of
a movie, the best-performing feature set in terms of
MAE turned out to be all the features. However, for
the second task of predicting the per screen revenue,
addition of the last feature subset consisting of infor-
mation related to the actors and directors hurt perfor-
mance (MAE increased). Therefore, for the second
task, the best performing set contained only the first
six types of metadata features.
4.2 Text Features
We extract three types of text features (described be-
low). We only included feature instances that oc-
curred in at least five different movies? reviews. We
stem and downcase individual word components in
all our features.
I. n-grams. We considered unigrams, bigrams, and
trigrams. A 25-word stoplist was used; bigrams
and trigrams were only filtered if all words were
stopwords.
II. Part-of-speech n-grams. As with words, we
added unigrams, bigrams, and trigrams. Tags
were obtained from the Stanford part-of-speech
tagger (Toutanova and Manning, 2000).
III. Dependency relations. We used the Stanford
parser (Klein and Manning, 2003) to parse the
critic reviews and extract syntactic dependen-
cies. The dependency relation features consist
of just the relation part of a dependency triple
?relation, head word, modifier word?.
We consider three ways to combine the collec-
tion of reviews for a given movie. The first (???)
simply concatenates all of a movie?s reviews into
a single document before extracting features. The
second (?+?) conjoins each feature with the source
site (e.g., New York Times) from whose review it was
extracted. A third version (denoted ?B?) combines
both the site-agnostic and site-specific features.
Features Site
Total Per Screen
MAE MAE
($M) r ($K) r
Predict mean 11.672 ? 6.862 ?
Predict median 10.521 ? 6.642 ?
m
et
a
Best 5.983 0.722 6.540 0.272
te
xt
I
? 8.013 0.743 6.509 0.222
+ 7.722 0.781 6.071 0.466
see Tab. 3 B 7.627 0.793 6.060 0.411
I ? II
? 8.060 0.743 6.542 0.233
+ 7.420 0.761 6.240 0.398
B 7.447 0.778 6.299 0.363
I ? III
? 8.005 0.744 6.505 0.223
+ 7.721 0.785 6.013 0.473
B 7.595 0.796 ?6.010 0.421
m
et
a
?
te
xt
I
? 5.921 0.819 6.509 0.222
+ 5.757 0.810 6.063 0.470
B 5.750 0.819 6.052 0.414
I ? II
? 5.952 0.818 6.542 0.233
+ 5.752 0.800 6.230 0.400
B 5.740 0.819 6.276 0.358
I ? III
? 5.921 0.819 6.505 0.223
+ 5.738 0.812 6.003 0.477
B 5.750 0.819 ?5.998 0.423
Table 2: Test-set performance for various models, mea-
sured using mean absolute error (MAE) and Pearson?s
correlation (r), for two prediction tasks. Within a column,
boldface shows the best result among ?text? and ?meta ?
text? settings. ?Significantly better than the meta baseline
with p < 0.01, using the Wilcoxon signed rank test.
4.3 Results
Table 2 shows our results for both prediction tasks.
For the total first-weekend revenue prediction task,
metadata features baseline result (r2 = 0.521) is
comparable to that reported by Simonoff and Spar-
row (2000) on a similar task of movie gross predic-
tion (r2 = 0.446). Features from critics? reviews
by themselves improve correlation on both predic-
tion tasks, however improvement in MAE is only
observed for the per screen revenue prediction task.
A combination of the meta and text features
achieves the best performance both in terms of MAE
and r. While the text-only models have some high
negative weight features, the combined models do
not have any negatively weighted features and only
a very few metadata features. That is, the text is able
to substitute for the other metadata features.
Among the different types of text-based features
that we tried, lexical n-grams proved to be a strong
baseline to beat. None of the ?I ? ?? feature sets are
significantly better than n-grams alone, but adding
295
the dependency relation features (set III) to the n-
grams does improve the performance enough to
make it significantly better than the metadata-only
baseline for per screen revenue prediction.
Salient Text Features: Table 3 lists some of the
highly weighted features, which we have catego-
rized manually. The features are from the text-only
model annotated in Table 2 (total, not per screen).
The feature weights can be directly interpreted as
U.S. dollars contributed to the predicted value y? by
each occurrence of the feature. Sentiment-related
features are not as prominent as might be expected,
and their overall proportion in the set of features
with non-zero weights is quite small (estimated in
preliminary trials at less than 15%). Phrases that
refer to metadata are the more highly weighted
and frequent ones. Consistent with previous re-
search, we found some positively-oriented sentiment
features to be predictive. Some other prominent
features not listed in the table correspond to spe-
cial effects (?Boston Globe: of the art?, ?and cgi?),
particular movie franchises (?shrek movies?, ?Vari-
ety: chronicle of?, ?voldemort?), hype/expectations
(?blockbuster?, ?anticipation?), film festival (?Vari-
ety: canne? with negative weight) and time of re-
lease (?summer movie?).
5 Conclusion
We conclude that text features from pre-release re-
views can substitute for and improve over a strong
metadata-based first-weekend movie revenue pre-
diction. The dataset used in this paper has been
made available for research at http://www.
ark.cs.cmu.edu/movie$-data.
References
J. Friedman, T. Hastie, and R. Tibshirani. 2008. Regular-
ized paths for generalized linear models via coordinate
descent. Technical report, Stanford University.
A. Ghose, P. G. Ipeirotis, and A. Sundararajan. 2007.
Opinion mining using econometrics: A case study on
reputation systems. In Proc. of ACL.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
S. Kogan, D. Levin, B. R. Routledge, J. Sagi, and N. A.
Smith. 2009. Predicting risk from financial reports
with regression. In Proc. of NAACL, pages 272?280.
Feature Weight ($M)
ra
ti
ng
pg +0.085
New York Times: adult -0.236
New York Times: rate r -0.364
se
qu
el
s this series +13.925
LA Times: the franchise +5.112
Variety: the sequel +4.224
pe
op
le Boston Globe: will smith +2.560
Variety: brittany +1.128
? producer brian +0.486
ge
nr
e
Variety: testosterone +1.945
Ent. Weekly: comedy for +1.143
Variety: a horror +0.595
documentary -0.037
independent -0.127
se
nt
im
en
t Boston Globe: best parts of +1.462
Boston Globe: smart enough +1.449
LA Times: a good thing +1.117
shame $ -0.098
bogeyman -0.689
pl
ot
Variety: torso +9.054
vehicle in +5.827
superhero $ +2.020
Table 3: Highly weighted features categorized manu-
ally. ? and $ denote sentence boundaries. ?brittany?
frequently refers to Brittany Snow and Brittany Murphy.
?? producer brian? refers to producer Brian Grazer (The
Da Vinci Code, among others).
G. Mishne and N. Glance. 2006. Predicting movie sales
from blogger sentiment. In AAAI Spring Symposium
on Computational Approaches to Analysing Weblogs.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79?86.
R. Sharda and D. Delen. 2006. Predicting box office suc-
cess of motion pictures with neural networks. Expert
Systems with Applications, 30(2):243?254.
J. S. Simonoff and I. R. Sparrow. 2000. Predicting movie
grosses: Winners and losers, blockbusters and sleep-
ers. Chance, 13(3):15?24.
N. Terry, M. Butler, and D. De?Armond. 2005. The de-
terminants of domestic box office performance in the
motion picture industry. Southwestern Economic Re-
view, 32:137?148.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63?70.
W. Zhang and S. Skiena. 2009. Improving movie gross
prediction through news analysis. In Web Intelligence,
pages 301?304.
H. Zou and T. Hastie. 2005. Regularization and variable
selection via the elastic net. Journal Of The Royal Sta-
tistical Society Series B, 67(5):768?768.
296
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 564?572,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Variational Inference for Adaptor Grammars
Shay B. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
scohen@cs.cmu.edu
David M. Blei
Computer Science Department
Princeton University
Princeton, NJ 08540, USA
blei@cs.princeton.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
Adaptor grammars extend probabilistic
context-free grammars to define prior dis-
tributions over trees with ?rich get richer?
dynamics. Inference for adaptor grammars
seeks to find parse trees for raw text. This
paper describes a variational inference al-
gorithm for adaptor grammars, providing
an alternative to Markov chain Monte Carlo
methods. To derive this method, we develop
a stick-breaking representation of adaptor
grammars, a representation that enables us
to define adaptor grammars with recursion.
We report experimental results on a word
segmentation task, showing that variational
inference performs comparably to MCMC.
Further, we show a significant speed-up when
parallelizing the algorithm. Finally, we report
promising results for a new application for
adaptor grammars, dependency grammar
induction.
1 Introduction
Recent research in unsupervised learning for NLP
focuses on Bayesian methods for probabilistic gram-
mars (Goldwater and Griffiths, 2007; Toutanova and
Johnson, 2007; Johnson et al, 2007). Such meth-
ods have been made more flexible with nonparamet-
ric Bayesian (NP Bayes) methods, such as Dirichlet
process mixture models (Antoniak, 1974; Pitman,
2002). One line of research uses NP Bayes meth-
ods on whole tree structures, in the form of adaptor
grammars (Johnson et al, 2006; Johnson, 2008b;
Johnson, 2008a; Johnson and Goldwater, 2009), in
order to identify recurrent subtree patterns.
Adaptor grammars provide a flexible distribu-
tion over parse trees that has more structure than
a traditional context-free grammar. Adaptor gram-
mars are used via posterior inference, the compu-
tational problem of determining the posterior distri-
bution of parse trees given a set of observed sen-
tences. Current posterior inference algorithms for
adaptor grammars are based on MCMC sampling
methods (Robert and Casella, 2005). MCMC meth-
ods are theoretically guaranteed to converge to the
true posterior, but come at great expense: they are
notoriously slow to converge, especially with com-
plex hidden structures such as syntactic trees. John-
son (2008b) comments on this, and suggests the use
of variational inference as a possible remedy.
Variational inference provides a deterministic al-
ternative to sampling. It was introduced for Dirich-
let process mixtures by Blei and Jordan (2005) and
applied to infinite grammars by Liang et al (2007).
With NP Bayes models, variational methods are
based on the stick-breaking representation (Sethu-
raman, 1994). Devising a stick-breaking represen-
tation is a central challenge to using variational in-
ference in this setting.
The rest of this paper is organized as follows. In
?2 we describe a stick-breaking representation of
adaptor grammars, which enables variational infer-
ence (?3) and a well-defined incorporation of recur-
sion into adaptor grammars. In ?4 we give an em-
pirical comparison of the algorithm to MCMC in-
ference and describe a novel application of adaptor
grammars to unsupervised dependency parsing.
2 Adaptor Grammars
We review adaptor grammars and develop a stick-
breaking representation of the tree distribution.
2.1 Definition of Adaptor Grammars
Adaptor grammars capture syntactic regularities in
sentences by placing a nonparametric prior over the
distribution of syntactic trees that underlie them.
The model exhibits ?rich get richer? dynamics: once
a tree is generated, it is more likely to reappear.
Adaptor grammars were developed by Johnson et
al. (2006). An adaptor grammar is a tuple A =
?G,M,a, b,??, which contains: (i) a context-free
grammar G = ?W,N,R, S? where W is the set of
564
terminals, N is the set of nonterminals, R is a set of
production rules, and S ? N is the start symbol?we
denote byRA the subset ofR with left-hand sideA;
(ii) a set of adapted nonterminals, M ? N; and (iii)
parameters a, b and ?, which are described below.
An adaptor grammar assumes the following gen-
erative process of trees. First, the multinomial dis-
tributions ? for a PCFG based on G are drawn
from Dirichlet distributions. Specifically, multino-
mial ?A ? Dir(?A) where? is collection of Dirich-
let parameters, indexed by A ? N.
Trees are then generated top-down starting with
S. Any non-adapted nonterminal A ? N \ M is
expanded by drawing a rule from RA. There are
two ways to expand A ?M:
1. With probability (nz ? bA)/(nA + aA) we ex-
pand A to subtree z (a tree rooted at A with a
yield in W?), where nz is the number of times
the tree z was previously generated and nA is the
total number of subtrees (tokens) previously gen-
erated root being A. We denote by a the concen-
tration parameters and b the discount parameters,
both indexed by A ? M. We have aA ? 0 and
bA ? [0, 1].
2. With probability (aA + kAbA)/(nA + aA), A is
expanded as in a PCFG by a draw from ?A over
RA, where kA is the number of subtrees (types)
previously generated with root A.
For the expansion of adapted nonterminals, this
process can be explained using the Chinese restau-
rant process (CRP) metaphor: a ?customer? (cor-
responding to a partially generated tree) enters a
?restaurant? (corresponding to a nonterminal) and
selects a ?table? (corresponding to a subtree) to at-
tach to the partially generated tree. If she is the first
customer at the table, the PCFG ?G,?? produces the
new table?s associated ?dish? (a subtree).1
When adaptor grammars are defined using the
CRP, the PCFG G has to be non-recursive with re-
1We note that our construction deviates from the strict def-
inition of adaptor grammars (Johnson et al, 2006): (i) in our
construction, we assume (as prior work does in practice) that
the adaptors in A = ?G,M,a, b,?? follow the Pitman-Yor
(PY) process (Pitman and Yor, 1997), though in general other
stochastic processes might be used; and (ii) we place a sym-
metric Dirichlet over the parameters of the PCFG, ?, whereas
Johnson et al used a fixed PCFG for the definition (though they
experimented with a Dirichlet prior).
spect to the adapted nonterminals. More precisely,
for A ? N, denote by Reachable(G, A) all the non-
terminals that can be reached from A using a partial
derivation from G. Then we restrict G such that
for all A ? M, we have A /? Reachable(G, A).
Without this restriction, we might end up in a sit-
uation where the generative process is ill-defined:
in the CRP terminology, a customer could enter a
restaurant and select a table whose dish is still in
the process of being selected.2 In the more general
form of adaptor grammars with arbitrary adaptors,
the problem amounts to mutually dependent defini-
tions of distributions which rely on the others to be
defined. We return to this problem in ?3.1.
Inference The inference problem is to compute
the posterior distribution of parse trees given ob-
served sentences x = ?x1, . . . , xn?. Typically, in-
ference with adaptor grammars is done with Gibbs
sampling. Johnson et al (2006) use an embedded
Metropolis-Hastings sampler (Robert and Casella,
2005) inside a Gibbs sampler. The proposal distribu-
tion is a PCFG, resembling a tree substitution gram-
mar (TSG; Joshi, 2003). The sampler of Johnson et
al. is based on the representation of the PY process
as a distribution over partitions of integers. This rep-
resentation is not amenable to variational inference.
2.2 Stick-Breaking Representation
To develop a variational inference algorithm for
adaptor grammars, we require an alternative repre-
sentation of the model in ?2.1. The CRP-based def-
inition implicitly marginalizes out a random distri-
bution over trees. For variational inference, we con-
struct that distribution.
We first review the Dirichlet process and its stick-
breaking representation. The Dirichlet process de-
fines a distribution over distributions. Samples from
the Dirichlet process tend to deviate from a base
distribution depending on a concentration parame-
ter. Let G ? DP(G0, a) be a distribution sampled
from the Dirichlet process with base distribution G0
2Consider the simple grammar with rules { S ? S S, S ? a
}. Assume that a customer enters the restaurant for S. She sits
at a table, and selects a dish, a subtree, which starts with the rule
S ? S S. Perhaps the first child S is expanded by S ? a. For
the second child S, it is possible to re-enter the ?S restaurant?
and choose the first table, where the ?dish? subtree is still being
generated.
565
and concentration parameter a. The distribution G
is discrete, which means it puts positive mass on a
countable number of atoms drawn from G0. Re-
peated draws from G exhibit the ?clustering prop-
erty,? which means that they will be assigned to the
same value with positive probability. Thus, they ex-
hibit a partition structure. Marginalizing out G, the
distribution of that partition structure is given by a
CRP with parameter a (Pitman, 2002).
The stick-breaking process gives a constructive
definition of G (Sethuraman, 1994). With the stick-
breaking process (for the PY process), we first sam-
ple ?stick lengths? pi ? GEM(a, b) (in the case of
Dirichlet process, we have b = 0). The GEM par-
titions the interval [0, 1] into countably many seg-
ments. First, draw vi ? Beta(1 ? b, a + ib) for
i ? {1, . . .}. Then, define pii , vi
?i?1
j=1(1 ? vj).
In addition, we also sample infinitely many ?atoms?
independently zi ? G0. Define G as:
G(z) =
??
i=1 pii?(zi, z) (1)
where ?(zi, z) is 1 if zi = z and 0 otherwise. This
random variable is drawn from a Pitman-Yor pro-
cess. Notice the discreteness of G is laid bare in the
stick-breaking construction.
With the stick-breaking representation in hand,
we turn to a constructive definition of the distri-
bution over trees given by an adaptor grammar.
Let A1, . . . , AK be an enumeration of the nonter-
minals in M which satisfies: i ? j ? Aj /?
Reachable(G, Ai). (That this exists follows from
the assumption about the lack of recursiveness of
adapted nonterminals.) Let Yield(z) be the yield of
a tree derivation z. The process that generates ob-
served sentences x = ?x1, . . . , xn? from the adaptor
grammarA = ?G,M,a, b,?? is as follows:
1. For each A ? N, draw ?A ? Dir(?A).
2. For A from A1 to AK , define GA as follows:
(a) Draw piA | aA, bA ? GEM(aA, bA).
(b) For i ? {1, . . .}, grow a tree zA,i as follows:
i. Draw A? B1 . . . Bn fromRA.
ii. zA,i = A
HHH

B1 ? ? ? Bn
iii. While Yield(zA,i) has nonterminals:
A. Choose an unexpanded nonterminal B
from yield of zA,i.
B. If B ? M, expand B according to GB
(defined on previous iterations of step 2).
C. If B ? N \M, expand B with a rule from
RB according to Mult(?B).
(c) For i ? {1, . . .}, define GA(zA,i) = piA,i
3. For i ? {1, . . . , n} draw zi as follows:
(a) If S ?M, draw zi | GS ? GS .
(b) If S /? M, draw zi as in 2(b) (omitted for
space).
4. Set xi = Yield(zi) for i ? {1, . . . , n}.
Here, there are four collections of hidden variables:
the PCFG multinomials ? = {?A | A ? N}, the
stick length proportions v = {vA | A ? M} where
vA = ?vA,1, vA,2, . . .?, the adapted nonterminals?
subtrees zA = {zA,i | A ? M; i ? {1, . . .}} and
the derivations z1:n = z1, . . . , zn. The symbol z
refers to the collection of {zA | A ? M}, and z1:n
refers to the derivations of the data x.
Note that the distribution in 2(c) is defined with
the GEM distribution, as mentioned earlier. It is a
sample from the Pitman-Yor process (or the Dirich-
let process), which is later used in 3(a) to sample
trees for an adapted non-terminal.
3 Variational Inference
Variational inference is a deterministic alternative
to MCMC, which casts posterior inference as an
optimization problem (Jordan et al, 1999; Wain-
wright and Jordan, 2008). The optimized function
is a bound on the marginal likelihood of the obser-
vations, which is expressed in terms of a so-called
?variational distribution? over the hidden variables.
When the bound is tightened, that distribution is
close to the posterior of interest. Variational meth-
ods tend to converge faster than MCMC, and can be
more easily parallelized over multiple processors in
a framework such as MapReduce (Dean and Ghe-
mawat, 2004).
The variational bound on the likelihood of the
data is:
log p(x | a,?) ? H(q) +
?
A?M
Eq[log p(vA | aA)]
+
?
A?M
Eq[log p(?A | ?A)]
+
?
A?M
Eq[log p(zA | v,?)] + Eq[log p(z | vA)]
566
Expectations are taken with respect to the variational
distribution q(v,?, z) and H(q) is its entropy.
Before tightening the bound, we define the func-
tional form of the variational distribution. We use
the mean-field distribution in which all of the hid-
den variables are independent and governed by in-
dividual variational parameters. (Note that in the
true posterior, the hidden variables are highly cou-
pled.) To account for the infinite collection of ran-
dom variables, for which we cannot define a varia-
tional distribution, we use the truncated stick distri-
bution (Blei and Jordan, 2005). Hence, we assume
that, for all A ? M, there is some value NA such
that q(vA,NA = 1) = 1. The assigned probability to
parse trees in the stick will be 0 for i > NA, so we
can ignore zA,i for i > NA. This leads to a factor-
ized variational distribution:
q(v,?, z) = (2)
?
A?M
(
q(?A)
NA?
i=1
q(vA,i)? q(zA,i)
)
?
n?
i=1
q(zi)
It is natural to define the variational distributions
over ? and v to be Dirichlet distributions with pa-
rameters ?A and Beta distributions with parameters
?A,i, respectively. The two distributions over trees,
q(zA,i) and q(zi), are more problematic. For ex-
ample, with q(zi | ?), we need to take into ac-
count different subtrees that could be generated by
the model and use them with the proper probabilities
in the variational distribution q(zi | ?). We follow
and extend the idea from Johnson et al (2006) and
use grammatons for these distributions. Gramma-
tons are ?mini-grammars,? inspired by the grammar
G.
For two strings in s, t ? W?, we use ?t ? s?
to mean that t is a substring of s. In that case, a
grammaton is defined as follows:
Definition 1. LetA = ?G,M,a, b,?? be an adap-
tor grammar with G = ?W,N,R, S?. Let s be a fi-
nite string over the alphabet ofG andA ? N. Let U
be the set of nonterminals U , Reachable(G, A) ?
(N \M). The grammaton G(A, s) is the context-
free grammar with the start symbol A and the rules
RA?
(
?
B?U
RB
)
?
?
A?B1...Bn?RA
?
i?{i|Bi?M}
{Bi ?
t | t ? s}.
Using a grammaton, we define the distributions
q(zA,i | ?A) and q(zi | ?). This requires a pre-
processing step (described in detail in ?3.3) that de-
fines, for each A ? M, a list of strings sA =
?sA,1, . . . , sA,NA?. Then, for q(zA,i | ?A) we use
the grammaton G(A, sA,i) and for q(zi | ?) we
use the grammaton G(A, xi) where xi is the ith
observed sentence. We parametrize the grammaton
with weights ?A (or ?) for each rule in the gramma-
ton. This makes the variational distributions over the
trees for strings s (and trees for x) globally normal-
ized weighted grammars. Choosing such distribu-
tions is motivated by their ability to make the varia-
tional bound tight (similar to Cohen et al, 2008, and
Cohen and Smith, 2009). In practice we do not have
to use rewrite rules for all strings t ? s in the gram-
maton. It suffices to add rewrite rules only for the
strings t = sA,i that have some grammaton attached
to them,G(A, sA,i).
The variational distribution above yields a vari-
ational inference algorithm for approximating the
posterior by estimating ?A,i, ?A, ?A and ? it-
eratively, given a fixed set of hyperparameters
a, b and ?. Let r be a PCFG rule. Let
f?(r, sB,k) = Eq(zk|?B,k)[f(r; zk)], where f(r; zk)
counts the number of times that rule r is applied in
the derivation zk. Let A ? ? denote a rule from
G. The quantity f?(r, sB,k) is computed using the
inside-outside (IO) algorithm. Fig. 1 gives the vari-
ational inference updates.
Variational EM We use variational EM to fit the
hyperparameters. Variational EM is an EM algo-
rithm where the E step is replaced by variational in-
ference (Fig. 1). The M-step optimizes the hyperpa-
rameters (a, b and ?) with respect to expected suffi-
cient statistics under the variational distribution. We
use Newton-Raphson for each (Boyd and Vanden-
berghe, 2004); Fig. 2 gives the objectives.
3.1 Note about Recursive Grammars
With recursive grammars, the stick-breaking pro-
cess representation gives probability mass to events
which are ill-defined. In step 2(iii)(c) of the stick-
breaking representation, we assign nonzero proba-
bility to an event in which we choose to expand the
current tree using a subtree with the same index that
we are currently still expanding (see footnote 2). In
567
short, with recursive grammars, we can get ?loops?
inside the trees.
We would still like to use recursion in the cases
which are not ill-defined. In the case of recur-
sive grammars, there is no problem with the stick-
breaking representation and the order by which we
enumerate the nonterminals. This is true because the
stick-breaking process separates allocating the prob-
abilities for each index in the stick and allocating the
atoms for each index in the stick.
Our variational distributions give probability 0 to
any event which is ill-defined in the sense men-
tioned above. Optimizing the variational bound in
this case is equivalent to optimizing the same vari-
ational bound with a model p? that (i) starts with p,
(ii) assigns probability 0 to ill-defined events, and
(iii) renormalizes:
Proposition 2. Let p(x, z) be a probability distri-
bution, where z ? Z, and let S ? Z. Let Q = {q |
q(z) = 0, ?z ? S}, a set of distributions. Then:
argmax
q?Q
Eq[log p(x, z)] = argmax
q
Eq[log p?(x, z)]
where p?(x, z) is a probability distribution defined
as p?(x, z) = p(x, z)/
?
z?S p(x, z) for z ? S and
0 otherwise.
For this reason, our variational approximation al-
lows the use of recursive grammars. The use of re-
cursive grammars with MCMC methods is problem-
atic, since it has no corresponding probabilistic in-
terpretation, enabled by zeroing events that are ill-
defined in the variational distribution. There is no
underlying model such as p?, and thus the inference
algorithm is invalid.
3.2 Time Complexity
The algorithm in Johnson et al (2006) works by
sampling from a PCFG containing rewrite rules that
rewrite to a whole tree fragment. This requires
a procedure that uses the inside-outside algorithm.
Despite the grammar being bigger (because of the
rewrite rules to a string), the asymptotic complexity
of the IO algorithm stays O(|N|2|xi|3 + |N|3|xi|2)
where |xi| is the length of the ith sentence.3
3This analysis is true for CNF grammars augmented with
rules rewriting to a whole string, like those used in our study.
?1A,i = 1? bA +
?
B?M
?NB
k=1 f?(A? sA,i, sB,k)
?2A,i = aA + ibA
+
?i?1
j=1
?
B?M
?NB
k=1 f?(A? sA,j , sB,k)
?A,A?? =
?
B?M
?NB
k=1 f?(A? ?, sB,k)
?A,A?sA,i = ?(?
1
A,i)??(?
1
A,i + ?
2
A,i)
+
?i?1
j=1
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
?A,A?? = ?(?A,A??)??
(?
? ?A,A??
)
Figure 1: Updates for variational inference with adaptor
grammars. ? is the digamma function.
Our algorithm requires running the IO algorithm
for each yield in the variational distribution, for each
nonterminal, and for each sentence. However, IO
runs with much smaller grammars coming from the
grammatons. The cost of running the IO algorithm
on the yields in the sticks for A ? M can be taken
into account parsing a string that appears in the cor-
pus with the full grammars. This leads to an asymp-
totic complexity of O(|N|2|xi|3 + |N|3|xi|2) for the
ith sentence in the corpus each iteration.
Asymptotically, both sampling and variational
EM behave the same. However, there are different
constants that hide in these asymptotic runtimes: the
number of iterations that the algorithm takes to con-
verge (for which variational EM generally has an ad-
vantage over sampling) and the number of additional
rewrite rules that rewrite to a string representing a
tree (for which MCMC has a relative advantage, be-
cause it does not use a fixed set of strings; instead,
the size of the grammars it uses grow as sampling
proceeds). In ?4, we see that variational EM and
sampling methods are similar in the time it takes to
complete because of a trade-off between these two
constants. Simple parallelization, however, which
is possible only with variational inference, provides
significant speed-ups.4
3.3 Heuristics for Variational Inference
For the variational approximation from ?3, we need
to decide on a set of strings, sA,i (for A ? M and
i ? {1, . . . , NA}) to define the grammatons in the
4Newman et al (2009) show how to parallelize sampling al-
gorithms, but in general, parallelizing these algorithms is more
complicated than parallelizing variational algorithms and re-
quires further approximation.
568
max?A log ?(|RA|?A)? |RA| log ?(?A) + (?A ? 1)
(?
A???RA
?(?A??)??
(?
A???RA
?A??
))
maxaA
?NA
i=1 aA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(ibA + aA)
maxbA
?NA
i=1 ibA
(
?(?2A,i)??(?
1
A,i + ?
2
A,i)
)
+ log ?(aA + 1 + ibA)? log ?(1? bA)? log ?(ibA + aA)
Figure 2: Variational M-step updates. ? is the gamma function.
nonparametric stick. Any set of strings will give
a valid approximation, but to make the variational
approximation as accurate as possible, we require
that: (i) the strings in the set must be likely to be
generated using the adaptor grammar as constituents
headed by the relevant nonterminal, and (ii) strings
that are more likely to be generated should be asso-
ciated with a lower index in the stick. The reason for
the second requirement is the exponential decay of
coefficients as the index increases.
We show that a simple heuristic leads to an order
over the strings generated by the adaptor grammars
that yields an accurate variational estimation. We
begin with a weighted context-free grammar Gheur
that has the same rules as in G, only the weight for
all of its rules is 1. We then compute the quantity:
c(A, s) =
1
n
(
n?
i=1
EGheur [fi(z;A, s)]
)
? ? log |s|
(3)
where fi(z;A, s) is a function computing the count
of constituents headed by A with yield s in the tree
z for the sentence xi. This quantity can be com-
puted by using the IO algorithm onGheur. The term
? log |s| is subtracted to avoid preference for shorter
constituents, similar to Mochihashi et al (2009).
While computing c(A, s) using the IO algorithm,
we sort the set of all substrings of s according to
their expected counts (aggregated over all strings s).
Then, we use the top NA strings in the sorted list for
the grammatons of A.5
3.4 Decoding
The variational inference algorithm gives a distribu-
tions over parameters and hidden structures (through
the grammatons). We experiment with two com-
monly used decoding methods: Viterbi decoding
5The requirement to select NA in advance is strict. We ex-
perimented with dynamic expansions of the stick, in the spirit
of Kurihara et al (2006) and Wang and Blei (2009), but we did
not achieve better performance and it had an adverse effect on
runtime. For completeness, we give these results in ?4.
and minimum Bayes risk decoding (MBR; Good-
man, 1996).
To parse a string with Viterbi (or MBR) decoding,
we find the tree with highest score for the gramma-
ton which is attached to that string. For all rules
which rewrite to strings in the resulting tree, we
again perform Viterbi (or MBR) decoding recur-
sively using other grammatons.
4 Experiments
We describe experiments with variational inference
for adaptor grammars for word segmentation and de-
pendency grammar induction.
4.1 Word Segmentation
We follow the experimental setting of Johnson and
Goldwater (2009), who present state-of-the-art re-
sults for inference with adaptor grammars using
Gibbs sampling on a segmentation problem. We
use the standard Brent corpus (Brent and Cartwright,
1996), which includes 9,790 unsegmented phone-
mic representations of utterances of child-directed
speech from the Bernstein-Ratner (1987) corpus.
Johnson and Goldwater (2009) test three gram-
mars for this segmentation task. The first grammar
is a character unigram grammar (GUnigram). The
second grammar is a grammar that takes into con-
sideration collocations (GColloc) which includes the
rules { Sentence? Colloc, Sentence? Colloc Sen-
tence, Colloc ? Word+, Word ? Char+ }. The
third grammar incorporates more prior knowledge
about the syllabic structure of English (GSyllable).
GUnigram and GSyllable can be found in Johnson
and Goldwater (2009). Once an utterance is parsed,
Word constituents denote segments.
The value of ? (penalty term for string length) had
little effect on our results and was fixed at ? = ?0.2.
When NA (number of strings used in the variational
distributions) is fixed, we use NA = 15,000. We re-
port results using Viterbi and MBR decoding. John-
son and Goldwater (2009) experimented with two
569
this paper J&G 2009
grammar model Vit. MBR SA MM
GU
ni
gr
am
Dir 0.49 0.84 0.57 0.54
PY 0.49 0.84 0.81 0.75
PY+inc 0.42 0.59 - -
GC
ol
lo
c Dir 0.40 0.86 0.75 0.72
PY 0.40 0.86 0.83 0.86
PY+inc 0.43 0.60 - -
G S
yl
la
bl
e Dir 0.77 0.83 0.84 0.84
PY 0.77 0.83 0.89 0.88
PY+inc 0.75 0.76 - -
Table 1: F1 performance for word segmentation on the
Brent corpus. Dir. stands for Dirichlet Process adaptor
(b = 0), PY stands for Pitman-Yor adaptor (b optimized),
and PY+inc. stands for Pitman-Yor with iteratively in-
creasing NA for A ? M (see footnote 5). J&G 2009 are
the results adapted from Johnson and Goldwater (2009);
SA is sample average decoding, and MM is maximum
marginal decoding.
Truncated stick length
F1 sc
ore
65
70
75
80
l
l
l
l
l l l
l l l l l l
l l
l
l
l
l
l
l l
l l
l l l l l l
2000 4000 6000 8000 10000 12000 14000
Figure 3: F1 performance of GUnigram as influenced by
the length of the stick, NWord.
decoding methods, sample average (SA) and maxi-
mal marginal decoding (MM), which are closely re-
lated to Viterbi and MBR, respectively. With MM,
we marginalize the tree structure, rather than the
word segmentation induced, similar to MBR decod-
ing. With SA, we compute the probability of a whole
tree, by averaging its count in the samples, an ap-
proximation to finding the tree with highest proba-
bility, like Viterbi.
Table 1 gives the results for our experiments. No-
tice that the results for the Pitman-Yor process and
the Dirichlet process are similar. When inspecting
the learned parameters, we noticed that the discount
parameters (b) learned by the variational inference
algorithm for the Pitman-Yor process are very close
to 0. In this case, the Pitman-Yor process is reduced
to the Dirichlet process.
Similar to Johnson and Goldwater?s comparisons,
we see superior performance when using minimum
Bayes risk over Viterbi decoding. Further notice that
the variational inference algorithm obtains signifi-
cantly superior performance for simpler grammars
than Johnson et al, while performance using the syl-
lable grammar is lower. The results also suggest that
it is better to decide ahead on the set of strings avail-
able in the sticks, instead of working gradually and
increase the size of the sticks as described in foot-
note 5. We believe that the reason is that the varia-
tional inference algorithm settles in a trajectory that
uses fewer strings, then fails to exploit the strings
that are added to the stick later. Given that select-
ing NA in advance is advantageous, we may inquire
if choosing NA to be too large can lead to degraded
performance, because of fragmention of the gram-
mar. Fig. 3 suggests it is not the case, and per-
formance stays steady after NA reaches a certain
value.
One of the advantages of variational approxima-
tion over sampling methods is the ability to run
for fewer iterations. For example, with GUnigram
convergence typically takes 40 iterations with vari-
ational inference, while Johnson and Goldwater
(2009) ran their sampler for 2,000 iterations, for
which 1,000 were for burning in. The inside-outside
algorithm dominates the iteration?s runtime, both
for sampling and variational EM. Each iteration
with sampling, however, takes less time, despite the
asymptotic analysis in ?3.2, because of different im-
plementations and the different number of rules that
rewrite to a string. We now give a comparison of
clock time for GUnigram for variational inference
and sampling as described in Johnson and Goldwa-
ter (2009).6 Replicating the experiment in Johnson
and Goldwater (first row in Table 1) took 2 hours
and 11 minutes. With the variational approximation,
we had the following: (i) the preprocessing (?3.3)
step took 114 seconds; (ii) each iteration took ap-
proximately 204 seconds, with convergence after 40
iterations, leading to 8,160 seconds of pure varia-
6We used the code and data available at http://www.
cog.brown.edu/?mj/Software.htm. The machine
used for this comparison is a 64-bit machine with 2.6GHz CPU,
4MB of cache memory and 8GB of RAM.
570
tional EM processing; (iii) parsing took another 952
seconds. The total time is 2 hours and 34 minutes.
At first glance it seems that variational inference
is slower than MCMC sampling. However, note that
the cost of the grammar preprocessing step is amor-
tized over all experiments with the specific gram-
mar, and the E-step with variational inference can be
parallelized, while sampling requires an update of a
global set of parameters after each tree update. We
ran our algorithm on a cluster of 20 1.86GHz CPUs
and achieved a significant speed-up: preprocessing
took 34 seconds, each variational EM iteration took
43 seconds and parsing took 208 seconds. The total
time was 47 minutes, which is 2.8 times faster than
sampling.
4.2 Dependency Grammar Induction
We conclude our experiments with preliminary re-
sults for unsupervised syntax learning. This is a new
application of adaptor grammars, which have so far
been used in segmentation (Johnson and Goldwater,
2009) and named entity recognition (Elsner et al,
2009).
The grammar we use is the dependency model
with valence (DMV Klein and Manning, 2004) rep-
resented as a probabilistic context-free grammar,
GDMV (Smith, 2006). We note that GDMV is re-
cursive; this is not a problem (?3.1).
We used part-of-speech sequences from the Wall
Street Journal Penn Treebank (Marcus et al, 1993),
stripped of words and punctuation. We follow stan-
dard parsing conventions and train on sections 2?
21 and test on section 23 (while using sentences of
length 10 or less). Because of the unsupervised na-
ture of the problem, we report results on the training
set, in addition to the test set.
The nonterminals that we adapted correspond to
nonterminals that define noun constituents. We then
use the preprocessing step defined in ?3.3 with a uni-
form grammar and take the top 3,000 strings for each
nonterminal of a noun constituent.
The results are in Table 4.2. We report attach-
ment accuracy, the fraction of parent-child relation-
ships that the algorithm classified correctly. Notice
that the results are not very different for Viterbi and
MBR decoding, unlike the case with word segmen-
tation. It seems like the DMV grammar, applied
to this task, is more robust to changes in decod-
model Vit. MBR
tr
ai
n
non-Bayesian 48.2 48.3
Dirichlet prior 48.3 48.6
Adaptor grammar 54.0 ?53.7
te
st
non-Bayesian 45.8 46.1
Dirichlet prior 45.9 46.1
Adaptor grammar 48.3 50.2
Table 2: Attachment accuracy for different models for
dependency grammar induction. Bold marks best overall
accuracy per evaluation set, and ? marks figures that are
not significantly worse (binomial sign test, p < 0.05).
ing mechanism. Adaptor grammars improve perfor-
mance over classic EM and variational EM with a
Dirichlet prior significantly.
We note that adaptor grammars are not limited to
a selection of a Dirichlet distribution as a prior for
the grammar rules. Our variational inference algo-
rithm, for example, can be extended to use the lo-
gistic normal prior instead of the Dirichlet, shown
successful by Cohen and Smith (2009).7
5 Conclusion
We described a variational inference algorithm for
adaptor grammars based on a stick-breaking process
representation, which solves a problem with adaptor
grammars and recursive PCFGs. We tested it for a
segmentation task, and showed results which are ei-
ther comparable or an imporvement of state of the
art. We showed that significant speed-ups can be
obtained using parallelization of the algorithm. We
also tested the algorithm on a novel task for adap-
tor grammars, dependency grammar induction. We
showed that an improvement can be obtained using
adaptor grammars over non-Bayesian and paramet-
ric baselines.
Acknowledgments
The authors would like to thank the anonymous review-
ers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson,
and Chong Wang for their useful feedback and com-
ments. This work was supported by the following grants:
ONR 175-6343 and NSF CAREER 0745520 to Blei; NSF
IIS-0836431 and IIS-0915187 to Smith.
7The performance of Cohen and Smith (2009), like the per-
formance of Headden et al (2009), is greater than what we re-
port, but those developments are orthogonal to the contributions
of this paper.
571
References
C. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
N. Bernstein-Ratner. 1987. The phonology of parent
child speech. Children?s Language, 6.
D. Blei and M. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Journal of Bayesian Anal-
ysis, 1(1):121?144.
S. Boyd and L. Vandenberghe. 2004. Convex Optimiza-
tion. Cambridge Press.
M. Brent and T. Cartwright. 1996. Distributional reg-
ularity and phonotactic constraints are useful for seg-
mentation. Cognition, 6:93?125.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic
normal priors for unsupervised probabilistic grammar
induction. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Proc. of
OSDI.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL.
W. P. Headden, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL-
HLT.
M. Johnson and S. Goldwater. 2009. Improving nonpa-
rameteric Bayesian inference experiments on unsuper-
vised word segmentation with adaptor grammars. In
Proc. of NAACL-HLT.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying com-
positional nonparameteric Bayesian models. In NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
M. Johnson. 2008a. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proceedings
of the Tenth Meeting of ACL Special Interest Group on
Computational Morphology and Phonology.
M. Johnson. 2008b. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proc. of ACL.
M. I. Jordan, Z. Ghahramani, T. S. Jaakola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37(2):183?
233.
A. Joshi. 2003. Tree adjoining grammars. In R. Mitkov,
editor, The Oxford Handbook of Computational Lin-
guistics, pages 483?501. Oxford University Press.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Kurihara, M. Welling, and N. A. Vlassis. 2006. Ac-
celerated variational Dirichlet process mixtures. In
NIPS.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proc. of EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2009. Distributed algorithms for topic models. Jour-
nal of Machine Learning Research, 10:1801?1828.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25(2):855?900.
J. Pitman. 2002. Combinatorial Stochastic Processes.
Lecture Notes for St. Flour Summer School. Springer-
Verlag, New York, NY.
C. P. Robert and G. Casella. 2005. Monte Carlo Statisti-
cal Methods. Springer.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proc. of NIPS.
M. J. Wainwright and M. I. Jordan. 2008. Graphi-
cal models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1:1?305.
C. Wang and D. M. Blei. 2009. Variational inference for
the nested Chinese restaurant process. In NIPS.
572
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 609?617,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Good Question! Statistical Ranking for Question Generation
Michael Heilman Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,nasmith}@cs.cmu.edu
Abstract
We address the challenge of automatically
generating questions from reading materials
for educational practice and assessment. Our
approach is to overgenerate questions, then
rank them. We use manually written rules to
perform a sequence of general purpose syn-
tactic transformations (e.g., subject-auxiliary
inversion) to turn declarative sentences into
questions. These questions are then ranked by
a logistic regression model trained on a small,
tailored dataset consisting of labeled output
from our system. Experimental results show
that ranking nearly doubles the percentage of
questions rated as acceptable by annotators,
from 27% of all questions to 52% of the top
ranked 20% of questions.
1 Introduction
In this paper, we focus on question generation (QG)
for the creation of educational materials for read-
ing practice and assessment. Our goal is to gener-
ate fact-based questions about the content of a given
article. The top-ranked questions could be filtered
and revised by educators, or given directly to stu-
dents for practice. Here we restrict our investigation
to questions about factual information in texts.
We begin with a motivating example. Consider
the following sentence from the Wikipedia article on
the history of Los Angeles:1 During the Gold Rush
years in northern California, Los Angeles became
known as the ?Queen of the Cow Counties? for its
role in supplying beef and other foodstuffs to hungry
miners in the north.
Consider generating the following question from
that sentence: What did Los Angeles become known
1?History of Los Angeles.? Wikipedia. 2009. Wikimedia
Foundation, Inc. Retrieved Nov. 17, 2009 from: http://en.
wikipedia.org/wiki/History_of_Los_Angeles.
as the ?Queen of the Cow Counties? for?
We observe that the QG process can be viewed
as a two-step process that essentially ?factors? the
problem into simpler components.2 Rather than si-
multaneously trying to remove extraneous informa-
tion and transform a declarative sentence into an in-
terrogative one, we first transform the input sentence
into a simpler sentence such as Los Angeles become
known as the ?Queen of the Cow Counties? for its
role in supplying beef and other foodstuffs to hungry
miners in the north, which we then can then trans-
form into a more succinct question.
Question transformation involves complex long
distance dependencies. For example, in the ques-
tion about Los Angeles, the word what at the begin-
ning of the sentence is a semantic argument of the
verb phrase known as . . . at the end of the ques-
tion. The characteristics of such phenomena are (ar-
guably) difficult to learn from corpora, but they have
been studied extensively in linguistics (Ross, 1967;
Chomsky, 1973). We take a rule-based approach in
order to leverage this linguistic knowledge.
However, since many phenomena pertaining to
question generation are not so easily encoded with
rules, we include statistical ranking as an integral
component. Thus, we employ an overgenerate-and-
rank approach, which has been applied successfully
in areas such as generation (Walker et al, 2001;
Langkilde and Knight, 1998) and syntactic parsing
(Collins, 2000). Since large datasets of the appro-
priate domain, style, and form of questions are not
available to train our ranking model, we learn to rank
from a relatively small, tailored dataset of human-
labeled output from our rule-based system.
The remainder of the paper is organized as fol-
2The motivating example does not exhibit lexical semantic
variations such as synonymy. In this work, we do not model
complex paraphrasing, but believe that paraphrase generation
techniques could be incorporated into our approach.
609
lows. ?2 clarifies connections to prior work and enu-
merates our contributions. ?3 discusses particular
terms and conventions we will employ. ?4 discusses
rule-based question transformation. ?5 describes the
data used to learn and to evaluate our question rank-
ing model, and ?6 then follows with details on the
ranking approach itself. We then present and dis-
cuss results from an evaluation of ranked question
output in ?7 and conclude in ?8.
2 Connections with Prior Work
The generation of questions by humans has long mo-
tivated theoretical work in linguistics (e.g., Ross,
1967), particularly work that portrays questions as
transformations of canonical declarative sentences
(Chomsky, 1973).
Questions have also been a major topic of study
in computational linguistics, but primarily with the
goal of answering questions (Dang et al, 2008).
While much of the question answering research has
focused on retrieval or extraction (e.g., Ravichan-
dran and Hovy, 2001; Hovy et al, 2001), mod-
els of the transformation from answers to questions
have also been developed (Echihabi and Marcu,
2003) with the goal of finding correct answers given
a question (e.g., in a source-channel framework).
Also, Harabagiu et al (2005) present a system that
automatically generates questions from texts to pre-
dict which user-generated questions the text might
answer. In such work on question answering, ques-
tion generation models are typically not evaluated
for their intrinsic quality, but rather with respect to
their utility as an intermediate step in the question
answering process.
QG is very different from many natural language
generation problems because the input is natural lan-
guage rather than a formal representation (cf. Reiter
and Dale, 1997). It is also different from some other
tasks related to generation: unlike machine transla-
tion (e.g., Brown et al, 1990), the input and output
for QG are in the same language, and their length
ratio is often far from one to one; and unlike sen-
tence compression (e.g., Knight and Marcu, 2000),
QG may involve substantial changes to words and
their ordering, beyond simple removal of words.
Some previous research has directly approached
the topic of generating questions for educational
purposes (Mitkov and Ha, 2003; Kunichika et al,
2004; Gates, 2008; Rus and Graessar, 2009; Rus and
Lester, 2009), but to our knowledge, none has in-
volved statistical models for choosing among output
candidates. Mitkov et al (2006) demonstrated that
automatic generation and manual correction of ques-
tions can be more time-efficient than manual author-
ing alone. Much of the prior QG research has evalu-
ated systems in specific domains (e.g., introductory
linguistics, English as a Second Language), and thus
we do not attempt empirical comparisons. Exist-
ing QG systems model their transformations from
source text to questions with many complex rules
for specific question types (e.g., a rule for creating
a question Who did the Subject Verb? from a
sentence with SVO word order and an object refer-
ring to a person), rather than with sets of general
rules.
This paper?s contributions are as follows:
? We apply statistical ranking to the task of gen-
erating natural language questions. In doing so,
we show that question rankings are improved by
considering features beyond surface characteris-
tics such as sentence lengths.
? We model QG as a two-step process of first
simplifying declarative input sentences and then
transforming them into questions, the latter step
being achieved by a sequence of general rules.
? We incorporate linguistic knowledge to explic-
itly model well-studied phenomena related to long
distance dependencies in WH questions, such as
noun phrase island constraints.
? We develop a QG evaluation methodology, in-
cluding the use of broad-domain corpora.
3 Definitions and Conventions
The term ?source sentence? refers to a sentence
taken directly from the input document, from which
a question will be generated (e.g., Kenya is located
in Africa.). The term ?answer phrase? refers to
phrases in declarative sentences which may serve
as targets for WH-movement, and therefore as possi-
ble answers to generated questions (e.g., in Africa).
The term ?question phrase? refers to the phrase con-
taining the WH word that replaces an answer phrase
(e.g., Where in Where is Kenya located?).
610
To represent the syntactic structure of sentences,
we use simplified Penn Treebank-style phrase struc-
ture trees, including POS and category labels, as
produced by the Stanford Parser (Klein and Man-
ning, 2003). Noun phrase heads are selected using
Collins? rules (Collins, 1999).
To implement the rules for transforming source
sentences into questions, we use Tregex, a tree
query language, and Tsurgeon, a tree manipula-
tion language built on top of Tregex (Levy and An-
drew, 2006). The Tregex language includes vari-
ous relational operators based on the primitive re-
lations of immediate dominance (denoted ?<?) and
immediate precedence (denoted ?.?). Tsurgeon
adds the ability to modify trees by relabeling, delet-
ing, moving, and inserting nodes.
4 Rule-based Overgeneration
Many useful questions can be viewed as lexical, syn-
tactic, or semantic transformations of the declarative
sentences in a text. We describe how to model this
process in two steps, as proposed in ?1.3
4.1 Sentence Simplification
In the first step for transforming sentences into ques-
tions, each of the sentences from the source text is
expanded into a set of derived declarative sentences
(which also includes the original sentence) by al-
tering lexical items, syntactic structure, and seman-
tics. Many existing NLP transformations could po-
tentially be exploited in this step, including sentence
compression, paraphrase generation, or lexical se-
mantics for word substitution.
In our implementation, a set of transformations
derive a simpler form of the source sentence by
removing phrase types such as leading conjunc-
tions, sentence-level modifying phrases, and apposi-
tives. Tregex expressions identify the constituents
to move, alter, or delete. Similar transformations
have been utilized in previous work on headline gen-
eration (Dorr and Zajic, 2003) and summarization
(Toutanova et al, 2007).
To enable questions about syntactically embedded
content, our implementation also extracts a set of
declarative sentences from any finite clauses, rela-
3See Heilman and Smith (2009) for details on the rule-based
component.
tive clauses, appositives, and participial phrases that
appear in the source sentence. For example, it trans-
forms the sentence Selling snowballed because of
waves of automatic stop-loss orders, which are trig-
gered by computer when prices fall to certain lev-
els into Automatic stop-loss orders are triggered by
computer when prices fall to certain levels, from
which the next step will produce What are triggered
by computer when prices fall to certain levels?.
4.2 Question Transformation
In the second step, the declarative sentences de-
rived in step 1 are transformed into sets of ques-
tions by a sequence of well-defined syntactic and
lexical transformations (subject-auxiliary inversion,
WH-movement, etc.). It identifies the answer phrases
which may be targets for WH-movement and con-
verts them into question phrases.4
In the current implementation, answer phrases can
be noun phrases or prepositional phrases, which en-
ables who, what, where, when, and how much ques-
tions. The system could be extended to transform
other types of phrases into other types of questions
(e.g., how, why, and what kind of ). It should be
noted that the transformation from answer to ques-
tion is achieved by applying a series of general-
purpose rules. This would allow, for example, the
addition of a rule to generate why questions that
builds off of the existing rules for subject-auxiliary
inversion, verb decomposition, etc. In contrast, pre-
vious QG approaches have employed separate rules
for specific sentence types (e.g., Mitkov and Ha,
2003; Gates, 2008).
For each sentence, many questions may be pro-
duced: there are often multiple possible answer
phrases, and multiple question phrases for each an-
swer phrase. Hence many candidates may result
from the transformations.
These rules encode a substantial amount of lin-
guistic knowledge about the long distance depen-
dencies prevalent in questions, which would be chal-
lenging to learn from existing corpora of questions
and answers consisting typically of only thousands
of examples (e.g., Voorhees, 2003).
Specifically, the following sequence of transfor-
4We leave the generation of correct answers and distractors
to future work.
611
During the Gold Rush years in northern California, 
Los Angeles became known as the "Queen of the 
Cow Counties" for its role in supplying beef and 
other foodstuffs to hungry miners in the north.
Los Angeles became known as the "Queen of the 
Cow Counties" for its role in supplying beef and 
other foodstuffs to hungry miners in the north.
Los Angeles became known as the "QotCC" for 
Los Angeles did become known as the "QotCC" for 
did Los Angeles become known as the "QotCC" for
What did Los Angeles become known as the  "QotCC" for?
Source Sentence
Answer Phrase: its role...
(other possibilities)
(other possibilities)
(other possibilities)
Sentence Simplification
Answer Phrase Selection
Subject-Auxiliary
Inversion
Main Verb Decomposition
Movement and Insertion of Question Phrase
Statistical Ranking
(other possibilities)
1. What became known as ...?
2. What did Los Angeles become known...for?
3. What did Los Angeles become known...as?
4. During the Gold Rush years... ?
5. Whose role in supplying beef...?
...
NP
S
VBD
VP
PP
INVBN
VP
PP
IN NP
VB
VP
PP
INVBN
VP
PP
IN NPNPVBDWP
WHNP
SQ
SBARQ
Figure 1: An illustration of the sequence of steps for generating questions. For clarity, trees are not shown for all steps.
Also, while many questions may be generated from a single source sentence, only one path is shown.
mations is performed, as illustrated in Figure 1:
mark phrases that cannot be answer phrases due to
constraints on WH movement (?4.2.1, not in figure);
select an answer phrase, remove it, and generate pos-
sible question phrases for it (?4.2.2); decompose the
main verb; invert the subject and auxiliary verb; and
insert one of the possible question phrases.
Some of these steps do not apply in all cases. For
example, no answer phrases are removed when gen-
erating yes-no questions.
4.2.1 Marking Unmovable Phrases
In English, various constraints determine whether
phrases can be involved in WH-movement and other
phenomena involving long distance dependencies.
In a seminal dissertation, Ross (1967) described
many of these phenomena. Goldberg (2006) pro-
vides a concise summary of them.
For example, noun phrases are ?islands? to
movement, meaning that constituents dominated
by a noun phrase typically cannot undergo WH-
movement. Thus, from John liked the book that I
gave him, we generate What did John like? but not
*Who did John like the book that gave him?.
We operationalize this linguistic knowledge to ap-
propriately restrict the set of questions produced.
Eight Tregex expressions mark phrases that cannot
be answer phrases due to WH-movement constraints.
For example, the following expression encodes
the noun phrase island constraint described above,
where unmv indicates unmovable noun phrases:
NP << NP=unmv.
4.2.2 Generating Possible Question Phrases
After marking unmovable phrases, we iteratively
remove each possible answer phrase and generate
possible question phrases from it. The system an-
notates the source sentence with a set of entity types
taken from the BBN Identifinder Text Suite (Bikel
et al, 1999) and then uses these entity labels along
with the syntactic structure of a given answer phrase
to generate zero or more question phrases, each of
which is used to generate a final question. (This step
is skipped for yes-no questions.)
5 Rating Questions for Evaluation and
Learning to Rank
Since different sentences from the input text, as well
as different transformations of those sentences, may
be more or less likely to lead to high-quality ques-
tions, each question is scored according to features
of the source sentence, the input sentence, the ques-
tion, and the transformations used in its generation.
The scores are used to rank the questions. This is
612
an example of an ?overgenerate-and-rank? strategy
(Walker et al, 2001; Langkilde and Knight, 1998).
This section describes the acquisition of a set
of rated questions produced by the steps described
above. Separate portions of these labeled data will
be used to develop a discriminative question ranker
(?6), and to evaluate ranked lists of questions (?7).
Fifteen native English-speaking university stu-
dents rated a set of questions produced from steps
1 and 2, indicating whether each question exhibited
any of the deficiencies listed in Table 1.5 If a ques-
tion exhibited no deficiencies, raters were asked to
label it ?acceptable.? Annotators were asked to read
the text of a newswire or encyclopedia article (?5.1
describes the corpora used), and then rate approxi-
mately 100 questions generated from that text. They
were asked to consider each question independently,
such that similar questions about the same informa-
tion would receive similar ratings.
For a predefined training set, each question was
rated by a single annotator (not the same for each
question), leading to a large number of diverse ex-
amples. For the test set, each question was rated by
three people (again, not the same for each question)
to provide a more reliable gold standard. To assign
final labels to the test data, a question was labeled as
acceptable only if a majority of the three raters rated
it as acceptable (i.e., without deficiencies).6
An inter-rater agreement of Fleiss?s ? = 0.42
was computed from the test set?s acceptability rat-
ings. This value corresponds to ?moderate agree-
ment? (Landis and Koch, 1977) and is somewhat
lower than for other rating schemes.7
5.1 Corpora
The training and test datasets consisted of 2,807
and 428 questions, respectively. The questions were
5The ratings from one person were excluded due to an ex-
tremely high rate of accepting questions as error-free and other
irregularities.
6The percentages in Table 1 do not add up to 100% for two
reasons: first, questions are labeled acceptable in the test set
only if the majority of raters labeled them as having no defi-
ciencies, rather than the less strict criterion of requiring no de-
ficiencies to be identified by a majority of raters; second, the
categories are not mutually exclusive.
7E.g., Dolan and Brockett (2005) and Glickman et al (2005)
report ? values around 0.6 for paraphrase identification and tex-
tual entailment, respectively.
generated from three corpora.
The first corpus was a random sample from the
featured articles in the English Wikipedia8 with be-
tween 250 and 2,000 word tokens. This English
Wikipedia corpus provides expository texts written
at an adult reading level from a variety of domains,
which roughly approximates the prose that a sec-
ondary or post-secondary student would encounter.
By choosing from the featured articles, we intended
to select well-edited articles on topics of general in-
terest. The training set included 1,328 questions
about 12 articles, and the test set included 120 ques-
tions about 2 articles from this corpus.
The second corpus was a random sample from the
articles in the Simple English Wikipedia of simi-
lar length. This corpus provides similar text but at
a reading level corresponding to elementary educa-
tion or intermediate second language learning.9 The
training set included 1,195 questions about 16 arti-
cles, and the test set included 118 questions about 2
articles from this corpus.
The third corpus was Section 23 of the Wall Street
Journal data in the Penn Treebank (Marcus et al,
1993).10 The training set included 284 questions
about 8 articles, and the test set included 190 ques-
tions about 2 articles from this corpus.
6 Ranking
We use a discriminative ranker to rank questions,
similar to the approach described by Collins (2000)
for ranking syntactic parses. Questions are ranked
by the predictions of a logistic regression model of
question acceptability. Given the question q and
source text t, the model defines a binomial distribu-
tion p(R | q, t), with binary random variableR rang-
ing over a (?acceptable?) and u (?unacceptable?).
We estimate the parameters by optimizing the reg-
ularized log-likelihood of the training data (cf. ?5.1)
with a variant of Newton?s method (le Cessie and
8The English and Simple English Wikipedia data were
downloaded on December 16, 2008 from http://en.
wikipedia.org and http://simple.wikipedia.
org, respectively.
9The subject matter of the articles in the two Wikipedia cor-
pora was not matched.
10In separate experiments with the Penn Treebank, gold-
standard parses led to an absolute increase of 15% in the per-
centage of acceptable questions (Heilman and Smith, 2009).
613
Question Deficiency Description %
Ungrammatical The question is not a valid English sentence. (e.g., In what were nests excavated exposed to the
sun? from . . . eggs are usually laid . . . , in nests excavated in pockets of earth exposed to the
sun.. This error results from the incorrect attachment by the parser of exposed to the sun to the
verb phrase headed by excavated)
14.0
Does not make sense The question is grammatical but indecipherable. (e.g., Who was the investment?) 20.6
Vague The question is too vague to know exactly what it is asking about, even after reading the article
(e.g., What do modern cities also have? from . . . , but modern cities also have many problems).
19.6
Obvious answer The correct answer would be obvious even to someone who has not read the article (e.g., a
question where the answer is obviously the subject of the article).
0.9
Missing answer The answer to the question is not in the article. 1.4
Wrong WH word The question would be acceptable if the WH phrase were different (e.g., a what question with a
person?s name as the answer).
4.9
Formatting There are minor formatting errors (e.g., with respect to capitalization, punctuation). 8.9
Other The question was unacceptable for other reasons. 1.2
None The question exhibits none of the above deficiencies and is thus acceptable. 27.3
Table 1: Deficiencies a question may exhibit, and the percentages of test set questions labeled with them.
van Houwelingen, 1997). In our experiments, the
regularization constant was selected through cross-
validation on the training data.
The features used by the ranker can be organized
into several groups described in this section. This
feature set was developed by an analysis of ques-
tions generated from the training set. The num-
bers of distinct features for each type are denoted in
parentheses, with the second number, after the ad-
dition symbol, indicating the number of histogram
features (explained below) for that type.
Length Features (3 + 24) The set includes integer
features for the numbers of tokens in the question,
the source sentence, and the answer phrase from
which the WH phrase was generated. These num-
bers of tokens will also be used for computing the
histogram features discussed below.
WH Words (9 + 0) The set includes boolean fea-
tures for the presence of each possible WH word in
the question.
Negation (1 + 0) This is a boolean feature for the
presence of not, never, or no in the question.
N -Gram Language Model Features (6 + 0) The
set includes real valued features for the log like-
lihoods and length-normalized log likelihoods of
the question, the source sentence, and the answer
phrase. Separate likelihood features are included for
unigram and trigram language models. These lan-
guage models were estimated from the written por-
tion of the American National Corpus Second Re-
lease (Ide and Suderman, 2004), which consists of
approximately 20 million tokens, using Kneser and
Ney (1995) smoothing.
Grammatical Features (23 + 95) The set includes
integer features for the numbers of proper nouns,
pronouns, adjectives, adverbs, conjunctions, num-
bers, noun phrases, prepositional phrases, and sub-
ordinate clauses in the phrase structure parse trees
for the question and answer phrase. It also includes
one integer feature for the number of modifying
phrases at the start of the question (e.g., as in At
the end of the Civil War, who led the Union Army?);
three boolean features for whether the main verb is
in past, present, or future tense; and one boolean fea-
ture for whether the main verb is a form of be.
Transformations (8 + 0) The set includes bi-
nary features for the possible syntactic transforma-
tions (e.g., removal of appositives and parentheti-
cals, choosing the subject of source sentence as the
answer phrase).
Vagueness (3 + 15) The set includes integer fea-
tures for the numbers of noun phrases in the ques-
tion, source sentence, and answer phrase that are
potentially vague. We define this set to include pro-
nouns as well as common nouns that are not speci-
fied by a subordinate clause, prepositional phrase, or
possessive. In the training data, we observed many
vague questions resulting from such noun phrases
(e.g., What is the bridge named for?).
614
Histograms In addition to the integer features for
lengths, counts of grammatical types, and counts of
vague noun phrases, the set includes binary ?his-
togram? features for each length or count. These
features indicate whether a count or length exceeds
various thresholds: 0, 1, 2, 3, and 4 for counts; 0,
4, 8, 12, 16, 20, 24, and 28 for lengths. We aim to
account for potentially non-linear relationships be-
tween question quality and these values (e.g., most
good questions are neither very long nor very short).
7 Evaluation and Discussion
This section describes the results of experiments to
evaluate the quality of generated questions before
and after ranking. Results are aggregated across the
3 corpora (?5.1). The evaluation metric we employ
is the percentage of test set questions labeled as ac-
ceptable. For rankings, our metric is the percentage
of the top N% labeled as acceptable, for various N .
7.1 Results for Unranked Questions
First, we present results for the unranked questions
produced by the rule-based overgenerator. As shown
in Table 1, 27.3% of test set questions were labeled
acceptable (i.e., having no deficiencies) by a major-
ity of raters.11
The most frequent deficiencies were ungrammati-
cality (14.0%), vagueness (19.6%), and semantic er-
rors labeled with the ?Does not make sense? cate-
gory (20.6%). Formatting errors (8.9%) were due
to both straightforward issues with pre-processing
and more challenging issues such as failing to iden-
tifying named entities (e.g., Who was nixon?s second
vice president?).
While Table 1 provides data on how often bad
questions were generated, a measure of how often
good questions were not generated would require
knowing the number of possible valid questions. In-
stead, we provide a measure of productivity: the sys-
tem produced an average of 6.0 acceptable questions
per 250 words (i.e., the approximate average number
of words on a single page in a printed book).
7.2 Configurations and Baselines
For ranking experiments, we present results for the
following configurations of features:
1112.1% of test set questions were unanimously acceptable.
All This configuration includes the entire set of
features described in ?6.
Surface Features This configuration includes
only features that can be computed from the sur-
face form of the question, source sentence, and
answer phrase?that is, without hidden linguistic
structures such as parts of speech or syntactic struc-
tures. Specifically, it includes features for lengths,
length histograms, WH words, negation, and lan-
guage model likelihoods.
Question Only This configuration includes all
features of questions, but no features involving the
source sentence or answer phrase (e.g., it does not
include source sentence part of speech counts). It
does not include transformation features.
We also present two baselines for comparison:
Random The expectation of the performance if
questions were ranked randomly.
Oracle The expected performance if all questions
that were labeled acceptable were ranked higher
than all questions that were labeled unacceptable.
7.3 Ranking Results
Figure 2 shows that the percentage of questions
rated as acceptable generally increases as the set
of questions is restricted from the full 428 ques-
tions in the test set to only the top ranked questions.
While 27.3% of all test set questions were accept-
able, 52.3% of the top 20% of ranked questions were
acceptable. Thus, the quality of the top fifth was
nearly doubled by ranking with all the features.
Ranking with surface features also improved
question quality, but to a lesser extent. Thus, unob-
served linguistic features such as parts of speech and
syntax appear to add value for ranking questions.12
The ranker seems to have focused on the ?Does
not make sense? and ?Vague? categories. The
percentage of nonsensical questions dropped from
20.6% to 4.7%, and vagueness dropped from 19.6%
12Ranking with all features was statistically significantly bet-
ter (p < .05) in terms of the percentage of acceptable questions
in the top ranked 20% than ranking with the ?question only?
or ?surface? configurations, or the random baseline, as verified
by computing 95% confidence intervals with the BCa Bootstrap
(Efron and Tibshirani, 1993).
615
50%60%70%
Pct. Rated Acceptable
Oracle All Fe
atures Questio
n Only
Surfac
e Feat
ures
Rando
m
20%30%40%50%60%70% 0
100
200
300
400
Numb
er of T
op-Ra
nked Q
uestio
ns
Oracle All Fe
atures Questio
n Only
Surfac
e Feat
ures
Rando
m
20%30%40%50%60%70% 0
100
200
300
400
Numb
er of T
op-Ra
nked Q
uestio
ns
Oracle All Fe
atures Questio
n Only
Surfac
e Feat
ures
Rando
m
Figure 2: A graph of the percentage of acceptable ques-
tions in the top-N questions in the test set, using various
rankings, for N varying from 0 to the size of the test set.
The percentages become increasingly unstable when re-
stricted to very few questions (e.g., < 50).
to 7.0%, while ungrammaticality dropped from
14.0% to 10.5%, and the other, less prevalent, cat-
egories changed very little.13
7.4 Ablation Study
Ablation experiments were also conducted to study
the effects of removing each of the different types of
features. Table 2 presents the percentages of accept-
able test set questions in the top 20% and top 40%
when they are scored by rankers trained with vari-
ous feature sets that are defined by removing various
feature types from the set of all possible features.
Grammatical features appear to be the most im-
portant: removing them from the feature set resulted
in a 9.0% absolute drop in acceptability in the top
20% of questions, from 52.3% to 43.3%.
Some of the features did not appear to be partic-
ularly helpful, notably the N -gram language model
features. We speculate that they might improve re-
sults when used with a larger, less noisy training set.
Performance did not drop precipitously upon the
removal of any particular feature type, indicating a
high amount of shared variance among the features.
However, removing several types of features at once
led to somewhat larger drops in performance. For
example, using only surface features led to a 12.8%
13We speculate that improvements in syntactic parsing and
entity recognition would reduce the proportion of ungrammati-
cal questions and incorrect WH words, respectively.
Features # Top 20% Top 40%
All 187 52.3 40.8
All ? Length 160 52.3 42.1
All ? WH 178 50.6 39.8
All ? Negation 186 51.7 39.3
All ? Lang. Model 181 51.2 39.9
All ? Grammatical 69 43.2 38.7
All ? Transforms 179 46.5 39.0
All ? Vagueness 169 48.3 41.5
All ? Histograms 53 49.4 39.8
Surface 43 39.5 37.6
Question Only 91 41.9 39.5
Random - 27.3 27.3
Oracle - 100.0 87.3
Table 2: The total numbers of features (#) and the per-
centages of the top 20% and 40% of ranked test set ques-
tions labeled acceptable, for rankers built from variations
of the complete set of features (?All?). E.g., ?All ? WH?
is the set of all features except WH word features.
drop in acceptability in the top 20%, and using only
features of questions led to a 10.4% drop.
8 Conclusion
By ranking the output of rule-based natural lan-
guage generation system, existing knowledge about
WH-movement from linguistics can be leveraged to
model the complex transformations and long dis-
tance dependencies present in questions. Also, in
this overgenerate-and-rank framework, a statistical
ranker trained from a small set of annotated ques-
tions can capture trends related to question quality
that are not easily encoded with rules. In our exper-
iments, we found that ranking approximately dou-
bled the acceptability of the top-ranked questions
generated by our approach.
Acknowledgments
We acknowledge partial support from the Institute
of Education Sciences, U.S. Department of Educa-
tion, through Grant R305B040063 to Carnegie Mel-
lon University; and from the National Science Foun-
dation through a Graduate Research Fellowship for
the first author and grant IIS-0915187 to the second
author. We thank the anonymous reviewers for their
comments.
616
References
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning, 34(1-3).
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2).
N. Chomsky. 1973. Conditions on transformations. A
Festschrift for Morris Halle.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
H. T. Dang, D. Kelly, and J. Lin. 2008. Overview of
the TREC 2007 question answering track. In Proc. of
TREC.
W. B. Dolan and C. Brockett. 2005. Automatically con-
structing a corpus of sentential paraphrases. In Proc.
of IWP.
B. Dorr and D. Zajic. 2003. Hedge Trimmer: A parse-
and-trim approach to headline generation. In Proc. of
Workshop on Automatic Summarization.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proc. of ACL.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman & Hall/CRC.
D. M. Gates. 2008. Generating reading comprehension
look-back strategy questions from expository texts.
Master?s thesis, Carnegie Mellon University.
O. Glickman, I. Dagan, and M. Koppel. 2005. A prob-
abilistic classification approach for lexical textual en-
tailment. In Proc. of AAAI.
A. Goldberg. 2006. Constructions at Work: The Na-
ture of Generalization in Language. Oxford Univer-
sity Press, New York.
S. Harabagiu, A. Hickl, J. Lehmann, and D. Moldovan.
2005. Experiments with interactive question-
answering. In Proc. of ACL.
Michael Heilman and Noah A. Smith. 2009. Ques-
tion generation via overgenerating transformations and
ranking. Technical Report CMU-LTI-09-013, Lan-
guage Technologies Institute, Carnegie Mellon Uni-
versity.
E. Hovy, U. Hermjakob, and C. Lin. 2001. The use of
external knowledge in factoid QA. In Proc. of TREC.
N. Ide and K. Suderman. 2004. The american national
corpus first release. In Proc. of LREC.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. of IEEE Int.
Conf. Acoustics, Speech and Signal Processing.
K. Knight and D. Marcu. 2000. Statistics-based summa-
rization - step one: Sentence compression. In Proc. of
the Seventeenth National Conference on Artificial In-
telligence and Twelfth Conference on Innovative Ap-
plications of Artificial Intelligence.
H. Kunichika, T. Katayama, T. Hirashima, and
A. Takeuchi. 2004. Automated question generation
methods for intelligent English learning systems and
its evaluation. In Proc. of ICCE.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33.
I. Langkilde and Kevin Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Proc.
of ACL.
S. le Cessie and J. C. van Houwelingen. 1997. Ridge es-
timators in logistic regression. Applied Statistics, 41.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proc. of LREC.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
R. Mitkov and L. A. Ha. 2003. Computer-aided gen-
eration of multiple-choice tests. In Proc. of the HLT-
NAACL 03 workshop on Building educational appli-
cations using natural language processing.
R. Mitkov, L. A. Ha, and N. Karamanis. 2006. A
computer-aided environment for generating multiple-
choice test items. Natural Language Engineering,
12(2).
D. Ravichandran and E. Hovy. 2001. Learning surface
text patterns for a question answering system. In Proc.
of ACL.
E. Reiter and R. Dale. 1997. Building applied natural
language generation systems. Nat. Lang. Eng., 3(1).
J. R. Ross. 1967. Constraints on Variables in Syntax.
Phd dissertation, MIT, Cambridge, MA.
V. Rus and A. Graessar, editors. 2009. The Question
Generation Shared Task and Evaluation Challenge.
http://www.questiongeneration.org.
V. Rus and J. Lester, editors. 2009. Proc. of the 2nd
Workshop on Question Generation. IOS Press.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
summarization system: Microsoft research at duc
2007. In Proc. of DUC.
E. M. Voorhees. 2004. Overview of the TREC 2003
question answering track. In Proc. of TREC 2003.
M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proc. of NAACL.
617
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733?736,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Softmax-Margin CRFs: Training Log-Linear Models with Cost Functions
Kevin Gimpel Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We describe a method of incorporating task-
specific cost functions into standard condi-
tional log-likelihood (CLL) training of linear
structured prediction models. Recently intro-
duced in the speech recognition community,
we describe the method generally for struc-
tured models, highlight connections to CLL
and max-margin learning for structured pre-
diction (Taskar et al, 2003), and show that
the method optimizes a bound on risk. The
approach is simple, efficient, and easy to im-
plement, requiring very little change to an
existing CLL implementation. We present
experimental results comparing with several
commonly-used methods for training struc-
tured predictors for named-entity recognition.
1 Introduction
Conditional random fields (CRFs; Lafferty et al
2001) and other conditional log-linear models
(Berger et al, 1996) achieve strong performance
for many NLP problems, but the conditional log-
likelihood (CLL) criterion optimized when training
these models cannot take a task-specific cost func-
tion into account.
In this paper, we describe a simple approach
for training conditional log-linear models with cost
functions. We show how the method relates to other
methods and how it provides a bound on risk. We
apply the method to train a discriminative model
for named-entity recognition, showing a statistically
significant improvement over CLL.
2 Structured Log-Linear Models
Let X denote a structured input space and, for a par-
ticular x ? X, let Y(x) denote a structured output
space for x. The size of Y(x) is often exponential
in x, which differentiates structured prediction from
multiclass classification. For named-entity recogni-
tion, for example, x might be a sentence and Y(x)
the set of all possible named-entity labelings for the
sentence. Given an x ? X and a y ? Y(x), we use a
conditional log-linear model for p?(y|x):
p?(y|x) =
exp{?>f(x, y)}
?
y??Y(x) exp{?
>f(x, y?)}
(1)
where f(x, y) is a feature vector representation of
x and y and ? is a parameter vector containing one
component for each feature.
2.1 Training Criteria
Many criteria exist for training the weights ?. We
next review three choices in detail. For the follow-
ing, we assume a training set consisting of n exam-
ples {?x(i), y(i)?}ni=1. Some criteria will make use of
a task-specific cost function that measures the extent
to which a structure y differs from the true structure
y(i), denoted by cost(y(i), y).
2.1.1 Conditional Log-Likelihood
The learning problem for maximizing conditional
log-likelihood is shown in Eq. 3 in Fig. 1 (we trans-
form it into a minimization problem for easier com-
parison). This criterion is commonly used when a
probabilistic interpretation of the model is desired.
2.1.2 Max-Margin
An alternative approach to training structured lin-
ear classifiers is based on maximum-margin Markov
networks (Taskar et al, 2003). The basic idea is
to choose weights such that the linear score of each
?x(i), y(i)? is better than ?x(i), y? for all alternatives
y ? Y(x(i)) \ {y(i)}, with a larger margin for those
y with higher cost. The ?margin rescaling? form of
this training criterion is shown in Eq. 4. Note that
the cost function is incorporated into the criterion.
2.1.3 Risk
Risk is defined as the expected value of the cost
with respect to the conditional distribution p?(y|x);
733
on training data:
?n
i=1
?
y?Y(x(i)) p?(y|x
(i))cost(y(i), y) (2)
With a log-linear model, learning then requires solv-
ing the problem shown in Eq. 5. Unlike the previous
two criteria, risk is typically non-convex.
Risk minimization first appeared in the speech
recognition community (Kaiser et al, 2000; Povey
and Woodland, 2002). In NLP, Smith and Eis-
ner (2006) minimized risk using k-best lists to de-
fine the distribution over output structures. Li and
Eisner (2009) introduced a novel semiring for min-
imizing risk using dynamic programming; Xiong et
al. (2009) minimized risk in a CRF.
2.1.4 Other Criteria
Many other criteria have been proposed to at-
tempt to tailor training conditions to match task-
specific evaluation metrics. These include the aver-
age per-label marginal likelihood for sequence label-
ing (Kakade et al, 2002), minimum error-rate train-
ing for machine translation (Och, 2003), F1 for lo-
gistic regression classifiers (Jansche, 2005), and a
wide range of possible metrics for sequence label-
ing and segmentation tasks (Suzuki et al, 2006).
3 Softmax-Margin
The softmax-margin objective is shown as Eq. 6 and
is a generalization of that used by Povey et al (2008)
and similar to that used by Sha and Saul (2006).
The simple intuition is the same as the intuition
in max-margin learning: high-cost outputs for x(i)
should be penalized more heavily. Another view
says that we replace the probabilistic score inside
the exp function of CLL with the ?cost-augmented?
score from max-margin. A third view says that we
replace the ?hard? maximum of max-margin with
the ?softmax? (log
?
exp) from CLL; hence we use
the name ?softmax-margin.? Like CLL and max-
margin, the objective is convex; a proof is provided
in Gimpel and Smith (2010).
3.1 Relation to Other Objectives
We next show how the softmax-margin criterion
(Eq. 6) bounds the risk criterion (Eq. 5). We first
define some additional notation:
E(i)[F ] =
?
y?Y(x(i)) p?(y | x
(i))F (y)
for some function F : Y(x(i)) ? R. First note that
the softmax-margin objective (Eq. 6) is equal to:
(Eq. 3) +
?n
i=1 logE(i)[exp cost(y
(i), ?)] (7)
The first term must be nonnegative. Taking each part
of the second term, and using Jensen?s inequality,
logE(i)[e
cost(y(i),?)] ? E(i)[log e
cost(y(i),?)]
= E(i)[cost(y
(i), ?)]
which is exactly Eq. 5. Softmax-margin is also an
upper bound on the CLL criterion because, assum-
ing cost is nonnegative, logE[exp cost] ? 0. Fur-
thermore, softmax-margin is a differentiable upper
bound on max-margin, because the softmax function
is a differentiable upper bound on the max function.
We note that it may also be interest-
ing to consider minimizing the function
?n
i=1 logE(i)[exp cost(y
(i), ?)], since it is an
upper bound on risk but requires less computation
for computing the gradient.1 We call this objec-
tive the Jensen risk bound and include it in our
experimental comparison below.
3.2 Implementation
Most methods for training structured models with
cost functions require the cost function to decom-
pose across the pieces of the structure in the same
way as the features, such as the standard methods
for maximizing margin and minimizing risk (Taskar
et al, 2003; Li and Eisner, 2009). If the same con-
ditions hold, softmax-margin training can be im-
plemented atop standard CRF training simply by
adding additional ?features? to encode the local
cost components, only when computing the partition
function during training.2 The weights of these ?cost
features? are not learned.
4 Experiments
We consider the problem of named-entity recog-
nition (NER) and use the English data from the
CoNLL 2003 shared task (Tjong Kim Sang and De
Meulder, 2003). The data consist of news articles
1Space does not permit a full discussion; see Gimpel and
Smith (2010) for details.
2Since cost(y(i), y(i)) = 0 by definition, these ?features?
will never fire for the numerator and can be ignored.
734
CLL: min
?
n?
i=1
??>f(x(i), y(i)) + log
?
y?Y(x(i))
exp{?>f(x(i), y)} (3)
Max-Margin: min
?
n?
i=1
??>f(x(i), y(i)) + max
y?Y(x(i))
(
?>f(x(i), y) + cost(y(i), y)
)
(4)
Risk: min
?
n?
i=1
?
y?Y(x(i))
cost(y(i), y)
exp{?>f(x(i), y)}
?
y??Y(x(i)) exp{?
>f(x(i), y?)}
(5)
Softmax-Margin: min
?
n?
i=1
??>f(x(i), y(i)) + log
?
y?Y(x(i))
exp{?>f(x(i), y) + cost(y(i), y)} (6)
Figure 1: Objective functions for training linear models. Regularization terms (e.g., C
?d
j=1 ?
2
j ) are not shown here.
annotated with four entity types: person, location,
organization, and miscellaneous. Our experiments
focus on comparing training objectives for struc-
tured sequential models for this task. For all objec-
tives, we use the same standard set of feature tem-
plates, following Kazama and Torisawa (2007) with
additional token shape like those in Collins (2002b)
and simple gazetteer features. A feature was in-
cluded if it occurred at least once in training data
(total 1,312,255 features).
The task is evaluated using the F1 score, which
is the harmonic mean of precision and recall (com-
puted at the level of entire entities). Since this metric
is computed from corpus-level precision and recall,
it is not easily decomposable into features used in
standard chain CRFs. For simplicity, we only con-
sider Hamming cost in this paper; experiments with
other cost functions more targeted to NER are pre-
sented in Gimpel and Smith (2010).
4.1 Baselines
We compared softmax-margin to several baselines:
the structured perceptron (Collins, 2002a), 1-best
MIRA with cost-augmented inference (Crammer et
al., 2006), CLL, max-margin, risk, and our Jensen
risk bound (JRB) introduced above.
We used L2 regularization, experimenting with
several coefficients for each method. For CLL,
softmax-margin, max-margin, and MIRA, we used
regularization coefficients C ? {0.01, 0.1, 1}. Risk
has not always been used with regularization, as reg-
ularization does not have as clear a probabilistic in-
terpretation with risk as it does with CLL; so, for
risk and JRB we only used C ? {0.0, 0.01}. In
addition, since these two objectives are non-convex,
we initialized with the output of the best-performing
CLL model on dev data (which was the CLL model
with C = 0.01).3 All methods except CLL and the
perceptron make use of a cost function, for which
we used Hamming cost. We experimented with dif-
ferent fixed multipliers m for the cost function, for
m ? {1, 5, 10, 20}.
The hyperparameters C and m were tuned on the
development data and the best-performing combina-
tion was used to label the test data. We also tuned
the decision to average parameters across all train-
ing iterations; this has generally been found to help
the perceptron and MIRA, but in our experiments
had mixed results for the other methods.
We ran 100 iterations through the training data for
each method. For CLL, softmax-margin, risk, and
JRB, we used stochastic gradient ascent with a fixed
step size of 0.01. For max-margin, we used stochas-
tic subgradient ascent (Ratliff et al, 2006) also with
a fixed step size of 0.01.4 For the perceptron and
MIRA, we used their built-in step size formulas.
4.2 Results
Table 1 shows our results. On test data, softmax-
margin is statistically indistinguishable from MIRA,
risk, and JRB, but performs significantly better
than CLL, max-margin, and the perceptron (p <
0.03, paired bootstrap with 10,000 samples; Koehn,
3When using initialization of all ones for risk and JRB, re-
sults were several points below the results here, and with all
zeroes, learning failed, resulting in 0.0 F-measure on dev data.
Thus, risk and JRB appear sensitive to model initialization.
4In preliminary experiments, we tried other fixed and de-
creasing step sizes for (sub)gradient ascent and found that a
fixed step of 0.01 consistently performed well across training
objectives, so we used it for all settings for simplicity.
735
Method Dev. Test (C, m, avg.?)
Perceptron 90.48 83.98 (Y)
MIRA 91.13 85.72 (0.01, 20, Y)
CLL 90.79 85.46 (0.01, N)
Max-Margin 91.17 85.28 (0.01, 1, Y)
Risk 91.14 85.59 (0.01, 10, N)
JRB 91.05 85.65 (0.01, 1, N)
Softmax-Margin 91.30 85.84 (0.01, 5, N)
Table 1: Results on development and test sets, along with
hyperparameter values chosen using development set.
2004). It may be surprising that an improvement
of 0.38 in F1 could be significant, but this indicates
that the improvements are not limited to certain cate-
gories of phenomena in a small number of sentences
but rather appear throughout the majority of the test
set. The Jensen risk bound performs comparably to
risk, and takes roughly half as long to train.
5 Discussion
The softmax-margin approach offers (1) a convex
objective, (2) the ability to incorporate task-specific
cost functions, and (3) a probabilistic interpretation
(which supports, e.g., hidden-variable learning and
computation of posteriors). In contrast, max-margin
training and MIRA do not provide (3); risk and
JRB do not provide (1); and CLL does not support
(2). Furthermore, softmax-margin training improves
over standard CLL training of CRFs, is straightfor-
ward to implement, and requires the same amount of
computation as CLL.
We have also presented the Jensen risk bound,
which is easier to implement and faster to train than
risk, yet gives comparable performance. The pri-
mary limitation of all these approaches, including
softmax-margin, is that they only support cost func-
tions that factor in the same way as the features of
the model. Future work might exploit approximate
inference for more expressive cost functions.
Acknowledgments
We thank the reviewers, John Lafferty, and Andre? Martins
for helpful comments and feedback on this work. This
research was supported by NSF grant IIS-0844507.
References
A. Berger, V. J. Della Pietra, and S. A. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
M. Collins. 2002a. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. of EMNLP.
M. Collins. 2002b. Ranking algorithms for named-entity ex-
traction: Boosting and the voted perceptron. In Proc. of
ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
Journal of Machine Learning Research, 7:551?585.
K. Gimpel and N. A. Smith. 2010. Softmax-margin training
for structured log-linear models. Technical report, Carnegie
Mellon University.
M. Jansche. 2005. Maximum expected F -measure training of
logistic regression models. In Proc. of HLT-EMNLP.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss function
for the overall risk criterion based discriminative training of
HMM models. In Proc. of ICSLP.
S. Kakade, Y. W. Teh, and S. Roweis. 2002. An alternate ob-
jective function for Markovian fields. In Proc. of ICML.
J. Kazama and K. Torisawa. 2007. A new perceptron algorithm
for sequence labeling with non-local features. In Proc. of
EMNLP-CoNLL.
P. Koehn. 2004. Statistical significance tests for machine trans-
lation evaluation. In Proc. of EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML.
Z. Li and J. Eisner. 2009. First- and second-order expecta-
tion semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP.
F. J. Och. 2003. Minimum error rate training for statistical
machine translation. In Proc. of ACL.
D. Povey and P. C. Woodland. 2002. Minimum phone error and
I-smoothing for improved discrimative training. In Proc. of
ICASSP.
D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran,
G. Saon, and K. Visweswariah. 2008. Boosted MMI for
model and feature space discriminative training. In Proc. of
ICASSP.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006. Subgradient
methods for maximum margin structured learning. In ICML
Workshop on Learning in Structured Output Spaces.
F. Sha and L. K. Saul. 2006. Large margin hidden Markov
models for automatic speech recognition. In Proc. of NIPS.
D. A. Smith and J. Eisner. 2006. Minimum risk annealing for
training log-linear models. In Proc. of COLING-ACL.
J. Suzuki, E. McDermott, and H. Isozaki. 2006. Training con-
ditional random fields with multivariate evaluation measures.
In Proc. of COLING-ACL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in NIPS 16.
E. F. Tjong Kim Sang and F. DeMeulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent named
entity recognition. In Proc. of CoNLL.
Y. Xiong, J. Zhu, H. Huang, and H. Xu. 2009. Minimum tag
error for discriminative training of conditional random fields.
Information Sciences, 179(1-2):169?179.
736
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 948?956,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Probabilistic Frame-Semantic Parsing
Dipanjan Das Nathan Schneider Desai Chen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan@cs,nschneid@cs,desaic@andrew,nasmith@cs}.cmu.edu
Abstract
This paper contributes a formalization of
frame-semantic parsing as a structure predic-
tion problem and describes an implemented
parser that transforms an English sentence
into a frame-semantic representation. It finds
words that evoke FrameNet frames, selects
frames for them, and locates the arguments
for each frame. The system uses two feature-
based, discriminative probabilistic (log-linear)
models, one with latent variables to permit
disambiguation of new predicate words. The
parser is demonstrated to significantly outper-
form previously published results.
1 Introduction
FrameNet (Fillmore et al, 2003) is a rich linguistic
resource containing considerable information about
lexical and predicate-argument semantics in En-
glish. Grounded in the theory of frame semantics
(Fillmore, 1982), it suggests?but does not formally
define?a semantic representation that blends word-
sense disambiguation and semantic role labeling.
In this paper, we present a computational and
statistical model for frame-semantic parsing, the
problem of extracting from text semantic predicate-
argument structures such as those shown in Fig. 1.
We aim to predict a frame-semantic representation
as a structure, not as a pipeline of classifiers. We
use a probabilistic framework that cleanly integrates
the FrameNet lexicon and (currently very limited)
available training data. Although our models often
involve strong independence assumptions, the prob-
abilistic framework we adopt is highly amenable to
future extension through new features, relaxed in-
dependence assumptions, and semisupervised learn-
ing. Some novel aspects of our current approach
include a latent-variable model that permits disam-
biguation of words not in the FrameNet lexicon, a
unified model for finding and labeling arguments,
TRANSITIVE_
ACTION
Agent
Patient
Event
Cause
Place
Time
CAUSE_TO_
MAKE_NOISE
Agent
Sound_maker
Cause
Place
Time
MAKE_NOISE
Noisy_event
Sound
Sound_source
Place
Time
cough.v, gobble.v, 
ring.v, yodel.v, ...
blare.v, play.v, 
ring.v, toot.v, ...
?
Inheritance relation Causative_of relation
Excludes relation
Purpose
Figure 2. Partial illustration of frames, roles, and LUs
related to the CAUSE TO MAKE NOISE frame, from the
FrameNet lexicon. ?Core? roles are filled ovals. 8 addi-
tional roles of CAUSE TO MAKE NOISE are not shown.
and a precision-boosting constraint that forbids ar-
guments of the same predicate to overlap. Our parser
achieves the best published results to date on the
SemEval?07 FrameNet task (Baker et al, 2007).
2 Resources and Task
We consider frame-semantic parsing resources.
2.1 FrameNet Lexicon
The FrameNet lexicon is a taxonomy of manu-
ally identified general-purpose frames for English.1
Listed in the lexicon with each frame are several
lemmas (with part of speech) that can denote the
frame or some aspect of it?these are called lexi-
cal units (LUs). In a sentence, word or phrase to-
kens that evoke a frame are known as targets. The
set of LUs listed for a frame in FrameNet may not
be exhaustive; we may see a target in new data that
does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame el-
ements, or roles, corresponding to different aspects
of the concept represented by the frame, such as par-
ticipants, props, and attributes. We use the term ar-
1Like the SemEval?07 participants, we used FrameNet v. 1.3
(http://framenet.icsi.berkeley.edu).
948
bell.n
ring.v
there be.v
enough.a
LU
NOISE_MAKERS
SUFFICIENCY
Frame
EXISTENCE
CAUSE_TO_MAKE_NOISE
.bells
 
 
N_m
more than six of the eight
Sound_maker
Enabled_situation
ringtoringers
Item
enough
Entity
Agent
n'tarestillthereBut
Figure 1. A sentence from PropBank and the SemEval?07 training data, and a partial depiction of gold FrameNet
annotations. Each frame is a row below the sentence (ordered for readability). Thick lines indicate targets that evoke
frames; thin solid/dotted lines with labels indicate arguments. ?N m? under bells is short for the Noise maker role of
the NOISE MAKERS frame. The last row indicates that there. . . are is a discontinuous target. In PropBank, the verb
ring is the only annotated predicate for this sentence, and it is not related to other predicates with similar meanings.
FRAMENET LEXICON V. 1.3
lexical exemplars
entries counts coverage
8379 LUs 139K sentences, 3.1M words 70% LUs
795 frames 1 frame annotation / sentence 63% frames
7124 roles 285K overt arguments 56% roles
Table 1. Snapshot of lexicon entries and exemplar sen-
tences. Coverage indicates the fraction of types attested
in at least one exemplar.
gument to refer to a sequence of word tokens anno-
tated as filling a frame role. Fig. 1 shows an exam-
ple sentence from the training data with annotated
targets, LUs, frames, and role-argument pairs. The
FrameNet lexicon also provides information about
relations between frames and between roles (e.g.,
INHERITANCE). Fig. 2 shows a subset of the rela-
tions between three frames and their roles.
Accompanying most frame definitions in the
FrameNet lexicon is a set of lexicographic exemplar
sentences (primarily from the British National Cor-
pus) annotated for that frame. Typically chosen to il-
lustrate variation in argument realization patterns for
the frame in question, these sentences only contain
annotations for a single frame. We found that using
exemplar sentences directly to train our models hurt
performance as evaluated on SemEval?07 data, even
though the number of exemplar sentences is an order
of magnitude larger than the number of sentences in
our training set (?2.2). This is presumably because
the exemplars are neither representative as a sample
nor similar to the test data. Instead, we make use of
these exemplars in features (?4.2).
2.2 Data
Our training, development, and test sets consist
of documents annotated with frame-semantic struc-
tures for the SemEval?07 task, which we refer to col-
FULL-TEXT SemEval?07 data
ANNOTATIONS train dev test
Size (words sentences documents)
all 43.3K1.7K 22 6.3K 251 4 2.8K 120 3
ANC (travel) 3.9K 154 2 .8K 32 1 1.3K 67 1
NTI (bureaucratic) 32.2K1.2K 15 5.5K 219 3 1.5K 53 2
PropBank (news) 7.3K 325 5 0 0 0 0 0 0
Annotations (frames/word overt arguments/word)
all 0.23 0.39 0.22 0.37 0.37 0.65
Coverage of lexicon (% frames % roles % LUs)
all 64.1 27.4 21.0 34.0 10.2 7.3 29.3 7.7 4.9
Out-of-lexicon types (frames roles LUs)
all 14 69 71 2 4 2 39 99 189
Out-of-lexicon tokens (% frames % roles % LUs)
all 0.7 0.9 1.1 1.0 0.4 0.2 9.8 11.2 25.3
Table 2. Snapshot of the SemEval?07 annotated data.
lectively as the SemEval?07 data.2 For the most
part, the frames and roles used in annotating these
documents were defined in the FrameNet lexicon,
but there are some exceptions for which the annota-
tors defined supplementary frames and roles; these
are included in the possible output of our parser.
Table 2 provides a snapshot of the SemEval?07
data. We randomly selected three documents from
the original SemEval training data to create a devel-
opment set for tuning model hyperparameters. No-
tice that the test set contains more annotations per
word, both in terms of frames and arguments. More-
over, there are many more out-of-lexicon frame,
role, and LU types in the test set than in the training
set. This inconsistency in the data results in poor re-
call scores for all models trained on the given data
split, a problem we have not sought to address here.
2http://framenet.icsi.berkeley.edu/
semeval/FSSE.html
949
Preprocessing. We preprocess sentences in our
dataset with a standard set of annotations: POS
tags from MXPOST (Ratnaparkhi, 1996) and depen-
dency parses from the MST parser (McDonald et al,
2005) since manual syntactic parses are not available
for most of the FrameNet-annotated documents. We
used WordNet (Fellbaum, 1998) for lemmatization.
We also labeled each verb in the data as having AC-
TIVE or PASSIVE voice, using code from the SRL
system described by Johansson and Nugues (2008).
2.3 Task and Evaluation
Automatic annotations of frame-semantic structure
can be broken into three parts: (1) targets, the words
or phrases that evoke frames; (2) the frame type,
defined in the lexicon, evoked by each target; and
(3) the arguments, or spans of words that serve to
fill roles defined by each evoked frame. These cor-
respond to the three subtasks in our parser, each
described and evaluated in turn: target identifica-
tion (?3), frame identification (?4, not unlike word-
sense disambiguation), and argument identification
(?5, not unlike semantic role labeling).
The standard evaluation script from the
SemEval?07 shared task calculates precision,
recall, and F1-measure for frames and arguments;
it also provides a score that gives partial credit
for hypothesizing a frame related to the correct
one. We present precision, recall, and F1-measure
microaveraged across the test documents, report
labels-only matching scores (spans must match
exactly), and do not use named entity labels. More
details can be found in Baker et al (2007). For our
experiments, statistical significance is measured us-
ing a reimplementation of Dan Bikel?s randomized
parsing evaluation comparator.3
2.4 Baseline
A strong baseline for frame-semantic parsing is
the system presented by Johansson and Nugues
(2007, hereafter J&N?07), the best system in the
SemEval?07 shared task. For frame identifica-
tion, they used an SVM classifier to disambiguate
frames for known frame-evoking words. They used
WordNet synsets to extend the vocabulary of frame-
evoking words to cover unknown words, and then
3http://www.cis.upenn.edu/?dbikel/
software.html#comparator
TARGET IDENTIFICATION P R F1
Our technique (?3) 89.92 70.79 79.21
Baseline: J&N?07 87.87 67.11 76.10
Table 3. Target identification results for our system and
the baseline. Scores in bold denote significant improve-
ments over the baseline (p < 0.05).
used a collection of separate SVM classifiers?one
for each frame?to predict a single evoked frame for
each occurrence of a word in the extended set.
J&N?07 modeled the argument identification
problem by dividing it into two tasks: first, they
classified candidate spans as to whether they were
arguments or not; then they assigned roles to those
that were identified as arguments. Both phases used
SVMs. Thus, their formulation of the problem in-
volves a multitude of classifiers?whereas ours uses
two log-linear models, each with a single set of
weights, to find a full frame-semantic parse.
3 Target Identification
Target identification is the problem of deciding
which word tokens (or word token sequences) evoke
frames in a given sentence. In other semantic role
labeling schemes (e.g. PropBank), simple part-of-
speech criteria typically distinguish predicates from
non-predicates. But in frame semantics, verbs,
nouns, adjectives, and even prepositions can evoke
frames under certain conditions. One complication
is that semantically-impoverished support predi-
cates (such as make in make a request) do not
evoke frames in the context of a frame-evoking,
syntactially-dependent noun (request). Further-
more, only temporal, locative, and directional senses
of prepositions evoke frames.
We found that, because the test set is more com-
pletely annotated?that is, it boasts far more frames
per token than the training data (see Table 2)?
learned models did not generalize well and achieved
poor test recall. Instead, we followed J&N?07 in us-
ing a small set of rules to identify targets.
For a span to be a candidate target, it must ap-
pear (up to morphological variation) as a target in the
training data or the lexicon. We consider multiword
targets,4 unlike J&N?07 (though we do not consider
4There are 629 multiword LUs in the lexicon, and they cor-
respond to 4.8% of the targets in the training set; among them
are screw up.V, shoot the breeze.V, and weapon of mass de-
950
FRAME IDENTIFICATION exact frame matching partial frame matching
(?4) targets P R F1 P R F1
Frame identification (oracle targets) ? 60.21 60.21 60.21 74.21 74.21 74.21
Frame identification (predicted targets) auto ?3 69.75 54.91 61.44 77.51 61.03 68.29
Baseline: J&N?07 auto 66.22 50.57 57.34 73.86 56.41 63.97
Table 4. Frame identification results. Precision, recall, and F1 were evaluated under exact and partial frame matching;
see ?2.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).
discontinuous targets). Using rules from ?3.1.1 of
J&N?07, we further prune the list, with two modi-
fications: we prune all prepositions, including loca-
tive, temporal, and directional ones, but do not prune
support verbs. This is a conservative approach; our
automatic target identifier will never propose a target
that was not seen in the training data or FrameNet.
Results. Table 3 shows results on target identifica-
tion; our system gains 3 F1 points over the baseline.
4 Frame Identification
Given targets, the parser next identifies their frames.
4.1 Lexical units
FrameNet specifies a great deal of structural infor-
mation both within and among frames. For frame
identification we make use of frame-evoking lexical
units, the (lemmatized and POS-tagged) words and
phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING
frame are 10 LUs, including boast.N, boast.V, boast-
ful.A, brag.V, and braggart.N. Of course, due to pol-
ysemy and homonymy, the same LU may be associ-
ated with multiple frames; for example, gobble.V is
listed under both the INGESTION and MAKE NOISE
frames. All targets in the exemplar sentences, and
most in our training and test data, correspond to
known LUs (see Table 2).
To incorporate frame-evoking expressions found
in the training data but not the lexicon?and to avoid
the possibility of lemmatization errors?our frame
identification model will incorporate, via a latent
variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of
(unlemmatized and automatically POS-tagged) tar-
gets found in the exemplar sentences of the lexi-
con and/or the sentences in our training set. Let
Lf ? L be the subset of these targets annotated as
struction.N. In the SemEval?07 training data, there are just 99
discontinuous multiword targets (1% of all targets).
evoking a particular frame f . Let Ll and Llf de-
note the lemmatized versions of L and Lf respec-
tively. Then, we write boasted.VBD ? LBRAGGING
and boast.VBD ? LlBRAGGING to indicate that this in-
flected verb boasted and its lemma boast have been
seen to evoke the BRAGGING frame. Significantly,
however, another target, such as toot your own horn,
might be used in other data to evoke this frame. We
thus face the additional hurdle of predicting frames
for unknown words.
The SemEval annotators created 47 new frames
not present in the lexicon, out of which 14 belonged
to our training set. We considered these with the 795
frames in the lexicon when parsing new data. Pre-
dicting new frames is a challenge not yet attempted
to our knowledge (including here). Note that the
scoring metric (?2.3) gives partial credit for related
frames (e.g., a more general frame from the lexicon).
4.2 Model
For a given sentence x with frame-evoking targets t,
let ti denote the ith target (a word sequence). Let tli
denote its lemma. We seek a list f = ?f1, . . . , fm?
of frames, one per target. In our model, the set of
candidate frames for ti is defined to include every
frame f such that tli ? L
l
f?or if t
l
i 6? L
l, then every
known frame (the latter condition applies for 4.7%
of the gold targets in the development set). In both
cases, we let Fi be the set of candidate frames for
the ith target in x.
To allow frame identification for targets whose
lemmas were seen in neither the exemplars nor the
training data, our model includes an additional vari-
able, `i. This variable ranges over the seen targets
in Lfi , which can be thought of as prototypes for
the expression of the frame. Importantly, frames are
predicted, but prototypes are summed over via the
latent variable. The prediction rule requires a prob-
abilistic model over frames for a target:
fi ? argmaxf?Fi
?
`?Lf
p(f, ` | ti,x) (1)
951
We adopt a conditional log-linear model: for f ? Fi
and ` ? Lf , p?(f, ` | ti,x) =
exp?>g(f, `, ti,x)
?
f ??Fi
?
`??Lf ?
exp?>g(f ?, `?, ti,x)
(2)
where ? are the model weights, and g is a vector-
valued feature function. This discriminative formu-
lation is very flexible, allowing for a variety of (pos-
sibly overlapping) features; e.g., a feature might re-
late a frame type to a prototype, represent a lexical-
semantic relationship between a prototype and a tar-
get, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better
coverage during frame identification (Johansson and
Nugues, 2007; Burchardt et al, 2005, e.g., by ex-
panding the set of targets using synsets), and others
have sought to extend the lexicon itself (see ?6). We
differ in our use of a latent variable to incorporate
lexical-semantic features in a discriminative model,
relating known lexical units to unknown words that
may evoke frames. Here we are able to take advan-
tage of the large inventory of partially-annotated ex-
emplar sentences.
Note that this model makes a strong independence
assumption: each frame is predicted independently
of all others in the document. In this way the model
is similar to J&N?07. However, ours is a single
conditional model that shares features and weights
across all targets, frames, and prototypes, whereas
the approach of J&N?07 consists of many separately
trained models. Moreover, our model is unique in
that it uses a latent variable to smooth over frames
for unknown or ambiguous LUs.
Frame identification features depend on the pre-
processed sentence x, the prototype ` and its
WordNet lexical-semantic relationship with the tar-
get ti, and of course the frame f . Our model instan-
tiates 662,020 binary features; see Das et al (2010).
4.3 Training
Given the training subset of the SemEval?07 data,
which is of the form
?
?x(j), t(j), f (j),A(j)?
?N
j=1
(N = 1663 is the number of sentences), we dis-
criminatively train the frame identification model by
maximizing the following log-likelihood:5
5We found no benefit on development data from using an L2
regularizer (zero-mean Gaussian prior).
max
?
N?
j=1
mj?
i=1
log
?
`?L
f
(j)
i
p?(f
(j)
i , ` | t
(j)
i ,x
(j)) (3)
Note that the training problem is non-convex be-
cause of the summed-out prototype latent variable
` for each frame. To calculate the objective func-
tion, we need to cope with a sum over frames and
prototypes for each target (see Eq. 2), often an ex-
pensive operation. We locally optimize the function
using a distributed implementation of L-BFGS. This
is the most expensive model that we train: with 100
CPUs, training takes several hours. (Decoding takes
only a few minutes on one CPU for the test set.)
4.4 Results
We evaluate the performance of our frame identifi-
cation model given gold-standard targets and auto-
matically identified targets (?3); see Table 4.
Given gold-standard targets, our model is able
to predict frames for lemmas not seen in training,
of which there are 210. The partial-match evalua-
tion gives our model some credit for 190 of these,
4 of which are exactly correct. The hidden vari-
able model, then, is finding related (but rarely exact)
frames for unknown target words. The net effect of
our conservative target identifier on F1 is actually
positive: the frame identifier is far more precise for
targets seen explicitly in training. Together, our tar-
get and frame identification outperform the baseline
by 4 F1 points. To compare the frame identification
stage in isolation with that of J&N?07, we ran our
frame identification model with the targets identified
by their system as input. With partial matching, our
model achieves a relative improvement of 0.6% F1
over J&N?07 (though this is not significant).
While our frame identification model thus per-
forms on par with the current state of the art for
this task, it improves upon J&N?s formulation of
the problem because it requires only a single model,
learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand
the vocabulary of frame-evoking words, and is prob-
abilistic, which can facilitate global reasoning.
5 Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of tar-
gets t = ?t1, . . . , tm?, and a list of evoked frames
952
f = ?f1, . . . , fm? corresponding to each target, ar-
gument identification is the task of choosing which
of each fi?s roles are filled, and by which parts of x.
This task is most similar to the problem of semantic
role labeling, but uses frame-specific labels that are
richer than the PropBank annotations.
5.1 Model
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s roles
(named frame element types) observed in an exem-
plar sentence and/or our training set. A subset of
each frame?s roles are marked as core roles; these
roles are conceptually and/or syntactically necessary
for any given use of the frame, though they need
not be overt in every sentence involving the frame.
These are roughly analogous to the core arguments
A0?A5 and AA in PropBank. Non-core roles?
analogous to the various AMs in PropBank?loosely
correspond to syntactic adjuncts, and carry broadly-
applicable information such as the time, place, or
purpose of an event. The lexicon imposes some
additional structure on roles, including relations to
other roles in the same or related frames, and se-
mantic types with respect to a small ontology (mark-
ing, for instance, that the entity filling the protag-
onist role must be sentient for frames of cogni-
tion). Fig. 2 illustrates some of the structural ele-
ments comprising the frame lexicon by considering
the CAUSE TO MAKE NOISE frame.
We identify a set S of spans that are candidates for
filling any role r ? Rfi . In principle, S could con-
tain any subsequence of x, but in this work we only
consider the set of contiguous spans that (a) contain
a single word or (b) comprise a valid subtree of a
word and all its descendants in the dependency parse
produced by the MST parser. This covers 81% of ar-
guments in the development data. The empty span
is also included in S, since some roles are not ex-
plicitly filled; in the development data, the average
number of roles an evoked frame defines is 6.7, but
the average number of overt arguments is only 1.7.6
In training, if a labeled argument is not a valid sub-
6In the annotated data, each core role is filled with one of
three types of null instantiations indicating how the role is con-
veyed implicitly. E.g., the imperative construction implicitly
designates a role as filled by the addressee, and the correspond-
ing filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiations.
tree of the dependency parse, we add its span to S .
Let Ai denote the mapping of roles in Rfi to
spans in S. Our model makes a prediction for each
Ai(rk) (for all roles rk ? Rfi) using:
Ai(rk)? argmaxs?S p(s | rk, fi, ti,x) (4)
We use a conditional log-linear model over spans for
each role of each evoked frame:
p?(Ai(rk) = s | fi, ti,x) = (5)
exp?>h(s, rk, fi, ti,x)
?
s??S exp?
>h(s?, rk, fi, ti,x)
Note that our model chooses the span for each
role separately from the other roles and ignores all
frames except the frame the role belongs to. Our
model departs from the traditional SRL literature by
modeling the argument identification problem in a
single stage, rather than first classifying token spans
as arguments and then labeling them. A constraint
implicit in our formulation restricts each role to have
at most one overt argument, which is consistent with
96.5% of the role instances in the training data.
Out of the overt argument spans in the training
data, 12% are duplicates, having been used by some
previous frame in the sentence (supposing some ar-
bitrary ordering of frames). Our role-filling model,
unlike a sentence-global argument detection-and-
classification approach,7 permits this sort of argu-
ment sharing among frames. The incidence of span
overlap among frames is much higher; Fig. 1 illus-
trates a case with a high degree of overlap. Word
tokens belong to an average of 1.6 argument spans
each, including the quarter of words that do not be-
long to any argument.
Features for our log-linear model (Eq. 5) depend
on the preprocessed sentence x; the target t; a
role r of frame f ; and a candidate argument span
s ? S. Our model includes lexicalized and unlexi-
calized features considering aspects of the syntactic
parse (most notably the dependency path in the parse
from the target to the argument); voice; word order-
ing/overlap/distance of the argument with respect to
the target; and POS tags within and around the argu-
ment. Many features have a version specific to the
frame and role, plus a smoothed version incorporat-
ing the role name, but not the frame. These features
7J&N?07, like us, identify arguments for each target.
953
are fully enumerated in (Das et al, 2010); instanti-
ating them for our data yields 1,297,857 parameters.
5.2 Training
We train the argument identification model by:
max
?
N?
j=1
mj?
i=1
|R
f
(j)
i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i ,x
(j))
(6)
This objective function is concave, and we globally
optimize it using stochastic gradient ascent (Bottou,
2004). We train this model until the argument iden-
tification F1 score stops increasing on the develop-
ment data. Best results on this dataset were obtained
with a batch size of 2 and 23 passes through the data.
5.3 Approximate Joint Decoding
Na??ve prediction of roles using Eq. 4 may result
in overlap among arguments filling different roles
of a frame, since the argument identification model
fills each role independently of the others. We want
to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans. We dis-
allow illegal overlap using a 10000-hypothesis beam
search; the algorithm is given in (Das et al, 2010).
5.4 Results
Performance of the argument identification model
is presented in Table 5. The table shows how per-
formance varies given different types of perfect in-
put: correct targets, correct frames, and the set of
correct spans; correct targets and frames, with the
heuristically-constructed set of candidate spans; cor-
rect targets only, with model frames; and ultimately,
no oracle input (the full frame parsing scenario).
The first four rows of results isolate the argu-
ment identification task from the frame identifica-
tion task. Given gold targets and frames and an ora-
cle set of argument spans, our local model achieves
about 87% precision and 75% recall. Beam search
decoding to eliminate illegal argument assignments
within a frame (?5.3) further improves precision by
about 1.6%, with negligible harm to recall. Note
that 96.5% recall is possible under the constraint that
roles are not multiply-filled (?5.1); there is thus con-
siderable room for improvement with this constraint
in place. Joint prediction of each frame?s arguments
is worth exploring to capture correlations not en-
coded in our local models or joint decoding scheme.
The 15-point drop in recall when the heuristically-
built candidate argument set replaces the set of true
argument spans is unsurprising: an estimated 19% of
correct arguments are excluded because they are nei-
ther single words nor complete subtrees (see ?5.1).
Qualitatively, the problem of candidate span recall
seems to be largely due to syntactic parse errors.8
Still, the 10-point decrease in precision when using
the syntactic parse to determine candidate spans sug-
gests that the model has trouble discriminating be-
tween good and bad arguments, and that additional
feature engineering or jointly decoding arguments of
a sentence?s frames may be beneficial in this regard.
The fifth and sixth rows show the effect of auto-
matic frame identification on overall frame parsing
performance. There is a 22% decrease in F1 (18%
when partial credit is given for related frames), sug-
gesting that improved frame identification or joint
prediction of frames and arguments is likely to have
a sizeable impact on overall performance.
The final two rows of the table compare our full
model (target, frame, and argument identification)
with the baseline, showing significant improvement
of more than 4.4 F1 points for both exact and partial
frame matching. As with frame identification, we
compared the argument identification stage with that
of J&N?07 in isolation, using the automatically iden-
tified targets and frames from the latter as input to
our model. With partial frame matching, this gave us
an F1 score of 48.1% on the test set?significantly
better (p < 0.05) than 45.6%, the full parsing re-
sult from J&N?07. This indicates that our argument
identification model?which uses a single discrim-
inative model with a large number of features for
role filling (rather than argument labeling)?is more
powerful than the previous state of the art.
6 Related work
Since Gildea and Jurafsky (2002) pioneered statis-
tical semantic role labeling, a great deal of com-
8Note that, because of our labels-only evaluation scheme
(?2.3), arguments missing a word or containing an extra word
receive no credit. In fact, of the frame roles correctly predicted
as having an overt span, the correct span was predicted 66% of
the time, while 10% of the time the predicted starting and end-
ing boundaries of the span were off by a total of 1 or 2 words.
954
ARGUMENT IDENTIFICATION exact frame matching
targets frames spans decoding P R F1
Argument identifica-
tion (oracle spans)
? ? ? na??ve 86.61 75.11 80.45
? ? ? beam ?5.3 88.29 74.77 80.97
Argument identifica-
tion (full)
? ? model ?5 na??ve 77.43 60.76 68.09 partial frame matching
? ? model ?5 beam ?5.3 78.71 60.57 68.46 P R F1
Parsing (oracle targets) ? model ?4 model ?5 beam ?5.3 49.68 42.82 46.00 57.85 49.86 53.56
Parsing (full) auto ?3 model ?4 model ?5 beam ?5.3 58.08 38.76 46.49 62.76 41.89 50.24
Baseline: J&N?07 auto model model N/A 51.59 35.44 42.01 56.01 38.48 45.62
Table 5. Argument identification results. ? indicates that gold-standard labels were used for a given pipeline stage.
For full parsing, bolded scores indicate significant improvements relative to the baseline (p < 0.05).
putational work has investigated predicate-argument
structures for semantics. Briefly, we highlight some
relevant work, particularly research that has made
use of FrameNet. (Note that much related research
has focused on PropBank (Kingsbury and Palmer,
2002), a set of shallow predicate-argument annota-
tions for Wall Street Journal articles from the Penn
Treebank (Marcus et al, 1993); a recent issue of CL
(Ma`rquez et al, 2008) was devoted to the subject.)
Most work on frame-semantic role labeling has
made use of the exemplar sentences in the FrameNet
corpus (see ?2.1), each of which is annotated for a
single frame and its arguments. On the probabilis-
tic modeling front, Gildea and Jurafsky (2002) pre-
sented a discriminative model for arguments given
the frame; Thompson et al (2003) used a gener-
ative model for both the frame and its arguments;
and Fleischman et al (2003) first used maximum
entropy models to find and label arguments given
the frame. Shi and Mihalcea (2004) developed a
rule-based system to predict frames and their argu-
ments in text, and Erk and Pado? (2006) introduced
the Shalmaneser tool, which employs Na??ve Bayes
classifiers to do the same. Other FrameNet SRL
systems (Giuglea and Moschitti, 2006, for instance)
have used SVMs. Most of this work was done on an
older, smaller version of FrameNet.
Recent work on frame-semantic parsing?in
which sentences may contain multiple frames to be
recognized along with their arguments?has used
the SemEval?07 data (Baker et al, 2007). The LTH
system of Johansson and Nugues (2007), our base-
line (?2.4), performed the best in the SemEval?07
task. Matsubayashi et al (2009) trained a log-
linear model on the SemEval?07 data to evaluate
argument identification features exploiting various
types of taxonomic relations to generalize over roles.
A line of work has sought to extend the coverage
of FrameNet by exploiting VerbNet, WordNet, and
Wikipedia (Shi and Mihalcea, 2005; Giuglea and
Moschitti, 2006; Pennacchiotti et al, 2008; Tonelli
and Giuliano, 2009), and projecting entries and an-
notations within and across languages (Boas, 2002;
Fung and Chen, 2004; Pado? and Lapata, 2005;
Fu?rstenau and Lapata, 2009). Others have applied
frame-semantic structures to question answering,
paraphrase/entailment recognition, and information
extraction (Narayanan and Harabagiu, 2004; Shen
and Lapata, 2007; Pado? and Erk, 2005; Burchardt,
2006; Moschitti et al, 2003; Surdeanu et al, 2003).
7 Conclusion
We have provided a supervised model for rich
frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic
models trained on SemEval?07 data, and expedi-
ent heuristics. Our system achieves improvements
over the state of the art at each stage of process-
ing and collectively, and is amenable to future ex-
tension. Our parser is available for download at
http://www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We thank Collin Baker, Katrin Erk, Richard Johansson,
and Nils Reiter for software, data, evaluation scripts, and
methodological details. We thank the reviewers, Alan
Black, Ric Crabbe, Michael Ellsworth, Rebecca Hwa,
Dan Klein, Russell Lee-Goldman, Dan Roth, Josef Rup-
penhofer, and members of the ARK group for helpful
comments. This work was supported by DARPA grant
NBCH-1080004, NSF grant IIS-0836431, and computa-
tional resources provided by Yahoo.
955
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: frame semantic structure extraction. In
Proc. of SemEval.
H. C. Boas. 2002. Bilingual FrameNet dictionaries for
machine translation. In Proc. of LREC.
L. Bottou. 2004. Stochastic learning. In Advanced Lec-
tures on Machine Learning. Springer-Verlag.
A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet
detour to FrameNet. In B. Fisseni, H.-C. Schmitz,
B. Schro?der, and P. Wagner, editors, Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8. Peter Lang.
A. Burchardt. 2006. Approaching textual entailment
with LFG and FrameNet frames. In Proc. of the Sec-
ond PASCAL RTE Challenge Workshop.
D. Das, N. Schneider, D. Chen, and N. A. Smith.
2010. SEMAFOR 1.0: A probabilistic frame-semantic
parser. Technical Report CMU-LTI-10-001, Carnegie
Mellon University.
K. Erk and S. Pado?. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proc. of LREC.
C. Fellbaum, editor. 1998. WordNet: an electronic lexi-
cal database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP.
P. Fung and B. Chen. 2004. BiFrameNet: bilin-
gual frame semantics resource construction by cross-
lingual induction. In Proc. of COLING.
H. Fu?rstenau and M. Lapata. 2009. Semi-supervised se-
mantic role labeling. In Proc. of EACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
A.-M. Giuglea and A. Moschitti. 2006. Shallow
semantic parsing based on FrameNet, VerbNet and
PropBank. In Proc. of ECAI 2006.
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proc. of
EMNLP.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19(2).
L. Ma`rquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A
comparative study on generalization of semantic roles
in FrameNet. In Proc. of ACL-IJCNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Moschitti, P. Mora?rescu, and S. M. Harabagiu. 2003.
Open-domain information extraction via automatic se-
mantic labeling. In Proc. of FLAIRS.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proc. of COLING.
S. Pado? and K. Erk. 2005. To cause or not to cause:
cross-lingual semantic matching for paraphrase mod-
elling. In Proc. of the Cross-Language Knowledge In-
duction Workshop.
S. Pado? and M. Lapata. 2005. Cross-linguistic projec-
tion of role-semantic information. In Proc. of HLT-
EMNLP.
M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and
M. Roth. 2008. Automatic induction of FrameNet
lexical units. In Proc. of EMNLP.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proc. of EMNLP-
CoNLL.
L. Shi and R. Mihalcea. 2004. An algorithm for open
text semantic parsing. In Proc. of Workshop on Robust
Methods in Analysis of Natural Language Data.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Computational Linguis-
tics and Intelligent Text Processing: Proc. of CICLing
2005. Springer-Verlag.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL.
C. A. Thompson, R. Levy, and C. D. Manning. 2003. A
generative model for semantic role labeling. In Proc.
of ECML.
S. Tonelli and C. Giuliano. 2009. Wikipedia as frame
information repository. In Proc. of EMNLP.
956
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1011?1019,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Tree Edit Models for Recognizing Textual Entailments, Paraphrases,
and Answers to Questions
Michael Heilman Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{mheilman,nasmith}@cs.cmu.edu
Abstract
We describe tree edit models for representing
sequences of tree transformations involving
complex reordering phenomena and demon-
strate that they offer a simple, intuitive, and
effective method for modeling pairs of seman-
tically related sentences. To efficiently extract
sequences of edits, we employ a tree kernel
as a heuristic in a greedy search routine. We
describe a logistic regression model that uses
33 syntactic features of edit sequences to clas-
sify the sentence pairs. The approach leads to
competitive performance in recognizing tex-
tual entailment, paraphrase identification, and
answer selection for question answering.
1 Introduction
Many NLP tasks involve modeling relations be-
tween pairs of sentences or short texts in the same
language. Examples include recognizing textual en-
tailment, paraphrase identification, and question an-
swering. Generic approaches are, of course, desir-
able; we believe such approaches are also feasible
because these tasks exhibit some similar semantic
relationships between sentences.
A popular method for such tasks is Tree Edit Dis-
tance (TED), which models sentence pairs by find-
ing a low or minimal cost sequence of editing oper-
ations to transform a tree representation of one sen-
tence (e.g., a dependency or phrase structure parse
tree) into a tree for the other. Unlike grammar-
based models and shallow-feature discriminative ap-
proaches, TED provides an intuitive story for tree
pairs where one tree is derived from the other by a
sequence of simple transformations.
The available operations in standard TED are the
following: insertion of a node, relabeling (i.e., re-
naming) of a node, and deletion (i.e., removal) of a
node. While the restriction to these three operations
permits efficient dynamic programming solutions
for finding a minimum-cost edit sequence (Klein,
1989; Zhang and Shasha, 1989), certain interesting
and prevalent phenomena involving reordering and
movement cannot be elegantly captured. For exam-
ple, consider the following sentence pair, which is
a simplified version of a true entailment (i.e., the
premise entails the hypothesis) in the development
data for the RTE-3 task.
Premise: Pierce built the home for his daughter off
Rossville Blvd, as he lives nearby.
Hypothesis: Pierce lives near Rossville Blvd.
In a plausible dependency tree representation of
the premise, live and Rossville Blvd would be in sep-
arate subtrees under built. In the hypothesis tree,
however, the corresponding nodes would be in a
grandparent-child relationship as part of the same
phrase, lives near Rossville Blvd. In general, one
would expect that short transformation sequences to
provide good evidence of true entailments. How-
ever, to account for the grandparent-child relation-
ship in the hypothesis, TED would produce a fairly
long sequence, relabeling nearby to be near, delet-
ing the two nodes for Rossville Blvd, and then re-
inserting those nodes under near.
We describe a tree edit approach that allows for
more effective modeling of such complex reordering
phenomena. Our approach can find a shorter and
more intuitive edit sequence, relabeling nearby to be
near, and then moving the whole subtree Rossville
Blvd to be a child of near, as shown in Figure 1.
A model should also be able to consider character-
istics of the tree edit sequence other than its overall
length (e.g., how many proper nouns were deleted).
Using a classifier with a small number of syntactic
1011
Pierce lives near Rossville Blvd.
Pierce built the home for his daughter off Rossville Blvd, as he lives nearby.
Pierce built the home for his daughter off Rossville Blvd, as he lives near.
built the home for his daughter off, as Pierce he lives near Rossville Blvd.
Pierce built the home for his daughter off, as he lives near Rossville Blvd.
Piercie lvsinaRoBd.b
uvti hmcfPiinPg,,y2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 221?231,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Structured Ramp Loss Minimization for Machine Translation
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
This paper seeks to close the gap between
training algorithms used in statistical machine
translation and machine learning, specifically
the framework of empirical risk minimization.
We review well-known algorithms, arguing
that they do not optimize the loss functions
they are assumed to optimize when applied to
machine translation. Instead, most have im-
plicit connections to particular forms of ramp
loss. We propose to minimize ramp loss di-
rectly and present a training algorithm that is
easy to implement and that performs compa-
rably to others. Most notably, our structured
ramp loss minimization algorithm, RAMPION,
is less sensitive to initialization and random
seeds than standard approaches.
1 Introduction
Every statistical MT system relies on a training al-
gorithm to fit the parameters of a scoring function to
examples from parallel text. Well-known examples
include MERT (Och, 2003), MIRA (Chiang et al,
2008), and PRO (Hopkins and May, 2011). While
such procedures can be analyzed as machine learn-
ing algorithms?e.g., in the general framework of
empirical risk minimization (Vapnik, 1998)?their
procedural specifications have made this difficult.
From a practical perspective, such algorithms are of-
ten complex, difficult to replicate, and sensitive to
initialization, random seeds, and other hyperparam-
eters.
In this paper, we consider training algorithms that
are first specified declaratively, as loss functions to
be minimized. We relate well-known training algo-
rithms for MT to particular loss functions. We show
that a family of structured ramp loss functions (Do
et al, 2008) is useful for this analysis. For example,
McAllester and Keshet (2011) recently suggested
that, while Chiang et al (2008, 2009) described their
algorithm as ?MIRA? (Crammer et al, 2006), in fact
it targets a kind of ramp loss. We note here other ex-
amples: Liang et al (2006) described their algorithm
as a variant of the perceptron (Collins, 2002), which
has a unique loss function, but the loss actually opti-
mized is closer to a particular ramp loss (that differs
from the one targeted by Chiang et al). Och and
Ney (2002) sought to optimize log loss (likelihood
in a probabilistic model; Lafferty et al, 2001) but
actually optimized a version of the soft ramp loss.
Why isn?t the application of ML to MT more
straightforward? We note two key reasons: (i) ML
generally assumes that the correct output can always
be scored by a model, but in MT the reference trans-
lation is often unreachable, due to a model?s limited
expressive power or search error, requiring the use
of ?surrogate? references; (ii) MT models nearly al-
ways include latent derivation variables, leading to
non-convex losses that have generally received little
attention in ML. In this paper, we discuss how these
two have caused a disconnect between the loss func-
tion minimized by an algorithm in ML and the loss
minimized when it is adapted for MT.
From a practical perspective, our framework leads
to a simple training algorithm for structured ramp
loss based on general optimization techniques. Our
algorithm is simple to implement and, being a batch
algorithm like MERT and PRO, can easily be inte-
221
grated with any decoder. Our experiments show that
our algorithm, which we call RAMPION, performs
comparably to MERT and PRO, is less sensitive to
randomization and initialization conditions, and is
robust in large-feature scenarios.
2 Notation and Background
Let X denote the set of all strings in a source lan-
guage and, for a particular x ? X, let Y(x) denote
the set of its possible translations (correct and incor-
rect) in the target language. In typical models for
machine translation, a hidden variable is assumed
to be constructed during the translation process.1
Regardless of its specific form, we will refer to it as
a derivation and denote it h ? H(x), where H(x)
is the set of possible values of h for the input x.
Derivations will always be coupled with translations
and therefore we define the set T(x) ? Y(x)?H(x)
of valid output pairs ?y,h? for x.
To model translation, we use a linear model pa-
rameterized by a parameter vector ? ? ?. Given a
vector f(x,y,h) of feature functions on x, y, and
h, and assuming ? contains a component for each
feature function, output pairs ?y,h? for a given in-
put x are selected using a simple argmax decision
rule: ?y?,h?? = argmax
?y,h??T(x)
?>f(x,y,h)
? ?? ?
score(x,y,h;?)
.
The training problem for machine translation cor-
responds to choosing ?. There are many ways to do
this, and we will describe each in terms of a partic-
ular loss function loss : XN ? YN ?? ? R that
maps an input corpus, its reference translations, and
the model parameters to a real value indicating the
quality of the parameters. Risk minimization cor-
responds to choosing
argmin??? Ep(X,Y ) [loss (X,Y ,?)] (1)
where p(X,Y ) is the (unknown) true joint distri-
bution over corpora. We note that the loss function
depends on the entire corpus, while the decoder op-
erates independently on one sentence at a time. This
is done to fit the standard assumptions in MT sys-
tems: the evaluation metric (e.g., BLEU) depends on
1For phrase-based MT, a segmentation of the source
and target sentences into phrases and an alignment between
them (Koehn et al, 2003). For hierarchical phrase-based MT, a
derivation under a synchronous CFG (Chiang, 2005).
the entire corpus and does not decompose linearly,
while the model score does. Since in practice we do
not know p(X,Y ), but we do have access to an ac-
tual corpus pair ?X?, Y? ?, where X? = {x(i)}Ni=1 and
Y? = {y(i)}Ni=1, we instead consider regularized
empirical risk minimization:
argmin??? loss(X?, Y? ,?) +R(?) (2)
where R(?) is the regularization function used to
mitigate overfitting. The regularization function is
frequently a squared norm of the parameter vector,
such as the `1 or `2 norm, but many other choices
are possible. In this paper, we use `2.
Models are evaluated using a task-specific notion
of error, here encoded as a cost function, cost :
YN ? YN ? R?0, such that the worse a translation
is, the higher its cost. The cost function will typi-
cally make use of an automatic evaluation metric for
machine translation; e.g., cost might be 1 minus the
BLEU score (Papineni et al, 2001).2
We note that our analysis in this paper is appli-
cable for understanding the loss function being op-
timized given a fixed set of k-best lists.3 However,
most training procedures periodically invoke the de-
coder to generate new k-best lists, which are then
typically merged with those from previous training
iterations. It is an open question how this practice
affects the loss function being optimized by the pro-
cedure as a whole.
Example 1: MERT. The most commonly-used
training algorithm for machine translation is mini-
mum error rate training, which seeks to directly
minimize the cost of the predictions on the training
data. This idea has been used in the pattern recogni-
tion and speech recognition communities (Duda and
Hart, 1973; Juang et al, 1997); its first application
to MT was by Och (2003). The loss function takes
the following form: losscost
(
X?, Y? ,?
)
=
cost
?
?Y? ,
{
argmax
?y,h??T(x(i))
score(x(i),y,h;?)
}N
i=1
?
?
(3)
2We will abuse notation and allow cost to operate on both
sets of sentences as well as individual sentences. For nota-
tional convenience we also let cost accept hidden variables but
assume that the hidden variables do not affect the value; i.e.,
cost(?y,h?, ?y?,h??) = cost(y, ?y?,h??) = cost(y,y?).
3Cherry and Foster (2012) have concurrently performed a
similar analysis.
222
MERT directly minimizes the corpus-level cost
function of the best outputs from the decoder with-
out any regularization (i.e., R(?) = 0).4 The loss is
non-convex and not differentiable for cost functions
like BLEU, so Och (2003) developed a coordinate
ascent procedure with a specialized line search.
MERT avoids the need to compute feature vec-
tors for the references (?1(i)) and allows corpus-
level metrics like BLEU to be easily incorporated.
However, the complexity of the loss and the diffi-
culty of the search lead to instabilities during learn-
ing. Remedies have been suggested, typically in-
volving additional search directions and experiment
replicates (Cer et al, 2008; Moore and Quirk, 2008;
Foster and Kuhn, 2009; Clark et al, 2011). But de-
spite these improvements, MERT is ineffectual for
training weights for large numbers of features; in
addition to anecdotal evidence from the MT com-
munity, Hopkins and May (2011) illustrated with
synthetic data experiments that MERT struggles in-
creasingly to find the optimal solution as the number
of parameters grows.
Example 2: Probabilistic Models. By exponenti-
ating and normalizing score(x,y,h;?), we obtain
a conditional log-linear model, which is useful for
training criteria with probabilistic interpretations:
p?(y,h|x) = 1Z(x,?) exp{score(x,y,h;?)} (4)
The log loss then defines losslog(X?, Y? ,?) =
?
?N
i=1 log p?(y
(i) | x(i)).
Example 3: Bayes Risk. The term ?risk? as used
above should not be confused with the Bayes risk
framework, which uses a probability distribution
(Eq. 4) and a cost function to define a loss:
lossB risk =
?N
i=1 Ep?(y,h|x(i))[cost(y
(i),y)] (5)
The use of this loss is often simply called ?risk
minimization? in the speech and MT communities.
Bayes risk is non-convex, whether or not latent vari-
ables are present. Like MERT, it naturally avoids
the need to compute features for y(i) and uses a
cost function, making it appealing for MT. Bayes
risk minimization first appeared in the speech recog-
nition community (Kaiser et al, 2000; Povey and
4However, Cer et al (2008) and Macherey et al (2008)
achieved a sort of regularization by altering MERT?s line search.
Woodland, 2002) and more recently has been ap-
plied to MT (Smith and Eisner, 2006; Zens et al,
2007; Li and Eisner, 2009).
3 Training Methods for MT
In this section we consider other ML-inspired ap-
proaches to MT training, situating each in the frame-
work from ?2: ramp, perceptron, hinge, and ?soft?
losses. Each of the first three kinds of losses can be
understood as a way of selecting, for each x(i), two
candidate translation/derivation pairs: ?y?,h?? and
?y?,h??. During training, the loss function can be
improved by increasing the score of the former and
decreasing the score of the latter, through manipu-
lation of the parameters ?. Figure 1 gives a general
visualization of some of the key output pairs that are
considered for these roles. Learning alters the score
function, or, in the figure, moves points horizontally
so that scores approximate negated costs.
3.1 Structured Ramp Loss Minimization
The structured ramp loss (Do et al, 2008) is a
non-convex loss function with certain attractive the-
oretical properties. It is an upper bound on losscost
(Eq. 3) and is a tighter bound than other loss func-
tions (Collobert et al, 2006). Ramp loss has been
shown to be statistically consistent in the sense
that, in the limit of infinite training data, mini-
mizing structured ramp loss reaches the minimum
value of losscost that is achievable with a linear
model (McAllester and Keshet, 2011). This is true
whether or not latent variables are present.
Consistency in this sense is not a common prop-
erty of loss functions; commonly-used convex loss
functions such as the perceptron, hinge, and log
losses (discussed below) are not consistent, because
they are all sensitive to outliers or otherwise noisy
training examples. Ramp loss is better at dealing
with outliers in the training data (Collobert et al,
2006).
There are three forms of latent structured ramp
loss: Eq. 6?8 (Fig. 2). Ramp losses are appealing for
MT because they do not require computing the fea-
ture vector of y(i) (?1(i)). The first form, Eq. 6, sets
?y?,h?? to be the current model prediction (?y?, h??
in Fig. 1) and ?y?,h?? to be an output that is both
favored by the model and has high cost. Such an
223
yy
^
y*
^?*^y*
score
- cost
score
- c
os
t
- cost
score
- cost y*
y
score
- cost y*
y^
score
- cost y*
y^
cost diminished
?+
y-
?y?,h?? = argmin
?y,h??T (x(i))
cost(y(i),y)
?y,h? = argmax
?y,h??T (x(i))
score(x(i),y,h;?)? cost(y(i),y)
?y?,h?? = argmax
?y,h??T (x(i))
score(x(i),y,h;?) + cost(y(i),y) score
- c
os
t
?y?, h?? = argmax
?y,h??T (x(i))
score(x(i),y,h;?)
score
- c
os
t ?y
?,h?? = argmin
?y,h??T (x(i))
cost(y(i),y)
?y,h? = argmax
Ty,h???(x(i))
score(x(i),y,h;?)? cost(y(i),y)
?y?,h?? = argmax
?y,h??T (x(i))
score(x(i),y,h;?) + cost(y(i),y)
?=y, =h? ; argmat
?y,h??T (x(i))
score(x(i),y,h+?)
score
- c
os
t
argmin
?y,h??K(x(i))
cost(y(i),y)
argmin
?y,h??K(x(i))
cost(y(i),y)
Figure 1: Hypothetical output space of a translation model for an input sentence x(i). Each point corresponds to a
single translation/derivation output pair. Horizontal ?bands? are caused by output pairs with the same translation (and
hence the same cost) but different derivations. The left plot shows the entire output space and the right plot highlights
outputs in the k-best list. Choosing the output with the lowest cost in the k-best list is similar to finding ?y+,h+?.
output is shown as ?y?,h?? in Fig. 1; finding y?
is often called cost-augmented decoding, which is
also used to define hinge loss (?3.3).
The second form, Eq. 7, penalizes the model
prediction (?y?,h?? = ?y?, h??) and favors an out-
put pair that has both high model score and low
cost; this is the converse of cost-augmented decod-
ing and therefore we call it cost-diminished decod-
ing; ?y?,h?? = ?y+,h+? in Fig. 1. The third form,
Eq. 8, sets ?y?,h?? = ?y+,h+? and ?y?,h?? =
?y?,h??. This loss underlies RAMPION. It is sim-
ilar to the loss optimized by the MIRA-inspired al-
gorithm used by Chiang et al (2008, 2009).
Optimization The ramp losses are continuous but
non-convex and non-differentiable, so gradient-
based optimization methods are not available.5 For-
tunately, Eq. 8 can be optimized by using a concave-
convex procedure (CCCP; Yuille and Rangarajan,
2002). CCCP is a batch optimization algorithm for
any function that is the the sum of a concave and a
convex function. The idea is to approximate the sum
as the convex term plus a tangent line to the con-
cave function at the current parameter values; the
resulting sum is convex and can be optimized with
(sub)gradient methods.
5For non-differentiable, continuous, convex functions, sub-
gradient-based methods are available, such as stochastic sub-
gradient descent (SSD), and it is tempting to apply them here.
However, non-convex functions are not everywhere subdiffer-
entiable and so a straightforward application of SSD may en-
counter problems in practice.
With our loss functions, CCCP first imputes the
outputs in the concave terms in each loss (i.e., solves
the negated max expressions) for the entire training
set and then uses an optimization procedure to op-
timize the loss with the imputed values fixed. Any
convex optimization procedure can be used once the
negated max terms are solved; we use stochastic
subgradient descent (SSD) but MIRA could be eas-
ily used instead.
The CCCP algorithm we use for optimizing
lossramp 3, which we call RAMPION, is shown as
Alg. 1. Similar algorithms can easily be derived for
the other ramp losses. The first step done on each
iteration is to generate k-best lists for the full tun-
ing set (line 3). We then run CCCP on the k-best
lists for T ? iterations (lines 4?15). This involves first
finding the translation to update towards for all sen-
tences in the tuning set (lines 5?7), then making pa-
rameter updates in an online fashion with T ?? epochs
of stochastic subgradient descent (lines 8?14). The
subgradient update for the `2 regularization term is
done in line 11 and then for the loss in line 12.6
Unlike prior work that targeted similar loss func-
tions (Watanabe et al, 2007; Chiang et al, 2008;
Chiang et al, 2009), we do not use a fully online al-
gorithm such as MIRA in an outer loop because we
are not aware of an online learning algorithm with
theoretical guarantees for non-differentiable, non-
convex loss functions like the ramp losses. CCCP
6`2 regularization done here regularizes toward ?0, not 0.
224
lossramp 1 =
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)) + max
?y,h??Ti
(scorei(y,h;?) + costi(y)) (6)
lossramp 2 =
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)? costi(y)) + max
?y,h??Ti
(scorei(y,h;?)) (7)
lossramp 3 =
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)? costi(y)) + max
?y,h??Ti
(scorei(y,h;?) + costi(y)) (8)
lossperc =
N?
i=1
? max
h:?y(i),h??Ti
scorei(y(i),h;?) + max
?y,h??Ti
scorei(y,h;?) (9)
lossperc kbest =
n?
i=1
?score
(
x(i), argmin
?y,h??Ki
(costi(y)) ;?
)
+ max
?y,h??Ti
scorei(y,h;?) (10)
?
N?
i=1
? max
?y,h??Ti
(scorei(y,h;?)? ?icosti(y)) + max
?y,h??Ti
scorei(y,h;?) (11)
Figure 2: Formulae mentioned in text for latent-variable loss functions. Each loss is actually a function loss(X?, Y? ,?);
we suppress the arguments for clarity. ?Ti? is shorthand for ?T(x(i)).? ?Ki? is shorthand for the k-best list for x(i).
?costi(?)? is shorthand for ?cost(y(i), ?).? ?scorei(?)? is shorthand for ?score(x(i), ?).? As noted in ?3.4, any operator
of the form maxs?S can be replaced by log
?
s?S exp, known as softmax, giving many additional loss functions.
is fundamentally a batch optimization algorithm and
has been used for solving many non-convex learn-
ing problems, such as latent structured SVMs (Yu
and Joachims, 2009).
3.2 Structured Perceptron
The stuctured perceptron algorithm (Collins, 2002)
was considered by Liang et al (2006) as an alterna-
tive to MERT. It requires only a decoder and comes
with some attractive guarantees, at least for mod-
els without latent variables. Liang et al modified
the perceptron in several ways for use in MT. The
first was to generalize it to handle latent variables.
The second change relates to the need to compute
the feature vector for the reference translation y(i),
which may be unreachable (?1(i)). To address this,
researchers have proposed the use of surrogates that
are both favored by the current model parameters
and similar to the reference. Och and Ney (2002)
were the first to do so, using the translation on a
k-best list with the highest evaluation metric score
as y?. This practice was followed by Liang et al
(2006) and others with success (Arun and Koehn,
2007; Watanabe et al, 2007).7
Perceptron Loss Though typically described and
7Liang et al (2006) also tried a variant that updated directly
to the reference when it is reachable (?bold updating?), but they
and others found that Och and Ney?s strategy worked better.
analyzed procedurally, it is straightforward to show
that Collins? perceptron (without latent variables)
equates to SSD with fixed step size 1 on loss:
N?
i=1
?score(x(i),y(i);?)+ max
y?Y(x(i))
score(x(i),y;?)
(12)
This loss is convex but ignores cost functions.
In our notation, y? = y(i) and y? =
argmaxy?Y(x(i)) score(x
(i),y;?).
Adaptation for MT We chart the transformations
from Eq. 12 toward the loss Liang et al?s algorithm
actually optimized. First, generalize to latent vari-
ables; see Eq. 9 (Fig. 2), sacrificing convexity. Sec-
ond, to cope with unreachable references, use a k-
best surrogate as shown in Eq. 10 (Fig. 2), where
Ki ? T(x(i))k is a set containing the k best out-
put pairs for x(i). Now the loss only depends on
y(i) through the cost function. (Even without hid-
den variables, this loss can only be convex when the
k-best list is fixed, keeping y? unchanged across it-
erations. Updating the k-best lists makes y? depend
on ?, resulting in a non-convex loss.)
It appears that Eq. 10 (Fig. 2) is the loss that
Liang et al (2006) sought to optimize, using SSD. In
light of footnote 5 and the non-convexity of Eq. 10
(Fig. 2), we have no theoretical guarantee that such
an algorithm will find a (local) optimum.
225
Input: inputs {x(i)}Ni=1, references {y
(i)}Ni=1, init.
weights ?0, k-best list size k, step size ?, `2
reg. coeff. C, # iters T , # CCCP iters T ?, #
SSD iters T ??
Output: learned weights: ?
? ? ?0;1
for iter ? 1 to T do2
{Ki}Ni=1 ? Decode({x
(i)}Ni=1,?, k);3
for iter ? ? 1 to T ? do4
for i? 1 to N do5
?y+i ,h
+
i ? ?6
argmax?y,h??Ki scorei(y,h;?)? costi(y);
end7
for iter ?? ? 1 to T ?? do8
for i? 1 to N do9
?y?,h?? ?10
argmax?y,h??Ki scorei(y,h;?) + costi(y);
? ?= ?C
(
???0
N
)
;11
? += ?
(
f(x(i),y+i ,h
+
i )? f(x
(i),y?,h?)
)
;12
end13
end14
end15
end16
return ?;17
Algorithm 1: RAMPION.
We note that Eq. 10 is similar to Eq. 11 (Fig. 2),
where each ? is used to trade off between model and
cost. Fig. 1 illustrates the similarity by showing that
the min-cost output on a k-best list resides in a simi-
lar region of the output space as ?y+,h+? computed
from the full output space. While it is not the case
that we can always choose ?i so as to make the two
losses equivalent, they are similar in that they up-
date towards some y? with high model score and
low cost. Eq. 11 corresponds to Eq. 7 (Fig. 2), the
second form of the latent structured ramp loss.
Thus, one way to understand Liang et al?s algo-
rithm is as a form of structured ramp loss. However,
another interpretation is given by McAllester et al
(2010), who showed that procedures like that used
by Liang et al approach direct cost minimization in
the limiting case.
3.3 Large-Margin Methods
A related family of approaches for training MT mod-
els involves the margin-infused relaxed algorithm
(MIRA; Crammer et al, 2006), an online large-
margin training algorithm. It has recently shown
success for MT, particularly when training models
with large feature sets (Watanabe et al, 2007; Chi-
ang et al, 2008; Chiang et al, 2009). In order to
apply it to MT, Watanabe et al and Chiang et al
made modifications similar to those made by Liang
et al for perceptron training, namely the extension
to latent variables and the use of a surrogate refer-
ence with high model score and low cost.
Hinge Loss It can be shown that 1-best MIRA corre-
sponds to dual coordinate ascent for the structured
hinge loss when using `2 regularization (Martins et
al., 2010). The structured hinge is the loss underly-
ing maximum-margin Markov networks (Taskar et
al., 2003): setting y? = y(i) and:
y? = argmax
y?Y(x(i))
(
score(x(i),y;?) + cost(y(i),y)
)
(13)
Unlike the perceptron losses, which penalize the
highest-scoring outputs, hinge loss penalizes an out-
put that is both favored by the model and has high
cost. Such an output is shown as ?y?,h?? in Fig. 1;
the structured hinge loss focuses on pushing such
outputs to the left. As mentioned in ?3.1, finding y?
is often called cost-augmented decoding.
Structured hinge loss is convex, can incorporate
a cost function, and can be optimized with several
algorithms, including SSD (Ratliff et al, 2006).
Adaptation for MT While prior work has used
MIRA-like algorithms for training machine transla-
tion systems, the proposed algorithms did not actu-
ally optimize the structured hinge loss, for similar
reasons to those mentioned above for the perceptron:
latent variables and surrogate references. Incorpo-
rating latent variables in the hinge loss results in
the latent structured hinge loss (Yu and Joachims,
2009). Like the latent perceptron, this loss is non-
convex and inappropriate for MT because it requires
computing the feature vector for y(i). By using a
surrogate instead of y(i), the actual loss optimized
becomes closer to Eq. 8 (Fig. 2), the third form of
the latent structured ramp loss.
Watanabe et al (2007) and Arun and Koehn
(2007) used k-best oracles like Liang et al, but Chi-
ang et al (2008, 2009) used a different approach, ex-
plicitly defining the surrogate as ?y+,h+? in Fig. 1.
While the method of Chiang et al showed impres-
226
sive performance improvements, its implementation
is non-trivial, involving a complex cost function and
a parallel architecture, and it has not yet been em-
braced by the MT community. Indeed, the com-
plexity of Chiang et als algorithm was one of the
reasons cited for the development of PRO (Hopkins
and May, 2011). In this paper, we have sought to
isolate the loss functions used in prior work like that
by Chiang et al and identify simple, generic opti-
mization procedures for optimizing them. We offer
RAMPION as an alternative to Chiang et als MIRA
that is simpler to implement and achieves empirical
success in experiments (?4).
3.4 Likelihood and Softened Losses
We can derive new loss functions from the above
by converting any ?max? operator to a ?softmax?
(log
?
exp, where the set of elements under the
summation is the same as under the max). For exam-
ple, the softmax version of the perceptron loss is the
well-known log loss (?2, Ex. 2), the loss underlying
the conditional likelihood training criterion which
is frequently used when a probabilistic interpreta-
tion of the learned model is desired, as in conditional
random fields (Lafferty et al, 2001).
Och and Ney (2002) popularized the use of log-
linear models for MT and initially sought to opti-
mize log loss, but by using the min-cost transla-
tion on a k-best list as their surrogate, we argue that
their loss was closer to the soft ramp loss obtained
by softening the second max in lossramp 2 in Eq. 7
(Fig. 2). The same is true for others who aimed to
optimize log loss for MT (Smith and Eisner, 2006;
Zens et al, 2007; Cer, 2011).
The softmax version of the latent variable percep-
tron loss, Eq. 9 (Fig. 2), is the latent log loss inher-
ent in latent-variable CRFs (Quattoni et al, 2004).
Blunsom et al (2008) and Blunsom and Osborne
(2008) actually did optimize latent log loss for MT,
discarding training examples for which y(i) was un-
reachable by the model.
Finally, we note that ?softening? the ramp loss
in Eq. 6 (Fig. 2) results in the Jensen risk
bound from Gimpel and Smith (2010), which is
a computationally-attractive upper bound on the
Bayes risk.
4 Experiments
The goal of our experiments is to compare RAM-
PION (Alg. 1) to state-of-the-art methods for train-
ing MT systems. RAMPION minimizes lossramp 3,
which we found in preliminary experiments to work
better than other loss functions tested.8
System and Datasets We use the Moses phrase-
based MT system (Koehn et al, 2007) and consider
Urdu?English (UR?EN), Chinese?English
(ZH?EN) translation, and Arabic?English
(AR?EN) translation.9 We trained a Moses system
using default settings and features, except for
setting the distortion limit to 10. Word alignment
was performed using GIZA++ (Och and Ney, 2003)
in both directions, the grow-diag-final-and
heuristic was used to symmetrize the alignments,
and a max phrase length of 7 was used for phrase
extraction. We estimated 5-gram language models
using the SRI toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing (Chen and Goodman, 1998).
For each language pair, we used the English side
of the parallel text and 600M words of randomly-
selected sentences from the Gigaword v4 corpus
(excluding NYT and LAT).
For UR?EN, we used parallel data from the
NIST MT08 evaluation consisting of 1.2M Urdu
words and 1.1M English words. We used half of
the documents (882 sentences) from the MT08 test
set for tuning. We used the remaining half for
one test set (?MT08??) and MT09 as our other test
set. For ZH?EN, we used 303k sentence pairs
from the FBIS corpus (LDC2003E14). We seg-
mented the Chinese data using the Stanford Chi-
nese segmenter (Chang et al, 2008) in ?CTB? mode,
giving us 7.9M Chinese words and 9.4M English
words. We used MT03 for tuning and used MT02
and MT05 for testing.
For AR?EN, we used data provided by the LDC
8We only present full results using lossramp 3. We found
that minimizing lossramp 1 did poorly, resulting in single-digit
BLEU scores, and that lossramp 2 reached high BLEU scores on
the tuning data but failed to generalize well. Softened versions
of the ramp losses performed comparably to lossramp 3 but were
slightly worse on both tuning and held-out data.
9We found similar trends for other language pairs and sys-
tems, including Hiero (Chiang, 2005). A forthcoming report
will present these results, as well as experiments with additional
loss functions, in detail.
227
for the NIST evaluations, including 3.29M sentence
pairs of UN data and 982k sentence pairs of non-
UN data. The Arabic data was preprocessed using
an HMM segmenter that splits off attached prepo-
sitional phrases, personal pronouns, and the future
marker (Lee et al, 2003). The common stylistic
sentence-initial wa# (and ...) was removed from the
training and test data. The resulting corpus con-
tained 130M Arabic tokens and 130M English to-
kens. We used MT06 for tuning and three test sets:
MT05, the MT08 newswire test set (?MT08 NW?),
and the MT08 weblog test set (?MT08 WB?).
For all languages we evaluated translation output
using case-insensitive IBM BLEU (Papineni et al,
2001).
Training Algorithms Our baselines are MERT and
PRO as implemented in the Moses toolkit.10 PRO
uses the hyperparameter settings from Hopkins and
May (2011), including k-best lists of size 1500 and
25 training iterations.11 MERT uses k-best lists of
size 100 and was run to convergence. For both
MERT and PRO, previous iterations? k-best lists
were merged in.
For RAMPION, we used T = 20, T ? = 10,
T ?? = 5, k = 500, ? = 0.0001, and C = 1.
Our cost function is ?(1 ? BLEU+1(y,y?)) where
BLEU+1(y,y?) returns the BLEU+1 score (Lin and
Och, 2004) for reference y and hypothesis y?. We
used ? = 10. We used these same hyperparameter
values for all experiments reported here and found
them to perform well across other language pairs
and systems.12
4.1 Results
Table 1 shows our results. MERT and PRO were run
3 times with differing random seeds and averages
10The PRO algorithm samples pairs of translations from k-
best lists on each iteration and trains a binary classifier to rank
pairs according to the cost function. The loss function under-
lying PRO depends on the choice of binary classifier and also
on the sampling strategy. We leave an analysis of PRO?s loss
function to future work.
11Hopkins and May used 30 iterations, but showed that train-
ing had converged by 25.
12We found performance to be better when using a smaller
value of T ?; we suspect that using small T ? guards against over-
fitting to any particular set of k-best lists. We also found the
value of ? to affect performance, although ? ? {1, 5, 10} all
worked well. Performance was generally insensitive to C. We
fixed ? = 0.0001 early on and did little tuning to it.
35 36
35
36
M
T
02
 B
L
E
U
Tune BLEU
35 36
34
35
M
T
05
 B
L
E
U
Tune BLEU
MERT
PRO
Rampion
Figure 3: ZH?EN training runs. The cluster of PRO
points to the left corresponds to one of the random initial
models; MERT and RAMPION were able to recover while
PRO was not.
and standard deviations are shown. The three al-
gorithms perform very similarly on the whole, with
certain algorithms performing better on certain lan-
guages. MERT shows larger variation across ran-
dom seeds, as reported by many others in the com-
munity. On average across all language pairs and
test sets, RAMPION leads to slightly higher BLEU
scores.
4.2 Sensitivity Analysis
We now measure the sensitivity of these training
methods to different initializers and to randomness
in the algorithms. RAMPION is deterministic, but
MERT uses random starting points and search di-
rections and PRO uses random sampling to choose
pairs for training its binary classifier.
For initial models, we used the default parame-
ters in Moses as well as two randomly-generated
models.13 We ran RAMPION once with each of the
three initial models, and MERT and PRO three times
with each. This allows us to compare variance due
to initializers as well as due to the nondeterminism
in each algorithm. Fig. 3 plots the results. While
PRO exhibits a small variance for a given initializer,
as also reported by Hopkins and May (2011), it had
13The default weights are 0.3 for reordering features, 0.2 for
phrase table features, 0.5 for the language model, and -1 for the
word penalty. We generated each random model by sampling
each feature weight from aN(?, ?2) with ? equal to the default
weight for that feature and ? = |?/2|.
228
Method
UR?EN ZH?EN AR?EN
avg
MT08? MT09 MT02 MT05 MT05 MT08 NW MT08 WB
MERT 24.5 (0.1) 24.6 (0.0) 35.7 (0.3) 34.2 (0.2) 55.0 (0.7) 49.8 (0.3) 32.6 (0.2) 36.6
PRO 24.2 (0.1) 24.2 (0.1) 36.3 (0.1) 34.5 (0.0) 55.6 (0.1) 49.6 (0.0) 31.7 (0.0) 36.6
RAMPION 24.5 24.6 36.4 34.7 55.5 49.8 32.1 36.8
Table 1: %BLEU on several test sets for UR?EN, ZH?EN, and AR?EN translation. Algorithms with randomization
(MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by
standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).
The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT
and PRO and 7 for RAMPION.
Method
UR?EN ZH?EN
Tune MT08? MT09 Tune MT02 MT05
PRO 29.4 22.3 23.0 40.9 35.7 33.6
RAMPION 27.8 24.2 24.6 38.8 36.2 34.3
Table 2: %BLEU with large feature sets.
trouble recovering from one of the random initializ-
ers. Therefore, while the within-initializer variance
for PRO tended to be smaller than that of MERT,
PRO?s overall range was larger. RAMPION found
very similar weights regardless of ?0.
4.3 Adding Features
Finally, we compare RAMPION and PRO with an ex-
tended feature set; MERT is excluded as it fails in
such settings (Hopkins and May, 2011).
We added count features for common monolin-
gual and bilingual lexical patterns from the parallel
corpus: the 1k most common bilingual word pairs
from phrase extraction, 200 top unigrams, 1k top bi-
grams, 1k top trigrams, and 4k top trigger pairs ex-
tracted with the method of Rosenfeld (1996), ranked
by mutual information. We integrated the features
with our training procedure by using Moses to gen-
erate lattices instead of k-best lists. We used cube
pruning (Chiang, 2007) to incorporate the additional
(potentially non-local) features while extracting k-
best lists from the lattices to pass to the training al-
gorithms.14
Results are shown in Table 2. We find that PRO
finds much higher BLEU scores on the tuning data
but fails to generalize, leading to poor performance
on the held-out test sets. We suspect that incorporat-
ing regularization into training the binary classifier
within PRO may mitigate this overfitting. RAMPION
is more stable by contrast. This is a challenging
learning task, as lexical features are prone to over-
14In cube pruning, each node?s local n-best list had n = 100.
fitting with a small tuning set. Hopkins and May
(2011) similarly found little gain on test data when
using extended feature sets in phrase-based transla-
tion for these two language pairs.
Results for AR?EN translation were similar and
are omitted for space; these and additional experi-
ments will be included in a forthcoming report.
5 Conclusion
We have framed MT training as empirical risk min-
imization and clarified loss functions that were op-
timized by well-known procedures. We have pro-
posed directly optimizing the structured ramp loss
implicit in prior work with a novel algorithm?
RAMPION?which performs comparably to state-
of-the-art training algorithms and is empirically
more stable. Our source code, which integrates
easily with Moses, is available at www.ark.cs.
cmu.edu/MT.
Acknowledgments
We thank Colin Cherry, Chris Dyer, Joseph Keshet,
David McAllester, and members of the ARK research
group for helpful comments that improved this paper.
This research was supported in part by the NSF through
CAREER grant IIS-1054319, the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533, and Sandia
National Laboratories (fellowship to K. Gimpel).
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL.
229
D. Cer, D. Jurafsky, and C. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of ACL-2008 Workshop on Statistical Machine
Translation.
D. Cer. 2011. Parameterizing Phrase Based Statisti-
cal Machine Translation Models: An Analytic Study.
Ph.D. thesis, Stanford University.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of ACL-2008 Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proc. of NAACL.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang, W. Wang, and K. Knight. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL-HLT.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011.
Better hypothesis testing for statistical machine trans-
lation: Controlling for optimizer instability. In Proc.
of ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
R. Collobert, F. Sinz, J. Weston, and L. Bottou. 2006.
Trading convexity for scalability. In ICML.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola.
2008. Tighter bounds for structured estimation. In
Proc. of NIPS.
R. O. Duda and P. E. Hart. 1973. Pattern classification
and scene analysis. John Wiley, New York.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. In Proc. of Fourth Workshop on Statisti-
cal Machine Translation.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In Proc. of NAACL.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
B. H. Juang, W. Chou, and C. H. Lee. 1997. Minimum
classification error rate methods for speech recogni-
tion. Speech and Audio Processing, IEEE Transac-
tions on, 5(3):257?265, may.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss
function for the overall risk criterion based discrimina-
tive training of hmm models. In Proc. of ICSLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.
2003. Language model based Arabic word segmenta-
tion. In Proc. of ACL.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. of EMNLP.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proc. of Coling.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In EMNLP.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M A. T. Figueiredo. 2010. Learn-
ing structured classifiers with dual coordinate descent.
Technical report, Carnegie Mellon University.
D. McAllester and J. Keshet. 2011. Generalization
bounds and consistency for latent structural probit and
ramp loss. In Proc. of NIPS.
D. McAllester, T. Hazan, and J. Keshet. 2010. Direct
loss minimization for structured prediction. In Proc.
of NIPS.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of Coling.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
230
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
D. Povey and P. C. Woodland. 2002. Minimum phone
error and I-smoothing for improved discrimative train-
ing. In Proc. of ICASSP.
A. Quattoni, M. Collins, and T. Darrell. 2004. Condi-
tional random fields for object recognition. In NIPS
17.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006.
Subgradient methods for maximum margin structured
learning. In ICML Workshop on Learning in Struc-
tured Output Spaces.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10(3).
D. A. Smith and J. Eisner. 2006. Minimum risk an-
nealing for training log-linear models. In Proc. of
COLING-ACL.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in NIPS 16.
V. Vapnik. 1998. Statistical learning theory. Wiley.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP-CoNLL.
C. J. Yu and T. Joachims. 2009. Learning structural
SVMs with latent variables. In Proc. of ICML.
A. L. Yuille and Anand Rangarajan. 2002. The concave-
convex procedure (CCCP). In Proc. of NIPS. MIT
Press.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP.
231
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 577?581,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Concavity and Initialization for Unsupervised Dependency Parsing
Kevin Gimpel and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
We investigate models for unsupervised learn-
ing with concave log-likelihood functions. We
begin with the most well-known example,
IBM Model 1 for word alignment (Brown
et al, 1993) and analyze its properties, dis-
cussing why other models for unsupervised
learning are so seldom concave. We then
present concave models for dependency gram-
mar induction and validate them experimen-
tally. We find our concave models to be effec-
tive initializers for the dependency model of
Klein and Manning (2004) and show that we
can encode linguistic knowledge in them for
improved performance.
1 Introduction
In NLP, unsupervised learning typically implies op-
timization of a ?bumpy? objective function riddled
with local maxima. However, one exception is IBM
Model 1 (Brown et al, 1993) for word alignment,
which is the only model commonly used for unsu-
pervised learning in NLP that has a concave log-
likelihood function.1 For other models, such as
those used in unsupervised part-of-speech tagging
and grammar induction, and indeed for more sophis-
ticated word alignment models, the log-likelihood
function maximized by EM is non-concave. As a
result, researchers are obligated to consider initial-
ization in addition to model design (Klein and Man-
ning, 2004; Goldberg et al, 2008).
For example, consider the dependency grammar
induction results shown in Table 1 when training the
1It is not strictly concave (Toutanova and Galley, 2011).
widely used dependency model with valence (DMV;
Klein and Manning, 2004). Using uniform distri-
butions for initialization (UNIF) results in an accu-
racy of 17.6% on the test set, well below the base-
line of attaching each word to its right neighbor
(ATTACHRIGHT, 31.7%). Furthermore, when using
a set of 50 random initializers (RAND), the standard
deviation of the accuracy is an alarming 8.3%.
In light of this sensitivity to initialization, it is
compelling to consider unsupervised models with
concave log-likelihood functions, which may pro-
vide stable, data-supported initializers for more
complex models. In this paper, we explore the issues
involved with such an expedition and elucidate the
limitations of such models for unsupervised NLP.
We then present simple concave models for depen-
dency grammar induction that are easy to implement
and offer efficient optimization. We also show how
linguistic knowledge can be encoded without sacri-
ficing concavity. Using our models to initialize the
DMV, we find that they lead to an improvement in
average accuracy across 18 languages.
2 IBM Model 1 and Concavity
IBM Model 1 is a conditional model of a target-
language sentence e of length m and an alignment
a given a source-language sentence f of length l.
The generation of m is assumed to occur with some
(inconsequential) uniform probability . The align-
ment vector a, a hidden variable, has an entry for
each element of e that contains the index in f of
the aligned word. These entries are used to define
which translation parameters t(ej | faj ) are active.
Model 1 assumes that the probability of the ith ele-
577
ment in a, denoted a(i | j, l,m), is simply a uni-
form distribution over all l source words plus the
null word. These assumptions result in the follow-
ing log-likelihood for a sentence pair ?f , e? under
Model 1 (marginalizing a):
log p(e | f) = log (l+1)m+
?m
j=1 log
?l
i=0 t(ej | fi)
(1)
The only parameters to be learned in the model are
t = {t(e | f)}e,f . Since a parameter is concave in
itself, the sum of concave functions is concave, and
the log of a concave function is concave, Eq. 1 is
concave in t (Brown et al, 1993).
IBM Model 2 involves a slight change to Model
1 in which the probability of a word link depends
on the word positions. However, this change renders
it no longer concave. Consider the log-likelihood
function for Model 2:
log +
?m
j=1 log
?l
i=0 t(ej | fi) ?a(i | j, l,m) (2)
Eq. 2 is not concave in the parameters t(ej | fi) and
a(i | j, l,m) because a product is neither convex nor
concave in its vector of operands. This can be shown
by computing the Hessian matrix of f(x, y) = xy
and showing that it is indefinite.
In general, concavity is lost when the log-
likelihood function contains a product of model pa-
rameters enclosed within a log
?
. If the sum is not
present, the log can be used to separate the prod-
uct of parameters, making the function concave. It
can also be shown that a ?featurized? version (Berg-
Kirkpatrick et al, 2010) of Model 1 is not con-
cave. More generally, any non-concave function en-
closed within log
?
will cause the log-likelihood
function to be non-concave, though there are few
other non-concave functions with a probabilistic se-
mantics than those just discussed.
3 Concave, Unsupervised Models
Nearly every other model used for unsupervised
learning in NLP has a non-concave log-likelihood
function. We now proceed to describe the conditions
necessary to develop concave models for two tasks.
3.1 Part-of-Speech Tagging
Consider a standard first-order hidden Markov
model for POS tagging. Letting y denote the tag
sequence for a sentence e with m tokens, the single-
example log-likelihood is:
log
?
y p(stop | ym)
?m
j=1 p(yj | yj?1) ? p(ej | yj)
(3)
where y0 is a designated ?start? symbol. Unlike IBM
Models 1 and 2, we cannot reverse the order of the
summation and product here because the transition
parameters p(yj | yj?1) cause each tag decision to
affect its neighbors. Therefore, Eq. 3 is non-concave
due to the presence of a product within a log
?
.
However, if the tag transition probabilities p(yj |
yj?1) are all constants and also do not depend on
the previous tag yj?1, then we can rewrite Eq. 3 as
the following concave log-likelihood function (using
C(y) to denote a constant function of tag y, e.g., a
fixed tag prior distribution):
logC(stop) + log
?m
j=1
?
yj
C(yj) ? p(ej | yj)
Lacking any transition modeling power, this model
appears weak for POS tagging. However, we note
that we can add additional conditioning information
to the p(ej | yj) distributions and retain concavity,
such as nearby words and tag dictionary informa-
tion. We speculate that such a model might learn
useful patterns about local contexts and provide an
initializer for unsupervised part-of-speech tagging.
3.2 Dependency Grammar Induction
To develop dependency grammar induction models,
we begin with a version of Model 1 in which a sen-
tence e is generated from a copy of itself (denoted
e?): log p(e | e?)
= log (m+1)m +
?m
j=1 log
?m
i=0,i 6=j c(ej | e
?
i) (4)
If a word ej is ?aligned? to e?0, ej is a root. This
is a simple child-generation model with no tree con-
straint. In order to preserve concavity, we are forbid-
den from conditioning on other parent-child assign-
ments or including any sort of larger constraints.
However, we can condition the child distributions
on additional information about e? since it is fully
observed. This conditioning information may in-
clude the direction of the edge, its distance, and
any properties about the words in the sentence. We
found that conditioning on direction improved per-
formance: we rewrite the c distributions as c(ej |
e?i, sign(j ? i)) and denote this model by CCV1.
578
We note that we can also include constraints in the
sum over possible parents and still preserve concav-
ity. Naseem et al (2010) found that adding parent-
child constraints to a grammar induction system can
improve performance dramatically. We employ one
simple rule: roots are likely to be verbs.2 We mod-
ify CCV1 to restrict the summation over parents to
exclude e?0 if the child word is not a verb.
3 We only
employ this restriction during EM learning for sen-
tences containing at least one verb. For sentences
without verbs, we allow all words to be the root. We
denote this model by CCV2.
In related work, Brody (2010) also developed
grammar induction models based on the IBM word
alignment models. However, while our goal is to
develop concave models, Brody employed Bayesian
nonparametrics in his version of Model 1, which
makes the model non-concave.
4 Experiments
We ran experiments to determine how well our con-
cave grammar induction models CCV1 and CCV2 can
perform on their own and when used as initializers
for the DMV (Klein and Manning, 2004). The DMV
is a generative model of POS tag sequences and pro-
jective dependency trees over them. It is the foun-
dation of most state-of-the-art unsupervised gram-
mar induction models (several of which are listed in
Tab. 1). The model includes multinomial distribu-
tions for generating each POS tag given its parent
and the direction of generation: where ei is the par-
ent POS tag and ej the child tag, these distributions
take the form c(ej | ei, sign(j ? i)), analogous to
the distributions used in our concave models. The
DMV also has multinomial distributions for decid-
ing whether to stop or continue generating children
in each direction considering whether any children
have already been generated in that direction.
The majority of researchers use the original ini-
tializer from Klein and Manning (2004), denoted
here K&M. K&M is a deterministic harmonic initial-
izer that sets parent-child token affinities inversely
2This is similar to the rule used by Marec?ek and Z?abokrtsky?
(2011) with empirical success.
3As verbs, we take all tags that map to V in the universal tag
mappings from Petrov et al (2012). Thus, to apply this con-
straint to a new language, one would have to produce a similar
tag mapping or identify verb tags through manual inspection.
Train ? 10 Train ? 20
Test Test
Model Init. ?10 ?? ?10 ??
ATTRIGHT N/A 38.4 31.7 38.4 31.7
CCV1 UNIF 31.4 25.6 31.0 23.7
CCV2 UNIF 43.1 28.6 43.9 27.1
UNIF 21.3 17.6 21.3 16.4
RAND? 41.0 31.8 - -
DMV K&M 44.1 32.9 51.9 37.8
CCV1 45.3 30.9 53.9 36.7
CCV2 54.3 43.0 64.3 53.1
Shared LN K&M 61.3 41.4
L-EVG RAND? 68.8 -
Feature DMV K&M 63.0 -
LexTSG-DMV K&M 67.7 55.7
Posterior Reg. K&M 64.3 53.3
Punc/UTags K&M? - 59.1?
Table 1: English attachment accuracies on Section 23, for
short sentences (?10 words) and all (??). We include
selected results on this same test set: Shared LN = Cohen
and Smith (2009), L-EVG = Headden III et al (2009),
Feature DMV = Berg-Kirkpatrick et al (2010), LexTSG-
DMV = Blunsom and Cohn (2010), Posterior Reg. =
Gillenwater et al (2010), Punc/UTags = Spitkovsky et
al. (2011a). K&M? is from Spitkovsky et al (2011b).
?Accuracies are averages over 50 random initializers;
? = 10.9 for test sentences ? 10 and 8.3 for all. ?Used
many random initializers with unsupervised run selec-
tion. ?Used staged training with sentences ? 45 words.
proportional to their distances, then normalizes to
obtain probability distributions. K&M is often de-
scribed as corresponding to an initial E step for an
unspecified model that favors short attachments.
Procedure We run EM for our concave models for
100 iterations. We evaluate the learned models di-
rectly as parsers on the test data and also use them
to initialize the DMV. When using them directly as
parsers, we use dynamic programming to ensure that
a valid tree is recovered. When using the concave
models as initializers for the DMV, we copy the c
parameters over directly since they appear in both
models. We do not have the stop/continue parame-
ters in our concave models, so we simply initialize
them uniformly for the DMV. We train each DMV
for 200 iterations and use minimum Bayes risk de-
coding with the final model on the test data. We use
several initializers for training the DMV, including
the uniform initializer (UNIF), K&M, and our trained
concave models CCV1 and CCV2.
579
Init. eu bg ca zh cs da nl en de el hu
UNIF 24/21 32/26 27/29 44/40 32/30 24/19 21/21 21/18 31/24 37/32 23/18
K&M 32/26 48/40 24/25 38/33 31/29 34/23 39/33 44/33 47/37 50/41 23/20
CCV1 22/21 34/27 44/51 46/45 33/31 19/14 24/24 45/31 46/31 51/45 32/28
CCV2 26/25 34/26 29/35 46/44 50/40 29/18 50/43 54/43 49/33 50/45 60/46
it ja pt sl es sv tr avg. accuracy avg. log-likelihood
UNIF 31/24 35/30 49/36 20/20 29/24 26/22 33/30 29.8 / 25.7 -15.05
K&M 32/24 39/31 44/28 33/27 19/11 46/33 39/36 36.7 / 29.4 -14.84
CCV1 34/25 42/27 50/38 30/25 41/33 45/33 37/29 37.5 / 30.9 -14.93
CCV2 55/48 49/31 50/38 22/21 57/50 46/32 31/22 43.7 / 35.5 -14.45
Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences ? 10
words and second is for all sentences. For training, sentences ? 10 words from each treebank were used. In order,
languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian,
Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish.
Data We use data prepared for the CoNLL
2006/07 shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007).4 We follow standard practice
in removing punctuation and using short sentences
(? 10 or ? 20 words) for training. For all experi-
ments, we train on separate data from that used for
testing and use gold POS tags for both training and
testing. We report accuracy on (i) test set sentences
?10 words and (ii) all sentences from the test set.
Results Results for English are shown in Tab. 1.
We train on ?2?21 and test on ?23 in the Penn Tree-
bank. The constraint on sentence roots helps a great
deal, as CCV2 by itself is competitive with the DMV
when testing on short sentences. The true benefit of
the concave models, however, appears when using
them as initializers. The DMV initialized with CCV2
achieves a substantial improvement over all others.
When training on sentences of length ? 20 words
(bold), the performance even rivals that of several
more sophisticated models shown in the table, de-
spite only using the DMV with a different initializer.
Tab. 2 shows results for 18 languages. On av-
erage, CCV2 performs best and CCV1 does at least
as well as K&M. This shows that a simple, concave
model can be as effective as a state-of-the-art hand-
designed initializer (K&M), and that concave mod-
els can encode linguistic knowledge to further im-
prove performance.
4In some cases, we did not use official CoNLL test sets but
instead took the training data and reserved the first 80% of the
sentences for training, the next 10% for development, and the
final 10% as our test set; dataset details are omitted for space
but are the same as those given by Cohen (2011).
Average log-likelihoods (micro-averaged across
sentences) achieved by EM training are shown in the
final column of Tab. 2. CCV2 leads to substantially-
higher likelihoods than the other initializers, sug-
gesting that the verb-root constraint is helping EM
to find better local optima.5
5 Discussion
Staged training has been shown to help unsupervised
learning in the past, from early work in grammar in-
duction (Lari and Young, 1990) and word alignment
(Brown et al, 1993) to more recent work in depen-
dency grammar induction (Spitkovsky et al, 2010).
While we do not yet offer a generic procedure for
extracting a concave approximation from any model
for unsupervised learning, our results contribute evi-
dence in favor of the general methodology of staged
training in unsupervised learning, and provide a sim-
ple and powerful initialization method for depen-
dency grammar induction.
Acknowledgments
We thank Shay Cohen, Dipanjan Das, Val Spitkovsky,
and members of the ARK research group for helpful com-
ments that improved this paper. This research was sup-
ported in part by the NSF through grant IIS-0915187, the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-
1-0533, and Sandia National Laboratories (fellowship to
K. Gimpel).
5However, while CCV1 leads to a higher average accuracy
than K&M, the latter reaches slightly higher likelihood, sug-
gesting that the success of the concave initializers is only par-
tially due to reaching high training likelihood.
580
References
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In Proc. of EMNLP.
S. Brody. 2010. It depends on the translation: Unsu-
pervised dependency parsing via word alignment. In
Proc. of EMNLP.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
S. Cohen and N. A. Smith. 2009. Shared logistic normal
distributions for soft parameter tying in unsupervised
grammar induction. In Proc. of NAACL.
S. Cohen. 2011. Computational Learning of Probabilis-
tic Grammars in the Unsupervised Setting. Ph.D. the-
sis, Carnegie Mellon University.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, , and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Journal of Machine Learning
Research.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proc. of ACL.
W. Headden III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proc. of NAACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. of ACL.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language, 4:35?56.
D. Marec?ek and Z. Z?abokrtsky?. 2011. Gibbs sampling
with treeness constraint in unsupervised dependency
parsing. In Proc. of Workshop on Robust Unsuper-
vised and Semisupervised Methods in Natural Lan-
guage Processing.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proc. of EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
CoNLL.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From Baby Steps to Leapfrog: How ?Less is More?
in unsupervised dependency parsing. In Proc. of
NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, A. X. Chang, and D. Juraf-
sky. 2011a. Unsupervised dependency parsing with-
out gold part-of-speech tags. In Proc. of EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b.
Punctuation: Making a point in unsupervised depen-
dency parsing. In Proc. of CoNLL.
K. Toutanova and M. Galley. 2011. Why initialization
matters for IBM Model 1: Multiple optima and non-
strict convexity. In Proc. of ACL.
581
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677?687,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties
Dipanjan Das and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan,nasmith}@cs.cmu.edu
Abstract
We present novel methods to construct com-
pact natural language lexicons within a graph-
based semi-supervised learning framework,
an attractive platform suited for propagating
soft labels onto new natural language types
from seed data. To achieve compactness,
we induce sparse measures at graph vertices
by incorporating sparsity-inducing penalties
in Gaussian and entropic pairwise Markov
networks constructed from labeled and unla-
beled data. Sparse measures are desirable for
high-dimensional multi-class learning prob-
lems such as the induction of labels on natu-
ral language types, which typically associate
with only a few labels. Compared to standard
graph-based learning methods, for two lexicon
expansion problems, our approach produces
significantly smaller lexicons and obtains bet-
ter predictive performance.
1 Introduction
Semi-supervised learning (SSL) is attractive for the
learning of complex phenomena, for example, lin-
guistic structure, where data annotation is expen-
sive. Natural language processing applications have
benefited from various SSL techniques, such as dis-
tributional word representations (Huang and Yates,
2009; Turian et al, 2010; Dhillon et al, 2011),
self-training (McClosky et al, 2006), and entropy
regularization (Jiao et al, 2006; Smith and Eisner,
2007). In this paper, we focus on semi-supervised
learning that uses a graph constructed from labeled
and unlabeled data. This framework, graph-based
SSL?see Bengio et al (2006) and Zhu (2008) for
introductory material on this topic?has been widely
used and has been shown to perform better than sev-
eral other semi-supervised algorithms on benchmark
datasets (Chapelle et al, 2006, ch. 21). The method
constructs a graph where a small portion of ver-
tices correspond to labeled instances, and the rest
are unlabeled. Pairs of vertices are connected by
weighted edges denoting the similarity between the
pair. Traditionally, Markov random walks (Szum-
mer and Jaakkola, 2001; Baluja et al, 2008) or op-
timization of a loss function based on smoothness
properties of the graph (Corduneanu and Jaakkola,
2003; Zhu et al, 2003; Subramanya and Bilmes,
2008, inter alia) are performed to propagate labels
from the labeled vertices to the unlabeled ones.
In this work, we are interested in multi-class gen-
eralizations of graph-propagation algorithms suit-
able for NLP applications, where each graph ver-
tex can assume one or more out of many possible
labels (Talukdar and Crammer, 2009; Subramanya
and Bilmes, 2008, 2009). For us, graph vertices cor-
respond to natural language types (not tokens) and
undirected edges between them are weighted using a
similarity metric. Recently, this setup has been used
to learn soft labels on natural language types (say,
word n-grams or syntactically disambiguated pred-
icates) from seed data, resulting in large but noisy
lexicons, which are used to constrain structured pre-
diction models. Applications have ranged from
domain adaptation of part-of-speech (POS) taggers
(Subramanya et al, 2010), unsupervised learning of
POS taggers by using bilingual graph-based projec-
tions (Das and Petrov, 2011), and shallow seman-
tic parsing for unknown predicates (Das and Smith,
2011). However, none of the above captured the em-
pirical fact that only a few categories typically asso-
ciate with a given type (vertex). Take the case of
POS tagging: Subramanya et al (2010) construct a
graph over trigram types as vertices, with 45 pos-
sible tags for the middle word of a trigram as the
677
label set for each vertex. It is empirically observed
that contextualized word types can assume very few
(most often, one) POS tags. However, along with
graph smoothness terms, they apply a penalty that
encourages distributions to be close to uniform, the
premise being that it would maximize the entropy
of the distribution for a vertex that is far away or
disconnected from a labeled vertex. To prefer maxi-
mum entropy solutions in low confidence regions of
graphs, a similar entropic penalty is applied by Sub-
ramanya and Bilmes (2008, 2009).
In this paper, we make two major algorithmic con-
tributions. First, we relax the assumption made by
most previous work (Zhu and Ghahramani, 2002;
Baluja et al, 2008; Subramanya and Bilmes, 2008;
Subramanya and Bilmes, 2009; Subramanya et al,
2010; Das and Petrov, 2011; Das and Smith, 2011)
that the `1 norm of the masses assigned to the la-
bels for a given vertex must be 1. In other words,
in our framework, the label distribution at each ver-
tex is unnormalized?the only constraint we put on
the vertices? vectors is that they must be nonnega-
tive.1 This relaxation simplifies optimization: since
only a nonnegativity constraint for each label?s mass
at each vertex needs to be imposed, we can apply a
generic quasi-Newton method (Zhu et al, 1997).
Second, we replace the penalties that prefer max-
imum entropy, used in prior work, with penalties
that aim to identify sparse unnormalized measures
at each graph vertex. We achieve this by penalizing
the graph propagation objective with the `1 norm or
the mixed `1,2 norm (Kowalski and Torre?sani, 2009)
of the measures at each vertex, aiming for global and
vertex-level sparsity, respectively. Importantly, the
proposed graph objective functions are convex, so
we avoid degenerate solutions and local minima.
We present experiments on two natural language
lexicon expansion problems in a semi-supervised
setting: (i) inducing distributions of POS tags over
n-gram types in the Wall Street Journal section of
the Penn Treebank corpus (Marcus et al, 1993)
and (ii) inducing distributions of semantic frames
(Fillmore, 1982) over predicates unseen in anno-
1Moreover, we also assume the edge weights in a given
graph are unconstrained, consistent with prior work on graph-
based SSL (Das and Petrov, 2011; Das and Smith, 2011; Subra-
manya and Bilmes, 2008; Subramanya and Bilmes, 2009; Sub-
ramanya et al, 2010; Zhu and Ghahramani, 2002).
tated data. Our methods produce sparse measures
at graph vertices resulting in compact lexicons, and
also result in better performance with respect to la-
bel propagation using Gaussian penalties (Zhu and
Ghahramani, 2002) and entropic measure propaga-
tion (Subramanya and Bilmes, 2009), two state-of-
the-art graph propagation algorithms.
2 Model
2.1 Graph-Based SSL as MAP Inference
Let Dl = {(xj , rj)}lj=1 denote l annotated data
types;2 xj?s empirical label distribution is rj . Let
the unlabeled data types be denoted by Du =
{xi}mi=l+1. Usually, l  m. Thus, the entire dataset
can be called D , Dl ? Du. Traditionally, the
graph-based SSL problem has been set up as fol-
lows. Let G = (V,E) correspond to an undirected
graph with vertices V and edges E. G is constructed
by transforming each data type xi ? D to a ver-
tex; thus V = {1, 2, . . . ,m}, and E ? V ? V .
Let Vl (Vu) denote the labeled (unlabeled) vertices.
Moreover, we assume a symmetric weight matrixW
that defines the similarity between a pair of vertices
i, k ? V . We first define a component of this ma-
trix as wij , [W]ik = sim(xi,xk). We also fix
wii = 0 and set wik = wki = 0 if k 6? N (i)
and i 6? N (k), where N (j) denotes the K-nearest
neighbors of vertex j, to reduce the density of the
graph. We next define an unnormalized measure qi
for every vertex i ? V . As mentioned before, we
have rj , a probability distribution estimated from
annotated data for a labeled vertex j ? Vl. qi and
rj are |Y |-dimensional measures, where Y is the
possible set of labels; while rj lies within the |Y |-
dimensional probability simplex,3 qi are unnormal-
ized with each component qi(y) ? 0. For most NLP
problems, rj are expected to be sparse, with very
few components active, the rest being zero.
Graph-based SSL aims at finding the best q =
{qi : 1 ? i ? m} given the empirical distribu-
tions rj , and the weight matrix W, which provides
2As explained in more detail in ?4, these types are entities
like n-grams or individual predicates, not tokens in running text.
3Note that our framework does not necessitate that rj be a
normalized probability distribution; we could have unnormal-
ized rj to allow strongly evident types appearing in more data
to have larger influence than types that appear infrequently. We
leave this extension to future work.
678
the geometry of all the vertices. We visualize this
problem using a pairwise Markov network (MN).
For every vertex (including labeled ones) i ? V , we
create a variable Xi. Additionally, for labeled ver-
tices j ? Vl, we create variables X?j . All variables in
the MN are defined to be vector-valued; specifically,
variables Xi, ?i ? V , take value qi, and variables
X?j corresponding to the labeled vertices in G are ob-
served with values rj . An example factor graph for
this MN, with only four vertices, is shown in Fig-
ure 1. In the figure, the variables indexed by 1 and 4
correspond to labeled vertices. Factor ?j with scope
{Xj , X?j} encourages qj to be close to rj . For every
edge i ? k ? E, factor ?i?k encourages similarity
between qi and qk, making use of the weight matrix
W (i.e., when wik is larger, the two measures are
more strongly encouraged to be close). These fac-
tors are white squares with solid boundaries in the
figure. Finally, we define unary factors on all vari-
ablesXi, i ? V , named ?i(Xi), that can incorporate
prior information. In Figure 1, these factors are rep-
resented by white squares with dashed boundaries.
According to the factor graph, the joint probabil-
ity for all the measures qi, ?i ? V that we want to
induce, is defined as: P (X; ?) =
1
Z
l?
j=1
?j(Xj , X?j) ?
?
i?k?E
?i?k(Xi, Xk) ?
m?
i=1
?i(Xi)
where ? is the set of all factors in the factor graph,
and Z is a partition function that normalizes the fac-
tor products for a given configuration of q. Since the
graph-based SSL problem aims at finding the best q,
we optimize lnP (X; ?); equivalently,
arg max
q s.t. q?0
l?
j=1
ln?j(Xj , X?j) +
?
i?k?E
ln?i?k(Xi, Xk)
+
m?
i=1
ln?i(Xi) (1)
The above denotes an optimization problem with
only non-negativity constraints. It equates to max-
imum a posteriori (MAP) inference; hence, the par-
tition function Z can be ignored. We next discuss
the nature of the three different factors in Eq. 1.
2.2 Log-Factors as Penalties
The nature of the three types of factors in Eq. 1
governs the behavior of a graph-based SSL algo-
rithm. Hence, the equation specifies a family of
X1
X4 X3
X2
Figure 1: An example factor graph for the graph-based
SSL problem. See text for the significance of the shaded
and dotted factors, and the shaded variables.
graph-based methods that generalize prior research.
We desire the following properties to be satisfied in
the factors: (i) convexity of Eq. 1, (ii) amenability
to scalable optimization algorithms, and (iii) sparse
solutions as expected in natural language lexicons.
Pairwise factors: In our work, for the pairwise
factors ?j(Xj , X?j) and ?i?k(Xi, Xk), we examine
two functions that penalize inconsistencies between
neighboring vertices: the squared `2 norm and the
Jensen-Shannon (JS) divergence (Burbea and Rao,
1982; Lin, 1991), which is a symmetrized gener-
alization of the Kullback-Leibler (KL) divergence
(Kullback and Leibler, 1951; Cover and Thomas,
1991). These two divergences are symmetric. Both
are inspired by previous work; however, the use of
the JS divergence is a novel extension to Subra-
manya and Bilmes (2008). Specifically, the factors
are:
ln?j(Xj , X?j) = ??(qj , rj) (2)
ln?i?k(Xi, Xk) = ?2 ? ? ? wik ? ?(qi, qk) (3)
where ? is a hyperparameter whose choice we dis-
cuss in ?4. The function ?(u, v) for two vectors u
and v is defined in two ways:
?(u, v)
Gaussian
= ?u? v?22 (4)
?(u, v)
Entropic
= 12
?
y?Y
(
u(y) ? ln 2 ? u(y)u(y) + v(y)
+ v(y) ? ln 2 ? v(y)u(y) + v(y)
)
(5)
We call the version of ?(u, v) that uses the squared
`2 distance (Eq. 4) Gaussian, as it represents the idea
of label propagation via Gaussian fields proposed by
Zhu et al (2003). A minor difference lies in the
fact that we include variables Xj , j ? Vl for labeled
679
vertices too, and allow them to change, but penal-
ize them if they go too far away from the observed
labeled distributions rj . The other ?(u, v) shown in
Eq. 5 uses the generalized JS-divergence defined in
terms of the generalized KL-divergence for unnor-
malized measures (O?Sullivan, 1998).4
Eq. 5 improves prior work by replacing the asym-
metric KL-divergence used to bring the distributions
at labeled vertices close to the corresponding ob-
served distributions, as well as replacing the KL-
based graph smoothness term with the symmetric
JS-divergence (Subramanya and Bilmes, 2008, see
first two terms in Eq. 1). Empirical evidence shows
that entropic divergences help in multiclass prob-
lems where a vertex can assume multiple labels, and
may perform better than objectives with quadratic
penalties (Subramanya and Bilmes, 2008, 2009).
A major departure from prior work is the use of
unnormalized measures in Eq. 4-5, which simplifies
optimization even with the complex JS-divergence
in the objective function (see ?3), and, we will see,
produces comparable and often better results than
baselines using normalized distributions (see ?4).
Unary factors: The unary factors in our factor
graph ?i(Xi) can incorporate prior information spe-
cific to a particular vertex xi embodied by the vari-
able Xi. Herein, we examine three straightforward
penalties, which can be thought of as penalties that
encourage either uniformity or sparsity:
Uniform squared `2: ln?i(Xi) = ?? ?
?
?
?qi ? 1|Y |
?
?
?
2
2
(6)
Sparse `1: ln?i(Xi) = ?? ? ?qi?1 (7)
Sparse `1,2: ln?i(Xi) = ?? ? ?qi?21 (8)
where ? is a hyperparameter whose choice we dis-
cuss in ?4. The penalty expressed in Eq. 6 penal-
izes qi if it is far away from the uniform distribu-
tion. This penalty has been used previously (Das and
Petrov, 2011; Das and Smith, 2011; Subramanya et
al., 2010), and is similar to the maximum entropy
penalty of Subramanya and Bilmes (2008, 2009).
The intuition behind its use is that for low confi-
dence or disconnected regions, one would prefer to
have a uniform measure on a graph vertex. The
penalties in equations 7?8, on the other hand, en-
courage sparsity in the measure qi; these are related
4The generalized KL divergence is defined asDKL(u?v) =
?
y
(
u(y) ln u(y)v(y) ? u(y) + v(y)
)
.
to regularizers for generalized linear models: the
lasso (Tibshirani, 1996) and the elitist lasso (Kowal-
ski and Torre?sani, 2009). The former encourages
global sparsity, the latter sparsity per vertex.5 For
each vertex, the `1,2 penalty takes the form:
?qi?21 =
?
?
?
y?Y
|qi(y)|
?
?
2
(9)
The `1 norm encourages its argument to be sparse,
while the usual observed effect of an `2 norm is a
dense vector without many extreme values. The `1,2
penalty is the squared `2 norm of the `1 norms of
every qi, hence it promotes sparsity within each ver-
tex, but we observe density over the vertices that are
selected.
Talukdar (2010) enforced label sparsity for infor-
mation extraction by discarding poorly scored la-
bels during graph propagation updates, but did not
use a principled mechanism to arrive at sparse mea-
sures at graph vertices. Unlike the uniform penalty
(Eq. 6), sparsity corresponds to the idea of entropy
minimization (Grandvalet and Bengio, 2004). Since
we use unnormalized measures at each variable Xi,
for low confidence graph regions or disconnected
vertices, sparse penalties will result in all zero com-
ponents in qi, which conveys that the graph prop-
agation algorithm is not confident on any potential
label, a condition that is perfectly acceptable.
Model variants: We compare six objective func-
tions: we combine factor representations from each
of Eqs. 4?5 with those from each of Eqs. 6?8, replac-
ing them in the generic graph objective function of
Eq. 1. The nature of these six models is succinctly
summarized in Table 1. For each model, we find
the best set of measures q that maximize the corre-
sponding graph objective functions, such that q ? 0.
Note that in each of the graph objectives, we have
two hyperparameters ? and ? that control the influ-
ence of the second and the third terms of Eq. 1 re-
5One could additionally consider a non-sparse penalty based
on the squared `2 norm with zero mean: ln?i(Xi) = ?? ?
?qi?
2
2. We experimented with this unary penalty (along with the
pairwise Gaussian penalty for binary factors) for the semantic
frame lexicon expansion problem, and found that it performs
exactly on par with the squared `2 penalty with uniform mean.
To limit the number of non-sparse graph objectives, we omit
detailed discussion of experiments with this unary penalty.
680
abbrev.
factors
pairwise unary
UGF-`2 Gaussian Uniform squared `2
UGF-`1 Gaussian Sparse `1
UGF-`1,2 Gaussian Sparse `1,2
UJSF-`2 Entropic Uniform squared `2
UJSF-`1 Entropic Sparse `1
UJSF-`1,2 Entropic Sparse `1,2
Table 1: Six variants of graph objective functions novel
to this work. These variants combine the pairwise factor
representations from Eqs. 4?5 with unary factor repre-
sentations from each of Eqs. 6?8 (which either encour-
age uniform or sparse measures), to be used in the graph
objective function expressed in Eq. 1.
spectively. We discuss how these hyperparameters
are chosen in ?4.
Baseline Models: We compare the performance
of the six graph objectives of Table 1 with two
strong baselines that have been used in previous
work. These two models use the following two ob-
jective functions, and find q s.t. q ? 0 and ?i ?
V,
?
y?Y qi(y) = 1. The first is a normalized Gaus-
sian field with a squared uniform `2 penalty as the
unary factor (NGF-`2):
arg min
q, s.t. q?0,
?i?V,?qi?1=1
l?
j=1
?qj ? rj?22 +
m?
i=1
?
??
?
k?N (i)
wik ?qi ? qk?22 + ?
?
?
?qi ? 1|Y |
?
?
?
2
2
?
?(10)
The second is a normalized KL field with an entropy
penalty as the unary factor (NKLF-ME):
arg min
q, s.t. q?0,
?i?V,?qi?1=1
l?
j=1
DKL(rj ? qj)+
m?
i=1
?
??
?
k?N (i)
wikDKL(qi ? qk)? ? ?H(qi)
?
?(11)
whereH(qi) denotes the Shannon entropy of the dis-
tribution qi. Both these objectives are constrained
by the fact that every qi must be within the |Y |-
dimensional probability simplex. The objective
function in 10 has been used previously (Das and
Smith, 2011; Subramanya et al, 2010) and serves as
a generalization of Zhu et al (2003). The entropic
objective function in 11, originally called measure
propagation, performed better at multiclass prob-
lems when compared to graph objectives using the
quadratic criterion (Subramanya and Bilmes, 2008).
3 Optimization
The six variants of Eq. 1 in Table 1 are convex
in q. This is because the `1, squared `2 and the
`1,2 penalties are convex. Moreover, the general-
ized JS-divergence term, which is a sum of two KL-
divergence terms, is convex (Cover and Thomas,
1991). Since we choose ?, ? and wik to be non-
negative, these terms? sums are also convex. The
graph objectives of the two baselines noted in ex-
pressions 10?11 are also convex because negative
entropy in expression 11 is convex, and rest of the
penalties are the same as our six objectives. In our
work, to optimize the objectives of Table 1, we use a
generic quasi-Newton gradient-based optimizer that
can handle bound-inequality constraints, called L-
BFGS-B (Zhu et al, 1997). Partial derivatives of
the graph objectives are computed with respect to
each parameter ?i, y, qi(y) of q and passed on to
the optimizer which updates them such that the ob-
jective function of Eq. 1 is maximized. Note that
since the `1 and `1,2 penalties are non-differentiable
at 0, special techniques are usually used to compute
updates for unconstrained parameters (Andrew and
Gao, 2007). However, since q ? 0, their absolute
value can be assumed to be right-continuous, mak-
ing the function differentiable. Thus,
?
?qi(y)
?qi?1 = 1
?
?qi(y)
?qi?21 = 2 ? ?qi?1
(We omit the form of the derivatives of the other
penalties for space.) There are several advantages to
taking this route towards optimization. The `2 and
the JS-divergence penalties for the pairwise terms
can be replaced with more interesting convex di-
vergences if required, and still optimization will be
straightforward. Moreover, the nonnegative con-
straints make optimization with sparsity inducing
penalties easy. Finally, computing the objective
function and the partial derivatives is easily paral-
lelizable on MPI (Gropp et al, 1994) or MapReduce
(Dean and Ghemawat, 2008) architectures, by divid-
ing up the computation across graph vertices.
In comparison, constrained problems such as the
one in Eq. 11 require a specialized alternating mini-
681
mization technique (Subramanya and Bilmes, 2008,
2009), that performs two passes through the graph
vertices during one iteration of updates, introduces
an auxiliary set of probability distributions (thus, in-
creasing memory requirements) and another hyper-
parameter ? that is used to transform the weight
matrix W to be suitable for the alternating mini-
mization procedure. To optimize the baseline ob-
jectives, we borrow the gradient-free iterative up-
dates described by Subramanya and Bilmes (2009)
and Subramanya et al (2010).
4 Experiments
In this section, we compare the six graph objective
functions in Table 1 with the two baseline objectives
on two lexicon expansion tasks.
4.1 POS Lexicon Expansion
We expand a POS lexicon for word types with a con-
text word on each side, using distributional similar-
ity in an unlabeled corpus and few labeled trigrams.
Data and task: We constructed a graph over word
trigram types as vertices, using co-occurrence statis-
tics. Following Das and Petrov (2011) and Sub-
ramanya et al (2010), a similarity score between
two trigram types was computed by measuring the
cosine similarity between their empirical senten-
tial context statistics. This similarity score resulted
in the symmetric weight matrix W, defining edge
weights between pairs of graph vertices. Details
of the similarity computation are given in those pa-
pers. W is thresholded so that only the K near-
est neighbors for each vertex have similarity greater
than zero, giving a sparse graph. We set K = 8 as it
resulted in the sparsest graph which was fully con-
nected.6 For this task, Y is the set of 45 POS tags
defined in the Penn Treebank (Marcus et al, 1993),
and the measure qi for vertex i (for trigram type xi)
corresponds to the set of tags that can be associated
with the middle word of xi. The trigram represen-
tation, as in earlier work, helps reduce the ambi-
guity of POS tags for the middle word, and helps
in graph construction. The 690,705-vertex graph
was constructed over all trigram types appearing in
6Our proposed methods can deal with graphs containing dis-
connected components perfectly well. Runtime is asymptoti-
cally linear in K for all objectives considered here.
Sections 00?21 (union of the training and develop-
ment sets used for POS tagging experiments in prior
work) of the WSJ section of the Penn Treebank, but
co-occurrence statistics for graph construction were
gathered from a million sentences drawn from the
English Gigaword corpus (Graff, 2003).
Given the graph G with m vertices, we assume
that the tag distributions r for l labeled vertices are
also provided. Our goal is to find the best set of
measures q over the 45 tags for all vertices in the
graph. Prior work used a similar lexicon for POS
domain adaptation and POS induction for resource-
poor languages (Das and Petrov, 2011; Subramanya
et al, 2010); such applications of a POS lexicon are
out of scope here; we consider only the lexicon ex-
pansion problem and do an intrinsic evaluation at a
type-level to compare the different graph objectives.
Experimental details: To evaluate, we randomly
chose 6,000 out of the 690,705 types for devel-
opment. From the remaining types, we randomly
chose 588,705 vertices for testing. This left us with
96,000 types from which we created sets of differ-
ent sizes containing 3,000, 6,000, 12,000, 24,000,
48,000 and 96,000 labeled types, creating 6 increas-
ingly easy transduction settings. The development
and the test types were kept constant for direct per-
formance comparison across the six settings and our
eight models. After running inference, the mea-
sure qi at vertex i was normalized to 1. Next, for
all thresholds ranging from 0 to 1, with steps of
0.001, we measured the average POS tag precision
and recall on the development data ? this gave us
the area under the precision-recall curve (prAUC),
which is often used to measure performance on re-
trieval tasks. Given a transduction setting and the
final q for an objective, hyperparameters ? and ?
were tuned on the development set by performing a
grid search, targeting prAUC.7 We ran 100 rounds
7For the objectives using the uniform `2 and the maxi-
mum entropy penalties, namely UGF-`2, UJSF-`2, NGF-`2
and NKLF-ME, we chose ? from {0, 10?6, 10?4, 0.1}. For the
rest of the models using sparsity inducing penalties, we chose
? from {10?6, 10?4, 0.1}. This suggests that for the former
type of objectives, we allowed a zero unary penalty if that set-
ting resulted in the best development performance, while for the
latter type of models, we enforced a positive unary penalty. In
fact, ? = 0 was chosen in several cases for the objectives with
uniform penalties indicating that uniformity hurts performance.
We chose ? from {0.1, 0.5, 1.0}.
682
|Dl|: 3K 6K 12K 24K 48K 96K
NGF-`2 0.208 0.219 0.272 0.335 0.430 0.544
NKLF-ME 0.223 0.227 0.276 0.338 0.411 0.506
UGF-`2 0.223 0.257 0.314 0.406 0.483 0.564
UGF-`1 0.223 0.257 0.309 0.406 0.483 0.556
UGF-`1,2 0.223 0.256 0.313 0.403 0.478 0.557
UJSF-`2 0.271 0.250 0.310 0.364 0.409 0.481
UJSF-`1 0.227 0.257 0.317 0.369 0.410 0.481
UJSF-`1,2 0.227 0.258 0.309 0.369 0.409 0.479
Table 2: Area under the precision recall curve for the two
baseline objectives and our methods for POS tag lexicon
induction. This is a measure of how well the type lexicon
(for some types unlabeled during training) is recovered
by each method. The test set contains 588,705 types.
of iterative updates for all 8 graph objectives.
Type-level evaluation: To measure the quality of
the lexicons, we perform type level evaluation us-
ing area under the precision-recall curve (prAUC).
The same measure (on development data) was used
to tune the two hyperparameters. Table 2 shows the
results measured on 588,705 test vertices (the same
test set was used for all the transduction settings).
The general pattern we observe is that our unnor-
malized approaches almost always perform better
than the normalized baselines. (The exception is
the 3,000 labeled example case, where most unnor-
malized models are on par with the better baseline.)
In scenarios with fewer labeled types, pairwise en-
tropic penalties perform better than Gaussian ones,
and the pattern reverses as more labeled types come
available. This trend is the same when we compare
only the two baselines. In four out of the six trans-
duction settings, one of the sparsity-inducing graph
objectives achieves the best performance in terms of
prAUC, which is encouraging given that they gener-
ally produce smaller models than the baselines.
Overall, though, using sparsity-inducing unary
factors seems to have a weak negative effect on per-
formance. Their practical advantage, however is ap-
parent when we consider the size of the model. Af-
ter the induction of the set of measures q for all
transduction settings and all graph objectives, we
noticed that our numerical optimizer (LBFGS-B) of-
ten assigns extremely small positive values rather
than zero. This problem can be attributed to sev-
eral artifacts, including our limit of 100 iterations of
optimization. Hence, we use a global threshold of
10?6, and treat any real value below this threshold
0M
5M
11M
16M
21M
27M
32M
3k 6k 12k 24k 48k 96k
                           
                              
UGF-?1 UGF-?1,2 UJSF-?2
UJSF-?1 UJSF-?1,2
Figure 2: The number of non-zero components in q for
five graph objective functions proposed in this work, plot-
ted against various numbers of labeled datapoints. Note
that NGF-`2, NKLF-ME and UGF-`2 produce non-zero
components for virtually all q, and are therefore not
shown (the dotted line marks the maximally non-sparse
solution, with 31,081,725 components). All of these five
objectives result in sparsity. On average, the objectives
employing entropic pairwise penalties with sparse unary
penalties UJSF-`1 and UJSF-`1,2 produce very sparse
lexicons. Although UGF-`2 produces no sparsity at all,
its entropic counterpart UJSF-`2 produces considerable
sparsity, which we attribute to JS-divergence as a pair-
wise penalty.
to be zero. Figure 2 shows the number of non-zero
components in q (or, the lexicon size) for the graph
objectives that achieve sparsity (baselines NGF-`2
and NKLF-ME, plus our UGF-`2 are not expected
to, and do not, achieve sparsity; surprisingly UJSF-
`2 does and is shown). Even though the hyperpa-
rameters ? and ? in the graph objective functions
were not tuned towards sparsity, we see that sparsity-
inducing factors are able to achieve far more com-
pact lexicons. Sparsity is desirable in settings where
labeled development data for tuning thresholds that
select the most probable labels for a given type is
unavailable (e.g., Das and Petrov, 2011).
4.2 Expansion of a Semantic Frame Lexicon
In a second set of experiments, we follow Das and
Smith (2011, D&S11 henceforth) in expanding a
lexicon that associates lexical predicates (targets)
with semantic frames (abstract events or scenarios
that a predicate evokes when used in a sentential
context) as labels. More concretely, each vertex in
the graph corresponds to a lemmatized word type
with its coarse part of speech, and the labels are
frames from the FrameNet lexicon (Fillmore et al,
2003). Graph construction leverages distributional
683
UNKNOWN ALL
PREDICATES PREDICATES lexicon
exact partial exact partial size
Supervised 23.08 46.62 82.97 90.51 -
?NGF-`2 39.86 62.35 83.51 91.02 128,960
NKLF-ME 36.36 60.07 83.40 90.95 128,960
UGF-`2 37.76 60.81 83.44 90.97 128,960
UGF-`1 39.86 62.85 83.51 91.04 122,799
UGF-`1,2 39.86 62.85 83.51 91.04 128,732
UJSF-`2 40.56 62.81 83.53 91.04 128,232
UJSF-`1 39.16 62.43 83.49 91.02 128,771
UJSF-`1,2 42.67 65.29 83.60 91.12 45,544
Table 3: Exact and partial frame identification accuracy
with lexicon size (non-zero frame components). The ?un-
known predicates? section of the test data contains 144
targets, while the entire test set contains 4,458 targets.
Bold indicates best results. The UJSF-`1,2 model pro-
duces statistically significant results (p < 0.001) for all
metrics with respect to the supervised baseline used in
D&S11. For both the unknown targets as well as the
whole test set. However, it is weakly significant (p < 0.1)
compared to the NGF-`2 model for the unseen portion of
the test set, when partial frame matching is used. For rest
of the settings, the two are statistically indistinguishable.
? indicates the best results in D&S11.
similarity as well as linguistic annotations.
Data: We borrow the graph-based SSL process of
D&S11 in its entirety. The constructed graph con-
tains 64,480 vertices, each corresponding to a tar-
get, out of which 9,263 were drawn from the labeled
data. The possible set of labels Y is the set of 877
frames defined in FrameNet; the measure qi corre-
sponds to the set of frames that a target can evoke.
The targets drawn from FrameNet annotated data
(l = 9,263) have frame distributions ri with which
the graph objectives are seeded.8
Evaluation: The evaluation metric used for this
task is frame disambiguation accuracy on a blind test
set containing marked targets in free text. A section
of this test set contained 144 targets, previously un-
seen in annotated FrameNet data; this section is of
interest to us and we present separate accuracy re-
sults on it. Given the measure qi over frames in-
duced using graph-based SSL for target i, we trun-
cate it to keep at most the top M frames that get
the highest mass under qi, only retaining those with
non-zero values. If all components of qi are zero,
we remove target i from the lexicon, which is of-
ten the case in the sparsity-inducing graph objec-
tives. If a target is unseen in annotated data, a sep-
arate probabilistic model (which serves as a super-
vised baseline like in D&S11, row 1 in Table 3) dis-
ambiguates among the M filtered frames observing
the sentential context of the target instance. This
can be thought of as combining type- and token-
level information for inference. If the target was
previously seen, it is disambiguated using the su-
8We refer the reader to D&S11 for the details of the graph
construction method, the FrameNet dataset used, example se-
mantic frames, and an excerpt of the graph over targets.
pervised baseline. The test set and the probabilis-
tic model are identical to the ones in D&S11. We
fixed K, the number of nearest neighbors for each
vertex, to be 10. For each graph objective, ?, ? and
M were chosen by five-fold cross-validation. The
cross-validation sets were the same as the ones de-
scribed in ?6.3 of D&S11.9
Results and discussion: Table 3 shows frame iden-
tification accuracy, both using exact match as well
as partial match that assigns partial credit when a re-
lated frame is predicted (Baker et al, 2007). The
final column presents lexicon size in terms of the set
of truncated frame distributions (filtered according
to the top M frames in qi) for all the targets in a
graph. All the graph-based models are better than
the supervised baseline; for our objectives using
pairwise Gaussian fields with sparse unary penal-
ties, the accuracies are equal or better with respect
to NGF-`2; however, the lexicon sizes are reduced
by a few hundred to a few thousand entries. Massive
reduction in lexicon sizes (as in the POS problem in
?4.1) is not visible for these objectives because we
throw out most of the components of the entire set
of distributions q and keep only at most the top M
(which is automatically chosen to be 2 for all ob-
jectives) frames per target. Although a significant
number of components in the whole distribution q
in the sparse objectives get zero mass, the M com-
ponents for a target tend to be non-zero for a major-
ity of the targets. Better results are observed for the
objectives using entropic pairwise penalties; the ob-
9We chose ? from {0.01, 0.1, 0.3, 0.5, 1.0}; ? was chosen
from the same sets as the POS problem. The graph construction
hyperparameter ? described by D&S11 was fixed to 0.2. As in
D&S11, M was chosen from {2, 3, 5, 10}.
684
(a)
t = discrepancy.N t = contribution.N t = print.V t = mislead.V
?SIMILARITY ?GIVING ?TEXT CREATION EXPERIENCER OBJ
NATURAL FEATURES MONEY SENDING ?PREVARICATION
PREVARICATION COMMITMENT DISPERSAL MANIPULATE INTO DOING
QUARRELING ASSISTANCE READING COMPLIANCE
DUPLICATION EARNINGS AND LOSSES STATEMENT EVIDENCE
t = abused.A t = maker.N t = inspire.V t = failed.A
OFFENSES COMMERCE SCENARIO CAUSE TO START SUCCESS OR FAILURE
KILLING ?MANUFACTURING EXPERIENCER OBJ ?SUCCESSFUL ACTION
COMPLIANCE BUSINESSES ?SUBJECTIVE INFLUENCE UNATTRIBUTED INFORMATION
DIFFERENTIATION BEHIND THE SCENES EVOKING PIRACY
COMMITTING CRIME SUPPLY ATTEMPT SUASION WANT SUSPECT
(b)
t = discrepancy.N t = contribution.N t = print.V t = mislead.V
?SIMILARITY ?GIVING ?TEXT CREATION ?PREVARICATION
NON-COMMUTATIVE STATEMENT COMMERCE PAY STATE OF ENTITY EXPERIENCER OBJ
NATURAL FEATURES COMMITMENT DISPERSAL MANIPULATE INTO DOING
ASSISTANCE CONTACTING REASSURING
EARNINGS AND LOSSES READING EVIDENCE
t = abused.A t = maker.N t = inspire.V t = failed.A
?MANUFACTURING CAUSE TO START ?SUCCESSFUL ACTION
BUSINESSES ?SUBJECTIVE INFLUENCE SUCCESSFULLY COMMUNICATE MESSAGE
COMMERCE SCENARIO OBJECTIVE INFLUENCE
SUPPLY EXPERIENCER OBJ
BEING ACTIVE SETTING FIRE
Table 4: Top 5 frames (if there are ? 5 frames with mass greater than zero) according to the graph posterior qt(f)
for (a) NGF-`2 and (b) UJSF-`1,2, given eight unseen predicates in annotated FrameNet data. ? marks the correct
frame, according to the predicate instances in test data (each of these predicates appear only once in test data). Note
that UJSF-`1,2 ranks the correct frame higher than NGF-`2 for several predicates, and produces sparsity quite often;
for the predicate abused.A, the correct frame is not listed by NGF-`2, while UJSF-`1,2 removes it altogether from the
expanded lexicon, resulting in compactness.
jective UJSF-`1,2 gives us the best absolute result by
outperforming the baselines by strong margins, and
also resulting in a tiny lexicon, less than half the size
of the baseline lexicons. The size can be attributed to
the removal of predicates for which all frame com-
ponents were zero (qi = 0). Table 4 contrasts the
induced frames for several unseen predicates for the
NGF-`2 and the UJSF-`2 objectives; the latter often
ranks the correct frame higher, and produces a small
set of frames per predicate.
5 Conclusion
We have presented a family of graph-based SSL ob-
jective functions that incorporate penalties encour-
aging sparse measures at each graph vertex. Our
methods relax the oft-used assumption that the mea-
sures at each vertex form a normalized probabil-
ity distribution, making optimization and the use of
complex penalties easier than prior work. Optimiza-
tion is also easy when there are additional terms in
a graph objective suited to a specific problem; our
generic optimizer would simply require the compu-
tation of new partial derivatives, unlike prior work
that required specialized techniques for a novel ob-
jective function. Finally, experiments on two natural
language lexicon learning problems show that our
methods produce better performance with respect to
state-of-the-art graph-based SSL methods, and also
result in much smaller lexicons.
Acknowledgments
We thank Andre? Martins, Amar Subramanya, and Partha
Talukdar for helpful discussion during the progress of this
work and the three anonymous reviewers for their valu-
able feedback. This research was supported by Qatar Na-
tional Research Foundation grant NPRP 08-485-1-083,
Google?s support of the Worldly Knowledge Project, and
TeraGrid resources provided by the Pittsburgh Supercom-
puting Center under NSF grant number TG-DBS110003.
685
References
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
C. Baker, M. Ellsworth, and K. Erk. 2007. Task 19:
frame semantic structure extraction. In Proc. of Se-
mEval.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for Youtube: taking random
walks through the view graph. In Proc. of WWW.
Y. Bengio, O. Delalleau, and N. Le Roux. 2006. La-
bel propagation and quadratic criterion. In Olivier
Chapelle, Bernhard Scho?lkopf, and Alexander Zien,
editors, Semi-Supervised Learning, pages 193?216.
MIT Press.
J. Burbea and C. R. Rao. 1982. On the convexity of
some divergence measures based on entropy functions.
IEEE Transactions on Information Theory, 28:489?
495.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press.
A. Corduneanu and T. Jaakkola. 2003. On information
regularization. In Proc. of UAI.
T. M. Cover and J. A. Thomas. 1991. Elements of infor-
mation theory. Wiley-Interscience.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proc. of ACL.
D. Das and N. A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Proc. of
ACL.
J. Dean and S. Ghemawat. 2008. MapReduce: simplified
data processing on large clusters. Communications of
the ACM, 51:107?113, January.
P. S. Dhillon, D. Foster, and L. Ungar. 2011. Multi-
view learning of word embeddings via cca. In Proc. of
NIPS.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium.
Y. Grandvalet and Y. Bengio. 2004. Semi-supervised
learning by entropy minimization. In Proc. of NIPS.
W. Gropp, E. Lusk, and A. Skjellum. 1994. Using MPI:
Portable Parallel Programming with the Message-
Passing Interface. MIT Press.
F. Huang and A. Yates. 2009. Distributional representa-
tions for handling sparsity in supervised sequence la-
beling. In Proc. of ACL.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schu-
urmans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In Proc. of ACL.
M. Kowalski and B. Torre?sani. 2009. Sparsity and per-
sistence: mixed norms provide simple signal models
with dependent coefficients. Signal, Image and Video
Processing, 3:251?264.
S. Kullback and R. A. Leibler. 1951. On information and
sufficiency. Annals of Mathematical Statistics, 22.
J. Lin. 1991. Divergence measures based on the shan-
non entropy. IEEE Transactions on Information the-
ory, 37:145?151.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proc. of HLT-
NAACL.
J. A. O?Sullivan. 1998. Alternating minimization
algorithms: from Blahut-Arimoto to Expectation-
Maximization. In A. Vardy, editor, Codes, Curves,
and Signals: Common Threads in Communications,
pages 173?192. Kluwer.
D. A. Smith and J. Eisner. 2007. Bootstrapping feature-
rich dependency parsers with entropic priors. In Proc.
of EMNLP.
A. Subramanya and J. Bilmes. 2008. Soft-supervised
learning for text classification. In Proc. of EMNLP.
A. Subramanya and J. Bilmes. 2009. Entropic graph reg-
ularization in non-parametric semi-supervised classifi-
cation. In Proc. of NIPS.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Efficient
Graph-based Semi-Supervised Learning of Structured
Tagging Models. In Proc. of EMNLP.
M. Szummer and T. Jaakkola. 2001. Partially labeled
classification with Markov random walks. In Proc. of
NIPS. MIT Press.
P. P. Talukdar and K. Crammer. 2009. New regularized
algorithms for transductive learning. In Proc. of the
ECML-PKDD.
P. P. Talukdar. 2010. Graph-Based Weakly-Supervised
Methods for Information Extraction and Integration.
Ph.D. thesis, University of Pennsylvania.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society
(Series B), 58:267?288.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proc. of ACL.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with Label Propagation. Technical
report, Carnegie Mellon University.
686
C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. 1997. Algo-
rithm 778: L-BFGS-B: Fortran subroutines for large-
scale bound-constrained optimization. ACM Transac-
tions on Mathematical Software, 23:550?560.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of ICML.
X. Zhu. 2008. Semi-Supervised Learning Literature Sur-
vey. Online publication., July.
687
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 793?802,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Textual Predictors of Bill Survival in Congressional Committees
Tae Yano Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{taey,nasmith}@cs.cmu.edu
John D. Wilkerson
Department of Political Science
University of Washington
Seattle, WA 98195, USA
jwilker@u.washington.edu
Abstract
A U.S. Congressional bill is a textual artifact
that must pass through a series of hurdles to
become a law. In this paper, we focus on one
of the most precarious and least understood
stages in a bill?s life: its consideration, behind
closed doors, by a Congressional committee.
We construct predictive models of whether a
bill will survive committee, starting with a
strong, novel baseline that uses features of the
bill?s sponsor and the committee it is referred
to. We augment the model with information
from the contents of bills, comparing different
hypotheses about how a committee decides a
bill?s fate. These models give significant re-
ductions in prediction error and highlight the
importance of bill substance in explanations of
policy-making and agenda-setting.
1 Introduction
In representative governments, laws result from a
complex social process. Central to that process is
language. Text data emerging from the process in-
clude debates among legislators (Laver et al, 2003;
Quinn et al, 2010; Beigman Klebanov et al, 2008),
press releases (Grimmer, 2010), accounts of these
debates in the press, policy proposals, and laws.
In the work reported here, we seek to exploit text
data?specifically, the text of Congressional bills?
to understand the lawmaking process. We consider
an especially murky part of that process that is dif-
ficult to study because it happens largely behind
closed doors: the handling of bills by Congressional
committees. This early stage of a bill?s life is precar-
ious: roughly 85% of bills do not survive commit-
tee. By contrast, nearly 90% of bills that are recom-
mended by a committee (i.e., survive the committee
and are introduced for debate on the floor) will sur-
vive a roll call vote by the legislature. Because fil-
tering by these powerful Congressional committees
is both more opaque and more selective than the ac-
tions of the legislature as a whole, we believe that
text-based models can play a central role in under-
standing this stage of lawmaking.
This paper?s contributions are: (i) We formu-
late computationally the prediction of which bills
will a survive Congressional committee, presenting
a (baseline) model based on observable features as-
sociated with a bill, the committee(s) it is assigned
to, members of that committee, the Congress as a
whole, and expert combinations of those features.
The task formulation and baseline model are novel.
(ii) We propose several extensions of that strong
baseline with information derived from the text of
a bill. (iii) We validate our models on a hard predic-
tive task: predicting which bills will survive com-
mittee. Text is shown to be highly beneficial. (iv)
We present a discussion of the predictive features se-
lected by our model and what they suggest about the
underlying political process. (v) We release our cor-
pus of over 50,000 bills and associated metadata to
the research community for further study.1
We give brief background on how bills become
U.S. laws in ?2. We describe our data in ?3. The
modeling framework and baseline are then intro-
duced (?4), followed by our text-based models with
experiments (?5), then further discussion (?6).
1http://www.ark.cs.cmu.edu/bills
793
2 How Bills Become Laws
In the U.S., federal laws are passed by the
U.S. Congress, which consists of two ?chambers,?
the House of Representatives (commonly called the
?House?) and the Senate. To become law, a bill (i.e.,
a proposed law) must pass a vote in both chambers
and then be signed by the U.S. President. If the Pres-
ident refuses to sign a bill (called a ?veto?), it may
still become law if both chambers of Congress over-
rides the veto through a two-thirds majority.
Much less discussed is the process by which bills
come into existence. A bill is formally proposed
by a member of Congress, known as its sponsor.
Once proposed, it is routed to one or more (usu-
ally just one) of about twenty subject-specializing
committees in each chamber. Unlike floor proceed-
ings, transcripts of the proceedings of Congressional
committees are published at the discretion of the
committee and are usually publicly unavailable.
Each committee has a chairman (a member of the
majority party in the chamber) and is further divided
into subcommittees. Collectively a few thousand
bills per year are referred to Congress? committees
for consideration. Committees then recommend (re-
port) only about 15% for consideration and voting
by the full chamber.
The U.S. House is larger (435 voting members
compared to 100 in the Senate) and, in recent his-
tory, understood to be more polarized than the Sen-
ate (McCarty et al, 2006). All of its seats are up
for election every two years. A ?Congress? often
refers to a two-year instantiation of the body with a
particular set of legislators (e.g., the 112th Congress
convened on January 3, 2011 and adjourns on Jan-
uary 3, 2013). In this paper, we limit our attention
to bills referred to committees in the House.
3 Data
We have collected the text of all bills introduced in
the U.S. House of Representatives from the 103rd
to the 111th Congresses (1/3/1993?1/3/2011). Here
we consider only the version of the bill as originally
introduced. After introduction, a bill?s title and con-
tents can change significantly, which we ignore here.
These bills were downloaded directly from the
Library of Congress?s Thomas website.2 Informa-
2http://thomas.loc.gov/home/thomas.php
Cong. Maj.
Total Survival Rate (%)
Introduced Total Rep. Dem.
103 Dem. 5,311 11.7 3.4 16.2
104 Rep. 4,345 13.7 19.7 6.1
105 Rep. 4,875 13.2 19.0 5.4
106 Rep. 5,682 15.1 20.9 7.0
107 Rep. 5,768 12.1 17.5 5.8
108 Rep. 5,432 14.0 21.0 5.9
109 Rep. 6,437 11.8 16.9 5.1
110 Dem. 7,341 14.5 8.5 18.0
111 Dem. 6,571 12.6 8.1 14.5
Total 51,762 13.2 15.9 10.7
Table 1: Count of introduced bills per Congress, along
with survival rate, and breakdown by the bill sponsor?s
party affiliation. Note that the probability of survival in-
creases by a factor of 2?5 when the sponsor is in the ma-
jority party. Horizontal lines delineate presidential ad-
ministrations (Clinton, Bush, and Obama).
tion about the makeup of House committees was
obtained from Charles Stewart?s resources at MIT,3
while additional sponsor and bill information (e.g.,
sponsor party affiliation and bill topic) was obtained
from E. Scott Adler and John Wilkerson?s Congres-
sional Bills Project at the University of Washing-
ton.4
In our corpus, each bill is associated with its title,
text, committee referral(s), and a binary value indi-
cating whether or not the committee reported the bill
to the chamber. We also extracted metadata, such as
sponsor?s name, from each bill?s summary page pro-
vided by the Library of Congress.
There were a total of 51,762 bills in the House
during this seventeen-year period, of which 6,828
survived committee and progressed further. See Ta-
ble 1 for the breakdown by Congress and party.
In this paper, we will consider a primary train-test
split of the bills by Congress, with the 103rd?110th
Congresses serving as the training dataset and the
111th as the test dataset. This allows us to simulate
the task of ?forecasting? which bills will survive in a
future Congress. In ?5.5, we will show that a similar
result is obtained on different data splits.
These data are, in principle, ?freely available?
to the public, but they are not accessible in a uni-
3http://web.mit.edu/17.251/www/data_
page.html
4http://congressionalbills.org
794
fied, structured form. Considerable effort must
be expended to align databases from a variety of
sources, and significant domain knowledge about
the structure of Congress and its operation is re-
quired to disambiguate the data. Further exploration
of the deeper relationships among the legislators,
their roles in past Congresses, their standing with
their constituencies, their political campaigns, and
so on, will require ongoing effort in joining data
from disparate sources.
When we consider a larger goal of understanding
legislative behavior across many legislative bodies
(e.g., states in the U.S., other nations, or interna-
tional bodies), the challenge of creating and main-
taining such reliable, clean, and complete databases
seems insurmountable.
We view text content?noisy and complex as it
is?as an attractive alternative, or at least a comple-
mentary information source. Though unstructured,
text is made up of features that are relatively easy
for humans to interpret, offering a way to not only
predict, but also explain legislative outcomes.
4 A Predictive Model
We next consider a modeling framework for predict-
ing bill survival or death in committee. We briefly
review logistic regression models (section 4.1), then
turn to the non-textual features that form a baseline
and a starting point for the use of text (section 4.2).
4.1 Modeling Framework
Our approach to predicting a bill?s survival is logis-
tic regression. Specifically, let X be a random vari-
able associated with a bill, and let f be a feature vec-
tor function that encodes observable features of the
bill. Let Y be a binary random variable correspond-
ing to bill survival (Y = 1) or death (Y = 0). Let:
pw(Y = 1 | X = x) =
expw>f(x)
1 + expw>f(x)
(1)
where w are ?weight? parameters associating each
feature in the feature vector f(x) with each outcome.
This leads to the predictive rule:
y?(x) =
{
1 if w>f(x) > 0
0 otherwise
(2)
We train the model by maximizing log-likelihood
plus a a sparsity-inducing log-prior that encourages
many weights to go to zero:
maxw
?
i log pw(yi | xi)? ??w?1 (3)
where i indexes training examples (specifically, each
training instance is a bill referred to a single com-
mittee). The second term is an `1 norm, equivalent
to a Laplacian prior on the weights. The value of
?, which controls sparsity, is chosen on a held-out
subset of the training data.
Linear models like this one, commonly called
?exponential? or ?max ent? models, are attractive
because they are intelligible. The magnitude of a
weight indicates a feature?s importance in the pre-
diction, and its sign indicates the direction of the ef-
fect.
We note that the `1 regularizer is not ideal for
identifying predictive features. When two features
are strongly correlated, it tends to choose one of
them to include in the model and eliminate the other,
despite the fact that they are both predictive. It is
therefore important to remember that a weight of
zero does not imply that the corresponding feature
is unimportant. We chose to cope with this poten-
tial elimination of good features so that our models
would be compact and easily interpretable.
4.2 Features
In American politics, the survival or death of many
bills can be explained in terms of expertise, en-
trepreneurship, and procedural control, which are
manifest in committee membership, sponsor at-
tributes, and majority party affiliation. We there-
fore begin with a strong baseline that includes fea-
tures encoding many expected effects on bill suc-
cess. These include basic structural features and
some interactions.
The basic features are all binary. The value of
the random variable X includes information about
the bill, its sponsor, and the committee to which the
bill is referred. In addition to a bias feature (always
equal to 1), we include the following features:
1. For each party p, is the bill?s sponsor affiliated with
p?
2. Is the bill?s sponsor in the same party as the com-
mittee chair? Equivalently, is the bill?s sponsor in
the majority party of the House?
3. Is the bill?s sponsor a member of the committee?
795
4. Is the bill?s sponsor a majority member of the com-
mittee? (This feature conjoins 2 and 3.)
5. Is the bill?s sponsor the chairman of the committee?
6. For each House member j, did j sponsor the bill?
7. For each House member j, is the bill sponsored by j
and referred to a committee he chairs? (This feature
conjoins 5 and 6.)
8. For each House member j, is the bill sponsored by
j and is j in the same party as the committee chair?
(This feature conjoins 2 and 6.)
9. For each state s, is the bill?s sponsor from s?
10. For each month m, is the bill introduced during m?
11. For v ? {1, 2}, is the bill introduced during the vth
year of the (two-year) Congress?
The features above were engineered in prelimi-
nary model development, before text was incorpo-
rated.5
4.3 Experiment
Performance. Considering the 111th Congress as a
test set (6,571 instances), a most-frequent-class pre-
dictor (i.e., a constant prediction that no bill will
survive committee) achieves an error rate of 12.6%
(more details in Table 3). A model trained on
the 103rd?110th Congresses (45,191 bills) contains
3,731 instantiated features above achieved 11.8% er-
ror (again, see Table 3).
Discussion. When inspecting linear models, consid-
ering feature weights can be misleading, since (even
with regularization) large weights often correspond
to small effects in the training data. Our method-
ology for inspecting models is therefore as follows:
we calculate the impact of each feature on the final
decision for class y, defined for feature j as
wj
N
?N
i=1 fj(xi) (4)
where i indexes test examples (of which there are
N ). Impact is the average effect of a feature on the
model?s score for class y. Note that it is not affected
5One surprisingly detrimental feature, omitted here, was
the identity of the committee. Bill success rates vary greatly
across committees (e.g., Appropriations recommends about half
of bills, while Ways and Means only 7%). We suspect that
this feature simply has poor generalization ability across Con-
gresses. (In ?5.2 we will consider preferences of individuals on
committees, based on text, which appears to benefit predictive
performance.)
Bill Survival
sponsor is in the majority party (2) 0.525
sponsor is in the majority party and on the
committee (4)
0.233
sponsor is a Democrat (1) 0.135
sponsor is on the committee (3) 0.108
bill introduced in year 1 (11) 0.098
sponsor is the referred committee?s chair (5) 0.073
sponsor is a Republican (1) 0.069
Bill Death
bill?s sponsor is from NY (9) -0.036
sponsor is Ron Paul (Rep., TX) (6) -0.023
bill introduced in December (10) -0.018
sponsor is Bob Filner (Dem., CA) (6) -0.013
Table 2: Baseline model: high-impact features associated
with each outcome and their impact scores (eq. 4).
by the true label for an example. Impact is addi-
tive, which allows us to measure and compare the
influence of sets of features within a model on model
predictions. Impact is not, however, directly compa-
rable across models.
The highest impact features are shown in Table 2.
Unsurprisingly, the model?s predictions are strongly
influenced (toward survival) when a bill is sponsored
by someone who is on the committee and/or in the
majority party. Feature 2, the sponsor being on the
committee, accounted for nearly 27% of all (abso-
lute) impact, followed by the member-specific fea-
tures (6?8, 19%), the sponsor being in the majority
and on the committee (4, 12%), and the party of the
sponsor (1, 10%).
We note that impact as a tool for interpreting mod-
els has some drawbacks. If a large portion of bills
in the test set happen to have a particular feature,
that feature may have a high impact score for the
dominant class (death). This probably explains the
high impact of ?sponsor is a Democrat? (Table 2);
Democrats led the 111th Congress, and introduced
more bills, most of which died.
5 Adding Text
We turn next to the use of text data to augment the
predictive power of our baseline model. We will
propose three ways of using the title and/or text of
a bill to create features. From a computational per-
spective, each approach merely augments the base-
line model with features that may reduce predictive
796
errors?our measure of the success of the hypothe-
sis. From a political science perspective, each pro-
posal corresponds to a different explanation of how
committees come to decisions.
5.1 Functional Bill Categories
An important insight from political science is that
bills can be categorized in general ways that are re-
lated to their likelihood of success. In their study on
legislative success, Adler and Wilkerson (2005) dis-
tinguish Congressional bills into several categories
that capture bills that are on the extremes in terms
of the importance and/or urgency of the issue ad-
dressed. We expect to find that distinguishing bills
by their substance will reduce prediction errors.
? bills addressing trivial issues, such as those nam-
ing a federal building or facility or coining com-
memorative medals;
? bills that make technical changes to existing laws,
usually at the request of the executive agency re-
sponsible for its implementation;
? bills addressing recurring issues, such as annual
appropriations or more sporadic reauthorizations
of expiring federal programs or laws; and
? bills addressing important, urgent issues, such as
bills introduced in response to the 9/11 terrorist
attacks or a sharp spike in oil prices.
Adler and Wilkerson (2005) annotated House bills
for the 101st?105th Congresses using the above cat-
egories (all other bills were deemed to be ?discre-
tionary?). Out of this set we use the portion that
overlaps with our bill collection (103rd?105th). Of
14,528 bills, 1,580 were labeled as trivial, 119 as
technical, 972 as recurring, and 1,508 as important.
Our hypothesis is that these categories can help ex-
plain which bills survive committees.
To categorize the bills in the other Congresses
of our dataset, we trained binary logistic regression
models to label bills with each of the three most fre-
quent bill types above (trivial, recurring, and impor-
tant) based on unigram features of the body of bill
text. (There is some overlap among categories in the
annotated data, so we opted for three binary clas-
sifiers rather than multi-class.) In a ten-fold cross-
validated experiment, this model averaged 83% ac-
curacy across the prediction tasks. We used the man-
ually annotated labels for the bills in the 103rd?
105th Congresses; for other bills, we calculated each
model?s probability that the bill belonged to the tar-
get category.6 These values were used to define bi-
nary indicators for each classifier?s probability re-
gions: [0, 0.3); [0.3, 0.4); [0.4, 0.5); [0.5, 1.0]. For
each of the three labels, we included two classifiers
trained with different hyperparameter settings, giv-
ing a total of 24 additional features. All baseline
features were retained.
Performance. Including functional category fea-
tures reduces the prediction error slightly but signif-
icantly relative to the baseline (just over 1% relative
error reduction)?see Table 3.7
Discussion. Considering the model?s weights, the
log-odds are most strongly influenced toward bill
success by bills that seem ?important? according to
the classifiers. 55% of this model?s features had non-
zero impact on test-set predictions; compare this to
only 36% of the baseline model?s features.8 Further,
the category features accounted for 66% of the total
(absolute) impact of all features. Taken altogether,
these observations suggest that bill category features
are a more compact substitute for many of the base-
line features,9 but that they do not offer much ad-
ditional predictive information beyond the baseline
(error is only slightly reduced). It is also possi-
ble that our categories do not perfectly capture the
perceptions of committees making decisions about
bills. Refinement of the categories within the pre-
6In preliminary experiments, we used the 103rd?105th data
to measure the effect of automatic vs. manual categories.
Though the particulars of the earlier model and the smaller
dataset size make controlled comparison impossible, we note
that gold-standard annotations achieved 1?2% lower absolute
error across cross-validation folds.
7We note that preliminary investigations conjoining the bill
category features with baseline features did not show any gains.
Prior work by Adler and Wilkerson (2012) suggests that bill cat-
egory interacts with the sponsor?s identity, but does not consider
bill success prediction; we leave a more careful exploration of
this interaction in our framework to future work.
8Note that `1-regularized models make global decisions
about which features to include, so the new features influence
which baseline features get non-zero weights. Comparing the
absolute number of features in the final selected models is not
meaningful, since it depends on the hyperparameter ?, which is
tuned separately for each model.
9This substitutability is unsurprising in some scenarios; e.g.,
successful reauthorization bills are often sponsored by commit-
tee leadership.
797
Model Error (%) False + False ? True + # Feats. Size Effective
most frequent class 12.6 0 828 0 ? ? ?
?4.2 baseline (no text) 11.8 69 709 119 3,731 1,284 460
?5.1 bill categories 11.7 52 716 112 3,755 274 152
?5.2
proxy vote, chair only 10.8 111 596 232 3,780 1,111 425
proxy vote, majority 11.3 134 606 222 3,777 526 254
proxy vote, whole committee 10.9 123 596 232 3,777 1,131 433
proxy vote, all three 10.9 110 606 222 3,872 305 178
?5.3 unigram & bigram 9.8 106 541 287 28,246 199 194
?5.4 full model (all of the above) 9.6 120 514 314 28,411 1,096 1,069
Table 3: Key experimental results; models were trained on the 103rd?110th Congresses and tested on the 111th.
Baseline features are included in each model listed below the baseline. ?# Feats.? is the total number of features
available to the model; ?Size? is the number of features with non-zero weights in the final selected sparse model;
?Effective? is the number of features with non-zero impact (eq. 4) on test data. Each model?s improvement over the
baseline is significant (McNemar?s test, p < 0.0001 except bill categories, for which p < 0.065).
dictive framework we have laid out here is left to
future research.
5.2 Textual Proxy Votes
We next consider a different view of text: as a means
of profiling the preferences and agendas of legisla-
tors. Our hypothesis here is that committees oper-
ate similarly to the legislature as a whole: when a
bill comes to a committee for consideration, mem-
bers of the committee vote on whether it will sur-
vive. Of course, deliberation and compromise may
take place before such a vote; our simple model does
not attempt to account for such complex processes,
instead merely positing a hidden roll call vote.
Although the actions of legislators on commit-
tees are hidden, their voting behavior on the floor
is observed. Roll call data is frequently used in po-
litical science to estimate spatial models of legis-
lators and legislation (Poole and Rosenthal, 1985;
Poole and Rosenthal, 1991; Jackman, 2001; Clinton
et al, 2004). These models help visualize politics in
terms of intuitive, low-dimensional spaces which of-
ten correspond closely to our intuitions about ?left?
and ?right? in American politics. Recently, Gerrish
and Blei (2011) showed how such models could nat-
urally be augmented with models of text. Such mod-
els are based on observed voting; it is left to future
work to reduce the dimensionality of hidden votes
within the survival prediction model here.
Our approach is to construct a proxy vote; an es-
timate of a roll call vote by members of the com-
mittee on the bill. We consider three variants, each
based on the same estimate of the individual com-
mittee members? votes:
? Only the committee chairman?s vote matters.
? Only majority-party committee members vote.
? All committee members vote.
We will compare these three versions of the proxy
vote feature experimentally, but abstractly they can
all be defined the same way. Let C denote the set of
committee members who can vote on a bill x. Then
the proxy vote equals:
1
|C|
?
j?C E[Vj,x] (5)
(If x is referred to more than one committee, we av-
erage the above feature across committees.) We treat
the vote by representative j on bill x as a binary ran-
dom variable Vj,x corresponding to a vote for (1) or
against (0) the bill. We do not observe Vj,x; instead
we estimate its expected value, which will be be-
tween 0 and 1. Note that, by linearity of expecta-
tion, the sum in equation 5 is the expected value of
the number of committee members who ?voted? for
the bill; dividing by |C| gives a value that, if our esti-
mates are correct, should be close to 1 when the bill
is likely to be favored by the committee and 0 when
it is likely to be disfavored.
To estimate E[Vj,x], we use a simple probabilis-
tic model of Vj,x given the bill x and the past vot-
ing record of representative j.10 Let Rj be a set of
10We note that the observable roll call votes on the floor of
798
bills that representative j has publicly voted on, on
the floor of the House, in the past.11 For x ? Rj ,
let Vj,x be 1 if j voted for the bill and 0 if j voted
against it. Further, define a similarity measure be-
tween bills; here we use cosine similarity of two
bills? tfidf vectors.12 We denote by sim(x, x?) the
similarity of bills x and x?.
The probabilistic model is as follows. First, the
representative selects a bill he has voted on previ-
ously; he is likely to choose a bill that is similar to
x. More formally, given representative j and bill x,
randomly choose a bill X ? from Rj according to:
p(X ? = x? | j, x) = exp sim(x,x
?)P
x???Rj
exp sim(x,x??) (6)
An attractive property of this distribution is that it
has no parameters to estimate; it is defined entirely
by the text of bills in Rj . Second, the representa-
tive votes on x identically to how he voted on X ?.
Formally, let Vj,x = Vj,x? , which is observed.
The above model gives a closed form for the ex-
pectation of Vj,x:
E[Vj,x] =
?
x??Rj p(X
? = x? | j, x) ? Vj,x? (7)
In addition to the proxy vote score in eq. 5, we cal-
culate a similar expected vote based on ?nay? votes,
and consider a second score that is the ratio of the
?yea? proxy vote to the ?nay? proxy vote. Both
of these scores are continuous values; we quantize
them into bins, giving 141 features.13
Performance. Models built using the baseline fea-
tures plus, in turn, each of the three variations of the
proxy vote feature (C defined to include the chair
the U.S. House consist of a very different sample of bills than
those we consider in this study; indeed, votes on the floor cor-
respond to bills that survived committee. We leave attempts to
characterize and control for this bias to future work.
11To simplify matters, we use all bills from the training pe-
riod that j has voted on. For future predictions (on the test set),
these are all in the past, but in the training set they may include
bills that come later than a given training example.
12We first eliminated punctutation and numbers from the
texts, then removed unigrams which occured in more than 75%
or less than 0.05% of the training documents. Tfidf scores were
calculated based on the result.
13We discretized the continuous values by 0.01 increment for
proxy vote score, and 0.1 increment for proxy vote rate scores.
We further combined outlier bins (one for exremely large val-
ues, one for extremely small values).
only, majority party members, or the full commit-
tee), and all three sets of proxy vote features, were
compared?see Table 3. All three models showed
improvement over the baseline. Using the chairman-
only committee (followed closely by whole commit-
tee and all three) turned out to be the best performing
among them, with a 8% relative error reduction.
Discussion. Nearly 58% of the features in the com-
bined model had non-zero impact at test time, and
38% of total absolute impact was due to these fea-
tures. Comparing the performance of these four
models suggests that, as is widely believed in polit-
ical science, the preferences of the committee chair
are a major factor in which bills survive.
5.3 Direct Use of Content: Bag of Words
Our third hypothesis is that committees make collec-
tive decisions by considering the contents of bills di-
rectly. A sensible starting point is to treat our model
as a document classifier and incorporate standard
features of the text directly into the model, rather
than deriving functional categories or proxy votes
from the text.14 Perhaps unsurprisingly, this ap-
proach will perform better than the previous two.
Following Pang and Lee (2004), who used word
and bigram features to model an author?s sentiment,
and Kogan et al (2009), who used word and bigram
features to directly predict a future outcome, we in-
corporate binary features for the presence or absence
of terms in the body and (separately) in the title of
the bill. We include unigram features for the body
and unigram and bigram features for the title.15 The
result is 28,246 features, of which 24,515 are lexical.
Performance. Combined with baseline features,
word and bigram features led to nearly 18% relative
error reduction compared to the baseline and 9% rel-
ative to the best model above (Table 3). The model
is very small (under 200 features), and 98% of the
features in the model impacted test-time predictions.
The model?s gain over the baseline is not sensitive to
the score threshold; see Figure 1.
A key finding is that the bag of words model out-
14The models from ?5.1 and ?5.2 can be understood from
a machine learning perspective as task-specific dimensionality
reduction methods on the words.
15Punctuation marks are removed from the text, and numbers
are collapsed into single indicator. We filtered terms appearing
in fewer than 0.5% and more than 30% of training documents.
799
Bill Survival Bill Death
Contents Title Contents Title
resources 0.112 title as 0.052 percent -0.074 internal -0.058
ms 0.056 other purposes 0.041 revenue -0.061 the internal 0.024
authorization 0.053 for other 0.028 speaker -0.050 revenue -0.022
information 0.049 amended by 0.017 security -0.037 prohibit -0.020
authorize 0.030 of the 0.017 energy -0.037 internal revenue -0.019
march 0.029 for the 0.014 make -0.030 the social -0.018
amounts 0.027 public 0.012 require -0.029 amend title -0.016
its 0.026 extend 0.011 human -0.029 to provide -0.015
administration 0.026 designate the 0.010 concerned -0.029 establish -0.015
texas 0.024 as amended 0.009 department -0.027 SYMBOL to -0.014
interior 0.023 located 0.009 receive -0.025 duty on -0.013
judiciary 0.021 relief 0.009 armed -0.024 revenue code -0.013
Table 4: Full model:
text terms with
highest impact
(eq. 4). Impact
scores are not
comparable across
models, so for com-
parison, the impacts
for the features from
Table 2 here are,
respectively: 0.534,
0.181, 10?4, 0.196,
0.123, 0.063, 0.053;
-0.011, 0, 0.003, 0.
0.0 0.2 0.4 0.6 0.8
0.2
0.4
0.6
0.8
1.0
Recall
Pre
cis
ion
Bag of Words
Baseline
Figure 1: Precision-recall curve (survival is the target
class) comparing the bag of words model to the baseline.
performs the bill categories and proxy vote models.
This suggests that there is more information in the
text contents than either the functional categories or
similarity to past bills.16
5.4 Full Model
Finally, we considered a model using all three kinds
of text features. Shown in Table 3, this reduces error
only 2% relative to the bag of words model. This
leads us to believe that direct use of text captures
most of what functional bill category and proxy vote
features capture about bill success.
16We also experimented with dimensionality reduction with
latent Dirichlet alocation (Blei et al, 2003). We used the topic
posteriors as features in lieu of words during training and test-
ing. The symmetric Dirichlet hyperparameter was fixed at 0.1,
and we explored 10?200 topics. Although this offered speedups
in training time, the performance was consistently worse than
the bag of words model, for each number of topics.
Table 4 shows the terms with greatest impact.
When predicting bills to survive, the model seems
to focus on explanations for minor legislation. For
example, interior and resources may indicate non-
controversial local land transfer bills. In titles, des-
ignate and located have to do with naming federal
buildings (e.g., post offices).
As for bills that die, the model appears to have
captured two related facts about proposed legisla-
tion. One is that legislators often sponsor bills to
express support or concern about an issue with little
expectation that the bill will become a law. If such
?position-taking? accounts for many of the bills pro-
posed, then we would expect features with high im-
pact toward failure predictions to relate to such is-
sues. This would explain the terms energy, security,
and human (if used in the context of human rights or
human cloning). The second fact is that some bills
die because committees ultimately bundle their con-
tents into bigger bills. There are many such bills re-
lating to tax policy (leading to the terms contained in
the trigram Internal Revenue Service, the American
tax collection agency) and Social Security policy (a
collection of social welfare and social insurance pro-
grams), for example.17
17The term speaker likely refers to the first ten bill numbers,
which are ?reserved for the speaker,? which actually implies that
no bill was introduced. Our process for marking bills that sur-
vive (based on committee recommendation data) leaves these
unmarked, hence they ?died? in our gold-standard data. The
experiments revealed this uninteresting anomaly.
800
Model
Error (%)
109th 110th
most frequent class 11.8 14.5
?4.2 baseline (no text) 11.1 13.9
?5.1 bill categories 10.9 13.6
?5.2 proxy vote, all three 9.9 12.7
?5.3 unigram & bigram 8.9 10.6
?5.4 full model 8.9 10.9
Table 5: Replicated results on two different data splits.
Columns are marked by the test-set Congress. See ?5.5.
5.5 Replication
To avoid drawing conclusions based on a single,
possibly idiosyncratic Congress, we repeated the ex-
periment using the 109th and 110th Congresses as
test datasets, training only on bills prior to the test
set. The error patterns are similar to the primary
split; see Table 5.
6 Discussion
From a political science perspective, our experimen-
tal results using text underscore the importance of
considering the substance of policy proposals (here,
bills) when attempting to explain their progress. An
important research direction in political science, one
in which NLP must play a role, is how different
types of issues are managed in legislatures. Our re-
sults also suggest that political considerations may
induce lawmakers to sponsor certain types of bills
with no real expectation of seeing them enacted into
law.
Considerable recent work has modeled text along-
side data about social behavior. This includes pre-
dictive settings (Kogan et al, 2009; Lerman et
al., 2008), various kinds of sentiment and opin-
ion analysis (Thomas et al, 2006; Monroe et al,
2008; O?Connor et al, 2010; Das et al, 2009), and
exploratory models (Steyvers and Griffiths, 2007).
In political science specifically, the ?text as data?
movement (Grimmer and Stewart, 2012; O?Connor
et al, 2011) has leveraged tools from NLP in quan-
titative research. For example, Grimmer (2010) and
Quinn et al (2006) used topic models to study, re-
spectively, Supreme Court proceedings and Senate
speeches. Closest to this work, Gerrish and Blei
(2011) combined topic models with spatial roll call
models to predict votes in the legislature from text
alone. Their best results, however, came from a
text regression model quite similar to our direct text
model.
7 Conclusions
We presented a novel task: predicting whether a
Congressional bill will be recommended by a com-
mittee. We introduced a strong, expert-informed
baseline that uses basic social features, then demon-
strated substantial improvents on the task using text
in a variety of ways. Comparison leads to insights
about American lawmaking. The data are available
to the research community.
Acknowledgments
We thank the anonymous reviewers, David Bamman,
Justin Grimmer, Michael Heilman, Brendan O?Connor,
Dani Yogatama, and other members of the ARK research
group for helpful feedback. This research was supported
by DARPA grant N10AP20042.
References
E. Scott Adler and John Wilkerson. 2005. The scope
and urgency of legislation: Reconsidering bill success
in the house of representatives. Paper presented at the
annual meeting of the American Political Science As-
sociation.
E. Scott Adler and John Wilkerson. 2012. Congress and
the Politics of Problem Solving. Cambridge University
Press, London.
Beata Beigman Klebanov, Daniel Diermeier, and Eyal
Beigman. 2008. Lexical cohesion analysis of politi-
cal speech. Political Analysis, 16(4):447?463.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Joshua Clinton, Simon Jackman, and Doug Rivers. 2004.
The statistical analysis of roll-call data. American Po-
litical Science Review, 98(2):355?370.
Pradipto Das, Rohini Srihari, and Smruthi Mukund.
2009. Discovering voter preferences in blogs using
mixtures of topic models. In Proceedings of the Third
Workshop on Analytics for Noisy Unstructured Text
Data.
Sean Gerrish and David Blei. 2011. Predicting legisla-
tive roll calls from text. In Proc. of ICML.
Justin Grimmer and Brandon Stewart. 2012. Text as
data: The promise and pitfalls of automatic content
analysis methods for political documents. http://
www.stanford.edu/?jgrimmer/tad2.pdf.
801
Justin Grimmer. 2010. A Bayesian hierarchical topic
model for political texts: Measuring expressed agen-
das in Senate press releases. Political Analysis,
18(1):1?35.
Simon Jackman. 2001. Multidimensional analysis of
roll call data via Bayesian simulation: Identification,
estimation, inference, and model checking. Political
Analysis, 9(3):227?241.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Ja-
cob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc. of
NAACL.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311?331.
Kevin Lerman, Ari Gilder, Mark Dredze, and Fernando
Pereira. 2008. Reading the markets: Forecasting pub-
lic opinion of political candidates by news analysis. In
Proc. of COLING.
Nolan McCarty, Howard Rosenthal, and Keith T. Poole.
2006. Polarized America: The Dance of Ideology and
Unequal Riches. MIT Press.
Burt Monroe, Michael Colaresi, and Kevin M. Quinn.
2008. Fightin? words: Lexical feature selection and
evaluation for identifying the content of political con-
flict. Political Analysis, 16(4):372?403.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, David Bamman, and Noah A. Smith.
2011. Computational text analysis for social science:
Model complexity and assumptions. In Proc. of the
NIPS Workshop on Comptuational Social Science and
the Wisdom of Crowds.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL.
Keith T. Poole and Howard Rosenthal. 1985. A spatial
model for legislative roll call analysis. American Jour-
nal of Political Science, 29(2):357?384.
Keith T. Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, 35(1):228?278.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2006.
An automated method of topic-coding legislative
speech over time with application to the 105th?108th
U.S. Senate. Paper presented at the meeting of the
Midwest Political Science Association.
Kevin M. Quinn, Burt L Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2010.
How to analyze political attention with minimal as-
sumptions and costs. American Journal of Political
Science, 54(1):209?228.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. In T. Landauer, D. McNamara, S. Den-
nis, and W. Kintsch, editors, Handbook of Latent Se-
mantic Analysis. Lawrence Erlbaum.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proc. of
EMNLP.
802
T2: Structured Sparsity in Natural Language 
Processing: Models, Algorithms and 
Applications 
Andr? F. T. Martins, M?rio A. T. Figueiredo, and Noah A. Smith 
ABSTRACT
This tutorial will cover recent advances in sparse modeling with diverse applications in 
natural language processing (NLP). A sparse model is one that uses a relatively small 
number of features to map an input to an output, such as a label sequence or parse 
tree. The advantages of sparsity are, among others, compactness and interpretability; in 
fact, sparsity is currently a major theme in statistics, machine learning, and signal 
processing. The goal of sparsity can be seen in terms of earlier goals of feature 
selection and therefore model selection (Della Pietra et al, 1997; Guyon and Elisseeff, 
2003; McCallum, 2003). 
This tutorial will focus on methods which embed sparse model selection into the 
parameter estimation problem. In such methods, learning is carried out by minimizing a 
regularized empirical risk functional composed of two terms: a "loss term," which 
controls the goodness of fit to the data (e.g., log loss or hinge loss), and a "regularizer 
term," which is designed to promote sparsity. The simplest example is L1-norm 
regularization (Tibshirani, 2006), which penalizes weight components individually, and 
has been explored in various NLP applications (Kazama and Tsujii, 2003; Goodman, 
2004; Gao, 2007). More sophisticated regularizers, those that use mixed norms and 
groups of weights, are able to promote "structured" sparsity: i.e., they promote sparsity 
patterns that are compatible with a priori knowledge about the structure of the feature 
space. These kind of regularizers have been proposed in the statistical and signal 
processing literature (Yuan and Lin, 2006; Zhao et al, 2009; Kim et al, 2010; Bach et 
al., 2011) and are a recent topic of research in NLP (Eisenstein et al, 2011; Martins et 
al, 2011, Das and Smith, 2012). Sparsity-inducing regularizers require the use of 
specialized optimization routines for learning (Wright et al, 2009; Xiao, 2009; Langford 
et al, 2009). 
The tutorial will consist of three parts: (1) how to formulate the problem, i.e., how to 
choose the right regularizer for the kind of sparsity pattern intended; (2) how to solve the 
optimization problem efficiently; and (3) examples of the use of sparsity within natural 
language processing problems. 
OUTLINE
1. Introduction 
(30 minutes) 
o What is sparsity? 
o Why sparsity is often desirable in NLP 
o Feature selection: wrappers, filters, and embedded methods 
o What has been done in other areas: the Lasso and group-Lasso, 
compressive sensing, and recovery guarantees 
o Theoretical and practical limitations of previous methods to typical NLP 
problems 
o Beyond cardinality: structured sparsity 
2. Group-Lasso and Mixed-Norm Regularizers 
(45 minutes) 
o Selecting columns in a grid-shaped feature space 
o Examples: multiple classes, multi-task learning, multiple kernel learning 
o Mixed L2/L1 and Linf/L1 norms: the group Lasso 
o Non-overlapping groups 
o Example: feature template selection 
o Tree-structured groups 
o The general case: a DAG 
o Coarse-to-fine regularization 
3. Coffee Break 
(15 minutes) 
4. Optimization Algorithms 
(45 minutes) 
o Non-smooth optimization: limitations of subgradient algorithms 
o Quasi-Newton methods: OWL-QN 
o Proximal gradient algorithms: iterative soft-thresholding, forward-backward 
and other splittings 
o Computing proximal steps 
o Other algorithms: FISTA, Sparsa, ADMM, Bregman iterations 
o Convergence rates 
o Online algorithms: limitations of stochastic subgradient descent 
o Online proximal gradient algorithms 
o Managing general overlapping groups 
o Memory footprint, time/space complexity, etc. 
o The "Sparseptron" algorithm and debiasing 
5. Applications 
(30 minutes): 
o Sociolinguistic association discovery 
o Sequence problems: named entity recognition, chunking 
o Multilingual dependency parsing 
o Lexicon expansion 
6. Closing Remarks and Discussion 
(15 minutes) 
BIOS
Andr? F. T. Martins
Instituto de Telecomunica??es, Instituto Superior T?cnico 
Av. Rovisco Pais, 1, 1049-001 Lisboa, Portugal, 
and Priberam Inform?tica 
Al. Afonso Henriques, 41 - 2., 1000-123 Lisboa, Portugal 
afm--AT--cs.cmu.edu 
A. Martins is a final year Ph.D. student in Carnegie Mellon's School of Computer 
Science and the Instituto de Telecomunica??es at Instituto Superior T?cnico, where he 
is working on a degree in Language Technologies. Martins' research interests include 
natural language processing, machine learning, convex optimization, and sparse 
modeling. His dissertation focuses on new models and algorithms for structured 
prediction with non-local features. His paper "Concise Integer Linear Programming 
Formulations for Dependency Parsing" received a best paper award at ACL 2009. 
M?rio A. T. Figueiredo
Instituto de Telecomunica??es, Instituto Superior T?cnico 
Av. Rovisco Pais, 1, 1049-001 Lisboa, Portugal 
mario.figueiredo--AT--lx.it.pt 
M. Figueiredo is a professor of electrical and computer engineering at Instituto Superior 
T?cnico (the engineering school of the Technical University of Lisbon) and his main 
research interests include machine learning, statistical signal processing, and 
optimization. He recently guest co-edited a special issue of the IEEE Journal on Special 
Topics in Signal Processing devoted to compressive sensing (one of the central areas 
of research on sparsity) and gave several invited talks (and a tutorial at ICASSP 2012) 
on optimization algorithms for problems involving sparsity. 
Noah A. Smith
School of Computer Science, Carnegie Mellon University 
5000 Forbes Avenue, Pittsburgh, PA 15213, USA 
nasmith--AT--cs.cmu.edu 
N. Smith is Finmeccanica associate professor in language technologies and machine 
learning at CMU. His research interests include statistical natural language processing, 
especially unsupervised methods, machine learning for structured data, and 
applications of natural language processing, including machine translation and 
statistical modeling of text and quantitative social data. He recently published a book, 
Linguistic Structure Prediction, about many of these topics; in 2009 he gave a tutorial at 
ICML about structured prediction in NLP. 
ACKNOWLEDGMENTS
This tutorial was enabled by support from the following organizations: 
? National Science Foundation (USA), CAREER grant IIS-1054319. 
? Funda??o para a Ci?ncia e Tecnologia (Portugal), grant PEst-
OE/EEI/LA0008/2011. 
? Funda??o para a Ci?ncia e Tecnologia and Information and Communication 
Technologies Institute (Portugal/USA), through the CMU-Portugal Program. 
? QREN/POR Lisboa (Portugal), EU/FEDER programme, Discooperio project, 
contract 2011/18501. 
Proceedings of NAACL-HLT 2013, pages 380?390,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Part-of-Speech Tagging for Online Conversational Text
with Word Clusters
Olutobi Owoputi? Brendan O?Connor? Chris Dyer?
Kevin Gimpel? Nathan Schneider? Noah A. Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
Corresponding author: brenocon@cs.cmu.edu
Abstract
We consider the problem of part-of-speech
tagging for informal, online conversational
text. We systematically evaluate the use of
large-scale unsupervised word clustering
and new lexical features to improve tagging
accuracy. With these features, our system
achieves state-of-the-art tagging results on
both Twitter and IRC POS tagging tasks;
Twitter tagging is improved from 90% to 93%
accuracy (more than 3% absolute). Quali-
tative analysis of these word clusters yields
insights about NLP and linguistic phenomena
in this genre. Additionally, we contribute the
first POS annotation guidelines for such text
and release a new dataset of English language
tweets annotated using these guidelines.
Tagging software, annotation guidelines, and
large-scale word clusters are available at:
http://www.ark.cs.cmu.edu/TweetNLP
This paper describes release 0.3 of the ?CMU
Twitter Part-of-Speech Tagger? and annotated
data.
1 Introduction
Online conversational text, typified by microblogs,
chat, and text messages,1 is a challenge for natu-
ral language processing. Unlike the highly edited
genres that conventional NLP tools have been de-
veloped for, conversational text contains many non-
standard lexical items and syntactic patterns. These
are the result of unintentional errors, dialectal varia-
tion, conversational ellipsis, topic diversity, and cre-
ative use of language and orthography (Eisenstein,
2013). An example is shown in Fig. 1. As a re-
sult of this widespread variation, standard model-
ing assumptions that depend on lexical, syntactic,
and orthographic regularity are inappropriate. There
1Also referred to as computer-mediated communication.
ikr
!
smh
G
he
O
asked
V
fir
P
yo
D
last
A
name
N
so
P
he
O
can
V
add
V
u
O
on
P
fb
?
lololol
!
Figure 1: Automatically tagged tweet showing nonstan-
dard orthography, capitalization, and abbreviation. Ignor-
ing the interjections and abbreviations, it glosses as He
asked for your last name so he can add you on Facebook.
The tagset is defined in Appendix A. Refer to Fig. 2 for
word clusters corresponding to some of these words.
is preliminary work on social media part-of-speech
(POS) tagging (Gimpel et al, 2011), named entity
recognition (Ritter et al, 2011; Liu et al, 2011), and
parsing (Foster et al, 2011), but accuracy rates are
still significantly lower than traditional well-edited
genres like newswire. Even web text parsing, which
is a comparatively easier genre than social media,
lags behind newspaper text (Petrov and McDonald,
2012), as does speech transcript parsing (McClosky
et al, 2010).
To tackle the challenge of novel words and con-
structions, we create a new Twitter part-of-speech
tagger?building on previous work by Gimpel et
al. (2011)?that includes new large-scale distribu-
tional features. This leads to state-of-the-art results
in POS tagging for both Twitter and Internet Relay
Chat (IRC) text. We also annotated a new dataset of
tweets with POS tags, improved the annotations in
the previous dataset from Gimpel et al, and devel-
oped annotation guidelines for manual POS tagging
of tweets. We release all of these resources to the
research community:
? an open-source part-of-speech tagger for online
conversational text (?2);
? unsupervised Twitter word clusters (?3);
? an improved emoticon detector for conversational
text (?4);
380
? POS annotation guidelines (?5.1); and
? a new dataset of 547 manually POS-annotated
tweets (?5).
2 MEMM Tagger
Our tagging model is a first-order maximum en-
tropy Markov model (MEMM), a discriminative se-
quence model for which training and decoding are
extremely efficient (Ratnaparkhi, 1996; McCallum
et al, 2000).2 The probability of a tag yt is condi-
tioned on the input sequence x and the tag to its left
yt?1, and is parameterized by a multiclass logistic
regression:
p(yt = k | yt?1,x, t;?) ?
exp
(
?(trans)yt?1,k +
?
j ?
(obs)
j,k fj(x, t)
)
We use transition features for every pair of labels,
and extract base observation features from token t
and neighboring tokens, and conjoin them against
all K = 25 possible outputs in our coarse tagset
(Appendix A). Our feature sets will be discussed
below in detail.
Decoding. For experiments reported in this paper,
we use the O(|x|K2) Viterbi algorithm for predic-
tion; K is the number of tags. This exactly max-
imizes p(y | x), but the MEMM also naturally al-
lows a fasterO(|x|K) left-to-right greedy decoding:
for t = 1 . . . |x|:
y?t ? argmaxk p(yt = k | y?t?1,x, t;?)
which we find is 3 times faster and yields similar ac-
curacy as Viterbi (an insignificant accuracy decrease
of less than 0.1% absolute on the DAILY547 test set
discussed below). Speed is paramount for social me-
dia analysis applications?which often require the
processing of millions to billions of messages?so
we make greedy decoding the default in the released
software.
2Although when compared to CRFs, MEMMs theoretically
suffer from the ?label bias? problem (Lafferty et al, 2001), our
system substantially outperforms the CRF-based taggers of pre-
vious work; and when comparing to Gimpel et al system with
similar feature sets, we observed little difference in accuracy.
This is consistent with conventional wisdom that the quality
of lexical features is much more important than the paramet-
ric form of the sequence model, at least in our setting: part-of-
speech tagging with a small labeled training set.
This greedy tagger runs at 800 tweets/sec. (10,000
tokens/sec.) on a single CPU core, about 40 times
faster than Gimpel et al?s system. The tokenizer by
itself (?4) runs at 3,500 tweets/sec.3
Training and regularization. During training,
the MEMM log-likelihood for a tagged tweet ?x,y?
is the sum over the observed token tags yt, each con-
ditional on the tweet being tagged and the observed
previous tag (with a start symbol before the first to-
ken in x),
`(x,y,?) =
?|x|
t=1 log p(yt | yt?1,x, t;?).
We optimize the parameters ? with OWL-QN, an
L1-capable variant of L-BFGS (Andrew and Gao,
2007; Liu and Nocedal, 1989) to minimize the regu-
larized objective
argmin
?
? 1N
?
?x,y? `(x,y,?) +R(?)
where N is the number of tokens in the corpus and
the sum ranges over all tagged tweets ?x,y? in the
training data. We use elastic net regularization (Zou
and Hastie, 2005), which is a linear combination of
L1 and L2 penalties; here j indexes over all features:
R(?) = ?1
?
j |?j |+
1
2?2
?
j ?
2
j
Using even a very small L1 penalty eliminates many
irrelevant or noisy features.
3 Unsupervised Word Clusters
Our POS tagger can make use of any number of pos-
sibly overlapping features. While we have only a
small amount of hand-labeled data for training, we
also have access to billions of tokens of unlabeled
conversational text from the web. Previous work has
shown that unlabeled text can be used to induce un-
supervised word clusters which can improve the per-
formance of many supervised NLP tasks (Koo et al,
2008; Turian et al, 2010; T?ckstr?m et al, 2012, in-
ter alia). We use a similar approach here to improve
tagging performance for online conversational text.
We also make our induced clusters publicly avail-
able in the hope that they will be useful for other
NLP tasks in this genre.
3Runtimes observed on an Intel Core i5 2.4 GHz laptop.
381
Binary path Top words (by frequency)
A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol
A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
A3 111010100100 yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
A4 111010100101 yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
A5 11101011011100 smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
B 011101011 u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo yue juu dya youz yyou
C 11100101111001 w fo fa fr fro ov fer fir whit abou aft serie fore fah fuh w/her w/that fron isn agains
D 111101011000 facebook fb itunes myspace skype ebay tumblr bbm flickr aim msn netflix pandora
E1 0011001 tryna gon finna bouta trynna boutta gne fina gonn tryina fenna qone trynaa qon
E2 0011000 gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
F 0110110111 soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
G1 11101011001010 ;) :p :-) xd ;-) ;d (; :3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-d
G2 11101011001011 :) (: =) :)) :] :?) =] ^_^ :))) ^.^ [: ;)) ((: ^__^ (= ^-^ :))))
G3 1110101100111 :( :/ -_- -.- :-( :?( d: :| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /: :(( >_< =[ :[ #fml
G4 111010110001 <3 xoxo <33 xo <333 #love s2 <URL-twitition.com> #neversaynever <3333
Figure 2: Example word clusters (HMM classes): we list the most probable words, starting with the most probable, in
descending order. Boldfaced words appear in the example tweet (Figure 1). The binary strings are root-to-leaf paths
through the binary cluster tree. For example usage, see e.g. search.twitter.com, bing.com/social and
urbandictionary.com.
3.1 Clustering Method
We obtained hierarchical word clusters via Brown
clustering (Brown et al, 1992) on a large set of
unlabeled tweets.4 The algorithm partitions words
into a base set of 1,000 clusters, and induces a hi-
erarchy among those 1,000 clusters with a series of
greedy agglomerative merges that heuristically opti-
mize the likelihood of a hidden Markov model with a
one-class-per-lexical-type constraint. Not only does
Brown clustering produce effective features for dis-
criminative models, but its variants are better unsu-
pervised POS taggers than some models developed
nearly 20 years later; see comparisons in Blunsom
and Cohn (2011). The algorithm is attractive for our
purposes since it scales to large amounts of data.
When training on tweets drawn from a single
day, we observed time-specific biases (e.g., nu-
merical dates appearing in the same cluster as the
word tonight), so we assembled our unlabeled data
from a random sample of 100,000 tweets per day
from September 10, 2008 to August 14, 2012,
and filtered out non-English tweets (about 60% of
the sample) using langid.py (Lui and Baldwin,
2012).5 Each tweet was processed with our to-
4As implemented by Liang (2005), v. 1.3: https://
github.com/percyliang/brown-cluster
5https://github.com/saffsd/langid.py
kenizer and lowercased. We normalized all at-
mentions to ?@MENTION? and URLs/email ad-
dresses to their domains (e.g. http://bit.ly/
dP8rR8 ? ?URL-bit.ly?). In an effort to reduce
spam, we removed duplicated tweet texts (this also
removes retweets) before word clustering. This
normalization and cleaning resulted in 56 million
unique tweets (847 million tokens). We set the
clustering software?s count threshold to only cluster
words appearing 40 or more times, yielding 216,856
word types, which took 42 hours to cluster on a sin-
gle CPU.
3.2 Cluster Examples
Fig. 2 shows example clusters. Some of the chal-
lenging words in the example tweet (Fig. 1) are high-
lighted. The term lololol (an extension of lol for
?laughing out loud?) is grouped with a large number
of laughter acronyms (A1: ?laughing my (fucking)
ass off,? ?cracking the fuck up?). Since expressions
of laughter are so prevalent on Twitter, the algorithm
creates another laughter cluster (A1?s sibling A2),
that tends to have onomatopoeic, non-acronym vari-
ants (e.g., haha). The acronym ikr (?I know, right??)
is grouped with expressive variations of ?yes? and
?no? (A4). Note that A1?A4 are grouped in a fairly
specific subtree; and indeed, in this message ikr and
382
lololol are both tagged as interjections.
smh (?shaking my head,? indicating disapproval)
seems related, though is always tagged in the an-
notated data as a miscellaneous abbreviation (G);
the difference between acronyms that are interjec-
tions versus other acronyms may be complicated.
Here, smh is in a related but distinct subtree from the
above expressions (A5); its usage in this example
is slightly different from its more common usage,
which it shares with the other words in its cluster:
message-ending expressions of commentary or emo-
tional reaction, sometimes as a metacomment on the
author?s message; e.g., Maybe you could get a guy
to date you if you actually respected yourself #smh
or There is really NO reason why other girls should
send my boyfriend a goodmorning text #justsaying.
We observe many variants of categories tradition-
ally considered closed-class, including pronouns (B:
u = ?you?) and prepositions (C: fir = ?for?).
There is also evidence of grammatical categories
specific to conversational genres of English; clusters
E1?E2 demonstrate variations of single-word con-
tractions for ?going to? and ?trying to,? some of
which have more complicated semantics.6
Finally, the HMM learns about orthographic vari-
ants, even though it treats all words as opaque sym-
bols; cluster F consists almost entirely of variants
of ?so,? their frequencies monotonically decreasing
in the number of vowel repetitions?a phenomenon
called ?expressive lengthening? or ?affective length-
ening? (Brody and Diakopoulos, 2011; Schnoebe-
len, 2012). This suggests a future direction to jointly
model class sequence and orthographic informa-
tion (Clark, 2003; Smith and Eisner, 2005; Blunsom
and Cohn, 2011).
We have built an HTML viewer to browse these
and numerous other interesting examples.7
3.3 Emoticons and Emoji
We use the term emoticon to mean a face or icon
constructed with traditional alphabetic or punctua-
6One coauthor, a native speaker of the Texan English dialect,
notes ?finna? (short for ?fixing to?, cluster E1) may be an im-
mediate future auxiliary, indicating an immediate future tense
that is present in many languages (though not in standard En-
glish). To illustrate: ?She finna go? approximately means ?She
will go,? but sooner, in the sense of ?She is about to go.?
7http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
tion symbols, and emoji to mean symbols rendered
in software as small pictures, in line with the text.
Since our tokenizer is careful to preserve emoti-
cons and other symbols (see ?4), they are clustered
just like other words. Similar emoticons are clus-
tered together (G1?G4), including separate clusters
of happy [[ :) =) ?_? ]], sad/disappointed [[ :/ :(
-_- </3 ]], love [[ ?xoxo ?.? ]] and winking [[
;) (?_-) ]] emoticons. The clusters are not per-
fectly aligned with our POS annotation guidelines;
for example, the ?sad? emoticon cluster included
emotion-bearing terms that our guidelines define as
non-emoticons, such as #ugh, #tear, and #fml (?fuck
my life?), though these seem potentially useful for
sentiment analysis.
One difficult task is classifying different types
of symbols in tweets: our annotation guidelines
differentiate between emoticons, punctuation, and
garbage (apparently non-meaningful symbols or to-
kenization errors). Several Unicode character ranges
are reserved for emoji-style symbols (including the
three Unicode hearts in G4); however, depending
on the user?s software, characters in these ranges
might be rendered differently or not at all. We
have found instances where the clustering algo-
rithm groups proprietary iOS emoji symbols along
with normal emoticons; for example, the character
U+E056, which is interpreted on iOS as a smiling
face, is in the same G2 cluster as smiley face emoti-
cons. The symbol U+E12F, which represents a pic-
ture of a bag of money, is grouped with the words
cash and money.
3.4 Cluster-Based Features
Since Brown clusters are hierarchical in a binary
tree, each word is associated with a tree path rep-
resented as a bitstring with length ? 16; we use pre-
fixes of the bitstring as features (for all prefix lengths
? {2, 4, 6, . . . , 16}). This allows sharing of statisti-
cal strength between similar clusters. Using prefix
features of hierarchical clusters in this way was sim-
ilarly found to be effective for named-entity recog-
nition (Turian et al, 2010) and Twitter POS tag-
ging (Ritter et al, 2011).
When checking to see if a word is associated with
a cluster, the tagger first normalizes the word using
the same techniques as described in ?3.1, then cre-
ates a priority list of fuzzy match transformations
383
of the word by removing repeated punctuation and
repeated characters. If the normalized word is not
in a cluster, the tagger considers the fuzzy matches.
Although only about 3% of the tokens in the devel-
opment set (?6) did not appear in a clustering, this
method resulted in a relative error decrease of 18%
among such word tokens.
3.5 Other Lexical Features
Besides unsupervised word clusters, there are two
other sets of features that contain generalized lexi-
cal class information. We use the tag dictionary fea-
ture from Gimpel et al, which adds a feature for
a word?s most frequent part-of-speech tag.8 This
can be viewed as a feature-based domain adaptation
method, since it gives lexical type-level information
for standard English words, which the model learns
to map between PTB tags to the desired output tags.
Second, since the lack of consistent capitalization
conventions on Twitter makes it especially difficult
to recognize names?Gimpel et al and Foster et
al. (2011) found relatively low accuracy on proper
nouns?we added a token-level name list feature,
which fires on (non-function) words from names
from several sources: Freebase lists of celebrities
and video games (Google, 2012), the Moby Words
list of US Locations,9 and lists of male, female, fam-
ily, and proper names from Mark Kantrowitz?s name
corpus.10
4 Tokenization and Emoticon Detection
Word segmentation on Twitter is challenging due
to the lack of orthographic conventions; in partic-
ular, punctuation, emoticons, URLs, and other sym-
bols may have no whitespace separation from textual
8Frequencies came from the Wall Street Journal and Brown
corpus sections of the Penn Treebank. If a word has multiple
PTB tags, each tag is a feature with value for the frequency rank;
e.g. for three different tags in the PTB, this feature gives a value
of 1 for the most frequent tag, 2/3 for the second, etc. Coarse
versions of the PTB tags are used (Petrov et al, 2011). While
88% of words in the dictionary have only one tag, using rank
information seemed to give a small but consistent gain over only
using the most common tag, or using binary features conjoined
with rank as in Gimpel et al
9http://icon.shef.ac.uk/Moby/mwords.html
10http://www.cs.cmu.edu/afs/cs/project/
ai-repository/ai/areas/nlp/corpora/names/
0.html
words (e.g. no:-d,yes should parse as four tokens),
and internally may contain alphanumeric symbols
that could be mistaken for words: a naive split(/[^a-
zA-Z0-9]+/) tokenizer thinks the words ?p? and ?d?
are among the top 100 most common words on Twit-
ter, due to misanalysis of :p and :d. Traditional Penn
Treebank?style tokenizers are hardly better, often
breaking a string of punctuation characters into a
single token per character.
We rewrote twokenize (O?Connor et al,
2010), a rule-based tokenizer, emoticon, and URL
detector, for use in the tagger. Emoticons are es-
pecially challenging, since they are open-class and
productive. We revise O?Connor et al?s regular ex-
pression grammar that describes possible emoticons,
adding a grammar of horizontal emoticons (e.g. -_-),
known as ?Eastern-style,?11 though we observe high
usage in English-speaking Twitter (Fig. 2, G2?G3).
We also add a number of other improvements to the
patterns. Because this system was used as prepro-
cessing for the word clustering experiment in ?3, we
were able to infer the emoticon clusters in Fig. 2.
Furthermore, whether a token matches the emoticon
pattern is also used as a feature in the tagger (?2).
URL recognition is also difficult, since the http://
is often dropped, resulting in protocol-less URLs
like about.me. We add recognition patterns for these
by using a list of top-level and country domains.
5 Annotated Dataset
Gimpel et al (2011) provided a dataset of POS-
tagged tweets consisting almost entirely of tweets
sampled from one particular day (October 27,
2010). We were concerned about overfitting to time-
specific phenomena; for example, a substantial frac-
tion of the messages are about a basketball game
happening that day.
We created a new test set of 547 tweets for eval-
uation. The test set consists of one random English
tweet from every day between January 1, 2011 and
June 30, 2012. In order for a tweet to be considered
English, it had to contain at least one English word
other than a URL, emoticon, or at-mention. We no-
ticed biases in the outputs of langid.py, so we
instead selected these messages completely manu-
11http://en.wikipedia.org/wiki/List_of_
emoticons
384
ally (going through a random sample of one day?s
messages until an English message was found).
5.1 Annotation Methodology
Gimpel et al provided a tagset for Twitter (shown in
Appendix A), which we used unmodified. The orig-
inal annotation guidelines were not published, but in
this work we recorded the rules governing tagging
decisions and made further revisions while annotat-
ing the new data.12 Some of our guidelines reiter-
ate or modify rules made by Penn Treebank annota-
tors, while others treat specific phenomena found on
Twitter (refer to the next section).
Our tweets were annotated by two annotators who
attempted to match the choices made in Gimpel et
al.?s dataset. The annotators also consulted the POS
annotations in the Penn Treebank (Marcus et al,
1993) as an additional reference. Differences were
reconciled by a third annotator in discussion with all
annotators.13 During this process, an inconsistency
was found in Gimpel et al?s data, which we cor-
rected (concerning the tagging of this/that, a change
to 100 labels, 0.4%). The new version of Gimpel et
al.?s data (called OCT27), as well as the newer mes-
sages (called DAILY547), are both included in our
data release.
5.2 Compounds in Penn Treebank vs. Twitter
Ritter et al (2011) annotated tweets using an aug-
mented version of the PTB tagset and presumably
followed the PTB annotation guidelines. We wrote
new guidelines because the PTB conventions are in-
appropriate for Twitter in several ways, as shown in
the design of Gimpel et al?s tagset. Importantly,
?compound? tags (e.g., nominal+verbal and nomi-
nal+possessive) are used because tokenization is dif-
ficult or seemingly impossible for the nonstandard
word forms that are commonplace in conversational
text.
For example, the PTB tokenization splits contrac-
tions containing apostrophes: I?m? I/PRP ?m/VBP.
But conversational text often contains variants that
resist a single PTB tag (like im), or even chal-
lenge traditional English grammatical categories
12The annotation guidelines are available online at
http://www.ark.cs.cmu.edu/TweetNLP/
13Annotators are coauthors of this paper.
(like imma or umma, which both mean ?I am go-
ing to?). One strategy would be to analyze these
forms into a PTB-style tokenization, as discussed in
Forsyth (2007), who proposes to analyze doncha as
do/VBP ncha/PRP, but notes it would be difficult.
We think this is impossible to handle in the rule-
based framework used by English tokenizers, given
the huge (and possibly growing) number of large
compounds like imma, gonna, w/that, etc. These
are not rare: the word clustering algorithm discov-
ers hundreds of such words as statistically coherent
classes (e.g. clusters E1 and E2 in Fig. 2); and the
word imma is the 962nd most common word in our
unlabeled corpus, more frequent than cat or near.
We do not attempt to do Twitter ?normalization?
into traditional written English (Han and Baldwin,
2011), which we view as a lossy translation task. In
fact, many of Twitter?s unique linguistic phenomena
are due not only to its informal nature, but also a set
of authors that heavily skews towards younger ages
and minorities, with heavy usage of dialects that are
different than the standard American English most
often seen in NLP datasets (Eisenstein, 2013; Eisen-
stein et al, 2011). For example, we suspect that
imma may implicate tense and aspect markers from
African-American Vernacular English.14 Trying to
impose PTB-style tokenization on Twitter is linguis-
tically inappropriate: should the lexico-syntactic be-
havior of casual conversational chatter by young mi-
norities be straightjacketed into the stylistic conven-
tions of the 1980s Wall Street Journal? Instead, we
would like to directly analyze the syntax of online
conversational text on its own terms.
Thus, we choose to leave these word forms un-
tokenized and use compound tags, viewing com-
positional multiword analysis as challenging fu-
ture work.15 We believe that our strategy is suf-
ficient for many applications, such as chunking or
named entity recognition; many applications such
as sentiment analysis (Turney, 2002; Pang and Lee,
2008, ?4.2.3), open information extraction (Carl-
son et al, 2010; Fader et al, 2011), and informa-
tion retrieval (Allan and Raghavan, 2002) use POS
14See ?Tense and aspect? examples in http:
//en.wikipedia.org/wiki/African_American_
Vernacular_English
15For example, wtf has compositional behavior in ?Wtf just
happened???, but only debatably so in ?Huh wtf?.
385
#Msg. #Tok. Tagset Dates
OCT27 1,827 26,594 App. A Oct 27-28, 2010
DAILY547 547 7,707 App. A Jan 2011?Jun 2012
NPSCHAT 10,578 44,997 PTB-like Oct?Nov 2006
(w/o sys. msg.) 7,935 37,081
RITTERTW 789 15,185 PTB-like unknown
Table 1: Annotated datasets: number of messages, to-
kens, tagset, and date range. More information in ?5,
?6.3, and ?6.2.
patterns that seem quite compatible with our ap-
proach. More complex downstream processing like
parsing is an interesting challenge, since contraction
parsing on traditional text is probably a benefit to
current parsers. We believe that any PTB-trained
tool requires substantial retraining and adaptation
for Twitter due to the huge genre and stylistic differ-
ences (Foster et al, 2011); thus tokenization conven-
tions are a relatively minor concern. Our simple-to-
annotate conventions make it easier to produce new
training data.
6 Experiments
We are primarily concerned with performance on
our annotated datasets described in ?5 (OCT27,
DAILY547), though for comparison to previous
work we also test on other corpora (RITTERTW in
?6.2, NPSCHAT in ?6.3). The annotated datasets
are listed in Table 1.
6.1 Main Experiments
We use OCT27 to refer to the entire dataset de-
scribed in Gimpel et al; it is split into train-
ing, development, and test portions (OCT27TRAIN,
OCT27DEV, OCT27TEST). We use DAILY547 as
an additional test set. Neither OCT27TEST nor
DAILY547 were extensively evaluated against until
final ablation testing when writing this paper.
The total number of features is 3.7 million, all
of which are used under pure L2 regularization; but
only 60,000 are selected by elastic net regularization
with (?1, ?2) = (0.25, 2), which achieves nearly the
same (but no better) accuracy as pure L2,16 and we
use it for all experiments. We observed that it was
16We conducted a grid search for the regularizer values on
part of DAILY547, and many regularizer values give the best or
nearly the best results. We suspect a different setup would have
yielded similar results.
l
l
l
l l l l
1e+03 1e+05 1e+07
75
80
85
90
Number of Unlabeled Tweets
Ta
gg
ing
 Ac
cu
rac
y
l
l
l l
l l l
1e+03 1e+05 1e+07
0.6
0
0.6
5
0.7
0
Number of Unlabeled Tweets
To
ke
n 
Co
ve
ra
ge
Figure 3: OCT27 development set accuracy using only
clusters as features.
Model In dict. Out of dict.
Full 93.4 85.0
No clusters 92.0 (?1.4) 79.3 (?5.7)
Total tokens 4,808 1,394
Table 3: DAILY547 accuracies (%) for tokens in and out
of a traditional dictionary, for models reported in rows 1
and 3 of Table 2.
possible to get radically smaller models with only
a slight degradation in performance: (4, 0.06) has
0.5% worse accuracy but uses only 1,632 features, a
small enough number to browse through manually.
First, we evaluate on the new test set, training on
all of OCT27. Due to DAILY547?s statistical repre-
sentativeness, we believe this gives the best view of
the tagger?s accuracy on English Twitter text. The
full tagger attains 93.2% accuracy (final row of Ta-
ble 2).
To facilitate comparisons with previous work, we
ran a series of experiments training only on OCT27?s
training and development sets, then report test re-
sults on both OCT27TEST and all of DAILY547,
shown in Table 2. Our tagger achieves substantially
higher accuracy than Gimpel et al (2011).17
Feature ablation. A number of ablation tests in-
dicate the word clusters are a very strong source of
lexical knowledge. When dropping the tag dictio-
naries and name lists, the word clusters maintain
most of the accuracy (row 2). If we drop the clus-
ters and rely only on tag dictionaries and namelists,
accuracy decreases significantly (row 3). In fact,
we can remove all observation features except for
word clusters?no word features, orthographic fea-
17These numbers differ slightly from those reported by Gim-
pel et al, due to the corrections we made to the OCT27 data,
noted in Section 5.1. We retrained and evaluated their tagger
(version 0.2) on our corrected dataset.
386
Feature set OCT27TEST DAILY547 NPSCHATTEST
All features 91.60 92.80 91.19 1
with clusters; without tagdicts, namelists 91.15 92.38 90.66 2
without clusters; with tagdicts, namelists 89.81 90.81 90.00 3
only clusters (and transitions) 89.50 90.54 89.55 4
without clusters, tagdicts, namelists 86.86 88.30 88.26 5
Gimpel et al (2011) version 0.2 88.89 89.17 6
Inter-annotator agreement (Gimpel et al, 2011) 92.2 7
Model trained on all OCT27 93.2 8
Table 2: Tagging accuracies (%) in ablation experiments. OCT27TEST and DAILY547 95% confidence intervals are
roughly ?0.7%. Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.
tures, affix n-grams, capitalization, emoticon pat-
terns, etc.?and the accuracy is in fact still better
than the previous work (row 4).18
We also wanted to know whether to keep the tag
dictionary and name list features, but the splits re-
ported in Fig. 2 did not show statistically signifi-
cant differences; so to better discriminate between
ablations, we created a lopsided train/test split of
all data with a much larger test portion (26,974 to-
kens), having greater statistical power (tighter con-
fidence intervals of ? 0.3%).19 The full system got
90.8% while the no?tag dictionary, no-namelists ab-
lation had 90.0%, a statistically significant differ-
ence. Therefore we retain these features.
Compared to the tagger in Gimpel et al, most of
our feature changes are in the new lexical features
described in ?3.5.20 We do not reuse the other lex-
ical features from the previous work, including a
phonetic normalizer (Metaphone), a name list con-
sisting of words that are frequently capitalized, and
distributional features trained on a much smaller un-
labeled corpus; they are all worse than our new
lexical features described here. (We did include,
however, a variant of the tag dictionary feature that
uses phonetic normalization for lookup; it seemed to
yield a small improvement.)
18Furthermore, when evaluating the clusters as unsupervised
(hard) POS tags, we obtain a many-to-one accuracy of 89.2%
on DAILY547. Before computing this, we lowercased the text
to match the clusters and removed tokens tagged as URLs and
at-mentions.
19Reported confidence intervals in this paper are 95% bino-
mial normal approximation intervals for the proportion of cor-
rectly tagged tokens: ?1.96
?
p(1? p)/ntokens . 1/
?
n.
20Details on the exact feature set are available in a technical
report (Owoputi et al, 2012), also available on the website.
Non-traditional words. The word clusters are es-
pecially helpful with words that do not appear in tra-
ditional dictionaries. We constructed a dictionary
by lowercasing the union of the ispell ?American?,
?British?, and ?English? dictionaries, plus the stan-
dard Unix words file from Webster?s Second Inter-
national dictionary, totalling 260,985 word types.
After excluding tokens defined by the gold stan-
dard as punctuation, URLs, at-mentions, or emoti-
cons,21 22% of DAILY547?s tokens do not appear in
this dictionary. Without clusters, they are very dif-
ficult to classify (only 79.2% accuracy), but adding
clusters generates a 5.7 point improvement?much
larger than the effect on in-dictionary tokens (Ta-
ble 3).
Varying the amount of unlabeled data. A tagger
that only uses word clusters achieves an accuracy of
88.6% on the OCT27 development set.22 We created
several clusterings with different numbers of unla-
beled tweets, keeping the number of clusters con-
stant at 800. As shown in Fig. 3, there was initially
a logarithmic relationship between number of tweets
and accuracy, but accuracy (and lexical coverage)
levels out after 750,000 tweets. We use the largest
clustering (56 million tweets and 1,000 clusters) as
the default for the released tagger.
6.2 Evaluation on RITTERTW
Ritter et al (2011) annotated a corpus of 787
tweets23 with a single annotator, using the PTB
21We retain hashtags since by our guidelines a #-prefixed to-
ken is ambiguous between a hashtag and a normal word, e.g. #1
or going #home.
22The only observation features are the word clusters of a
token and its immediate neighbors.
23https://github.com/aritter/twitter_nlp/
blob/master/data/annotated/pos.txt
387
Tagger Accuracy
This work 90.0 ? 0.5
Ritter et al (2011), basic CRF tagger 85.3
Ritter et al (2011), trained on more data 88.3
Table 4: Accuracy comparison on Ritter et al?s Twitter
POS corpus (?6.2).
Tagger Accuracy
This work 93.4 ? 0.3
Forsyth (2007) 90.8
Table 5: Accuracy comparison on Forsyth?s NPSCHAT
IRC POS corpus (?6.3).
tagset plus several Twitter-specific tags, referred
to in Table 1 as RITTERTW. Linguistic concerns
notwithstanding (?5.2), for a controlled comparison,
we train and test our system on this data with the
same 4-fold cross-validation setup they used, attain-
ing 90.0% (?0.5%) accuracy. Ritter et al?s CRF-
based tagger had 85.3% accuracy, and their best tag-
ger, trained on a concatenation of PTB, IRC, and
Twitter, achieved 88.3% (Table 4).
6.3 IRC: Evaluation on NPSCHAT
IRC is another medium of online conversational
text, with similar emoticons, misspellings, abbrevi-
ations and acronyms as Twitter data. We evaluate
our tagger on the NPS Chat Corpus (Forsyth and
Martell, 2007),24 a PTB-part-of-speech annotated
dataset of Internet Relay Chat (IRC) room messages
from 2006.
First, we compare to a tagger in the same setup as
experiments on this data in Forsyth (2007), training
on 90% of the data and testing on 10%; we average
results across 10-fold cross-validation.25 The full
tagger model achieved 93.4% (?0.3%) accuracy,
significantly improving over the best result they re-
port, 90.8% accuracy with a tagger trained on a mix
of several POS-annotated corpora.
We also perform the ablation experiments on this
corpus, with a slightly different experimental setup:
we first filter out system messages then split data
24Release 1.0: http://faculty.nps.edu/
cmartell/NPSChat.htm
25Forsyth actually used 30 different 90/10 random splits; we
prefer cross-validation because the same test data is never re-
peated, thus allowing straightforward confidence estimation of
accuracy from the number of tokens (via binomial sample vari-
ance, footnote 19). In all cases, the models are trained on the
same amount of data (90%).
into 5,067 training and 2,868 test messages. Results
show a similar pattern as the Twitter data (see final
column of Table 2). Thus the Twitter word clusters
are also useful for language in the medium of text
chat rooms; we suspect these clusters will be appli-
cable for deeper syntactic and semantic analysis in
other online conversational text mediums, such as
text messages and instant messages.
7 Conclusion
We have constructed a state-of-the-art part-of-
speech tagger for the online conversational text
genres of Twitter and IRC, and have publicly re-
leased our new evaluation data, annotation guide-
lines, open-source tagger, and word clusters at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgements
This research was supported in part by the National Sci-
ence Foundation (IIS-0915187 and IIS-1054319).
A Part-of-Speech Tagset
N common noun
O pronoun (personal/WH; not possessive)
^ proper noun
S nominal + possessive
Z proper noun + possessive
V verb including copula, auxiliaries
L nominal + verbal (e.g. i?m), verbal + nominal (let?s)
M proper noun + verbal
A adjective
R adverb
! interjection
D determiner
P pre- or postposition, or subordinating conjunction
& coordinating conjunction
T verb particle
X existential there, predeterminers
Y X + verbal
# hashtag (indicates topic/category for tweet)
@ at-mention (indicates a user as a recipient of a tweet)
~ discourse marker, indications of continuation across
multiple tweets
U URL or email address
E emoticon
$ numeral
, punctuation
G other abbreviations, foreign words, possessive endings,
symbols, garbage
Table 6: POS tagset from Gimpel et al (2011) used in this
paper, and described further in the released annotation
guidelines.
388
References
J. Allan and H. Raghavan. 2002. Using part-of-speech
patterns to reduce query ambiguity. In Proc. of SIGIR.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech in-
duction. In Proc. of ACL.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proc. of EMNLP.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18(4).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hr-
uschka Jr, and T. M. Mitchell. 2010. Toward an archi-
tecture for never-ending language learning. In Proc. of
AAAI.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proc. of EACL.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
J. Eisenstein. 2013. What to do about bad language on
the internet. In Proc. of NAACL.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proc. of
EMNLP.
E. N. Forsyth and C. H. Martell. 2007. Lexical and dis-
course analysis of online chat dialog. In Proc. of ICSC.
E. N. Forsyth. 2007. Improving automated lexical and
discourse analysis of online chat dialog. Master?s the-
sis, Naval Postgraduate School.
J. Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan,
J. Nivre, D. Hogan, and J. van Genabith. 2011. #hard-
toparse: POS tagging and parsing the Twitterverse. In
Proc. of AAAI-11 Workshop on Analysing Microtext.
K. Gimpel, N. Schneider, B. O?Connor, D. Das, D. Mills,
J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan,
and N. A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proc. of ACL.
Google. 2012. Freebase data dumps. http://
download.freebase.com/datadumps/.
B. Han and T. Baldwin. 2011. Lexical normalisation of
short text messages: Makn sens a #twitter. In Proc. of
ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical programming, 45(1).
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recogniz-
ing named entities in tweets. In Proc. of ACL.
M. Lui and T. Baldwin. 2012. langid.py: An off-the-
shelf language identification tool. In Proc. of ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2).
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Proc. of
NAACL.
B. O?Connor, M. Krieger, and D. Ahn. 2010.
TweetMotif: exploratory search and topic summariza-
tion for Twitter. In Proc. of AAAI Conference on We-
blogs and Social Media.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel, and
N. Schneider. 2012. Part-of-speech tagging for Twit-
ter: Word clusters and other advances. Technical Re-
port CMU-ML-12-107, Carnegie Mellon University.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
S. Petrov and R. McDonald. 2012. Overview of the 2012
shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimental
study. In Proc. of EMNLP.
T. Schnoebelen. 2012. Do you smile with your nose?
Stylistic variation in Twitter emoticons. University of
Pennsylvania Working Papers in Linguistics, 18(2):14.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
O. T?ckstr?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proc. of NAACL.
389
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proc. of ACL.
P. D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
67(2):301?320.
390
Proceedings of NAACL-HLT 2013, pages 644?648,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Simple, Fast, and Effective Reparameterization of IBM Model 2
Chris Dyer Victor Chahuneau Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{cdyer,vchahune,nasmith}@cs.cmu.edu
Abstract
We present a simple log-linear reparame-
terization of IBM Model 2 that overcomes
problems arising from Model 1?s strong
assumptions and Model 2?s overparame-
terization. Efficient inference, likelihood
evaluation, and parameter estimation algo-
rithms are provided. Training the model is
consistently ten times faster than Model 4.
On three large-scale translation tasks, systems
built using our alignment model outperform
IBM Model 4.
An open-source implementation of the align-
ment model described in this paper is available
from http://github.com/clab/fast align .
1 Introduction
Word alignment is a fundamental problem in statis-
tical machine translation. While the search for more
sophisticated models that provide more nuanced ex-
planations of parallel corpora is a key research activ-
ity, simple and effective models that scale well are
also important. These play a crucial role in many
scenarios such as parallel data mining and rapid
large scale experimentation, and as subcomponents
of other models or training and inference algorithms.
For these reasons, IBM Models 1 and 2, which sup-
port exact inference in time ?(|f| ? |e|), continue to
be widely used.
This paper argues that both of these models are
suboptimal, even in the space of models that per-
mit such computationally cheap inference. Model
1 assumes all alignment structures are uniformly
likely (a problematic assumption, particularly for
frequent word types), and Model 2 is vastly overpa-
rameterized, making it prone to degenerate behav-
ior on account of overfitting.1 We present a simple
log-linear reparameterization of Model 2 that avoids
both problems (?2). While inference in log-linear
models is generally computationally more expen-
sive than in their multinomial counterparts, we show
how the quantities needed for alignment inference,
likelihood evaluation, and parameter estimation us-
ing EM and related methods can be computed using
two simple algebraic identities (?3), thereby defus-
ing this objection. We provide results showing our
model is an order of magnitude faster to train than
Model 4, that it requires no staged initialization, and
that it produces alignments that lead to significantly
better translation quality on downstream translation
tasks (?4).
2 Model
Our model is a variation of the lexical translation
models proposed by Brown et al (1993). Lexical
translation works as follows. Given a source sen-
tence f with length n, first generate the length of
the target sentence, m. Next, generate an alignment,
a = ?a1, a2, . . . , am?, that indicates which source
word (or null token) each target word will be a trans-
lation of. Last, generate the m output words, where
each ei depends only on fai .
The model of alignment configurations we pro-
pose is a log-linear reparameterization of Model 2.
1Model 2 has independent parameters for every alignment
position, conditioned on the source length, target length, and
current target index.
644
Given : f, n = |f|, m = |e|, p0, ?, ?
h(i, j,m, n) = ?
?
?
?
?
i
m ?
j
n
?
?
?
?
?(ai = j | i,m, n) =
?
??
??
p0 j = 0
(1? p0)? e
?h(i,j,m,n)
Z?(i,m,n)
0 < j ? n
0 otherwise
ai | i,m, n ? ?(? | i,m, n) 1 ? i ? m
ei | ai, fai ? ?(? | fai) 1 ? i ? m
null
j? = 1
j? = 2
j? = 3
j? = 4
j? = 5
i =
3
}
n
=
5
}
m = 6
i =
1
i =
2
i =
4
i =
5
i =
6
j?
j?
Figure 1: Our proposed generative process yielding a translation e and its alignment a to a source sentence f, given the
source sentence f, alignment parameters p0 and ?, and lexical translation probabilities ? (left); an example visualization
of the distribution of alignment probability mass under this model (right).
Our formulation, which we write as ?(ai = j |
i,m, n), is shown in Fig. 1.2 The distribution over
alignments is parameterized by a null alignment
probability p0 and a precision ? ? 0 which con-
trols how strongly the model favors alignment points
close to the diagonal. In the limiting case as ?? 0,
the distribution approaches that of Model 1, and, as
it gets larger, the model is less and less likely to de-
viate from a perfectly diagonal alignment. The right
side of Fig. 1 shows a graphical illustration of the
alignment distribution in which darker squares indi-
cate higher probability.
3 Inference
We now discuss two inference problems and give ef-
ficient techniques for solving them. First, given a
sentence pair and parameters, compute the marginal
likelihood and the marginal alignment probabilities.
Second, given a corpus of training data, estimate
likelihood maximizing model parameters using EM.
3.1 Marginals
Under our model, the marginal likelihood of a sen-
tence pair ?f, e? can be computed exactly in time
2Vogel et al (1996) hint at a similar reparameterization of
Model 2; however, its likelihood and its gradient are not effi-
cient to evaluate, making it impractical to train and use. Och
and Ney (2003) likewise remark on the overparameterization
issue, removing a single variable of the original conditioning
context, which only slightly improves matters.
?(|f| ? |e|). This can be seen as follows. For
each position in the sentence being generated, i ?
[1, 2, . . . ,m], the alignment to the source and its
translation is independent of all other translation and
alignment decisions. Thus, the probability that the
ith word of e is ei can be computed as:
p(ei, ai | f,m, n) = ?(ai | i,m, n)? ?(ei | fai)
p(ei | f,m, n) =
n?
j=0
p(ei, ai = j | f,m, n).
We can also compute the posterior probability over
alignments using the above probabilities,
p(ai | ei, f,m, n) =
p(ei, ai | f,m, n)
p(ei | f,m, n)
. (1)
Finally, since all words in e (and their alignments)
are conditionally independent,3
p(e | f) =
m?
i=1
p(ei | f,m, n)
=
m?
i=1
n?
j=0
?(ai | i,m, n)? ?(ei | fai).
3We note here that Brown et al (1993) derive their variant
of this expression by starting with the joint probability of an
alignment and translation, marginalizing, and then reorganizing
common terms. While identical in implication, we find the di-
rect probabilistic argument far more intuitive.
645
3.2 Efficient Partition Function Evaluation
Evaluating and maximizing the data likelihood un-
der log-linear models can be computationally ex-
pensive since this requires evaluation of normalizing
partition functions. In our case,
Z?(i,m, n) =
n?
j?=1
exp?h(i, j?,m, n).
While computing this sum is obviously possible
in ?(|f|) operations, our formulation permits exact
computation in ?(1), meaning our model can be ap-
plied even in applications where computational ef-
ficiency is paramount (e.g., MCMC simulations).
The key insight is that the partition function is the
(partial) sum of two geometric series of unnormal-
ized probabilities that extend up and down from the
probability-maximizing diagonal. The closest point
on or above the diagonal j?, and the next point down
j? (see the right side of Fig. 1 for an illustration), is
computed as follows:
j? =
? i? n
m
?
, j? = j? + 1.
Starting at j? and moving up the alignment col-
umn, as well as starting at j? and moving down, the
unnormalized probabilities decrease by a factor of
r = exp ??n per step.
To compute the value of the partition, we only
need to evaluate the unnormalized probabilities at
j? and j? and then use the following identity, which
gives the sum of the first ` terms of a geometric se-
ries (Courant and Robbins, 1996):
s`(g1, r) =
?`
k=1
g1rk?1 = g1
1? r`
1? r .
Using this identity, Z?(i,m, n) can be computed as
sj?(e?h(i,j?,m,n), r) + sn?j?(e?h(i,j?,m,n), r).
3.3 Parameter Optimization
To optimize the likelihood of a sample of parallel
data under our model, one can use EM. In the E-step,
the posterior probabilities over alignments are com-
puted using Eq. 1. In the M-step, the lexical trans-
lation probabilities are updated by aggregating these
as counts and normalizing (Brown et al, 1993). In
the experiments reported in this paper, we make the
further assumption that ?f ? Dirichlet(?) where
?i = 0.01 and approximate the posterior distribu-
tion over the ?f ?s using a mean-field approximation
(Riley and Gildea, 2012).4
During the M-step, the ? parameter must also
be updated to make the E-step posterior distribu-
tion over alignment points maximally probable un-
der ?(? | i,m, n). This maximizing value cannot
be computed analytically, but a gradient-based op-
timization can be used, where the first derivative
(here, for a single target word) is:
??L = Ep(ai|ei,f,m,n) [h(i, ai,m, n)]
? E?(j?|i,m,n)
[
h(i, j?,m, n)
]
(2)
The first term in this expression (the expected value
of h under the E-step posterior) is fixed for the du-
ration of each M-step, but the second term?s value
(the derivative of the log-partition function) changes
many times as ? is optimized.
3.4 Efficient Gradient Evaluation
Fortunately, like the partition function, the deriva-
tive of the log-partition function (i.e., the second
term in Eq. 2) can be computed in constant time us-
ing an algebraic identity. To derive this, we observe
that the values of h(i, j?,m, n) form an arithmetic
sequence about the diagonal, with common differ-
ence d = ?1/n. Thus, the quantity we seek is the
sum of a series whose elements are the products of
terms from an arithmetic sequence and those of the
geometric sequence above, divided by the partition
function value. This construction is referred to as
an arithmetico-geometric series, and its sum may be
computed as follows (Fernandez et al, 2006):
t`(g1,a1, r, d) =
?`
k=1
[a1 + d(k ? 1)] g1rk?1
= a`g`+1 ? a1g11? r +
d (g`+1 ? g1r)
(1? r)2 .
In this expression r, the g1?s and the `?s have the
same values as above, d = ?1/n and the a1?s are
4The ?i value was fixed at the beginning of experimentation
by minimizing the AER on the 10k sentence French-English cor-
pus discussed below.
646
equal to the value of h evaluated at the starting in-
dices, j? and j?; thus, the derivative we seek at each
optimization iteration inside the M-step is:
??L =Ep(ai|ei,f,m,n) [h(i, ai,m, n)]
? 1Z?
(tj?(e?h(i,j?,m,n), h(i, j?,m, n), r, d)
+ tn?j?(e?h(i,j?,m,n), h(i, j?,m, n), r, d)).
4 Experiments
In this section we evaluate the performance of
our proposed model empirically. Experiments are
conducted on three datasets representing different
language typologies and dataset sizes: the FBIS
Chinese-English corpus (LDC2003E14); a French-
English corpus consisting of version 7 of the Eu-
roparl and news-commentary corpora;5 and a large
Arabic-English corpus consisting of all parallel data
made available for the NIST 2012 Open MT evalua-
tion. Table 1 gives token counts.
We begin with several preliminary results. First,
we quantify the benefit of using the geometric series
trick (?3.2) for computing the partition function rel-
ative to na??ve summation. Our method requires only
0.62 seconds to compute all partition function values
for 0 < i,m, n < 150, whereas the na??ve algorithm
requires 6.49 seconds for the same.6
Second, using a 10k sample of the French-English
data set (only 0.5% of the corpus), we determined
1) whether p0 should be optimized; 2) what the op-
timal Dirichlet parameters ?i are; and 3) whether
the commonly used ?staged initialization? procedure
(in which Model 1 parameters are used to initialize
Model 2, etc.) is necessary for our model. First,
like Och and Ney (2003) who explored this issue for
training Model 3, we found that EM tended to find
poor values for p0, producing alignments that were
overly sparse. By fixing the value at p0 = 0.08,
we obtained minimal AER. Second, like Riley and
Gildea (2012), we found that small values of ? im-
proved the alignment error rate, although the im-
pact was not particularly strong over large ranges of
5http://www.statmt.org/wmt12
6While this computational effort is a small relative to the
total cost in EM training, in algorithms where ? changes more
rapidly, for example in Bayesian posterior inference with Monte
Carlo methods (Chahuneau et al, 2013), this savings can have
substantial value.
Table 1: CPU time (hours) required to train alignment
models in one direction.
Language Pair Tokens Model 4 Log-linear
Chinese-English 17.6M 2.7 0.2
French-English 117M 17.2 1.7
Arabic-English 368M 63.2 6.0
Table 2: Alignment quality (AER) on the WMT 2012
French-English and FBIS Chinese-English. Rows with
EM use expectation maximization to estimate the ?f , and
?Dir use variational Bayes.
Model Estimator FR-EN ZH-EN
Model 1 EM 29.0 56.2
Model 1 ?Dir 26.6 53.6
Model 2 EM 21.4 53.3
Log-linear EM 18.5 46.5
Log-linear ?Dir 16.6 44.1
Model 4 EM 10.4 45.8
Table 3: Translation quality (BLEU) as a function of
alignment type.
Language Pair Model 4 Log-linear
Chinese-English 34.1 34.7
French-English 27.4 27.7
Arabic-English 54.5 55.7
?. Finally, we (perhaps surprisingly) found that the
standard staged initialization procedure was less ef-
fective in terms of AER than simply initializing our
model with uniform translation probabilities and a
small value of ? and running EM. Based on these
observations, we fixed p0 = 0.08, ?i = 0.01, and
set the initial value of ? to 4 for the remaining ex-
periments.7
We next compare the alignments produced by our
model to the Giza++ implementation of the standard
IBM models using the default training procedure
and parameters reported in Och and Ney (2003).
Our model is trained for 5 iterations using the pro-
cedure described above (?3.3). The algorithms are
7As an anonymous reviewer pointed out, it is a near certainty
that tuning of these hyperparameters for each alignment task
would improve results; however, optimizing hyperparameters of
alignment models is quite expensive. Our intention is to show
that it is possible to obtain reasonable (if not optimal) results
without careful tuning.
647
compared in terms of (1) time required for training;
(2) alignment error rate (AER, lower is better);8 and
(3) translation quality (BLEU, higher is better) of hi-
erarchical phrase-based translation system that used
the alignments (Chiang, 2007). Table 1 shows the
CPU time in hours required for training (one direc-
tion, English is generated). Our model is at least
10? faster to train than Model 4. Table 3 reports
the differences in BLEU on a held-out test set. Our
model?s alignments lead to consistently better scores
than Model 4?s do.9
5 Conclusion
We have presented a fast and effective reparameteri-
zation of IBM Model 2 that is a compelling replace-
ment for for the standard Model 4. Although the
alignment quality results measured in terms of AER
are mixed, the alignments were shown to work ex-
ceptionally well in downstream translation systems
on a variety of language pairs.
Acknowledgments
This work was sponsored by the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533.
References
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
V. Chahuneau, N. A. Smith, and C. Dyer. 2013.
Knowledge-rich morphological priors for Bayesian
language models. In Proc. NAACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
R. Courant and H. Robbins. 1996. The geometric pro-
gression. In What Is Mathematics?: An Elementary
Approach to Ideas and Methods, pages 13?14. Oxford
University Press.
8Our Arabic training data was preprocessed using a seg-
mentation scheme optimized for translation (Habash and Sadat,
2006). Unfortunately the existing Arabic manual alignments
are preprocessed quite differently, so we did not evaluate AER.
9The alignments produced by our model were generally
sparser than the corresponding Model 4 alignments; however,
the extracted grammar sizes were sometimes smaller and some-
times larger, depending on the language pair.
P. A. Fernandez, T. Foregger, and J. Pahikkala. 2006.
Arithmetic-geometric series. PlanetMath.org.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Riley and D. Gildea. 2012. Improving the IBM align-
ment models using Variational Bayes. In Proc. of ACL.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
648
Proceedings of NAACL-HLT 2013, pages 661?667,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supersense Tagging for Arabic: the MT-in-the-Middle Attack
Nathan Schneider? Behrang Mohit? Chris Dyer? Kemal Oflazer? Noah A. Smith?
School of Computer Science
Carnegie Mellon University
?Pittsburgh, PA 15213, USA
?Doha, Qatar
{nschneid@cs.,behrang@,cdyer@cs.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the task of tagging Arabic nouns
with WordNet supersenses. Three approaches
are evaluated. The first uses an expert-
crafted but limited-coverage lexicon, Arabic
WordNet, and heuristics. The second uses un-
supervised sequence modeling. The third and
most successful approach uses machine trans-
lation to translate the Arabic into English,
which is automatically tagged with English
supersenses, the results of which are then pro-
jected back into Arabic. Analysis shows gains
and remaining obstacles in four Wikipedia
topical domains.
1 Introduction
A taxonomic view of lexical semantics groups word
senses/usages into categories of varying granulari-
ties. WordNet supersense tags denote coarse seman-
tic classes, including person and artifact (for nouns)
and motion and weather (for verbs); these categories
can be taken as the top level of a taxonomy. Nominal
supersense tagging (Ciaramita and Johnson, 2003)
is the task of identifying lexical chunks in the sen-
tence for common as well as proper nouns, and la-
beling each with one of the 25 nominal supersense
categories. Figure 1 illustrates two such labelings of
an Arabic sentence. Like the narrower problem of
named entity recognition, supersense tagging of text
holds attraction as a way of inferring representations
that move toward language independence. Here we
consider the problem of nominal supersense tagging
for Arabic, a language with ca. 300 million speak-
ers and moderate linguistic resources, including a
WordNet (Elkateb et al, 2006), annotated datasets
(Maamouri et al, 2004; Hovy et al, 2006), monolin-
gual corpora, and large amounts of Arabic-English
parallel data.
The supervised learning approach that is used
in state-of-the-art English supersense taggers (Cia-
. HA

?J
J.?

J? @
	
Y
	
? @?
	
K ?? ?? ?
	
?? ?


	
?
	
Y
	
? @?
	
J? @ QK
Y? ??j

JK

Ann-A Gloss Ann-B
controls
communication
manager
communicationthe-windows
in
attribute configuration relation
shape and-layout shape
communication
windows
communicationthe-applications
?The window manager controls the configuration and
layout of application windows.?
Figure 1: A sentence from the ?X Window System? ar-
ticle with supersense taggings from two annotators and
post hoc English glosses and translation.
ramita and Altun, 2006) is problematic for Ara-
bic, since there are supersense annotations for only
a small amount of Arabic text (65,000 words by
Schneider et al, 2012, versus the 360,000 words that
are annotated for English). Here, we reserve that
corpus for development and evaluation, not training.
We explore several approaches in this paper, the
most effective of which is to (1) translate the Arabic
sentence into English, returning the alignment struc-
ture between the source and target, (2) apply En-
glish supersense tagging to the target sentence, and
(3) heuristically project the tags back to the Arabic
sentence across these alignments. This ?MT-in-the-
middle? approach has also been successfully used
for mention detection (Zitouni and Florian, 2008)
and coreference resolution (Rahman and Ng, 2012).
We first discuss the task and relevant resources
(?2), then the approaches we explored (?3), and fi-
nally present experimental results and analysis in ?4.
2 Task and Resources
A gold standard corpus of sentences annotated
with nominal supersenses (as in figure 1) fa-
cilitates automatic evaluation of supersense tag-
gers. For development and evaluation we use
661
the AQMAR Arabic Wikipedia Supersense Corpus1
(Schneider et al, 2012), which augmented a named
entity corpus (Mohit et al, 2012) with nominal
supersense tags. The corpus consists of 28 ar-
ticles selected from four topical areas: history
(e.g., ?Islamic Golden Age?), science (?Atom?),
sports (?Real Madrid?), and technology (?Linux?).
Schneider et al (2012) found the distributions of
supersense categories in these four topical domains
to be markedly different; e.g., most instances of
communication (which includes kinds of software)
occurred in the technology domain, whereas most
substances were found in the science domain.
The 18 test articles have 1,393 sentences (39,916
tokens) annotated at least once.2 As the corpus
was released with two annotators? (partially overlap-
ping) taggings, rather than a single gold standard,
we treat the output of each annotator as a separate
test set. Both annotated some of every article; the
first (Ann-A) annotated 759 sentences, the second
(Ann-B) 811 sentences.
Lexicon. What became known as ?supersense
tags? arose from a high-level partitioning of synsets
in the original English WordNet (Fellbaum, 1998)
into lexicographer files. Arabic WordNet (AWN)
(Elkateb et al, 2006) allows us to recover super-
sense categories for some 10,500 Arabic nominal
types, since many of the synsets in AWN are cross-
referenced to English WordNet, and can therefore
be associated with supersense categories. Further,
OntoNotes contains named entity annotations for
Arabic (Hovy et al, 2006).
From these, we construct an Arabic supersense
lexicon, mapping Arabic noun lemmas to supersense
tags. This lexicon contains 23,000 types, of which
11,000 are multiword units. Token coverage of the
test set is 18% (see table 1). Lexical units encoun-
tered in the test data were up to 9-ways supersense-
ambiguous; the average ambiguity of in-vocabulary
tokens was 2.0 supersenses.
Unlabeled Arabic text. For unsupervised learn-
ing we collected 100,000 words of Arabic Wikipedia
text, not constrained by topic. The articles in this
sample were subject to a minimum length threshold
1http://www.ark.cs.cmu.edu/ArabicSST
2Our development/test split of the data follows Mohit et al
(2012), but we exclude two test set documents??Light? and
?Ibn Tolun Mosque??due to preprocessing issues.
and are all cross-linked to corresponding articles in
English, Chinese, and German.
Arabic?English machine translation. We used
two independently developed Arabic-English MT
systems. One (QCRI) is a phrase-based system
(Koehn et al, 2003), similar to Moses (Koehn et
al., 2007); the other (cdec) is a hierarchical phrase-
based system (Chiang, 2007), as implemented in
cdec (Dyer et al, 2010). Both were trained on
about 370M tokens of parallel data provided by the
LDC (by volume, mostly newswire and UN data).
Each system includes preprocessing for Arabic mor-
phological segmentation and orthographic normal-
ization.3 The QCRI system used a 5-gram modi-
fied Kneser-Ney language model that generated full-
cased forms (Chen and Goodman, 1999). cdec
used a 4-gram KN language model over lowercase
forms and was recased in a post-processing step.
Both language models were trained using the Giga-
word v. 4 corpus. Both systems were tuned to opti-
mize BLEU on a held-out development set (Papineni
et al, 2002).
English supersense tagger. For English super-
sense tagging, an open-source reimplementation of
the approach of Ciaramita and Altun (2006) was
released by Michael Heilman.4 This tagger was
trained on the SemCor corpus (Miller et al, 1993)
and achieves 77% F1 in-domain.
3 Methods
We explored 3 approaches to the supersense tagging
of Arabic: heuristic tagging with a lexicon, unsuper-
vised sequence tagging, and MT-in-the-middle.
3.1 Heuristic Tagging with a Lexicon
Using the lexicon built from AWN and OntoNotes
(see ?2), our heuristic approach works as follows:
1. Stem and vocalize; we used MADA (Habash
and Rambow, 2005; Roth et al, 2008).
2. Greedily detect word sequences matching lexi-
con entries from left to right.
3. If a lexicon entry has more than one associated
supersense, Arabic WordNet synsets are
3QCRI accomplishes this using MADA (Habash and Ram-
bow, 2005; Roth et al, 2008). cdec includes a custom CRF-
based segmenter and standard normalization rules.
4http://www.ark.cs.cmu.edu/mheilman/questions
662
E? person location artifact substance Automatic English supersense tagging
e? 1 2 3 4 5 6 7 8 9 English sentence
a 1 2 3 4 5 6 Arabic sentence (e.g., token 6 aligns to English tokens 7?9)
N P N A N N Arabic POS tagging
A? person location artifact Projected supersense tagging
Figure 2: A hypothetical aligned sentence pair of 9 English words (with their supersense tags) and 6 Ara-
bic words (with their POS tags). Step 4 of the projection procedure constructs the Arabic-to-English mapping
{1?person11, 4?location
4
3, {5, 6}?artifact
7
6}, resulting in the tagging shown in the bottom row.
weighted to favor earlier senses (presumed
by lexicographers to be more frequent) and
then the supersense with the greatest aggregate
weight is selected. Formally: Let senses(w) be
the ordered list of AWN senses of lemma w.
Let senses(w, s) ? senses(w) be those senses
that map to a given supersense s. We choose
arg maxs(|senses(w, s)|/ mini:senses(w)i?senses(w,s) i).
3.2 Unsupervised Sequence Models
Unsupervised sequence labeling is our second ap-
proach (Merialdo, 1994). Although it was largely
developed for part-of-speech tagging, the hope is
to use in-domain Arabic data (the unannotated
Wikipedia corpus discussed in ?2) to infer clus-
ters that correlate well with supersense groupings.
We applied the generative, feature-based model of
Berg-Kirkpatrick et al (2010), replicating a feature-
set used previously for NER (Mohit et al, 2012)?
including context tokens, character n-grams, and
POS?and adding the vocalized stem and several
stem shape features: 1) ContainsDigit?; 2) dig-
its replaced by #; 3) digit sequences replaced by
# (for stems mixing digits with other characters);
4) YearLike??true for 4-digit numerals starting with
19 or 20; 5) LatinWord?, per the morphological an-
alysis; 6) the shape feature of Ciaramita and Al-
tun (2006) (Latin words only). We used 50 itera-
tions of learning (tuned on dev data). For evaluation,
a many-to-one mapping from unsupervised clusters
to supersense tags is greedily induced to maximize
their correspondence on evaluation data.
3.3 MT-in-the-Middle
A standard approach to using supervised linguistic
resources in a second language is cross-lingual pro-
jection (Yarowsky and Ngai, 2001; Yarowsky et al,
2001; Smith and Smith, 2004; Hwa et al, 2005; Mi-
halcea et al, 2007; Burkett and Klein, 2008; Burkett
et al, 2010; Das and Petrov, 2011; Kim et al, 2012,
who use parallel sentences extracted from Wikipedia
for NER). The simplest such approach starts with an
aligned parallel corpus, applies supersense tagging
to the English side, and projects the labels through
the word alignments. A supervised monolingual tag-
ger is then trained on the projected labels. Prelimi-
nary experiments, however, showed that this under-
performed even the simple heuristic baseline above
(likely due to domain mismatch), so it was aban-
doned in favor of a technique that we call MT-in-
the-middle projection.
This approach does not depend on having par-
allel data in the training domain, but rather on an
Arabic?English machine translation system that
can be applied to the sentences we wish to tag. The
approach is inspired by token-level pseudo-parallel
data methods of previous work (Zitouni and Flo-
rian, 2008; Rahman and Ng, 2012). MT output for
this language pair is far from perfect?especially for
Wikipedia text, which is distant from the domain
of the translation system?s training data?but, in the
spirit of Church and Hovy (1993), we conjecture that
it may still be useful. The method is as follows:
1. Preprocess the input Arabic sentence a to
match the decoder?s model of Arabic.
2. Translate the sentence, recovering not just
the English output e? but also the deriva-
tion/alignment structure z relating words and/or
phrases of the English output to words and/or
phrases of the Arabic input.
3. Apply the English supersense tagger to the En-
glish translation, discarding any verbal super-
sense tags. Call the tagger output E?.
4. Project the supersense tags back to the Ara-
bic sentence, yielding A?: Each Arabic token
a ? a that is (a) a noun, or (b) an adjec-
tive following 0 or more adjectives following a
noun is mapped to the first English supersense
mention in E? containing some word aligned
to a. Then, for each English supersense men-
663
Coverage Ann-A Ann-B
Nouns All Tokens Mentions P R F1 P R F1
Lexicon heuristics (?3.1) 8,058 33% 8,465 18% 8,407 32 55 16 29 21.6 37.9 29 53 15 27 19.4 35.6
Unsupervised (?3.2) 20 59 16 48 17.5 52.6 14 56 10 39 11.6 45.9
MT-in-the-middle
(?3.3)
QCRI 14,401 59% 16,461 35% 12,861 34 65 27 50 29.9 56.4 36 64 28 51 31.6 56.6
cdec 14,270 58% 15,542 33% 13,704 37 69 31 57 33.8 62.4 38 67 32 56 34.6 61.0
MTitM + Lex. cdec 16,798 68% 18,461 40% 16,623 35 64 36 65 35.5 64.6 36 63 36 63 36.0 63.2
Table 1: Supersense tagging results on the test set: coverage measures5 and gold-standard evaluation?exact la-
beled/unlabeled6 mention precision, recall, and F-score against each annotator. The last row is a hybrid: MT-in-the-
middle followed by lexicon heuristics to improve recall. Best single-technique and best hybrid results are bolded.
tion, all its mapped Arabic words are grouped
into a single mention and the supersense cat-
egory for that mention is projected. Figure 2
illustrates this procedure. The cdec system
provides word alignments for its translations
derived from the training data; whereas QCRI
only produces phrase-level alignments, so for
every aligned phrase pair ?a?, e?? ? z, we con-
sider every word in a? as aligned to every word
in e? (introducing noise when English super-
sense mention boundaries do not line up with
phrase boundaries).
4 Experiments and Analysis
Table 1 compares the techniques (?3) for full Arabic
supersense tagging.7 The number of nouns, tokens,
and mentions covered by the automatic tagging is
reported, as is the mention-level evaluation against
human annotations. The evaluation is reported sep-
arately for the two annotators in the dataset.
With heuristic lexicon lookup, 18% of the tokens
are marked as part of a nominal supersense mention.
Both labeled and unlabeled mention recall with this
method are below 30%; labeled precision is about
30%, and unlabeled mention precision is above
50%. From this we conclude that the biggest prob-
lems are (a) out-of-vocabulary items and (b) poor
semantic disambiguation of in-vocabulary items.
The unsupervised sequence tagger does even
worse on the labeled evaluation. It has some success
at detecting supersense mentions?unlabeled recall
is substantially improved, and unlabeled precision is
5The unsupervised evaluation greedily maps clusters to tags,
separately for each version of the test set; coverage numbers
thus differ and are not shown here.
6Unlabeled tagging refers to noun chunk detection only.
7It was produced in part using the chunkeval.py script: see
https://github.com/nschneid/pyutil
slightly improved. But it seems to be much worse
at assigning semantic categories; the number of la-
beled true positive mentions is actually lower than
with the lexicon-based approach.
MT-in-the-middle is by far the most success-
ful single approach: both systems outperform the
lexicon-only baseline by about 10 F1 points, de-
spite many errors in the automatic translation, En-
glish tagging, and projection, as well as underlying
linguistic differences between English and Arabic.
The baseline?s unlabeled recall is doubled, indicat-
ing substantially more nominal expressions are de-
tected, in addition to the improved labeled scores.
We further tested simple hybrids combining the
lexicon-based and MT-based approaches. Applying
MT-in-the-middle first, then expanding token cover-
age with the lexicon improves recall at a small cost
to precision (table 1, last row). Combining the tech-
niques in the reverse order is slightly worse than MT-
based projection without consulting the lexicon.
MT-in-the middle improves upon the lexicon-only
baseline, yet performance is still dwarfed by the su-
pervised English tagger (at least in the SemCor eval-
uation; see ?2), and also well below the 70% inter-
annotator F1 reported by Schneider et al (2012). We
therefore examine the weaknesses of our approach
for Arabic.
4.1 MT for Projection
In analyzing our projection framework, we per-
formed a small-scale MT evaluation with the
Wikipedia data. Reference English translations for
140 Arabic Wikipedia sentences?5 per article in
the corpus?were elicited from a bilingual linguist.
Table 2 compares the two systems under three stan-
dard metrics of overall sentence translation quality.8
8BLEU (Papineni et al, 2002); METEOR (Banerjee and
Lavie, 2005; Lavie and Denkowski, 2009), with default options;
664
. ????@ ?


	
? @Yg.

?Q

	??

?
	
Jj ??@

?J.k. ??

?@?
	
K ??k ??mProceedings of NAACL-HLT 2013, pages 1206?1215,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Knowledge-Rich Morphological Priors for Bayesian Language Models
Victor Chahuneau Noah A. Smith Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,nasmith,cdyer}@cs.cmu.edu
Abstract
We present a morphology-aware nonparamet-
ric Bayesian model of language whose prior
distribution uses manually constructed finite-
state transducers to capture the word forma-
tion processes of particular languages. This
relaxes the word independence assumption
and enables sharing of statistical strength
across, for example, stems or inflectional
paradigms in different contexts. Our model
can be used in virtually any scenario where
multinomial distributions over words would
be used. We obtain state-of-the-art results in
language modeling, word alignment, and un-
supervised morphological disambiguation for
a variety of morphologically rich languages.
1 Introduction
Despite morphological phenomena?s salience in
most human languages, many NLP systems treat
fully inflected forms as the atomic units of language.
By assuming independence of lexical stems? vari-
ous surface forms, this avoidance approach exacer-
bates the problem of data sparseness. If it is em-
ployed at all, morphological analysis of text tends
to be treated as a preprocessing step to other NLP
modules. While this latter disambiguation approach
helps address data sparsity concerns, it has substan-
tial drawbacks: it requires supervised learning from
expert-annotated corpora, and determining the op-
timal morphological granularity is labor-intensive
(Habash and Sadat, 2006).
Neither approach fully exploits the finite-state
transducer (FST) technology that has been so suc-
cessful for modeling the mapping between surface
forms and their morphological analyses (Karttunen
and Beesley, 2005), and the mature collections of
high quality transducers that already exist for many
languages (e.g., Turkish, Russian, Arabic). Much
linguistic knowledge is encoded in such FSTs.
In this paper, we develop morphology-aware non-
parametric Bayesian language models that bring to-
gether hand-written FSTs with statistical modeling
and require no token-level annotation. The sparsity
issue discussed above is addressed by hierarchical
priors that share statistical strength across different
inflections of the same stem by backing off to word
formation models that piece together morphemes us-
ing FSTs. Furthermore, because of the nonparamet-
ric formulation of our models, the regular morpho-
logical patterns found in the long tail of word types
will rely more heavily on deeper analysis, while fre-
quent and idiosyncratically behaved forms are mod-
eled opaquely.
Our prior can be used in virtually any generative
model of language as a replacement for multino-
mial distributions over words, bringing morphologi-
cal awareness to numerous applications. For various
morphologically rich languages, we show that:
? our model can provide rudimentary unsuper-
vised disambiguation for a highly ambiguous
analyzer;
? integrating morphology into n-gram language
models allows better generalization to unseen
words and can improve the performance of ap-
plications that are truly open vocabulary; and
? bilingual word alignment models also bene-
fit greatly from sharing translation information
1206
across stems.
We are particularly interested in low-resource sce-
narios, where one has to make the most of the
small quantity of available data, and overcoming
data sparseness is crucial. If analyzers exist in such
settings, they tend to be highly ambiguous, and an-
notated data for learning to disambiguate are also
likely to be scarce or non-existent. Therefore, in our
experiments with Russian, we compare two analyz-
ers: a rapidly-developed guesser, which models reg-
ular inflectional paradigms but contains no lexicon
or irregular forms, and a high-quality analyzer.
2 Word Models with Morphology
In this section, we describe a generative model of
word formation based on Pitman-Yor processes that
generates word types using a finite-state morpho-
logical generator. At a high level, the process first
produces lexicons of stems and inflectional patterns;
then it generates a lexicon of inflected forms us-
ing the finite-state generator. Finally, the inflected
forms are used to generate observed data. Different
independence assumptions can be made at each of
these levels to encode beliefs about where stems, in-
flections, and surface forms should share statistical
strength.
2.1 Pitman-Yor Processes
Our work relies extensively on Pitman-Yor pro-
cesses, which provide a flexible framework for ex-
pressing backoff and interpolation relationships and
extending standard models with richer word distri-
butions (Pitman and Yor, 1997). They have been
shown to match the performance of state-of-the-art
language models and to give estimates that follow
appropriate power laws (Teh, 2006).
A draw from a Pitman-Yor process (PYP), de-
noted G ? PY(d, ?,G0), is a discrete distribution
over a (possibly infinite) set of events, which we de-
note abstractly E . The process is parameterized by a
discount parameter 0 ? d < 1, a strength parameter
? > ?d, and a base distribution G0 over the event
space E .
In this work, our focus is on the base distribution
G0. We place vague priors on the hyperparameters
d ? U([0, 1]) and (? + d) ? Gamma(1, 1). Infer-
ence in PYPs is discussed below.
2.2 Unigram Morphology Model
The most basic expression of our model is a uni-
gram model of text. So far, we only assume that
each word can be analyzed into a stem and a se-
quence of morphemes forming an inflection pattern.
LetGs be a distribution over stems,Gp be a distribu-
tion over inflectional patterns, and let GENERATE be
a deterministic mapping from ?stem, pattern? pairs
to inflected word forms.1 An inflected word type is
generated with the following process, which we des-
ignate MP(Gs, Gd,GENERATE):
stem ? Gs
pattern ? Gp
word = GENERATE(stem, pattern)
For example, in Russian, we might sample stem
= ??????,2 pattern = STEM+Adj+Pl+Dat, and
obtain word = ??????.
This model could be used directly to generate ob-
served tokens. However, we have said nothing about
Gs and Gp, and the assumption that stems and pat-
terns are independent is clearly unsatisfying. We
therefore assume that both the stem and the pattern
distributions are generated from PY processes, and
that MP(Gs, Gp,GENERATE) is itself the base dis-
tribution of a PYP.
Gs ? PY(ds, ?s, G0s)
Gp ? PY(dp, ?p, G0p)
Gw ? PY(d, ?,MP(Gs, Gp,GENERATE))
A draw Gw from this PYP is a unigram distribu-
tion over tokens.
2.3 Base Stem Model G0s
In general there are an unbounded number of stems
possible in any language, so we set G0s to be charac-
ter trigram model, which we statically estimate, with
Kneser-Ney smoothing, from a large corpus of word
types in the language being modeled. While using
fixed parameters estimated to maximize likelihood is
1The assumption of determinism is only inappropriate in
cases of inflectional spelling variants (e.g., modeled vs. mod-
elled) or pronunciation variants (e.g., reduced forms in certain
environments).
2?????? (pronounced [pr5tCij]) = other
1207
questionable from the perspective of Bayesian learn-
ing, it is tremendously beneficial for computational
reasons. For some applications (e.g., word align-
ment), the set of possible stems for a corpus S can be
precomputed, so we will also experiment with using
a uniform stem distribution based on this set.
2.4 Base Pattern Model G0p
Several choices are possible for the base pattern dis-
tribution:
MP0 We can assume a uniformG0p when the num-
ber of patterns is small.
MP1 To be able to generalize to new patterns, we
can draw the length of the pattern from a Poisson
distribution and generate morphemes one by one
from a uniform distribution.
MP2 A more informative prior is a Markov chain
of morphemes, where each morpheme is generated
conditional on the preceding morpheme.
The choice of the base pattern distribution could
depend on the complexity of the inflectional patterns
produced by the morphological analyzer, reflecting
the type of morphological phenomena present in a
given language. For example, the number of possi-
ble patterns can practically be considered finite in
Russian, but this assumption is not valid for lan-
guages with more extensive derivational morphol-
ogy like Turkish.
2.5 Posterior Inference
For most applications, rather than directly gener-
ating from a model using the processes outlined
above, we seek to infer posterior distributions over
latent parameters and structures, given a sample of
data.
Although there is no known analytic form of
the PYP density, it is possible to marginalize the
draws from it and to work directly with observa-
tions. This marginalization produces the classi-
cal Chinese restaurant process representation (Teh,
2006). When working with the morphology mod-
els we are proposing, we also need to marginalize
the different latent forms (stems s and patterns p)
that may have given rise to a given word w. Thus,
we require that the inverse relation of GENERATE is
available to compute the marginal base word distri-
bution:
p(w | G0w) =
?
GENERATE(s,p)=w
p(s | Gs) p(p | Gp)
Since our approach encodes morphology using
FSTs, which are invertible, this poses no problem.
To illustrate, consider the Russian word ??????,
which may be analyzed in several ways:
?????? +Adj +Sg +Neut +Instr
?????? +Adj +Sg +Masc +Instr
?????? +Adj +Pl +Dat
??????? +Verb +Pl +1P
?????? +Pro +Sg +Ins
Because the set of possible analyses is in general
small, marginalization is fast and complex blocked
sampling is not necessary.
Finally, to infer hyperparameter values (d, ?, . . .),
a Metropolis-Hastings update is interleaved with
Gibbs sampling steps for the rest of the hidden vari-
ables.3
Having described a model for generating words,
we now show its usage in several contexts.
3 Unsupervised Morphological
Disambiguation
Given a rule-based morphological analyzer encoded
as an unweighted FST and a corpus on which the
analyzer has been run ? possibly generating multi-
ple analyses for each token ? we can use our un-
igram model to learn a probabilistic model of dis-
ambiguation in an unsupervised setting (i.e., with-
out annotated examples). The corpus is assumed to
be generated from the unigram distribution Gw, and
the base stem model is set to a fixed character tri-
gram model.4 After learning the parameters of the
model, we can find for each word in the vocabulary
its most likely analysis and use this as a crude dis-
ambiguation step.
3The proposal distribution for Metropolis-Hastings is a Beta
distribution (d) or a Gamma distribution (?+d) centered on the
previous parameter values.
4Experiments suggest that this is important to constrain the
model to realistic stems.
1208
3.1 Morphological Guessers
Finite-state morphological analyzers are usually
specified in three parts: a stem lexicon, which de-
fines the words in the language and classifies them
into several categories according to their grammat-
ical function and their morphological properties; a
set of prefixes and suffixes that can be applied to
each category to form surface words; and possibly
alternation rules that can encode exceptions and
spelling variations. The combination of these parts
provides a powerful framework for defining a gener-
ative model of words. Such models can be reversed
to obtain an analyzer. However, while the two latter
parts can be relatively easy to specify, enumerating
a comprehensive stem lexicon is a time consuming
and necessarily incomplete process, as some cate-
gories are truly open-class.
To allow unknown words to be analyzed, one
can use a guesser that attempts to analyze words
missing in the lexicon. Can we eliminate the stem
lexicon completely and use only the guesser? This
is what we try to do by designing a lexicon-free
analyzer for Russian. A guesser was developed
in three hours; it is prone to over-generation and
produces ambiguous analyses for most words
but covers a large number of morphological phe-
nomena (gender, case, tense, etc.). For example,
the word ??????5 can be correctly analyzed as
?????+Noun+Masc+Prep+Sg but also as the in-
correct forms: ??????+Verb+Pres+2P+Pl,
??????+Noun+Fem+Dat+Sg, ????-
??+Noun+Fem+Prep+Sg, and more.
3.2 Disambiguation Experiments
We train the unigram model on a 1.7M-word cor-
pus of TED talks transcriptions translated into Rus-
sian (Cettolo et al, 2012) and evaluate our ana-
lyzer against a test set consisting of 1,500 gold-
standard analyses obtained from the morphology
disambiguation task of the DIALOG 2010 confer-
ence (Lya?evskaya et al, 2010).6
Each analysis is composed of a lemma (?????),
a part of speech (Noun), and a sequence of ad-
ditional functional morphemes (Masc,Prep,Sg).
We consider only open-class categories: nouns, ad-
5?????? = Hebrew (masculine noun, prepositional case)
6http://ru-eval.ru
jectives, adverbs and verbs, and evaluate the output
of our model with three metrics: the lemma accu-
racy, the part-of-speech accuracy, and the morphol-
ogy F -measure.7
As a baseline, we consider picking a random anal-
ysis from output of the analyzer or choosing the
most frequent lemma and the most frequent morpho-
logical pattern.8 Then, we use our model with each
of the three versions of the pattern model described
in ?2.2. Finally, as an upper bound, we use the gold
standard to select one of the analyses produced by
the guesser.
Since our evaluation is not directly comparable
to the standard for this task, we use for reference
a high-quality analyzer from Xerox9 disambiguated
with the MP0 model (all of the models have very
close accuracy in this case).
Model Lemma POS Morph.
Random 29.8 70.9 50.2
Frequency 31.1 74.4 48.8
Guesser MP0 60.0 82.2 66.3
Guesser MP1 58.9 82.5 69.5
Guesser MP2 59.9 82.4 65.5
Guesser oracle 68.4 84.9 83.0
Xerox MP0 83.6 96.4 78.1
Table 1: Russian morphological disambiguation.
Considering the amount of effort put in develop-
ing the guesser, the baseline POS tagging accuracy
is relatively good. However, the disambiguation is
largely improved by using our unigram model with
respect to all the evaluation categories. We are still
far from the performance of a high-quality analyzer
but, in absence of such a resource, our technique
might be a sensible option. We also note that there is
no clear winner in terms of pattern model, and con-
clude that this choice is task-specific.
7F -measure computed for the set of additional morphemes
and averaged over the words in the corpus.
8We estimate these frequencies by assuming each analysis of
each token is uniformly likely, then summing fractional counts.
9http://open.xerox.com/Services/
fst-nlp-tools/Pages/morphology
1209
4 Open Vocabulary Language Models
We now integrate our unigram model in a hierar-
chical Pitman-Yor n-gram language model (Fig. 1).
The training corpus words are assumed to be
generated from a distribution Gnw drawn from
PY(dn, ?n, Gn?1w ), where Gn?1w is defined recur-
sively down to the base model G0w. Previous work
Teh (2006) simply used G0w = U(V ) where V is
the word vocabulary, but in our case G0w is the MP
defined in ?2.2.
G2wG
3
w G
1
w
d3, ?3 d2, ?2 d1, ?1
Gs
ds, ?s
Gp G0p
dp, ?p
G0sw
Figure 1: The trigram version of our language model rep-
resented as a graphical model. G1w is the unigram model
of ?2.2.
We are interested in evaluating our model in an
open vocabulary scenario where the ability to ex-
plain new unseen words matters. We expect our
model to be able to generalize better thanks to the
combination of a morphological analyzer and a stem
distribution which is less sparse than the word dis-
tribution (for example, for the 1.6M word Turkish
corpus, |V | ? 3.5|S| ? 140k).
To integrate out-of-vocabulary words in our eval-
uation, we use infinite base distributions: G0w (in the
baseline model) or G0s (in the MP) are character tri-
gram models. We define perplexity of a held-out test
corpus in the standard way:
ppl = exp
(
?
1
N
N?
i=1
log p (wi | wi?n+1 ? ? ?wi?1)
)
but compared to the common practice, we do not
need to discount OOVs from this sum since the
model vocabulary is infinite. Note that we also
marginalize by summing over all the possible analy-
ses for a given word when computing its base prob-
ability according to the MP.
4.1 Language Modeling Experiments
We train several trigram models on the Russian TED
talks corpus used in the previous section. Our base-
line is a hierarchical PY trigram model with a tri-
gram character model as the base word distribution.
We compare it with our model using the same char-
acter model for the base stem distribution. Both of
the morphological analyzers described in the previ-
ous section help obtaining perplexity reductions (Ta-
ble 2). We ran a similar experiment on the Turkish
version of this corpus (1.6M words) with a high-
quality analyzer (Oflazer, 1994) and obtain even
larger gains (Table 3).
Model ppl
PY-character LM 563
Guesser MP2 530
Xerox MP2 522
Table 2: Evaluation of the Russian n-gram model.
Model ppl
PY-character LM 1,640
Oflazer MP2 1,292
Table 3: Evaluation of the Turkish n-gram model.
These results can partly be attributed to the high
OOV rate in these conditions: 4% for the Russian
corpus and 6% for the Turkish corpus.
4.2 Predictive Text Input
It is difficult to know whether a decrease in perplex-
ity, as measured in the previous section, will result in
a performance improvement in downstream applica-
tions. As a confirmation that correctly modeling new
words matters, we consider a predictive task with a
truly open vocabulary and that requires only a lan-
guage model: predictive text input.
Given some text, we encode it using a lossy de-
terministic character mapping, and try to recover the
original content by computing the most likely word
sequence. This task is inspired by predictive text
input systems available on cellphones with a 9-key
keypad. For example, the string gave me a cup
is encoded as 4283 63 2 287, which could also
be decoded as: hate of a bus.
1210
Silfverberg et al (2012) describe a system de-
signed for this task in Finnish, which is composed
of a weighted finite-state morphological analyzer
trained on IRC logs. However, their system is re-
stricted to words that are encoded in the analyzer?s
lexicon and does not use context for disambiguation.
In our experiments, we use the same Turkish TED
talks corpus as the previous section. As a baseline,
we use a trigram character language model. We pro-
duce a character lattice which encodes all the pos-
sible interpretations for a word and compose it with
a finite-state representation of the character LM us-
ing OpenFST (Allauzen et al, 2007). Alternatively,
we can use a unigram word model to decode this lat-
tice, backing off to the character language model if
no solution is found. Finally, to be able to make use
of word context, we can extract the k most likely
paths according to the character LM and produce a
word lattice, which is in turn decoded with a lan-
guage model defined over the extracted vocabulary.
Model WER CER
Character LM 48.37 16.72
1-gram + character LM 8.50 3.28
1-gram MP2 6.46 2.37
3-gram + character LM 7.86 3.07
3-gram MP2 5.73 2.15
Table 4: Evaluation of Turkish predictive text input.
We measure word and character error rate (WER,
CER) on the predicted word sequence and observe
large improvements in both of these metrics by mod-
eling morphology, both at the unigram level and
when context is used (Table 4).
Preliminary experiments with a corpus of 1.6M
Turkish tweets, an arguably more appropriate do-
main this task, show smaller but consistent improv-
ing: the trigram word error rate is reduced from 26%
to 24% when our model is used.
4.3 Limitations
While our model is an important step forward in
practical modeling of OOVs using morphological
processes, we have made the linguistically naive as-
sumption that morphology applies inside the lan-
guage?s lexicon but has no effect on the process that
put inflected lexemes together into sentences. In this
regard, our model is a minor variant on traditional n-
gram models that work with ?opaque? word forms.
How to best relax this assumption in a computation-
ally tractable way is an important open question left
for future work.
5 Word Alignment Model
Monolingual models of language are not the only
models that can benefit from taking into account
morphology. In fact, alignment models are a good
candidate for using richer word distributions: they
assume a target word distribution conditioned on
each source word. When the target language is mor-
phologically rich, classic independence assumptions
produce very weak models unless some kind of pre-
processing is applied to one side of the corpus. An
alternative is to use our unigram model as a word
translation distribution for each source word in the
corpus.
Our alignment model is based on a simple variant
of IBM Model 2 where the alignment distribution is
only controlled by two parameters, ? and p0 (Dyer et
al., 2013). p0 is the probability of the null alignment.
For a source sentence f of length n, a target sentence
e of lengthm and a latent alignment a, we define the
following alignment link probabilities (j 6= 0):
p(ai = j | n,m) ? (1? p0) exp
(
??
?
?
?
?
i
m ?
j
n
?
?
?
?
)
? controls the flatness of this distribution: larger val-
ues make the probabilities more peaked around the
diagonal of the alignment matrix.
Each target word is then generated given a source
word and a latent alignment link from the word
translation distribution p(ei | fai , Gw). Note that
this is effectively a unigram distribution over tar-
get words, albeit conditioned on the source word
fj . Here is where our model differs from classic
alignment models: the unigram distribution Gw is
assumed be generated from a PY process. There are
two choices for the base word distribution:
? As a baseline, we use a uniform base distribu-
tion over the target vocabulary: G0w = U(V ).
? We define a stem distribution Gs[f ] for each
source word f , a shared pattern distributionGp,
and set G0w[f ] = MP(Gs[f ], Gp). In this case,
1211
we obtain the model depicted in Fig. 2. The
stem and the pattern models are also given PY
priors with uniform base distribution (G0s =
U(S)).
Finally, we put uninformative priors on the align-
ment distribution parameters: p0 ? Beta(?, ?) is
collapsed and ? ? Gamma(k, ?) is inferred using
Metropolis-Hastings.
f e
a
p0 
Gw
dw, ?w
Gp
G0sGs
G0p
dp, ?p
?, 
ds, ?s
Figure 2: Our alignment model, represented as a graphi-
cal model.
Experiments
We evaluate the alignment error rate of our models
for two language pairs with rich morphology on the
target side. We compare to alignments inferred us-
ing IBM Model 4 trained with EM (Brown et al,
1993),10 a version of our baseline model (described
above) without PY priors (learned using EM), and
the PY-based baseline. We consider two language
pairs.
English-Turkish We use a 2.8M word cleaned
version of the South-East European Times corpus
(Tyers and Alperen, 2010) and gold-standard align-
ments from ?akmak et al (2012). Our morphologi-
cal analyzer is identical to the one used in the previ-
ous sections.
English-Czech We use the 1.3M word News
Commentary corpus and gold-standard alignments
10We use the default GIZA++ stage training scheme:
Model 1 + HMM + Model 3 + Model 4.
from Bojar and Prokopov? (2006). The morpholog-
ical analyzer is provided by Xerox.
Results Results are shown in Table 5. Our lightly
parameterized model performs much better than
IBM Model 4 in these small-data conditions. With
an identical model, we find PY priors outperform
traditional multinomial distributions. Adding mor-
phology further reduced the alignment error rate, for
both languages.
AER
Model en-tr en-cs
Model 4 52.1 34.5
EM 43.8 28.9
PY-U(V ) 39.2 25.7
PY-U(S) 33.8 24.8
Table 5: Word alignment experiments on English-Turkish
(en-tr) and English-Czech (en-cs) data.
As an example of how our model generalizes bet-
ter, consider the sentence pair in Fig. 3, taken from
the evaluation data. The two words composing the
Turkish sentence are not found elsewhere in the cor-
pus, but several related inflections occur.11 It is
therefore trivial for the stem-base model to find the
correct alignment (marked in black), while all the
other models have no evidence for it and choose an
arbitrary alignment (gray points).
I w
a
s
n
o
t
a
b
l
e
t
o
f
i
n
i
s
h
m
y
h
o
m
e
w
o
r
k
?devimi
bitiremedim
Figure 3: A complex Turkish-English word alignment
(alignment points in gray: EM/PY-U(V ); black: PY-
U(S)).
6 Related Work
Computational morphology has received consider-
able attention in NLP since the early work on two-
level morphology (Koskenniemi, 1984; Kaplan and
11?devinin, ?devini, ?devleri; bitmez, bitirileceg?inden,
bitmesiyle, ...
1212
Kay, 1994). It is now widely accepted that finite-
state transducers have sufficient power to express
nearly all morphological phenomena, and the XFST
toolkit (Beesley and Karttunen, 2003) has con-
tributed to the practical adoption of this modeling
approach. Recently, open-source tools have been re-
leased: in this paper, we used Foma (Hulden, 2009)
to develop the Russian guesser.
Since some inflected forms have several possible
analyses, there has been a great deal of work on se-
lecting the intended one in context (Hakkani-T?r et
al., 2000; Hajic? et al, 2001; Habash and Rambow,
2005; Smith et al, 2005; Habash et al, 2009). Our
disambiguation model is closely related to genera-
tive models used for this purpose (Hakkani-T?r et
al., 2000).
Rule-based analysis is not the only approach
to modeling morphology, and many unsupervised
models have been proposed.12 Heuristic segmenta-
tion approaches based on the minimum description
length principle (Goldsmith, 2001; Creutz and La-
gus, 2007; de Marcken, 1996; Brent et al, 1995)
have been shown to be effective, and Bayesian
model-based versions have been proposed as well
(Goldwater et al, 2011; Snyder and Barzilay, 2008;
Snover and Brent, 2001). In ?3, we suggested a third
way between rule-based approaches and fully un-
supervised learning that combines the best of both
worlds.
Morphological analysis or segmentation is crucial
to the performance of several applications: machine
translation (Goldwater and McClosky, 2005; Al-
Haj and Lavie, 2010; Oflazer and El-Kahlout, 2007;
Minkov et al, 2007; Habash and Sadat, 2006, in-
ter alia), automatic speech recognition (Creutz et al,
2007), and syntactic parsing (Tsarfaty et al, 2010).
Several methods have been proposed to integrate
morphology into n-gram language models, includ-
ing factored language models (Bilmes and Kirch-
hoff, 2003), discriminative language modeling (Ar?-
soy et al, 2008), and more heuristic approaches
(Monz, 2011).
Despite the fundamentally open nature of the lex-
icon (Heaps, 1978), there has been distressingly lit-
12Developing a high-coverage analyzer can be a time-
consuming process even with the simplicity of modern toolkits,
and unsupervised morphology learning is an attractive problem
for computational cognitive science.
tle attention to the general problem of open vocabu-
lary language modeling problem (most applications
make a closed-vocabulary assumption). The classic
exploration of open vocabulary language modeling
is Brown et al (1992), which proposed the strategy
of interpolating between word- and character-based
models. Character-based language models are re-
viewed by Carpenter (2005). So-called hybrid mod-
els that model both words and sublexical units have
become popular in speech recognition (Shaik et al,
2012; Parada et al, 2011; Bazzi, 2002). Open-
vocabulary language language modeling has also re-
cently been explored in the context of assistive tech-
nologies (Roark, 2009).
Finally, Pitman-Yor processes (PYPs) have be-
come widespread in natural language processing
since they are natural power-law generators. It has
been shown that the widely used modified Kneser-
Ney estimator (Chen and Goodman, 1998) for n-
gram language models is an approximation of the
posterior predictive distribution of a language model
with hierarchical PYP priors (Goldwater et al, 2011;
Teh, 2006).
7 Conclusion
We described a generative model which makes use
of morphological analyzers to produce richer word
distributions through sharing of statistical strength
between stems. We have shown how it can be in-
tegrated into several models central to NLP appli-
cations and have empirically validated the effective-
ness of these changes. Although this paper mostly
focused on languages that are well studied and for
which high-quality analyzers are available, our mod-
els are especially relevant in low-resource scenarios
because they do not require disambiguated analyses.
In future work, we plan to apply these techniques to
languages such as Kinyarwanda, a resource-poor but
morphologically rich language spoken in Rwanda.
It is our belief that knowledge-rich models can help
bridge the gap between low- and high-resource lan-
guages.
Acknowledgments
We thank Kemal Oflazer for making his Turkish lan-
guage morphological analyzer available to us and Bren-
dan O?Connor for gathering the Turkish tweets used in
1213
the predictive text experiments. This work was spon-
sored by the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant number
W911NF-10-1-0533.
References
H. Al-Haj and A. Lavie. 2010. The impact
of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. Proc. of AMTA.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata, pages 11?23.
Ebru Ar?soy, Brian Roark, Izhak Shafran, and Murat
Sara?lar. 2008. Discriminative n-gram language mod-
eling for Turkish. In Proc. of Interspeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, MIT.
K.R. Beesley and L. Karttunen. 2003. Finite-state mor-
phology: Xerox tools and techniques. CSLI, Stanford.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Ondr?ej Bojar and Magdalena Prokopov?. 2006. Czech-
English word alignment. In Proc. of LREC.
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in MDL induction. In Proceedings of the
Fifth International Workshop on Artificial Intelligence
and Statistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, Robert L. Mercer, and Jennifer C. Lai.
1992. An estimate of an upper bound for the entropy
of English. Computational Linguistics, 18(1):31?40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Bob Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard University.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
M. Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,
J. Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Sara?lar, and A. Stolcke. 2007. Morph-based
speech recognition and modeling of out-of-vocabulary
words across languages. ACM Transactions on Speech
and Language Processing, 5(1):3.
Carl G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of IBM Model 2. In Proc. of NAACL.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and D. McClosky. 2005. Improving statis-
tical MT through morphological analysis. In Proc. of
EMNLP.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2011. Producing power-law distributions and
damping word frequencies with two-stage language
models. Journal of Machine Learning Research,
12:2335?2382.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging, and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proc. of NAACL.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceedings
of the Second International Conference on Arabic Lan-
guage Resources and Tools.
Jan Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petrovic?.
2001. Serial combination of rules and statistics. In
Proc. of ACL.
D. Z. Hakkani-T?r, Kemal Oflazer, and G. T?r. 2000.
Statistical morphological disambiguation for aggluti-
native languages. In Proc. of COLING.
Harold Stanley Heaps. 1978. Information Retrieval:
Computational and Theoretical Aspects. Academic
Press.
M. Hulden. 2009. Foma: a finite-state compiler and li-
brary. In Proc. of EACL.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3):331?378.
Lauri Karttunen and Kenneth R. Beesley. 2005. Twenty-
five years of finite-state morphology. In Inquiries into
Words, Constraints and Contexts, pages 71?83. CSLI.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
Proc. of ACL-COLING.
1214
O. Lya?evskaya, I. Astaf?yeva, A. Bonch-Osmolovskaya,
A. Garej?ina, Y. Gri?ina, V. D?yac?kov, M. Ionov,
A. Koroleva, M. Kudrinskij, A. Lityagina, Y. Luc?ina,
Y. Sidorova, S. Toldova, S. Savc?uk, and S. Ko-
val?. 2010. Ocenka metodov avtomatic?eskogo
analiza teksta: morfologic?eskie parseri russkogo
yazyka. Komp?juternaya lingvistika i intellektual?nye
texnologii (Computational linguistics and intellectual
technologies).
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Christof Monz. 2011. Statistical machine translation
with local language models. In Proc. of EMNLP.
Kemal Oflazer and I?lknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
StatMT.
K. Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Computing,
9(2):137?148.
Carolina Parada, Mark Dredze, Abhinav Sethy, and Ariya
Rastrow. 2011. Learning sub-word units for open vo-
cabulary speech recognition. In Proc. of ACL.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?90.
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
Report CSLU-09-001, Oregon Health & Science Uni-
versity.
M. Ali Basha Shaik, David Rybach, Stefan Hahn, Ralf
Schlu?ter, and Hermann Ney. 2012. Hierarchical hy-
brid language models for open vocabulary continuous
speech recognition using wfst. In Proc. of SAPA.
M. Silfverberg, K. Lind?n, and M. Hyv?rinen. 2012.
Predictive text entry for agglutinative languages using
unsupervised morphological segmentation. In Proc.
of Computational Linguistics and Intelligent Text Pro-
cessing.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
Matt G. Snover and Michael R. Brent. 2001. A Bayesian
model for morpheme and paradigm identification. In
Proc. of ACL.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proc. of ACL.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL.
Reut Tsarfaty, Djam? Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages: What, how
and whither. In Proc. of Workshop on Statistical Pars-
ing of Morphologically Rich Languages.
F. Tyers and M.S. Alperen. 2010. South-east european
times: A parallel corpus of Balkan languages. In
Proceedings of the LREC workshop on Exploitation
of multilingual resources and tools for Central and
(South) Eastern European Languages.
M. Talha ?akmak, S?leyman Acar, and G?ls?en Eryig?it.
2012. Word alignment for English-Turkish language
pair. In Proc. of LREC.
1215
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1502?1511,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Viterbi Training for PCFGs:
Hardness Results and Competitiveness of Uniform Initialization
Shay B. Cohen and Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{scohen,nasmith}@cs.cmu.edu
Abstract
We consider the search for a maximum
likelihood assignment of hidden deriva-
tions and grammar weights for a proba-
bilistic context-free grammar, the problem
approximately solved by ?Viterbi train-
ing.? We show that solving and even ap-
proximating Viterbi training for PCFGs is
NP-hard. We motivate the use of uniform-
at-random initialization for Viterbi EM as
an optimal initializer in absence of further
information about the correct model pa-
rameters, providing an approximate bound
on the log-likelihood.
1 Introduction
Probabilistic context-free grammars are an essen-
tial ingredient in many natural language process-
ing models (Charniak, 1997; Collins, 2003; John-
son et al, 2006; Cohen and Smith, 2009, inter
alia). Various algorithms for training such models
have been proposed, including unsupervised meth-
ods. Many of these are based on the expectation-
maximization (EM) algorithm.
There are alternatives to EM, and one such al-
ternative is Viterbi EM, also called ?hard? EM or
?sparse? EM (Neal and Hinton, 1998). Instead
of using the parameters (which are maintained in
the algorithm?s current state) to find the true pos-
terior over the derivations, Viterbi EM algorithm
uses a posterior focused on the Viterbi parse of
those parameters. Viterbi EM and variants have
been used in various settings in natural language
processing (Yejin and Cardie, 2007; Wang et al,
2007; Goldwater and Johnson, 2005; DeNero and
Klein, 2008; Spitkovsky et al, 2010).
Viterbi EM can be understood as a coordinate
ascent procedure that locally optimizes a function;
we call this optimization goal ?Viterbi training.?
In this paper, we explore Viterbi training for
probabilistic context-free grammars. We first
show that under the assumption that P 6= NP, solv-
ing and even approximating the Viterbi training
problem is hard. This result holds even for hid-
den Markov models. We extend the main hardness
result to the EM algorithm (giving an alternative
proof to this known result), as well as the problem
of conditional Viterbi training. We then describe
a ?competitiveness? result for uniform initializa-
tion of Viterbi EM: we show that initialization of
the trees in an E-step which uses uniform distri-
butions over the trees is optimal with respect to a
certain approximate bound.
The rest of this paper is organized as follows. ?2
gives background on PCFGs and introduces some
notation. ?3 explains Viterbi training, the declar-
ative form of Viterbi EM. ?4 describes a hardness
result for Viterbi training. ?5 extends this result to
a hardness result of approximation and ?6 further
extends these results for other cases. ?7 describes
the advantages in using uniform-at-random initial-
ization for Viterbi training. We relate these results
to work on the k-means problem in ?8.
2 Background and Notation
We assume familiarity with probabilistic context-
free grammars (PCFGs). A PCFGG consists of:
? A finite set of nonterminal symbols N;
? A finite set of terminal symbols ?;
? For each A ? N, a set of rewrite rules R(A) of
the form A ? ?, where ? ? (N ? ?)?, and
R = ?A?NR(A);
? For each rule A ? ?, a probability ?A??. The
collection of probabilities is denoted ?, and they
are constrained such that:
?(A? ?) ? R(A), ?A?? ? 0
?A ? N,
?
?:(A??)?R(A)
?A?? = 1
That is, ? is grouped into |N| multinomial dis-
tributions.
1502
Under the PCFG, the joint probability of a string
x ? ?? and a grammatical derivation z is1
p(x, z | ?) =
?
(A??)?R
(?A??)
fA??(z) (1)
= exp
?
(A??)?R
fA??(z) log ?A??
where fA??(z) is a function that ?counts? the
number of times the rule A ? ? appears in
the derivation z. fA(z) will similarly denote the
number of times that nonterminal A appears in z.
Given a sample of derivations z = ?z1, . . . , zn?,
let:
FA??(z) =
n?
i=1
fA??(zi) (2)
FA(z) =
n?
i=1
fA(zi) (3)
We use the following notation forG:
? L(G) is the set of all strings (sentences) x that
can be generated using the grammar G (the
?language ofG?).
? D(G) is the set of all possible derivations z that
can be generated using the grammarG.
? D(G, x) is the set of all possible derivations z
that can be generated using the grammarG and
have the yield x.
3 Viterbi Training
Viterbi EM, or ?hard? EM, is an unsupervised
learning algorithm, used in NLP in various set-
tings (Yejin and Cardie, 2007; Wang et al, 2007;
Goldwater and Johnson, 2005; DeNero and Klein,
2008; Spitkovsky et al, 2010). In the context of
PCFGs, it aims to select parameters ? and phrase-
structure trees z jointly. It does so by iteratively
updating a state consisting of (?, z). The state
is initialized with some value, then the algorithm
alternates between (i) a ?hard? E-step, where the
strings x1, . . . , xn are parsed according to a cur-
rent, fixed ?, giving new values for z, and (ii) an
M-step, where the ? are selected to maximize like-
lihood, with z fixed.
With PCFGs, the E-step requires running an al-
gorithm such as (probabilistic) CKY or Earley?s
1Note that x = yield(z); if the derivation is known, the
string is also known. On the other hand, there may be many
derivations with the same yield, perhaps even infinitely many.
algorithm, while the M-step normalizes frequency
counts FA??(z) to obtain the maximum likeli-
hood estimate?s closed-form solution.
We can understand Viterbi EM as a coordinate
ascent procedure that approximates the solution to
the following declarative problem:
Problem 1. ViterbiTrain
Input: G context-free grammar, x1, . . . , xn train-
ing instances from L(G)
Output: ? and z1, . . . , zn such that
(?, z1, . . . , zn) = argmax
?,z
n?
i=1
p(xi, zi | ?) (4)
The optimization problem in Eq. 4 is non-
convex and, as we will show in ?4, hard to op-
timize. Therefore it is necessary to resort to ap-
proximate algorithms like Viterbi EM.
Neal and Hinton (1998) use the term ?sparse
EM? to refer to a version of the EM algorithm
where the E-step finds the modes of hidden vari-
ables (rather than marginals as in standard EM).
Viterbi EM is a variant of this, where the E-
step finds the mode for each xi?s derivation,
argmaxz?D(G,xi) p(xi, z | ?).
We will refer to
L(?, z) =
n?
i=1
p(xi, zi | ?) (5)
as ?the objective function of ViterbiTrain.?
Viterbi training and Viterbi EM are closely re-
lated to self-training, an important concept in
semi-supervised NLP (Charniak, 1997; McClosky
et al, 2006a; McClosky et al, 2006b). With self-
training, the model is learned with some seed an-
notated data, and then iterates by labeling new,
unannotated data and adding it to the original an-
notated training set. McClosky et al consider self-
training to be ?one round of Viterbi EM? with su-
pervised initialization using labeled seed data. We
refer the reader to Abney (2007) for more details.
4 Hardness of Viterbi Training
We now describe hardness results for Problem 1.
We first note that the following problem is known
to be NP-hard, and in fact, NP-complete (Sipser,
2006):
Problem 2. 3-SAT
Input: A formula ? =
?m
i=1 (ai ? bi ? ci) in con-
junctive normal form, such that each clause has 3
1503
S?2
ccccc
ccccc
ccccc
ccccc
ccccc
ccccc
c
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
TT
S?1
A1
eee
eee
eee
eee
eee
eee
e
YYY
YYY
YYY
YYY
YYY
YYY
Y
A2
eee
eee
eee
eee
eee
eee
e
YYY
YYY
YYY
YYY
YYY
YYY
Y
UY1,0
qq
qq
qq
q
MM
MM
MM
M
UY2,1
qq
qq
qq
q
MM
MM
MM
M
UY4,0
qq
qq
qq
q
MM
MM
MM
M
UY1,0
qq
qq
qq
q
MM
MM
MM
M
UY2,1
qq
qq
qq
q
MM
MM
MM
M
UY3,1
qq
qq
qq
q
MM
MM
MM
M
VY?1 VY1 VY2 VY?2 VY?4 VY4 VY?1 VY1 VY2 VY?2 VY3 VY?3
1 0 1 0 1 0 1 0 1 0 1 0
Figure 1: An example of a Viterbi parse tree which represents a satisfying assignment for ? = (Y1?Y2? Y?4)? (Y?1? Y?2?Y3).
In ??, all rules appearing in the parse tree have probability 1. The extracted assignment would be Y1 = 0, Y2 = 1, Y3 =
1, Y4 = 0. Note that there is no usage of two different rules for a single nonterminal.
literals.
Output: 1 if there is a satisfying assignment for ?
and 0 otherwise.
We now describe a reduction of 3-SAT to Prob-
lem 1. Given an instance of the 3-SAT problem,
the reduction will, in polynomial time, create a
grammar and a single string such that solving the
ViterbiTrain problem for this grammar and string
will yield a solution for the instance of the 3-SAT
problem.
Let ? =
?m
i=1 (ai ? bi ? ci) be an instance of
the 3-SAT problem, where ai, bi and ci are liter-
als over the set of variables {Y1, . . . , YN} (a literal
refers to a variable Yj or its negation, Y?j). Let Cj
be the jth clause in ?, such that Cj = aj ? bj ? cj .
We define the following context-free grammarG?
and string to parse s?:
1. The terminals of G? are the binary digits ? =
{0, 1}.
2. We create N nonterminals VYr , r ?
{1, . . . , N} and rules VYr ? 0 and VYr ? 1.
3. We create N nonterminals VY?r , r ?
{1, . . . , N} and rules VY?r ? 0 and VY?r ? 1.
4. We create UYr,1 ? VYrVY?r and UYr,0 ?
VY?rVYr .
5. We create the rule S?1 ? A1. For each j ?
{2, . . . ,m}, we create a rule S?j ? S?j?1Aj
where S?j is a new nonterminal indexed by
?j ,
?j
i=1Ci and Aj is also a new nonterminal
indexed by j ? {1, . . . ,m}.
6. Let Cj = aj ? bj ? cj be clause j in ?. Let
Y (aj) be the variable that aj mentions. Let
(y1, y2, y3) be a satisfying assignment for Cj
where yk ? {0, 1} and is the value of Y (aj),
Y (bj) and Y (cj) respectively for k ? {1, 2, 3}.
For each such clause-satisfying assignment, we
add the rule:
Aj ? UY (aj),y1UY (bj),y2UY (cj),y3 (6)
For each Aj , we would have at most 7 rules of
that form, since one rule will be logically incon-
sistent with aj ? bj ? cj .
7. The grammar?s start symbol is S?n .
8. The string to parse is s? = (10)3m, i.e. 3m
consecutive occurrences of the string 10.
A parse of the string s? using G? will be used
to get an assignment by setting Yr = 0 if the rule
VYr ? 0 or VY?r ? 1 are used in the derivation of
the parse tree, and 1 otherwise. Notice that at this
point we do not exclude ?contradictions? coming
from the parse tree, such as VY3 ? 0 used in the
tree together with VY3 ? 1 or VY?3 ? 0. The fol-
lowing lemma gives a condition under which the
assignment is consistent (so contradictions do not
occur in the parse tree):
Lemma 1. Let ? be an instance of the 3-SAT
problem, and letG? be a probabilistic CFG based
on the above grammar with weights ??. If the
(multiplicative) weight of the Viterbi parse of s?
is 1, then the assignment extracted from the parse
tree is consistent.
Proof. Since the probability of the Viterbi parse
is 1, all rules of the form {VYr , VY?r} ? {0, 1}
which appear in the parse tree have probability 1
as well. There are two possible types of inconsis-
tencies. We show that neither exists in the Viterbi
parse:
1504
1. For any r, an appearance of both rules of the
form VYr ? 0 and VYr ? 1 cannot occur be-
cause all rules that appear in the Viterbi parse
tree have probability 1.
2. For any r, an appearance of rules of the form
VYr ? 1 and VY?r ? 1 cannot occur, because
whenever we have an appearance of the rule
VYr ? 0, we have an adjacent appearance of
the rule VY?r ? 1 (because we parse substrings
of the form 10), and then again we use the fact
that all rules in the parse tree have probability 1.
The case of VYr ? 0 and VY?r ? 0 is handled
analogously.
Thus, both possible inconsistencies are ruled out,
resulting in a consistent assignment.
Figure 1 gives an example of an application of
the reduction.
Lemma 2. Define ?, G? as before. There exists
?? such that the Viterbi parse of s? is 1 if and only
if ? is satisfiable. Moreover, the satisfying assign-
ment is the one extracted from the parse tree with
weight 1 of s? under ??.
Proof. (=?) Assume that there is a satisfying as-
signment. Each clause Cj = aj ? bj ? cj is satis-
fied using a tuple (y1, y2, y3) which assigns value
for Y (aj), Y (bj) and Y (cj). This assignment cor-
responds the following rule
Aj ? UY (aj),y1UY (bj),y2UY (cj),y3 (7)
Set its probability to 1, and set al other rules of
Aj to 0. In addition, for each r, if Yr = y, set the
probabilities of the rules VYr ? y and VY?r ? 1?y
to 1 and VY?r ? y and VYr ? 1? y to 0. The rest
of the weights for S?j ? S?j?1Aj are set to 1.
This assignment of rule probabilities results in a
Viterbi parse of weight 1.
(?=) Assume that the Viterbi parse has prob-
ability 1. From Lemma 1, we know that we can
extract a consistent assignment from the Viterbi
parse. In addition, for each clause Cj we have a
rule
Aj ? UY (aj),y1UY (bj),y2UY (cj),y3 (8)
that is assigned probability 1, for some
(y1, y2, y3). One can verify that (y1, y2, y3)
are the values of the assignment for the corre-
sponding variables in clause Cj , and that they
satisfy this clause. This means that each clause is
satisfied by the assignment we extracted.
In order to show an NP-hardness result, we need
to ?convert? ViterbiTrain to a decision problem.
The natural way to do it, following Lemmas 1
and 2, is to state the decision problem for Viter-
biTrain as ?given G and x1, . . . , xn and ? ? 0,
is the optimized value of the objective function
L(?, z) ? ??? and use ? = 1 together with Lem-
mas 1 and 2. (Naturally, an algorithm for solving
ViterbiTrain can easily be used to solve its deci-
sion problem.)
Theorem 3. The decision version of the Viterbi-
Train problem is NP-hard.
5 Hardness of Approximation
A natural path of exploration following the hard-
ness result we showed is determining whether an
approximation of ViterbiTrain is also hard. Per-
haps there is an efficient approximation algorithm
for ViterbiTrain we could use instead of coordi-
nate ascent algorithms such as Viterbi EM. Recall
that such algorithms? main guarantee is identify-
ing a local maximum; we know nothing about how
far it will be from the global maximum.
We next show that approximating the objective
function of ViterbiTrain with a constant factor of ?
is hard for any ? ? (12 , 1] (i.e., 1/2 +  approxima-
tion is hard for any  ? 1/2). This means that, un-
der the P 6= NP assumption, there is no efficient al-
gorithm that, given a grammar G and a sample of
sentences x1, . . . , xn, returns ?? and z? such that:
L(??, z?) ? ? ?max
?,z
n?
i=1
p(xi, zi | ?) (9)
We will continue to use the same reduction from
?4. Let s? be the string from that reduction, and
let (?, z) be the optimal solution for ViterbiTrain
given G? and s?. We first note that if p(s?, z |
?) < 1 (implying that there is no satisfying as-
signment), then there must be a nonterminal which
appears along with two different rules in z.
This means that we have a nonterminal B ? N
with some rule B ? ? that appears k times,
while the nonterminal appears in the parse r ?
k + 1 times. Given the tree z, the ? that maxi-
mizes the objective function is the maximum like-
lihood estimate (MLE) for z (counting and nor-
malizing the rules).2 We therefore know that
the ViterbiTrain objective function, L(?, z), is at
2Note that we can only make p(z | ?, x) greater by using
? to be the MLE for the derivation z.
1505
most
(
k
r
)k
, because it includes a factor equal
to
(
fB??(z)
fB(z)
)fB??(z)
, where fB(z) is the num-
ber of times nonterminal B appears in z (hence
fB(z) = r) and fB??(z) is the number of times
B ? ? appears in z (hence fB??(z) = k). For
any k ? 1, r ? k + 1:
(
k
r
)k
?
(
k
k + 1
)k
?
1
2
(10)
This means that if the value of the objective func-
tion of ViterbiTrain is not 1 using the reduction
from ?4, then it is at most 12 . If we had an efficient
approximate algorithm with approximation coeffi-
cient ? > 12 (Eq. 9 holds), then in order to solve
3-SAT for formula ?, we could run the algorithm
on G? and s? and check whether the assignment
to (?, z) that the algorithm returns satisfies ? or
not, and return our response accordingly.
If ? were satisfiable, then the true maximal
value of L would be 1, and the approximation al-
gorithm would return (?, z) such that L(?, z) ?
? > 12 . z would have to correspond to a satisfy-
ing assignment, and in fact p(z | ?) = 1, because
in any other case, the probability of a derivation
which does not represent a satisfying assignment
is smaller than 12 . If ? were not satisfiable, then
the approximation algorithm would never return a
(?, z) that results in a satisfying assignment (be-
cause such a (?, z) does not exist).
The conclusion is that an efficient algorithm for
approximating the objective function of Viterbi-
Train (Eq. 4) within a factor of 12 +  is unlikely
to exist. If there were such an algorithm, we could
use it to solve 3-SAT using the reduction from ?4.
6 Extensions of the Hardness Result
An alternative problem to Problem 1, a variant of
Viterbi-training, is the following (see, for exam-
ple, Klein and Manning, 2001):
Problem 3. ConditionalViterbiTrain
Input: G context-free grammar, x1, . . . , xn train-
ing instances from L(G)
Output: ? and z1, . . . , zn such that
(?, z1, . . . , zn) = argmax
?,z
n?
i=1
p(zi | ?, xi) (11)
Here, instead of maximizing the likelihood, we
maximize the conditional likelihood. Note that
there is a hidden assumption in this problem def-
inition, that xi can be parsed using the grammar
G. Otherwise, the quantity p(zi | ?, xi) is not
well-defined. We can extend ConditionalViterbi-
Train to return ? in the case of not having a parse
for one of the xi?this can be efficiently checked
using a run of a cubic-time parser on each of the
strings xi with the grammarG.
An approximate technique for this problem is
similar to Viterbi EM, only modifying the M-
step to maximize the conditional, rather than joint,
likelihood. This new M-step will not have a closed
form and may require auxiliary optimization tech-
niques like gradient ascent.
Our hardness result for ViterbiTrain applies to
ConditionalViterbiTrain as well. The reason is
that if p(z, s? | ??) = 1 for a ? with a satisfying
assignment, thenL(G) = {s?} andD(G) = {z}.
This implies that p(z | ??, s?) = 1. If ? is unsat-
isfiable, then for the optimal ? of ViterbiTrain we
have z and z? such that 0 < p(z, s? | ??) < 1
and 0 < p(z?, s? | ??) < 1, and therefore p(z |
??, s?) < 1, which means the conditional objec-
tive function will not obtain the value 1. (Note
that there always exist some parameters ?? that
generate s?.) So, again, given an algorithm for
ConditionalViterbiTrain, we can discern between
a satisfiable formula and an unsatisfiable formula,
using the reduction from ?4 with the given algo-
rithm, and identify whether the value of the objec-
tive function is 1 or strictly less than 1. We get the
result that:
Theorem 4. The decision problem of Condition-
alViterbiTrain problem is NP-hard.
where the decision problem of ConditionalViter-
biTrain is defined analogously to the decision
problem of ViterbiTrain.
We can similarly show that finding the global
maximum of the marginalized likelihood:
max
?
1
n
n?
i=1
log
?
z
p(xi, z | ?) (12)
is NP-hard. The reasoning follows. Using the
reduction from before, if ? is satisfiable, then
Eq. 12 gets value 0. If ? is unsatisfiable, then we
would still get value 0 only if L(G) = {s?}. If
G? generates a single derivation for (10)3m, then
we actually do have a satisfying assignment from
1506
Lemma 1. Otherwise (more than a single deriva-
tion), the optimal ? would have to give fractional
probabilities to rules of the form VYr ? {0, 1} (or
VY?r ? {0, 1}). In that case, it is no longer true
that (10)3m is the only generated sentence, which
is a contradiction.
The quantity in Eq. 12 can be maximized ap-
proximately using algorithms like EM, so this
gives a hardness result for optimizing the objec-
tive function of EM for PCFGs. Day (1983) pre-
viously showed that maximizing the marginalized
likelihood for hidden Markov models is NP-hard.
We note that the grammar we use for all of our
results is not recursive. Therefore, we can encode
this grammar as a hidden Markov model, strength-
ening our result from PCFGs to HMMs.3
7 Uniform-at-Random Initialization
In the previous sections, we showed that solving
Viterbi training is hard, and therefore requires an
approximation algorithm. Viterbi EM, which is an
example of such algorithm, is dependent on an ini-
tialization of either ? to start with an E-step or z
to start with an M-step. In the absence of a better-
informed initializer, it is reasonable to initialize
z using a uniform distribution over D(G, xi) for
each i. If D(G, xi) is finite, it can be done effi-
ciently by setting ? = 1 (ignoring the normaliza-
tion constraint), running the inside algorithm, and
sampling from the (unnormalized) posterior given
by the chart (Johnson et al, 2007). We turn next
to an analysis of this initialization technique that
suggests it is well-motivated.
The sketch of our result is as follows: we
first give an asymptotic upper bound for the log-
likelihood of derivations and sentences. This
bound, which has an information-theoretic inter-
pretation, depends on a parameter ?, which de-
pends on the distribution from which the deriva-
tions were chosen. We then show that this bound
is minimized when we pick ? such that this distri-
bution is (conditioned on the sentence) a uniform
distribution over derivations.
Let q(x) be any distribution over L(G) and ?
some parameters for G. Let f(z) be some feature
function (such as the one that counts the number
of appearances of a certain rule in a derivation),
and then:
Eq,?[f ] ,
?
x?L(G)
q(x)
?
z?D(G,x)
p(z | ?, x)f(z)
3We thank an anonymous reviewer for pointing this out.
which gives the expected value of the feature func-
tion f(z) under the distribution q(x)?p(z | ?, x).
We will make the following assumption aboutG:
Condition 1. There exists some ?I such that
?x ? L(G),?z ? D(G, x), p(z | ?I , x) =
1/|D(G, x)|.
This condition is satisfied, for example, whenG
is in Chomsky normal form and for all A,A? ? N,
|R(A)| = |R(A?)|. Then, if we set ?A?? =
1/|R(A)|, we get that all derivations of x will
have the same number of rules and hence the same
probability. This condition does not hold for gram-
mars with unary cycles because |D(G, x)|may be
infinite for some derivations. Such grammars are
not commonly used in NLP.
Let us assume that some ?correct? parameters
?? exist, and that our data were drawn from a dis-
tribution parametrized by ??. The goal of this sec-
tion is to motivate the following initialization for
?, which we call UniformInit:
1. Initialize z by sampling from the uniform dis-
tribution over D(G, xi) for each xi.
2. Update the grammar parameters using maxi-
mum likelihood estimation.
7.1 Bounding the Objective
To show our result, we require first the following
definition due to Freund et al (1997):
Definition 5. A distribution p1 is within ? ? 1 of
a distribution p2 if for every event A, we have
1
?
?
p1(A)
p2(A)
? ? (13)
For any feature function f(z) and any two
sets of parameters ?2 and ?1 for G and for any
marginal q(x), if p(z | ?1, x) is within ? of
p(z | ?2, x) for all x then:
Eq,?1 [f ]
?
? Eq,?2 [f ] ? ?Eq,?1 [f ] (14)
Let ?0 be a set of parameters such that we perform
the following procedure in initializing Viterbi EM:
first, we sample from the posterior distribution
p(z | ?0, x), and then update the parameters with
maximum likelihood estimate, in a regular M-step.
Let ? be such that p(z | ?0, x) is within ? of
p(z | ??, x) (for all x ? L(G)). (Later we will
show that UniformInit is a wise choice for making
? small. Note that UniformInit is equivalent to the
procedure mentioned above with ?0 = ?I .)
1507
Consider p?n(x), the empirical distribution over
x1, . . . , xn. As n ? ?, we have that p?n(x) ?
p?(x), almost surely, where p? is:
p?(x) =
?
z
p?(x, z | ??) (15)
This means that as n ? ? we have Ep?n,?[f ] ?
Ep?,?[f ]. Now, let z0 = (z0,1, . . . , z0,n) be sam-
ples from p(z | ?0, xi) for i ? {1, . . . , n}. Then,
from simple MLE computation, we know that the
value
max
??
n?
i=1
p(xi, z0,i | ??) (16)
=
?
(A??)?R
(
FA??(z0)
FA(z0)
)FA??(z0)
We also know that for ?0, from the consistency of
MLE, for large enough samples:
FA??(z0)
FA(z0)
?
Ep?n,?0 [fA??]
Ep?n,?0 [fA]
(17)
which means that we have the following as n
grows (starting from the ViterbiTrain objective
with initial state z = z0):
max
??
n?
i=1
p(xi, z0,i | ??) (18)
(Eq. 16)
=
?
(A??)?R
(
FA??(z0)
FA(z0)
)FA??(z0)
(19)
(Eq. 17)
?
?
(A??)?R
(
Ep?n,?0 [fA??]
Ep?n,?0 [fA]
)FA??(z0)
(20)
We next use the fact that p?n(x) ? p?(x) for large
n, and apply Eq. 14, noting again our assumption
that p(z | ?0, x) is within ? of p(z | ??, x). We
also let B =
?
i
|zi|, where |zi| is the number of
nodes in the derivation zi. Note that FA(zi) ?
B. The above quantity (Eq. 20) is approximately
bounded above by
?
(A??)?R
1
?2B
(
Ep?,?? [fA??]
Ep?,?? [fA]
)FA??(z0)
(21)
=
1
?2|R|B
?
(A??)?R
(??A??)
FA??(z0) (22)
Eq. 22 follows from:
??A?? =
Ep?,?? [fA??]
Ep?,?? [fA]
(23)
If we continue to develop Eq. 22 and apply
Eq. 17 and Eq. 23 again, we get that:
1
?2|R|B
?
(A??)?R
(??A??)
FA??(z0)
=
1
?2|R|B
?
(A??)?R
(??A??)
FA??(z0)?
FA(z0)
FA(z0)
?
1
?2|R|B
?
(A??)?R
(??A??)
Ep?,?0
[fA??]
Ep?,?0
[fA]
?FA(z0)
?
1
?2|R|B
?
(A??)?R
(??A??)
?2??A??FA(z0)
?
1
?2|R|B
?
?
?
(A??)?R
(??A??)
n??A??
?
?
? ?? ?
T (??,n)
B?2/n
(24)
=
(
1
?2|R|B
)
T (??, n)B?
2/n (25)
, d(?;??, |R|, B) (26)
where Eq. 24 is the result of FA(z0) ? B.
For two series {an} and {bn}, let ?an ' bn?
denote that limn?? an ? limn?? bn. In other
words, an is asymptotically larger than bn. Then,
if we changed the representation of the objec-
tive function of the ViterbiTrain problem to log-
likelihood, for ?? that maximizes Eq. 18 (with
some simple algebra) we have:
1
n
n?
i=1
log2 p(xi, z0,i | ?
?) (27)
' ?
2|R|B
n
log2 ?+
B?2
n
(
1
n
log2 T (?
?, n)
)
= ?
2|R|B
n
log2 ?? |N|
B?2
|N|n
?
A?N
H(??, A)
(28)
where
H(??, A) = ?
?
(A??)?R(A)
??A?? log2 ?
?
A??
(29)
is the entropy of the multinomial for nonter-
minal A. H(??, A) can be thought of as the
minimal number of bits required to encode a
choice of a rule from A, if chosen independently
from the other rules. All together, the quantity
B
|N|n
(?
A?NH(?
?, A)
)
is the average number of
bits required to encode a tree in our sample using
1508
??, while removing dependence among all rules
and assuming that each node at the tree is chosen
uniformly.4 This means that the log-likelihood, for
large n, is bounded from above by a linear func-
tion of the (average) number of bits required to
optimally encode n trees of total size B, while as-
suming independence among the rules in a tree.
We note that the quantityB/nwill tend toward the
average size of a tree, which, under Condition 1,
must be finite.
Our final approximate bound from Eq. 28 re-
lates the choice of distribution, from which sample
z0, to ?. The lower bound in Eq. 28 is a monotone-
decreasing function of ?. We seek to make ? as
small as possible to make the bound tight. We next
show that the uniform distribution optimizes ? in
that sense.
7.2 Optimizing ?
Note that the optimal choice of ?, for a single x
and for candidate initializer ??, is
?opt(x,??;?0) = sup
z?D(G,x)
p(z | ?0, x)
p(z | ??, x)
(30)
In order to avoid degenerate cases, we will add an-
other condition on the true model, ??:
Condition 2. There exists ? > 0 such that, for
any x ? L(G) and for any z ? D(G, x), p(z |
??, x) ? ? .
This is a strong condition, forcing the cardinal-
ity of D(G) to be finite, but it is not unreason-
able if natural language sentences are effectively
bounded in length.
Without further information about ?? (other
than that it satisfies Condition 2), we may want
to consider the worst-case scenario of possible ?,
hence we seek initializer ?0 such that
?(x;?0) , sup
?
?opt(x,?;?0) (31)
is minimized. If ?0 = ?I , then we have that
p(z | ?I , x) = |D(G, x)|?1 , ?x. Together with
Condition 2, this implies that
p(z | ?I , x)
p(z | ??, x)
?
?x
?
(32)
4We note that Grenander (1967) describes a (lin-
ear) relationship between the derivational entropy and
H(??, A). The derivational entropy is defined as h(??, A) =
?
P
x,z p(x, z | ?
?) log p(x, z | ??), where z ranges over
trees that have nonterminal A as the root. It follows im-
mediately from Grenander?s result that
P
AH(?
?, A) ?P
A h(?
?, A).
and hence ?opt(x,??) ? ?x/? for any ??, hence
?(x;?I) ? ?x/? . However, if we choose ?0 6=
?I , we have that p(z? | ?0, x) > ?x for some z?,
hence, for ?? such that it assigns probability ? on
z?, we have that
sup
z?D(G,x)
p(z | ?0, x)
p(z | ??, x)
>
?x
?
(33)
and hence ?opt(x,??;??) > ?x/? , so ?(x;??) >
?x/? . So, to optimize for the worst-case scenario
over true distributions with respect to ?, we are
motivated to choose ?0 = ?I as defined in Con-
dition 1. Indeed, UniformInit uses ?I to initialize
the state of Viterbi EM.
We note that if ?I was known for a specific
grammar, then we could have used it as a direct
initializer. However, Condition 1 only guarantees
its existence, and does not give a practical way to
identify it. In general, as mentioned above, ? = 1
can be used to obtain a weighted CFG that sat-
isfies p(z | ?, x) = 1/|D(G, x)|. Since we re-
quire a uniform posterior distribution, the num-
ber of derivations of a fixed length is finite. This
means that we can converted the weighted CFG
with ? = 1 to a PCFG with the same posterior
(Smith and Johnson, 2007), and identify the ap-
propriate ?I .
8 Related Work
Viterbi training is closely related to the k-means
clustering problem, where the objective is to find
k centroids for a given set of d-dimensional points
such that the sum of distances between the points
and the closest centroid is minimized. The ana-
log for Viterbi EM for the k-means problem is the
k-means clustering algorithm (Lloyd, 1982), a co-
ordinate ascent algorithm for solving the k-means
problem. It works by iterating between an E-like-
step, in which each point is assigned the closest
centroid, and an M-like-step, in which the cen-
troids are set to be the center of each cluster.
?k? in k-means corresponds, in a sense, to the
size of our grammar. k-means has been shown to
be NP-hard both when k varies and d is fixed and
when d varies and k is fixed (Aloise et al, 2009;
Mahajan et al, 2009). An open problem relating to
our hardness result would be whether ViterbiTrain
(or ConditionalViterbiTrain) is hard even if we do
not permit grammars of arbitrarily large size, or
at least, constrain the number of rules that do not
rewrite to terminals (in our current reduction, the
1509
size of the grammar grows as the size of the 3-SAT
formula grows).
On a related note to ?7, Arthur and Vassilvit-
skii (2007) described a greedy initialization al-
gorithm for initializing the centroids of k-means,
called k-means++. They show that their ini-
tialization is O(log k)-competitive; i.e., it ap-
proximates the optimal clusters assignment by a
factor of O(log k). In ?7.1, we showed that
uniform-at-random initialization is approximately
O(|N|L?2/n)-competitive (modulo an additive
constant) for CNF grammars, where n is the num-
ber of sentences, L is the total length of sentences
and ? is a measure for distance between the true
distribution and the uniform distribution.5
Many combinatorial problems in NLP involv-
ing phrase-structure trees, alignments, and depen-
dency graphs are hard (Sima?an, 1996; Good-
man, 1998; Knight, 1999; Casacuberta and de la
Higuera, 2000; Lyngs? and Pederson, 2002;
Udupa and Maji, 2006; McDonald and Satta,
2007; DeNero and Klein, 2008, inter alia). Of
special relevance to this paper is Abe and Warmuth
(1992), who showed that the problem of finding
maximum likelihood model of probabilistic au-
tomata is hard even for a single string and an au-
tomaton with two states. Understanding the com-
plexity of NLP problems, we believe, is crucial as
we seek effective practical approximations when
necessary.
9 Conclusion
We described some properties of Viterbi train-
ing for probabilistic context-free grammars. We
showed that Viterbi training is NP-hard and, in
fact, NP-hard to approximate. We gave motivation
for uniform-at-random initialization for deriva-
tions in the Viterbi EM algorithm.
Acknowledgments
We acknowledge helpful comments by the anony-
mous reviewers. This research was supported by
NSF grant 0915187.
References
N. Abe and M. Warmuth. 1992. On the computational
complexity of approximating distributions by prob-
5Making the assumption that the grammar is in CNF per-
mits us to use L instead of B, since there is a linear relation-
ship between them in that case.
abilistic automata. Machine Learning, 9(2?3):205?
260.
S. Abney. 2007. Semisupervised Learning for Compu-
tational Linguistics. CRC Press.
D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
2009. NP-hardness of Euclidean sum-of-squares
clustering. Machine Learning, 75(2):245?248.
D. Arthur and S. Vassilvitskii. 2007. k-means++: The
advantages of careful seeding. In Proc. of ACM-
SIAM symposium on Discrete Algorithms.
F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on probabilistic
grammars and transducers. In Proc. of ICGI.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. of AAAI.
S. B. Cohen and N. A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proc. of HLT-
NAACL.
M. Collins. 2003. Head-driven statistical models for
natural language processing. Computational Lin-
guistics, 29(4):589?637.
W. H. E. Day. 1983. Computationally difficult parsi-
mony problems in phylogenetic systematics. Jour-
nal of Theoretical Biology, 103.
J. DeNero and D. Klein. 2008. The complexity of
phrase alignment problems. In Proc. of ACL.
Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1997.
Selective sampling using the query by committee al-
gorithm. Machine Learning, 28(2?3):133?168.
S. Goldwater and M. Johnson. 2005. Bias in learning
syllable structure. In Proc. of CoNLL.
J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
U. Grenander. 1967. Syntax-controlled probabilities.
Technical report, Brown University, Division of Ap-
plied Mathematics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2006.
Adaptor grammars: A framework for specifying
compositional nonparameteric Bayesian models. In
Advances in NIPS.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL.
D. Klein and C. Manning. 2001. Natural lan-
guage grammar induction using a constituent-
context model. In Advances in NIPS.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615.
S. P. Lloyd. 1982. Least squares quantization in PCM.
In IEEE Transactions on Information Theory.
R. B. Lyngs? and C. N. S. Pederson. 2002. The con-
sensus string problem and the complexity of com-
paring hidden Markov models. Journal of Comput-
ing and System Science, 65(3):545?569.
M. Mahajan, P. Nimbhorkar, and K. Varadarajan. 2009.
The planar k-means problem is NP-hard. In Proc. of
International Workshop on Algorithms and Compu-
tation.
1510
D. McClosky, E. Charniak, and M. Johnson. 2006a.
Effective self-training for parsing. In Proc. of HLT-
NAACL.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
Proc. of COLING-ACL.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of IWPT.
R. M. Neal and G. E. Hinton. 1998. A view of the
EM algorithm that justifies incremental, sparse, and
other variants. In Learning and Graphical Models,
pages 355?368. Kluwer Academic Publishers.
K. Sima?an. 1996. Computational complexity of prob-
abilistic disambiguation by means of tree-grammars.
In In Proc. of COLING.
M. Sipser. 2006. Introduction to the Theory of Com-
putation, Second Edition. Thomson Course Tech-
nology.
N. A. Smith and M. Johnson. 2007. Weighted and
probabilistic context-free grammars are equally ex-
pressive. Computational Linguistics, 33(4):477?
491.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010. Viterbi training improves unsuper-
vised dependency parsing. In Proc. of CoNLL.
R. Udupa and K. Maji. 2006. Computational com-
plexity of statistical machine translation. In Proc. of
EACL.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for question answering. In Proc. of EMNLP.
C. Yejin and C. Cardie. 2007. Structured local training
and biased potential functions for conditional ran-
dom fields with application to coreference resolu-
tion. In Proc. of HLT-NAACL.
1511
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 409?419,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Word Alignment with Arbitrary Features
Chris Dyer Jonathan Clark Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{cdyer,jhclark,alavie,nasmith}@cs.cmu.edu
Abstract
We introduce a discriminatively trained, glob-
ally normalized, log-linear variant of the lex-
ical translation models proposed by Brown
et al (1993). In our model, arbitrary, non-
independent features may be freely incorpo-
rated, thereby overcoming the inherent limita-
tion of generative models, which require that
features be sensitive to the conditional inde-
pendencies of the generative process. How-
ever, unlike previous work on discriminative
modeling of word alignment (which also per-
mits the use of arbitrary features), the param-
eters in our models are learned from unanno-
tated parallel sentences, rather than from su-
pervised word alignments. Using a variety
of intrinsic and extrinsic measures, including
translation performance, we show our model
yields better alignments than generative base-
lines in a number of language pairs.
1 Introduction
Word alignment is an important subtask in statis-
tical machine translation which is typically solved
in one of two ways. The more common approach
uses a generative translation model that relates bilin-
gual string pairs using a latent alignment variable to
designate which source words (or phrases) generate
which target words. The parameters in these models
can be learned straightforwardly from parallel sen-
tences using EM, and standard inference techniques
can recover most probable alignments (Brown et al,
1993). This approach is attractive because it only
requires parallel training data. An alternative to the
generative approach uses a discriminatively trained
alignment model to predict word alignments in the
parallel corpus. Discriminative models are attractive
because they can incorporate arbitrary, overlapping
features, meaning that errors observed in the predic-
tions made by the model can be addressed by engi-
neering new and better features. Unfortunately, both
approaches are problematic, but in different ways.
In the case of discriminative alignment mod-
els, manual alignment data is required for train-
ing, which is problematic for at least three reasons.
Manual alignments are notoriously difficult to cre-
ate and are available only for a handful of language
pairs. Second, manual alignments impose a commit-
ment to a particular preprocessing regime; this can
be problematic since the optimal segmentation for
translation often depends on characteristics of the
test set or size of the available training data (Habash
and Sadat, 2006) or may be constrained by require-
ments of other processing components, such parsers.
Third, the ?correct? alignment annotation for differ-
ent tasks may vary: for example, relatively denser or
sparser alignments may be optimal for different ap-
proaches to (downstream) translation model induc-
tion (Lopez, 2008; Fraser, 2007).
Generative models have a different limitation: the
joint probability of a particular setting of the ran-
dom variables must factorize according to steps in a
process that successively ?generates? the values of
the variables. At each step, the probability of some
value being generated may depend only on the gen-
eration history (or a subset thereof), and the possible
values a variable will take must form a locally nor-
malized conditional probability distribution (CPD).
While these locally normalized CPDs may be pa-
409
rameterized so as to make use of multiple, overlap-
ping features (Berg-Kirkpatrick et al, 2010), the re-
quirement that models factorize according to a par-
ticular generative process imposes a considerable re-
striction on the kinds of features that can be incor-
porated. When Brown et al (1993) wanted to in-
corporate a fertility model to create their Models 3
through 5, the generative process used in Models 1
and 2 (where target words were generated one by
one from source words independently of each other)
had to be abandoned in favor of one in which each
source word had to first decide how many targets it
would generate.1
In this paper, we introduce a discriminatively
trained, globally normalized log-linear model of lex-
ical translation that can incorporate arbitrary, over-
lapping features, and use it to infer word alignments.
Our model enjoys the usual benefits of discrimina-
tive modeling (e.g., parameter regularization, well-
understood learning algorithms), but is trained en-
tirely from parallel sentences without gold-standard
word alignments. Thus, it addresses the two limita-
tions of current word alignment approaches.
This paper is structured as follows. We begin by
introducing our model (?2), and follow this with a
discussion of tractability, parameter estimation, and
inference using finite-state techniques (?3). We then
describe the specific features we used (?4) and pro-
vide experimental evaluation of the model, showing
substantial improvements in three diverse language
pairs (?5). We conclude with an analysis of related
prior work (?6) and a general discussion (?8).
2 Model
In this section, we develop a conditional model
p(t | s) that, given a source language sentence s with
length m = |s|, assigns probabilities to a target sen-
tence t with length n, where each word tj is an el-
ement in the finite target vocabulary ?. We begin
by using the chain rule to factor this probability into
two components, a translation model and a length
model.
p(t | s) = p(t, n | s) = p(t | s, n)
? ?? ?
translation model
? p(n | s)
? ?? ?
length model
1Moore (2005) likewise uses this example to motivate the
need for models that support arbitrary, overlapping features.
In the translation model, we then assume that each
word tj is a translation of one source word, or a
special null token. We therefore introduce a latent
alignment variable a = ?a1, a2, . . . , an? ? [0,m]n,
where aj = 0 represents a special null token.
p(t | s, n) =
?
a
p(t, a | s, n)
So far, our model is identical to that of (Brown et
al., 1993); however, we part ways here. Rather than
using the chain rule to further decompose this prob-
ability and motivate opportunities to make indepen-
dence assumptions, we use a log-linear model with
parameters ? ? Rk and feature vector function H
that maps each tuple ?a, s, t, n? into Rk to model
p(t, a | s, n) directly:
p?(t, a | s, n) =
exp?>H(t, a, s, n)
Z?(s, n)
, where
Z?(s, n) =
?
t???n
?
a?
exp?>H(t?, a?, s, n)
Under some reasonable assumptions (a finite target
vocabulary ? and that all ?k < ?), the partition
function Z?(s, n) will always take on finite values,
guaranteeing that p(t, a | s, n) is a proper probability
distribution.
So far, we have said little about the length model.
Since our intent here is to use the model for align-
ment, where both the target length and target string
are observed, it will not be necessary to commit to
any length model, even during training.
3 Tractability, Learning, and Inference
The model introduced in the previous section is
extremely general, and it can incorporate features
sensitive to any imaginable aspects of a sentence
pair and their alignment, from linguistically in-
spired (e.g., an indicator feature for whether both
the source and target sentences contain a verb), to
the mundane (e.g., the probability of the sentence
pair and alignment under Model 1), to the absurd
(e.g., an indicator if s and t are palindromes of each
other).
However, while our model can make use of arbi-
trary, overlapping features, when designing feature
functions it is necessary to balance expressiveness
and the computational complexity of the inference
410
algorithms used to reason under models that incor-
porate these features.2 To understand this tradeoff,
we assume that the random variables being modeled
(t, a) are arranged into an undirected graph G such
that the vertices represent the variables and the edges
are specified so that the feature function H decom-
poses linearly over all the cliques C in G,
H(t, a, s, n) =
?
C
h(tC , aC , s, n) ,
where tC and aC are the components associated with
subgraph C and h(?) is a local feature vector func-
tion. In general, exact inference is exponential in
the width of tree-decomposition of G, but, given a
fixed width, they can be solved in polynomial time
using dynamic programming. For example, when
the graph has a sequential structure, exact infer-
ence can be carried out using the familiar forward-
backward algorithm (Lafferty et al, 2001). Al-
though our features look at more structure than this,
they are designed to keep treewidth low, meaning
exact inference is still possible with dynamic pro-
gramming. Figure 1 gives a graphical representation
of our model as well as the more familiar genera-
tive (directed) variants. The edge set in the depicted
graph is determined by the features that we use (?4).
3.1 Parameter Learning
To learn the parameters of our model, we select the
?? that minimizes the `1 regularized conditional log-
likelihood of a set of training data T :
L(?) =?
?
?s,t??T
log
?
a
p?(t, a | s, n) + ?
?
k
|?k| .
Because of the `1 penalty, this objective is not every-
where differentiable, but the gradient with respect to
the parameters of the log-likelihood term is as fol-
lows.
?L
??
=
?
?s,t??T
Ep?(a|s,t,n)[H(?)]? Ep?(t,a|s,n)[H(?)]
(1)
To optimize L, we employ an online method that
approximates `1 regularization and only depends on
2One way to understand expressiveness is in terms of inde-
pendence assumptions, of course. Research in graphical models
has done much to relate independence assumptions to the com-
plexity of inference algorithms (Koller and Friedman, 2009).
the gradient of the unregularized objective (Tsu-
ruoka et al, 2009). This method is quite attrac-
tive since it is only necessary to represent the active
features, meaning impractically large feature spaces
can be searched provided the regularization strength
is sufficiently high. Additionally, not only has this
technique been shown to be very effective for opti-
mizing convex objectives, but evidence suggests that
the stochasticity of online algorithms often results
in better solutions than batch optimizers for non-
convex objectives (Liang and Klein, 2009). On ac-
count of the latent alignment variable in our model,
L is non-convex (as is the likelihood objective of the
generative variant).
To choose the regularization strength ? and the
initial learning rate ?0,3 we trained several mod-
els on a 10,000-sentence-pair subset of the French-
English Hansards, and chose values that minimized
the alignment error rate, as evaluated on a 447 sen-
tence set of manually created alignments (Mihalcea
and Pedersen, 2003). For the remainder of the ex-
periments, we use the values we obtained, ? = 0.4
and ?0 = 0.3.
3.2 Inference with WFSAs
We now describe how to use weighted finite-state
automata (WFSAs) to compute the quantities neces-
sary for training. We begin by describing the ideal
WFSA representing the full translation search space,
which we call the discriminative neighborhood, and
then discuss strategies for reducing its size in the
next section, since the full model is prohibitively
large, even with small data sets.
For each training instance ?s, t?, the contribution
to the gradient (Equation 1) is the difference in two
vectors of expectations. The first term is the ex-
pected value of H(?) when observing ?s, n, t? and
letting a range over all possible alignments. The
second is the expectation of the same function, but
observing only ?s, n? and letting t? and a take on
any possible values (i.e., all possible translations
of length n and all their possible alignments to s).
To compute these expectations, we can construct
a WFSA representing the discriminative neighbor-
hood, the set ?n?[0,m]n, such that every path from
the start state to goal yields a pair ?t?, a? with weight
3For the other free parameters of the algorithm, we use the
default values recommended by Tsuruoka et al (2009).
411
a1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
Fully directed model (Brown et al, 1993;
Vogel et al, 1996; Berg-Kirkpatrick et al, 2010)
Our model
...
a
1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
...
......
s
s s s s
ss s
Figure 1: A graphical representation of a conventional generative lexical translation model (left) and our model with
an undirected translation model. For clarity, the observed node s (representing the full source sentence) is drawn in
multiple locations. The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value).
H(t?, a, s, n). With our feature set (?4), number of
states in this WFSA isO(m?n) since at each target
index j, there is a different state for each possible in-
dex of the source word translated at position j ? 1.4
Once the WFSA representing the discriminative
neighborhood is built, we use the forward-backward
algorithm to compute the second expectation term.
We then intersect the WFSA with an unweighted
FSA representing the target sentence t (because of
the restricted structure of our WFSA, this amounts
to removing edges), and finally run the forward-
backward algorithm on the resulting WFSA to com-
pute the first expectation.
3.3 Shrinking the Discriminative
Neighborhood
The WFSA we constructed requires m? |?| transi-
tions between all adjacent states, which is impracti-
cally large. We can reduce the number of edges by
restricting the set of words that each source word can
translate into. Thus, the model will not discriminate
4States contain a bit more information than the index of the
previous source word, for example, there is some additional in-
formation about the previous translation decision that is passed
forward. However, the concept of splitting states to guarantee
distinct paths for different values of non-local features is well
understood by NLP and machine translation researchers, and
the necessary state structure should be obvious from the feature
description.
among all candidate target strings in ?n, but rather
in ?ns , where ?s =
?m
i=1 ?si , and where ?s is the
set of target words that s may translate into.5
We consider four different definitions of ?s: (1)
the baseline of the full target vocabulary, (2) the set
of all target words that co-occur in sentence pairs
containing s, (3) the most probable words under
IBM Model 1 that are above a threshold, and (4) the
same Model 1, except we add a sparse symmetric
Dirichlet prior (? = 0.01) on the translation distri-
butions and use the empirical Bayes (EB) method to
infer a point estimate, using variational inference.
Table 1: Comparison of alternative definitions ?s (arrows
indicate whether higher or lower is better).
?s time (s) ?
?
s |?s| ? AER ?
= ? 22.4 86.0M 0.0
co-occ. 8.9 0.68M 0.0
Model 1 0.2 0.38M 6.2
EB-Model 1 1.0 0.15M 2.9
Table 1 compares the average per-sentence time
required to run the inference algorithm described
5Future work will explore alternative formulations of the
discriminative neighborhood with the goal of further improving
inference efficiency. Smith and Eisner (2005) show that good
performance on unsupervised syntax learning is possible even
when learning from very small discriminative neighborhoods,
and we posit that the same holds here.
412
above under these four different definitions of ?s on
a 10,000 sentence subset of the Hansards French-
English corpus that includes manual word align-
ments. While our constructions guarantee that all
references are reachable even in the reduced neigh-
borhoods, not all alignments between source and tar-
get are possible. The last column is the oracle AER.
Although EB variant of Model 1 neighborhood is
slightly more expensive to do inference with than
regular Model 1, we use it because it has a lower
oracle AER.6
During alignment prediction (rather than during
training) for a sentence pair ?s, t?, it is possible to
further restrict ?s to be just the set of words occur-
ring in t, making extremely fast inference possible
(comparable to that of the generative HMM align-
ment model).
4 Features
Feature engineering lets us encode knowledge about
what aspects of a translation derivation are useful in
predicting whether it is good or not. In this section
we discuss the features we used in our model. Many
of these were taken from the discriminative align-
ment modeling literature, but we also note that our
features can be much more fine-grained than those
used in supervised alignment modeling, since we
learn our models from a large amount of parallel
data, rather than a small number of manual align-
ments.
Word association features. Word association fea-
tures are at the heart of all lexical translation models,
whether generative or discriminative. In addition to
fine-grained boolean indicator features ?saj , tj? for
pair types, we have several orthographic features:
identity, prefix identity, and an orthographic simi-
larity measure designed to be informative for pre-
dicting the translation of named entities in languages
that use similar alphabets.7 It has the property that
source-target pairs of long words that are similar are
given a higher score than word pairs that are short
and similar (dissimilar pairs have a score near zero,
6We included all translations whose probability was within
a factor of 10?4 of the highest probability translation.
7In experiments with Urdu, which uses an Arabic-derived
script, the orthographic feature was computed after first ap-
plying a heuristic Romanization, which made the orthographic
forms somewhat comparable.
regardless of length). We also include ?global? asso-
ciation scores that are precomputed by looking at the
full training data: Dice?s coefficient (discretized),
which we use to measure association strength be-
tween pairs of source and target word types across
sentence pairs (Dice, 1945), IBM Model 1 forward
and reverse probabilities, and the geometric mean of
the Model 1 forward and reverse probabilities. Fi-
nally, we also cluster the source and target vocab-
ularies (Och, 1999) and include class pair indicator
features, which can learn generalizations that, e.g.,
?nouns tend to translate into nouns but not modal
verbs.?
Positional features. Following Blunsom and
Cohn (2006), we include features indicating
closeness to the alignment matrix diagonal,
h(aj , j,m, n) =
?
?
?
aj
m ?
j
n
?
?
?. We also conjoin this
feature with the source word class type indicator to
enable the model to learn that certain word types
are more or less likely to favor a location on the
diagonal (e.g. Urdu?s sentence-final verbs).
Source features. Some words are functional el-
ements that fulfill purely grammatical roles and
should not be the ?source? of a translation. For ex-
ample, Romance languages require a preposition in
the formation of what could be a noun-noun com-
pound in English, thus, it may be useful to learn not
to translate certain words (i.e. they should not par-
ticipate in alignment links), or to have a bias to trans-
late others. To capture this intuition we include an
indicator feature that fires each time a source vocab-
ulary item (and source word class) participates in an
alignment link.
Source path features. One class of particularly
useful features assesses the goodness of the align-
ment ?path? through the source sentence (Vogel et
al., 1996). Although assessing the predicted path
requires using nonlocal features, since each aj ?
[0,m] and m is relatively small, features can be sen-
sitive to a wider context than is often practical.
We use many overlapping source path features,
some of which are sensitive to the distance and di-
rection of the jump between aj?1 and aj , and oth-
ers which are sensitive to the word pair these two
points define, and others that combine all three el-
ements. The features we use include a discretized
413
jump distance, the discretized jump conjoined with
an indicator feature for the target length n, the dis-
cretized jump feature conjoined with the class of saj ,
and the discretized jump feature conjoined with the
class of saj and saj?1 . To discretize the features we
take a log transform (base 1.3) of the jump width and
let an indicator feature fire for the closest integer.
In addition to these distance-dependent features, we
also include indicator features that fire on bigrams
?saj?1 , saj ? and their word classes. Thus, this fea-
ture can capture our intuition that, e.g., adjectives
are more likely to come before or after a noun in
different languages.
Target string features. Features sensitive to mul-
tiple values in the predicted target string or latent
alignment variable must be handled carefully for the
sake of computational tractability. While features
that look at multiple source words can be computed
linearly in the number of source words considered
(since the source string is always observable), fea-
tures that look at multiple target words require ex-
ponential time and space!8 However, by grouping
the tj?s into coarse equivalence classes and looking
at small numbers of variables, it is possible to incor-
porate such features. We include a feature that fires
when a word translates as itself (for example, a name
or a date, which occurs in languages that share the
same alphabet) in position j, but then is translated
again (as something else) in position j ? 1 or j + 1.
5 Experiments
We now turn to an empirical assessment of our
model. Using various datasets, we evaluate the
performance of the models? intrinsic quality and
theirtheir alignments? contribution to a standard ma-
chine translation system. We make use of parallel
corpora from languages with very different typolo-
gies: a small (0.8M words) Chinese-English corpus
from the tourism and travel domain (Takezawa et al,
2002), a corpus of Czech-English news commen-
tary (3.1M words),9 and an Urdu-English corpus
(2M words) provided by NIST for the 2009 Open
MT Evaluation. These pairs were selected since
each poses different alignment challenges (word or-
8This is of course what makes history-based language model
integration an inference challenge in translation.
9http://statmt.org/wmt10
der in Chinese and Urdu, morphological complex-
ity in Czech, and a non-alphabetic writing system in
Chinese), and confining ourselves to these relatively
small corpora reduced the engineering overhead of
getting an implementation up and running. Future
work will explore the scalability characteristics and
limits of the model.
5.1 Methodology
For each language pair, we train two log-linear
translation models as described above (?3), once
with English as the source and once with English
as the target language. For a baseline, we use
the Giza++ toolkit (Och and Ney, 2003) to learn
Model 4, again in both directions. We symmetrize
the alignments from both model types using the
grow-diag-final-and heuristic (Koehn et al,
2003) producing, in total, six alignment sets. We
evaluate them both intrinsically and in terms of their
performance in a translation system.
Since we only have gold alignments for Czech-
English (Bojar and Prokopova?, 2006), we can re-
port alignment error rate (AER; Och and Ney, 2003)
only for this pair. However, we offer two further
measures that we believe are suggestive and that
do not require gold alignments. One is the aver-
age alignment ?fertility? of source words that occur
only a single time in the training data (so-called ha-
pax legomena). This assesses the impact of a typical
alignment problem observed in generative models
trained to maximize likelihood: infrequent source
words act as ?garbage collectors?, with many target
words aligned to them (the word dislike in the Model
4 alignment in Figure 2 is an example). Thus, we ex-
pect lower values of this measure to correlate with
better alignments. The second measure is the num-
ber of rule types learned in the grammar induction
process used for translation that match the transla-
tion test sets.10 While neither a decrease in the aver-
age singleton fertility nor an increase in the number
of rules induced guarantees better alignment quality,
we believe it is reasonable to assume that they are
positively correlated.
For the translation experiments in each language
pair, we make use of the cdec decoder (Dyer et al,
10This measure does not assess whether the rule types are
good or bad, but it does suggest that the system?s coverage is
greater.
414
2010), inducing a hierarchical phrase based trans-
lation grammar from two sets of symmetrized align-
ments using the method described by Chiang (2007).
Additionally, recent work that has demonstrated that
extracting rules from n-best alignments has value
(Liu et al, 2009; Venugopal et al, 2008). We
therefore define a third condition where rules are
extracted from the corpus under both the Model 4
and discriminative alignments and merged to form
a single grammar. We incorporate a 3-gram lan-
guage model learned from the target side of the
training data as well as 50M supplemental words
of monolingual training data consisting of sentences
randomly sampled from the English Gigaword, ver-
sion 4. In the small Chinese-English travel domain
experiment, we just use the LM estimated from the
bitext. The parameters of the translation model were
tuned using ?hypergraph? minimum error rate train-
ing (MERT) to maximize BLEU on a held-out de-
velopment set (Kumar et al, 2009). Results are
reported using case-insensitive BLEU (Papineni et
al., 2002), METEOR11 (Lavie and Denkowski, 2009),
and TER (Snover et al, 2006), with the number of
references varying by task. Since MERT is a non-
deterministic optimization algorithm and results can
vary considerably between runs, we follow Clark et
al. (2011) and report the average score and stan-
dard deviation of 5 independent runs, 30 in the case
of Chinese-English, since observed variance was
higher.
5.2 Experimental Results
Czech-English. Czech-English poses problems
for word alignment models since, unlike English,
Czech words have a complex inflectional morphol-
ogy, and the syntax permits relatively free word or-
der. For this language pair, we evaluate alignment
error rate using the manual alignment corpus de-
scribed by Bojar and Prokopova? (2006). Table 2
summarizes the results.
Chinese-English. Chinese-English poses a differ-
ent set of problems for alignment. While Chinese
words have rather simple morphology, the Chinese
writing system renders our orthographic features
useless. Despite these challenges, the Chinese re-
11Meteor 1.0 with exact, stem, synonymy, and paraphrase
modules and HTER parameters.
Table 2: Czech-English experimental results. ??sing. is the
average fertility of singleton source words.
AER ? ??sing. ? # rules ?
Model 4 e | f 24.8 4.1
f | e 33.6 6.6
sym. 23.4 2.7 993,953
Our model e | f 21.9 2.3
f | e 29.3 3.8
sym. 20.5 1.6 1,146,677
Alignment BLEU ? METEOR ? TER ?
Model 4 16.3?0.2 46.1?0.1 67.4?0.3
Our model 16.5?0.1 46.8?0.1 67.0?0.2
Both 17.4?0.1 47.7?0.1 66.3?0.5
sults in Table 3 show the same pattern of results as
seen in Czech-English.
Table 3: Chinese-English experimental results.
??sing. ? # rules ?
Model 4 e | f 4.4
f | e 3.9
sym. 3.6 52,323
Our model e | f 3.5
f | e 2.6
sym. 3.1 54,077
Alignment BLEU ? METEOR ? TER ?
Model 4 56.5?0.3 73.0?0.4 29.1?0.3
Our model 57.2?0.8 73.8?0.4 29.3?1.1
Both 59.1?0.6 74.8?0.7 27.6?0.5
Urdu-English. Urdu-English is a more challeng-
ing language pair for word alignment than the pre-
vious two we have considered. The parallel data is
drawn from numerous genres, and much of it was ac-
quired automatically, making it quite noisy. So our
models must not only predict good translations, they
must cope with bad ones as well. Second, there has
been no previous work on discriminative modeling
of Urdu, since, to our knowledge, no manual align-
ments have been created. Finally, unlike English,
Urdu is a head-final language: not only does it have
SOV word order, but rather than prepositions, it has
post-positions, which follow the nouns they modify,
meaning its large scale word order is substantially
415
different from that of English. Table 4 demonstrates
the same pattern of improving results with our align-
ment model.
Table 4: Urdu-English experimental results.
??sing. ? # rules ?
Model 4 e | f 6.5
f | e 8.0
sym. 3.2 244,570
Our model e | f 4.8
f | e 8.3
sym. 2.3 260,953
Alignment BLEU ? METEOR ? TER ?
Model 4 23.3?0.2 49.3?0.2 68.8?0.8
Our model 23.4?0.2 49.7?0.1 67.7?0.2
Both 24.1?0.2 50.6?0.1 66.8?0.5
5.3 Analysis
The quantitative results presented in this section
strongly suggest that our modeling approach pro-
duces better alignments. In this section, we try to
characterize how the model is doing what it does
and what it has learned. Because of the `1 regular-
ization, the number of active (non-zero) features in
the inferred models is small, relative to the number
of features considered during training. The num-
ber of active features ranged from about 300k for
the small Chinese-English corpus to 800k for Urdu-
English, which is less than one tenth of the available
features in both cases. In all models, the coarse fea-
tures (Model 1 probabilities, Dice coefficient, coarse
positional features, etc.) typically received weights
with large magnitudes, but finer features also played
an important role.
Language pair differences manifested themselves
in many ways in the models that were learned.
For example, orthographic features were (unsurpris-
ingly) more valuable in Czech-English, with their
largely overlapping alphabets, than in Chinese or
Urdu. Examining the more fine-grained features is
also illuminating. Table 5 shows the most highly
weighted source path bigram features on the three
models where English was the source language, and
in each, we may observe some interesting character-
istics of the target language. Left-most is English-
Czech. At first it may be surprising that words like
since and that have a highly weighted feature for
transitioning to themselves. However, Czech punc-
tuation rules require that relative clauses and sub-
ordinating conjunctions be preceded by a comma
(which is only optional or outright forbidden in En-
glish), therefore our model translates these words
twice, once to produce the comma, and a second
time to produce the lexical item. The middle col-
umn is the English-Chinese model. In the training
data, many of the sentences are questions directed to
a second person, you. However, Chinese questions
do not invert and the subject remains in the canon-
ical first position, thus the transition from the start
of sentence to you is highly weighted. Finally, Fig-
ure 2 illustrates how Model 4 (left) and our discrimi-
native model (right) align an English-Urdu sentence
pair (the English side is being conditioned on in both
models). A reflex of Urdu?s head-final word order
is seen in the list of most highly weighted bigrams,
where a path through the English source where verbs
that transition to end-of-sentence periods are predic-
tive of good translations into Urdu.
Table 5: The most highly weighted source path bigram
features in the English-Czech, -Chinese, and -Urdu mod-
els.
Bigram ?k
. ?/s? 3.08
like like 1.19
one of 1.06
? . 0.95
that that 0.92
is but 0.92
since since 0.84
?s? when 0.83
, how 0.83
, not 0.83
Bigram ?k
. ?/s? 2.67
? ? 2.25
?s? please 2.01
much ? 1.61
?s? if 1.58
thank you 1.47
?s? sorry 1.46
?s? you 1.45
please like 1.24
?s? this 1.19
Bigram ?k
. ?/s? 1.87
?s? this 1.24
will . 1.17
are . 1.16
is . 1.09
is that 1.00
have . 0.97
has . 0.96
was . 0.91
will ?/s? 0.88
6 Related Work
The literature contains numerous descriptions of dis-
criminative approaches to word alignment motivated
by the desire to be able to incorporate multiple,
overlapping knowledge sources (Ayan et al, 2005;
Moore, 2005; Taskar et al, 2005; Blunsom and
Cohn, 2006; Haghighi et al, 2009; Liu et al, 2010;
DeNero and Klein, 2010; Setiawan et al, 2010).
This body of work has been an invaluable source
of useful features. Several authors have dealt with
the problem training log-linear models in an unsu-
416
IBM Model 4 alignment Our model's alignment
Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model
4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model
does not exhibit these problems, and in fact, makes no mistakes in the alignment.
pervised setting. The contrastive estimation tech-
nique proposed by Smith and Eisner (2005) is glob-
ally normalized (and thus capable of dealing with ar-
bitrary features), and closely related to the model we
developed; however, they do not discuss the problem
of word alignment. Berg-Kirkpatrick et al (2010)
learn locally normalized log-linear models in a gen-
erative setting. Globally normalized discriminative
models with latent variables (Quattoni et al, 2004)
have been used for a number of language processing
problems, including MT (Dyer and Resnik, 2010;
Blunsom et al, 2008a). However, this previous
work relied on translation grammars constructed us-
ing standard generative word alignment processes.
7 Future Work
While we have demonstrated that this model can be
substantially useful, it is limited in some important
ways which are being addressed in ongoing work.
First, training is expensive, and we are exploring al-
ternatives to the conditional likelihood objective that
is currently used, such as contrastive neighborhoods
advocated by (Smith and Eisner, 2005). Addition-
ally, there is much evidence that non-local features
like the source word fertility are (cf. IBM Model 3)
useful for translation and alignment modeling. To be
truly general, it must be possible to utilize such fea-
tures. Unfortunately, features like this that depend
on global properties of the alignment vector, a, make
the inference problem NP-hard, and approximations
are necessary. Fortunately, there is much recent
work on approximate inference techniques for incor-
porating nonlocal features (Blunsom et al, 2008b;
Gimpel and Smith, 2009; Cromie`res and Kurohashi,
2009; Weiss and Taskar, 2010), suggesting that this
problem too can be solved using established tech-
niques.
8 Conclusion
We have introduced a globally normalized, log-
linear lexical translation model that can be trained
discriminatively using only parallel sentences,
which we apply to the problem of word alignment.
Our approach addresses two important shortcomings
of previous work: (1) that local normalization of
generative models constrains the features that can be
used, and (2) that previous discriminatively trained
word alignment models required supervised align-
ments. According to a variety of measures in a vari-
ety of translation tasks, this model produces superior
alignments to generative approaches. Furthermore,
the features learned by our model reveal interesting
characteristics of the language pairs being modeled.
Acknowledgments
This work was supported in part by the DARPA GALE
program; the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
417
ber W911NF-10-1-0533; and the National Science Foun-
dation through grants IIS-0844507, IIS-0915187, IIS-
0713402, and IIS-0915327 and through TeraGrid re-
sources provided by the Pittsburgh Supercomputing Cen-
ter under grant number TG-DBS110003. We thank
Ondr?ej Bojar for providing the Czech-English alignment
data, and three anonymous reviewers for their detailed
suggestions and comments on an earlier draft of this pa-
per.
References
N. F. Ayan, B. J. Dorr, and C. Monz. 2005. NeurAlign:
combining word alignments using neural networks. In
Proc. of HLT-EMNLP.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In Proc. of
ACL.
P. Blunsom, T. Cohn, and M. Osborne. 2008a. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2008b. Proba-
bilistic inference for machine translation. In Proc. of
EMNLP 2008.
O. Bojar and M. Prokopova?. 2006. Czech-English word
alignment. In Proc. of LREC.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Bet-
ter hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proc. of
ACL.
F. Cromie`res and S. Kurohashi. 2009. An alignment al-
gorithm using belief propagation and a structure-based
distortion model. In Proc. of EACL.
J. DeNero and D. Klein. 2010. Discriminative modeling
of extraction sets for machine translation. In Proc. of
ACL.
L. R. Dice. 1945. Measures of the amount of eco-
logic association between species. Journal of Ecology,
26:297?302.
C. Dyer and P. Resnik. 2010. Context-free reordering,
finite-state translation. In Proc. of NAACL.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
A. Fraser. 2007. Improved Word Alignments for Statis-
tical Machine Translation. Ph.D. thesis, University of
Southern California.
K. Gimpel and N. A. Smith. 2009. Cube summing, ap-
proximate inference with non-local features, and dy-
namic programming without semirings. In Proc. of
EACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL-IJCNLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. Lavie and M. Denkowski. 2009. The METEOR metric
for automatic evaluation of machine translation. Ma-
chine Translation Journal, 23(2?3):105?115.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
Y. Liu, T. Xia, X. Xiao, and Q. Liu. 2009. Weighted
alignment matrices for statistical machine translation.
In Proc. of EMNLP.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3):303?339.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proc. of the Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proc. of HLT-EMNLP.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 1999. An efficient method for determining bilin-
gual word classes. In Proc. of EACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
418
A. Quattoni, M. Collins, and T. Darrell. 2004. Condi-
tional random fields for object recognition. In NIPS
17.
H. Setiawan, C. Dyer, and P. Resnik. 2010. Discrimina-
tive word alignment with a function word reordering
model. In Proc. of EMNLP.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
M. Snover, B. J. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proc. of LREC.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. of HLT-EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
A. Venugopal, A. Zollmann, N. A. Smith, and S. Vogel.
2008. Wider pipelines: n-best alignments and parses
in MT training. In Proc. of AMTA.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS.
419
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1365?1374,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Discovering Sociolinguistic Associations with Structured Sparsity
Jacob Eisenstein Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,nasmith,epxing}@cs.cmu.edu
Abstract
We present a method to discover robust and
interpretable sociolinguistic associations from
raw geotagged text data. Using aggregate de-
mographic statistics about the authors? geo-
graphic communities, we solve a multi-output
regression problem between demographics
and lexical frequencies. By imposing a com-
posite `1,? regularizer, we obtain structured
sparsity, driving entire rows of coefficients
to zero. We perform two regression studies.
First, we use term frequencies to predict de-
mographic attributes; our method identifies a
compact set of words that are strongly asso-
ciated with author demographics. Next, we
conjoin demographic attributes into features,
which we use to predict term frequencies. The
composite regularizer identifies a small num-
ber of features, which correspond to com-
munities of authors united by shared demo-
graphic and linguistic properties.
1 Introduction
How is language influenced by the speaker?s so-
ciocultural identity? Quantitative sociolinguistics
usually addresses this question through carefully
crafted studies that correlate individual demographic
attributes and linguistic variables?for example, the
interaction between income and the ?dropped r? fea-
ture of the New York accent (Labov, 1966). But
such studies require the knowledge to select the
?dropped r? and the speaker?s income, from thou-
sands of other possibilities. In this paper, we present
a method to acquire such patterns from raw data. Us-
ing multi-output regression with structured sparsity,
our method identifies a small subset of lexical items
that are most influenced by demographics, and dis-
covers conjunctions of demographic attributes that
are especially salient for lexical variation.
Sociolinguistic associations are difficult to model,
because the space of potentially relevant interactions
is large and complex. On the linguistic side there
are thousands of possible variables, even if we limit
ourselves to unigram lexical features. On the demo-
graphic side, the interaction between demographic
attributes is often non-linear: for example, gender
may negate or amplify class-based language differ-
ences (Zhang, 2005). Thus, additive models which
assume that each demographic attribute makes a lin-
ear contribution are inadequate.
In this paper, we explore the large space of po-
tential sociolinguistic associations using structured
sparsity. We treat the relationship between language
and demographics as a set of multi-input, multi-
output regression problems. The regression coeffi-
cients are arranged in a matrix, with rows indicating
predictors and columns indicating outputs. We ap-
ply a composite regularizer that drives entire rows
of the coefficient matrix to zero, yielding compact,
interpretable models that reuse features across dif-
ferent outputs. If we treat the lexical frequencies
as inputs and the author?s demographics as outputs,
the induced sparsity pattern reveals the set of lexi-
cal items that is most closely tied to demographics.
If we treat the demographic attributes as inputs and
build a model to predict the text, we can incremen-
tally construct a conjunctive feature space of demo-
graphic attributes, capturing key non-linear interac-
tions.
1365
The primary purpose of this research is ex-
ploratory data analysis to identify both the most
linguistic-salient demographic features, and the
most demographically-salient words. However, this
model also enables predictions about demographic
features by analyzing raw text, potentially support-
ing applications in targeted information extraction
or advertising. On the task of predicting demo-
graphics from text, we find that our sparse model
yields performance that is statistically indistinguish-
able from the full vocabulary, even with a reduction
in the model complexity an order of magnitude. On
the task of predicting text from author demograph-
ics, we find that our incrementally constructed fea-
ture set obtains significantly better perplexity than a
linear model of demographic attributes.
2 Data
Our dataset is derived from prior work in which
we gathered the text and geographical locations of
9,250 microbloggers on the website twitter.
com (Eisenstein et al, 2010). Bloggers were se-
lected from a pool of frequent posters whose mes-
sages include metadata indicating a geographical lo-
cation within a bounding box around the continen-
tal United States. We limit the vocabulary to the
5,418 terms which are used by at least 40 authors; no
stoplists are applied, as the use of standard or non-
standard orthography for stopwords (e.g., to vs. 2)
may convey important information about the author.
The dataset includes messages during the first week
of March 2010.
O?Connor et al (2010) obtained aggregate demo-
graphic statistics for these data by mapping geoloca-
tions to publicly-available data from the U. S. Cen-
sus ZIP Code Tabulation Areas (ZCTA).1 There
are 33,178 such areas in the USA (the 9,250 mi-
crobloggers in our dataset occupy 3,458 unique ZC-
TAs), and they are designed to contain roughly
equal numbers of inhabitants and demographically-
homogeneous populations. The demographic at-
tributes that we consider in this paper are shown
in Table 1. All attributes are based on self-reports.
The race and ethnicity attributes are not mutually
exclusive?individuals can indicate any number of
races or ethnicities. The ?other language? attribute
1http://www.census.gov/support/cen2000.
html
mean std. dev.
race & ethnicity
% white 52.1 29.0
% African American 32.2 29.1
% Hispanic 15.7 18.3
language
% English speakers 73.7 18.4
% Spanish speakers 14.6 15.6
% other language speakers 11.7 9.2
socioeconomic
% urban 95.1 14.3
% with family 64.1 14.4
% renters 48.9 23.4
median income ($) 42,500 18,100
Table 1: The demographic attributes used in this research.
aggregates all languages besides English and Span-
ish. ?Urban areas? refer to sets of census tracts or
census blocks which contain at least 2,500 residents;
our ?% urban? attribute is the percentage of individ-
uals in each ZCTA who are listed as living in an ur-
ban area. We also consider the percentage of indi-
viduals who live with their families, the percentage
who live in rented housing, and the median reported
income in each ZCTA.
While geographical aggregate statistics are fre-
quently used to proxy for individual socioeconomic
status in research areas such as public health (e.g.,
Rushton, 2008), it is clear that interpretation must
proceed with caution. Consider an author from a ZIP
code in which 60% of the residents are Hispanic:2
we do not know the likelihood that the author is His-
panic, because the set of Twitter users is not a rep-
resentative sample of the overall population. Polling
research suggests that users of both Twitter (Smith
and Rainie, 2010) and geolocation services (Zick-
uhr and Smith, 2010) are much more diverse with
respect to age, gender, race and ethnicity than the
general population of Internet users. Nonetheless,
at present we can only use aggregate statistics to
make inferences about the geographic communities
in which our authors live, and not the authors them-
selves.
2In the U.S. Census, the official ethnonym is Hispanic or
Latino; for brevity we will use Hispanic in the rest of this paper.
1366
3 Models
The selection of both words and demographic fea-
tures can be framed in terms of multi-output regres-
sion with structured sparsity. To select the lexical
indicators that best predict demographics, we con-
struct a regression problem in which term frequen-
cies are the predictors and demographic attributes
are the outputs; to select the demographic features
that predict word use, this arrangement is reversed.
Through structured sparsity, we learn models in
which entire sets of coefficients are driven to zero;
this tells us which words and demographic features
can safely be ignored.
This section describes the model and implemen-
tation for output-regression with structured sparsity;
in Section 4 and 5 we give the details of its applica-
tion to select terms and demographic features. For-
mally, we consider the linear equationY = XB+,
where,
? Y is the dependent variable matrix, with di-
mensions N ? T , where N is the number of
samples and T is the number of output dimen-
sions (or tasks);
? X is the independent variable matrix, with di-
mensions N ? P , where P is the number of
input dimensions (or predictors);
? B is the matrix of regression coefficients, with
dimensions P ? T ;
?  is a N ? T matrix in which each element is
noise from a zero-mean Gaussian distribution.
We would like to solve the unconstrained opti-
mization problem,
minimizeB ||Y ?XB||
2
F + ?R(B), (1)
where ||A||2F indicates the squared Frobenius norm?
i
?
j a
2
ij , and the function R(B) defines a norm
on the regression coefficients B. Ridge regres-
sion applies the `2 norm R(B) =
?T
t=1
??P
p b
2
pt,
and lasso regression applies the `1 norm R(B) =?T
t=1
?P
p |bpt|; in both cases, it is possible to de-
compose the multi-output regression problem, treat-
ing each output dimension separately. However, our
working hypothesis is that there will be substantial
correlations across both the vocabulary and the de-
mographic features?for example, a demographic
feature such as the percentage of Spanish speakers
will predict a large set of words. Our goal is to select
a small set of predictors yielding good performance
across all output dimensions. Thus, we desire struc-
tured sparsity, in which entire rows of the coefficient
matrix B are driven to zero.
Structured sparsity is not achieved by the lasso?s
`1 norm. The lasso gives element-wise sparsity, in
which many entries ofB are driven to zero, but each
predictor may have a non-zero value for some output
dimension. To drive entire rows of B to zero, we re-
quire a composite regularizer. We consider the `1,?
norm, which is the sum of `? norms across output
dimensions: R(B) =
?T
t maxp bpt (Turlach et al,
2005). This norm, which corresponds to a multi-
output lasso regression, has the desired property of
driving entire rows of B to zero.
3.1 Optimization
There are several techniques for solving the `1,?
normalized regression, including interior point
methods (Turlach et al, 2005) and projected gradi-
ent (Duchi et al, 2008; Quattoni et al, 2009). We
choose the blockwise coordinate descent approach
of Liu et al (2009) because it is easy to implement
and efficient: the time complexity of each iteration
is independent of the number of samples.3
Due to space limitations, we defer to Liu et al
(2009) for a complete description of the algorithm.
However, we note two aspects of our implementa-
tion which are important for natural language pro-
cessing applications. The algorithm?s efficiency is
accomplished by precomputing the matrices C =
X?TY? and D = X?TX?, where X? and Y? are the stan-
dardized versions ofX andY, obtained by subtract-
ing the mean and scaling by the variance. Explicit
mean correction would destroy the sparse term fre-
quency data representation and render us unable to
store the data in memory; however, we can achieve
the same effect by computing C = XTY ?N x?Ty?,
where x? and y? are row vectors indicating the means
3Our implementation is available at http://sailing.
cs.cmu.edu/sociolinguistic.html.
1367
ofX andY respectively.4 We can similarly compute
D = XTX?N x?Tx?.
If the number of predictors is too large, it may
not be possible to store the dense matrix D in mem-
ory. We have found that approximation based on the
truncated singular value decomposition provides an
effective trade-off of time for space. Specifically, we
compute XTX ?
USVT
(
USVT
)T
= U
(
SVTVSTUT
)
= UM.
Lower truncation levels are less accurate, but are
faster and require less space: for K singular val-
ues, the storage cost is O(KP ), instead of O(P 2);
the time cost increases by a factor of K. This ap-
proximation was not necessary in the experiments
presented here, although we have found that it per-
forms well as long as the regularizer is not too close
to zero.
3.2 Regularization
The regularization constant ? can be computed us-
ing cross-validation. As ? increases, we reuse the
previous solution of B for initialization; this ?warm
start? trick can greatly accelerate the computation
of the overall regularization path (Friedman et al,
2010). At each ?i, we solve the sparse multi-output
regression; the solution Bi defines a sparse set of
predictors for all tasks.
We then use this limited set of predictors to con-
struct a new input matrix X?i, which serves as the
input in a standard ridge regression, thus refitting
the model. The tuning set performance of this re-
gression is the score for ?i. Such post hoc refitting
is often used in tandem with the lasso and related
sparse methods; the effectiveness of this procedure
has been demonstrated in both theory (Wasserman
and Roeder, 2009) and practice (Wu et al, 2010).
The regularization parameter of the ridge regression
is determined by internal cross-validation.
4 Predicting Demographics from Text
Sparse multi-output regression can be used to select
a subset of vocabulary items that are especially in-
dicative of demographic and geographic differences.
4Assume without loss of generality that X and Y are scaled
to have variance 1, because this scaling does not affect the spar-
sity pattern.
Starting from the regression problem (1), the predic-
tors X are set to the term frequencies, with one col-
umn for each word type and one row for each author
in the dataset. The outputsY are set to the ten demo-
graphic attributes described in Table 1 (we consider
much larger demographic feature spaces in the next
section) The `1,? regularizer will drive entire rows
of the coefficient matrix B to zero, eliminating all
demographic effects for many words.
4.1 Quantitative Evaluation
We evaluate the ability of lexical features to predict
the demographic attributes of their authors (as prox-
ied by the census data from the author?s geograph-
ical area). The purpose of this evaluation is to as-
sess the predictive ability of the compact subset of
lexical items identified by the multi-output lasso, as
compared with the full vocabulary. In addition, this
evaluation establishes a baseline for performance on
the demographic prediction task.
We perform five-fold cross-validation, using the
multi-output lasso to identify a sparse feature set
in the training data. We compare against several
other dimensionality reduction techniques, match-
ing the number of features obtained by the multi-
output lasso at each fold. First, we compare against
a truncated singular value decomposition, with the
truncation level set to the number of terms selected
by the multi-output lasso; this is similar in spirit to
vector-based lexical semantic techniques (Schu?tze
and Pedersen, 1993). We also compare against sim-
ply selecting the N most frequent terms, and the N
terms with the greatest variance in frequency across
authors. Finally, we compare against the complete
set of all 5,418 terms. As before, we perform post
hoc refitting on the training data using a standard
ridge regression. The regularization constant for the
ridge regression is identified using nested five-fold
cross validation within the training set.
We evaluate on the refit models on the heldout
test folds. The scoring metric is Pearson?s correla-
tion coefficient between the predicted and true de-
mographics: ?(y, y?) = cov(y,y?)?y?y? , with cov(y, y?) in-
dicating the covariance and ?y indicating the stan-
dard deviation. On this metric, a perfect predictor
will score 1 and a random predictor will score 0. We
report the average correlation across all ten demo-
1368
102 103
0.16
0.18
0.2
0.22
0.24
0.26
0.28
number of features
av
er
age
 co
rre
lati
on
 
 
multi?output lasso
SVD
highest variance
most frequent
Figure 1: Average correlation plotted against the number
of active features (on a logarithmic scale).
graphic attributes, as well as the individual correla-
tions.
Results Table 2 shows the correlations obtained
by regressions performed on a range of different vo-
cabularies, averaged across all five folds. Linguistic
features are best at predicting race, ethnicity, lan-
guage, and the proportion of renters; the other de-
mographic attributes are more difficult to predict.
Among feature sets, the highest average correlation
is obtained by the full vocabulary, but the multi-
output lasso obtains nearly identical performance
using a feature set that is an order of magnitude
smaller. Applying the Fischer transformation, we
find that all correlations are statistically significant
at p < .001.
The Fischer transformation can also be used to
estimate 95% confidence intervals around the cor-
relations. The extent of the confidence intervals
varies slightly across attributes, but all are tighter
than ?0.02. We find that the multi-output lasso and
the full vocabulary regression are not significantly
different on any of the attributes. Thus, the multi-
output lasso achieves a 93% compression of the fea-
ture set without a significant decrease in predictive
performance. The multi-output lasso yields higher
correlations than the other dimensionality reduction
techniques on all of the attributes; these differences
are statistically significant in many?but not all?
cases. The correlations for each attribute are clearly
not independent, so we do not compare the average
across attributes.
Recall that the regularization coefficient was cho-
sen by nested cross-validation within the training
set; the average number of features selected is
394.6. Figure 1 shows the performance of each
dimensionality-reduction technique across the reg-
ularization path for the first of five cross-validation
folds. Computing the truncated SVD of a sparse ma-
trix at very large truncation levels is computationally
expensive, so we cannot draw the complete perfor-
mance curve for this method. The multi-output lasso
dominates the alternatives, obtaining a particularly
strong advantage with very small feature sets. This
demonstrates its utility for identifying interpretable
models which permit qualitative analysis.
4.2 Qualitative Analysis
For a qualitative analysis, we retrain the model on
the full dataset, and tune the regularization to iden-
tify a compact set of 69 features. For each identified
term, we apply a significance test on the relationship
between the presence of each term and the demo-
graphic indicators shown in the columns of the ta-
ble. Specifically, we apply the Wald test for compar-
ing the means of independent samples, while mak-
ing the Bonferroni correction for multiple compar-
isons (Wasserman, 2003). The use of sparse multi-
output regression for variable selection increases the
power of post hoc significance testing, because the
Bonferroni correction bases the threshold for sta-
tistical significance on the total number of compar-
isons. We find 275 associations at the p < .05 level;
at the higher threshold required by a Bonferroni cor-
rection for comparisons among all terms in the vo-
cabulary, 69 of these associations would have been
missed.
Table 3 shows the terms identified by our model
which have a significant correlation with at least one
of the demographic indicators. We divide words in
the list into categories, which order alphabetically
by the first word in each category: emoticons; stan-
dard English, defined as words with Wordnet entries;
proper names; abbreviations; non-English words;
non-standard words used with English. The cate-
gorization was based on the most frequent sense in
an informal analysis of our data. A glossary of non-
standard terms is given in Table 4.
Some patterns emerge from Table 3. Standard
English words tend to appear in areas with more
1369
vocabulary # features av
er
ag
e
w
hi
te
A
fr
.A
m
.
H
is
p.
E
ng
.l
an
g.
S
pa
n.
la
ng
.
ot
he
r
la
ng
.
ur
ba
n
fa
m
il
y
re
nt
er
m
ed
.i
nc
.
full 5418 0.260 0.337 0.318 0.296 0.384 0.296 0.256 0.155 0.113 0.295 0.152
multi-output lasso
394.6
0.260 0.326 0.308 0.304 0.383 0.303 0.249 0.153 0.113 0.302 0.156
SVD 0.237 0.321 0.299 0.269 0.352 0.272 0.226 0.138 0.081 0.278 0.136
highest variance 0.220 0.309 0.287 0.245 0.315 0.248 0.199 0.132 0.085 0.250 0.135
most frequent 0.204 0.294 0.264 0.222 0.293 0.229 0.178 0.129 0.073 0.228 0.126
Table 2: Correlations between predicted and observed demographic attributes, averaged across cross validation folds.
English speakers; predictably, Spanish words tend
to appear in areas with Spanish speakers and His-
panics. Emoticons tend to be used in areas with
many Hispanics and few African Americans. Ab-
breviations (e.g., lmaoo) have a nearly uniform
demographic profile, displaying negative correla-
tions with whites and English speakers, and posi-
tive correlations with African Americans, Hispanics,
renters, Spanish speakers, and areas classified as ur-
ban.
Many non-standard English words (e.g., dats)
appear in areas with high proportions of renters,
African Americans, and non-English speakers,
though a subset (haha, hahaha, and yep) display
the opposite demographic pattern. Many of these
non-standard words are phonetic transcriptions of
standard words or phrases: that?s?dats, what?s
up?wassup, I?m going to?ima. The relationship
between these transcriptions and the phonological
characteristics of dialects such as African-American
Vernacular English is a topic for future work.
5 Conjunctive Demographic Features
Next, we demonstrate how to select conjunctions of
demographic features that predict text. Again, we
apply multi-output regression, but now we reverse
the direction of inference: the predictors are demo-
graphic features, and the outputs are term frequen-
cies. The sparsity-inducing `1,? norm will select a
subset of demographic features that explain the term
frequencies.
We create an initial feature set f (0)(X) by bin-
ning each demographic attribute, using five equal-
frequency bins. We then constructive conjunctive
features by applying a procedure inspired by related
work in computational biology, called ?Screen and
Clean? (Wu et al, 2010). On iteration i:
? Solve the sparse multi-output regression prob-
lem Y = f (i)(X)B(i) + .
? Select a subset of features S(i) such that m ?
S(i) iff maxj |b
(i)
m,j | > 0. These are the row
indices of the predictors with non-zero coeffi-
cients.
? Create a new feature set f (i+1)(X), including
the conjunction of each feature (and its nega-
tion) in S(i) with each feature in the initial set
f (0)(X).
We iterate this process to create features that con-
join as many as three attributes. In addition to the
binned versions of the demographic attributes de-
scribed in Table 1, we include geographical infor-
mation. We built Gaussian mixture models over the
locations, with 3, 5, 8, 12, 17, and 23 components.
For each author we include the most likely cluster
assignment in each of the six mixture models. For
efficiency, the outputs Y are not set to the raw term
frequencies; instead we compute a truncated sin-
gular value decomposition of the term frequencies
W ? UVDT, and use the basis U. We set the trun-
cation level to 100.
5.1 Quantitative Evaluation
The ability of the induced demographic features to
predict text is evaluated using a traditional perplex-
ity metric. The same test and training split is used
from the vocabulary experiments. We construct a
language model from the induced demographic fea-
tures by training a multi-output ridge regression,
which gives a matrix B? that maps from demographic
features to term frequencies across the entire vocab-
ulary. For each document in the test set, the ?raw?
predicted language model is y?d = f(xd)B, which
is then normalized. The probability mass assigned
1370
w
hi
te
A
fr
.A
m
.
H
is
p.
E
ng
.l
an
g.
S
pa
n.
la
ng
.
ot
he
r
la
ng
.
ur
ba
n
fa
m
il
y
re
nt
er
m
ed
.i
nc
.
- - - + - + + +
;) - + - +
:( -
:) -
:d + - + - +
as - + -
awesome + - - - +
break - + - -
campus - + - -
dead - + - + + +
hell - + - -
shit - +
train - + +
will - + -
would + -
atlanta - + - -
famu + - + - - -
harlem - +
bbm - + - + + +
lls + - + - -
lmaoo - + + - + + + +
lmaooo - + + - + + + +
lmaoooo - + + - + + +
lmfaoo - + - + + +
lmfaooo - + - + + +
lml - + + - + + + + -
odee - + - + + +
omw - + + - + + + +
smfh - + + - + + + +
smh - + + +
w| - + - + + + +
con + - + +
la - + - +
si - + - +
dats - + - + -
deadass - + + - + + + +
haha + - -
hahah + -
hahaha + - - +
ima - + - + +
madd - - + +
nah - + - + + +
ova - + - +
sis - + +
skool - + - + + + -
wassup - + + - + + + + -
wat - + + - + + + + -
ya - + +
yall - +
yep - + - - - -
yoo - + + - + + + +
yooo - + - + +
Table 3: Demographically-indicative terms discovered by
multi-output sparse regression. Statistically significant
(p < .05) associations are marked with a + or -.
term definition
bbm Blackberry Messenger
dats that?s
dead(ass) very
famu Florida Agricultural
and Mechanical Univ.
ima I?m going to
lls laughing like shit
lm(f)ao+ laughing my (fucking)
ass off
lml love my life
madd very, lots
nah no
odee very
term definition
omw on my way
ova over
sis sister
skool school
sm(f)h shake my (fuck-
ing) head
w| with
wassup what?s up
wat what
ya your, you
yall you plural
yep yes
yoo+ you
Table 4: A glossary of non-standard terms from Ta-
ble 3. Definitions are obtained by manually inspecting
the context in which the terms appear, and by consulting
www.urbandictionary.com.
model perplexity
induced demographic features 333.9
raw demographic attributes 335.4
baseline (no demographics) 337.1
Table 5: Word perplexity on test documents, using
language models estimated from induced demographic
features, raw demographic attributes, and a relative-
frequency baseline. Lower scores are better.
to unseen words is determined through nested cross-
validation. We compare against a baseline language
model obtained from the training set, again using
nested cross-validation to set the probability of un-
seen terms.
Results are shown in Table 5. The language mod-
els induced from demographic data yield small but
statistically significant improvements over the base-
line (Wilcoxon signed-rank test, p < .001). More-
over, the model based on conjunctive features signif-
icantly outperforms the model constructed from raw
attributes (p < .001).
5.2 Features Discovered
Our approach discovers 37 conjunctive features,
yielding the results shown in Table 5. We sort all
features by frequency, and manually select a sub-
set to display in Table 6. Alongside each feature,
we show the words with the highest and lowest log-
odds ratios with respect to the feature. Many of these
terms are non-standard; while space does not permit
a complete glossary, some are defined in Table 4 or
in our earlier work (Eisenstein et al, 2010).
1371
feature positive terms negative terms
1 geo: Northeast m2 brib mangoville soho odeee fasho #ilovefamu foo coo fina
2 geo: NYC mangoville lolss m2 brib wordd bahaha fasho goofy #ilovefamu
tacos
4 geo: South+Midwest renter? 0.615 white? 0.823 hme muthafucka bae charlotte tx odeee m2 lolss diner mangoville
7 Afr. Am.> 0.101 renter> 0.615 Span. lang.> 0.063 dhat brib odeee lolss wassupp bahaha charlotte california ikr en-
ter
8 Afr. Am.? 0.207 Hispanic> 0.119 Span. lang.> 0.063 les ahah para san donde bmore ohio #lowkey #twitterjail
nahhh
9 geo: NYC Span. lang.? 0.213 mangoville thatt odeee lolss
buzzin
landed rodney jawn wiz golf
12 Afr. Am.> 0.442 geo: South+Midwest white? 0.823 #ilovefamu panama midtermswillies #lowkey knoe esta pero odeee hii
15 geo: West Coast other lang.> 0.110 ahah fasho san koo diego granted pride adore phat pressure
17 Afr. Am.> 0.442 geo: NYC other lang.? 0.110 lolss iim buzzin qonna qood foo tender celebs pages pandora
20 Afr. Am.? 0.207 Span. lang.> 0.063 white> 0.823 del bby cuando estoy muscle knicks becoming uncomfortablelarge granted
23 Afr. Am.? 0.050 geo: West Span. lang.? 0.106 leno it?d 15th hacked government knicks liquor uu hunn homee
33 Afr. Am.> 0.101 geo: SF Bay Span. lang.> 0.063 hella aha california bay o.o aj everywhere phones shift re-gardless
36 Afr. Am.? 0.050 geo: DC/Philadelphia Span. lang.? 0.106 deh opens stuffed yaa bmore hmmmmm dyin tea cousin hella
Table 6: Conjunctive features discovered by our method with a strong sparsity-inducing prior, ordered by frequency.
We also show the words with high log-odds for each feature (postive terms) and its negation (negative terms).
In general, geography was a strong predictor, ap-
pearing in 25 of the 37 conjunctions. Features 1
and 2 (F1 and F2) are purely geographical, captur-
ing the northeastern United States and the New York
City area. The geographical area of F2 is completely
contained by F1; the associated terms are thus very
similar, but by having both features, the model can
distinguish terms which are used in northeastern ar-
eas outside New York City, as well as terms which
are especially likely in New York.5
Several features conjoin geography with demo-
graphic attributes. For example, F9 further refines
the New York City area by focusing on communities
that have relatively low numbers of Spanish speak-
ers; F17 emphasizes New York neighborhoods that
have very high numbers of African Americans and
few speakers of languages other than English and
Spanish. The regression model can use these fea-
tures in combination to make fine-grained distinc-
tions about the differences between such neighbor-
hoods. Outside New York, we see that F4 combines
a broad geographic area with attributes that select at
least moderate levels of minorities and fewer renters
(a proxy for areas that are less urban), while F15
identifies West Coast communities with large num-
5Mangoville and M2 are clubs in New York; fasho and coo
were previously found to be strongly associated with the West
Coast (Eisenstein et al, 2010).
bers of speakers of languages other than English and
Spanish.
Race and ethnicity appear in 28 of the 37 con-
junctions. The attribute indicating the proportion of
African Americans appeared in 22 of these features,
strongly suggesting that African American Vernac-
ular English (Rickford, 1999) plays an important
role in social media text. Many of these features
conjoined the proportion of African Americans with
geographical features, identifying local linguistic
styles used predominantly in either African Amer-
ican or white communities. Among features which
focus on minority communities, F17 emphasizes the
New York area, F33 focuses on the San Francisco
Bay area, and F12 selects a broad area in the Mid-
west and South. Conversely, F23 selects areas with
very few African Americans and Spanish-speakers
in the western part of the United States, and F36 se-
lects for similar demographics in the area of Wash-
ington and Philadelphia.
Other features conjoined the proportion of
African Americans with the proportion of Hispan-
ics and/or Spanish speakers. In some cases, features
selected for high proportions of both African Amer-
icans and Hispanics; for example, F7 seems to iden-
tify a general ?urban minority? group, emphasizing
renters, African Americans, and Spanish speakers.
Other features differentiate between African Ameri-
1372
cans and Hispanics: F8 identifies regions with many
Spanish speakers and Hispanics, but few African
Americans; F20 identifies regions with both Span-
ish speakers and whites, but few African Americans.
F8 and F20 tend to emphasize more Spanish words
than features which select for both African Ameri-
cans and Hispanics.
While race, geography, and language predom-
inate, the socioeconomic attributes appear in far
fewer features. The most prevalent attribute is the
proportion of renters, which appears in F4 and F7,
and in three other features not shown here. This at-
tribute may be a better indicator of the urban/rural
divide than the ?% urban? attribute, which has a
very low threshold for what counts as urban (see
Table 1). It may also be a better proxy for wealth
than median income, which appears in only one of
the thirty-seven selected features. Overall, the se-
lected features tend to include attributes that are easy
to predict from text (compare with Table 2).
6 Related Work
Sociolinguistics has a long tradition of quantitative
and computational research. Logistic regression has
been used to identify relationships between demo-
graphic features and linguistic variables since the
1970s (Cedergren and Sankoff, 1974). More re-
cent developments include the use of mixed factor
models to account for idiosyncrasies of individual
speakers (Johnson, 2009), as well as clustering and
multidimensional scaling (Nerbonne, 2009) to en-
able aggregate inference across multiple linguistic
variables. However, all of these approaches assume
that both the linguistic indicators and demographic
attributes have already been identified by the re-
searcher. In contrast, our approach focuses on iden-
tifying these indicators automatically from data. We
view our approach as an exploratory complement to
more traditional analysis.
There is relatively little computational work on
identifying speaker demographics. Chang et al
(2010) use U.S. Census statistics about the ethnic
distribution of last names as an anchor in a latent-
variable model that infers the ethnicity of Facebook
users; however, their paper analyzes social behav-
ior rather than language use. In unpublished work,
David Bamman uses geotagged Twitter text and U.S.
Census statistics to estimate the age, gender, and
racial distributions of various lexical items.6 Eisen-
stein et al (2010) infer geographic clusters that are
coherent with respect to both location and lexical
distributions; follow-up work by O?Connor et al
(2010) applies a similar generative model to demo-
graphic data. The model presented here differs in
two key ways: first, we use sparsity-inducing regu-
larization to perform variable selection; second, we
eschew high-dimensional mixture models in favor of
a bottom-up approach of building conjunctions of
demographic and geographic attributes. In a mix-
ture model, each component must define a distribu-
tion over all demographic variables, which may be
difficult to estimate in a high-dimensional setting.
Early examples of the use of sparsity in natu-
ral language processing include maximum entropy
classification (Kazama and Tsujii, 2003), language
modeling (Goodman, 2004), and incremental pars-
ing (Riezler and Vasserman, 2004). These papers all
apply the standard lasso, obtaining sparsity for a sin-
gle output dimension. Structured sparsity has rarely
been applied to language tasks, but Duh et al (2010)
reformulated the problem of reranking N -best lists
as multi-task learning with structured sparsity.
7 Conclusion
This paper demonstrates how regression with struc-
tured sparsity can be applied to select words and
conjunctive demographic features that reveal soci-
olinguistic associations. The resulting models are
compact and interpretable, with little cost in accu-
racy. In the future we hope to consider richer lin-
guistic models capable of identifying multi-word ex-
pressions and syntactic variation.
Acknowledgments We received helpful feedback
from Moira Burke, Scott Kiesling, Seyoung Kim, Andre?
Martins, Kriti Puniyani, and the anonymous reviewers.
Brendan O?Connor provided the data for this research,
and Seunghak Lee shared a Matlab implementation of
the multi-output lasso, which was the basis for our C
implementation. This research was enabled by AFOSR
FA9550010247, ONR N0001140910758, NSF CAREER
DBI-0546594, NSF CAREER IIS-1054319, NSF IIS-
0713379, an Alfred P. Sloan Fellowship, and Google?s
support of the Worldly Knowledge project at CMU.
6http://www.lexicalist.com
1373
References
Henrietta J. Cedergren and David Sankoff. 1974. Vari-
able rules: Performance as a statistical reflection of
competence. Language, 50(2):333?355.
Jonathan Chang, Itamar Rosenn, Lars Backstrom, and
Cameron Marlow. 2010. ePluribus: Ethnicity on so-
cial networks. In Proceedings of ICWSM.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto the
`1-ball for learning in high dimensions. In Proceed-
ings of ICML.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. n-best rerank-
ing by multitask learning. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
Metrics.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model of ge-
ographic lexical variation. In Proceedings of EMNLP.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization paths for generalized linear
models via coordinate descent. Journal of Statistical
Software, 33(1):1?22.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proceedings of NAACL-HLT.
Daniel E. Johnson. 2009. Getting off the GoldVarb
standard: Introducing Rbrul for mixed-effects variable
rule analysis. Language and Linguistics Compass,
3(1):359?383.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP.
William Labov. 1966. The Social Stratification of En-
glish in New York City. Center for Applied Linguis-
tics.
Han Liu, Mark Palatucci, and Jian Zhang. 2009. Block-
wise coordinate descent procedures for the multi-task
lasso, with applications to neural semantic basis dis-
covery. In Proceedings of ICML.
John Nerbonne. 2009. Data-driven dialectology. Lan-
guage and Linguistics Compass, 3(1):175?198.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science.
Ariadna Quattoni, Xavier Carreras, Michael Collins, and
Trevor Darrell. 2009. An efficient projection for `1,?
regularization. In Proceedings of ICML.
John R. Rickford. 1999. African American Vernacular
English. Blackwell.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and `1 regularization for re-
laxed maximum-entropy modeling. In Proceedings of
EMNLP.
Gerard Rushton, Marc P. Armstrong, Josephine Gittler,
Barry R. Greene, Claire E. Pavlik, Michele M. West,
and Dale L. Zimmerman, editors. 2008. Geocoding
Health Data: The Use of Geographic Codes in Cancer
Prevention and Control, Research, and Practice. CRC
Press.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Aaron Smith and Lee Rainie. 2010. Who tweets? Tech-
nical report, Pew Research Center, December.
Berwin A. Turlach, William N. Venables, and Stephen J.
Wright. 2005. Simultaneous variable selection. Tech-
nometrics, 47(3):349?363.
Larry Wasserman and Kathryn Roeder. 2009. High-
dimensional variable selection. Annals of Statistics,
37(5A):2178?2201.
Larry Wasserman. 2003. All of Statistics: A Concise
Course in Statistical Inference. Springer.
Jing Wu, Bernie Devlin, Steven Ringquist, Massimo
Trucco, and Kathryn Roeder. 2010. Screen and clean:
A tool for identifying interactions in genome-wide as-
sociation studies. Genetic Epidemiology, 34(3):275?
285.
Qing Zhang. 2005. A Chinese yuppie in Beijing: Phono-
logical variation and the construction of a new profes-
sional identity. Language in Society, 34:431?466.
Kathryn Zickuhr and Aaron Smith. 2010. 4% of online
Americans use location-based services. Technical re-
port, Pew Research Center, November.
1374
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1435?1444,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised Frame-Semantic Parsing for Unknown Predicates
Dipanjan Das and Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dipanjan,nasmith}@cs.cmu.edu
Abstract
We describe a new approach to disambiguat-
ing semantic frames evoked by lexical predi-
cates previously unseen in a lexicon or anno-
tated data. Our approach makes use of large
amounts of unlabeled data in a graph-based
semi-supervised learning framework. We con-
struct a large graph where vertices correspond
to potential predicates and use label propa-
gation to learn possible semantic frames for
new ones. The label-propagated graph is used
within a frame-semantic parser and, for un-
known predicates, results in over 15% abso-
lute improvement in frame identification ac-
curacy and over 13% absolute improvement
in full frame-semantic parsing F1 score on a
blind test set, over a state-of-the-art supervised
baseline.
1 Introduction
Frame-semantic parsing aims to extract a shallow se-
mantic structure from text, as shown in Figure 1.
The FrameNet lexicon (Fillmore et al, 2003) is
a rich linguistic resource containing expert knowl-
edge about lexical and predicate-argument seman-
tics. The lexicon suggests an analysis based on the
theory of frame semantics (Fillmore, 1982). Recent
approaches to frame-semantic parsing have broadly
focused on the use of two statistical classifiers cor-
responding to the aforementioned subtasks: the first
one to identify the most suitable semantic frame for
a marked lexical predicate (target, henceforth) in a
sentence, and the second for performing semantic
role labeling (SRL) given the frame.
The FrameNet lexicon, its exemplar sentences
containing instantiations of semantic frames, and
full-text annotations provide supervision for learn-
ing frame-semantic parsers. Yet these annotations
lack coverage, including only 9,300 annotated tar-
get types. Recent papers have tried to address the
coverage problem. Johansson and Nugues (2007)
used WordNet (Fellbaum, 1998) to expand the list of
targets that can evoke frames and trained classifiers
to identify the best-suited frame for the newly cre-
ated targets. In past work, we described an approach
where latent variables were used in a probabilistic
model to predict frames for unseen targets (Das et
al., 2010a).1 Relatedly, for the argument identifica-
tion subtask, Matsubayashi et al (2009) proposed
a technique for generalization of semantic roles to
overcome data sparseness. Unseen targets continue
to present a major obstacle to domain-general se-
mantic analysis.
In this paper, we address the problem of idenfi-
fying the semantic frames for targets unseen either
in FrameNet (including the exemplar sentences) or
the collection of full-text annotations released along
with the lexicon. Using a standard model for the ar-
gument identification stage (Das et al, 2010a), our
proposed method improves overall frame-semantic
parsing, especially for unseen targets. To better han-
dle these unseen targets, we adopt a graph-based
semi-supervised learning stategy (?4). We construct
a large graph over potential targets, most of which
1Notwithstanding state-of-the-art results, that approach was
only able to identify the correct frame for 1.9% of unseen tar-
gets in the test data available at that time. That system achieves
about 23% on the test set used in this paper.
1435
bell.n
ring.v
there be.v
enough.a
LU
NOISE_MAKERS
SUFFICIENCY
Frame
EXISTENCE
CAUSE_TO_MAKE_NOISE
.bells
 
 
N_m
more than six of the eight
Sound_maker
Enabled_situation
ringtoringers
Item
enough
Entity
Agent
n'tarestillthereBut
Figure 1: An example sentence from the PropBank section of the full-text annotations released as part of FrameNet
1.5. Each row under the sentence correponds to a semantic frame and its set of corresponding arguments. Thick lines
indicate targets that evoke frames; thin solid/dotted lines with labels indicate arguments. N m under ?bells? is short
for the Noise maker role of the NOISE MAKERS frame.
are drawn from unannotated data, and a fraction
of which come from seen FrameNet annotations.
Next, we perform label propagation on the graph,
which is initialized by frame distributions over the
seen targets. The resulting smoothed graph con-
sists of posterior distributions over semantic frames
for each target in the graph, thus increasing cover-
age. These distributions are then evaluated within
a frame-semantic parser (?5). Considering unseen
targets in test data (although few because the test
data is also drawn from the training domain), sig-
nificant absolute improvements of 15.7% and 13.7%
are observed for frame identification and full frame-
semantic parsing, respectively, indicating improved
coverage for hitherto unobserved predicates (?6).
2 Background
Before going into the details of our model, we pro-
vide some background on two topics relevant to
this paper: frame-semantic parsing and graph-based
learning applied to natural language tasks.
2.1 Frame-semantic Parsing
Gildea and Jurafsky (2002) pioneered SRL, and
since then there has been much applied research
on predicate-argument semantics. Early work on
frame-semantic role labeling made use of the ex-
emplar sentences in the FrameNet corpus, each of
which is annotated for a single frame and its argu-
ments (Thompson et al, 2003; Fleischman et al,
2003; Shi and Mihalcea, 2004; Erk and Pado?, 2006,
inter alia). Most of this work was done on an older,
smaller version of FrameNet. Recently, since the re-
lease of full-text annotations in SemEval?07 (Baker
et al, 2007), there has been work on identifying
multiple frames and their corresponding sets of ar-
guments in a sentence. The LTH system of Jo-
hansson and Nugues (2007) performed the best in
the SemEval?07 shared task on frame-semantic pars-
ing. Our probabilistic frame-semantic parser out-
performs LTH on that task and dataset (Das et al,
2010a). The current paper builds on those proba-
bilistic models to improve coverage on unseen pred-
icates.2
Expert resources have limited coverage, and
FrameNet is no exception. Automatic induction of
semantic resources has been a major effort in re-
cent years (Snow et al, 2006; Ponzetto and Strube,
2007, inter alia). In the domain of frame semantics,
previous work has sought to extend the coverage
of FrameNet by exploiting resources like VerbNet,
WordNet, or Wikipedia (Shi and Mihalcea, 2005;
Giuglea and Moschitti, 2006; Pennacchiotti et al,
2008; Tonelli and Giuliano, 2009), and projecting
entries and annotations within and across languages
(Boas, 2002; Fung and Chen, 2004; Pado? and La-
pata, 2005). Although these approaches have in-
creased coverage to various degrees, they rely on
other lexicons and resources created by experts.
Fu?rstenau and Lapata (2009) proposed the use of un-
labeled data to improve coverage, but their work was
limited to verbs. Bejan (2009) used self-training to
improve frame identification and reported improve-
ments, but did not explicitly model unknown tar-
gets. In contrast, we use statistics gathered from
large volumes of unlabeled data to improve the cov-
erage of a frame-semantic parser on several syntactic
categories, in a novel framework that makes use of
graph-based semi-supervised learning.
2SEMAFOR, the system presented by Das et al (2010a) is
publicly available at http://www.ark.cs.cmu.edu/
SEMAFOR and has been extended in this work.
1436
2.2 Graph-based Semi-Supervised Learning
In graph-based semi-supervised learning, one con-
structs a graph whose vertices are labeled and unla-
beled examples. Weighted edges in the graph, con-
necting pairs of examples/vertices, encode the de-
gree to which they are expected to have the same
label (Zhu et al, 2003). Variants of label propaga-
tion are used to transfer labels from the labeled to the
unlabeled examples. There are several instances of
the use of graph-based methods for natural language
tasks. Most relevant to our work an approach to
word-sense disambiguation due to Niu et al (2005).
Their formulation was transductive, so that the test
data was part of the constructed graph, and they did
not consider predicate-argument analysis. In con-
trast, we make use of the smoothed graph during in-
ference in a probabilistic setting, in turn using it for
the full frame-semantic parsing task. Recently, Sub-
ramanya et al (2010) proposed the use of a graph
over substructures of an underlying sequence model,
and used a smoothed graph for domain adaptation of
part-of-speech taggers. Subramanya et al?s model
was extended by Das and Petrov (2011) to induce
part-of-speech dictionaries for unsupervised learn-
ing of taggers. Our semi-supervised learning setting
is similar to these two lines of work and, like them,
we use the graph to arrive at better final structures, in
an inductive setting (i.e., where a parametric model
is learned and then separately applied to test data,
following most NLP research).
3 Approach Overview
Our overall approach to handling unobserved targets
consists of four distinct stages. Before going into the
details of each stage individually, we provide their
overview here:
Graph Construction: A graph consisting of ver-
tices corresponding to targets is constructed us-
ing a combination of frame similarity (for ob-
served targets) and distributional similarity as
edge weights. This stage also determines a
fixed set of nearest neighbors for each vertex
in the graph.
Label Propagation: The observed targets (a small
subset of the vertices) are initialized with
empirical frame distributions extracted from
FrameNet annotations. Label propagation re-
sults in a distribution of frames for each vertex
in the graph.
Supervised Learning: Frame identification and ar-
gument identification models are trained fol-
lowing Das et al (2010a). The graph is used
to define the set of candidate frames for unseen
targets.
Parsing: The frame identification model of
Das et al disambiguated among only those
frames associated with a seen target in the
annotated data. For an unseen target, all frames
in the FrameNet lexicon were considered (a
large number). The current work replaces that
strategy, considering only the top M frames in
the distribution produced by label propagation.
This strategy results in large improvements
in frame identification for the unseen targets
and makes inference much faster. Argument
identification is done exactly like Das et al
(2010a).
4 Semi-Supervised Learning
We perform semi-supervised learning by construct-
ing a graph of vertices representing a large number
of targets, and learn frame distributions for those
which were not observed in FrameNet annotations.
4.1 Graph Construction
We construct a graph with targets as vertices. For
us, each target corresponds to a lemmatized word
or phrase appended with a coarse POS tag, and it
resembles the lexical units in the FrameNet lexicon.
For example, two targets corresponding to the same
lemma would look like boast.N and boast.V. Here,
the first target is a noun, while the second is a verb.
An example multiword target is chemical weapon.N.
We use two resources for graph construction.
First, we take all the words and phrases present in
the dependency-based thesaurus constructed using
syntactic cooccurrence statistics (Lin, 1998).3 To
construct this resource, a corpus containing 64 mil-
lion words was parsed with a fast dependency parser
(Lin, 1993; Lin, 1994), and syntactic contexts were
used to find similar lexical items for a given word
3This resource is available at http://webdocs.cs.
ualberta.ca/?lindek/Downloads/sim.tgz
1437
difference.N
similarity.N
discrepancy.N
resemble.V
disparity.N
resemblance.Ninequality.N
variant.N
divergence.N
poverty.N
homelessness.N
wealthy.Arich.A
deprivation.N
destitution.N
joblessness.N
unemployment.N employment.N
unemployment rate.N
powerlessness.N
UNEMPLOYMENT_RATE UNEMPLOYMENT_RATE
UNEMPLOYMENT_RATE
POVERTY POVERTY
POVERTY
SIMILARITY SIMILARITY
SIMILARITY
SIMILARITYSIMILARITY
Figure 2: Excerpt from a graph
over targets. Green targets are
observed in the FrameNet data.
Above/below them are shown the
most frequently observed frame
that these targets evoke. The black
targets are unobserved and label
propagation produces a distribution
over most likely frames that they
could evoke.
or phrase. Lin separately treated nouns, verbs and
adjectives/adverbs and the thesaurus contains three
parts for each of these categories. For each item in
the thesaurus, 200 nearest neighbors are listed with a
symmetric similarity score between 0 and 1. We pro-
cessed this thesaurus in two ways: first, we lower-
cased and lemmatized each word/phrase and merged
entries which shared the same lemma; second, we
separated the adjectives and adverbs into two lists
from Lin?s original list by scanning a POS-tagged
version of the Gigaword corpus (Graff, 2003) and
categorizing each item into an adjective or an ad-
verb depending on which category the item associ-
ated with more often in the data. The second step
was necessary because FrameNet treats adjectives
and adverbs separately. At the end of this processing
step, we were left with 61,702 units?approximately
six times more than the targets found in FrameNet
annotations?each labeled with one of 4 coarse tags.
We considered only the top 20 most similar targets
for each target, and noted Lin?s similarity between
two targets t and u, which we call simDL(t, u).
The second component of graph construction
comes from FrameNet itself. We scanned the exem-
plar sentences in FrameNet 1.54 and the training sec-
tion of the full-text annotations that we use to train
the probabilistic frame parser (see ?6.1), and gath-
ered a distribution over frames for each target. For
a pair of targets t and u, we measured the Euclidean
distance5 between their frame distributions. This
distance was next converted to a similarity score,
namely, simFN (t, u) between 0 and 1 by subtract-
ing each one from the maximum distance found in
4http://framenet.icsi.berkeley.edu
5This could have been replaced by an entropic distance metric
like KL- or JS-divergence, but we leave that exploration to fu-
ture work.
the whole data, followed by normalization. Like
simDL(t, u), this score is symmetric. This resulted
in 9,263 targets, and again for each, we considered
the 20 most similar targets. Finally, the overall sim-
ilarity between two given targets t and u was com-
puted as:
sim(t, u) = ? ? simFN (t, u) + (1??) ? simDL(t, u)
Note that this score is symmetric because its two
components are symmetric. The intuition behind
taking a linear combination of the two types of sim-
ilarity functions is as follows. We hope that distri-
butionally similar targets would have the same se-
mantic frames because ideally, lexical units evoking
the same set of frames appear in similar syntactic
contexts. We would also like to involve the anno-
tated data in graph construction so that it can elim-
inate some noise in the automatically constructed
thesaurus.6 Let K(t) denote the K most similar tar-
gets to target t, under the score sim. We link vertices
t and u in the graph with edge weight wtu, defined
as:
wtu =
{
sim(t, u) if t ? K(u) or u ? K(t)
0 otherwise
(1)
The hyperparameters ? and K are tuned by cross-
validation (?6.3).
4.2 Label Propagation
First, we softly label those vertices of the con-
structed graph for which frame distributions are
available from the FrameNet data (the same distri-
butions that are used to compute simFN ). Thus, ini-
tially, a small fraction of the vertices in the graph
6In future work, one might consider learning a similarity metric
from the annotated data, so as to exactly suit the frame identi-
fication task.
1438
have soft frame labels on them. Figure 2 shows an
excerpt from a constructed graph. For simplicity,
only the most probable frames under the empirical
distribution for the observed targets are shown; we
actually label each vertex with the full empirical dis-
tribution over frames for the corresponding observed
target in the data. The dotted lines demarcate parts
of the graph that associate with different frames. La-
bel propagation helps propagate the initial soft labels
throughout the graph. To this end, we use a vari-
ant of the quadratic cost criterion of Bengio et al
(2006), also used by Subramanya et al (2010) and
Das and Petrov (2011).7
Let V denote the set of all vertices in the graph,
Vl ? V be the set of known targets and F denote the
set of all frames. Let N (t) denote the set of neigh-
bors of vertex t ? V . Let q = {q1, q2, . . . , q|V |}
be the set of frame distributions, one per vertex. For
each known target t ? Vl, we have an initial frame
distribution rt. For every edge in the graph, weights
are defined as in Eq. 1. We find q by solving:
argminq
?
t?Vl
?rt ? qt?2
+ ?
?
t?V,u?N (t)wtu?qt ? qu?
2
+ ?
?
t?V ?qt ?
1
|F|?
2
s.t. ?t ? V,
?
f?F qt(f) = 1
?t ? V, f ? F , qt(f) ? 0
(2)
We use a squared loss to penalize various pairs of
distributions over frames: ?a?b?2 =
?
f?F (a(f)?
b(f))2. The first term in Eq. 2 requires that, for
known targets, we stay close to the initial frame dis-
tributions. The second term is the graph smooth-
ness regularizer, which encourages the distributions
of similar nodes (large wtu) to be similar. The fi-
nal term is a regularizer encouraging all distributions
to be uniform to the extent allowed by the first two
terms. (If an unlabeled vertex does not have a path
to any labeled vertex, this term ensures that its con-
verged marginal will be uniform over all frames.) ?
and ? are hyperparameters whose choice we discuss
in ?6.3.
Note that Eq. 2 is convex in q. While it is possible
to derive a closed form solution for this objective
7Instead of a quadratic cost, an entropic distance measure could
have been used, e.g., KL-divergence, considered by Subra-
manya and Bilmes (2009). We do not explore that direction
in the current paper.
function, it would require the inversion of a |V |?|V |
matrix. Hence, like Subramanya et al (2010), we
employ an iterative method with updates defined as:
?t(f) ? rt(f)1{t ? Vl} (3)
+ ?
?
u?N (t)
wtuq
(m?1)
u (f) +
?
|F|
?t ? 1{t ? Vl}+ ? + ?
?
u?N (t)
wtu (4)
q(m)t (f) ? ?t(f)/?t (5)
Here, 1{?} is an indicator function. The iterative
procedure starts with a uniform distribution for each
q(0)t . For all our experiments, we run 10 iterations
of the updates. The final distribution of frames for a
target t is denoted by q?t .
5 Learning and Inference for
Frame-Semantic Parsing
In this section, we briefly review learning and infer-
ence techniques used in the frame-semantic parser,
which are largely similar to Das et al (2010a), ex-
cept the handling of unknown targets. Note that in
all our experiments, we assume that the targets are
marked in a given sentence of which we want to ex-
tract a frame-semantic analysis. Therefore, unlike
the systems presented in SemEval?07, we do not de-
fine a target identification module.
5.1 Frame Identification
For a given sentence x with frame-evoking targets
t, let ti denote the ith target (a word sequence). We
seek a list f = ?f1, . . . , fm? of frames, one per tar-
get. LetL be the set of targets found in the FrameNet
annotations. Let Lf ? L be the subset of these tar-
gets annotated as evoking a particular frame f .
The set of candidate frames Fi for ti is defined to
include every frame f such that ti ? Lf . If ti 6? L
(in other words, ti is unseen), then Das et al (2010a)
considered all frames F in FrameNet as candidates.
Instead, in our work, we check whether ti ? V ,
where V are the vertices of the constructed graph,
and set:
Fi = {f : f ?M -best frames under q
?
ti} (6)
The integer M is set using cross-validation (?6.3).
If ti 6? V , then all frames F are considered as Fi.
1439
The frame prediction rule uses a probabilistic model
over frames for a target:
fi ? argmaxf?Fi
?
`?Lf
p(f, ` | ti,x) (7)
Note that a latent variable ` ? Lf is used, which
is marginalized out. Broadly, lexical semantic re-
lationships between the ?prototype? variable ` (be-
longing to the set of seen targets for a frame f ) and
the target ti are used as features for frame identifi-
cation, but since ` is unobserved, it is summed out
both during inference and training. A conditional
log-linear model is used to model this probability:
for f ? Fi and ` ? Lf , p?(f, ` | ti,x) =
exp?>g(f, `, ti,x)
?
f ??Fi
?
`??Lf ?
exp?>g(f ?, `?, ti,x)
(8)
where ? are the model weights, and g is a vector-
valued feature function. This discriminative formu-
lation is very flexible, allowing for a variety of (pos-
sibly overlapping) features; e.g., a feature might re-
late a frame f to a prototype `, represent a lexical-
semantic relationship between ` and ti, or encode
part of the syntax of the sentence (Das et al, 2010b).
Given some training data, which is of the form
?
?x(j), t(j), f (j),A(j)?
?N
j=1 (where N is the number
of sentences in the data and A is the set of argu-
ment in a sentence), we discriminatively train the
frame identification model by maximizing the fol-
lowing log-likelihood:8
max
?
N?
j=1
mj?
i=1
log
?
`?L
f
(j)
i
p?(f
(j)
i , ` | t
(j)
i ,x
(j)) (9)
This non-convex objective function is locally op-
timized using a distributed implementation of L-
BFGS (Liu and Nocedal, 1989).9
5.2 Argument Identification
Given a sentence x = ?x1, . . . , xn?, the set of tar-
gets t = ?t1, . . . , tm?, and a list of evoked frames
8We found no benefit from using an L2 regularizer.
9While training, in the partition function of the log-linear
model, all frames F in FrameNet are summed up for a target ti
instead of only Fi (as in Eq. 8), to learn interactions between
the latent variables and different sentential contexts.
f = ?f1, . . . , fm? corresponding to each target, ar-
gument identification or SRL is the task of choos-
ing which of each fi?s roles are filled, and by which
parts of x. We directly adopt the model of Das et
al. (2010a) for the argument identification stage and
briefly describe it here.
Let Rfi = {r1, . . . , r|Rfi |} denote frame fi?s
roles observed in FrameNet annotations. A set S of
spans that are candidates for filling any role r ? Rfi
are identified in the sentence. In principle, S could
contain any subsequence of x, but we consider only
the set of contiguous spans that (a) contain a sin-
gle word or (b) comprise a valid subtree of a word
and all its descendants in a dependency parse. The
empty span is also included in S, since some roles
are not explicitly filled. During training, if an argu-
ment is not a valid subtree of the dependency parse
(this happens due to parse errors), we add its span
to S. Let Ai denote the mapping of roles in Rfi to
spans in S. The model makes a prediction for each
Ai(rk) (for all roles rk ? Rfi):
Ai(rk)? argmaxs?S p(s | rk, fi, ti,x) (10)
A conditional log-linear model over spans for each
role of each evoked frame is defined as:
p?(Ai(rk) = s | fi, ti,x) = (11)
exp?>h(s, rk, fi, ti,x)
?
s??S exp?
>h(s?, rk, fi, ti,x)
This model is trained by optimizing:
max
?
N?
j=1
mj?
i=1
|R
f
(j)
i
|
?
k=1
log p?(A
(j)
i (rk) | f
(j)
i , t
(j)
i ,x
(j))
This objective function is convex, and we globally
optimize it using the distributed implementation of
L-BFGS. We regularize by including ? 110???
2
2 in
the objective (the strength is not tuned). Na??ve pre-
diction of roles using Equation 10 may result in
overlap among arguments filling different roles of a
frame, since the argument identification model fills
each role independently of the others. We want
to enforce the constraint that two roles of a sin-
gle frame cannot be filled by overlapping spans.
Hence, illegal overlap is disallowed using a 10,000-
hypothesis beam search.
1440
UNKNOWN TARGETS ALL TARGETS
Model
Exact
Match
Partial
Match
Exact
Match
Partial
Match
SEMAFOR 23.08 46.62 82.97 90.51
Self-training 18.88 42.67 82.45 90.19
LinGraph 36.36 59.47 83.40 90.93
FullGraph 39.86 62.35? 83.51 91.02?
Table 1: Frame identification results in percentage accu-
racy on 4,458 test targets. Bold scores indicate significant
improvements relative to SEMAFOR and (?) denotes sig-
nificant improvements over LinGraph (p < 0.05).
6 Experiments and Results
Before presenting our experiments and results, we
will describe the datasets used in our experiments,
and the various baseline models considered.
6.1 Data
We make use of the FrameNet 1.5 lexicon released
in 2010. This lexicon is a superset of previous ver-
sions of FrameNet. It contains 154,607 exemplar
sentences with one marked target and frame-role an-
notations. 78 documents with full-text annotations
with multiple frames per sentence were also released
(a superset of the SemEval?07 dataset). We ran-
domly selected 55 of these documents for training
and treated the 23 remaining ones as our test set.
After scanning the exemplar sentences and the train-
ing data, we arrived at a set of 877 frames, 1,068
roles,10 and 9,263 targets. Our training split of
the full-text annotations contained 3,256 sentences
with 19,582 frame annotatations with correspond-
ing roles, while the test set contained 2,420 sen-
tences with 4,458 annotations (the test set contained
fewer annotated targets per sentence). We also di-
vide the 55 training documents into 5 parts for cross-
validation (see ?6.3). The raw sentences in all the
training and test documents were preprocessed us-
ing MXPOST (Ratnaparkhi, 1996) and the MST de-
pendency parser (McDonald et al, 2005) following
Das et al (2010a). In this work we assume the
frame-evoking targets have been correctly identified
in training and test data.
10Note that the number of listed roles in the lexicon is nearly
9,000, but their number in actual annotations is a lot fewer.
6.2 Baselines
We compare our model with three baselines. The
first baseline is the purely supervised model of Das
et al (2010a) trained on the training split of 55
documents. Note that this is the strongest baseline
available for this task;11 we refer to this model as
?SEMAFOR.?
The second baseline is a semi-supervised self-
trained system, where we used SEMAFOR to label
70,000 sentences from the Gigaword corpus with
frame-semantic parses. For finding targets in a raw
sentence, we used a relaxed target identification
scheme, where we marked every target seen in the
lexicon and all other words which were not prepo-
sitions, particles, proper nouns, foreign words and
Wh-words as potential frame evoking units. This
was done so as to find unseen targets and get frame
annotations with SEMAFOR on them. We appended
these automatic annotations to the training data, re-
sulting in 711,401 frame annotations, more than 36
times the supervised data. These data were next used
to train a frame identification model (?5.1).12 This
setup is very similar to Bejan (2009) who used self-
training to improve frame identification. We refer to
this model as ?Self-training.?
The third baseline uses a graph constructed only
with Lin?s thesaurus, without using supervised data.
In other words, we followed the same scheme as in
?4.1 but with the hyperparameter ? = 0. Next, la-
bel propagation was run on this graph (and hyper-
parameters tuned using cross validation). The poste-
rior distribution of frames over targets was next used
for frame identification (Eq. 6-7), with SEMAFOR
as the trained model. This model, which is very sim-
ilar to our full model, is referred to as ?LinGraph.?
?FullGraph? refers to our full system.
6.3 Experimental Setup
We used five-fold cross-validation to tune the hy-
perparameters ?, K, ?, and M in our model. The
11We do not compare our model with other systems, e.g. the
ones submitted to SemEval?07 shared task, because SE-
MAFOR outperforms them significantly (Das et al, 2010a)
on the previous version of the data. Moreover, we trained our
models on the new FrameNet 1.5 data, and training code for
the SemEval?07 systems was not readily available.
12Note that we only self-train the frame identification model and
not the argument identification model, which is fixed through-
out.
1441
UNKNOWN TARGETS ALL TARGETS
Model
Exact Match Partial Match Exact Match Partial Match
P R F1 P R F1 P R F1 P R F1
SEMAFOR 19.59 16.48 17.90 33.03 27.80 30.19 66.15 61.64 63.82 70.68 65.86 68.18
Self-training 15.44 13.00 14.11 29.08 24.47 26.58 65.78 61.30 63.46 70.39 65.59 67.90
LinGraph 29.74 24.88 27.09 44.08 36.88 40.16 66.43 61.89 64.08 70.97 66.13 68.46
FullGraph 35.27? 28.84? 31.74? 48.81? 39.91? 43.92? 66.59? 62.01? 64.22? 71.11? 66.22? 68.58?
Table 2: Full frame-semantic parsing precision, recall and F1 score on 2,420 test sentences. Bold scores indicate
significant improvements relative to SEMAFOR and (?) denotes significant improvements over LinGraph (p < 0.05).
uniform regularization hyperparameter ? for graph
construction was set to 10?6 and not tuned. For
each cross-validation split, four folds were used to
train a frame identification model, construct a graph,
run label propagation and then the model was tested
on the fifth fold. This was done for all hyperpa-
rameter settings, which were ? ? {0.2, 0.5, 0.8},
K ? {5, 10, 15, 20}, ? ? {0.01, 0.1, 0.3, 0.5, 1.0}
and M ? {2, 3, 5, 10}. The joint setting which per-
formed the best across five-folds was ? = 0.2,K =
10, ? = 1.0,M = 2. Similar tuning was also done
for the baseline LinGraph, where ? was set to 0,
and rest of the hyperparameters were tuned (the se-
lected hyperparameters were K = 10, ? = 0.1 and
M = 2). With the chosen set of hyperparameters,
the test set was used to measure final performance.
The standard evaluation script from the Se-
mEval?07 task calculates precision, recall, and F1-
score for frames and arguments; it also provides a
score that gives partial credit for hypothesizing a
frame related to the correct one in the FrameNet lex-
icon. We present precision, recall, and F1-measure
microaveraged across the test documents, report
labels-only matching scores (spans must match ex-
actly), and do not use named entity labels. This eval-
uation scheme follows Das et al (2010a). Statistical
significance is measured using a reimplementation
of Dan Bikel?s parsing evaluation comparator.13
6.4 Results
Tables 1 and 2 present results for frame identifica-
tion and full frame-semantic parsing respectively.
They also separately tabulate the results achieved
for unknown targets. Our full model, denoted by
?FullGraph,? outperforms all the baselines for both
tasks. Note that the Self-training model even falls
13http://www.cis.upenn.edu/?dbikel/
software.html#comparator
short of the supervised baseline SEMAFOR, unlike
what was observed by Bejan (2009) for the frame
identification task. The model using a graph con-
structed solely from the thesaurus (LinGraph) out-
performs both the supervised and the self-training
baselines for all tasks, but falls short of the graph
constructed using the similarity metric that is a lin-
ear combination of distributional similarity and su-
pervised frame similarity. This indicates that a graph
constructed with some knowledge of the supervised
data is more powerful.
For unknown targets, the gains of our approach
are impressive: 15.7% absolute accuracy improve-
ment over SEMAFOR for frame identification, and
13.7% absolute F1 improvement over SEMAFOR
for full frame-semantic parsing (both significant).
When all the test targets are considered, the gains
are still significant, resulting in 5.4% relative error
reduction over SEMAFOR for frame identification,
and 1.3% relative error reduction over SEMAFOR
for full-frame semantic parsing.
Although these improvements may seem modest,
this is because only 3.2% of the test set targets are
unseen in training. We expect that further gains
would be realized in different text domains, where
FrameNet coverage is presumably weaker than in
news data. A semi-supervised strategy like ours is
attractive in such a setting, and future work might
explore such an application.
Our approach also makes decoding much faster.
For the unknown component of the test set, SE-
MAFOR takes a total 111 seconds to find the best
set of frames, while the FullGraph model takes only
19 seconds to do so, thus bringing disambiguation
time down by a factor of nearly 6. This is be-
cause our model now disambiguates between only
M = 2 frames instead of the full set of 877 frames
in FrameNet. For the full test set too, the speedup
1442
t = discrepancy.N t = contribution.N t = print.V t = mislead.V
f q?t (f) f q
?
t (f) f q
?
t (f) f q
?
t (f)
?SIMILARITY 0.076 ?GIVING 0.167 ?TEXT CREATION 0.081 EXPERIENCER OBJ 0.152
NATURAL FEATURES 0.066 MONEY 0.046 SENDING 0.054 ?PREVARICATION 0.130
PREVARICATION 0.012 COMMITMENT 0.046 DISPERSAL 0.054 MANIPULATE INTO DOING 0.046
QUARRELING 0.007 ASSISTANCE 0.040 READING 0.042 COMPLIANCE 0.041
DUPLICATION 0.007 EARNINGS AND LOSSES 0.024 STATEMENT 0.028 EVIDENCE 0.038
Table 3: Top 5 frames according to the graph posterior distribution q?t (f) for four targets: discrepancy.N, contri-
bution.N, print.V and mislead.V. None of these targets were present in the supervised FrameNet data. ? marks the
correct frame, according to the test data. EXPERIENCER OBJ is described in FrameNet as ?Some phenomenon (the
Stimulus) provokes a particular emotion in an Experiencer.?
is noticeable, as SEMAFOR takes 131 seconds for
frame identification, while the FullGraph model only
takes 39 seconds.
6.5 Discussion
The following is an example from our test set show-
ing SEMAFOR?s output (for one target):
REASON
Discrepancies
discrepancy.N
between North Korean de-
clarations and IAEA inspection findingsAction
indicate that North Korea might have re-
processed enough plutonium for one or
two nuclear weapons.
Note that the model identifies an incorrect frame
REASON for the target discrepancy.N, in turn identi-
fying the wrong semantic role Action for the under-
lined argument. On the other hand, the FullGraph
model exactly identifies the right semantic frame,
SIMILARITY, as well as the correct role, Entities. This
improvement can be easily explained. The excerpt
from our constructed graph in Figure 2 shows the
same target discrepancy.N in black, conveying that
it did not belong to the supervised data. However,
it is connected to the target difference.N drawn from
annotated data, which evokes the frame SIMILARITY.
Thus, after label propagation, we expect the frame
SIMILARITY to receive high probability for the target
discrepancy.N.
Table 3 shows the top 5 frames that are assigned
the highest posterior probabilities in the distribu-
tion q?t for four hand-selected test targets absent in
supervised data, including discrepancy.N. For all
of them, the FullGraph model identifies the correct
frames for all four words in the test data by rank-
ing these frames in the top M = 2. LinGraph
also gets all four correct, Self-training only gets
print.V/TEXT CREATION, and SEMAFOR gets none.
Across unknown targets, on average the M = 2
most common frames in the posterior distribution
q?t found by FullGraph have q
(?)
t (f) =
7
877 , or
seven times the average across all frames. This sug-
gests that the graph propagation method is confi-
dent only in predicting the top few frames out of
the whole possible set. Moreover, the automatically
selected number of frames to extract per unknown
target, M = 2, suggests that only a few meaningful
frames were assigned to unknown predicates. This
matches the nature of FrameNet data, where the av-
erage frame ambiguity for a target type is 1.20.
7 Conclusion
We have presented a semi-supervised strategy to
improve the coverage of a frame-semantic pars-
ing model. We showed that graph-based label
propagation and resulting smoothed frame distri-
butions over unseen targets significantly improved
the coverage of a state-of-the-art semantic frame
disambiguation model to previously unseen pred-
icates, also improving the quality of full frame-
semantic parses. The improved parser is available at
http://www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We are grateful to Amarnag Subramanya for helpful dis-
cussions. We also thank Slav Petrov, Nathan Schneider,
and the three anonymous reviewers for valuable com-
ments. This research was supported by NSF grants IIS-
0844507, IIS-0915187 and TeraGrid resources provided
by the Pittsburgh Supercomputing Center under NSF
grant number TG-DBS110003.
1443
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: frame semantic structure extraction. In
Proc. of SemEval.
C. A. Bejan. 2009. Learning Event Structures From Text.
Ph.D. thesis, The University of Texas at Dallas.
Y. Bengio, O. Delalleau, and N. Le Roux. 2006. La-
bel propagation and quadratic criterion. In Semi-
Supervised Learning. MIT Press.
H. C. Boas. 2002. Bilingual FrameNet dictionaries for
machine translation. In Proc. of LREC.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proc. of ACL-HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of
NAACL-HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith.
2010b. SEMAFOR 1.0: A probabilistic frame-
semantic parser. Technical Report CMU-LTI-10-001,
Carnegie Mellon University.
K. Erk and S. Pado?. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proc. of LREC.
C. Fellbaum, editor. 1998. WordNet: an electronic lexi-
cal database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111?137. Hanshin Publish-
ing Co., Seoul, South Korea.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP.
P. Fung and B. Chen. 2004. BiFrameNet: bilin-
gual frame semantics resource construction by cross-
lingual induction. In Proc. of COLING.
H. Fu?rstenau and M. Lapata. 2009. Semi-supervised se-
mantic role labeling. In Proc. of EACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
A.-M. Giuglea and A. Moschitti. 2006. Shallow se-
mantic parsing based on FrameNet, VerbNet and Prop-
Bank. In Proc. of ECAI 2006.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium.
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proc. of ACL.
D. Lin. 1994. Principar?an efficient, broadcoverage,
principle-based parser. In Proc. of COLING.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING-ACL.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Programming, 45(3).
Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A
comparative study on generalization of semantic roles
in FrameNet. In Proc. of ACL-IJCNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
Z.-Y. Niu, D.-H. Ji, and C. L. Tan. 2005. Word sense
disambiguation using label propagation based semi-
supervised learning. In Proc. of ACL.
S. Pado? and M. Lapata. 2005. Cross-linguistic projec-
tion of role-semantic information. In Proc. of HLT-
EMNLP.
M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and
M. Roth. 2008. Automatic induction of FrameNet
lexical units. In Proc. of EMNLP.
S. P. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. In Proc. of AAAI.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
L. Shi and R. Mihalcea. 2004. An algorithm for open
text semantic parsing. In Proc. of Workshop on Robust
Methods in Analysis of Natural Language Data.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Computational Linguis-
tics and Intelligent Text Processing: Proc. of CICLing
2005. Springer-Verlag.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
Proc. of COLING-ACL.
A. Subramanya and J. A. Bilmes. 2009. Entropic graph
regularization in non-parametric semi-supervised clas-
sification. In Proc. of NIPS.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Efficient
Graph-based Semi-Supervised Learning of Structured
Tagging Models. In Proc. of EMNLP.
C. A. Thompson, R. Levy, and C. D. Manning. 2003. A
generative model for semantic role labeling. In Proc.
of ECML.
S. Tonelli and C. Giuliano. 2009. Wikipedia as frame
information repository. In Proc. of EMNLP.
X. Zhu, Z. Ghahramani, and J. D. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of ICML.
1444
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
Abstract
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
1 Introduction
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al, 2010; Shar-
ifi et al, 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O?Connor et al, 2010a; Thelwall
et al, 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al, 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al, 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (?tweet?). Figure 1
shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo? willV goV nutsA
whenR PA? electsV aD RepublicanA GovernorN
nextP Tue? ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&
#2$ notR takinV druN booN toP jailN ., ThankV
uO God? ., #amen#
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
? we developed a POS tagset for Twitter,
? we manually tagged 1,827 tweets,
? we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
? we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
42
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not
possessive; PRP, WP)
it you u meeee 6.8
S nominal + possessive books? someone?s 0.1
? proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America?s 0.2
L nominal + verbal he?s book?ll iono
(= I don?t know)
1.6
M proper noun + verbal Mark?ll 0.0
Other open-class words
V verb incl. copula,
auxiliaries (V*, MD)
might gonna
ought couldn?t is
eats
15.1
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea
right
2.6
Other closed-class words
D determiner (WDT, DT,
WP$, PRP$)
the teh its it?s 6.5
P pre- or postposition, or
subordinating conjunction
(IN, TO)
while to for 2 (i.e.,
to) 4 (i.e., for)
8.7
& coordinating conjunction
(CC)
and n & + BUT 1.7
T verb particle (RP) out off Up UP 0.6
X existential there,
predeterminers (EX, PDT)
both 0.1
Y X + verbal there?s all?s 0.0
Twitter/online-specific
# hashtag (indicates
topic/category for tweet)
#acl 1.0
@ at-mention (indicates
another user as a recipient
of a tweet)
@BarackObama 4.9
~ discourse marker,
indications of continuation
of a message across
multiple tweets
RT and : in retweet
construction RT
@user : hello
3.4
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: <3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, '', (,
), ,, ., :, ``)
!!! .... ?!? 11.6
G other abbreviations, foreign
words, possessive endings,
symbols, garbage (FW,
POS, SYM, LS)
ily (I love you) wby
(what about you) ?s
 -->
awesome...I?m
1.1
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag?s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both < 0.0005.)
2 Annotation
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O?Connor et al, 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al, 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen?s ? value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
2.1 Tagset
We set out to develop a POS inventory for Twitter
that would be intuitive and informative?while at
the same time simple to learn and apply?so as to
maximize tagging consistency within and across an-
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
43
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, ~, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (?retweet?) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO ! This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! 
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply ~ to the RT and : (which are standard), and
also, which separates the author?s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (. . . ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These ?iconic deictics? have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I?m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} ? {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (~) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
3 System
Our tagger is a conditional random field (CRF; Laf-
ferty et al, 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWORTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap+?CN+C , where
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
44
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between ?1 and 1 on this scale; these are not shown.
N is the token count, Ncap is the capitalized to-
ken count, and ? and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N ? {1000, 2000, 3000, 5000, 10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Schu?tze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrixM, which
we approximate using a truncated singular value de-
composition: M ? USVT, where U is limited to
50 columns. Each term?s feature vector is its row
in U; following Turian et al (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
7? = 1100 , C = 10; this score is equivalent to the posterior
probability of capitalization with a Beta(0.1, 9.9) prior.
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we?re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model?s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both ?TAGDICT and ?METAPH ab-
lation experiments.)
4 Experiments
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of ?2 = 0.5 and our system with a Gaus-
sian prior of ?2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
10We used the following feature modules in the Stanford tag-
ger: bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
45
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
?DISTSIM 87.88 88.31 (?1.06)
?TAGDICT 88.28 88.31 (?1.06)
?TWORTH 87.51 88.37 (?1.00)
?METAPH 88.18 88.95 (?0.42)
?NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ? L 93 V
, 98 ~ & 98 ?
P 95 R U 97 ,
? 71 N $ 89 P
D 95 ? # 89 ?
O 97 ? G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ?
~ 91 ,
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The ?TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means ?shoutout?, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (?) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
5 Conclusion
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
Acknowledgments
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
References
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ? and <? in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
46
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Schu?tze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406?418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
47
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176?181,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Better Hypothesis Testing for Statistical Machine Translation:
Controlling for Optimizer Instability
Jonathan H. Clark Chris Dyer Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jhclark,cdyer,alavie,nasmith}@cs.cmu.edu
Abstract
In statistical machine translation, a researcher
seeks to determine whether some innovation
(e.g., a new feature, model, or inference al-
gorithm) improves translation quality in com-
parison to a baseline system. To answer this
question, he runs an experiment to evaluate the
behavior of the two systems on held-out data.
In this paper, we consider how to make such
experiments more statistically reliable. We
provide a systematic analysis of the effects of
optimizer instability?an extraneous variable
that is seldom controlled for?on experimen-
tal outcomes, and make recommendations for
reporting results more accurately.
1 Introduction
The need for statistical hypothesis testing for ma-
chine translation (MT) has been acknowledged since
at least Och (2003). In that work, the proposed
method was based on bootstrap resampling and was
designed to improve the statistical reliability of re-
sults by controlling for randomness across test sets.
However, there is no consistently used strategy that
controls for the effects of unstable estimates of
model parameters.1 While the existence of opti-
mizer instability is an acknowledged problem, it is
only infrequently discussed in relation to the relia-
bility of experimental results, and, to our knowledge,
there has yet to be a systematic study of its effects on
1We hypothesize that the convention of ?trusting? BLEU
score improvements of, e.g., > 1, is not merely due to an ap-
preciation of what qualitative difference a particular quantita-
tive improvement will have, but also an implicit awareness that
current methodology leads to results that are not consistently
reproducible.
hypothesis testing. In this paper, we present a series
of experiments demonstrating that optimizer insta-
bility can account for substantial amount of variation
in translation quality,2 which, if not controlled for,
could lead to incorrect conclusions. We then show
that it is possible to control for this variable with a
high degree of confidence with only a few replica-
tions of the experiment and conclude by suggesting
new best practices for significance testing for ma-
chine translation.
2 Nondeterminism and Other
Optimization Pitfalls
Statistical machine translation systems consist of a
model whose parameters are estimated to maximize
some objective function on a set of development
data. Because the standard objectives (e.g., 1-best
BLEU, expected BLEU, marginal likelihood) are
not convex, only approximate solutions to the op-
timization problem are available, and the parame-
ters learned are typically only locally optimal and
may strongly depend on parameter initialization and
search hyperparameters. Additionally, stochastic
optimization and search techniques, such as mini-
mum error rate training (Och, 2003) and Markov
chain Monte Carlo methods (Arun et al, 2010),3
constitute a second, more obvious source of noise
in the optimization procedure.
This variation in the parameter vector affects the
quality of the model measured on both development
2This variation directly affects the output translations, and
so it will propagate to both automated metrics as well as human
evaluators.
3Online subgradient techniques such as MIRA (Crammer et
al., 2006; Chiang et al, 2008) have an implicit stochastic com-
ponent as well based on the order of the training examples.
176
data and held-out test data, independently of any ex-
perimental manipulation. Thus, when trying to de-
termine whether the difference between two mea-
surements is significant, it is necessary to control for
variance due to noisy parameter estimates. This can
be done by replication of the optimization procedure
with different starting conditions (e.g., by running
MERT many times).
Unfortunately, common practice in reporting ma-
chine translation results is to run the optimizer once
per system configuration and to draw conclusions
about the experimental manipulation from this sin-
gle sample. However, it could be that a particu-
lar sample is on the ?low? side of the distribution
over optimizer outcomes (i.e., it results in relatively
poorer scores on the test set) or on the ?high? side.
The danger here is obvious: a high baseline result
paired with a low experimental result could lead to a
useful experimental manipulation being incorrectly
identified as useless. We now turn to the question of
how to reduce the probability falling into this trap.
3 Related Work
The use of statistical hypothesis testing has grown
apace with the adoption of empirical methods in
natural language processing. Bootstrap techniques
(Efron, 1979; Wasserman, 2003) are widespread
in many problem areas, including for confidence
estimation in speech recognition (Bisani and Ney,
2004), and to determine the significance of MT re-
sults (Och, 2003; Koehn, 2004; Zhang et al, 2004;
Zhang and Vogel, 2010). Approximate randomiza-
tion (AR) has been proposed as a more reliable tech-
nique for MT significance testing, and evidence sug-
gests that it yields fewer type I errors (i.e., claiming
a significant difference where none exists; Riezler
and Maxwell, 2005). Other uses in NLP include
the MUC-6 evaluation (Chinchor, 1993) and pars-
ing (Cahill et al, 2008). However, these previous
methods assume model parameters are elements of
the system rather than extraneous variables.
Prior work on optimizer noise in MT has fo-
cused primarily on reducing optimizer instability
(whereas our concern is how to deal with optimizer
noise, when it exists). Foster and Kuhn (2009) mea-
sured the instability of held-out BLEU scores across
10 MERT runs to improve tune/test set correlation.
However, they only briefly mention the implications
of the instability on significance. Cer et al (2008)
explored regularization of MERT to improve gener-
alization on test sets. Moore and Quirk (2008) ex-
plored strategies for selecting better random ?restart
points? in optimization. Cer et al (2010) analyzed
the standard deviation over 5 MERT runs when each
of several metrics was used as the objective function.
4 Experiments
In our experiments, we ran the MERT optimizer to
optimize BLEU on a held-out development set many
times to obtain a set of optimizer samples on two dif-
ferent pairs of systems (4 configurations total). Each
pair consists of a baseline system (System A) and an
?experimental? system (System B), which previous
research has suggested will perform better.
The first system pair contrasts a baseline phrase-
based system (Moses) and experimental hierarchi-
cal phrase-based system (Hiero), which were con-
structed from the Chinese-English BTEC corpus
(0.7M words), the later of which was decoded with
the cdec decoder (Koehn et al, 2007; Chiang, 2007;
Dyer et al, 2010). The second system pair con-
trasts two German-English Hiero/cdec systems con-
structed from the WMT11 parallel training data
(98M words).4 The baseline system was trained on
unsegmented words, and the experimental system
was constructed using the most probable segmenta-
tion of the German text according to the CRF word
segmentation model of Dyer (2009). The Chinese-
English systems were optimized 300 times, and the
German-English systems were optimized 50 times.
Our experiments used the default implementation
of MERT that accompanies each of the two de-
coders. The Moses MERT implementation uses 20
random restart points per iteration, drawn uniformly
from the default ranges for each feature, and, at each
iteration, 200-best lists were extracted with the cur-
rent weight vector (Bertoldi et al, 2009). The cdec
MERT implementation performs inference over the
decoder search space which is structured as a hyper-
graph (Kumar et al, 2009). Rather than using restart
points, in addition to optimizing each feature inde-
pendently, it optimizes in 5 random directions per it-
eration by constructing a search vector by uniformly
sampling each element of the vector from (?1, 1)
and then renormalizing so it has length 1. For all
systems, the initial weight vector was manually ini-
tialized so as to yield reasonable translations.
4http://statmt.org/wmt11/
177
Metric System Avg ssel sdev stest
BTEC Chinese-English (n = 300)
BLEU ?
System A 48.4 1.6 0.2 0.5
System B 49.9 1.5 0.1 0.4
MET ?
System A 63.3 0.9 - 0.4
System B 63.8 0.9 - 0.5
TER ?
System A 30.2 1.1 - 0.6
System B 28.7 1.0 - 0.2
WMT German-English (n = 50)
BLEU ?
System A 18.5 0.3 0.0 0.1
System B 18.7 0.3 0.0 0.2
MET ?
System A 49.0 0.2 - 0.2
System B 50.0 0.2 - 0.1
TER ?
System A 65.5 0.4 - 0.3
System B 64.9 0.4 - 0.4
Table 1: Measured standard deviations of different au-
tomatic metrics due to test-set and optimizer variability.
sdev is reported only for the tuning objective function
BLEU.
Results are reported using BLEU (Papineni et
al., 2002), METEOR5 (Banerjee and Lavie, 2005;
Denkowski and Lavie, 2010), and TER (Snover et
al., 2006).
4.1 Extraneous variables in one system
In this section, we describe and measure (on the ex-
ample systems just described) three extraneous vari-
ables that should be considered when evaluating a
translation system. We quantify these variables in
terms of standard deviation s, since it is expressed
in the same units as the original metric. Refer to
Table 1 for the statistics.
Local optima effects sdev The first extraneous
variable we discuss is the stochasticity of the opti-
mizer. As discussed above, different optimization
runs find different local maxima. The noise due to
this variable can depend on many number of fac-
tors, including the number of random restarts used
(in MERT), the number of features in a model, the
number of references, the language pair, the portion
of the search space visible to the optimizer (e.g. 10-
best, 100-best, a lattice, a hypergraph), and the size
of the tuning set. Unfortunately, there is no proxy to
estimate this effect as with bootstrap resampling. To
control for this variable, we must run the optimizer
multiple times to estimate the spread it induces on
the development set. Using the n optimizer samples,
with mi as the translation quality measurement of
5METEOR version 1.2 with English ranking parameters and
all modules.
the development set for the ith optimization run, and
m is the average of all mis, we report the standard
deviation over the tuning set as sdev:
sdev =
?
?
?
?
n?
i=1
(mi ?m)
2
n? 1
A high sdev value may indicate that the optimizer is
struggling with local optima and changing hyperpa-
rameters (e.g. more random restarts in MERT) could
improve system performance.
Overfitting effects stest As with any optimizer,
there is a danger that the optimal weights for a tuning
set may not generalize well to unseen data (i.e., we
overfit). For a randomized optimizer, this means that
parameters can generalize to different degrees over
multiple optimizer runs. We measure the spread in-
duced by optimizer randomness on the test set met-
ric score stest, as opposed to the overfitting effect in
isolation. The computation of stest is identical to sdev
except that the mis are the translation metrics cal-
culated on the test set. In Table 1, we observe that
stest > sdev, indicating that optimized parameters are
likely not generalizing well.
Test set selection ssel The final extraneous vari-
able we consider is the selection of the test set it-
self. A good test set should be representative of
the domain or language for which experimental ev-
idence is being considered. However, with only a
single test corpus, we may have unreliable results
because of idiosyncrasies in the test set. This can
be mitigated in two ways. First, replication of ex-
periments by testing on multiple, non-overlapping
test sets can eliminate it directly. Since this is not
always practical (more test data may not be avail-
abile), the widely-used bootstrap resampling method
(?3) also controls for test set effects by resampling
multiple ?virtual? test sets from a single set, making
it possible to infer distributional parameters such as
the standard deviation of the translation metric over
(very similar) test sets.6 Furthermore, this can be
done for each of our optimizer samples. By averag-
ing the bootstrap-estimated standard deviations over
6Unlike actually using multiple test sets, bootstrap resam-
pling does not help to re-estimate the mean metric score due to
test set spread (unlike actually using multiple test sets) since the
mean over bootstrap replicates is approximately the aggregate
metric score.
178
optimizer samples, we have a statistic that jointly
quantifies the impact of test set effects and optimizer
instability on a test set. We call this statistic ssel.
Different values of this statistic can suggest method-
ological improvements. For example, a large ssel in-
dicates that more replications will be necessary to
draw reliable inferences from experiments on this
test set, so a larger test set may be helpful.
To compute ssel, assume we have n indepen-
dent optimization runs which produced weight vec-
tors that were used to translate a test set n times.
The test set has ` segments with references R =
?R1, R2, . . . , R`?. Let X = ?X1,X2, . . . ,Xn?
where each Xi = ?Xi1, Xi2, . . . , Xi`? is the list of
translated segments from the ith optimization run
list of the ` translated segments of the test set. For
each hypothesis output Xi, we construct k bootstrap
replicates by drawing ` segments uniformly, with re-
placement, from Xi, together with its corresponding
reference. This produces k virtual test sets for each
optimization run i. We designate the score of the jth
virtual test set of the ith optimization run with mij .
If mi = 1k
?k
j=1 mij , then we have:
si =
?
?
?
?
k?
j=1
(mij ?mi)
2
k ? 1
ssel =
1
n
n?
i=1
si
4.2 Comparing Two Systems
In the previous section, we gave statistics about
the distribution of evaluation metrics across a large
number of experimental samples (Table 1). Because
of the large number of trials we carried out, we can
be extremely confident in concluding that for both
pairs of systems, the experimental manipulation ac-
counts for the observed metric improvements, and
furthermore, that we have a good estimate of the
magnitude of that improvement. However, it is not
generally feasible to perform as many replications
as we did, so here we turn to the question of how
to compare two systems, accounting for optimizer
noise, but without running 300 replications.
We begin with a visual illustration how opti-
mizer instability affects test set scores when com-
paring two systems. Figure 1 plots the histogram
of the 300 optimizer samples each from the two
BTEC Chinese-English systems. The phrase-based
46 47 48 49 50 51
BLEU
0
5
10
15
20
25
30
35
40
Ob
se
rv
at
ion
 C
ou
nt
Figure 1: Histogram of test set BLEU scores for the
BTEC phrase-based system (left) and BTEC hierarchical
system (right). While the difference between the systems
is 1.5 BLEU in expectation, there is a non-trivial region
of overlap indicating that some random outcomes will re-
sult in little to no difference being observed.
0.6 0.3 0.0 0.3 0.6 0.9
BLEU difference
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Pr
ob
ab
ilit
y 
of
 o
bs
er
va
tio
n 1 sample
3 samples
5 samples
10 samples
50 samples
Figure 2: Relative frequencies of obtaining differences
in BLEU scores on the WMT system as a function of the
number of optimizer samples. The expected difference
is 0.2 BLEU. While there is a reasonably high chance of
observing a non-trivial improvement (or even a decline)
for 1 sample, the distribution quickly peaks around the
expected value given just a few more samples.
system?s distribution is centered at the sample
mean 48.4, and the hierarchical system is centered
at 49.9, a difference of 1.5 BLEU, correspond-
ing to the widely replicated result that hierarchi-
cal phrase-based systems outperform conventional
phrase-based systems in Chinese-English transla-
tion. Crucially, although the distributions are dis-
tinct, there is a non-trivial region of overlap, and
experimental samples from the overlapping region
could suggest the opposite conclusion!
To further underscore the risks posed by this over-
lap, Figure 2 plots the relative frequencies with
which different BLEU score deltas will occur, as a
function of the number of optimizer samples used.
When is a difference significant? To determine
whether an experimental manipulation results in a
179
statistically reliable difference for an evaluation met-
ric, we use a stratified approximate randomization
(AR) test. This is a nonparametric test that approxi-
mates a paired permutation test by sampling permu-
tations (Noreen, 1989). AR estimates the probability
(p-value) that a measured difference in metric scores
arose by chance by randomly exchanging sentences
between the two systems. If there is no significant
difference between the systems (i.e., the null hypoth-
esis is true), then this shuffling should not change
the computed metric score. Crucially, this assumes
that the samples being analyzed are representative
of all extraneous variables that could affect the out-
come of the experiment. Therefore, we must include
multiple optimizer replications. Also, since metric
scores (such as BLEU) are in general not compa-
rable across test sets, we stratify, exchanging only
hypotheses that correspond to the same sentence.
Table 2 shows the p-values computed by AR, test-
ing the significance of the differences between the
two systems in each pair. The first three rows illus-
trate ?single sample? testing practice. Depending on
luck with MERT, the results can vary widely from
insignificant (at p > .05) to highly significant.
The last two lines summarize the results of the test
when a small number of replications are performed,
as ought to be reasonable in a research setting. In
this simulation, we randomly selected n optimizer
outputs from our large pool and ran the AR test to
determine the significance; we repeated this proce-
dure 250 times. The p-values reported are the p-
values at the edges of the 95% confidence interval
(CI) according to AR seen in the 250 simulated com-
parison scenarios. These indicate that we are very
likely to observe a significant difference for BTEC
at n = 5, and a very significant difference by n = 50
(Table 2). Similarly, we see this trend in the WMT
system: more replications leads to more significant
results, which will be easier to reproduce. Based on
the average performance of the systems reported in
Table 1, we expect significance over a large enough
number of independent trials.
5 Discussion and Recommendations
No experiment can completely control for all pos-
sible confounding variables. Nor are metric scores
(even if they are statistically reliable) a substitute
for thorough human analysis. However, we believe
that the impact of optimizer instability has been ne-
p-value
n System A System B BTEC WMT
1 high low 0.25 0.95
1 median median 0.15 0.13
1 low high 0.0003 0.003
p-value (95% CI)
5 random random 0.001?0.034 0.001?0.38
50 random random 0.001?0.001 0.001?0.33
Table 2: Two-system analysis: AR p-values for three
different ?single sample? scenarios that illustrate differ-
ent pathological scenarios that can result when the sam-
pled weight vectors are ?low? or ?high.? For ?random,?
we simulate an experiments with n optimization replica-
tions by drawing n optimized system outputs from our
pool and performing AR; this simulation was repeated
250 times and the 95% CI of the AR p-values is reported.
glected by standard experimental methodology in
MT research, where single-sample measurements
are too often used to assess system differences. In
this paper, we have provided evidence that optimizer
instability can have a substantial impact on results.
However, we have also shown that it is possible to
control for it with very few replications (Table 2).
We therefore suggest:
? Replication be adopted as standard practice in
MT experimental methodology, especially in
reporting results;7
? Replication of optimization (MERT) and test
set evaluation be performed at least three times;
more replications may be necessary for experi-
mental manipulations with more subtle effects;
? Use of the median system according to a trusted
metric when manually analyzing system out-
put; preferably, the median should be deter-
mined based on one test set and a second test
set should be manually analyzed.
Acknowledgments
We thank Michael Denkowski, Kevin Gimpel, Kenneth
Heafield, Michael Heilman, and Brendan O?Connor for
insightful feedback. This research was supported in part
by the National Science Foundation through TeraGrid re-
sources provided by Pittsburgh Supercomputing Center
under TG-DBS110003; the National Science Foundation
under IIS-0713402, IIS-0844507, IIS-0915187, and IIS-
0915327; the DARPA GALE program, the U. S. Army
Research Laboratory, and the U. S. Army Research Of-
fice under contract/grant number W911NF-10-1-0533.
7Source code to carry out the AR test for multiple optimizer
samples on the three metrics in this paper is available from
http://github.com/jhclark/multeval.
180
References
A. Arun, B. Haddow, P. Koehn, A. Lopez, C. Dyer,
and P. Blunsom. 2010. Monte Carlo techniques
for phrase-based translation. Machine Translation,
24:103?121.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In Proc. of ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
N. Bertoldi, B. Haddow, and J.-B. Fouet. 2009. Im-
proved minimum error rate training in Moses. Prague
Bulletin of Mathematical Linguistics, No. 91:7?16.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluation.
In Proc. of ICASSP.
A. Cahill, M. Burke, R. O?Donovan, S. Riezler, J. van
Genabith, and A. Way. 2008. Wide-coverage deep
statistical parsing using automatic dependency struc-
ture annotation. Computational Linguistics, 34(1):81?
124.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2010. The best
lexical metric for phrase-based statistical mt system
optimization. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 555?563. Proc. of ACL, June.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
N. Chinchor. 1993. The statistical significance of the
MUC-5 results. Proc. of MUC.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
M. Denkowski and A. Lavie. 2010. Extending the
METEOR machine translation evaluation metric to the
phrase level. In Proc. of NAACL.
C. Dyer, J. Weese, A. Lopez, V. Eidelman, P. Blunsom,
and P. Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proc. of ACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
B. Efron. 1979. Bootstrap methods: Another look at the
jackknife. The Annals of Statistics, 7(1):1?26.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
P. Koehn, A. Birch, C. Callison-burch, M. Federico,
N. Bertoldi, B. Cowan, C. Moran, C. Dyer, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of COLING, Manchester, UK.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-j. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proc. of the Workshop on Intrinsic and Extrin-
sic Evaluation Methods for Machine Translation and
Summarization.
M. Snover, B. Dorr, C. Park, R. Schwartz, L. Micciulla,
and J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
L. Wasserman. 2003. All of Statistics: A Concise Course
in Statistical Inference. Springer.
Y. Zhang and S. Vogel. 2010. Significance tests of auto-
matic machine translation metrics. Machine Transla-
tion, 24:51?65.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
BLEU/NIST scores: How much improvement do we
need to have a better system? In Proc. of LREC.
181
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 685?693,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Probabilistic Model for Canonicalizing Named Entity Mentions
Dani Yogatama Yanchuan Sim Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,ysim,nasmith}@cs.cmu.edu
Abstract
We present a statistical model for canonicalizing
named entity mentions into a table whose rows rep-
resent entities and whose columns are attributes (or
parts of attributes). The model is novel in that it
incorporates entity context, surface features, first-
order dependencies among attribute-parts, and a no-
tion of noise. Transductive learning from a few
seeds and a collection of mention tokens combines
Bayesian inference and conditional estimation. We
evaluate our model and its components on two
datasets collected from political blogs and sports
news, finding that it outperforms a simple agglom-
erative clustering approach and previous work.
1 Introduction
Proper handling of mentions in text of real-world
entities?identifying and resolving them?is a cen-
tral part of many NLP applications. We seek an al-
gorithm that infers a set of real-world entities from
mentions in a text, mapping each entity mention to-
ken to an entity, and discovers general categories of
words used in names (e.g., titles and last names).
Here, we use a probabilistic model to infer a struc-
tured representation of canonical forms of entity at-
tributes through transductive learning from named
entity mentions with a small number of seeds (see
Table 1). The input is a collection of mentions found
by a named entity recognizer, along with their con-
texts, and, following Eisenstein et al (2011), the
output is a table in which entities are rows (the num-
ber of which is not pre-specified) and attribute words
are organized into columns.
This paper contributes a model that builds on the
approach of Eisenstein et al (2011), but also:
? incorporates context of the mention to help with
disambiguation and to allow mentions that do not
share words to be merged liberally;
? conditions against shape features, which improve
the assignment of words to columns;
? is designed to explicitly handle some noise; and
? is learned using elements of Bayesian inference
with conditional estimation (see ?2).
We experiment with variations of our model,
comparing it to a baseline clustering method and the
model of Eisenstein et al (2011), on two datasets,
demonstrating improved performance over both at
recovering a gold standard table. In a political
blogs dataset, the mentions refer to political fig-
ures in the United States (e.g., Mrs. Obama and
Michelle Obama). As a result, the model discov-
ers parts of names??Mrs., Michelle, Obama??
while simultaneously performing coreference res-
olution for named entity mentions. In the sports
news dataset, the model is provided with named en-
tity mentions of heterogenous types, and success
here consists of identifying the correct team for ev-
ery player (e.g., Kobe Bryant and Los Angeles Lak-
ers). In this scenario, given a few seed examples,
the model begins to identify simple relations among
named entities (in addition to discovering attribute
structures), since attributes are expressed as named
entities across multiple mentions. We believe this
adaptability is important, as the salience of different
kinds of names and their usages vary considerably
across domains.
Bill Clinton Mr.
George Bush Mr. W.
Barack Obama Sen. Hussein
Hillary Clinton Mrs. Sen.
Bristol Palin Ms.
Emil Jones Jr.
Kay Hutchison Bailey
Ben Roethlisberger Steelers
Bryant Los Angeles
Derek Jeter New York
Table 1: Seeds for politics (above) and sports (below).
685
 x ?
 
1
1
f
w c
r
s
?
 
?
?
M
L
T
?
C
Figure 1: Graphical representation of our model. Top,
the generation of the table: C is the number of at-
tributes/columns, the number of rows is infinite, ? is a
vector of concentration parameters, ? is a multinomial
distribution over strings, and x is a word in a table cell.
Lower left, for choosing entities to be mentioned: ? deter-
mines the stick lengths and ? is the distribution over en-
tities to be selected for mention. Middle right, for choos-
ing attributes to use in a mention: f is the feature vector,
and ? is the weight vector drawn from a Laplace distri-
bution with mean zero and variance ?. Center, for gen-
erating mentions: M is the number of mentions in the
data, w is a word token set from an entity/row r and at-
tribute/column c. Lower right, for generating contexts: s
is a context word, drawn from a multinomial distribution
? with a Dirichlet prior ?. Variables that are known or
fixed are shaded; variables that are optimized are double
circled. Others are latent; dashed lines imply collapsing.
2 Model
We begin by assuming as input a set of mention to-
kens, each one or more words. In our experiments
these are obtained by running a named entity recog-
nizer. The output is a table in which rows are un-
derstood to correspond to entities (types, not men-
tion tokens) and columns are fields, each associated
with an attribute or a part of it. Our approach is
based on a probabilistic graphical model that gener-
ates the mentions, which are observed, and the table,
which is mostly unobserved, similar to Eisenstein et
al. (2011). Our learning procedure is a hybrid of
Bayesian inference and conditional estimation. The
generative story, depicted in Figure 1, is:
? For each column j ? {1, . . . , C}:
? Draw a multinomial distribution ?j over the
vocabulary from a Dirichlet process: ?j ?
DP(?j , G0). This is the lexicon for field j.
? Generate table entries. For each row i (of which
there are infinitely many), draw an entry xi,j
for cell i, j from ?j . A few of these entries (the
seeds) are observed; we denote those x?.
? Draw weights ?j that associate shape and po-
sitional features with columns from a 0-mean,
?-variance Laplace distribution.
? Generate the distribution over entities to be men-
tioned in general text: ? ? GEM(?) (?stick-
breaking? distribution).
? Generate context distributions. For each row r:
? Draw a multinomial over the context vocabu-
lary (distinct from mention vocabulary) from a
Dirichlet distribution, ?r ? Dir(?).
? For each mention token m:
? Draw an entity/row r ? ?.
? For each word in the mention w, given some of
its features f (assumed observed):
. Choose a column c ? 1Z exp(?
>
c f). This
uses a log-linear distribution with partition
function Z. In one variation of our model,
first-order dependencies among the columns
are enabled; these introduce a dynamic char-
acter to the graphical model that is not shown
in Figure 1.
. With probability 1 ? , set the text wm` to
be xrc. Otherwise, generate any word from a
unigram-noise distribution.
? Generate mention context. For each of the T =
10 context positions (five before and five after
the mention), draw the word s from ?r.
Our choices of prior distributions reflect our be-
liefs about the shapes of the various distributions.
We expect field lexicons ?j and the distributions
over mentioned entities ? to be ?Zipfian? and so use
tools from nonparametric statistics to model them.
We expect column-feature weights ? to be mostly
zero, so a sparsity-inducing Laplace prior is used
(Tibshirani, 1996).
Our goal is to maximize the conditional likeli-
hood of most of the evidence (mentions, contexts,
and seeds), p(w, s, x? | ?,?, ?, ?, ?, ,f) =
?
r
?
c
?
x\x?
?
d?
?
d?
?
d?
p(w, s, r, c, x, ?, ?, ? | ?,?, ?, ?, ?, ,f)
686
with respect to ? and ? . We fix ? (see ?3.3 for the
values of ? for each dataset), ? = 2 (equivalent to
add-one smoothing), ? = 2 ? 10?8,  = 10?10,
and each mention word?s f . Fixing ?, ?, and ? is
essentially just ?being Bayesian,? or fixing a hyper-
parameter based on prior beliefs. Fixing f is quite
different; it is conditioning our model on some ob-
servable features of the data, in this case word shape
features. We do this to avoid integrating over fea-
ture vector values. These choices highlight that the
design of a probabilistic model can draw from both
Bayesian and discriminative tools. Observing some
of x as seeds (x?) renders this approach transductive.
Exact inference in this model is intractable, so we
resort to an approximate inference technique based
on Markov Chain Monte Carlo simulation. The opti-
mization of ? can be described as ?contrastive? esti-
mation (Smith and Eisner, 2005), in which some as-
pects of the data are conditioned against for compu-
tational convenience. The optimization of ? can be
described as ?empirical Bayesian? estimation (Mor-
ris, 1983) in which the parameters of a prior are
fit to data. Our overall learning procedure is a
Monte Carlo Expectation Maximization algorithm
(Wei and Tanner, 1990).
3 Learning and Inference
Our learning procedure is an iterative algorithm con-
sisting of two steps. In the E-step, we perform col-
lapsed Gibbs sampling to obtain distributions over
row and column indices for every mention, given the
current value of the hyperparamaters. In the M-step,
we obtain estimates for the hyperparameters, given
the current posterior distributions.
3.1 E-step
For the mth mention, we sample row index r, then
for each word wm`, we sample column index c.
3.1.1 Sampling Rows
Similar to Eisenstein et al (2011), when we sam-
ple the row for a mention, we use Bayes? rule and
marginalize the columns. We further incorporate
context information and a notion of noise.
p(rm = r | . . .) ? p(rm = r | r?m, ?)
(
?
`
?
c p(wm` | x, rm = r, cm` = c))
(
?
t p(smt | rm = r))
We consider each quantity in turn.
Prior. The probability of drawing a row index fol-
lows a stick breaking distribution. This allows us
to have an unbounded number of rows and let the
model infer the optimal value from data. A standard
marginalization of ? gives us:
p(rm = r | r?m, ?) =
{
N?mr
N+? if N
?m
r > 0
?
N+? otherwise,
where N is the number of mentions, Nr is the num-
ber of mentions assigned to row r, and N?mr is the
number of mentions assigned to row r, excludingm.
Mention likelihood. In order to compute the likeli-
hood of observing mentions in the dataset, we have
to consider a few cases. If a cell in a table has al-
ready generated a word, it can only generate that
word. This hard constraint was a key factor in the
inference algorithm of Eisenstein et al (2011); we
speculate that softening it may reduce MCMC mix-
ing time, so introduce a notion of noise. With proba-
bility  = 10?10, the cell can generate any word. If a
cell has not generated any word, its probability still
depends on other elements of the table. With base
distribution G0,1 and marginalizing ?, we have:
p(wm` | x, rm = r, cm` = c, ?c) = (1)
?
????
????
1?  if xrc = wm`
 if xrc 6? {wm`,?}
N?m`cw
N?m`c +?c
if xrc = ? and Ncw > 0
G0(wm`)
?c
N?m`c +?c
if xrc = ? and Ncw = 0
where N?m`c is the number of cells in column c that
are not empty and N?m`cw is the number of cells in
column c that are set to the word wm`; both counts
excluding the current word under consideration.
Context likelihood. It is important to be able to
use context information to determine which row
a mention should go into. As a novel extension,
our model also uses surrounding words of a men-
tion as its ?context??similar context words can en-
courage two mentions that do not share any words
to be merged. We choose a Dirichlet-multinomial
distribution for our context distribution. For every
row in the table, we have a multinomial distribution
over context vocabulary ?r from a Dirichlet prior ?.
1We let G0 be a uniform distribution over the vocabulary.
687
Therefore, the probability of observing the tth con-
text word for mention m is p(smt | rm = r, ?)
=
{
N?mtrs +?s?1
N?mtr +
P
v ?v?V
if N?mtr > 0
?s?1P
v ?v?V
otherwise,
whereN?mtr is the number of context words of men-
tions assigned to row r, N?mtrs is the number of con-
text words of mentions assigned to row r that are
smt, both excluding the current context word, and v
ranges over the context vocabulary of size V .
3.1.2 Sampling Columns
Our column sampling procedure is novel to this
work and substantially differs from that of Eisen-
stein et al (2011). First, we note that when we sam-
ple column indices for each word in a mention, the
row index for the mention r has already been sam-
pled. Also, our model has interdependencies among
column indices of a mention.2 Standard Gibbs sam-
pling procedure breaks down these dependencies.
For faster mixing, we experiment with first-order
dependencies between columns when sampling col-
umn indices. This idea was suggested by Eisenstein
et al (2011, footnote 1) as a way to learn structure
in name conventions. We suppressed this aspect of
the model in Figure 1 for clarity.
We sample the column index c1 for the first word
in the mention, marginalizing out probabilities of
other words in the mention. After we sample the
column index for the first word, we sample the col-
umn index c2 for the second word, fixing the pre-
vious word to be in column c1, and marginalizing
out probabilities of c3, . . . , cL as before. We repeat
the above procedure until we reach the last word
in the mention. In practice, this can be done effi-
ciently using backward probabilities computed via
dynamic programming. This kind of blocked Gibbs
sampling was proposed by Jensen et al (1995) and
used in NLP by Mochihashi et al (2009). We have:
p(cm` = c | . . .) ?
p(cm` = c | fm`, ?)p(cm` = c | cm`? = c?)
(?
c+ pb(cm` = c | cm`+ = c+)
)
p(wm` | x, rm = r, cm` = c, ?c),
2As shown in Figure 1, column indices in a mention form
?v-structures? with the row index r. Since everyw` is observed,
there is an active path that goes through all these nodes.
where `? is the preceding word and c? is its sam-
pled index, `+ is the following word and c+ is its
possible index, and pb(?) are backward probabilities.
Alternatively, we can perform standard Gibbs sam-
pling and drop the dependencies between columns,
which makes the model rely more heavily on the fea-
tures. For completeness, we detail the computations.
Featurized log linear distribution. Our model can
use arbitrary features to choose a column index.
These features are incorporated as a log-linear dis-
tribution, p(cm` = c | fm`,?) =
exp(?>c fm`)P
c? exp(?
>
c?
fm`)
.
The list of features used in our experiments is:
1{w is the first word in the mention}; 1{w ends
with a period}; 1{w is the last word in the men-
tion}; 1{w is a Roman numeral}; 1{w starts with
an upper-case letter}; 1{w is an Arabic number};
1{w ? {mr,mrs,ms,miss, dr,mdm} }; 1{w con-
tains ? 1 punctuation symbol}; 1{w ? {jr, sr}};
1{w ? {is, in, of, for}}; 1{w is a person entity};
1{w is an organization entity}.
Forward and backward probabilities. Since
we introduce first-order dependencies between
columns, we have forward and backward probabili-
ties, as in HMMs. However, we always sample from
left to right, so we do not need to marginalize ran-
dom variables to the left of the current variable be-
cause their values are already sampled. Our transi-
tion probabilities are as follows:
p(cm` = c | cm`? = c?) =
N?mc?,c
P
c??
N?m
c??,c
,
whereN?mc?,c is the number of times we observe tran-
sitions from column c? to c, excluding mention m.
The forward and backward equations are simple (we
omit them for space).
Mention likelihood. Mention likelihood p(wm` |
x, rm = r, cm` = c, ?c) is identical to when we
sample the row index (Eq. 1).
3.2 M-step
In the M-step, we use gradient-based optimization
routines, L-BFGS (Liu and Nocedal, 1989) and
OWL-QN (Andrew and Gao, 2007) respectively, to
maximize with respect to ? and ?.
688
3.3 Implementation Details
We ran Gibbs sampling for 500 iterations,3 discard-
ing the first 200 for burn-in and averaging counts
over every 10th sample to reduce autocorrelation.
For each word in a mention w, we introduced 12
binary features f for our featurized log-linear distri-
bution (?3.1.2).
We then downcased all words in mentions for the
purpose of defining the table and the mention words
w. Ten context words (5 each to the left and right)
define s for each mention token.
For non-convex optimization problems like ours,
initialization is important. To guide the model to
reach a good local optimum without many restarts,
we manually initialized feature weights and put a
prior on transition probabilities to reflect phenom-
ena observed in the initial seeds. The initializer was
constructed once and not tuned across experiments.4
The M-step was performed every 50 Gibbs sampling
iterations. After inference, we filled each cell with
the word that occurred at least 80% of the time in the
averaged counts for the cell, if such a word existed.
4 Experiments
We compare several variations of our model to
Eisenstein et al (2011) (the authors provided their
implementation to us) and a clustering baseline.
4.1 Datasets
We collected named entity mentions from two cor-
pora: political blogs and sports news. The political
blogs corpus is a collection of blog posts about poli-
tics in the United States (Eisenstein and Xing, 2010),
and the sports news corpus contains news summaries
of major league sports games (National Basketball
3On our moderate-sized datasets (see ?4.1), each iteration
takes approximately three minutes on a 2.2GHz CPU.
4For the politics dataset, we set C = 6, ? =
?1.0, 1.0, 10?12, 10?15, 10?12, 10?8?, initialized ? = 1, and
used a Dirichlet prior on transition counts such that before ob-
serving any data: N0,1 = 10, N0,5 = 5, N2,0 = 10, N2,1 =
10, N2,3 = 10, N2,4 = 5, N3,0 = 10, N3,1 = 10, N5,1 = 15
(others are set to zero). For the sports dataset, we set C = 5,
? = ?1.0, 1.0, 10?15, 10?6, 10?6?, initialized ? = 1, and
used a Dirichlet prior on transition counts N0,1 = 10, N2,3 =
20, N3,4 = 10 (others are set to zero). We also manually initial-
ized the weights of some features? for both datasets. These val-
ues were obtained from preliminary experiments on a smaller
sample of the datasets, and updated on the first EM iteration.
Politics Sports
# source documents 3,000 700
# mentions 10,647 13,813
# unique mentions 528 884
size of mention vocabulary 666 1,177
size of context vocabulary 2,934 2,844
Table 2: Descriptive statistics about the datasets.
Association, National Football League, and Major
League Baseball) in 2009. Due to the large size of
the corpora, we uniformly sampled a subset of doc-
uments for each corpus and ran the Stanford NER
tagger (Finkel et al, 2005), which tagged named en-
tities mentions as person, location, and organization.
We used named entity of type person from the po-
litical blogs corpus, while we are interested in per-
son and organization entities for the sports news cor-
pus. Mentions that appear less than five times are
discarded. Table 2 summarizes statistics for both
datasets of named entity mentions.
Reference tables. We use Eisenstein et al?s man-
ually built 125-entity (282 vocabulary items) refer-
ence table for the politics dataset. Each entity in the
table is represented by the set of all tokens that app-
pear in its references, and the tokens are placed in its
correct column. For the sports data, we obtained a
roster of all NBA, NFL, and MLB players in 2009.
We built our sports reference table using the play-
ers? names, teams and locations, to get 3,642 play-
ers and 15,932 vocabulary items. The gold standard
table for the politics dataset is incomplete, whereas
it is complete for the sports dataset.
Seeds. Table 1 shows the seeds for both datasets.
4.2 Evaluation Scores
We propose both a row evaluation to determine
how well a model disambiguates entities and merges
mentions of the same entity and a column evaluation
to measure how well the model relates words used in
different mentions. Both scores are new for this task.
The first step in evaluation is to find a maximum
score bipartite matching between rows in the re-
sponse and reference table.5 Given the response and
5Treating each row as a set of words, we can optimize the
matching using the Jonker and Volgenant (1987) algorithm.
The column evaluation is identical, except that sets of words
that are matched are defined by columns. We use the Jaccard
similarity?for two sets A and B, |A?B||A?B|?for our similarity
function, Sim(i, j).
689
reference tables, xres and xref , we can compute:
Sres = 1|xres |
?
i?xres ,j?xref :Match(i,j)=1
Sim(i, j)
Sref = 1
|
xref |
?
i?xres ,j?xref :Match(i,j)=1
Sim(i, j)
where i and j denote rows, Match(i, j) is one if i and
j are matched to each other in the optimal matching
or zero otherwise. Sres is a precision-like score, and
Sref is a recall-like score.6 Column evaluation is the
same, but compares columns instead.
4.3 Baselines
Our simple baseline is an agglomerative clustering
algorithm that clusters mentions into entities using
the single-linkage criterion. Initially, each unique
mention forms its own cluster that we incremen-
tally merge together to form rows in the table. This
method requires a similarity score between two clus-
ters. For the politics dataset, we follow Eisenstein et
al. (2011) and use the string edit distance between
mention strings in each cluster to define the score.
For the sports dataset, since mentions contain per-
son and organization named entity types, our score
for clustering uses the Jaccard distance between con-
text words of the mentions. However, such cluster-
ings do not produce columns. Therefore, we first
match words in mentions of type person against
a regular expression to recognize entity attributes
from a fixed set of titles and suffixes, and the first,
middle and last names. We treat words in mentions
of type organization as a single attribute.7 As we
merge clusters together, we arrange words such that
6Eisenstein et al (2011) used precision and recall for their
similarity function. Precision prefers a more compact response
row (or column), which unfairly penalizes situations like those
of our sports dataset, where rows are heterogeneous (e.g., in-
cluding people and organizations). Consider a response ta-
ble made up of two rows: ?Kobe, Bryant? and ?Los, Ange-
les, Lakers?, and a reference table containing all NBA play-
ers and their team names, e.g., ?Kobe, Bryant, Los, Angeles,
Lakers?. Evaluating with the precision similarity function, we
will have perfect precision by matching the first row to the ref-
erence row for Kobe Bryant and the latter row to any Lakers
player. The system is not rewarded for merging the two rows
together, even if they are describing the same entity. Our eval-
uation scores, however, reward the system for accurately filling
in more cells.
7Note that the baseline system uses NER tags, list of titles
and suffixes; which are also provided to our model through the
features in ?3.1.2.
all words within a column belong to the same at-
tribute, creating columns as necessary to accomo-
date multiple similar attributes as a result of merg-
ing. We can evaluate the tables produced by each
step of the clustering to obtain the entire sequence
of response-reference scores.
As a strong baseline, we also compare our ap-
proach with the original implementation of the
model of Eisenstein et al (2011), denoted by EEA.
4.4 Results
For both the politics and sports dataset, we run the
procedure in ?3.3 three times and report the results.
Politics. The results for the politics dataset are
shown in Figure 2. Our model consistently outper-
formed both baselines. We also analyze how much
each of our four main extensions (shape features,
context information, noise, and first-order column
dependencies) to EEA contributes to overall per-
formance by ablating each in turn (also shown in
Fig. 2). The best-performing complete model has
415 rows, of which 113 were matched to the ref-
erence table. Shape features are useful: remov-
ing them was detrimental, and they work even bet-
ter without column dependencies. Indeed, the best
model did not have column dependencies. Remov-
ing context features had a strong negative effect,
though perhaps this could be overcome with a more
carefully tuned initializer.
In row evaluation, the baseline system can achieve
a high reference score by creating one entity row per
unique string, but as it merges strings, the clusters
encompass more word tokens, improving response
score at the expense of reference score. With fewer
clusters, there are fewer entities in the response ta-
ble for matching and the response score suffers. Al-
though we use the same seed table in both exper-
iments, the results from EEA are below the base-
line curve because it has the additional complexity
of inferring the number of columns from data. Our
model is simpler in this regard since it assumes that
the number of columns is known (C = 6). In col-
umn evaluation, our method without column depen-
dencies was best.
Sports. The results for the sports dataset are shown
in Figure 3. Our best-performing complete model?s
response table contains 599 rows, of which 561
were matched to the reference table. In row eval-
690
 0.2 0.2
1
 0.2
2
 0.2
3
 0.2
4
 0.2
5  0
.1 
0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
response score
ref
ere
nce
 sc
ore
 0.3 0.3
5 0.4
 0
 0.0
5 0.1 0.1
5 0.2 0.2
5 0.3
 0.1
 0.1
5
 0.2
 0.2
5
 0.3
 0.3
5
response score
ref
ere
nce
 sc
ore
bas
elin
e
EE
A
com
ple
te
-de
pen
den
cie
s
-no
ise
-co
nte
xt
-fe
atu
res
Figure 2: Row (left) and column (right) scores for the politics dataset. For all but ?baseline? (clustering), each point
denotes a unique sampling run. Note the change in scale in the left plot at y = 0.25. For the clustering baseline, points
correspond to iterations.
 0.2
5 0.3 0.3
5 0.4
 0
 0.0
2 
0.0
4 
0.0
6 
0.0
8
 0.1
response score
ref
ere
nce
 sc
ore
 0
 0.0
5 0.1 0.1
5 0.2 0.2
5  0
 0.0
5
 0.1
 0.1
5
 0.2
 0.2
5
response score
ref
ere
nce
 sc
ore
bas
elin
e
EE
A
com
ple
te
-de
pen
den
cie
s
-no
ise
-co
nte
xt
-fe
atu
res
Figure 3: Row (left) and column (right) scores for the sports dataset. Each point denotes a unique sampling run. The
reference score is low since the reference set includes all entities in the NBA, NFL, and MLB, but most of them were
not mentioned in our dataset.
uation, our model lies above the baseline response-
reference score curve, demonstrating its ability to
correctly identify and combine player mentions with
their team names. Similar to the previous dataset,
our model is also substantially better in column eval-
uation, indicating that it mapped mention words into
a coherent set of five columns.
4.5 Discussion
The two datasets illustrate that our model adapts to
somewhat different tasks, depending on its input.
Furthermore, fixing C (unlike EEA) does appear to
have benefits.
In the politics dataset, most of the mentions con-
tain information about people. Therefore, besides
canonicalizing named entities, the model also re-
solves within-document and cross-document coref-
erence, since it assigned a row index for every men-
tion. For example, our model learned that most men-
tions of John McCain, Sen. John McCain, Sen. Mc-
Cain, and Mr. McCain refer to the same entity. Ta-
ble 3 shows a few noteworthy entities from our com-
plete model?s output table.
Barack Obama Mr. Sen. Hussein
Michelle Obama Mrs.
Norm Coleman Sen.
Sarah Palin Ms.
John McCain Mr. Sen. Hussein
Table 3: A small segment of the output table for the poli-
tics dataset, showing a few noteworthy correct (blue) and
incorrect (red) examples. Black indicates seeds. Though
Ms. is technically correct, there is variation in prefer-
ences and conventions. Our data include eight instances
of Ms. Palin and none of Mrs. Palin or Mrs. Sarah
Palin.
The first entity is an easy example since it only
had to complete information provided in the seed ta-
ble. The model found the correct gender-specific ti-
tle for Barack Obama, Mr.. The rest of the examples
were fully inferred from the data. The model was es-
sentially correct for the second, third, and fourth en-
tities. The last row illustrates a partially erroneous
example, in which the model confused the middle
name of John McCain, possibly because of a com-
bination of a strong prior to reuse this row and the
691
Derek Jeter New York
Ben Roethlisberger Pittsburgh Steelers
Alex Rodriguez New York Yankees
Michael Vick Philadelphia Eagles
Kevin Garnett Los Angeles Lakers
Dave Toub The Bears
Table 4: A small segment of the output table for the sports
dataset, showing a few noteworthy correct (blue) and in-
correct (red) examples. Black indicates seed examples.
introduction of a notion of noise.
In the sports dataset, persons and organizations
are mentioned. Recall that success here consists of
identifying the correct team for every player. The
EEA model is not designed for this and performed
poorly. Our model can do better, since it makes use
of context information and features, and it can put a
person and an organization in one row even though
they do not share common words. Table 4 shows a
few noteworthy entities from our complete model?s
output.
Surprisingly, the model failed to infer that Derek
Jeter plays for New York Yankees, although men-
tions of the organization New York Yankees can be
found in our dataset. This is because the model did
not see enough evidence to put them in the same row.
However, it successfully inferred the missing infor-
mation for Ben Roethlisberger. The next two rows
show cases where our model successfully matched
players with their teams and put each word token to
its respective column. The most frequent error, by
far, is illustrated in the fifth row, where a player is
matched with a wrong team. Kevin Garnett plays for
the Boston Celtics, not the Los Angeles Lakers. It
shows that in some cases context information is not
adequate, and a possible improvement might be ob-
tained by providing more context to the model. The
sixth row is interesting because Dave Toub is indeed
affiliated with the Chicago Bears. However, when
the model saw a mention token The Bears, it did not
have any other columns to put the word token The,
and decided to put it in the fourth column although it
is not a location. If we added more columns, deter-
miners could become another attribute of the entities
that might go into one of these new columns.
5 Related Work
There has been work that attempts to fill predefined
templates using Bayesian nonparametrics (Haghighi
and Klein, 2010) and automatically learns template
structures using agglomerative clustering (Cham-
bers and Jurafsky, 2011). Charniak (2001) and El-
sner et al (2009) focused specifically on names and
discovering their structure, which is a part of the
problem we consider here. More similar to our
work, Eisenstein et al (2011) introduced a non-
parametric Bayesian approach to extract structured
databases of entities. A fundamental difference of
our approach from any of the previous work is it
maximizes conditional likelihood and thus allows
beneficial incorporation of arbitrary features.
Our model is focused on the problem of canoni-
calizing mention strings into their parts, though its r
variables (which map mentions to rows) could be in-
terpreted as (within-document and cross-document)
coreference resolution, which has been tackled us-
ing a range of probabilistic models (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). We have not evaluated it
as such, believing that further work should be done
to integrate appropriate linguistic cues before such
an application.
6 Conclusions
We presented an improved probabilistic model for
canonicalizing named entities into a table. We
showed that the model adapts to different tasks de-
pending on its input and seeds, and that it improves
over state-of-the-art performance on two corpora.
Acknowledgements
The authors thank Jacob Eisenstein and Tae Yano for
helpful discussions and providing us with the implemen-
tation of their model, Tim Hawes for helpful discussions,
Naomi Saphra for assistance in developing the gold stan-
dard for the politics dataset, and three anonymous review-
ers for comments on an earlier draft of this paper. This re-
search was supported in part by the U.S. Army Research
Office, Google?s sponsorship of the Worldly Knowledge
project at CMU, and A?STAR (fellowship to Y. Sim); the
contents of this paper do not necessarily reflect the posi-
tion or the policy of the sponsors, and no official endorse-
ment should be inferred.
692
References
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
N. Chambers and D. Jurafsky. 2011. Template-based
information extraction without the templates. In Proc.
of ACL-HLT.
E. Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proc. of NAACL.
J. Eisenstein and E. P. Xing. 2010. The CMU 2008 po-
litical blog corpus. Technical report, Carnegie Mellon
University.
J. Eisenstein, T. Yano, W. W. Cohen, N. A. Smith, and
E. P. Xing. 2011. Structured databases of named
entities from Bayesian nonparametrics. In Proc. of
EMNLP Workshop on Unsupervised Learning in NLP.
M. Elsner, E. Charniak, and M. Johnson. 2009. Struc-
tured generative models for unsupervised named-
entity clustering. In Proc. of NAACL-HLT.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proc. of ACL.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric Bayesian model.
In Proc. of ACL.
A. Haghighi and D. Klein. 2010. An entity-level ap-
proach to information extraction. In Proc. of ACL
Short Papers.
C. S. Jensen, U. Kjaerulff, and A. Kong. 1995. Blocking
Gibbs sampling in very large probabilistic expert sys-
tem. International Journal of Human-Computer Stud-
ies, 42(6):647?666.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
X. Li, P. Morie, and D. Roth. 2004. Identification and
tracing of ambiguous names: discriminative and gen-
erative approaches. In Proc. of AAAI.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503?528.
D. Mochihashi, T. Yamada, and N. Ueda. 2009.
Bayesian unsupervised word segmentation with nested
Pitman-Yor language modeling. In Proc. of ACL-
IJCNLP.
C. Morris. 1983. Parametric empirical Bayes inference:
Theory and applications. Journal of the American Sta-
tistical Association, 78(381):47?65.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov logic. In Proc. of
EMNLP.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum.
2011. Large-scale cross-document coreference using
distributed inference and hierarchical models. In Proc.
of ACL-HLT.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of Royal Statistical Society B,
58(1):267?288.
G. C. G. Wei and M. A. Tanner. 1990. A Monte Carlo
implementation of the EM algorithm and the poor
man?s data augmentation algorithms. Journal of the
American Statistical Association, 85(411):699?704.
693
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?258,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Coarse Lexical Semantic Annotation with Supersenses:
An Arabic Case Study
Nathan Schneider? Behrang Mohit? Kemal Oflazer? Noah A. Smith?
School of Computer Science, Carnegie Mellon University
?Doha, Qatar ?Pittsburgh, PA 15213, USA
{nschneid@cs.,behrang@,ko@cs.,nasmith@cs.}cmu.edu
Abstract
?Lightweight? semantic annotation of text
calls for a simple representation, ideally with-
out requiring a semantic lexicon to achieve
good coverage in the language and domain.
In this paper, we repurpose WordNet?s super-
sense tags for annotation, developing specific
guidelines for nominal expressions and ap-
plying them to Arabic Wikipedia articles in
four topical domains. The resulting corpus
has high coverage and was completed quickly
with reasonable inter-annotator agreement.
1 Introduction
The goal of ?lightweight? semantic annotation of
text, particularly in scenarios with limited resources
and expertise, presents several requirements for a
representation: simplicity; adaptability to new lan-
guages, topics, and genres; and coverage. This
paper describes coarse lexical semantic annotation
of Arabic Wikipedia articles subject to these con-
straints. Traditional lexical semantic representations
are either narrow in scope, like named entities,1 or
make reference to a full-fledged lexicon/ontology,
which may insufficiently cover the language/domain
of interest or require prohibitive expertise and ef-
fort to apply.2 We therefore turn to supersense tags
(SSTs), 40 coarse lexical semantic classes (25 for
nouns, 15 for verbs) originating in WordNet. Previ-
ously these served as groupings of English lexicon
1Some ontologies like those in Sekine et al (2002) and BBN
Identifinder (Bikel et al, 1999) include a large selection of
classes, which tend to be especially relevant to proper names.
2E.g., a WordNet (Fellbaum, 1998) sense annotation effort
reported by Passonneau et al (2010) found considerable inter-
annotator variability for some lexemes; FrameNet (Baker et
al., 1998) is limited in coverage, even for English; and Prop-
Bank (Kingsbury and Palmer, 2002) does not capture semantic
relationships across lexemes. We note that the Omega ontol-
ogy (Philpot et al, 2003) has been used for fine-grained cross-
lingual annotation (Hovy et al, 2006; Dorr et al, 2010).
Q.

J?K

considers
H. A

J?
book
?

	
JJ
k.
Guinness
?A

P?

C?
for-records

?J
?AJ


?? @
the-standard
COMMUNICATION
	
?

@
that

???Ag.
university
	
?@?Q


?? @
Al-Karaouine
ARTIFACT
?


	
?
in
?A
	
?
Fez
H. Q
	??? @
Morocco
LOCATION
?Y

?

@
oldest

???Ag.
university
GROUP
?


	
?
in
??A?? @
the-world
LOCATION
IJ
k
where
??

'
was
A?D?J
?

A

K
established
ACT
?


	
?
in

?
	
J?
year
859 ?


XCJ
?
AD
TIME
.
?The Guinness Book of World Records considers the
University of Al-Karaouine in Fez, Morocco, established
in the year 859 AD, the oldest university in the world.?
Figure 1: A sentence from the article ?Islamic Golden
Age,? with the supersense tagging from one of two anno-
tators. The Arabic is shown left-to-right.
entries, but here we have repurposed them as target
labels for direct human annotation.
Part of the earliest versions of WordNet, the
supersense categories (originally, ?lexicographer
classes?) were intended to partition all English noun
and verb senses into broad groupings, or semantic
fields (Miller, 1990; Fellbaum, 1990). More re-
cently, the task of automatic supersense tagging has
emerged for English (Ciaramita and Johnson, 2003;
Curran, 2005; Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009), as well as for Italian (Picca et al,
2008; Picca et al, 2009; Attardi et al, 2010) and
Chinese (Qiu et al, 2011), languages with WordNets
mapped to English WordNet.3 In principle, we be-
lieve supersenses ought to apply to nouns and verbs
in any language, and need not depend on the avail-
ability of a semantic lexicon.4 In this work we focus
on the noun SSTs, summarized in figure 2 and ap-
plied to an Arabic sentence in figure 1.
SSTs both refine and relate lexical items: they
capture lexical polysemy on the one hand?e.g.,
3Note that work in supersense tagging used text with fine-
grained sense annotations that were then coarsened to SSTs.
4The noun/verb distinction might prove problematic in some
languages.
253
Crusades ?Damascus ? Ibn Tolun Mosque ? Imam Hussein Shrine ? Islamic Golden Age ? Islamic History ?Ummayad Mosque 434s 16,185t 5,859m
Atom ? Enrico Fermi ? Light ? Nuclear power ? Periodic Table ? Physics ? Muhammad al-Razi 777s 18,559t 6,477m
2004 Summer Olympics ?Christiano Ronaldo ?Football ?FIFA World Cup ?Portugal football team ?Rau?l Gonza?les ?Real Madrid 390s 13,716t 5,149m
Computer ? Computer Software ? Internet ? Linux ? Richard Stallman ? Solaris ? X Window System 618s 16,992t 5,754m
Table 1: Snapshot of the supersense-annotated data. The 7 article titles (translated) in each domain, with total counts
of sentences, tokens, and supersense mentions. Overall, there are 2,219 sentences with 65,452 tokens and 23,239
mentions (1.3 tokens/mention on average). Counts exclude sentences marked as problematic and mentions marked ?.
disambiguating PERSON vs. POSSESSION for the
noun principal?and generalize across lexemes on
the other?e.g., principal, teacher, and student can
all be PERSONs. This lumping property might be
expected to give too much latitude to annotators; yet
we find that in practice, it is possible to elicit reason-
able inter-annotator agreement, even for a language
other than English. We encapsulate our interpreta-
tion of the tags in a set of brief guidelines that aims
to be usable by anyone who can read and understand
a text in the target language; our annotators had no
prior expertise in linguistics or linguistic annotation.
Finally, we note that ad hoc categorization
schemes not unlike SSTs have been developed for
purposes ranging from question answering (Li and
Roth, 2002) to animacy hierarchy representation for
corpus linguistics (Zaenen et al, 2004). We believe
the interpretation of the SSTs adopted here can serve
as a single starting point for diverse resource en-
gineering efforts and applications, especially when
fine-grained sense annotation is not feasible.
2 Tagging Conventions
WordNet?s definitions of the supersenses are terse,
and we could find little explicit discussion of the
specific rationales behind each category. Thus,
we have crafted more specific explanations, sum-
marized for nouns in figure 2. English examples
are given, but the guidelines are intended to be
language-neutral. A more systematic breakdown,
formulated as a 43-rule decision list, is included
with the corpus.5 In developing these guidelines
we consulted English WordNet (Fellbaum, 1998)
and SemCor (Miller et al, 1993) for examples and
synset definitions, occasionally making simplifying
decisions where we found distinctions that seemed
esoteric or internally inconsistent. Special cases
(e.g., multiword expressions, anaphora, figurative
5For example, one rule states that all man-made structures
(buildings, rooms, bridges, etc.) are to be tagged as ARTIFACTs.
language) are addressed with additional rules.
3 Arabic Wikipedia Annotation
The annotation in this work was on top of a small
corpus of Arabic Wikipedia articles that had al-
ready been annotated for named entities (Mohit et
al., 2012). Here we use two different annotators,
both native speakers of Arabic attending a university
with English as the language of instruction.
Data & procedure. The dataset (table 1) consists of
the main text of 28 articles selected from the topical
domains of history, sports, science, and technology.
The annotation task was to identify and categorize
mentions, i.e., occurrences of terms belonging to
noun supersenses. Working in a custom, browser-
based interface, annotators were to tag each relevant
token with a supersense category by selecting the to-
ken and typing a tag symbol. Any token could be
marked as continuing a multiword unit by typing <.
If the annotator was ambivalent about a token they
were to mark it with the ? symbol. Sentences were
pre-tagged with suggestions where possible.6 Anno-
tators noted obvious errors in sentence splitting and
grammar so ill-formed sentences could be excluded.
Training. Over several months, annotators alter-
nately annotated sentences from 2 designated arti-
cles of each domain, and reviewed the annotations
for consistency. All tagging conventions were deve-
loped collaboratively by the author(s) and annotators
during this period, informed by points of confusion
and disagreement. WordNet and SemCor were con-
sulted as part of developing the guidelines, but not
during annotation itself so as to avoid complicating
the annotation process or overfitting to WordNet?s
idiosyncracies. The training phase ended once inter-
annotator mention F1 had reached 75%.
6Suggestions came from the previous named entity annota-
tion of PERSONs, organizations (GROUP), and LOCATIONs, as
well as heuristic lookup in lexical resources?Arabic WordNet
entries (Elkateb et al, 2006) mapped to English WordNet, and
named entities in OntoNotes (Hovy et al, 2006).
254
O NATURAL OBJECT natural feature or nonliving object in
nature barrier reef nest neutron star
planet sky fishpond metamorphic rock Mediterranean cave
stepping stone boulder Orion ember universe
A ARTIFACT man-made structures and objects bridge
restaurant bedroom stage cabinet toaster antidote aspirin
L LOCATION any name of a geopolitical entity, as well as
other nouns functioning as locations or regions
Cote d?Ivoire New York City downtown stage left India
Newark interior airspace
P PERSON humans or personified beings; names of social
groups (ethnic, political, etc.) that can refer to an individ-
ual in the singular Persian deity glasscutter mother
kibbutznik firstborn worshiper Roosevelt Arab consumer
appellant guardsman Muslim American communist
G GROUP groupings of people or objects, including: orga-
nizations/institutions; followers of social movements
collection flock army meeting clergy Mennonite Church
trumpet section health profession peasantry People?s Party
U.S. State Department University of California population
consulting firm communism Islam (= set of Muslims)
$ SUBSTANCE a material or substance krypton mocha
atom hydrochloric acid aluminum sand cardboard DNA
H POSSESSION term for an entity involved in ownership or
payment birthday present tax shelter money loan
T TIME a temporal point, period, amount, or measurement
10 seconds day Eastern Time leap year 2nd millenium BC
2011 (= year) velocity frequency runtime latency/delay
middle age half life basketball season words per minute
curfew industrial revolution instant/moment August
= RELATION relations between entities or quantities
ratio scale reverse personal relation exponential function
angular position unconnectedness transitivity
Q QUANTITY quantities and units of measure, including
cardinal numbers and fractional amounts 7 cm 1.8 million
12 percent/12% volume (= spatial extent) volt real number
square root digit 90 degrees handful ounce half
F FEELING subjective emotions indifference wonder
murderousness grudge desperation astonishment suffering
M MOTIVE an abstract external force that causes someone
to intend to do something reason incentive
C COMMUNICATION information encoding and transmis-
sion, except in the sense of a physical object
grave accent Book of Common Prayer alphabet
Cree language onomatopoeia reference concert hotel bill
broadcast television program discussion contract proposal
equation denial sarcasm concerto software
? COGNITION aspects of mind/thought/knowledge/belief/
perception; techniques and abilities; fields of academic
study; social or philosophical movements referring to the
system of beliefs Platonism hypothesis
logic biomedical science necromancy hierarchical structure
democracy innovativeness vocational program woodcraft
reference visual image Islam (= Islamic belief system) dream
scientific method consciousness puzzlement skepticism
reasoning design intuition inspiration muscle memory skill
aptitude/talent method sense of touch awareness
S STATE stable states of affairs; diseases and their symp-
toms symptom reprieve potency
poverty altitude sickness tumor fever measles bankruptcy
infamy opulence hunger opportunity darkness (= lack of light)
@ ATTRIBUTE characteristics of people/objects that can be
judged resilience buxomness virtue immateriality
admissibility coincidence valence sophistication simplicity
temperature (= degree of hotness) darkness (= dark coloring)
! ACT things people do or cause to happen; learned pro-
fessions meddling malpractice faith healing dismount
carnival football game acquisition engineering (= profession)
E EVENT things that happens at a given place and time
bomb blast ordeal miracle upheaval accident tide
R PROCESS a sustained phenomenon or one marked by
gradual changes through a series of states
oscillation distillation overheating aging accretion/growth
extinction evaporation
X PHENOMENON a physical force or something that hap-
pens/occurs electricity suction tailwind tornado effect
+ SHAPE two and three dimensional shapes
D FOOD things used as food or drink
B BODY human body parts, excluding diseases and their
symptoms
Y PLANT a plant or fungus
N ANIMAL non-human, non-plant life
Science chemicals, molecules, atoms, and subatomic
particles are tagged as SUBSTANCE
Sports championships/tournaments are EVENTs
(Information) Technology Software names, kinds, and
components are tagged as COMMUNICATION (e.g. kernel,
version, distribution, environment). A connection is a RE-
LATION; project, support, and a configuration are tagged
as COGNITION; development and collaboration are ACTs.
Arabic conventions Masdar constructions (verbal
nouns) are treated as nouns. Anaphora are not tagged.
Figure 2: Above: The complete supersense tagset for nouns; each tag is briefly described by its symbol, NAME,
short description, and examples. Some examples and longer descriptions have been omitted due to space constraints.
Below: A few domain- and language-specific elaborations of the general guidelines.
255
Figure 3: Distribution of supersense mentions by
domain (left), and counts for tags occurring over
800 times (below). (Counts are of the union of the
annotators? choices, even when they disagree.)
tag num tag num
ACT (!) 3473 LOCATION (G) 1583
COMMUNICATION (C) 3007 GROUP (L) 1501
PERSON (P) 2650 TIME (T) 1407
ARTIFACT (A) 2164 SUBSTANCE ($) 1291
COGNITION (?) 1672 QUANTITY (Q) 1022
Main annotation. After training, the two annota-
tors proceeded on a per-document basis: first they
worked together to annotate several sentences from
the beginning of the article, then each was inde-
pendently assigned about half of the remaining sen-
tences (typically with 5?10 shared to measure agree-
ment). Throughout the process, annotators were en-
couraged to discuss points of confusion with each
other, but each sentence was annotated in its entirety
and never revisited. Annotation of 28 articles re-
quired approximately 100 annotator-hours. Articles
used in pilot rounds were re-annotated from scratch.
Analysis. Figure 3 shows the distribution of SSTs in
the corpus. Some of the most concrete tags?BODY,
ANIMAL, PLANT, NATURAL OBJECT, and FOOD?
were barely present, but would likely be frequent
in life sciences domains. Others, such as MOTIVE,
POSSESSION, and SHAPE, are limited in scope.
To measure inter-annotator agreement, 87 sen-
tences (2,774 tokens) distributed across 19 of the ar-
ticles (not including those used in pilot rounds) were
annotated independently by each annotator. Inter-
annotator mention F1 (counting agreement over en-
tire mentions and their labels) was 70%. Excluding
the 1,397 tokens left blank by both annotators, the
token-level agreement rate was 71%, with Cohen?s
? = 0.69, and token-level F1 was 83%.7
We also measured agreement on a tag-by-tag ba-
sis. For 8 of the 10 most frequent SSTs (fig-
ure 3), inter-annotator mention F1 ranged from 73%
to 80%. The two exceptions were QUANTITY at
63%, and COGNITION (probably the most heteroge-
neous category) at 49%. An examination of the con-
fusion matrix reveals four pairs of supersense cate-
gories that tended to provoke the most disagreement:
COMMUNICATION/COGNITION, ACT/COGNITION,
ACT/PROCESS, and ARTIFACT/COMMUNICATION.
7Token-level measures consider both the supersense label
and whether it begins or continues the mention.
The last is exhibited for the first mention in figure 1,
where one annotator chose ARTIFACT (referring to
the physical book) while the other chose COMMU-
NICATION (the content). Also in that sentence, an-
notators disagreed on the second use of university
(ARTIFACT vs. GROUP). As with any sense anno-
tation effort, some disagreements due to legitimate
ambiguity and different interpretations of the tags?
especially the broadest ones?are unavoidable.
A ?soft? agreement measure (counting as matches
any two mentions with the same label and at least
one token in common) gives an F1 of 79%, show-
ing that boundary decisions account for a major por-
tion of the disagreement. E.g., the city Fez, Mo-
rocco (figure 1) was tagged as a single LOCATION
by one annotator and as two by the other. Further
examples include the technical term ?thin client?,
for which one annotator omitted the adjective; and
?World Cup Football Championship?, where one an-
notator tagged the entire phrase as an EVENT while
the other tagged ?football? as a separate ACT.
4 Conclusion
We have codified supersense tags as a simple an-
notation scheme for coarse lexical semantics, and
have shown that supersense annotation of Ara-
bic Wikipedia can be rapid, reliable, and robust
(about half the tokens in our data are covered
by a nominal supersense). Our tagging guide-
lines and corpus are available for download at
http://www.ark.cs.cmu.edu/ArabicSST/.
Acknowledgments
We thank Nourhen Feki and Sarah Mustafa for assistance
with annotation, as well as Emad Mohamed, CMU ARK
members, and anonymous reviewers for their comments.
This publication was made possible by grant NPRP-08-
485-1-083 from the Qatar National Research Fund (a
member of the Qatar Foundation). The statements made
herein are solely the responsibility of the authors.
256
References
Giuseppe Attardi, Stefano Dei Rossi, Giulia Di Pietro,
Alessandro Lenci, Simonetta Montemagni, and Maria
Simi. 2010. A resource and tool for super-sense
tagging of Italian texts. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics (COLING-
ACL ?98), pages 86?90, Montreal, Quebec, Canada,
August. Association for Computational Linguistics.
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning, 34(1).
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in WordNet. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages 168?
175, Sapporo, Japan, July.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan, June.
Bonnie J. Dorr, Rebecca J. Passonneau, David Farwell,
Rebecca Green, Nizar Habash, Stephen Helmreich,
Eduard Hovy, Lori Levin, Keith J. Miller, Teruko
Mitamura, Owen Rambow, and Advaith Siddharthan.
2010. Interlingual annotation of parallel text corpora:
a new framework for annotation and evaluation. Nat-
ural Language Engineering, 16(03):197?243.
Sabri Elkateb, William Black, Horacio Rodr??guez, Musa
Alkhalifa, Piek Vossen, Adam Pease, and Christiane
Fellbaum. 2006. Building a WordNet for Arabic.
In Proceedings of The Fifth International Conference
on Language Resources and Evaluation (LREC 2006),
pages 29?34, Genoa, Italy.
Christiane Fellbaum. 1990. English verbs as a semantic
net. International Journal of Lexicography, 3(4):278?
301, December.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, MA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL (HLT-
NAACL), pages 57?60, New York City, USA, June.
Association for Computational Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Las Palmas, Canary Islands,
May.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING?02),
pages 1?7, Taipei, Taiwan, August. Association for
Computational Linguistics.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the Workshop on Human Language
Technology (HLT ?93), HLT ?93, pages 303?308,
Plainsboro, NJ, USA, March. Association for Compu-
tational Linguistics.
George A. Miller. 1990. Nouns in WordNet: a lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264, December.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, and Noah A. Smith. 2012.
Recall-oriented learning of named entities in Arabic
Wikipedia. In Proceedings of the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2012), pages 162?173, Avi-
gnon, France, April. Association for Computational
Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploiting
semantic constraints for estimating supersenses with
CRFs. In Proceedings of the Ninth SIAM International
Conference on Data Mining, pages 485?496, Sparks,
Nevada, USA, May. Society for Industrial and Applied
Mathematics.
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of polysemous words by multiple annotators.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
257
Andrew G. Philpot, Michael Fleischman, and Eduard H.
Hovy. 2003. Semi-automatic construction of a general
purpose ontology. In Proceedings of the International
Lisp Conference, New York, NY, USA, October.
Davide Picca, Alfio Massimiliano Gliozzo, and Mas-
similiano Ciaramita. 2008. Supersense Tagger
for Italian. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
Piperidis, and Daniel Tapias, editors, Proceedings of
the Sixth International Language Resources and Eval-
uation (LREC?08), pages 2386?2390, Marrakech, Mo-
rocco, May. European Language Resources Associa-
tion (ELRA).
Davide Picca, Alfio Massimiliano Gliozzo, and Simone
Campora. 2009. Bridging languages by SuperSense
entity tagging. In Proceedings of the 2009 Named
Entities Workshop: Shared Task on Transliteration
(NEWS 2009), pages 136?142, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Likun Qiu, Yunfang Wu, Yanqiu Shao, and Alexander
Gelbukh. 2011. Combining contextual and struc-
tural information for supersense tagging of Chinese
unknown words. In Computational Linguistics and In-
telligent Text Processing: Proceedings of the 12th In-
ternational Conference on Computational Linguistics
and Intelligent Text Processing (CICLing?11), volume
6608 of Lecture Notes in Computer Science, pages 15?
28. Springer, Berlin.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of the Third International Conference on Lan-
guage Resources and Evaluation (LREC-02), Las Pal-
mas, Canary Islands, May.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M. Catherine O?Connor, and Tom Wasow. 2004. An-
imacy encoding in English: why and how. In Bon-
nie Webber and Donna K. Byron, editors, ACL 2004
Workshop on Discourse Annotation, pages 118?125,
Barcelona, Spain, July. Association for Computational
Linguistics.
258
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352?361,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Latent Personas of Film Characters
David Bamman Brendan O?Connor Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dbamman,brenocon,nasmith}@cs.cmu.edu
Abstract
We present two latent variable models for
learning character types, or personas, in
film, in which a persona is defined as a
set of mixtures over latent lexical classes.
These lexical classes capture the stereo-
typical actions of which a character is the
agent and patient, as well as attributes by
which they are described. As the first
attempt to solve this problem explicitly,
we also present a new dataset for the
text-driven analysis of film, along with
a benchmark testbed to help drive future
work in this area.
1 Introduction
Philosophers and dramatists have long argued
whether the most important element of narrative
is plot or character. Under a classical Aristotelian
perspective, plot is supreme;1 modern theoretical
dramatists and screenwriters disagree.2
Without addressing this debate directly, much
computational work on narrative has focused on
learning the sequence of events by which a story
is defined; in this tradition we might situate sem-
inal work on learning procedural scripts (Schank
and Abelson, 1977; Regneri et al, 2010), narrative
chains (Chambers and Jurafsky, 2008), and plot
structure (Finlayson, 2011; Elsner, 2012; McIn-
tyre and Lapata, 2010; Goyal et al, 2010).
We present a complementary perspective that
addresses the importance of character in defining
1?Dramatic action . . . is not with a view to the representa-
tion of character: character comes in as subsidiary to the ac-
tions . . . The Plot, then, is the first principle, and, as it were,
the soul of a tragedy: Character holds the second place.? Po-
etics I.VI (Aristotle, 335 BCE).
2?Aristotle was mistaken in his time, and our scholars are
mistaken today when they accept his rulings concerning char-
acter. Character was a great factor in Aristotle?s time, and no
fine play ever was or ever will be written without it? (Egri,
1946, p. 94); ?What the reader wants is fascinating, complex
characters? (McKee, 1997, 100).
a story. Our testbed is film. Under this perspec-
tive, a character?s latent internal nature drives the
action we observe. Articulating narrative in this
way leads to a natural generative story: we first de-
cide that we?re going to make a particular kind of
movie (e.g., a romantic comedy), then decide on a
set of character types, or personas, we want to see
involved (the PROTAGONIST, the LOVE INTER-
EST, the BEST FRIEND). After picking this set, we
fill out each of these roles with specific attributes
(female, 28 years old, klutzy); with this cast of
characters, we then sketch out the set of events by
which they interact with the world and with each
other (runs but just misses the train, spills coffee
on her boss) ? through which they reveal to the
viewer those inherent qualities about themselves.
This work is inspired by past approaches that in-
fer typed semantic arguments along with narra-
tive schemas (Chambers and Jurafsky, 2009; Reg-
neri et al, 2011), but seeks a more holistic view
of character, one that learns from stereotypical at-
tributes in addition to plot events. This work also
naturally draws on earlier work on the unsuper-
vised learning of verbal arguments and semantic
roles (Pereira et al, 1993; Grenager and Manning,
2006; Titov and Klementiev, 2012) and unsuper-
vised relation discovery (Yao et al, 2011).
This character-centric perspective leads to two
natural questions. First, can we learn what those
standard personas are by how individual charac-
ters (who instantiate those types) are portrayed?
Second, can we learn the set of attributes and ac-
tions by which we recognize those common types?
How do we, as viewers, recognize a VILLIAN?
At its most extreme, this perspective reduces
to learning the grand archetypes of Joseph Camp-
bell (1949) or Carl Jung (1981), such as the HERO
or TRICKSTER. We seek, however, a more fine-
grained set that includes not only archetypes, but
stereotypes as well ? characters defined by a fixed
set of actions widely known to be representative of
352
a class. This work offers a data-driven method for
answering these questions, presenting two proba-
blistic generative models for inferring latent char-
acter types.
This is the first work that attempts to learn ex-
plicit character personas in detail; as such, we
present a new dataset for character type induction
in film and a benchmark testbed for evaluating fu-
ture work.3
2 Data
2.1 Text
Our primary source of data comes from 42,306
movie plot summaries extracted from the
November 2, 2012 dump of English-language
Wikipedia.4 These summaries, which have a
median length of approximately 176 words,5
contain a concise synopsis of the movie?s events,
along with implicit descriptions of the characters
(e.g., ?rebel leader Princess Leia,? ?evil lord Darth
Vader?). To extract structure from this data, we
use the Stanford CoreNLP library6 to tag and
syntactically parse the text, extract entities, and
resolve coreference within the document. With
this structured representation, we extract linguistic
features for each character, looking at immediate
verb governors and attribute syntactic dependen-
cies to all of the entity?s mention headwords,
extracted from the typed dependency tuples pro-
duced by the parser; we refer to ?CCprocessed?
syntactic relations described in de Marneffe and
Manning (2008):
? Agent verbs. Verbs for which the entity is an
agent argument (nsubj or agent).
? Patient verbs. Verbs for which the entity is
the patient, theme or other argument (dobj,
nsubjpass, iobj, or any prepositional argu-
ment prep *).
? Attributes. Adjectives and common noun
words that relate to the mention as adjecti-
val modifiers, noun-noun compounds, appos-
itives, or copulas (nsubj or appos governors,
or nsubj, appos, amod, nn dependents of an
entity mention).
3All datasets and software for replication can be found at
http://www.ark.cs.cmu.edu/personas.
4http://dumps.wikimedia.org/enwiki/
5More popular movies naturally attract more attention on
Wikipedia and hence more detail: the top 1,000 movies by
box office revenue have a median length of 715 words.
6http://nlp.stanford.edu/software/
corenlp.shtml
These three roles capture three different ways in
which character personas are revealed: the actions
they take on others, the actions done to them, and
the attributes by which they are described. For ev-
ery character we thus extract a bag of (r, w) tu-
ples, where w is the word lemma and r is one
of {agent verb,patient verb, attribute} as iden-
tified by the above rules.
2.2 Metadata
Our second source of information consists of char-
acter and movie metadata drawn from the Novem-
ber 4, 2012 dump of Freebase.7 At the movie
level, this includes data on the language, country,
release date and detailed genre (365 non-mutually
exclusive categories, including ?Epic Western,?
?Revenge,? and ?Hip Hop Movies?). Many of the
characters in movies are also associated with the
actors who play them; since many actors also have
detailed biographical information, we can ground
the characters in what we know of those real peo-
ple ? including their gender and estimated age at
the time of the movie?s release (the difference be-
tween the release date of the movie and the actor?s
date of birth).
Across all 42,306 movies, entities average 3.4
agent events, 2.0 patient events, and 2.1 attributes.
For all experiments described below, we restrict
our dataset to only those events that are among the
1,000 most frequent overall, and only characters
with at least 3 events. 120,345 characters meet this
criterion; of these, 33,559 can be matched to Free-
base actors with a specified gender, and 29,802 can
be matched to actors with a given date of birth. Of
all actors in the Freebase data whose age is given,
the average age at the time of movie is 37.9 (stan-
dard deviation 14.1); of all actors whose gender
is known, 66.7% are male.8 The age distribution
is strongly bimodal when conditioning on gender:
the average age of a female actress at the time of a
movie?s release is 33.0 (s.d. 13.4), while that of a
male actor is 40.5 (s.d. 13.7).
3 Personas
One way we recognize a character?s latent type
is by observing the stereotypical actions they
7http://download.freebase.com/
datadumps/
8Whether this extreme 2:1 male/female ratio reflects an
inherent bias in film or a bias in attention on Freebase (or
Wikipedia, on which it draws) is an interesting research ques-
tion in itself.
353
perform (e.g., VILLAINS strangle), the actions
done to them (e.g., VILLAINS are foiled and ar-
rested) and the words by which they are described
(VILLAINS are evil). To capture this intuition, we
define a persona as a set of three typed distribu-
tions: one for the words for which the character is
the agent, one for which it is the patient, and one
for words by which the character is attributively
modified. Each distribution ranges over a fixed set
of latent word classes, or topics. Figure 1 illus-
trates this definition for a toy example: a ZOMBIE
persona may be characterized as being the agent
of primarily eating and killing actions, the patient
of killing actions, and the object of dead attributes.
The topic labeled eat may include words like eat,
drink, and devour.
eat kill lov
e
dea
d
hap
py
agent
0.0
0.2
0.4
0.6
0.8
1.0
eat kill lov
e
dea
d
hap
py
patient
0.0
0.2
0.4
0.6
0.8
1.0
eat kill lov
e
dea
d
hap
py
attribute
0.0
0.2
0.4
0.6
0.8
1.0
Figure 1: A persona is a set of three distributions
over latent topics. In this toy example, the ZOM-
BIE persona is primarily characterized by being
the agent of words from the eat and kill topics, the
patient of kill words, and the object of words from
the dead topic.
4 Models
Both models that we present here simultaneously
learn three things: 1.) a soft clustering over words
to topics (e.g., the verb ?strangle? is mostly a type
of Assault word); 2.) a soft clustering over top-
ics to personas (e.g., VILLIANS perform a lot of
Assault actions); and 3.) a hard clustering over
characters to personas (e.g., Darth Vader is a VIL-
LAIN.) They each use different evidence: since
our data includes not only textual features (in the
form of actions and attributes of the characters) but
also non-textual information (such as movie genre,
age and gender), we design a model that exploits
this additional source of information in discrimi-
nating between character types; since this extra-
linguistic information may not always be avail-
able, we also design a model that learns only from
the text itself. We present the text-only model first
?
?
p
z?
w
r
?
?
?
W
E
D
?
p me
md
?
?
?
2
z?
w
r
?
?
?
W
E
D
P Number of personas (hyperparameter)
K Number of word topics (hyperparameter)
D Number of movie plot summaries
E Number of characters in movie d
W Number of (role, word) tuples used by character e
?k Topic k?s distribution over V words.
r Tuple role: agent verb, patient verb, attribute
?p,r Distribution over topics for persona p in role r
?d Movie d?s distribution over personas
pe Character e?s persona (integer, p ? {1..P})
j A specific (r, w) tuple in the data
zj Word topic for tuple j
wj Word for tuple j
? Concentration parameter for Dirichlet model
? Feature weights for regression model
?, ?2 Gaussian mean and variance (for regularizing ?)
md Movie features (from movie metadata)
me Entity features (from movie actor metadata)
?r , ? Dirichlet concentration parameters
Figure 2: Above: Dirichlet persona model (left)
and persona regression model (right). Bottom:
Definition of variables.
for simplicity. Throughout, V is the word vocab-
ulary size, P is the number of personas, and K is
the number of topics.
4.1 Dirichlet Persona Model
In the most basic model, we only use informa-
tion from the structured text, which comes as a
bag of (r, w) tuples for each character in a movie,
where w is the word lemma and r is the rela-
tion of the word with respect to the character (one
of agent verb, patient verb or attribute, as out-
lined in ?2.1 above). The generative story runs as
follows. First, let there be K latent word topics;
as in LDA (Blei et al, 2003), these are words that
will be soft-clustered together by virtue of appear-
ing in similar contexts. Each latent word cluster
354
?k ? Dir(?) is a multinomial over the V words in
the vocabulary, drawn from a Dirichlet parameter-
ized by ?. Next, let a persona p be defined as a set
of three multinomials ?p over these K topics, one
for each typed role r, each drawn from a Dirichlet
with a role-specific hyperparameter (?r).
Every document (a movie plot summary) con-
tains a set of characters, each of which is associ-
ated with a single latent persona p; for every ob-
served (r, w) tuple associated with the character,
we sample a latent topic k from the role-specific
?p,r. Conditioned on this topic assignment, the
observed word is drawn from ?k. The distribu-
tion of these personas for a given document is de-
termined by a document-specific multinomial ?,
drawn from a Dirichlet parameterized by ?.
Figure 2 (above left) illustrates the form of the
model. To simplify inference, we collapse out the
persona-topic distributions ?, the topic-word dis-
tributions ? and the persona distribution ? for each
document. Inference on the remaining latent vari-
ables ? the persona p for each character type and
the topic z for each word associated with that char-
acter ? is conducted via collapsed Gibbs sampling
(Griffiths and Steyvers, 2004); at each iteration,
for each character e, we sample their persona pe:
P (pe = k | p?e, z, ?, ?) ?
(
c?ed,k + ?k
)
??j
(c?erj ,k,zj+?rj )
(c?erj ,k,?+K?rj )
(1)
Here, c?ed,k is the count of all characters in docu-ment d whose current persona sample is also k
(not counting the current character e under con-
sideration);9 j ranges over all (rj , wj) tuples asso-
ciated with character e. Each c?erj ,k,zj is the countof all tuples with role rj and current topic zj used
with persona k. c?erj ,k,? is the same count, summingover all topics z. In other words, the probabil-
ity that character e embodies persona k is propor-
tional to the number of other characters in the plot
summary who also embody that persona (plus the
Dirichlet hyperparameter ?k) times the contribu-
tion of each observed word wj for that character,
given its current topic assignment zj .
Once all personas have been sampled, we sam-
9The?e superscript denotes counts taken without consid-
ering the current sample for character e.
ple the latent topics for each tuple as the following.
P (zj = k | p, z?j , w, r, ?, ?) ?
(c?jrj ,p,k+?rj )
(c?jrj ,p,?+K?rj )
?
(c?jk,wj+?)
(c?jk,?+V ?)
(2)
Here, conditioned on the current sample p for
the character?s persona, the probability that tuple
j originates in topic k is proportional to the num-
ber of other tuples with that same role rj drawn
from the same topic for that persona (c?jrj ,p,k), nor-malized by the number of other rj tuples associ-
ated with that persona overall (c?jrj ,p,?), multiplied
by the number of times word wj is associated with
that topic (c?jk,wj ) normalized by the total numberof other words associated with that topic overall
(c?jk,?).We optimize the values of the Dirichlet hyper-
parameters ?, ? and ? using slice sampling with a
uniform prior every 20 iterations for the first 500
iterations, and every 100 iterations thereafter. Af-
ter a burn-in phase of 10,000 iterations, we collect
samples every 10 iterations (to lessen autocorrela-
tion) until a total of 100 have been collected.
4.2 Persona Regression
To incorporate observed metadata in the form of
movie genre, character age and character gen-
der, we adopt an ?upstream? modeling approach
(Mimno and McCallum, 2008), letting those ob-
served features influence the conditional probabil-
ity with which a given character is expected to as-
sume a particular persona, prior to observing any
of their actions. This captures the increased likeli-
hood, for example, that a 25-year-old male actor in
an action movie will play an ACTION HERO than
he will play a VALLEY GIRL.
To capture these effects, each character?s la-
tent persona is no longer drawn from a document-
specific Dirichlet; instead, the P -dimensional sim-
plex is the output of a multiclass logistic regres-
sion, where the document genre metadata md and
the character age and gender metadatame together
form a feature vector that combines with persona-
specific feature weights to form the following log-
linear distribution over personas, with the proba-
bility for persona k being:
P (p = k | md,me, ?) = exp([md;me]
>?k)
1+PP?1j=1 exp([md;me]>?j)(3)
The persona-specific ? coefficients are learned
through Monte Carlo Expectation Maximization
355
(Wei and Tanner, 1990), in which we alternate be-
tween the following:
1. Given current values for ?, for all characters
e in all plot summaries, sample values of pe
and zj for all associated tuples.
2. Given input metadata features m and the as-
sociated sampled values of p, find the values
of ? that maximize the standard multiclass lo-
gistic regression log likelihood, subject to `2
regularization.
Figure 2 (above right) illustrates this model. As
with the Dirichlet persona model, inference on p
for step 1 is conducted with collapsed Gibbs sam-
pling; the only difference in the sampling prob-
ability from equation 1 is the effect of the prior,
which here is deterministically fixed as the output
of the regression.
P (pe = k | p?e, z, ?,md,me, ?) ?
exp([md;me]>?k)?
?
j
(c?erj ,k,zj+?rj )
(c?erj ,k,?+K?rj )
(4)
The sampling equation for the topic assign-
ments z is identical to that in equation 2. In
practice we optimize ? every 1,000 iterations, un-
til a burn-in phase of 10,000 iterations has been
reached; at this point we following the same sam-
pling regime as for the Dirichlet persona model.
5 Evaluation
We evaluate our methods in two quantitative ways
by measuring the degree to which we recover two
different sets of gold-standard clusterings. This
evaluation also helps offer guidance for model se-
lection (in choosing the number of latent topics
and personas) by measuring performance on an
objective task.
5.1 Character Names
First, we consider all character names that occur in
at least two separate movies, generally as a conse-
quence of remakes or sequels; this includes proper
names such as ?Rocky Balboa,? ?Oliver Twist,?
and ?Indiana Jones,? as well as generic type names
such as ?Gang Member? and ?The Thief?; to mini-
mize ambiguity, we only consider character names
consisting of at least two tokens. Each of these
names is used by at least two different characters;
for example, a character named ?Jason Bourne?
is portrayed in The Bourne Identity, The Bourne
Supremacy, and The Bourne Ultimatum. While
these characters are certainly free to assume dif-
ferent roles in different movies, we believe that,
in the aggregate, they should tend to embody the
same character type and thus prove to be a natu-
ral clustering to recover. 970 character names oc-
cur at least twice in our data, and 2,666 individual
characters use one of those names. Let those 970
character names define 970 unique gold clusters
whose members include the individual characters
who use that name.
5.2 TV Tropes
As a second external measure of validation, we
consider a manually created clustering presented
at the website TV Tropes,10 a wiki that col-
lects user-submitted examples of common tropes
(narrative, character and plot devices) found in
television, film, and fiction, among other me-
dia. While TV Tropes contains a wide range of
such conventions, we manually identified a set of
72 tropes that could reasonably be labeled char-
acter types, including THE CORRUPT CORPO-
RATE EXECUTIVE, THE HARDBOILED DETEC-
TIVE, THE JERK JOCK, THE KLUTZ and THE
SURFER DUDE.
We manually aligned user-submitted examples
of characters embodying these 72 character types
with the canonical references in Freebase to cre-
ate a test set of 501 individual characters. While
the 72 character tropes represented here are a more
subjective measure, we expect to be able to at least
partially recover this clustering.
5.3 Variation of Information
To measure the similarity between the two clus-
terings of movie characters, gold clusters G and
induced latent persona clusters C, we calculate the
variation of information (Meila?, 2007):
V I(G, C) = H(G) +H(C)? 2I(G, C) (5)
= H(G|C) +H(C|G) (6)
VI measures the information-theoretic distance
between the two clusterings: a lower value means
greater similarity, and VI = 0 if they are iden-
tical. Low VI indicates that (induced) clusters
and (gold) clusters tend to overlap; i.e., knowing a
character?s (induced) cluster usually tells us their
(gold) cluster, and vice versa. Variation of infor-
mation is a metric (symmetric and obeys triangle
10http://tvtropes.org
356
Character Names ?5.1 TV Tropes ?5.2
K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 100
25 Persona regression 7.73 7.32 6.79 6.26 6.13 5.74Dirichlet persona 7.83 7.11 6.44 6.29 6.01 5.57
50 Persona regression 7.59 7.08 6.46 6.30 5.99 5.65Dirichlet persona 7.57 7.04 6.35 6.23 5.88 5.60
100 Persona regression 7.58 6.95 6.32 6.11 6.05 5.49Dirichlet persona 7.64 6.95 6.25 6.24 5.91 5.42
Table 1: Variation of information between learned personas and gold clusters for different numbers of
topics K and personas P . Lower values are better. All values are reported in bits.
Character Names ?5.1 TV Tropes ?5.2
K Model P = 25 P = 50 P = 100 P = 25 P = 50 P = 100
25 Persona regression 62.8 (?41%) 59.5 (?40%) 53.7 (?33%) 42.3 (?31%) 38.5 (?24%) 33.1 (?25%)Dirichlet persona 54.7 (?27%) 50.5 (?26%) 45.4 (?17%) 39.5 (?20%) 31.7 (?28%) 25.1 (?21%)
50 Persona regression 63.1 (?42%) 59.8 (?42%) 53.6 (?34%) 42.9 (?30%) 39.1 (?33%) 31.3 (?20%)Dirichlet persona 57.2 (?34%) 49.0 (?23%) 44.7 (?16%) 39.7 (?30%) 31.5 (?32%) 24.6 (?22%)
100 Persona regression 63.1 (?42%) 57.7 (?39%) 53.0 (?34%) 43.5 (?33%) 32.1 (?28%) 26.5 (?22%)Dirichlet persona 55.3 (?30%) 49.5 (?24%) 45.2 (?18%) 39.7 (?34%) 29.9 (?24%) 23.6 (?19%)
Table 2: Purity scores of recovering gold clusters. Higher values are better. Each absolute purity score
is paired with its improvement over a controlled baseline of permuting the learned labels while keeping
the cluster proportions the same.
inequality), and has a number of other desirable
properties.
Table 1 presents the VI between the learned per-
sona clusters and gold clusters, for varying num-
bers of personas (P = {25, 50, 100}) and top-
ics (K = {25, 50, 100}). To determine signifi-
cance with respect to a random baseline, we con-
duct a permutation test (Fisher, 1935; Pitman,
1937) in which we randomly shuffle the labels of
the learned persona clusters and count the num-
ber of times in 1,000 such trials that the VI of
the observed persona labels is lower than the VI
of the permuted labels; this defines a nonparamet-
ric p-value. All results presented are significant at
p < 0.001 (i.e. observed VI is never lower than
the simulation VI).
Over all tests in comparison to both gold clus-
terings, we see VI improve as both P and, to
a lesser extent, K increase. While this may be
expected as the number of personas increase to
match the number of distinct types in the gold
clusters (970 and 72, respectively), the fact that VI
improves as the number of latent topics increases
suggests that more fine-grained topics are helpful
for capturing nuanced character types.11
The difference between the persona regression
model and the Dirichlet persona model here is not
11This trend is robust to the choice of cluster metric: here
VI and F -score have a correlation of ?0.87; as more latent
topics and personas are added, clustering improves (causing
the F -score to go up and the VI distance to go down).
significant; while VI allows us to compare mod-
els with different numbers of latent clusters, its re-
quirement that clusterings be mutually informative
places a high overhead on models that are funda-
mentally unidirectional (in Table 1, for example,
the room for improvement between two models
of the same P and K is naturally smaller than
the bigger difference between different P or K).
While we would naturally prefer a text-only model
to be as expressive as a model that requires po-
tentially hard to acquire metadata, we tease apart
whether a distinction actually does exist by evalu-
ating the purity of the gold clusters with respect to
the labels assigned them.
5.4 Purity
For gold clusters G = {g1 . . . gk} and inferred
clusters C = {c1 . . . cj} we calculate purity as:
Purity = 1N
?
k
max
j
|gk ? cj | (7)
While purity cannot be used to compare models of
different persona size P , it can help us distinguish
between models of the same size. A model can
attain perfect purity, however, by placing all char-
acters into a single cluster; to control for this, we
present a controlled baseline in which each char-
acter is assigned a latent character type label pro-
portional to the size of the latent clusters we have
learned (so that, for example, if one latent per-
sona cluster contains 3.2% of the total characters,
357
Batman
Jim 
Gordon
dark, major, henchman
shoot, aim, overpower
sentence, arrest, assign
Tony 
Stark
Jason 
Bourne
The 
Joker
shoot, aim, overpower
testify, rebuff, confess
hatch, vow, undergo
Van Helsing
Colin 
Sullivan
Dracula
The Departed
The Dark 
Knight
Iron Man
The Bourne 
Identity
approve, die, suffer
relent, refuse, agree
inherit live imagine
Jack 
Dawson
Rachel
Titanic
Figure 3: Dramatis personae of The Dark Knight (2008), illustrating 3 of the 100 character types learned
by the persona regression model, along with links from other characters in those latent classes to other
movies. Each character type is listed with the top three latent topics with which it is associated.
the probability of selecting that persona at random
is 3.2%). Table 2 presents each model?s absolute
purity score paired with its improvement over its
controlled permutation (e.g., ?41%).
Within each fixed-size partition, the use of
metadata yields a substantial improvement over
the Dirichlet model, both in terms of absolute pu-
rity and in its relative improvement over its sized-
controlled baseline. In practice, we find that while
the Dirichlet model distinguishes between charac-
ter personas in different movies, the persona re-
gression model helps distinguish between differ-
ent personas within the same movie.
6 Exploratory Data Analysis
As with other generative approaches, latent per-
sona models enable exploratory data analysis. To
illustrate this, we present results from the persona
regression model learned above, with 50 latent
lexical classes and 100 latent personas. Figure 3
visualizes this data by focusing on a single movie,
The Dark Knight (2008); the movie?s protagonist,
Batman, belongs to the same latent persona as De-
tective Jim Gordon, as well as other action movie
protagonists Jason Bourne and Tony Stark (Iron
Man). The movie?s antagonist, The Joker, belongs
to the same latent persona as Dracula from Van
Helsing and Colin Sullivan from The Departed, il-
lustrating the ability of personas to be informed
by, but still cut across, different genres.
Table 3 presents an exhaustive list of all 50 top-
ics, along with an assigned label that consists of
the single word with the highest PMI for that class.
Of note are topics relating to romance (unite,
marry, woo, elope, court), commercial transac-
tions (purchase, sign, sell, owe, buy), and the clas-
sic criminal schema from Chambers (2011) (sen-
tence, arrest, assign, convict, promote).
Table 4 presents the most frequent 14 personas
in our dataset, illustrated with characters from
the 500 highest grossing movies. The personas
learned are each three separate mixtures of the
50 latent topics (one for agent relations, one for
patient relations, and one for attributes), as illus-
trated in figure 1 above. Rather than presenting
a 3 ? 50 histogram for each persona, we illus-
trate them by listing the most characteristic top-
ics, movie characters, and metadata features asso-
ciated with it. Characteristic actions and features
are defined as those having the highest smoothed
pointwise mutual information with that class; ex-
emplary characters are those with the highest pos-
terior probability of being drawn from that class.
Among the personas learned are canonical male
action heroes (exemplified by the protagonists of
The Bourne Supremacy, Speed, and Taken), super-
heroes (Hulk, Batman and Robin, Hector of Troy)
and several romantic comedy types, largely char-
acterized by words drawn from the FLIRT topic,
including flirt, reconcile, date, dance and forgive.
358
Label Most characteristic words Label Most characteristic words
UNITE unite marry woo elope court SWITCH switch confirm escort report instruct
PURCHASE purchase sign sell owe buy INFATUATE infatuate obsess acquaint revolve concern
SHOOT shoot aim overpower interrogate kill ALIEN alien child governor bandit priest
EXPLORE explore investigate uncover deduce CAPTURE capture corner transport imprison trap
WOMAN woman friend wife sister husband MAYA maya monster monk goon dragon
WITCH witch villager kid boy mom INHERIT inherit live imagine experience share
INVADE invade sail travel land explore TESTIFY testify rebuff confess admit deny
DEFEAT defeat destroy transform battle inject APPLY apply struggle earn graduate develop
CHASE chase scare hit punch eat EXPEL expel inspire humiliate bully grant
TALK talk tell reassure assure calm DIG dig take welcome sink revolve
POP pop lift crawl laugh shake COMMAND command abduct invade seize surrender
SING sing perform cast produce dance RELENT relent refuse agree insist hope
APPROVE approve die suffer forbid collapse EMBARK embark befriend enlist recall meet
WEREWOLF werewolf mother parent killer father MANIPULATE manipulate conclude investigate conduct
DINER diner grandfather brother terrorist ELOPE elope forget succumb pretend like
DECAPITATE decapitate bite impale strangle stalk FLEE flee escape swim hide manage
REPLY reply say mention answer shout BABY baby sheriff vampire knight spirit
DEMON demon narrator mayor duck crime BIND bind select belong refer represent
CONGRATULATE congratulate cheer thank recommend REJOIN rejoin fly recruit include disguise
INTRODUCE introduce bring mock read hatch DARK dark major henchman warrior sergeant
HATCH hatch don exist vow undergo SENTENCE sentence arrest assign convict promote
FLIRT flirt reconcile date dance forgive DISTURB disturb frighten confuse tease scare
ADOPT adopt raise bear punish feed RIP rip vanish crawl drive smash
FAIRY fairy kidnapper soul slave president INFILTRATE infiltrate deduce leap evade obtain
BUG bug zombie warden king princess SCREAM scream faint wake clean hear
Table 3: Latent topics learned for K = 50 and P = 100. The words shown for each class are those with
the highest smoothed PMI, with the label being the single word with the highest PMI.
Freq Actions Characters Features
0.109 DARKm, SHOOTa,
SHOOTp
Jason Bourne (The Bourne Supremacy), Jack Traven
(Speed), Jean-Claude (Taken)
Action, Male, War
film
0.079 CAPTUREp,
INFILTRATEa, FLEEa
Aang (The Last Airbender), Carly (Transformers: Dark of
the Moon), Susan Murphy/Ginormica (Monsters vs. Aliens)
Female, Action,
Adventure
0.067 DEFEATa, DEFEATp,
INFILTRATEa
Glenn Talbot (Hulk), Batman (Batman and Robin), Hector
(Troy)
Action, Animation,
Adventure
0.060 COMMANDa, DEFEATp,
CAPTUREp
Zoe Neville (I Am Legend), Ursula (The Little Mermaid),
Joker (Batman)
Action, Adventure,
Male
0.046 INFILTRATEa,
EXPLOREa, EMBARKa
Peter Parker (Spider-Man 3), Ethan Hunt (Mission:
Impossible), Jason Bourne (The Bourne Ultimatum)
Male, Action, Age
34-36
0.036 FLIRTa, FLIRTp,
TESTIFYa
Mark Darcy (Bridget Jones: The Edge of Reason), Jerry
Maguire (Jerry Maguire), Donna (Mamma Mia!)
Female, Romance
Film, Comedy
0.033 EMBARKa, INFILTRATEa,
INVADEa
Perseus (Wrath of the Titans), Maximus Decimus Meridius
(Gladiator), Julius (Twins)
Male, Chinese
Movies, Spy
0.027 CONGRATULATEa,
CONGRATULATEp,
SWITCHa
Professor Albus Dumbledore (Harry Potter and the
Philosopher?s Stone), Magic Mirror (Shrek), Josephine
Anwhistle (Lemony Snicket?s A Series of Unfortunate
Events)
Age 58+, Family
Film, Age 51-57
0.025 SWITCHa, SWITCHp,
MANIPULATEa
Clarice Starling (The Silence of the Lambs), Hannibal
Lecter (The Silence of the Lambs), Colonel Bagley (The
Last Samurai)
Age 58+, Male,
Age 45-50
0.022 REPLYa, TALKp, FLIRTp Graham (The Holiday), Abby Richter (The Ugly Truth),
Anna Scott (Notting Hill)
Female, Comedy,
Romance Film
0.020 EXPLOREa, EMBARKa,
CAPTUREp
Harry Potter (Harry Potter and the Philosopher?s Stone),
Harry Potter (Harry Potter and the Chamber of Secrets),
Captain Leo Davidson (Planet of the Apes)
Adventure, Family
Film, Horror
0.018 FAIRYm, COMMANDa,
CAPTUREp
Captain Jack Sparrow (Pirates of the Caribbean: At
World?s End), Shrek (Shrek), Shrek (Shrek Forever After)
Action, Family
Film, Animation
0.018 DECAPITATEa,
DECAPITATEp, RIPa
Jericho Cane (End of Days), Martin Riggs (Lethal Weapon
2), Gabriel Van Helsing (Van Helsing)
Horror, Slasher,
Teen
0.017 APPLYa, EXPELp,
PURCHASEp
Oscar (Shark Tale), Elizabeth Halsey (Bad Teacher), Dre
Parker (The Karate Kid)
Female, Teen,
Under Age 22
Table 4: Of 100 latent personas learned, we present the top 14 by frequency. Actions index the latent
topic classes presented in table 3; subscripts denote whether the character is predominantly the agent (a),
patient (p) or is modified by an attribute (m).
359
7 Conclusion
We present a method for automatically inferring
latent character personas from text (and metadata,
when available). While our testbed has been tex-
tual synopses of film, this approach is easily ex-
tended to other genres (such as novelistic fiction)
and to non-fictional domains as well, where the
choice of portraying a real-life person as embody-
ing a particular kind of persona may, for instance,
give insight into questions of media framing and
bias in newswire; self-presentation of individual
personas likewise has a long history in communi-
cation theory (Goffman, 1959) and may be use-
ful for inferring user types for personalization sys-
tems (El-Arini et al, 2012). While the goal of this
work has been to induce a set of latent character
classes and partition all characters among them,
one interesting question that remains is how a spe-
cific character?s actions may informatively be at
odds with their inferred persona, given the choice
of that persona as the single best fit to explain the
actions we observe. By examining how any indi-
vidual character deviates from the behavior indica-
tive of their type, we might be able to paint a more
nuanced picture of how a character can embody a
specific persona while resisting it at the same time.
Acknowledgments
We thank Megan Morrison at the CMU School of
Drama for early conversations guiding our work,
as well as the anonymous reviewers for helpful
comments. The research reported in this article
was supported by U.S. National Science Founda-
tion grant IIS-0915187 and by an ARCS scholar-
ship to D.B. This work was made possible through
the use of computing resources made available by
the Pittsburgh Supercomputing Center.
References
Aristotle. 335 BCE. Poetics, translated by Samuel H.
Butcher (1902). Macmillan, London.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Joseph Campbell. 1949. The Hero with a Thousand
Faces. Pantheon Books.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meet-
ing of the ACL.
Nathanael Chambers. 2011. Inducing Event Schemas
and their Participants from Unlabeled Text. Ph.D.
thesis, Stanford University.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Lajos Egri. 1946. The Art of Dramatic Writing. Simon
and Schuster, New York.
Khalid El-Arini, Ulrich Paquet, Ralf Herbrich, Jurgen
Van Gael, and Blaise Agu?era y Arcas. 2012. Trans-
parent user models for personalization. In Proceed-
ings of the 18th ACM SIGKDD.
Micha Elsner. 2012. Character-based kernels for nov-
elistic plot structure. In Proceedings of the 13th
Conference of the EACL.
Mark Alan Finlayson. 2011. Learning Narrative
Structure from Annotated Folktales. Ph.D. thesis,
MIT.
R. A. Fisher. 1935. The Design of Experiments. Oliver
and Boyde, Edinburgh and London.
Erving Goffman. 1959. The Presentation of the Self in
Everyday Life. Anchor.
Amit Goyal, Ellen Riloff, and Hal Daume?, III. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of the 2010 Con-
ference on EMNLP.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on EMNLP.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Carl Jung. 1981. The Archetypes and The Collective
Unconscious, volume 9 of Collected Works. Bollin-
gen, Princeton, NJ, 2nd edition.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the ACL.
Association for Computational Linguistics.
Robert McKee. 1997. Story: Substance, Structure,
Style and the Principles of Screenwriting. Harper-
Colllins.
Marina Meila?. 2007. Comparing clusterings?an in-
formation based distance. Journal of Multivariate
Analysis, 98(5):873?895.
David Mimno and Andrew McCallum. 2008. Topic
models conditioned on arbitrary features with
dirichlet-multinomial regression. In Proceedings of
UAI.
360
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st Annual Meeting of the ACL.
E. J. G. Pitman. 1937. Significance tests which may
be applied to samples from any population. Supple-
ment to the Journal of the Royal Statistical Society,
4(1):119?130.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the ACL.
Michaela Regneri, Alexander Koller, Josef Ruppen-
hofer, and Manfred Pinkal. 2011. Learning script
participants from unlabeled data. In Proceedings of
the Conference on Recent Advances in Natural Lan-
guage Processing.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals, and understanding: An inquiry into
human knowledge structures. Lawrence Erlbaum,
Hillsdale, NJ.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
EACL.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo implementation of the EM algorithm and the
poor man?s data augmentation algorithms. Journal
of the American Statistical Association, 85:699?704.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on EMNLP.
361
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1094?1104,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning to Extract International Relations from Political Context
Brendan O?Connor
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
brenocon@cs.cmu.edu
Brandon M. Stewart
Department of Government
Harvard University
Cambridge, MA 02139, USA
bstewart@fas.harvard.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We describe a new probabilistic model
for extracting events between major polit-
ical actors from news corpora. Our un-
supervised model brings together famil-
iar components in natural language pro-
cessing (like parsers and topic models)
with contextual political information?
temporal and dyad dependence?to in-
fer latent event classes. We quantita-
tively evaluate the model?s performance
on political science benchmarks: recover-
ing expert-assigned event class valences,
and detecting real-world conflict. We also
conduct a small case study based on our
model?s inferences.
A supplementary appendix, and replica-
tion software/data are available online, at:
http://brenocon.com/irevents
1 Introduction
The digitization of large news corpora has pro-
vided an unparalleled opportunity for the system-
atic study of international relations. Since the mid-
1960s political scientists have used political events
data, records of public micro-level interactions be-
tween major political actors of the form ?someone
does something to someone else? as reported in
the open press (Schrodt, 2012), to study the pat-
terns of interactions between political actors and
how they evolve over time. Scaling this data effort
to modern corpora presents an information extrac-
tion challenge: can a structured collection of ac-
curate, politically relevant events between major
political actors be extracted automatically and ef-
ficiently? And can they be grouped into meaning-
ful event types with a low-dimensional structure
useful for further analysis?
We present an unsupervised approach to event
extraction, in which political structure and linguis-
tic evidence are combined. A political context
model of the relationship between a pair of polit-
ical actors imposes a prior distribution over types
of linguistic events. Our probabilistic model in-
fers latent frames, each a distribution over textual
expressions of a kind of event, as well as a repre-
sentation of the relationship between each political
actor pair at each point in time. We use syntactic
preprocessing and a logistic normal topic model,
including latent temporal smoothing on the politi-
cal context prior.
We apply the model in a series of compar-
isons to benchmark datasets in political science.
First, we compare the automatically learned verb
classes to a pre-existing ontology and hand-crafted
verb patterns from TABARI,1 an open-source and
widely used rule-based event extraction system for
this domain. Second, we demonstrate correlation
to a database of real-world international conflict
events, the Militarized Interstate Dispute (MID)
dataset (Jones et al, 1996). Third, we qualitatively
examine a prominent case not included in the MID
dataset, Israeli-Palestinian relations, and compare
the recovered trends to the historical record.
We outline the data used for event discovery
(?2), describe our model (?3), inference (?4), eval-
uation (?5), and comment on related work (?6).
2 Data
The model we describe in ?3 is learned from a
corpus of 6.5 million newswire articles from the
English Gigaword 4th edition (1994?2008, Parker
et al, 2009). We also supplement it with a sam-
ple of data from the New York Times Annotated
Corpus (1987?2007, Sandhaus, 2008).2 The Stan-
1Available from the Penn State Event Data Project:
http://eventdata.psu.edu/
2For arbitrary reasons this portion of the data is much
smaller (we only parse the first five sentences of each arti-
cle, while Gigaword has all sentences parsed), resulting in
less than 2% as many tuples as from the Gigaword data.
1094
ford CoreNLP system,3 under default settings, was
used to POS-tag and parse the articles, to eventu-
ally produce event tuples of the form
?s, r, t, wpredpath?
where s and r denote ?source? and ?receiver? ar-
guments, which are political actor entities in a pre-
defined set E , t is a timestep (i.e., a 7-day pe-
riod) derived from the article?s published date, and
wpredpath is a textual predicate expressed as a de-
pendency path that typically includes a verb (we
use the terms ?predicate-path? and ?verb-path? in-
terchangeably). For example, on January 1, 2000,
the AP reported ?Pakistan promptly accused In-
dia,? from which our preprocessing extracts the tu-
ple ?PAK, IND, 678, accuse dobj??? ?. (The path ex-
cludes the first source-side arc.) Entities and verb
paths are identified through the following sets of
rules.
Named entity recognition and resolution is done
deterministically by finding instances of country
names from the CountryInfo.txt dictionary from
TABARI,4 which contains proper noun and adjec-
tival forms for countries and administrative units.
We supplement these with a few entries for in-
ternational organizations from another dictionary
provided by the same project, and clean up a few
ambiguous names, resulting in a final actor dictio-
nary of 235 entities and 2,500 names.
Whenever a name is found, we identify its en-
tity?s mention as the minimal noun phrase that
contains it; if the name is an adjectival or noun-
noun compound modifier, we traverse any such
amod and nn dependencies to the noun phrase
head. Thus NATO bombing, British view, and
Palestinian militant resolve to the entity codes IG-
ONAT, GBR, and PSE respectively.
We are interested in identifying actions initi-
ated by agents of one country targeted towards an-
other, and hence concentrate on verbs, analyzing
the ?CCprocessed? version of the Stanford Depen-
dencies (de Marneffe and Manning, 2008). Verb
paths are identified by looking at the shortest de-
pendency path between two mentions in a sen-
tence. If one of the mentions is immediately dom-
inated by a nsubj or agent relation, we consider
that the Source actor, and the other mention is the
Receiver. The most common cases are simple di-
rect objects and prepositional arguments like talk
3http://nlp.stanford.edu/software/
corenlp.shtml
4http://eventdata.psu.edu/software.
dir/dictionaries.html.
prep with????? and fight prep alongside??????? (?talk with R,? ?fight
alongside R?) but many interesting multiword con-
structions also result, such as reject dobj??? allegation
poss??? (?rejected R?s allegation?) or verb chains as
in offer xcomp??? help dobj??? (?offer to help R?).
We wish to focus on instances of directly re-
ported events, so attempt to remove factively com-
plicated cases such as indirect reporting and hy-
potheticals by discarding all predicate paths for
which any verb on the path has an off-path gov-
erning verb with a non-conj relation. (For exam-
ple, the verb at the root of a sentence always sur-
vives this filter.) Without this filter, the ?s, r, w?
tuple ?USA, CUB, want xcomp??? seize dobj??? ? is ex-
tracted from the sentence ?Parliament Speaker Ri-
cardo Alarcon said the United States wants to seize
Cuba and take over its lands?; the filter removes
it since wants is dominated by an off-path verb
through say ccomp??? wants. The filter was iteratively
developed by inspecting dozens of output exam-
ples and their labelings under successive changes
to the rules.
Finally, only paths length 4 or less are allowed,
the final dependency relation for the receiver may
not be nsubj or agent, and the path may not contain
any of the dependency relations conj, parataxis,
det, or dep. We use lemmatized word forms in
defining the paths.
Several document filters are applied before tu-
ple extraction. Deduplication removes 8.5% of ar-
ticles.5 For topic filtering, we apply a series of
keyword filters to remove sports and finance news,
and also apply a text classifier for diplomatic and
military news, trained on several hundred man-
ually labeled news articles (using `1-regularized
logistic regression with unigram and bigram fea-
tures). Other filters remove non-textual junk and
non-standard punctuation likely to cause parse er-
rors.
For experiments we remove tuples where the
source and receiver entities are the same, and re-
strict to tuples with dyads that occur at least 500
times, and predicate paths that occur at least 10
times. This yields 365,623 event tuples from
235,830 documents, for 421 dyads and 10,457
unique predicate paths. We define timesteps
to be 7-day periods, resulting in 1,149 discrete
5We use a simple form of shingling (ch. 3, Rajaraman and
Ullman, 2011): represent a document signature as its J = 5
lowercased bigrams with the lowest hash values, and reject a
document with a signature that has been seen before within
the same month. J was manually tuned, as it affects the pre-
cision/recall tradeoff.
1095
ik
?k,s,r,t
?s,r,t
z
 
wpredpath
r
L
a
n
g
u
a
g
e
 
M
o
d
e
l
P
(
T
e
x
t
 
 
|
 
 
E
v
e
n
t
 
T
y
p
e
)
C
o
n
t
e
x
t
 
M
o
d
e
l
P
(
E
v
e
n
t
 
T
y
p
e
 
 
|
 
 
C
o
n
t
e
x
t
)
b
 2k
ss "Source"
    entity
r "Receiver"
    entity
t  Timestep
i  Event tuple
k  Frame
kk
 k,s,r,t 1  k,s,r,t
?2
?k
Figure 1: Directed probabilistic diagram of the model for one
(s, r, t) dyad-time context, for the smoothed model.
timesteps (1987 through 2008, though the vast ma-
jority of data starts in 1994).
3 Model
We design two models to learn linguistic event
classes over predicate paths by conditioning on
real-world contextual information about interna-
tional politics, p(wpredpath | s, r, t), leveraging the
fact there tends to be dyadic and temporal coher-
ence in international relations: the types of actions
that are likely to occur between nations tend to be
similar within the same dyad, and usually their dis-
tribution changes smoothly over time.
Our model decomposes into two submodels:
a Context submodel, which encodes how politi-
cal context affects the probability distribution over
event types, and a Language submodel, for how
those events are manifested as textual predicate
paths (Figure 1). The overall generative process is
as follows. We color global parameters for a frame
blue, and local context parameters red, and use
the term ?frame? as a synonym for ?event type.?
The fixed hyperparameter K denotes the number
of frames.
? The context model generates a frame prior ?s,r,t
for every context (s, r, t).
? Language model:
? Draw lexical sparsity parameter b from a dif-
fuse prior (see ?4).
? For each frame k, draw a multinomial distri-
bution of dependency paths, ?k ? Dir(b/V )
(where V is the number of dependency path
types).
? For each (s, r, t), for every event tuple i in
that context,
? Sample its frame z(i) ? Mult(?s,r,t).
? Sample its predicate realization
w(i)predpath ? Mult(?z(i)).
Thus the language model is very similar to a topic
model?s generation of token topics and wordtypes.
We use structured logistic normal distributions
to represent contextual effects. The simplest is the
vanilla (V) context model,
? For each frame k, draw global parameters from
diffuse priors: prevalence ?k and variability ?2k.
? For each (s, r, t),
? Draw ?k,s,r,t ? N(?k, ?2k) for each frame k.
? Apply a softmax transform,
?k,s,r,t =
exp ?k,s,r,t?K
k?=1 exp ?k?,s,r,t
Thus the vector ??,s,r,t encodes the relative log-
odds of the different frames for events appearing
in the context (s, r, t). This simple logistic nor-
mal prior is, in terms of topic models, analogous
to the asymmetric Dirichlet prior version of LDA
in Wallach et al (2009), since the ?k parameter
can learn that some frames tend to be more likely
than others. The variance parameters ?2k controladmixture sparsity, and are analogous to a Dirich-
let?s concentration parameter.
Smoothing Frames Across Time
The vanilla model is capable of inducing frames
through dependency path co-occurences, when
multiple events occur in a given context. How-
ever, many dyad-time slices are very sparse; for
example, most dyads (all but 18) have events in
fewer than half the time slices in the dataset. One
solution is to increase the bucket size (e.g., to
months); however, previous work in political sci-
ence has demonstrated that answering questions
of interest about reciprocity dynamics requires re-
covering the events at weekly or even daily gran-
ularity (Shellman, 2004), and in any case wide
buckets help only so much for dyads with fewer
events or less media attention. Therefore we pro-
pose a smoothed frames (SF) model, in which the
1096
frame distribution for a given dyad comes from a
latent parameter ??,s,r,t that smoothly varies over
time. For each (s, r), draw the first timestep?s val-
ues as ?k,s,r,1 ? N(0, 100), and for each context
(s, r, t > 1),
? Draw ?k,s,r,t ? N(?k,s,r,t?1, ?2)
? Draw ?k,s,r,t ? N(?k + ?k,s,r,t, ?2k)
Other parameters (?k, ?2k) are same as the vanillamodel. This model assumes a random walk pro-
cess on ?, a variable which exists even for contexts
that contain no events. Thus inferences about ?
will be smoothed according to event data at nearby
timesteps. This is an instance of a linear Gaussian
state-space model (also known as a linear dynami-
cal system or dynamic linear model), and is a con-
venient formulation because it has well-known ex-
act inference algorithms. Dynamic linear models
have been used elsewhere in machine learning and
political science to allow latent topic frequencies
(Blei and Lafferty, 2006; Quinn et al, 2010) and
ideological positions (Martin and Quinn, 2002) to
smoothly change over time, and thus share statis-
tical strength between timesteps.
4 Inference
After randomly initializing all ?k,s,r,t, inference is
performed by a blocked Gibbs sampler, alternat-
ing resamplings for three major groups of vari-
ables: the language model (z,?), context model
(?, ?, ?, p), and the ?, ? variables, which bottle-
neck between the submodels.
The language model sampler sequentially up-
dates every z(i) (and implicitly ? via collapsing)
in the manner of Griffiths and Steyvers (2004):
p(z(i)|?, w(i), b) ? ?s,r,t,z(nw,z + b/V )/(nz + b),
where counts n are for all event tuples besides i.
For the context model, ? is conjugate resam-
pled as a normal mean. The random walk vari-
ables ? are sampled with the forward-filtering-
backward-sampling algorithm (FFBS; Harrison
and West, 1997; Carter and Kohn, 1994); there is
one slight modification of the standard dynamic
linear model that the zero-count weeks have no ?
observation; the Kalman filter implementation is
appropriately modified to handle this.
The ? update step is challenging since it is a
nonconjugate prior to the z counts. Logistic nor-
mal distributions were introduced to text mod-
eling by Blei and Lafferty (2007), who devel-
oped a variational approximation; however, we
find that experimenting with different models is
easier in the Gibbs sampling framework. While
Gibbs sampling for logistic normal priors is pos-
sible using auxiliary variable methods (Mimno
et al, 2008; Holmes and Held, 2006; Polson et al,
2012), it can be slow to converge. We opt for
the more computationally efficient approach of
Zeger and Karim (1991) and Hoff (2003), using
a Laplace approximation to p(? | ??,?, z), which
is a mode-centered Gaussian having inverse co-
variance equal to the unnormalized log-posterior?s
negative Hessian (?8.4 in Murphy, 2012). We find
the mode with the linear-time Newton algorithm
from Eisenstein et al (2011), and sample in linear
time by only using the Hessian?s diagonal as the
inverse covariance (i.e., an axis-aligned normal),
since a full multivariate normal sample requires
a cubic-time-to-compute Cholesky root of the co-
variance matrix. This ?? sample is a proposal for a
Metropolis-within-Gibbs step, which is moved to
according to the standard Metropolis-Hastings ac-
ceptance rule. Acceptance rates differ by K, rang-
ing approximately from 30% (K = 100) to nearly
100% (small K).
Finally, we use diffuse priors on all global pa-
rameters, conjugate resampling variances ?2, ?k
once per iteration, and slice sampling (Neal, 2003)
the Dirichlet concentration b every 100 iterations.
Automatically learning these was extremely con-
venient for model-fitting; the only hyperparameter
we set manually wasK. It also allowed us to mon-
itor the convergence of dispersion parameters to
help debug and assess MCMC mixing. For other
modeling and implementation details, see the on-
line appendix and software.
5 Experiments
We fit the two models on the dataset described in
?2, varying the number of frames K, with 8 or
more separate runs for each setting. Posteriors are
saved and averaged from 11 Gibbs samples (every
100 iterations from 9,000 to 10,000) for analysis.
We present intrinsic (?5.1) and extrinsic (?5.2)
quantitative evaluations, and a qualitative case
study (?5.4).
5.1 Lexical Scale Impurity
In the international relations literature, much of
the analysis of text-based events data makes use of
a unidimensional conflict to cooperation scale. A
popular event ontology in this domain, CAMEO,
consists of around 300 different event types, each
1097
given an expert-assigned scale in the range from
?10 to +10 (Gerner et al, 2002), derived from
a judgement collection experiment in Goldstein
(1992). The TABARI pattern-based event extrac-
tion program comes with a list of almost 16,000
manually engineered verb patterns, each assigned
to one CAMEO event type.
It is interesting to consider the extent to which
our unsupervised model is able to recover the
expert-designed ontology. Given that many of
the categories are very fine-grained (e.g. ?Express
intent to de-escalate military engagement?), we
elect to measure model quality as lexical scale pu-
rity: whether all the predicate paths within one
automatically learned frame tend to have similar
gold-standard scale scores. (This measures clus-
ter cohesiveness against a one-dimensional con-
tinuous scale, instead of measuring cluster cohe-
siveness against a gold-standard clustering as in
VI, Rand index, or purity.) To calculate this, we
construct a mapping between our corpus-derived
verb path vocabulary and the TABARI verb pat-
terns, many of which contain one to several word
stems that are intended to be matched in surface
order. Many of our dependency paths, when tra-
versed from the source to receiver direction, also
follow surface order, due to English?s SVO word
order.6 Therefore we convert each path to a
word sequence and match against the TABARI
lexicon?plus a few modifications for differences
in infinitives and stemming?and find 528 depen-
dency path matches. We assign each path w a
gold-standard scale g(w) by resolving through its
matching pattern?s CAMEO code.
We formalize lexical scale impurity as the av-
erage absolute difference of scale values between
two predicate paths under the same frame. Specif-
ically, we want a token-level posterior expectation
E(|g(wi)? g(wj)| | zi = zj , wi 6= wj) (1)
which is taken over pairs of path instances (i, j)
where both paths wi, wj are in M , the set of verb
paths that were matched between the lexicons.
This can be reformulated at the type level as:7
1
N
?
k
?
w,v?M
w 6=v
nw,k nv,k |g(w)? g(v)| (2)
6There are plenty of exceptions where a Source-to-
Receiver path traversal can have a right-to-left move, such
as dependency edges for posessives. This approach can not
match them.
7Derivation in supplementary appendix.
where n refers to the averaged Gibbs samples?
counts of event tuples having frame k and a par-
ticular verb path,8 and N is the number of to-
ken comparisons (i.e. the same sum, but with a
1 replacing the distance). The worst possible im-
purity is upper bounded at 20 (= max(g(w)) ?
min(g(w))) and the best possible is 0. We also
compute a randomized null hypothesis to see how
low impurity can be by chance: each of ?1000
simulations randomly assigns each path in M to
one of K frames (all its instances are exclusively
assigned to that frame), and computes the impu-
rity. On average the impurity is same at all K,
but variance increases with K (since small clus-
ters might by chance get a highly similar paths in
them), necessitating this null hypothesis analysis.
We report the 5th percentile over simulations.
5.2 Conflict Detection
Political events data has shown considerable
promise as a tool for crisis early warning systems
(O?Brien, 2010; Brandt et al, 2011). While con-
flict forecasting is a potential application of our
model, we conduct a simpler prediction task to
validate whether the model is learning something
useful: based on news text, tell whether or not an
armed conflict is currently happening. For a gold
standard, we use the Militarized Interstate Dispute
(MID) dataset (Jones et al, 1996; Ghosn et al,
2004), which documents historical international
disputes. While not without critics, the MID data
is the most prominent dataset in the field of in-
ternational relations. We use the Dyadic MIDs,
each of which ranks hostility levels between pairs
of actors on a five point scale over a date inter-
val; we define conflict to be the top two categories
?Use of Force? (4) and ?War? (5). We convert
the data into a variable ys,r,t, the highest hostility
level reached by actor s directed towards receiver
r in the dispute that overlaps with our 7-day in-
terval t, and want to predict the binary indicator
1{ys,r,t ? 4}. For the illustrative examples (USA
to Iraq, and the Israel-Palestine example below)
we use results from a smaller but more internally
comparable dataset consisting of the 2 million As-
sociated Press articles within the Gigaword cor-
pus.
For an example of the MID data, see Figure 2,
which depicts three disputes between the US and
8Results are nearly identical whether we use counts av-
eraged across samples (thus giving posterior marginals),
or simply use counts from a single sample (i.e., iteration
10,000).
1098
kill, fire at, 
seal, invade, 
enter
accuse, 
criticize, warn, 
reject, urge
accuse, 
reject, blame, 
kill, take
criticize, call, 
ask, condemn, 
denounce
USA to Iraq (Vanilla Model)
0.
0
0.
4
0.
8
1995 1996 1997 1998 1999 2000 2001 2002
USA to Iraq (Smoothed Frames)
0.
0
0.
4
0.
8
1995 1996 1997 1998 1999 2000 2001 2002
Figure 2: The USA?Iraq directed dyad, analyzed by
smoothed (above) and vanilla (below) models, showing (1)
gold-standard MID values (red intervals along top), (2) weeks
with non-zero event counts (vertical lines along x-axis), (3)
posterior E[?k,USA,IRQ,t] inferences for two frames chosen
from two different K = 5 models, and (4) most common
verb paths for each frame (right). Frames corresponding to
material and verbal conflict were chosen for display. Vertical
line indicates Operation Desert Fox (see ?5.2).
Iraq in this time period. The MID labels are
marked in red.
The first dispute is a ?display of force? (level
3), cataloguing the U.S. response to a series of
troop movements along the border with Kuwait.
The third dispute (10/7/1997 to 10/10/2001) be-
gins with increasing Iraqi violations of the no-
fly zone, resulting in U.S. and U.K. retaliation,
reaching a high intensity with Operation Desert
Fox, a four-day bombing campaign from Decem-
ber 16 to 19, 1998?which is not shown in MID.
These cases highlight MID?s limitations?while it
is well regarded in the political science literature,
its coarse level of aggregation can fail to capture
variation in conflict intensity.
Figure 2 also shows model inferences. Our
smoothed model captures some of these phenom-
ena here, showing clear trends for two relevant
frames, including a dramatic change in Decem-
ber 1998. The vanilla model has a harder time,
since it cannot combine evidence between differ-
ent timesteps.
The MID dataset overlaps with our data for 470
weeks, from 1993 through 2001. After excluding
dyads with actors that the MID data does not in-
tend to include?Kosovo, Tibet, Palestine, and in-
ternational organizations?we have 267 directed
dyads for evaluation, 117 of which have at least
one dispute in the MID data. (Dyads with no dis-
pute in the MID data, such as Germany-France,
are assumed to have y = 0 throughout the time
period.) About 7% of the dyad-time contexts have
a dispute under these definitions.
We split the dataset by time, training on the first
half of the data and testing on the second half, and
measure area under the receiver operating charac-
teristic curve (AUC).9 For each model, we train an
`1-regularized logistic regression10 with the K el-
ements of ??,s,r,t as input features, tuning the reg-
ularization parameter within the training set (by
splitting it in half again) to optimize held-out like-
lihood. We weight instances to balance positive
and negative examples. Training is on all individ-
ual ? samples at once (thus accounting for pos-
terior uncertainty in learning), and final predicted
probabilities are averaged from individual proba-
bilities from each ? test set sample, thus propa-
gating posterior uncertainty into the predictions.
We also create a baseline `1-regularized logistic
regression that uses normalized dependency path
counts as the features (10,457 features). For both
the baseline and vanilla model, contexts with no
events are given a feature vector of all zeros.11
(We also explored an alternative evaluation setup,
to hold out by dyad; however, the performance
variance is quite high between different random
dyad splits.)
5.3 Results
Results are shown in Figure 3.12
The verb-path logistic regression performs
strongly at AUC 0.62; it outperforms all of
the vanilla frame models. This is an exam-
ple of individual lexical features outperforming a
topic model for predictive task, because the topic
model?s dimension reduction obscures important
indicators from individual words. Similarly, Ger-
rish and Blei (2011) found that word-based regres-
sion outperformed a customized topic model when
predicting Congressional bill passage, and Eisen-
9AUC can be interpreted as follows: given a positive and
negative example, what is the probability that the classifier?s
confidences order them correctly? Random noise or predict-
ing all the same class both give AUC 0.5.
10Using the R glmnet package (Friedman et al, 2010).
11For the vanilla model, this performed better than linear
interpolation (about 0.03 AUC), and with less variance be-
tween runs.
12Due to an implementation bug, the model put the vast
majority of the probability mass only on K ? 1 frames,
so these settings might be better thought of as K =
1, 2, 3, 4, 9, . . .; see the appendix for details.
1099
l l
0.4
0.5
0.6
0.7
2 3 4 5 10 20 50 100Number of frames (K)
Co
nfli
ct p
red
icti
on 
AU
C
(hig
her 
is b
ette
r)
model
l Log. Reg
Vanilla
Smoothed
l l l l
l
l
l
l
1.5
2.5
3.5
4.5
5.5
2 3 4 5 10 20 50 100Number of frames (K)
Sca
le i
mp
urit
y
(low
er
 is 
bet
ter
)
model
l Null
Vanilla
Smoothed
Figure 3: Evaluation results. Each point indicates one model
run. Lines show the average per K, with vertical lines indi-
cating the 95% bootstrapped interval. Top: Conflict detection
AUC for different models (?5.2). Green line is the verb-path
logistic regression baseline. Bottom: Lexical scale impurity
(?5.1). Top green line indicates the simple random baseline
E(|g(wi) ? g(wj)|) = 5.33; the second green line is from
the random assignment baseline.
stein et al (2010) found word-based regression
outperformed Supervised LDA for geolocation,13
and we have noticed this phenomenon for other
text-based prediction problems.
However, adding smoothing to the model sub-
stantially increases performance, and in fact out-
performs the verb-path regression at K = 100.
It is unclear why the vanilla model fails to in-
crease performance in K. Note also, the vanilla
model exhibits very little variability in prediction
performance between model runs, in comparison
to the smoothed model which is much more vari-
able (presumably due to the higher number of pa-
rameters in the model); at small values of K, the
smoothed model can perform poorly. It would also
be interesting to analyze the smoothed model with
higher values of K and find where it peaks.
We view the conflict detection task only as one
of several validations, and thus turn to lexical eval-
uation of the induced frames. For lexical scale
purity (bottom of Figure 3), the models perform
about the same, with the smoothed model a lit-
tle bit worse at some values of K (though some-
times with better stability of the fits?opposite of
the conflict detection task). This suggests that se-
mantic coherence does not benefit from the longer-
13In the latter, a problem-specific topic model did best.
range temporal dependencies.
In general, performance improves with higher
K, but not beyond K = 50. This suggests the
model reaches a limit for how fine-grained of se-
mantics it can learn.
5.4 Case study
Here we qualitatively examine the narrative story
between the dyad with the highest frequency of
events in our dataset, the Israeli-Palestinian rela-
tionship, finding qualitative agreement with other
case studies of this conflict (Brandt et al, 2012;
Goldstein et al, 2001; Schrodt and Gerner, 2004).
(The MID dataset does not include this conflict be-
cause the Palestinians are not considered a state
actor.) Using the Associated Press subset, we plot
the highest incidence frames from one run of the
K = 20 smoothed frame models, for the two di-
rected dyads, and highlight some of the interesting
relationships.
Figure 4(a) shows that tradeoffs in the use of
military vs. police action by Israel towards the
Palestinians tracks with major historical events.
The first period in the data where police actions
(?impose, seal, capture, seize, arrest?) exceed mil-
itary actions (?kill, fire, enter, attack, raid?) is
with the signing of the ?Interim Agreement on the
West Bank and the Gaza Strip,? also known as the
Oslo II agreement. This balance persists until the
abrupt breakdown in relations that followed the
unsuccessful Camp David Summit in July of 2000,
which generally marks the starting point of the
wave of violence known as the Second Intifada.
In Figure 4(b) we show that our model produces
a frame which captures the legal aftermath of par-
ticular events (?accuse, criticize,? but also ?detain,
release, extradite, charge?). Each of the major
spikes in the data coincides with a particular event
which either involves the investigation of a par-
ticular attack or series of attacks (as in A,B,E) or
a discussion about prisoner swaps or mass arrests
(as in events D, F, J).
Our model also picks up positive diplomatic
events, as seen in Figure 4(c), a frame describ-
ing Israeli diplomatic actions towards Palestine
(?meet with, sign with, praise, say with, arrive
in?). Not only do the spikes coincide with major
peace treaties and negotiations, but the model cor-
rectly characterizes the relative lack of positively
valenced action from the beginning of the Second
Intifada until its end around 2005?2006.
In Figure 4(d) we show the relevant frames de-
1100
a.
0.0
0.4
0.8
Israeli Use of Force Tradeoff
1994 1997 2000 2002 2005 2007
Second Intifada BeginsOslo II Signed
b.
0.
0
0.
4
0.
8
1.
2
Police Actions and Crime Response
A
B
C
D
E
F
G
H I J
1994 1997 2000 2002 2005 2007
A: Series of Suicide Attacks 
in Jerusalem
B: Island of Peace Massacre
C: Arrests over Protests
D: Tensions over Treatment 
of Pal. Prisoners
E: Passover Massacre
F: 400-Person Prisoner Swap
G: Gaza Street Bus Bombing
H: Stage Club Bombing
I: House to House Sweep for 7 
militant leaders
J: Major Prisoner Release
c.
0.
0
0.
4
0.
8
Israeli?Palestinian Diplomacy
A B C D E F
1994 1997 2000 2002 2005 2007
C: U.S. Calls for West Bank 
Withdrawal
D: Deadlines for Wye River Peace 
Accord
E: Negotiations in Mecca
F: Annapolis Conference
A: Israel-Jordan Peace 
Treaty
B: Hebron Protocol
d.
0.0
0.4
0.8
Palestinian Use of Force
1994 1997 2000 2002 2005 2007
Figure 4: For Israel-Palestinian directed dyads, plots ofE[?] (proportion of weekly events in a frame) over time, annotated with
historical events. (a): Words are ?kill, fire at, enter, kill, attack, raid, strike, move, pound, bomb? and ?impose, seal, capture,
seize, arrest, ease, close, deport, close, release? (b): ?accuse, criticize, reject, tell, hand to, warn, ask, detain, release, order? (c):
?meet with, sign with, praise, say with, arrive in, host, tell, welcome, join, thank? (d): again the same ?kill, fire at? frame in (a),
plus the erroneous frame (see text) ?include, join, fly to, have relation with, protest to, call, include bomber appos???? informer
for?. Figures (b) and (c) use linear interpolation for zero-count weeks (thus relying exclusively on the model for smoothing);
(a) and (d) apply a lowess smoother. (a-c) are for the ISR?PSE direction; (d) is PSE?ISR.
picting use of force from the Palestinians towards
the Israelis (brown trend line). At first, the drop
in the use of force frame immediately following
the start of the Second Intifada seems inconsis-
tent with the historical record. However, there is a
concucrrent rise in a different frame driven by the
word ?include?, which actually appears here due
to an NLP error compounded with an artifact of
the data source. A casualties report article, con-
taining variants of the text ?The Palestinian fig-
ure includes... 13 Israeli Arabs...?, is repeated 27
times over two years. ?Palestinian figure? is er-
roneously identified as the PSE entity, and several
noun phrases in a list are identified as separate re-
ceivers. This issue causes 39 of all 86 PSE?ISR
events during this period to use the word ?in-
clude?, accounting for the rise in that frame. (This
highlights how better natural language processing
could help the model, and the dangers of false
positives for this type of data analysis, especially
in small-sample drilldowns.) Discounting this er-
roneous inference, the results are consistent with
heightened violence during this period.
We conclude the frame extractions for the
Israeli-Palestinian case are consistent with the his-
torical record over the period of study.
6 Related Work
6.1 Events Data in Political Science
Projects using hand-collected events data repre-
sent some of the earliest efforts in the statisti-
cal study of international relations, dating back to
the 1960s (Rummel, 1968; Azar and Sloan, 1975;
McClelland, 1970). Beginning in the mid-1980s,
political scientists began experimenting with au-
tomated rule-based extraction systems (Schrodt
and Gerner, 1994). These efforts culminated in
the open-source program, TABARI, which uses
pattern matching from extensive hand-developed
phrase dictionaries, combined with basic part of
speech tagging (Schrodt, 2001); a rough analogue
in the information extraction literature might be
the rule-based, finite-state FASTUS system for
MUC IE (Hobbs et al, 1997), though TABARI is
restricted to single sentence analysis. Later pro-
prietary work has apparently incorporated more
extensive NLP (e.g., sentence parsing) though
few details are available (King and Lowe, 2003).
The most recent published work we know of, by
Boschee et al (2013), uses a proprietary parsing
and coreference system (BBN SERIF, Ramshaw
et al, 2011), and directly compares to TABARI,
finding significantly higher accuracy. The origi-
1101
nal TABARI system is still actively being devel-
oped, including just-released work on a new 200
million event dataset, GDELT (Schrodt and Lee-
taru, 2013).14 All these systems crucially rely on
hand-built pattern dictionaries.
It is extremely labor intensive to develop these
dictionaries. Schrodt (2006) estimates 4,000
trained person-hours were required to create dic-
tionaries of political actors in the Middle East, and
the phrase dictionary took dramatically longer; the
comments in TABARI?s phrase dictionary indicate
some of its 15,789 entries were created as early as
1991. Ideally, any new events data solution would
incorporate the extensive work already completed
by political scientists in this area while minimiz-
ing the need for further dictionary development. In
this work we use the actor dictionaries, and hope
to incorporate the verb patterns in future work.
6.2 Events in Natural Language Processing
Political event extraction from news has also re-
ceived considerable attention within natural lan-
guage processing in part due to government-
funded challenges such as MUC-3 and MUC-4
(Lehnert, 1994), which focused on the extraction
of terrorist events, as well as the more recent
ACE program. The work in this paper is inspired
by unsupervised approaches that seek to discover
types of relations and events, instead of assuming
them to be pre-specified; this includes research un-
der various headings such as template/frame/event
learning (Cheung et al, 2013; Modi et al, 2012;
Chambers and Jurafsky, 2011; Li et al, 2010; Be-
jan, 2008), script learning (Regneri et al, 2010;
Chambers and Jurafsky, 2009), relation learning
(Yao et al, 2011), open information extraction
(Banko et al, 2007; Carlson et al, 2010), verb
caseframe learning (Rooth et al, 1999; Gildea,
2002; Grenager and Manning, 2006; Lang and La-
pata, 2010; O? Se?aghdha, 2010; Titov and Klemen-
tiev, 2012), and a version of frame learning called
?unsupervised semantic parsing? (Titov and Kle-
mentiev, 2011; Poon and Domingos, 2009). Un-
like much of the previous literature, we do not
learn latent roles/slots. Event extraction is also
a large literature, including supervised systems
targeting problems similar to MUC and political
events (Piskorski and Atkinson, 2011; Piskorski
et al, 2011; Sanfilippo et al, 2008).
One can also see this work as a relational ex-
14http://eventdata.psu.edu/data.dir/
GDELT.html
tension of co-occurence-based methods such as
Gerrish (2013; ch. 4), Diesner and Carley (2005),
Chang et al (2009), or Newman et al (2006),
which perform bag-of-words-style analysis of text
fragments containing co-occurring entities. (Ger-
rish also analyzed the international relations do-
main, using supervised bag-of-words regression
to assess the expressed valence between a pair
of actors in a news paragraph, using the predic-
tions as observations in a latent temporal model,
and compared to MID.) We instead use parsing to
get a much more focused and interpretable repre-
sentation of the relationship between textually co-
occurring entities; namely, that they are the source
and target of an action event. This is more in line
with work in relation extraction on biomedical sci-
entific articles (Friedman et al, 2001; Rzhetsky
et al, 2004) which uses parsing to extracting a net-
work of how different entities, like drugs or pro-
teins, interact.
7 Conclusion
Large-scale information extraction can dramati-
cally enhance the study of political behavior. Here
we present a novel unsupervised approach to an
important data collection effort in the social sci-
ences. We see international relations as a rich
and practically useful domain for the development
of text analysis methods that jointly infer events,
relations, and sociopolitical context. There are
numerous areas for future work, such as: using
verb dictionaries as semi-supervised seeds or pri-
ors; interactive learning between political science
researchers and unsupervised algorithms; build-
ing low-dimensional scaling, or hierarchical struc-
ture, into the model; and learning the actor lists
to handle changing real-world situations and new
domains. In particular, adding more supervision
to the model will be crucial to improve semantic
quality and make it useful for researchers.
Acknowledgments
Thanks to Justin Betteridge for providing the parsed Giga-
word corpus, Erin Baggott for help in developing the doc-
ument filter, and the anonymous reviewers for helpful com-
ments. This research was supported in part by NSF grant IIS-
1211277, and was made possible through the use of comput-
ing resources made available by the Pittsburgh Supercomput-
ing Center. Brandon Stewart gratefully acknowledges fund-
ing from an NSF Graduate Research Fellowship.
References
Azar, E. E. and Sloan, T. (1975). Dimensions of interactions.
Technical report, University Center of International Stud-
ies, University of Pittsburgh, Pittsburgh.
1102
Banko, M., Cafarella, M. J., Soderland, S., Broadhead, M.,
and Etzioni, O. (2007). Open Information Extraction from
the Web. IJCAI.
Bejan, C. A. (2008). Unsupervised discovery of event sce-
narios from texts. In Proceedings of the 21st Florida Arti-
ficial Intelligence Research Society International Confer-
ence (FLAIRS), Coconut Grove, FL, USA.
Blei, D. M. and Lafferty, J. D. (2006). Dynamic topic models.
In Proceedings of ICML.
Blei, D. M. and Lafferty, J. D. (2007). A correlated topic
model of science. Annals of Applied Statistics, 1(1), 17?
35.
Boschee, E., Natarajan, P., and Weischedel, R. (2013). Au-
tomatic extraction of events from open source text for
predictive forecasting. Handbook of Computational Ap-
proaches to Counterterrorism, page 51.
Brandt, P. T., Freeman, J. R., and Schrodt, P. A. (2011). Real
time, time series forecasting of inter-and intra-state po-
litical conflict. Conflict Management and Peace Science,
28(1), 41?64.
Brandt, P. T., Freeman, J. R., Lin, T.-m., and Schrodt, P. A.
(2012). A Bayesian time series approach to the compari-
son of conflict dynamics. In APSA 2012 Annual Meeting
Paper.
Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hruschka,
E. R., and Mitchell, T. M. (2010). Toward an architecture
for never-ending language learning. In Proceedings of the
Conference on Artificial Intelligence (AAAI), pages 1306?
1313.
Carter, C. K. and Kohn, R. (1994). On Gibbs sampling for
state space models. Biometrika, 81(3), 541?553.
Chambers, N. and Jurafsky, D. (2009). Unsupervised learn-
ing of narrative schemas and their participants. In Pro-
ceedings of ACL-IJCNLP. Association for Computational
Linguistics.
Chambers, N. and Jurafsky, D. (2011). Template-based infor-
mation extraction without the templates. In Proceedings
of ACL.
Chang, J., Boyd-Graber, J., and Blei, D. M. (2009). Con-
nections between the lines: augmenting social networks
with text. In Proceedings of the 15th ACM SIGKDD in-
ternational conference on Knowledge discovery and data
mining, pages 169?178. ACM.
Cheung, J. C. K., Poon, H., and Vanderwende, L. (2013).
Probabilistic frame induction. In Proceedings of NAACL.
arXiv preprint arXiv:1302.4813.
de Marneffe, M.-C. and Manning, C. D. (2008). Stanford
typed dependencies manual. Technical report, Stanford
University.
Diesner, J. and Carley, K. M. (2005). Revealing social
structure from texts: meta-matrix text analysis as a novel
method for network text analysis. In Causal mapping for
information systems and technology research, pages 81?
108. Harrisburg, PA: Idea Group Publishing.
Eisenstein, J., O?Connor, B., Smith, N. A., and Xing, E. P.
(2010). A latent variable model for geographic lexical
variation. In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing, pages
1277?1287.
Eisenstein, J., Ahmed, A., and Xing, E. (2011). Sparse ad-
ditive generative models of text. In Proceedings of ICML,
pages 1041?1048.
Friedman, C., Kra, P., Yu, H., Krauthammer, M., and Rzhet-
sky, A. (2001). GENIES: a natural-language process-
ing system for the extraction of molecular pathways from
journal articles. Bioinformatics, 17(suppl 1), S74?S82.
Friedman, J., Hastie, T., and Tibshirani, R. (2010). Regular-
ization paths for generalized linear models via coordinate
descent. Journal of Statistical Software, 33(1).
Gerner, D. J., Schrodt, P. A., Yilmaz, O., and Abu-Jabr, R.
(2002). The Creation of CAMEO (Conflict and Media-
tion Event Observations): An Event Data Framework for
a Post Cold War World. Annual Meeting of the American
Political Science Association.
Gerrish, S. M. (2013). Applications of Latent Variable Mod-
els in Modeling Influence and Decision Making. Ph.D.
thesis, Princeton University.
Gerrish, S. M. and Blei, D. M. (2011). Predicting legislative
roll calls from text. In Proceedings of ICML.
Ghosn, F., Palmer, G., and Bremer, S. A. (2004). The MID3
data set, 1993?2001: Procedures, coding rules, and de-
scription. Conflict Management and Peace Science, 21(2),
133?154.
Gildea, D. (2002). Probabilistic models of verb-argument
structure. In Proceedings of COLING.
Goldstein, J. S. (1992). A conflict-cooperation scale for
WEIS events data. Journal of Conflict Resolution, 36,
369?385.
Goldstein, J. S., Pevehouse, J. C., Gerner, D. J., and Telhami,
S. (2001). Reciprocity, triangularity, and cooperation in
the middle east, 1979-97. Journal of Conflict Resolution,
45(5), 594?620.
Grenager, T. and Manning, C. D. (2006). Unsupervised dis-
covery of a statistical verb lexicon. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, page 18.
Griffiths, T. L. and Steyvers, M. (2004). Finding scientific
topics. PNAS, 101(suppl. 1), 5228?5235.
Harrison, J. and West, M. (1997). Bayesian forecasting and
dynamic models. Springer Verlag, New York.
Hobbs, J. R., Appelt, D., Bear, J., Israel, D., Kameyama,
M., Stickel, M., and Tyson, M. (1997). FASTUS: A
cascaded finite-state transducer for extracting information
from natural-language text. Finite-State Language Pro-
cessing, page 383.
Hoff, P. D. (2003). Nonparametric modeling of hierarchi-
cally exchangeable data. University of Washington Statis-
tics Department, Technical Report, 421.
Holmes, C. C. and Held, L. (2006). Bayesian auxiliary
variable models for binary and multinomial regression.
Bayesian Analysis, 1(1), 145?168.
Jones, D., Bremer, S., and Singer, J. (1996). Militarized in-
terstate disputes, 1816?1992: Rationale, coding rules, and
empirical patterns. Conflict Management and Peace Sci-
ence, 15(2), 163?213.
King, G. and Lowe, W. (2003). An automated information
extraction tool for international conflict data with perfor-
mance as good as human coders: A rare events evaluation
design. International Organization, 57(3), 617?642.
Lang, J. and Lapata, M. (2010). Unsupervised induction of
semantic roles. In Human Language Technologies: The
2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages
939?947. Association for Computational Linguistics.
Lehnert, W. G. (1994). Cognition, computers, and car bombs:
How Yale prepared me for the 1990s. In Beliefs, Reason-
ing, and Decision-Making. Psycho-Logic in Honor of Bob
Abelson, pages 143?173, Hillsdale, NJ, Hove, UK. Erl-
baum. http://ciir.cs.umass.edu/pubfiles/
cognition3.pdf.
Li, H., Li, X., Ji, H., and Marton, Y. (2010). Domain-
independent novel event discovery and semi-automatic
1103
event annotation. In Proceedings of the 24th Pacific Asia
Conference on Language, Information and Computation,
Sendai, Japan, November.
Martin, A. D. and Quinn, K. M. (2002). Dynamic ideal point
estimation via Markov chain Monte Carlo for the U.S.
Supreme Court, 1953?1999. Political Analysis, 10(2),
134?153.
McClelland, C. (1970). Some effects on theory from the in-
ternational event analysis movement. Mimeo, University
of Southern California.
Mimno, D., Wallach, H., and McCallum, A. (2008). Gibbs
sampling for logistic normal topic models with graph-
based priors. In NIPS Workshop on Analyzing Graphs.
Modi, A., Titov, I., and Klementiev, A. (2012). Unsuper-
vised induction of frame-semantic representations. In Pro-
ceedings of the NAACL-HLT Workshop on the Induction of
Linguistic Structure, pages 1?7. Association for Computa-
tional Linguistics.
Murphy, K. P. (2012). Machine Learning: a Probabilistic
Perspective. MIT Press.
Neal, R. M. (2003). Slice sampling. Annals of Statistics,
pages 705?741.
Newman, D., Chemudugunta, C., and Smyth, P. (2006). Sta-
tistical entity-topic models. In Proceedings of the 12th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 680?686. ACM.
O? Se?aghdha, D. (2010). Latent variable models of selectional
preference. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, pages
435?444. Association for Computational Linguistics.
O?Brien, S. P. (2010). Crisis early warning and decision sup-
port: Contemporary approaches and thoughts on future re-
search. International Studies Review, 12(1), 87?104.
Parker, R., Graff, D., Kong, J., Chen, K., and Maeda, K.
(2009). English Gigaword Fourth Edition. Linguistic Data
Consortium. LDC2009T13.
Piskorski, J. and Atkinson, M. (2011). Frontex real-time
news event extraction framework. In Proceedings of the
17th ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 749?752. ACM.
Piskorski, J., Tanev, H., Atkinson, M., van der Goot, E., and
Zavarella, V. (2011). Online news event extraction for
global crisis surveillance. Transactions on computational
collective intelligence V , pages 182?212.
Polson, N. G., Scott, J. G., and Windle, J. (2012). Bayesian
inference for logistic models using Polya-Gamma latent
variables. arXiv preprint arXiv:1205.0310.
Poon, H. and Domingos, P. (2009). Unsupervised semantic
parsing. In Proceedings of EMNLP, pages 1?10. Associa-
tion for Computational Linguistics.
Quinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H.,
and Radev, D. R. (2010). How to analyze political atten-
tion with minimal assumptions and costs. American Jour-
nal of Political Science, 54(1), 209228.
Rajaraman, A. and Ullman, J. D. (2011). Mining of mas-
sive datasets. Cambridge University Press; http://
infolab.stanford.edu/?ullman/mmds.html.
Ramshaw, L., Boschee, E., Freedman, M., MacBride, J.,
Weischedel, R., , and Zamanian, A. (2011). SERIF lan-
guage processing effective trainable language understand-
ing. Handbook of Natural Language Processing and Ma-
chine Translation, pages 636?644.
Regneri, M., Koller, A., and Pinkal, M. (2010). Learning
script knowledge with web experiments. In Proceedings
of the 48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ?10, pages 979?988.
Rooth, M., Riezler, S., Prescher, D., Carroll, G., and Beil,
F. (1999). Inducing a semantically annotated lexicon via
EM-based clustering. In Proceedings of the 37th annual
meeting of the Association for Computational Linguistics
on Computational Linguistics, page 104111.
Rummel, R. (1968). The Dimensionality of Nations project.
Rzhetsky, A., Iossifov, I., Koike, T., Krauthammer, M., Kra,
P., Morris, M., Yu, H., Duboue?, P. A., Weng, W., Wilbur,
W. J., Hatzivassiloglou, V., and Friedman, C. (2004).
GeneWays: a system for extracting, analyzing, visualiz-
ing, and integrating molecular pathway data. Journal of
Biomedical Informatics, 37(1), 43?53.
Sandhaus, E. (2008). The New York Times Annotated Cor-
pus. Linguistic Data Consortium. LDC2008T19.
Sanfilippo, A., Franklin, L., Tratz, S., Danielson, G., Mile-
son, N., Riensche, R., and McGrath, L. (2008). Automat-
ing frame analysis. Social computing, behavioral model-
ing, and prediction, pages 239?248.
Schrodt, P. (2012). Precedents, progress, and prospects in po-
litical event data. International Interactions, 38(4), 546?
569.
Schrodt, P. and Leetaru, K. (2013). GDELT: Global data
on events, location and tone, 1979-2012. In International
Studies Association Conference.
Schrodt, P. A. (2001). Automated coding of international
event data using sparse parsing techniques. International
Studies Association Conference.
Schrodt, P. A. (2006). Twenty Years of the Kansas Event
Data System Project. Political Methodologist.
Schrodt, P. A. and Gerner, D. J. (1994). Validity assessment
of a machine-coded event data set for the Middle East,
1982-1992. American Journal of Political Science.
Schrodt, P. A. and Gerner, D. J. (2004). An event data analy-
sis of third-party mediation in the middle east and balkans.
Journal of Conflict Resolution, 48(3), 310?330.
Shellman, S. M. (2004). Time series intervals and statistical
inference: The effects of temporal aggregation on event
data analysis. Political Analysis, 12(1), 97?104.
Titov, I. and Klementiev, A. (2011). A Bayesian model for
unsupervised semantic parsing. In Proceedings of ACL.
Titov, I. and Klementiev, A. (2012). A Bayesian approach
to unsupervised semantic role induction. Proceedings of
EACL.
Wallach, H., Mimno, D., and McCallum, A. (2009). Rethink-
ing lda: Why priors matter. Advances in Neural Informa-
tion Processing Systems, 22, 1973?1981.
Yao, L., Haghighi, A., Riedel, S., and McCallum, A. (2011).
Structured relation discovery using generative models. In
Proceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1456?1466. Associ-
ation for Computational Linguistics.
Zeger, S. L. and Karim, M. R. (1991). Generalized linear
models with random effects; a Gibbs sampling approach.
Journal of the American Statistical Association, 86(413),
79?86.
1104
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617?622,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers
Andre? F. T. Martins?? Miguel B. Almeida?? Noah A. Smith#
?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, 1049-001 Lisboa, Portugal
#School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
{atm,mba}@priberam.pt, nasmith@cs.cmu.edu
Abstract
We present fast, accurate, direct non-
projective dependency parsers with third-
order features. Our approach uses AD3,
an accelerated dual decomposition algo-
rithm which we extend to handle special-
ized head automata and sequential head
bigram models. Experiments in fourteen
languages yield parsing speeds competi-
tive to projective parsers, with state-of-
the-art accuracies for the largest datasets
(English, Czech, and German).
1 Introduction
Dependency parsing has become a prominent ap-
proach to syntax in the last few years, with in-
creasingly fast and accurate models being devised
(Ku?bler et al, 2009; Huang and Sagae, 2010;
Zhang and Nivre, 2011; Rush and Petrov, 2012).
In projective parsing, the arcs in the dependency
tree are constrained to be nested, and the problem
of finding the best tree can be addressed with dy-
namic programming. This results in cubic-time
decoders for arc-factored and sibling second-order
models (Eisner, 1996; McDonald and Pereira,
2006), and quartic-time for grandparent models
(Carreras, 2007) and third-order models (Koo and
Collins, 2010). Recently, Rush and Petrov (2012)
trained third-order parsers with vine pruning cas-
cades, achieving runtimes only a small factor
slower than first-order systems. Third-order fea-
tures have also been included in transition systems
(Zhang and Nivre, 2011) and graph-based parsers
with cube-pruning (Zhang and McDonald, 2012).
Unfortunately, non-projective dependency
parsers (appropriate for languages with a more
flexible word order, such as Czech, Dutch, and
German) lag behind these recent advances. The
main obstacle is that non-projective parsing is
NP-hard beyond arc-factored models (McDonald
and Satta, 2007). Approximate parsers have there-
fore been introduced, based on belief propagation
(Smith and Eisner, 2008), dual decomposition
(Koo et al, 2010), or multi-commodity flows
(Martins et al, 2009, 2011). These are all in-
stances of turbo parsers, as shown by Martins et
al. (2010): the underlying approximations come
from the fact that they run global inference in
factor graphs ignoring loop effects. While this
line of research has led to accuracy gains, none of
these parsers use third-order contexts, and their
speeds are well behind those of projective parsers.
This paper bridges the gap above by presenting
the following contributions:
? We apply the third-order feature models of Koo
and Collins (2010) to non-projective parsing.
? This extension is non-trivial since exact dy-
namic programming is not applicable. Instead,
we adapt AD3, the dual decomposition algo-
rithm proposed by Martins et al (2011), to han-
dle third-order features, by introducing special-
ized head automata.
? We make our parser substantially faster than the
many-components approach of Martins et al
(2011). While AD3 requires solving quadratic
subproblems as an intermediate step, recent re-
sults (Martins et al, 2012) show that they can be
addressed with the same oracles used in the sub-
gradient method (Koo et al, 2010). This enables
AD3 to exploit combinatorial subproblems like
the the head automata above.
Along with this paper, we provide a free distribu-
tion of our parsers, including training code.1
2 Dependency Parsing with AD3
Dual decomposition is a class of optimization
techniques that tackle the dual of combinatorial
1Released as TurboParser 2.1, and publicly available at
http://www.ark.cs.cmu.edu/TurboParser.
617
Figure 1: Parts considered in this paper. First-
order models factor over arcs (Eisner, 1996; Mc-
Donald et al, 2005), and second-order models in-
clude also consecutive siblings and grandparents
(Carreras, 2007). Our parsers add also arbitrary
siblings (not necessarily consecutive) and head bi-
grams, as in Martins et al (2011), in addition
to third-order features for grand- and tri-siblings
(Koo and Collins, 2010).
problems in a modular and extensible manner (Ko-
modakis et al, 2007; Rush et al, 2010). In this
paper, we employ alternating directions dual de-
composition (AD3; Martins et al, 2011). Like
the subgradient algorithm of Rush et al (2010),
AD3 splits the original problem into local sub-
problems, and seeks an agreement on the over-
lapping variables. The difference is that the AD3
subproblems have an additional quadratic term to
accelerate consensus. Recent analysis (Martins et
al., 2012) has shown that: (i) AD3 converges at
a faster rate,2 and (ii) the quadratic subproblems
can be solved using the same combinatorial ma-
chinery that is used in the subgradient algorithm.
This opens the door for larger subproblems (such
as the combination of trees and head automata in
Koo et al, 2010) instead of a many-components
approach (Martins et al, 2011), while still enjoy-
ing faster convergence.
2.1 Our Setup
Given a sentence with L words, to which we
prepend a root symbol $, let A := {?h,m? | h ?
{0, . . . , L}, m ? {1, . . . , L}, h 6= m} be the
set of possible dependency arcs. We parame-
terize a dependency tree via an indicator vector
u := ?ua?a?A, where ua is 1 if the arc a is in the
tree, and 0 otherwise, and we denote by Y ? R|A|
the set of such vectors that are indicators of well-
2Concretely, AD3 needs O(1/) iterations to converge to
a -accurate solution, while subgradient needs O(1/2).
formed trees. Let {As}Ss=1 be a cover of A, where
each As ? A. We assume that the score of a parse
tree u ? Y decomposes as f(u) :=?Ss=1 fs(zs),
where each zs := ?zs,a?a?As is a ?partial view? of
u, and each local score function fs comes from a
feature-based linear model.
Past work in dependency parsing considered ei-
ther (i) a few ?large? components, such as trees
and head automata (Smith and Eisner, 2008; Koo
et al, 2010), or (ii) many ?small? components,
coming from a multi-commodity flow formulation
(Martins et al, 2009, 2011). Let Ys ? R|As| de-
note the set of feasible realizations of zs, i.e., those
that are partial views of an actual parse tree. A tu-
ple of views ?z1, . . . ,zS? ??Ss=1 Ys is said to be
globally consistent if zs,a = zs?,a holds for every
a, s and s? such that a ? As?As? . We assume each
parse u ? Y corresponds uniquely to a globally
consistent tuple of views, and vice-versa. Follow-
ing Martins et al (2011), the problem of obtaining
the best-scored tree can be written as follows:
maximize ?Ss=1 fs(zs)
w.r.t. u ? R|A|, zs ? Ys, ?s
s.t. zs,a = ua, ?s, ?a ? As, (1)
where the equality constraint ensures that the par-
tial views ?glue? together to form a coherent parse
tree.3
2.2 Dual Decomposition and AD3
Dual decomposition methods dualize out the
equality constraint in Eq. 1 by introducing La-
grange multipliers ?s,a. In doing so, they solve a
relaxation where the combinatorial sets Ys are re-
placed by their convex hulls Zs := conv(Ys).4 All
that is necessary is the following assumption:
Assumption 1 (Local-Max Oracle). Every s ?
{1, . . . , S} has an oracle that solves efficiently any
instance of the following subproblem:
maximize fs(zs) +
?
a?As ?s,azs,a
w.r.t. zs ? Ys. (2)
Typically, Assumption 1 is met whenever the max-
imization of fs over Ys is tractable, since the ob-
jective in Eq. 2 just adds a linear function to fs.
3Note that any tuple ?z1, . . . , zS? ? ?Ss=1 Ys satisfyingthe equality constraints will be globally consistent; this fact,
due the assumptions above, will imply u ? Y.
4Let ?|Ys| := {? ? R|Ys| |? ? 0, ?ys?Ys ?ys = 1}be the probability simplex. The convex hull of Ys is the set
conv(Ys) := {
?
ys?Ys ?ysys | ? ? ?
|Ys|}. Its members
represent marginal probabilities over the arcs in As.
618
The AD3 algorithm (Martins et al, 2011) alter-
nates among the following iterative updates:
? z-updates, which decouple over s = 1, . . . , S,
and solve a penalized version of Eq. 2:
z(t+1)s := argmax
zs?Zs
fs(zs) +
?
a?As ?
(t)
s,azs,a
??2
?
a?As(zs,a ? u
(t)
a )2. (3)
Above, ? is a constant and the quadratic term
penalizes deviations from the current global so-
lution (stored in u(t)).5 We will see (Prop. 2)
that this problem can be solved iteratively using
only the Local-Max Oracle (Eq. 2).
? u-updates, a simple averaging operation:
u(t+1)a := 1|{s : a?As}|
?
s : a?As z
(t+1)
s,a . (4)
? ?-updates, where the Lagrange multipliers are
adjusted to penalize disagreements:
?(t+1)s,a := ?(t)s,a ? ?(z(t+1)s,a ? u(t+1)a ). (5)
In sum, the only difference between AD3 and
the subgradient method is in the z-updates, which
in AD3 require solving a quadratic problem.
While closed-form solutions have been developed
for some specialized components (Martins et al,
2011), this problem is in general more difficult
than the one arising in the subgradient algorithm.
However, the following result, proved in Martins
et al (2012), allows to expand the scope of AD3
to any problem which satisfies Assumption 1.
Proposition 2. The problem in Eq. 3 admits a
solution z?s which is spanned by a sparse basis
W ? Ys with cardinality at most |W| ? O(|As|).
In other words, there is a distribution ? with sup-
port in W such that z?s =
?
ys?W ?ysys.6
Prop. 2 has motivated an active set alorithm
(Martins et al, 2012) that maintains an estimate
of W by iteratively adding and removing elements
computed through the oracle in Eq. 2.7 Typically,
very few iterations are necessary and great speed-
ups are achieved by warm-starting W with the ac-
tive set computed in the previous AD3 iteration.
This has a huge impact in practice and is crucial to
obtain the fast runtimes in ?4 (see Fig. 2).
5In our experiments (?4), we set ? = 0.05.
6Note that |Ys| = O(2|As|) in general. What Prop. 2
tells us is that the solution of Eq. 3 can be represented as a
distribution over Ys with a very sparse support.
7The algorithm is a specialization of Nocedal and Wright
(1999), ?16.4, which effectively exploits the sparse represen-
tation of z?s . For details, see Martins et al (2012).
0 10 20 30 40 50sentence length (words)0.00
0.10
0.20
avera
geru
ntime
(sec.) AD3Subgrad.
Figure 2: Comparison between AD3 and subgra-
dient. We show averaged runtimes in PTB ?22 as
a function of the sentence length. For subgradi-
ent, we chose for each sentence the most favorable
stepsize in {0.001, 0.01, 0.1, 1}.
3 Solving the Subproblems
We next describe the actual components used in
our third-order parsers.
Tree component. We use an arc-factored score
function (McDonald et al, 2005): f TREE(z) =?L
m=1 ?ARC(pi(m),m), where pi(m) is the parent
of the mth word according to the parse tree z,
and ?ARC(h,m) is the score of an individual arc.
The parse tree that maximizes this function can be
found in time O(L3) via the Chu-Liu-Edmonds?
algorithm (Chu and Liu, 1965; Edmonds, 1967).8
Grand-sibling head automata. Let Ainh and
Aouth denote respectively the sets of incoming and
outgoing candidate arcs for the hth word, where
the latter subdivides into arcs pointing to the right,
Aouth,?, and to the left, Aouth,?. Define the sets
AGSIBh,? = Ainh ?Aouth,? andAGSIBh,? = Ainh ?Aouth,?. We
describe right-side grand-sibling head automata;
their left-side counterparts are analogous. For
each head word h in the parse tree z, define
g := pi(h), and let ?m0,m1, . . . ,mp+1? be the se-
quence of right modifiers of h, with m0 = START
and mp+1 = END. Then, we have the following
grand-sibling component:
fGSIBh,? (z|AGSIBh,?) =
?p+1
k=1
(
?SIB(h,mk?1,mk)
?GP(g, h,mk) + ?GSIB(g, h,mk?1,mk)
)
,
where we use the shorthand z|B to denote the
subvector of z indexed by the arcs in B ? A.
Note that this score function absorbs grandparent
and consecutive sibling scores, in addition to the
grand-sibling scores.9 For each h, fGSIBh,? can be
8In fact, there is an asymptotically fasterO(L2) algorithm
(Tarjan, 1977). Moreover, if the set of possible arcs is reduced
to a subset B ? A (via pruning), then the fastest known al-
gorithm (Gabow et al, 1986) runs in O(|B|+L logL) time.
9Koo et al (2010) used an identical automaton for their
second-order model, but leaving out the grand-sibling scores.
619
No pruning |Ainm| ? K same, + |Aouth | ? J
TREE O(L2) O(KL+ L logL) O(KL+ L logL)
GSIB O(L4) O(K2L2) O(JK2L)
TSIB O(L4) O(KL3) O(J2KL)
SEQ O(L3) O(K2L) O(K2L)
ASIB O(L3) O(KL2) O(JKL)
Table 1: Theoretical runtimes of each subproblem
without pruning, limiting the number of candidate
heads, and limiting (in addition) the number of
modifiers. Note the O(L logL) total runtime per
AD3 iteration in the latter case.
maximized in time O(L3) with dynamic program-
ming, yielding O(L4) total runtime.
Tri-sibling head automata. In addition, we de-
fine left and right-side tri-sibling head automata
that remember the previous two modifiers of a
head word. This corresponds to the following
component function (for the right-side case):
f TSIBh,? (z|Aouth,?) =
?p+1
k=2 ?TSIB(h,mk?2,mk?1,mk).
Again, each of these functions can be maximized
in time O(L3), yielding O(L4) runtime.
Sequential head bigram model. Head bigrams
can be captured with a simple sequence model:
f SEQ(z) =
?L
m=2 ?HB(m,pi(m), pi(m? 1)).
Each score ?HB(m,h, h?) is obtained via features
that look at the heads of consecutive words (as in
Martins et al (2011)). This function can be maxi-
mized in time O(L3) with the Viterbi algorithm.
Arbitrary siblings. We handle arbitrary siblings
as in Martins et al (2011), definingO(L3) compo-
nent functions of the form fASIBh,m,s(z?h,m?, z?h,s?) =
?ASIB(h,m, s). In this case, the quadratic problem
in Eq. 3 can be solved directly in constant time.
Tab. 1 details the time complexities of each sub-
problem. Without pruning, each iteration of AD3
has O(L4) runtime. With a simple strategy that
limits the number of candidate heads per word to
a constant K, this drops to cubic time.10 Further
speed-ups are possible with more pruning: by lim-
iting the number of possible modifiers to a con-
stant J , the runtime would reduce to O(L logL).
10In our experiments, we employed this strategy withK =
10, by pruning with a first-order probabilistic model. Fol-
lowing Koo and Collins (2010), for each word m, we also
pruned away incoming arcs ?h,m? with posterior probability
less than 0.0001 times the probability of the most likely head.
UAS Tok/sec
PTB-YM ?22, 1st ord 91.38 4,063
PTB-YM ?22, 2nd ord 93.15 1,338
PTB-YM ?22, 2nd ord, +ASIB, +HB 93.28 1,018
PTB-YM ?22, 3rd ord 93.29 709
PTB-YM ?22, 3rd ord, gold tags 94.01 722
This work (PTB-YM ?23, 3rd ord) 93.07 735
Koo et al (2010) 92.46 112?
Huang and Sagae (2010) 92.1? 587?
Zhang and Nivre (2011) 92.9? 680?
Martins et al (2011) 92.53 66?
Zhang and McDonald (2012) 93.06 220
This work (PTB-S ?23, 3rd ord) 92.82 604
Rush and Petrov (2012) 92.7? 4,460
Table 2: Results for the projective English dataset.
We report unlabeled attachment scores (UAS) ig-
noring punctuation, and parsing speeds in tokens
per second. Our speeds include the time necessary
for pruning, evaluating features, and decoding, as
measured on a Intel Core i7 processor @3.4 GHz.
The others are speeds reported in the cited papers;
those marked with ? were converted from times per
sentence.
4 Experiments
We first evaluated our non-projective parser in a
projective English dataset, to see how its speed and
accuracy compares with recent projective parsers,
which can take advantage of dynamic program-
ming. To this end, we converted the Penn Tree-
bank to dependencies through (i) the head rules
of Yamada and Matsumoto (2003) (PTB-YM) and
(ii) basic dependencies from the Stanford parser
2.0.5 (PTB-S).11 We trained by running 10 epochs
of cost-augmented MIRA (Crammer et al, 2006).
To ensure valid parse trees at test time, we rounded
fractional solutions as in Martins et al (2009)?
yet, solutions were integral ? 95% of the time.
Tab. 2 shows the results in the dev-set (top
block) and in the test-set (two bottom blocks). In
the dev-set, we see consistent gains when more ex-
pressive features are added, the best accuracies be-
ing achieved with the full third-order model; this
comes at the cost of a 6-fold drop in runtime com-
pared with a first-order model. By looking at the
two bottom blocks, we observe that our parser
has slightly better accuracies than recent projec-
tive parsers, with comparable speed levels (with
the exception of the highly optimized vine cascade
approach of Rush and Petrov, 2012).
11We train on sections ?02?21, use ?22 as validation data,
and test on ?23. We trained a simple 2nd-order tagger with
10-fold jackknifing to obtain automatic part-of-speech tags
for ?22?23, with accuracies 97.2% and 96.9%, respectively.
620
First Ord. Sec. Ord. Third Ord. Best published UAS RP12 ZM12
UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS
Arabic 77.23 2,481 78.50 388 79.64 197 81.12 - Ma11 - - -
Bulgarian 91.76 5,678 92.82 2,049 93.10 1,273 93.50 - Ma11 91.9 3,980 93.08
Chinese 88.49 18,094 90.14 4,284 89.98 2,592 91.89 - Ma10 90.9 7,800 -
Czech 87.66 1,840 90.00 751 90.32 501 89.46 - Ma11 - - -
Danish 89.42 4,110 91.20 1,053 91.48 650 91.86 - Ma11 - -
Dutch 83.61 3,884 86.37 1,294 86.19 599 85.81 121 Ko10 - - -
German 90.52 5,331 91.85 1,788 92.41 965 91.89 - Ma11 90.8 2,880 91.35
English 91.21 3,127 93.03 1,317 93.22 785 92.68 - Ma11 - - -
Japanese 92.78 23,895 93.14 5,660 93.52 2,996 93.72 - Ma11 92.3 8,600 93.24
Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69
Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - -
Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48
Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44
Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - -
Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008.
?Best Published UAS? includes the most accurate parsers among Nivre et al (2006), McDonald et al
(2006), Martins et al (2010, 2011), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald
(2012). The last two are shown separately in the rightmost columns.
In our second experiment (Tab. 3), we used 14
datasets, most of which are non-projective, from
the CoNLL 2006 and 2008 shared tasks (Buch-
holz and Marsi, 2006; Surdeanu et al, 2008).
Our third-order model achieved the best reported
scores for English, Czech, German, and Dutch?
which includes the three largest datasets and the
ones with the most non-projective dependencies?
and is on par with the state of the art for the
remaining languages. To our knowledge, the
speeds are the highest reported among higher-
order non-projective parsers, and only about 3?
4 times slower than the vine parser of Rush and
Petrov (2012), which has lower accuracies.
5 Conclusions
We presented new third-order non-projective
parsers which are both fast and accurate. We de-
coded with AD3, an accelerated dual decomposi-
tion algorithm which we adapted to handle large
components, including specialized head automata
for the third-order features, and a sequence model
for head bigrams. Results are above the state of
the art for large datasets and non-projective lan-
guages. In the hope that other researchers may find
our implementation useful or are willing to con-
tribute with further improvements, we made our
parsers publicly available as open source software.
Acknowledgments
We thank all reviewers for their insightful com-
ments and Lingpeng Kong for help in converting
the Penn Treebank to Stanford dependencies. This
work was partially supported by the EU/FEDER
programme, QREN/POR Lisboa (Portugal), under
the Intelligo project (contract 2012/24803), by a
FCT grant PTDC/EEI-SII/2312/2012, and by NSF
grant IIS-1054319.
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Inter-
national Conference on Natural Language Learn-
ing.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In International Con-
ference on Natural Language Learning.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc.
of International Conference on Computational Lin-
guistics, pages 340?345.
H. N. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan.
1986. Efficient algorithms for finding minimum
spanning trees in undirected and directed graphs.
Combinatorica, 6(2):109?122.
621
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of An-
nual Meeting of the Association for Computational
Linguistics, pages 1077?1086.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition:
Message-passing revisited. In Proc. of International
Conference on Computer Vision.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of Annual Meeting of the
Association for Computational Linguistics, pages 1?
11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and
D. Sontag. 2010. Dual decomposition for parsing
with non-projective head automata. In Proc. of Em-
pirical Methods for Natural Language Processing.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Morgan & Claypool Publishers.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proc. of Annual Meeting
of the Association for Computational Linguistics.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo
parsers: Dependency parsing by approximate vari-
ational inference. In Proc. of Empirical Methods for
Natural Language Processing.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Dual decomposition
with many overlapping components. In Proc. of Em-
pirical Methods for Natural Language Processing.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2012. Alternat-
ing directions dual decomposition. Arxiv preprint
arXiv:1212.6550.
R. T. McDonald and F. C. N. Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of Annual Meeting of the European
Chapter of the Association for Computational Lin-
guistics.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of International Conference on Parsing
Technologies.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proc. of Empirical
Methods for Natural Language Processing.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of International Confer-
ence on Natural Language Learning.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Procs. of In-
ternational Conference on Natural Language Learn-
ing.
J. Nocedal and S. J. Wright. 1999. Numerical opti-
mization. Springer-Verlag.
Alexander M Rush and Slav Petrov. 2012. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proc. of Conference of the North American Chapter
of the Association for Computational Linguistics.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming re-
laxations for natural language processing. In Proc.
of Empirical Methods for Natural Language Pro-
cessing.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of Empirical Methods
for Natural Language Processing.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,
and J. Nivre. 2008. The CoNLL-2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. Proc. of International Conference on Natural
Language Learning.
R.E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proc. of International Conference on Parsing Tech-
nologies.
H. Zhang and R. McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of Empirical Methods in Natural Language
Processing.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
of the Annual Meeting of the Association for Com-
putational Linguistics.
622
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 370?379,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Bayesian Mixed Effects Model of Literary Character
David Bamman
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dbamman@cs.cmu.edu
Ted Underwood
Department of English
University of Illinois
Urbana, IL 61801, USA
tunder@illinois.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of automatically
inferring latent character types in a collec-
tion of 15,099 English novels published
between 1700 and 1899. Unlike prior
work in which character types are assumed
responsible for probabilistically generat-
ing all text associated with a character,
we introduce a model that employs mul-
tiple effects to account for the influence
of extra-linguistic information (such as au-
thor). In an empirical evaluation, we find
that this method leads to improved agree-
ment with the preregistered judgments of a
literary scholar, complementing the results
of alternative models.
1 Introduction
Recent work in NLP has begun to exploit the
potential of entity-centric modeling for a vari-
ety of tasks: Chambers (2013) places entities at
the center of probabilistic frame induction, show-
ing gains over a comparable event-centric model
(Cheung et al, 2013); Bamman et al (2013) ex-
plicitly learn character types (or ?personas?) in a
dataset of Wikipedia movie plot summaries; and
entity-centric models form one dominant approach
in coreference resolution (Durrett et al, 2013;
Haghighi and Klein, 2010).
One commonality among all of these very dif-
ferent probabilistic approaches is that each learns
statistical regularities about how entities are de-
picted in text (whether for the sake of learning
a set of semantic roles, character types, or link-
ing anaphora to the entities to which they refer).
In each case, the text we observe associated with
an entity in a document is directly dependent on
the class of entity?and only that class. This re-
lationship between entity and text is a theoreti-
cal assumption, with important consequences for
learning: entity types learned in this way will
be increasingly similar the more similar the do-
main, author, and other extra-linguistic effects are
between them.
1
While in many cases the topi-
cally similar types learned under this assumption
may be desirable, we explore here the alterna-
tive, in which entity types are learned in a way
that controls for such effects. In introducing a
model based on different assumptions, we provide
a method that complements past work and pro-
vides researchers with more flexible tools to infer
different kinds of character types.
We focus here on the literary domain, exploring
a large collection of 15,099 English novels pub-
lished in the 18th and 19th centuries. By account-
ing for the influence of individual authors while in-
ferring latent character types, we are able to learn
personas that cut across different authors more ef-
fectively than if we learned types conditioned on
the text alone. Modeling the language used to de-
scribe a character as the joint result of that charac-
ter?s latent type and of other formal variables al-
lows us to test multiple models of character and
assess their value for different interpretive prob-
lems. As a test case, we focus on separating char-
acter from authorial diction, but this approach can
readily be generalized to produce models that pro-
visionally distinguish character from other factors
(such as period, genre, or point of view) as well.
2 Literary Background
Inferring character is challenging from a liter-
ary perspective partly because scholars have not
reached consensus about the meaning of the term.
It may seem obvious that a ?character? is a repre-
sentation of a (real or imagined) person, and many
humanists do use the term that way. But there is
1
For example, many entities in Early Modern English
texts may be judged to be more similar to each other than
to entities from later texts simply by virtue of using hath and
other archaic verb forms.
370
an equally strong critical tradition that treats char-
acter as a formal dimension of narrative. To de-
scribe a character as a ?blocking figure? or ?first-
person narrator,? for instance, is a statement less
about the attributes of an imagined person than
about a narrative function (Keen, 2003). Charac-
ters are in one sense collections of psychological
or moral attributes, but in another sense ?word-
masses? (Forster, 1927). This tension between
?referential? and ?formalist? models of character
has been a centrally ?divisive question in . . . liter-
ary theory? (Woloch, 2003).
Considering primary source texts (as distinct
from plot summaries) forces us to confront new
theoretical questions about character. In a plot
summary (such as those explored by Bamman et
al., 2013), a human reader may already have used
implicit models of character to extract high-level
features. To infer character types from raw narra-
tive text, researchers need to explicitly model the
relationship of character to narrative form. This is
not a solved problem, even for human readers.
For instance, it has frequently been remarked
that the characters of Charles Dickens share
certain similarities?including a reliance on tag
phrases and recurring tics. A referential model
of character might try to distinguish this common
stylistic element from underlying ?personalities.?
A strictly formalist model might refuse to separate
authorial diction from character at all. In prac-
tice, human readers can adopt either perspective:
we recognize that characters have a ?Dickensian?
quality but also recognize that a Dickens villain is
(in one sense) more like villains in other authors
than like a Dickensian philanthropist. Our goal is
to show that computational methods can support
the same range of perspectives?allowing a provi-
sional, flexible separation between the referential
and formal dimensions of narrative.
3 Data
The dataset for this work consists of 15,099 dis-
tinct narratives drawn from HathiTrust Digital Li-
brary.
2
From an initial collection of 469,200 vol-
umes written in English and published between
1700 and 1899 (including poetry, drama, and non-
fiction as well as prose narrative), we extract
32,209 volumes of prose fiction, remove dupli-
cates and fuse multi-volume works to create the fi-
nal dataset. Since the original texts were produced
2
http://www.hathitrust.org
by scanning and running OCR on physical books,
we automatically correct common OCR errors and
trim front and back matter from the volumes using
the page-level classifiers and HMM of Underwood
et al (2013)
Many aspects of this process would be sim-
pler if we used manually-corrected texts, such as
those drawn from Project Gutenberg. But we hope
to produce research that has historical as well as
computational significance, and doing so depends
on the provenance of a collection. Gutenberg?s
decentralized selection process tends to produce
exceptionally good coverage of currently-popular
genres like science fiction, whereas HathiTrust ag-
gregates university libraries. Library collections
are not guaranteed to represent the past perfectly,
but they are larger, and less strongly shaped by
contemporary preferences.
The goal of this work is to provide a method to
infer a set of character types in an unsupervised
fashion from the data. As with prior work (Bam-
man et al, 2013), we define this target, a character
persona, as a distribution over several categories
of typed dependency relations:
3
1. agent: the actions of which a character is
the agent (i.e., verbs for which the character
holds an nsubj or agent relation).
2. patient: the actions of which a character is
the patient (i.e., verbs for which the character
holds a dobj or nsubjpass relation).
3. possessive: the objects that a character pos-
sesses (i.e., all words for which the character
holds a poss relation).
4. predicative: attributes predicated of a char-
acter (i.e., adjectives or nouns holding an
nsubj relation to the character, with an inflec-
tion of be as a child).
This set captures the constellation of what a
character does and has done to them, what they
possess, and what they are described as being.
While previous work uses the Stanford
CoreNLP toolkit to identify characters and extract
typed dependencies for them, we found this
approach to be too slow for the scale of our data (a
total of 1.8 billion tokens); in particular, syntactic
parsing, with cubic complexity in sentence length,
and out-of-the-box coreference resolution (with
thousands of potential antecedents) prove to be
3
All categories are described using the Stanford typed de-
pendencies (de Marneffe and Manning, 2008), but any syn-
tactic formalism is equally applicable.
371
the biggest bottlenecks.
Before addressing character inference, we
present here a prerequisite NLP pipeline that
scales well to book-length documents.
4
This
pipeline uses the Stanford POS tagger (Toutanova
et al, 2003), the linear-time MaltParser (Nivre et
al., 2007) for dependency parsing (trained on Stan-
ford typed dependencies), and the Stanford named
entity recognizer (Finkel et al, 2005). It includes
the following components for clustering charac-
ter name mentions, resolving pronominal corefer-
ence, and reducing vocabulary dimensionality.
3.1 Character Clustering
First, let us terminologically distinguish between a
character mention in a text (e.g., the token Tom on
page 141 of The Adventures of Tom Sawyer) and a
character entity (e.g., TOM SAWYER the character,
to which that token refers). To resolve the former
to the latter, we largely follow Davis et al (2003)
and Elson et al (2010): we define a set of initial
characters corresponding to each unique charac-
ter name that is not a subset of another (e.g., Mr.
Tom Sawyer) and deterministically create a set of
allowable variants for each one (Mr. Tom Sawyer
? Tom, Sawyer, Tom Sawyer, Mr. Sawyer, and
Mr. Tom); then, from the beginning of the book
to the end, we greedily assign each mention to the
most recently linked entity for whom it is a vari-
ant. The result constitutes our set of characters,
with all mentions partitioned among them.
3.2 Pronominal Coreference Resolution
While the character clustering stage is essentially
performing proper noun coreference resolution,
approximately 74% of references to characters in
books come in the form of pronouns.
5
To resolve
this more difficult class at the scale of an entire
book, we train a log-linear discriminative classifier
only on the task of resolving pronominal anaphora
(i.e., ignoring generic noun phrases such as the
paint or the rascal).
For this task, we annotated a set of 832 coref-
erence links in 3 books (Pride and Prejudice, The
Turn of the Screw, and Heart of Darkness) and fea-
turized coreference/antecedent pairs with:
4
All code is available at http://www.ark.cs.cmu.
edu/literaryCharacter
5
Over all 15,099 narratives, the average number of char-
acter proper name mentions is 1,673; the average number of
gendered singular pronouns (he, she, him, his, her) is 4,641.
1. The syntactic dependency path from a
pronoun to its potential antecedent (e.g.,
dobj?pred??pred?nsubj (where ? de-
notes movement across sentence boundaries).
2. The salience of the antecedent character (de-
fined as the count of that character?s named
mentions in the previous 500 words).
3. The antecedent part of speech.
4. Whether or not the pronoun and antecedent
appear in the same quotation scope (false if
one appears in a quotation and one outside).
5. Whether or not the two agree for gender.
6. The syntactic tree distance between the two.
7. The linear (word) distance between the two.
With this featurization and training data, we train
a binary logistic regression classifier with `
1
regu-
larization (where negative examples are comprised
of all character entities in the previous 100 words
not labeled as the true antecedent). In a 10-fold
cross-validation on predicting the true nearest an-
tecedent for a pronominal anaphor, this method
achieves an average accuracy of 82.7%.
With this trained model, we then select the
highest-scoring antecedent within 100 words for
each pronominal anaphor in our data.
3.3 Dimensionality Reduction
To manage the degrees of freedom in the model
described in ?4, we perform dimensionality reduc-
tion on the vocabulary by learning word embed-
dings with a log-linear continuous skip-gram lan-
guage model (Mikolov et al, 2013) on the entire
collection of 15,099 books. This method learns a
low-dimensional real-valued vector representation
of each word to predict all of the words in a win-
dow around it; empirically, we find that with a suf-
ficient window size (we use n = 10), these word
embeddings capture semantic similarity (placing
topically similar words near each other in vector
space).
6
We learn a 100-dimensional embedding
for each of the 512,344 words in our vocabulary.
To create a partition over the vocabulary, we
use hard K-means clustering (with Euclidean dis-
tance) to group the 512,344 word types into 1,000
clusters. We then agglomeratively cluster those
1,000 groups to assign bitstring representations to
each one, forming a balanced binary tree by only
merging existing clusters at equal levels in the hi-
6
In comparison, Brown et al (1992) clusters learned from
the same data capture syntactic similarity (placing function-
ally similar words in the same cluster).
372
01
0
1
0
1
0111001110: hat coat cap cloak handkerchief
0111001111: pair boots shoes gloves leather
0111001100: dressed costume uniform clad clothed
0111001101: dress clothes wore worn wear
01110011 ?
Figure 1: Bitstring representations of neural agglomerative clusters, illustrating the leaf nodes in a binary tree rooted in the
prefix 01110011. Bitstring encodings of intermediate nodes and terminal leaves result by following the left (0) and right (1)
branches of the merge tree created through agglomerative clustering.
erarchy. We use Euclidean distance as a funda-
mental metric and a group-average similarity func-
tion for calculating the distance between groups.
Fig. 1 illustrates four of the 1,000 learned clusters.
4 Model
In order to separate out the effects that a charac-
ter?s persona has on the words that are associated
with them (as opposed to other factors, such as
time period, genre, or author), we adopt a hierar-
chical Bayesian approach in which the words we
observe are generated conditional on a combina-
tion of different effects captured in a log-linear (or
?maximum entropy?) distribution.
Maximum entropy approaches to language
modeling have been used since Rosenfeld (1996)
to incorporate long-distance information, such as
previously-mentioned trigger words, into n-gram
language models. This work has since been ex-
tended to a Bayesian setting by applying both
a Gaussian prior (Chen and Rosenfeld, 2000),
which dampens the impact of any individual fea-
ture, and sparsity-inducing priors (Kazama and
Tsujii, 2003; Goodman, 2004), which can drive
many feature weights to 0. The latter have been
applied specifically to the problem of estimating
word probabilities with sparse additive generative
(SAGE) models (Eisenstein et al, 2011), where
sparse extra-linguistic effects can influence a word
probability in a larger generative setting.
In contrast to previous work in which the prob-
ability of a word linked to a character is depen-
dent entirely on the character?s latent persona, in
our model, we see the probability of a word as
dependent on: (i) the background likelihood of
the word, (ii) the author, so that a word becomes
more probable if a particular author tends to use it
more, and (iii) the character?s persona, so that a
word is more probable if appearing with a partic-
ular persona. Intuitively, if the author Jane Austen
is associated with a high weight for the word man-
ners, and all personas have little effect for this
word, then manners will have little impact on de-
ciding which persona a particular Austen character
embodies, since its presence is explained largely
by Austen having penned the word. While we ad-
dress only the author as an observed effect, this
model is easily extended to other features as well,
including period, genre, point of view, and others.
The generative story runs as follows (Figure 2
depicts the full graphical model): Let there be
M unique authors in the data, P latent personas
(a hyperparameter to be set), and V words in
the vocabulary (in the general setting these may
be word types; in our data the vocabulary is the
set of 1,000 unique cluster IDs). Each role type
r ? {agent,patient,possessive,predicative}
and vocabulary word v (here, a cluster ID)
is associated with a real-valued vector ?
r,v
=
[?
meta
r,v
, ?
pers
r,v
, ?
0
r,v
] of length M + P + 1. The first
M + P elements are drawn from a Laplace prior
with mean ? = 0 and scale ? = 1; the last el-
ement ?
0
r,v
is an unregularized bias term account-
ing for the background. Each element in this vec-
tor captures the log-additive effect of each author,
persona, and the background distribution on the
word?s probability (Eq. 1, below).
Much like latent Dirichlet alocation (Blei et al,
2003), each document d in our dataset draws a
multinomial distribution ?
d
over personas from a
shared Dirichlet prior ?, which captures the pro-
portion of each character type in that particular
document. Every character c in the document
draws its persona p from this document-specific
multinomial. Given document metadata m (here,
one of a set of M authors) and persona p, each tu-
ple of a role r with word w is assumed to be drawn
from Eq. 1 in Fig. 3. This SAGE model can be
understood as a log-linear distribution with three
kinds of features (metadata, persona, and back-
373
P (w | m, p, r, ?) = exp
(
?
meta
r,w
[m] + ?
pers
r,w
[p] + ?
0
r,w
)
/
V
?
v=1
exp
(
?
meta
r,v
[m] + ?
pers
r,v
[p] + ?
0
r,v
)
(1)
P (b | m, p, r, ?) =
n?1
?
j=0
?
?
?
logit
?1
(
?
meta
r,b
1:j
[m] + ?
pers
r,b
1:j
[p] + ?
0
r,b
1:j
)
if b
j+1
= 1
1? logit
?1
(
?
meta
r,b
1:j
[m] + ?
pers
r,b
1:j
[p] + ?
0
r,b
1:j
)
otherwise
(2)
Figure 3: Parameterizations of the SAGE word distribution. Eq. 1 is a ?flat? multinomial logistic regression with one ?-vector
per role and word. Eq. 2 uses the hierarchical softmax formulation, with one ?-vector per role and node in the binary tree of
word clusters, giving a distribution over bit strings (b) with the same number of parameters as Eq. 1.
ground bias).
4.1 Hierarchical Softmax
The partition function in Eq. 1 can lead to slow
inference for any reasonably-sized vocabulary. To
address this, we reparameterize the model by ex-
ploiting the structure of the agglomerative clus-
tering in ?3.3 to perform a hierarchical softmax,
following Goodman (2001), Morin and Bengio
(2005) and Mikolov et al (2013).
The bitstring representations by which we en-
code each word in the vocabulary serve as natural,
and inherently meaningful, intermediate classes
that correspond to semantically related subsets of
the vocabulary, with each bitstring prefix denoting
one such class. Longer bitstrings correspond to
more fine-grained classes. In the example shown
in Figure 1, 011100111 is one such intermediate
class, containing the union of pair, boots, shoes,
gloves leather and hat, coat, cap cloak, handker-
chief. Because these classes recursively partition
the vocabulary, they offer a convenient way to
reparameterize the model through the chain rule
of probability.
Consider, for example, a word represented as
the bitstring c = 01011; calculating P (c =
01011)?we suppress conditioning variables for
clarity?involves the product: P (c
1
= 0) ?
P (c
2
= 1 | c
1
= 0) ? P (c
3
= 0 | c
1:2
=
01) ? P (c
4
= 1 | c
1:3
= 010) ? P (c
5
= 1 |
c
1:4
= 0101).
Since each multiplicand involves a binary pre-
diction, we can avoid partition functions and use
the classic binary logistic regression.
7
We have
converted the V -way multiclass logistic regression
problem of Eq. 1 into a sequence of log V evalua-
tions (assuming a perfectly balanced tree). Given
7
Recall that logistic regression lets P
LR
(y = 1 | x, ?) =
logit
?1
(x
>
?) = 1/(1 + exp?x
>
?) for binary dependent
variable y, independent variables x, and coefficients ?.
m, p, and r (as above) we let b = b
1
b
2
? ? ? b
n
de-
note the bitstring representation of a word cluster,
and the distribution is given by Eq. 2 in Fig. 3.
In this paramaterization, rather than one ?-
vector for each role and vocabulary term, we have
one ?-vector for each role and conditional binary
decision in the tree (each bitstring prefix). Since
the tree is binary with V leaves, this yields the
same total number of parameters. As Goodman
(2001) points out, while this reparameterization is
exact for true probabilities, it remains an approx-
imation for estimated models (with generalization
behavior dependent on how well the class hierar-
chy is supported by the data). In addition to en-
abling faster inference, one advantage of the bit-
string representation and the hierarchical softmax
parameterization is that we can easily calculate
probabilities of clusters at different granularities.
4.2 Inference
Our primary quantities of interest in this model
are p (the personas for each character) and ?, the
effects that each author and persona have on the
probability of a word. Rather than adopting a fully
Bayesian approach (e.g., sampling all variables),
we infer these values using stochastic EM, alter-
nating between collapsed Gibbs sampling for each
p and maximizing with respect to ?.
Collapsed Gibbs for personas.
8
At each step,
the required quantity is the probability that char-
acter c in document d has persona z, given ev-
erything else. This is proportional to the number
of other characters in document d who also (cur-
rently) have that persona (plus the Dirichlet hy-
perparameter which acts as a smoother) times the
probability (under p
d,c
= z) of all of the words
8
We assume the reader is familiar with collapsed Gibbs
sampling as used in latent-variable NLP models.
374
??
p
w
r
m
?
?
?
W
C
D
P Number of personas (hyperparameter)
D Number of documents
C
d
Number of characters in document d
W
d,c
Number of (cluster, role) tuples for character c
m
d
Metadata for document d (ranges over M authors)
?
d
Document d?s distribution over personas
p
d,c
Character c?s persona
j An index for a ?r, w? tuple in the data
w
j
Word cluster ID for tuple j
r
j
Role for tuple j ? {agent, patient, poss, pred}
? Coefficients for the log-linear language model
?, ? Laplace mean and scale (for regularizing ?)
? Dirichlet concentration parameter
Figure 2: Above: Probabilistic graphical model. Observed
variables are shaded, latent variables are clear, and collapsed
variables are dotted. Below: Definition of variables.
observed in each role r for that character:
(count(z; p
d,?c
) + ?
z
)?
R
?
r=1
?
j:r
j
=r
P (b
j
| m, p, r, ?)
(3)
The metadata features (like author, etc.) influence
this probability by being constant for all choices
of z; e.g., if the coefficient learned for Austen for
vocabulary term manners is high and all coeffi-
cients for all z are close to zero, then the proba-
bility of manners will change little under different
choices of z. Eq. 3 contains one multiplicand for
every word associated with a character, and only
one term reflecting the influence of the shared doc-
ument multinomial. The implication is that, for
major characters with many observed words, the
words will dominate the choice of persona; where
the document influence would have a bigger effect
is with characters for whom we don?t have much
data. In that case, it can act as a kind of informed
background; given what little data we have for that
character, it would nudge us toward the character
types that the other characters in the book embody.
Given an assignment of all p, we choose ?
to maximize the conditional log-likelihood of the
words, as represented by their bitstring cluster IDs,
given the observed author and background effects
and the sampled personas. This equates to solving
4V `
1
-regularized logistic regressions (see Eq. 2
in Figure 3), one for each role type and bitstring
prefix, each with M + P + 1 parameters. We ap-
ply OWL-QN (Andrew and Gao, 2007) to mini-
mize the `
1
-regularized objective with an absolute
convergence threshold of 10
?5
.
5 Evaluation
While standard NLP and machine learning prac-
tice is to evaluate the performance of an algorithm
on a held-out gold standard, articulating what a
true ?persona? might be for a character is inher-
ently problematic. Rather, we evaluate the perfor-
mance and output of our model by preregistering
a set of 29 hypotheses of varying scope and diffi-
culty and comparing the performance of different
models in either confirming, or failing to confirm,
those hypotheses. This kind of evaluation was pre-
viously applied to a subjective text measurement
problem by Sim et al (2013).
All hypotheses were created by a literary
scholar with specialization in the period to not
only give an empirical measure of the strengths
and weaknesses of different models, but also to
help explore exactly what the different models
may, or may not, be learning. All preregistered hy-
potheses establish the degrees of similarity among
three characters, taking the form: ?character X is
more similar to character Y than either X or Y is
to a distractor character Z?; for a given model and
definition of distance under that model, each hy-
pothesis yields two yes/no decisions that we can
evaluate:
? distance(X,Y ) < distance(X,Z)
? distance(X,Y ) < distance(Y,Z)
To tease apart the different kinds of similarities
we hope to explore, we divide the hypotheses into
four classes:
375
A. This class constitutes sanity checks: charac-
ter X and Y are more similar to each other
in every way than to character Z. E.g.: Eliz-
abeth Bennet in Pride and Prejudice resem-
bles Elinor Dashwood in Sense and Sensibil-
ity (Jane Austen) more than either character
resembles Allen Quatermain in Allen Quater-
main (H. Rider Haggard). (Austenian protag-
onists should resemble each other more than
they resemble a grizzled hunter.)
B. This class captures our ability to identify two
characters in the same author as being more
similar to each other than to a closely re-
lated character in a different author. E.g.:
Wickham in Pride and Prejudice resembles
Willoughby in Sense and Sensibility (Jane
Austen) more than either character resem-
bles Mr. Rochester in Jane Eyre (Charlotte
Bront?e).
C. This class captures our ability to discrimi-
nate among similar characters in the same au-
thor. In these hypotheses, two characters X
and Y from the same author are more simi-
lar to each other than to a third character Z
from that same author. E.g.: Wickham in
Pride and Prejudice (Jane Austen) resembles
Willoughby in Sense and Sensibility more
than either character resembles Mr. Darcy in
Pride and Prejudice.
D. This class constitutes more difficult, ex-
ploratory hypotheses, including differences
among point of view. E.g.: Montoni in
Mysteries of Udolpho (Radcliffe) resem-
bles Heathcliff in Wuthering Heights (Emily
Bront?e) more than either resembles Mr. Ben-
net in Pride and Prejudice. (Testing our
model?s ability to discern similarities in spite
of elapsed time.)
All 29 hypotheses can be found in a supplemen-
tary technical report (Bamman et al, 2014). We
emphasize that the full set of hypotheses was
locked before the model was estimated.
6 Experiments
Part of the motivation of our mixed effects model
is to be able to tackle hypothesis class C?by fac-
toring out the influence of a particular author on
the learning of personas, we would like to be able
to discriminate between characters that all have
a common authorial voice. In contrast, the Per-
sona Regression model of Bamman et al (2013),
which uses metadata variables (like authorship)
to encourage entities with similar covariates to
have similar personas, reflects an assumption that
makes it likely to perform well at class B.
To judge their respective strengths on different
hypothesis classes, we evaluate three models:
1. The mixed-effects Author/Persona model
(described above), which includes author in-
formation as a metadata effect; here, each
?-vector (of length M + P + 1) contains a
parameter for each of the distinct authors in
our data, a parameter for each persona, and a
background parameter.
2. A Basic persona model, which ablates au-
thor information but retains the same log-
linear architecture; here, the ?-vector is of
size P +1 and does not model author effects.
3. The Persona Regression model of Bam-
man et al (2013).
All models are run with P ? {10, 25, 50, 100,
250} personas; Persona Regression addition-
ally uses K = 25 latent topics. All configura-
tions use the full dataset of 15,099 novels, and all
characters with at least 25 total roles (a total of
257,298 entities). All experiments are run with
50 iterations of Gibbs sampling to collect samples
for the personas p, alternating with maximization
steps for ?. The value of ? is optimized using slice
sampling (with a non-informative prior) every 5
iterations. The value of ? is held constant at 1.
At the end of inference, we calculate the posterior
distributions over personas for all characters as the
sampling probability of the final iteration.
To formally evaluate ?similarity? between two
characters, we measure the Jensen-Shannon diver-
gence between personas (calculated as the average
JS distance over the cluster distributions for each
role type), marginalizing over the characters? pos-
terior distributions over personas; two characters
with a lower JS divergence are judged to be more
similar than two characters with a higher one.
As a Baseline, we also evaluate all hypotheses
on a model with no latent variables whatsoever,
which instead measures similarity as the average
JS divergence between the empirical word distri-
butions over each role type.
Table 1 presents the results of this compari-
son; for all models with latent variables, we re-
port the average of 5 sampling runs with different
random initializations. Figure 4 provides a syn-
376
P Model
Hypothesis Class
A B C D
250
Author/Persona 1.00 0.58 0.75 0.42
Basic Persona 1.00 0.73 0.58 0.53
Persona Reg. 0.90 0.70 0.58 0.44
100
Author/Persona 0.98 0.68 0.70 0.46
Basic Persona 0.95 0.73 0.53 0.47
Persona Reg. 0.93 0.78 0.63 0.49
50
Author/Persona 0.95 0.73 0.63 0.50
Basic Persona 0.98 0.75 0.48 0.53
Persona Reg. 1.00 0.75 0.65 0.38
25
Author/Persona 1.00 0.63 0.65 0.50
Basic Persona 1.00 0.63 0.50 0.50
Persona Reg. 0.90 0.78 0.60 0.39
10
Author/Persona 0.95 0.63 0.70 0.51
Basic Persona 0.78 0.80 0.48 0.46
Persona Reg. 0.90 0.73 0.43 0.41
Baseline 1.00 0.63 0.58 0.37
Table 1: Agreement rates with preregistered hypotheses, av-
eraged over 5 sampling runs with different initializations.
0
25
50
75
100
A B C D
Hypothesis class
Ac
cu
ra
cy
Author/Persona  Basic  Persona Reg.  Baseline
Figure 4: Synopsis of table 1: average accuracy across all P .
Persona regression is best able to judge characters in one
author to be more similar to each other than to characters in
another (B), while our mixed-effects Author/Persona model
outperforms other models at discriminating characters in the
same author (C).
opsis of this table by illustrating the average ac-
curacy across all choice of P . All models, in-
cluding the baseline, perform well on the sanity
checks (A). As expected, the Persona Regres-
sion model performs best at hypothesis class B
(correctly judging two characters from the same
author to be more similar to each other than to a
character from a different author); this behavior is
encouraged in this model by allowing an author (as
an external metadata variable) to directly influence
the persona choice, which has the effect of push-
ing characters from the same author to embody
the same character type. Our mixed effects Au-
thor/Persona model, in contrast, outperforms the
other models at hypothesis class C (correctly dis-
criminating different character types present in the
same author). By discounting author-specific lexi-
cal effects during persona inference, we are better
able to detect variation among the characters of a
single author that we are not able to capture oth-
erwise. While these different models complement
each other in this manner, we note that there is
no absolute separation among them, which may be
suggestive of the degree to which the formal and
referential dimensions are fused in novels. Nev-
ertheless, the strengths of these different models
on these different hypothesis classes gives us flex-
ible alternatives to use depending on the kinds of
character types we are looking to infer.
7 Analysis
The latent personas inferred from this model will
support further exploratory analysis of literary his-
tory. Figure 2 illustrates this with a selection of
three character types learned, displaying charac-
teristic clusters for all role types, along with the
distribution of that persona?s use across time and
the gender distribution of characters embodying
that persona. In general, the personas learned so
far do not align neatly with character types known
to literary historians. But they do have legible as-
sociations both with literary genres and with social
categories. Even though gender is not an observ-
able variable known to the model during inference,
personas tend to be clearly gendered. This is not
in itself surprising (since literary scholars know
that assumptions about character are strongly gen-
dered), but it does suggest that diachronic analysis
of latent character types might cast new light on
the history of gender in fiction. This is especially
true since the distribution of personas across the
time axis similarly reveals coherent trends.
Table 3 likewise illustrates what our model
learns by presenting a sample of the fixed effects
learned for a set of five major 19th-century au-
thors. These are clusters that are conditionally
more likely to appear associated with a character
in a work by the given author than they are in the
overall data; by factoring this information out of
the inference process for learning character types
(by attributing its presence in a text to the author
377
1800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900 1800 1820 1840 1860 1880 1900
Agent
carried ran threw sent received arrived turns begins returns
rose fell suddenly appeared struck showed thinks loves calls
is seems returned immediately waiting does knows comes
Patient
wounded killed murdered wounded killed murdered thinks loves calls
suffer yield acknowledge destroy bind crush love hope true
free saved unknown attend haste proceed turn hold show
Poss
death happiness future army officers troops lips cheek brow
lips cheek brow soldiers band armed eyes face eye
mouth fingers tongue party join camp table bed chair
Pred
crime guilty murder king emperor throne beautiful fair fine
youth lover hers general officer guard good kind ill
dead living died soldier knight hero dead living died
% Female 12.2 3.7 54.7
Table 2: Snapshots of three personas learned from the P = 50, Author/Persona model. Gender and time proportions are
calculated by summing and normalizing the posterior distributions over all characters with that feature. We truncate time series
at 1800 due to data sparsity before that date; the y-axis illustrates the frequency of its use in a given year, relative to its lifetime.
Author clusters
Jane Austen
praise gift consolation
letter read write
character natural taste
Charlotte Bront?e
lips cheek brow
book paper books
hat coat cap
Charles Dickens
hat coat cap
table bed chair
hand head hands
Herman Melville
boat ship board
hat coat cap
feet ground foot
Jules Verne
journey travel voyage
master company presence
success plan progress
Table 3: Characteristic possessive clusters in a sample of
major 19th-century authors.
rather than the persona), we are able to learn per-
sonas that cut across different topics more effec-
tively than if a character type is responsible for
explaining the presence of these terms as well.
8 Conclusion
Our method establishes the possibility of repre-
senting the relationship between character and nar-
rative form in a hierarchical Bayesian model. Pos-
tulating an interaction between authorial diction
and character allows models that consider the ef-
fect of the author to more closely reproduce a hu-
man reader?s judgments, especially by learning to
distinguish different character types within a sin-
gle author?s oeuvre. This opens the door to con-
sidering other structural and formal dimensions of
narration. For instance, representation of charac-
ter is notoriously complicated by narrative point of
view (Booth, 1961); and indeed, comparisons be-
tween first-person narrators and other characters
are a primary source of error for all models tested
above. The strategy we have demonstrated sug-
gests that it might be productive to address this by
modeling the interaction of character and point of
view as a separate effect analogous to authorship.
It is also worth noting that the models tested
above diverge from many structuralist theories of
narrative (Propp, 1998) by allowing multiple in-
stances of the same persona in a single work.
Learning structural limitations on the number of
?protagonists? likely to coexist in a single story,
for example, may be another fruitful area to ex-
plore. In all cases, the machinery of hierarchical
models gives us the flexibility to incorporate such
effects at will, while also being explicit about the
theoretical assumptions that attend them.
9 Acknowledgments
We thank the reviewers for their helpful com-
ments. The research reported here was supported
by a National Endowment for the Humanities
start-up grant to T.U., U.S. National Science Foun-
dation grant CAREER IIS-1054319 to N.A.S., and
an ARCS scholarship to D.B. This work was made
possible through the use of computing resources
made available by the Pittsburgh Supercomputing
Center. Eleanor Courtemanche provided advice
about the history of narrative theory.
378
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l
1
-regularized log-linear models. In Proc. of
ICML.
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2013. Learning latent personas of film char-
acters. Proc. of ACL.
David Bamman, Ted Underwood, and Noah A. Smith.
2014. Appendix to ?A Bayesian mixed effects
model of literary character?. Technical report,
Carnegie Mellon University, University of Illinois-
Urbana Champaign.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Wayne Booth. 1961. The Rhetoric of Fiction. Univer-
sity of Chicago Press, Chicago.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In Proc. of
EMNLP, Seattle, Washington, USA.
Stanley F. Chen and Roni Rosenfeld. 2000. A
survey of smoothing techniques for me models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proc. of NAACL.
Peter T. Davis, David K. Elson, and Judith L. Klavans.
2003. Methods for precise named entity matching in
digital collections. In Proc. of JCDL, Washington,
DC, USA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.
Greg Durrett, David Hall, and Dan Klein. 2013.
Decentralized entity-level modeling for coreference
resolution. In Proc. of ACL.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc.
of ICML.
David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. In Proc. of ACL, Stroudsburg, PA,
USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. of ACL.
E. M. Forster. 1927. Aspects of the Novel. Harcourt,
Brace & Co.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Proc. of ICASSP.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. of NAACL.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Proc.
of NAACL.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. of EMNLP.
Suzanne Keen. 2003. Narrative Form. Palgrave
Macmillan, Basingstoke.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proc. of ICLR.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proc. of AISTATS.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13:95?135, 5.
Vladimir Propp. 1998. Morphology of the Folktale.
University of Texas Press, 2nd edition.
Roni Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Com-
puter Speech and Language, 10(3):187 ? 228.
Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proc. of EMNLP,
Seattle, Washington, USA.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc. of NAACL.
Ted Underwood, Michael L Black, Loretta Auvil, and
Boris Capitanu. 2013. Mapping mutable genres in
structurally complex volumes. In Proc. of IEEE In-
ternational Conference on Big Data.
Alex Woloch. 2003. The One vs. the Many: Minor
Characters and the Space of the Protagonist in the
Novel. Princeton University Press, Princeton NJ.
379
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 786?796,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Linguistic Structured Sparsity in Text Categorization
Dani Yogatama
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dyogatama@cs.cmu.edu
Noah A. Smith
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We introduce three linguistically moti-
vated structured regularizers based on
parse trees, topics, and hierarchical word
clusters for text categorization. These
regularizers impose linguistic bias in fea-
ture weights, enabling us to incorporate
prior knowledge into conventional bag-
of-words models. We show that our
structured regularizers consistently im-
prove classification accuracies compared
to standard regularizers that penalize fea-
tures in isolation (such as lasso, ridge,
and elastic net regularizers) on a range of
datasets for various text prediction prob-
lems: topic classification, sentiment anal-
ysis, and forecasting.
1 Introduction
What is the best way to exploit linguistic infor-
mation in statistical text processing models? For
tasks like text classification, sentiment analysis,
and text-driven forecasting, this is an open ques-
tion, as cheap ?bag-of-words? models often per-
form well. Much recent work in NLP has fo-
cused on linguistic feature engineering (Joshi et
al., 2010) or representation learning (Glorot et al,
2011; Socher et al, 2013).
In this paper, we propose a radical alternative.
We embrace the conventional bag-of-words repre-
sentation of text, instead bringing linguistic bias
to bear on regularization. Since the seminal work
of Chen and Rosenfeld (2000), the importance of
regularization in discriminative models of text?
including language modeling, structured predic-
tion, and classification?has been widely recog-
nized. The emphasis, however, has largely been
on one specific kind of inductive bias: avoiding
large weights (i.e., coefficients in a linear model).
Recently, structured (or composite) regulariza-
tion has been introduced; simply put, it reasons
about different weights jointly. The most widely
explored variant, group lasso (Yuan and Lin, 2006)
seeks to avoid large `
2
norms for groups of
weights. Group lasso has been shown useful in
a range of applications, including computational
biology (Kim and Xing, 2008), signal processing
(Lv et al, 2011), and NLP (Eisenstein et al, 2011;
Martins et al, 2011; Nelakanti et al, 2013). For
text categorization problems, Yogatama and Smith
(2014) proposed groups based on sentences, an
idea generalized here to take advantage of richer
linguistic information.
In this paper, we show how linguistic informa-
tion of various kinds?parse trees, thematic topics,
and hierarchical word clusterings?can be used to
construct group lasso variants that impose linguis-
tic bias without introducing any new features. Our
experiments demonstrate that structured regulariz-
ers can squeeze higher performance out of conven-
tional bag-of-words models on seven out of eight
of text categorization tasks tested, in six cases with
more compact models than the best-performing
unstructured-regularized model.
2 Notation
We represent each document as a feature vector
x ? R
V
, where V is the vocabulary size. x
v
is the
frequency of the vth word (i.e., this is a ?bag of
words? model).
Consider a linear model that predicts a binary
response y ? {?1,+1} given x and weight vector
w ? R
V
. We denote our training data of D doc-
uments in the corpus by {x
d
, y
d
}
D
d=1
. The goal of
the learning procedure is to estimate w by mini-
mizing the regularized training data loss:
w? = arg min
w
?(w) +
?
D
d=1
L(x
d
,w, y
d
),
where L(x,w, y) is the loss function for docu-
ment d and ?(w) is the regularizer.
In this work, we use the log loss:
L(x
d
,w, y
d
) = ? log(1 + exp(?y
d
w
>
x
d
)),
786
Other loss functions (e.g., hinge loss, squared loss)
can also be used with any of the regularizers dis-
cussed in this paper.
Our focus is on the regularizer, ?(w). For high
dimensional data such as text, regularization is
crucial to avoid overfitting.
1
The usual starting points for regularization are
the ?lasso? (Tibshirani, 1996) and the ?ridge? (Ho-
erl and Kennard, 1970), based respectively on the
`
1
and squared `
2
norms:
?
las
(w) = ?
las
?w?
1
= ?
?
j
|w
j
|
?
rid
(w) = ?
rid
?w?
2
2
= ?
?
j
w
2
j
Both methods disprefer weights of large magni-
tude; smaller (relative) magnitude means a feature
(here, a word) has a smaller effect on the predic-
tion, and zero means a feature has no effect.
2
The
hyperparameter ? in each case is typically tuned
on a development dataset. A linear combination
of ridge and lasso is known as the elastic net (Zou
and Hastie, 2005). The lasso, ridge, and elastic net
are three strong baselines in our experiments.
3 Group Lasso
Structured regularizers penalize estimates of w in
which collections of weights are penalized jointly.
For example, in the group lasso (Yuan and Lin,
2006), predefined groups of weights (subvectors
of w) are encouraged to either go to zero (as
a group) or not (as a group)?this is known as
?group sparsity.?
3
The variant of group lasso we explore here uses
an `
1,2
norm. Let g index the G predefined groups
of weights and w
g
denote the subvector of w con-
taining weights for group g:
?
glas
(w) =?
glas
?
G
g=1
?
g
?w
g
?
2
,
1
A Bayesian interpretation of regularization is as a prior
on the weight vector w; in many cases ? can be under-
stood as a log-prior representing beliefs about the model held
before exposure to data. For lasso regression, the prior is
a zero-mean Laplace distribution, whereas for ridge regres-
sion the prior is a zero-mean Gaussian distribution. For non-
overlapping group lasso, the prior is a two-level hierarchical
Bayes model (Figueiredo, 2002). The Bayesian interpretation
of overlapping group lasso is not yet well understood.
2
The lasso leads to strongly sparse solutions, in which
many elements of the estimated w are actually zero. This
is an attractive property for efficiency and (perhaps) inter-
pretability. The ridge encourages weights to go toward zero,
but usually not all the way to zero; for this reason its solutions
are known as ?weakly? sparse.
3
Other structured regularizers include the fused lasso
(Tibshirani et al, 2005) and the elitist lasso (Kowalski and
Torresani, 2009).
where ?
glas
is a hyperparameter tuned on a devel-
opment data, and ?
g
is a group specific weight.
Typically the groups are non-overlapping, which
offers computational advantages, but this need not
be the case (Jacob et al, 2009; Jenatton et al,
2011).
4 Structured Regularizers for Text
Past work applying the group lasso to NLP prob-
lems has considered four ways of defining the
groups. Eisenstein et al (2011) defined groups
of coefficients corresponding to the same inde-
pendent variable applied to different (continuous)
output variables in multi-output regression. Mar-
tins et al (2011) defined groups based on fea-
ture templates used in chunking and parsing tasks.
Nelakanti et al (2013) defined groups based on n-
gram histories for language modeling. In each of
these cases, the groups were defined based on in-
formation from feature types alone; given the fea-
tures to be used, the groups were known.
Here we build on a fourth approach that exploits
structure in the data.
4
Yogatama and Smith (2014)
introduced the sentence regularizer, which uses
patterns of word cooccurrence in the training data
to define groups. We review this method, then ap-
ply the idea to three more linguistically informed
structure in text data.
4.1 Sentence Regularizer
The sentence regularizer exploits sentence bound-
aries in each training document. The idea is to
define a group g
d,s
for every sentence s in every
training document d. The group contains coeffi-
cients for words that occur in its sentence. This
means that a word is a member of one group for
every distinct (training) sentence it occurs in, and
that the regularizer is based on word tokens, not
types as in the approach of Martins et al (2011)
and Nelakanti et al (2013). The regularizer is:
?
sen
(w) =
?
D
d=1
?
S
d
s=1
?
d,s
?w
d,s
?
2
,
where S
d
is the number of sentences in document
d. This regularizer results in tens of thousands
to millions of heavily overlapping groups, since
a standard corpus typically contains thousands to
millions of sentences and many words that appear
in more than one sentence.
4
This provides a compelling reason not to view such
methods in a Bayesian framework: if the regularizer is in-
formed by the data, then it does not truly correspond to a
prior.
787
c0,++
c1 c4,+
c2 c3
The actors
c5,++ c8
c6 c7,+
are fantastic
.
Figure 1: An example of a parse tree from the Stanford sen-
timent treebank, which annotates sentiment at the level of
every constituent (indicated here by + and ++; no mark-
ing indicates neutral sentiment). The sentence is The ac-
tors are fantastic. Our regularizer constructs nine groups for
this sentence, corresponding to c
0
, c
1
, . . . , c
8
. g
c
0
consists of
5 weights??w
the
, w
actors
, w
are
, w
fantastic
, w
.
?, exactly the
same as the group in the sentence regularizer?g
c
1
consists
of 2 words, g
c
4
of 3 words, etc. Notice that c
2
, c
3
, c
6
, c
7
,
and c
8
each consist of only 1 word. The Stanford sentiment
treebank has an annotation of sentiments at the constituent
level. As in this example, most constituents are annotated as
neutral.
If the norm of w
g
d,s
is driven to zero, then the
learner has deemed the corresponding sentence ir-
relevant to the prediction. It is important to point
out that, while the regularizer prefers to zero out
the weights for all words in irrelevant sentences, it
also prefers not to zero out weights for words in
relevant sentences. Since the groups overlap and
may work against each other, the regularizer may
not be able to drive many weights to zero on its
own. Yogatama and Smith (2014) used a linear
combination of the sentence regularizer and the
lasso (a kind of sparse group lasso; Friedman et
al., 2010) to also encourage weights of irrelevant
word types to go to zero.
5
4.2 Parse Tree Regularizer
Sentence boundaries are a rather superficial kind
of linguistic structure; syntactic parse trees pro-
vide more fine-grained information. We introduce
a new regularizer, the parse tree regularizer, in
which groups are defined for every constituent in
every parse of a training data sentence.
Figure 1 illustrates the group structures derived
from an example sentence from the Stanford sen-
timent treebank (Socher et al, 2013). This regu-
larizer captures the idea that phrases might be se-
lected as relevant or (in most cases) irrelevant to
a task, and is expected to be especially useful in
sentence-level prediction tasks.
The parse-tree regularizer (omitting the group
5
Formally, this is equivalent to including one additional
group for each word type.
coefficients and ?) for one sentence with the parse
tree shown in Figure 1 is:
?
tree
(w) =
p
|w
the
|
2
+ |w
actors
|
2
+ |w
are
|
2
+ |w
fantastic
|
2
+ |w
.
|
2
+
p
|w
are
|
2
+ |w
fantastic
|
2
+ |w
2
.
|
+
p
|w
the
|
2
+ |w
actors
|
2
+
p
|w
are
|
2
+ |w
fantastic
|
2
+ |w
the
|+ |w
actors
|+ |w
are
|+ |w
fantastic
|+ |w
.
|
The groups have a tree structure, in that assign-
ing zero values to the weights in a group corre-
sponding to a higher-level constituent implies the
same for those constituents that are dominated by
it. This resembles the tree-guided group lasso in
Kim and Xing (2008), although the leaf nodes in
their tree represent tasks in multi-task regression.
Of course, in a corpus there are many parse trees
(one per sentence, so the number of parse trees is
the number of sentences). The parse-tree regular-
izer is:
?
tree
(w) =
?
D
d=1
?
S
d
s=1
?
C
d,s
c=1
?
d,s,c
?w
d,s,c
?
2
,
where ?
d,s,c
= ?
glas
?
?
size(g
d,s,c
), d ranges
over (training) documents and c ranges over con-
stituents in the parse of sentence s in docu-
ment d. Similar to the sentence regularizer,
the parse-tree regularizer operates on word to-
kens. Note that, since each word token is it-
self a constituent, the parse tree regularizer in-
cludes terms just like the lasso naturally, penal-
izing the absolute value of each word?s weight
in isolation. For the lasso-like penalty on each
word, instead of defining the group weights to be
1 ? the number of tokens for each word type, we
tune one group weight for all word types on a de-
velopment data. As a result, besides ?
glas
, we have
an additional hyperparameter, denoted by ?
las
.
To gain an intuition for this regularizer, consider
the case where we apply the penalty only for a sin-
gle tree (sentence), which for ease of exposition is
assumed not to use the same word more than once
(i.e., ?x?
?
= 1). Because it instantiates the tree-
structured group lasso, the regularizer will require
bigger constituents to be ?included? (i.e., their
words given nonzero weight) before smaller con-
stituents can be included. The result is that some
words may not be included. Of course, in some
sentences, some words will occur more than once,
and the parse tree regularizer instantiates groups
for constituents in every sentence in the training
corpus, and these groups may work against each
other. The parse tree regularizer should therefore
788
be understood as encouraging group behavior of
syntactically grouped words, or sharing of infor-
mation by syntactic neighbors.
In sentence level prediction tasks, such as
sentence-level sentiment analysis, it is known that
most constituents (especially those that corre-
spond to shorter phrases) in a parse tree are un-
informative (neutral sentiment). This was verified
by Socher et al (2013) when annotating phrases
in a sentence for building the Stanford sentiment
treebank. Our regularizer incorporates our prior
expectation that most constituents should have no
effect on prediction.
4.3 LDA Regularizer
Another type of structure to consider is topics.
For example, if we want to predict whether a pa-
per will be cited or not (Yogatama et al, 2011),
the model can perform better if it knows before-
hand the collections of words that represent certain
themes (e.g., in ACL papers, these might include
machine translation, parsing, etc.). As a result,
the model can focus on which topics will increase
the probability of getting citations, and penalize
weights for words in the same topic together, in-
stead of treating each word separately.
We do this by inferring topics in the training
corpus by estimating the latent Dirichlet aloca-
tion (LDA) model (Blei et al, 2003)). Note that
LDA is an unsupervised method, so we can in-
fer topical structures from any collection of docu-
ments that are considered related to the target cor-
pus (e.g., training documents, text from the web,
etc.). This contrasts with typical semi-supervised
learning methods for text categorization that com-
bine unlabeled and labeled data within a genera-
tive model, such as multinomial na??ve Bayes, via
expectation-maximization (Nigam et al, 2000) or
semi-supervised frequency estimation (Su et al,
2011). Our method does not use unlabeled data
to obtain more training documents or estimate the
joint distributions of words better, but it allows the
use of unlabeled data to induce topics. We leave
comparison with other semi-supervised methods
for future work.
There are many ways to associate inferred top-
ics with group structure. In our experiments, we
choose the R most probable words given a topic
and create a group for them.
6
The LDA regular-
6
Another possibility is to group the smallest set of words
whose total probability given a topic amounts to P (e.g.,
0.99). mass of a topic. Preliminary experiments found this
izer can be written as:
?
lda
(w) =
?
K
k=1
?
k
?w
k
?
2
,
where k ranges over the K topics. Similar to our
earlier notations, w
k
corresponds to the subvec-
tor of w such that the corresponding features are
present in topic k. Note that in this case we can
also have overlapping groups, since words can ap-
pear in the top R of many topics.
k = 1 k = 2 k = 3 k = 4
soccer injury physics monday
striker knee gravity tuesday
midfielder ligament moon april
goal shoulder sun june
defender cruciate relativity sunday
Table 1: A toy example of K = 4 topics. The top R = 5
words in each topics are displayed. The LDA regularizer
will construct four groups from these topics. The first group
is ?w
soccer
, w
striker
, w
midfielder
, w
goal
, w
defender
?, the sec-
ond group is ?w
injury
, w
knee
, w
ligament
, w
shoulder
, w
cruciate
?,
etc. In this example, there are no words occurring in the top
R of more than one topic, but that need not be the case in
general.
To gain an intuition for this regularizer, consider
the toy example in Table 1. the case where we
have K = 4 topics and we select R = 5 top words
from each topic. Supposed that we want to clas-
sify whether an article is a sports article or a sci-
ence article. The regularizer might encourage the
weights for the fourth topics? words toward zero,
since they are less useful for the task. Addition-
ally, the regularizer will penalize words in each of
the other three groups collectively. Therefore, if
(for example) ligament is deemed a useful feature
for classifying an article to be about sports, then
the other words in that topic will have a smaller ef-
fective penalty for getting nonzero weights?even
weights of the opposite sign as w
ligament
. It is im-
portant to distinguish this from unstructured reg-
ularizers such as the lasso, which penalize each
word?s weight on its own without regard for re-
lated word types.
Unlike the parse tree regularizer, the LDA regu-
larizer is not tree structured. Since the lasso-like
penalty does not occur naturally in a non tree-
structured regularizer, we add an additional lasso
penalty for each word type (with hyperparameter
?
las
) to also encourage weights of irrelevant words
to go to zero. Our LDA regularizer is an instance
of sparse group lasso (Friedman et al, 2010).
not to work well.
789
4.4 Brown Cluster Regularizer
Brown clustering is a commonly used unsuper-
vised method for grouping words into a hierarchy
of clusters (Brown et al, 1992). Because it uses
local information, it tends to discover words with
similar syntactic behavior, though semantic group-
ings are often evident, especially at the more fine-
grained end of the hierarchy.
We incorporate Brown clusters into a regular-
izer in a similar way to the topical word groups
inferred using LDA in ?4.3, but here we make use
of the hierarchy. Specifically, we construct tree-
structured groups, one per cluster (i.e., one per
node in the hierarchy). The Brown cluster regu-
larizer is:
?
brown
(w) =
?
N
v=1
?
v
?w
v
?
2
,
where v ranges over the N nodes in the Brown
cluster tree. As a tree structured regularizer, this
regularizer enforces constraints that a node v?s
group is given nonzero weights only if those nodes
that dominate v (i.e., are on a path from v to the
root) have their groups selected.
Consider a similar toy example to the LDA reg-
ularizer (sports vs. science) and the hierarchical
clustering of words in Figure 2. In this case, the
Brown cluster regularizer will create 17 groups,
one for every node in the clustering tree. The regu-
larizer for this tree (omitting the group coefficients
and ?) is:
?
brown
(w) =
?
7
i=0
?w
v
i
?
2
+ |w
goal
|+ |w
striker
|
+ |w
midfielder
|+ |w
knee
|+ |w
injury
|
+ |w
gravity
|+ |w
moon
|+ |w
sun
|
The regularizer penalizes words in a cluster to-
gether, exploiting discovered syntactic related-
ness. Additionally, the regularizer can zero out
weights of words corresponding to any of the in-
ternal nodes, such as v
7
if the words monday and
sunday are deemed irrelevant to prediction.
Note that the regularizer already includes terms
like the lasso naturally. Similar to the parse
tree regularizer, for the lasso-like penalty on each
word, we tune one group weight for all word types
on a development data with a hyperparameter ?
las
.
A key difference between the Brown cluster
regularizer and the parse tree regularizer is that
there is only one tree for Brown cluster regularizer,
whereas the parse tree regularizer can have mil-
lions (one per sentence in the training data). The
v0
v1 v5
v2 v4
v3 v10
v8 v9
goal striker
midfielder
v11 v12
knee injury
v6 v7
v13 v14
moon sun
v15 v16
monday sunday
Figure 2: An illustrative example of Brown clusters for N =
9. The Brown cluster regularizer constructs 17 groups, one
per node in for this tree, v
0
, v
1
, . . . , v
16
. v
0
contains 8 words,
v
1
contains 5, etc. Note that the leaves, v
8
, v
9
, . . . , v
16
, each
contain one word.
LDA and Brown cluster regularizers offer ways to
incorporate unlabeled data, if we believe that the
unlabeled data can help us infer better topics or
clusters. Note that the processes of learning topics
or clusters, or parsing training data sentences, are
a separate stage that precedes learning our predic-
tive model.
5 Learning
There are many optimization methods for learn-
ing models with structured regularizers, particu-
lary group lasso (Jacob et al, 2009; Jenatton et al,
2011; Chen et al, 2011; Qin and Goldfarb, 2012;
Yuan et al, 2013). We choose the optimization
method of Yogatama and Smith (2014) since it
handles millions of overlapping groups effectively.
The method is based on the alternating directions
method of multipliers (ADMM; Hestenes, 1969;
Powell, 1969). We review it here in brief, for com-
pleteness, and show how it can be applied to tree-
structured regularizers (such as the parse tree and
Brown cluster regularizers in ?4) in particular.
Our learning problem is, generically:
min
w
?(w) +
?
D
d=1
L(x
d
,w, y
d
).
Separating the lasso-like penalty for each word
type from our group regularizers, we can rewrite
this problem as:
min
w,v
?
las
(w) + ?
glas
(v) +
?
D
d=1
L(x
d
,w, y
d
)
s.t. v = Mw
where v consists of copies of the elements of
w. Notice that we work directly on w instead
of the copies for the lasso-like penalty, since it
does not have overlaps and has its own hyper-
parameters ?
las
. For the remaining groups with
size greater than one, we create copies v of size
790
L =
?
G
g=1
size(g). M ? {0, 1}
L?V
is a ma-
trix whose 1s link elements of w to their copies.
7
We now have a constrained optimization prob-
lem, from which we can create an augmented La-
grangian problem; let u be the Lagrange variables:
?
las
(w) + ?
glas
(v) + L(w)
+ u
>
(v ?Mw) +
?
2
?v ?Mw?
2
2
ADMM proceeds by iteratively updating each
of w, v, and u, amounting to the following sub-
problems:
min
w
?
las
(w) + L(w)? u
>
Mw +
?
2
?v ?Mw?
2
2
(1)
min
v
?
glas
(v) + u
>
v +
?
2
?v ?Mw?
2
2
(2)
u = u + ?(v ?Mw) (3)
Yogatama and Smith (2014) show that Eq. 1
can be rewritten in a form quite similar to `
2
-
regularized loss minimization.
8
Eq. 2 is the proximal operator of
1
?
?
glas
ap-
plied to Mw ?
u
?
. As such, it depends on the
form of M. Note that when applied to the col-
lection of ?copies? of the parameters, v, ?
glas
no
longer has overlapping groups. Defined M
g
as
the rows of M corresponding to weight copies as-
signed to group g. Let z
g
, M
g
w ?
u
g
?
. De-
note ?
g
= ?
glas
?
size(g). The problem can be
solved by applying the proximal operator used in
non-overlapping group lasso to each subvector:
v
g
= prox
?
glas
,
?
g
?
(z
g
)
=
?
?
?
0 if ?z
g
?
2
?
?
g
?
?z
g
?
2
?
?
g
?
?z
g
?
2
z
g
otherwise.
For a tree structured regularizer, we can get
speedups by working from the root node towards
the leaf nodes when applying the proximal oper-
ator in the second step. If g is a node in a tree
which is driven to zero, all of its children h that
has ?
h
? ?
g
will also be driven to zero.
Eq. 3 is a simple update of the dual variable u.
Algorithm 1 summarizes our learning procedure.
9
7
For the parse tree regularizer, L is the sum, over all
training-data word tokens t, of the number of constituents t
belongs to. For the LDA regularizer, L = R ? K. For the
Brown cluster regularizer, L = V ? 1.
8
The difference lies in that the squared `
2
norm in the
penalty penalizes the difference between w and a vector that
depends on the current values of u and v. This does not affect
the algorithm or its convergence in any substantive way.
9
We use relative changes in the `
2
norm of the parameter
vector w as our convergence criterion (threshold of 10
?3
),
and set the maximum number of iterations to 100. Other cri-
teria can also be used.
Algorithm 1 ADMM for overlapping group lasso
Input: augmented Lagrangian variable ?, regularization
strengths ?
glas
and ?
las
while stopping criterion not met do
w = arg min
w
?
las
(w)+L(w)+
?
2
P
V
i=1
N
i
(w
i
??
i
)
2
for g = 1 to G do
v
g
= prox
?
glas
,
?
g
?
(z
g
)
end for
u = u + ?(v ?Mw)
end while
6 Experiments
6.1 Datasets
We use publicly available datasets to evaluate our
model described in more detail below.
Topic classification. We consider four binary
categorization tasks from the 20 Newsgroups
dataset.
10
Each task involves categorizing a
document according to two related categories:
comp.sys: ibm.pc.hardware vs. mac.hardware;
rec.sport: baseball vs. hockey; sci: med vs. space;
and alt.atheism vs. soc.religion.christian.
Sentiment analysis. One task in sentiment anal-
ysis is predicting the polarity of a piece of text, i.e.,
whether the author is favorably inclined toward a
(usually known) subject of discussion or proposi-
tion (Pang and Lee, 2008). Sentiment analysis,
even at the coarse level of polarity we consider
here, can be confused by negation, stylistic use of
irony, and other linguistic phenomena. Our sen-
timent analysis datasets consist of movie reviews
from the Stanford sentiment treebank (Socher et
al., 2013),
11
and floor speeches by U.S. Congress-
men alongside ?yea?/?nay? votes on the bill under
discussion (Thomas et al, 2006).
12
For the Stan-
ford sentiment treebank, we only predict binary
classifications (positive or negative) and exclude
neutral reviews.
Text-driven forecasting. Forecasting from text
requires identifying textual correlates of a re-
sponse variable revealed in the future, most of
which will be weak and many of which will be
spurious (Kogan et al, 2009). We consider two
such problems. The first one is predicting whether
a scientific paper will be cited or not within three
years of its publication (Yogatama et al, 2011);
10
http://qwone.com/
?
jason/20Newsgroups
11
http://nlp.stanford.edu/sentiment/
12
http://www.cs.cornell.edu/
?
ainur/data.html
791
Dataset D # Dev. # Test V
2
0
N
science 952 235 790 30,154
sports 958 239 796 20,832
relig. 870 209 717 24,528
comp. 929 239 777 20,868
S
e
n
t
.
movie 6,920 872 1,821 17,576
vote 1,175 257 860 24,508
F
o
r
e
.
science 3,207 280 539 42,702
bill 37,850 7,341 6,571 10,001
Table 2: Descriptive statistics about the datasets.
the dataset comes from the ACL Anthology and
consists of research papers from the Association
for Computational Linguistics and citation data
(Radev et al, 2009). The second task is predicting
whether a legislative bill will be recommended by
a Congressional committee (Yano et al, 2012).
13
Table 2 summarizes statistics about the datasets
used in our experiments. In total, we evaluate our
method on eight binary classification tasks.
6.2 Setup
In all our experiments, we use unigram features
plus an additional bias term which is not regu-
larized. We compare our new regularizers with
state-of-the-art methods for document classifica-
tion: lasso, ridge, and elastic net regularization, as
well as the sentence regularizer discussed in ?4.1
(Yogatama and Smith, 2014).
14
We parsed all corpora using the Berkeley parser
(Petrov and Klein, 2007).
15
For the LDA regular-
izers, we ran LDA
16
on training documents with
K = 1, 000 and R = 10. For the Brown cluster
regularizers, we ran Brown clustering
17
on train-
ing documents with 5, 000 clusters for the topic
classification and sentiment analysis datasets, and
1, 000 for the larger text forecasting datasets (since
they are bigger datasets that took more time).
13
http://www.ark.cs.cmu.edu/bills
14
Hyperparameters are tuned on a separate develop-
ment dataset, using accuracy as the evaluation crite-
rion. For lasso and ridge models, we choose ? from
{10
?2
, 10
?1
, 1, 10, 10
2
, 10
3
}. For elastic net, we perform
grid search on the same set of values as ridge and lasso
experiments for ?
rid
and ?
las
. For the sentence, Brown
cluster, and LDA regularizers, we perform grid search on
the same set of values as ridge and lasso experiments for
?, ?
glas
, ?
las
. For the parse tree regularizer, because there
are many more groups than other regularizers, we choose
?
glas
from {10
?4
, 10
?3
, 10
?2
, 10
?1
, 10}, ? and ?
las
from
the same set of values as ridge and lasso experiments. If there
is a tie on development data we choose the model with the
smallest number of nonzero weights.
15
https://code.google.com/p/berkeleyparser/
16
http://www.cs.princeton.edu/
?
blei/lda-c/
17
https://github.com/percyliang/brown-cluster
6.3 Results
Table 3 shows the results of our experiments on
the eight datasets. The results demonstrate the su-
periority of structured regularizers. One of them
achieved the best result on all but one dataset.
18
It
is also worth noting that in most cases all variants
of the structured regularizers outperformed lasso,
ridge, and elastic net. In four cases, the new regu-
larizers in this paper outperform the sentence reg-
ularizer.
We can see that the parse tree regularizer per-
formed the best for the movie review dataset. The
task is to predict sentence-level sentiment, so each
training example is a sentence. Since constituent-
level annotations are available for this dataset, we
only constructed groups for neutral constituents
(i.e., we drive neutral constituents to zero during
training). It has been shown that syntactic in-
formation is helpful for sentence-level predictions
(Socher et al, 2013), so the parse tree regularizer
is naturally suitable for this task.
The Brown cluster and LDA regularizers per-
formed best for the forecasting scientific articles
dataset. The task is to predict whether an article
will be cited or not within three years after publi-
cation. Regularizers that exploit the knowledge of
semantic relations (e.g., topical categories), such
as the Brown cluster and LDA regularizers, are
therefore suitable for this type of prediction.
Table 4 shows model sizes obtained by each
of the regularizers for each dataset. While lasso
prunes more aggressively, it almost always per-
forms worse. Our structured regularizers were
able to obtain a significantly smaller model (27%,
34%, 19% as large on average for parse tree,
Brown, and LDA regularizers respectively) com-
pared to the ridge model.
Topic and cluster features. Another way to in-
corporate LDA topics and Brown clusters into a
linear model is by adding them as additional fea-
tures. For the 20N datasets, we also ran lasso,
ridge, and elastic net with additional LDA topic
and Brown cluster features.
19
Note that these new
baselines use more features than our model. We
can also add these additional features to our model
18
This ?bill? dataset, where they offered no improvement,
is the largest by far (37,850 documents), and therefore the
one where regularizers should matter the least. Note that the
differences are small across regularizers for this dataset.
19
For LDA, we took the top 10 words in a topic as a feature.
For Brown clusters, we add a cluster as an additional feature
if its size is less than 50.
792
Task Dataset
Accuracy (%)
m.f.c. lasso ridge elastic sentence parse Brown LDA
20N
science 50.13 90.63 91.90 91.65 96.20 92.66 93.04 93.67
sports 50.13 91.08 93.34 93.71 95.10 93.09 93.71 94.97
religion 55.51 90.52 92.47 92.47 92.75 94.98 92.89 93.03
computer 50.45 85.84 86.74 87.13 90.86 89.45 86.36 88.42
Sentiment
movie 50.08 78.03 80.45 80.40 80.72 81.55 80.34 78.36
vote 58.37 73.14 72.79 72.79 73.95 73.72 66.86 73.14
Forecasting
science 50.28 64.00 66.79 66.23 67.71 66.42 69.02 69.39
bill 87.40 88.36 87.70 88.48 88.11 87.98 88.20 88.27
Table 3:
Classification
accuracies on
various datasets.
?m.f.c.? is the
most frequent
class baseline.
Boldface shows
best results.
Task Dataset
Model size (%)
m.f.c. lasso ridge elastic sentence parse Brown LDA
20N
science - 1 100 34 12 2 42 9
sports - 2 100 15 3 3 16 9
religion - 0.3 100 48 94 72 41 15
computer - 2 100 24 10 5 24 8
Sentiment
movie - 10 100 54 83 87 59 12
vote - 2 100 44 6 2 30 4
Forecasting
science - 31 100 43 99 9 50 90
bill - 7 100 7 8 37 7 7
Table 4: Model
sizes (percentages
of nonzero
features in the
resulting models)
on various
datasets.
Dataset
+ LDA features LDA
lasso ridge elastic reg.
science 90.63 91.90 91.90 93.67
sports 91.33 93.47 93.84 94.97
religion 91.35 92.47 91.35 93.03
computer 85.20 86.87 86.35 88.42
Dataset
+ Brown features Brown
lasso ridge elastic reg.
science 86.96 90.51 91.14 93.04
sports 82.66 88.94 85.43 93.71
religion 94.98 96.93 96.93 92.89
computer 55.72 96.65 67.57 86.36
Table 5: Classification accuracies on the 20N datasets for
lasso, ridge, and elastic net models with additional LDA fea-
tures (top) and Brown cluster features (bottom). The last col-
umn shows structured regularized models from Table 3.
and treat them as regular features (i.e., they do
not belong to any groups and are regularized with
standard regularizer such as the lasso penalty).
The results in Table 5 show that for these datasets,
models that incorporate this information through
structured regularizers outperformed models that
encode this information as additional features in
4 out 4 of cases (LDA) and 2 out of 4 cases
(Brown). Sparse models with Brown clusters ap-
pear to overfit badly; recall that the clusters were
learned on only the training data?clusters from
a larger dataset would likely give stronger re-
sults. Of course, better performance might also
be achieved by incorporating new features as well
as using structured regularizers.
6.4 Examples
To gain an insight into the models, we inspect
group sparsity patterns in the learned models by
looking at the parameter copies v. This lets us see
which groups are considered important (i.e., ?se-
lected? vs. ?removed?). For each of the proposed
regularizers, we inspect the model a task in which
it performed well.
For the parse tree regularizer, we inspect the
model for the 20N:religion task. We observed that
the model included most of the sentences (root
node groups), but in some cases removed phrases
from the parse trees, such as ozzy osbourne in the
sentence ozzy osbourne , ex-singer and main char-
acter of the black sabbath of good ole days past ,
is and always was a devout catholic .
For the LDA regularizer, we inspect zero and
nonzero groups (topics) in the forecasting scien-
tific articles task. In this task, we observed that
642 out of 1,000 topics are driven to zero by
our model. Table 6 shows examples of zero and
nonzero topics for the dev.-tuned hyperparameter
values. We can see that in this particular case, the
model kept meaningful topics such as parsing and
speech processing, and discarded general topics
that are not correlated with the content of the pa-
pers (e.g., acknowledgment, document metadata,
equation, etc.). Note that most weights for non-
selected groups, even in w, are near zero.
For the Brown cluster regularizer, we inspect
the model from the 20N:science task. 771 out
of 5,775 groups were driven to zero for the best
model tuned on the development set. Examples
of zero and nonzero groups are shown in Ta-
ble 7. Similar to the LDA example, the groups
that were driven to zero tend to contain generic
words that are not relevant to the predictions. We
can also see the tree structure effect in the regu-
larizer. The group {underwater, industrial} was
793
= 0
?acknowledgment?: workshop arpa program session darpa research papers spoken technology systems
?document metadata?: university references proceedings abstract work introduction new been research both
?equation?: pr w h probability wi gram context z probabilities complete
?translation?: translation target source german english length alignment hypothesis translations position
6= 0
?translation?: korean translation english rules sentences parsing input evaluation machine verb
?speech processing?: speaker identification topic recognition recognizer models acoustic test vocabulary independent
?parsing?: parser parsing probabilistic prediction parse pearl edges chart phase theory
?classification?: documents learning accuracy bayes classification wt document naive method selection
Table 6: Examples of LDA regularizer-removed and -selected groups (in v) in the forecasting scientific articles dataset. Words
with weights (in w) of magnitude greater than 10
?3
are highlighted in red (not cited) and blue (cited).
= 0
underwater industrial
spotted hit reaped rejuvenated destroyed stretched undertake shake run
seeing developing tingles diminishing launching finding investigating receiving
maintaining
adds engage explains builds
6= 0
failure reproductive ignition reproduction
cyanamid planetary nikola fertility astronomical geophysical # lunar cometary
supplying astronautical
magnetic atmospheric
std underwater hpr wordscan exclusively aneutronic industrial peoples obsessive
congenital rare simple bowel hereditary breast
Table 7: Examples of Brown
regularizer-removed and
-selected groups (in v) in the
20N:science task. # denotes
any numeral. Words with
weights (in w) of magnitude
greater than 10
?3
are
highlighted in red (space) and
blue (medical).
driven to zero, but not once it combined with other
words such as hpr, std, obsessive. Note that we
ran Brown clustering only on the training docu-
ments; running it on a larger collection of (unla-
beled) documents relevant to the prediction task
(i.e., semi-supervised learning) is worth exploring
in future work.
7 Related and Future Work
Overall, our results demonstrate that linguistic
structure in the data can be used to improve bag-
of-words models, through structured regulariza-
tion. State-of-the-art approaches to some of these
problems have used additional features and repre-
sentations (Yessenalina et al, 2010; Socher et al,
2013). For example, for the vote sentiment analy-
sis datasets, latent variable models of Yessenalina
et al (2010) achieved a superior result of 77.67%.
To do so, they sacrificed convexity and had to rely
on side information for initialization. Our exper-
imental focus has been on a controlled compari-
son between regularizers for a fixed model family
(the simplest available, linear with bag-of-words
features). However, the improvements offered by
our regularization methods can be applied in fu-
ture work to other model families with more care-
fully engineered features, metadata features (espe-
cially important in forecasting), latent variables,
etc. In particular, note that other kinds of weights
(e.g., metadata) can be penalized conventionally,
or incorporated into the structured regularization
where it makes sense to do so (e.g., n-grams, as in
Nelakanti et al, 2013).
8 Conclusion
We introduced three data-driven, linguistically
informed structured regularizers based on parse
trees, topics, and hierarchical word clusters. We
empirically showed that models regularized us-
ing our methods consistently outperformed stan-
dard regularizers that penalize features in isolation
such as lasso, ridge, and elastic net on a range
of datasets for various text prediction problems:
topic classification, sentiment analysis, and fore-
casting.
Acknowledgments
The authors thank Brendan O?Connor for help
with visualization and three anonymous review-
ers for helpful feedback on an earlier draft of this
paper. This research was supported in part by
computing resources provided by a grant from the
Pittsburgh Supercomputing Center, a Google re-
search award, and the Intelligence Advanced Re-
search Projects Activity via Department of In-
terior National Business Center contract number
D12PC00347. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. The views and conclusions
contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either ex-
pressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
794
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467?479.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for me models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Car-
bonell, and Eric P. Xing. 2011. Smoothing prox-
imal gradient method for general structured sparse
learning. In Proc. of UAI.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. of ACL.
Mario A. T. Figueiredo. 2002. Adaptive sparseness
using Jeffreys? prior. In Proc. of NIPS.
Jerome Friedman, Trevor Hastie, and Robert Tibshiran.
2010. A note on the group lasso and a sparse group
lasso. Technical report, Stanford University.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc. of
ICML.
Magnus R. Hestenes. 1969. Multiplier and gradient
methods. Journal of Optimization Theory and Ap-
plications, 4:303?320.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55?67.
Laurent Jacob, Guillaume Obozinski, and Jean-
Philippe Vert. 2009. Group lasso with overlap and
graph lasso. In Proc. of ICML.
Rodolphe Jenatton, Jean-Yves Audibert, and Fran-
cis Bach. 2011. Structured variable selection
with sparsity-inducing norms. Journal of Machine
Learning Research, 12:2777?2824.
Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A. Smith. 2010. Movie reviews and rev-
enues: An experiment in text regression. In Proc.
of NAACL.
Seyoung Kim and Eric P. Xing. 2008. Feature selec-
tion via block-regularized regression. In Proc. of
UAI.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc.
of HLT-NAACL.
Matthieu Kowalski and Bruno Torresani. 2009. Spar-
sity and persistence: mixed norms provide simple
signal models with dependent coefficients. Signal,
Image and Video Processing, 3(3):251?0264.
Xiaolei Lv, Guoan Bi, and Chunru Wan. 2011. The
group lasso for stable recovery of block-sparse sig-
nal representations. IEEE Transactions on Signal
Processing, 59(4):1371?1382.
Andre F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Mario A. T. Figueiredo. 2011. Struc-
tured sparsity in structured prediction. In Proc. of
EMNLP.
Anil Nelakanti, Cedric Archambeau, Julien Mairal,
Francis Bach, and Guillaume Bouchard. 2013.
Structured penalties for log-linear language models.
In Proc. of EMNLP.
Kamal Nigam, Andrew McCallum, Sebastian Thrun,
and Tom Mitchell. 2000. Text classification from la-
beled and unlabeled documents using em. Machine
Learning, 39(2-3):103?134.
Bo Pang and Lilian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of HLT-NAACL.
M. J. D. Powell. 1969. A method for nonlinear con-
straints in minimization problems. In R. Fletcher,
editor, Optimization, pages 283?298. Academic
Press.
Zhiwei (Tony) Qin and Donald Goldfarb. 2012. Struc-
tured sparsity via alternating direction methods.
Journal of Machine Learning Research, 13:1435?
1468.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology net-
work corpus. In Proc. of ACL Workshop on Natural
Language Processing and Information Retrieval for
Digital Libraries.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Chris Manning, Andrew Ng, and Chris
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Proc.
of EMNLP.
Jiang Su, Jelber Sayyad-Shirabad, and Stan Matwin.
2011. Large scale text classication using semi-
supervised multinomial naive Bayes. In Proc. of
ICML.
Matt Thomas, Bo Pang, and Lilian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proc. of
EMNLP.
795
Robert Tibshirani, Michael Saunders, Saharon Ros-
set, Ji Zhu, and Keith Knight. 2005. Sparsity and
smoothness via the fused lasso. Journal of Royal
Statistical Society B, 67(1):91?108.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of Royal Statistical
Society B, 58(1):267?288.
Tae Yano, Noah A. Smith, and John D. Wilkerson.
2012. Textual predictors of bill survival in congres-
sional committees. In Proc. of NAACL.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document
sentiment classification. In Proc. of EMNLP.
Dani Yogatama and Noah A. Smith. 2014. Making the
most of bag of words: Sentence regularization with
alternating direction method of multipliers. In Proc.
of ICML.
Dani Yogatama, Michael Heilman, Brendan O?Connor,
Chris Dyer, Bryan R. Routledge, and Noah A.
Smith. 2011. Predicting a scientific community?s
response to an article. In Proc. of EMNLP.
Ming Yuan and Yi Lin. 2006. Model selection
and estimation in regression with grouped variables.
Journal of the Royal Statistical Society, Series B,
68(1):49?67.
Lei Yuan, Jun Liu, and Jieping Ye. 2013. Efficient
methods for overlapping group lasso. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 35(9):2104?2116.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. Journal of the
Royal Statistical Society, Series B, 67:301?320.
796
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1426?1436,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Discriminative Graph-Based Parser
for the Abstract Meaning Representation
Jeffrey Flanigan Sam Thomson Jaime Carbonell Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jflanigan,sthomson,jgc,cdyer,nasmith}@cs.cmu.edu
Abstract
Abstract Meaning Representation (AMR)
is a semantic formalism for which a grow-
ing set of annotated examples is avail-
able. We introduce the first approach
to parse sentences into this representa-
tion, providing a strong baseline for fu-
ture improvement. The method is based
on a novel algorithm for finding a maxi-
mum spanning, connected subgraph, em-
bedded within a Lagrangian relaxation of
an optimization problem that imposes lin-
guistically inspired constraints. Our ap-
proach is described in the general frame-
work of structured prediction, allowing fu-
ture incorporation of additional features
and constraints, and may extend to other
formalisms as well. Our open-source sys-
tem, JAMR, is available at:
http://github.com/jflanigan/jamr
1 Introduction
Semantic parsing is the problem of mapping nat-
ural language strings into meaning representa-
tions. Abstract Meaning Representation (AMR)
(Banarescu et al, 2013; Dorr et al, 1998) is a
semantic formalism in which the meaning of a
sentence is encoded as a rooted, directed, acyclic
graph. Nodes represent concepts, and labeled di-
rected edges represent the relationships between
them?see Figure 1 for an example AMR graph.
The formalism is based on propositional logic and
neo-Davidsonian event representations (Parsons,
1990; Davidson, 1967). Although it does not
encode quantifiers, tense, or modality, the set of
semantic phenomena included in AMR were se-
lected with natural language applications?in par-
ticular, machine translation?in mind.
In this paper we introduce JAMR, the first pub-
lished system for automatic AMR parsing. The
system is based on a statistical model whose pa-
rameters are trained discriminatively using anno-
tated sentences in the AMR Bank corpus (Ba-
narescu et al, 2013). We evaluate using the
Smatch score (Cai and Knight, 2013), establishing
a baseline for future work.
The core of JAMR is a two-part algorithm
that first identifies concepts using a semi-Markov
model and then identifies the relations that ob-
tain between these by searching for the maximum
spanning connected subgraph (MSCG) from an
edge-labeled, directed graph representing all pos-
sible relations between the identified concepts. To
solve the latter problem, we introduce an appar-
ently novel O(|V |
2
log |V |) algorithm that is sim-
ilar to the maximum spanning tree (MST) algo-
rithms that are widely used for dependency pars-
ing (McDonald et al, 2005). Our MSCG algo-
rithm returns the connected subgraph with maxi-
mal sum of its edge weights from among all con-
nected subgraphs of the input graph. Since AMR
imposes additional constraints to ensure seman-
tic well-formedness, we use Lagrangian relaxation
(Geoffrion, 1974; Fisher, 2004) to augment the
MSCG algorithm, yielding a tractable iterative al-
gorithm that finds the optimal solution subject to
these constraints. In our experiments, we have
found this algorithm to converge 100% of the time
for the constraint set we use.
The approach can be understood as an alterna-
tive to parsing approaches using graph transduc-
ers such as (synchronous) hyperedge replacement
grammars (Chiang et al, 2013; Jones et al, 2012;
Drewes et al, 1997), in much the same way that
spanning tree algorithms are an alternative to us-
ing shift-reduce and dynamic programming algo-
rithms for dependency parsing.
1
While a detailed
1
To date, a graph transducer-based semantic
parser has not been published, although the Bolinas
toolkit (http://www.isi.edu/publications/
licensed-sw/bolinas/) contains much of the neces-
sary infrastructure.
1426
want-01
boy
visit-01
city
name
?New? ?York?
?City?
ARG0
ARG1
ARG0
ARG1
name
op1
op2 op3
(a) Graph.
(w / want-01
:ARG0 (b / boy)
:ARG1 (g / visit-01
:ARG0 b
:ARG1 (c / city
:name (n / name
:op1 "New"
:op2 "York"
:op3 "City"))))
(b) AMR annotation.
Figure 1: Two equivalent ways of representing the AMR
parse for the sentence, ?The boy wants to visit New York
City.?
comparison of these two approaches is beyond the
scope of this paper, we emphasize that?as has
been observed with dependency parsing?a diver-
sity of approaches can shed light on complex prob-
lems such as semantic parsing.
2 Notation and Overview
Our approach to AMR parsing represents an AMR
parse as a graph G = ?V,E?; vertices and edges
are given labels from sets L
V
and L
E
, respec-
tively. G is constructed in two stages. The first
stage identifies the concepts evoked by words and
phrases in an input sentence w = ?w
1
, . . . , w
n
?,
each w
i
a member of vocabulary W . The second
stage connects the concepts by adding L
E
-labeled
edges capturing the relations between concepts,
and selects a root in G corresponding to the focus
of the sentence w.
Concept identification (?3) involves segmenting
w into contiguous spans and assigning to each
span a graph fragment corresponding to a concept
from a concept set denoted F (or to ? for words
that evoke no concept). In ?5 we describe how
F is constructed. In our formulation, spans are
contiguous subsequences of w. For example, the
words ?New York City? can evoke the fragment
represented by
(c / city
:name (n / name
:op1 "New"
:op2 "York"
:op3 "City"))))
We use a sequence labeling algorithm to identify
concepts.
The relation identification stage (?4) is similar
to a graph-based dependency parser. Instead of
finding the maximum-scoring tree over words, it
finds the maximum-scoring connected subgraph
that preserves concept fragments from the first
stage, links each pair of vertices by at most one
edge, and is deterministic
2
with respect to a spe-
cial set of edge labels L
?
E
? L
E
. The set L
?
E
consists of the labels ARG0?ARG5, and does not
include labels such as MOD or MANNER, for ex-
ample. Linguistically, the determinism constraint
enforces that predicates have at most one semantic
argument of each type; this is discussed in more
detail in ?4.
To train the parser, spans of words must be la-
beled with the concept fragments they evoke. Al-
though AMR Bank does not label concepts with
the words that evoke them, it is possible to build
an automatic aligner (?5). The alignments are
used to construct the concept lexicon and to train
the concept identification and relation identifica-
tion stages of the parser (?6). Each stage is a
discriminatively-trained linear structured predic-
tor with rich features that make use of part-of-
speech tagging, named entity tagging, and depen-
dency parsing.
In ?7, we evaluate the parser against gold-
standard annotated sentences from the AMR Bank
corpus (Banarescu et al, 2013) under the Smatch
score (Cai and Knight, 2013), presenting the first
published results on automatic AMR parsing.
3 Concept Identification
The concept identification stage maps spans of
words in the input sentence w to concept graph
fragments from F , or to the empty graph fragment
?. These graph fragments often consist of just
one labeled concept node, but in some cases they
are larger graphs with multiple nodes and edges.
3
2
By this we mean that, at each node, there is at most one
outgoing edge with that label type.
3
About 20% of invoked concept fragments are multi-
concept fragments.
1427
Concept identification is illustrated in Figure 2 us-
ing our running example, ?The boy wants to visit
New York City.?
Let the concept lexicon be a mapping clex :
W
?
? 2
F
that provides candidate graph frag-
ments for sequences of words. (The construc-
tion of F and clex is discussed below.) Formally,
a concept labeling is (i) a segmentation of w
into contiguous spans represented by boundaries
b, giving spans ?w
b
0
:b
1
,w
b
1
:b
2
, . . .w
b
k?1
:b
k
?, with
b
0
= 0 and b
k
= n, and (ii) an assignment of
each phrase w
b
i?1
:b
i
to a concept graph fragment
c
i
? clex (w
b
i?1
:b
i
) ? ?.
Our approach scores a sequence of spans b and
a sequence of concept graph fragments c, both of
arbitrary length k, using the following locally de-
composed, linearly parameterized function:
score(b, c;?) =
?
k
i=1
?
>
f(w
b
i?1
:b
i
, b
i?1
, b
i
, c
i
)
(1)
where f is a feature vector representation of a span
and one of its concept graph fragments in context.
The features are:
? Fragment given words: Relative frequency es-
timates of the probability of a concept graph
fragment given the sequence of words in the
span. This is calculated from the concept-word
alignments in the training corpus (?5).
? Length of the matching span (number of to-
kens).
? NER: 1 if the named entity tagger marked the
span as an entity, 0 otherwise.
? Bias: 1 for any concept graph fragment from F
and 0 for ?.
Our approach finds the highest-scoring b and
c using a dynamic programming algorithm: the
zeroth-order case of inference under a semi-
Markov model (Janssen and Limnios, 1999). Let
S(i) denote the score of the best labeling of the
first i words of the sentence, w
0:i
; it can be calcu-
lated using the recurrence:
S(0) = 0
S(i) = max
j:0?j<i,
c?clex(w
j:i
)??
{
S(j) + ?
>
f(w
j:i
, j, i, c)
}
The best score will be S(n), and the best scor-
ing concept labeling can be recovered using back-
pointers, as in typical implementations of the
Viterbi algorithm. Runtime is O(n
2
).
clex is implemented as follows. When clex is
called with a sequence of words, it looks up the
sequence in a table that contains, for every word
sequence that was labeled with a concept fragment
in the training data, the set of concept fragments it
was labeled with. clex also has a set of rules for
generating concept fragments for named entities
and time expressions. It generates a concept frag-
ment for any entity recognized by the named entity
tagger, as well as for any word sequence matching
a regular expression for a time expression. clex
returns the union of all these concept fragments.
4 Relation Identification
The relation identification stage adds edges among
the concept subgraph fragments identified in the
first stage (?3), creating a graph. We frame the
task as a constrained combinatorial optimization
problem.
Consider the fully dense labeled multigraph
D = ?V
D
, E
D
? that includes the union of all la-
beled vertices and labeled edges in the concept
graph fragments, as well as every possible labeled
edge u
`
?? v, for all u, v ? V
D
and every ` ? L
E
.
4
We require a subgraph G = ?V
G
, E
G
? that re-
spects the following constraints:
1. Preserving: all graph fragments (including la-
bels) from the concept identification phase are
subgraphs of G.
2. Simple: for any two vertices u and v ? V
G
,E
G
includes at most one edge between u and v. This
constraint forbids a small number of perfectly
valid graphs, for example for sentences such as
?John hurt himself?; however, we see that< 1%
of training instances violate the constraint. We
found in preliminary experiments that including
the constraint increases overall performance.
5
3. Connected: G must be weakly connected (ev-
ery vertex reachable from every other vertex, ig-
noring the direction of edges). This constraint
follows from the formal definition of AMR and
is never violated in the training data.
4. Deterministic: For each node u ? V
G
, and for
each label ` ? L
?
E
, there is at most one outgoing
edge in E
G
from u with label `. As discussed in
?2, this constraint is linguistically motivated.
4
To handle numbered OP labels, we pre-process the train-
ing data to convert OPN to OP, and post-process the output by
numbering the OP labels sequentially.
5
In future work it might be treated as a soft constraint, or
the constraint might be refined to specific cases.
1428
The
boy
wants to
visit New York
City
? ?
boy
want-01
visit-01
city
name
?New?
?York?
?City?
name
op1
op2
op3
Figure 2: A concept labeling for the sentence ?The boy wants to visit New York City.?
One constraint we do not include is acyclicity,
which follows from the definition of AMR. In
practice, graphs with cycles are rarely produced
by JAMR. In fact, none of the graphs produced on
the test set violate acyclicity.
Given the constraints, we seek the maximum-
scoring subgraph. We define the score to decom-
pose by edges, and with a linear parameterization:
score(E
G
;?) =
?
e?E
G
?
>
g(e) (2)
The features are shown in Table 1.
Our solution to maximizing the score in Eq. 2,
subject to the constraints, makes use of (i) an al-
gorithm that ignores constraint 4 but respects the
others (?4.1); and (ii) a Lagrangian relaxation that
iteratively adjusts the edge scores supplied to (i)
so as to enforce constraint 4 (?4.2).
4.1 Maximum Preserving, Simple, Spanning,
Connected Subgraph Algorithm
The steps for constructing a maximum preserving,
simple, spanning, connected (but not necessar-
ily deterministic) subgraph are as follows. These
steps ensure the resulting graph G satisfies the
constraints: the initialization step ensures the pre-
serving constraint is satisfied, the pre-processing
step ensures the graph is simple, and the core al-
gorithm ensures the graph is connected.
1. (Initialization) Let E
(0)
be the union of the
concept graph fragments? weighted, labeled, di-
rected edges. Let V denote its set of vertices.
Note that ?V,E
(0)
? is preserving (constraint 4),
as is any graph that contains it. It is also sim-
ple (constraint 4), assuming each concept graph
fragment is simple.
2. (Pre-processing) We form the edge set E by in-
cluding just one edge from E
D
between each
pair of nodes:
? For any edge e = u
`
?? v in E
(0)
, include e in
E, omitting all other edges between u and v.
? For any two nodes u and v, include only the
highest scoring edge between u and v.
Note that without the deterministic constraint,
we have no constraints that depend on the label
of an edge, nor its direction. So it is clear that
the edges omitted in this step could not be part
of the maximum-scoring solution, as they could
be replaced by a higher scoring edge without vi-
olating any constraints.
Note also that because we have kept exactly one
edge between every pair of nodes, ?V,E? is sim-
ple and connected.
3. (Core algorithm) Run Algorithm 1, MSCG, on
?V,E? and E
(0)
. This algorithm is a (to our
knowledge novel) modification of the minimum
spanning tree algorithm of Kruskal (1956).
Note that the directions of edges do not matter
for MSCG.
Steps 1?2 can be accomplished in one pass
through the edges, with runtime O(|V |
2
). MSCG
can be implemented efficiently in O(|V |
2
log |V |)
time, similarly to Kruskal?s algorithm, using a
disjoint-set data structure to keep track of con-
nected components.
6
The total asymptotic runtime
complexity is O(|V |
2
log |V |).
The details of MSCG are given in Algorithm 1.
In a nutshell, MSCG first adds all positive edges to
the graph, and then connects the graph by greedily
adding the least negative edge that connects two
previously unconnected components.
Theorem 1. MSCG finds a maximum spanning,
connected subgraph of ?V,E?
Proof. We closely follow the original proof of cor-
rectness of Kruskal?s algorithm. We first show by
induction that, at every iteration of MSCG, there
exists some maximum spanning, connected sub-
graph that contains G
(i)
= ?V,E
(i)
?:
6
For dense graphs, Prim?s algorithm (Prim, 1957) is
asymptotically faster (O(|V |
2
)). We conjecture that using
Prim?s algorithm instead of Kruskall?s to connect the graph
could improve the runtime of MSCG.
1429
Name Description
Label For each ` ? L
E
, 1 if the edge has that label
Self edge 1 if the edge is between two nodes in the same fragment
Tail fragment root 1 if the edge?s tail is the root of its graph fragment
Head fragment root 1 if the edge?s head is the root of its graph fragment
Path Dependency edge labels and parts of speech on the shortest syntactic path between any two
words in the two spans
Distance Number of tokens (plus one) between the two concepts? spans (zero if the same)
Distance indicators A feature for each distance value, that is 1 if the spans are of that distance
Log distance Logarithm of the distance feature plus one.
Bias 1 for any edge.
Table 1: Features used in relation identification. In addition to the features above, the following conjunctions are used (Tail and
Head concepts are elements of L
V
): Tail concept ? Label, Head concept ? Label, Path ? Label, Path ? Head concept, Path ?
Tail concept, Path ? Head concept ? Label, Path ? Tail concept ? Label, Path ? Head word, Path ? Tail word, Path ? Head
word ? Label, Path ? Tail word ? Label, Distance ? Label, Distance ? Path, and Distance ? Path ? Label. To conjoin the
distance feature with anything else, we multiply by the distance.
input : weighted, connected graph ?V,E?
and set of edges E
(0)
? E to be
preserved
output: maximum spanning, connected
subgraph of ?V,E? that preserves
E
(0)
let E
(1)
= E
(0)
? {e ? E | ?
>
g(e) > 0};
create a priority queue Q containing
{e ? E | ?
>
g(e) ? 0} prioritized by scores;
i = 1;
while Q nonempty and ?V,E
(i)
? is not yet
spanning and connected do
i = i+ 1;
E
(i)
= E
(i?1)
;
e = argmax
e
?
?Q
?
>
g(e
?
);
remove e from Q;
if e connects two previously unconnected
components of ?V,E
(i)
? then
add e to E
(i)
end
end
return G = ?V,E
(i)
?;
Algorithm 1: MSCG algorithm.
Base case: ConsiderG
(1)
, the subgraph contain-
ing E
(0)
and every positive edge. Take any maxi-
mum preserving spanning connected subgraph M
of ?V,E?. We know that such an M exists be-
cause ?V,E? itself is a preserving spanning con-
nected subgraph. Adding a positive edge to M
would strictly increase M ?s score without discon-
necting M , which would contradict the fact that
M is maximal. Thus M must contain G
(1)
.
Induction step: By the inductive hypothesis,
there exists some maximum spanning connected
subgraph M = ?V,E
M
? that contains G
(i)
.
Let e be the next edge added to E
(i)
by MSCG.
If e is in E
M
, then E
(i+1)
= E
(i)
? {e} ? E
M
,
and the hypothesis still holds.
Otherwise, since M is connected and does not
contain e, E
M
? {e} must have a cycle containing
e. In addition, that cycle must have some edge e
?
that is not in E
(i)
. Otherwise, E
(i)
? {e} would
contain a cycle, and e would not connect two un-
connected components of G
(i)
, contradicting the
fact that e was chosen by MSCG.
Since e
?
is in a cycle in E
M
? {e}, removing it
will not disconnect the subgraph, i.e. (E
M
?{e})\
{e
?
} is still connected and spanning. The score of
e is greater than or equal to the score of e
?
, oth-
erwise MSCG would have chosen e
?
instead of e.
Thus, ?V, (E
M
?{e}) \ {e
?
}? is a maximum span-
ning connected subgraph that containsE
(i+1)
, and
the hypothesis still holds.
When the algorithm completes, G = ?V,E
(i)
?
is a spanning connected subgraph. The maximum
spanning connected subgraph M that contains it
cannot have a higher score, because G contains
every positive edge. Hence G is maximal.
4.2 Lagrangian Relaxation
If the subgraph resulting from MSCG satisfies con-
straint 4 (deterministic) then we are done. Oth-
erwise we resort to Lagrangian relaxation (LR).
Here we describe the technique as it applies to our
task, referring the interested reader to Rush and
Collins (2012) for a more general introduction to
Lagrangian relaxation in the context of structured
prediction problems.
In our case, we begin by encoding a graph G =
?V
G
, E
G
? as a binary vector. For each edge e in
the fully dense multigraph D, we associate a bi-
1430
nary variable z
e
= 1{e ? E
G
}, where 1{P} is
the indicator function, taking value 1 if the propo-
sition P is true, 0 otherwise. The collection of z
e
form a vector z ? {0, 1}
|E
D
|
.
Determinism constraints can be encoded as a
set of linear inequalities. For example, the con-
straint that vertex u has no more than one outgoing
ARG0 can be encoded with the inequality:
?
v?V
1{u
ARG0
???? v ? E
G
} =
?
v?V
z
u
ARG0
????v
? 1.
All of the determinism constraints can collectively
be encoded as one system of inequalities:
Az ? b,
with each row A
i
inA and its corresponding entry
b
i
in b together encoding one constraint. For the
previous example we have a row A
i
that has 1s
in the columns corresponding to edges outgoing
from u with label ARG0 and 0?s elsewhere, and a
corresponding element b
i
= 1 in b.
The score of graph G (encoded as z) can be
written as the objective function ?
>
z, where ?
e
=
?
>
g(e). To handle the constraint Az ? b, we in-
troduce multipliers ? ? 0 to get the Lagrangian
relaxation of the objective function:
L?(z) = maxz (?
>
z+ ?
>
(b?Az)),
z
?
? = argmaxz L?(z).
And the dual objective:
L(z) = min
??0
L?(z),
z
?
= argmax
z
L(z).
Conveniently, L?(z) decomposes over edges:
L?(z) = maxz (?
>
z+ ?
>
(b?Az))
= max
z
(?
>
z? ?
>
Az)
= max
z
((??A
>
?)
>
z).
So for any ?, we can find z
?
?
by assigning edges
the new Lagrangian adjusted weights ? ? A
>
?
and reapplying the algorithm described in ?4.1.
We can find z
?
by projected subgradient descent,
by starting with ? = 0, and taking steps in the
direction:
?
?L?
??
(z
?
?) = Az
?
?.
If any components of ? are negative after taking a
step, they are set to zero.
L(z) is an upper bound on the unrelaxed ob-
jective function ?
>
z, and is equal to it if and
only if the constraints Az ? b are satisfied. If
L(z
?
) = ?
>
z
?
, then z
?
is also the optimal solu-
tion to the constrained solution. Otherwise, there
exists a duality gap, and Lagrangian relaxation
has failed. In that case we still return the sub-
graph encoded by z
?
, even though it might vio-
late one or more constraints. Techniques from in-
teger programming such as branch-and-bound or
cutting-planes methods could be used to find an
optimal solution when LR fails (Das et al, 2012),
but we do not use these techniques here. In our
experiments, with a stepsize of 1 and max number
of steps as 500, Lagrangian relaxation succeeds
100% of the time in our data.
4.3 Focus Identification
In AMR, one node must be marked as the focus of
the sentence. We notice this can be accomplished
within the relation identification step: we add a
special concept node root to the dense graph D,
and add an edge from root to every other node,
giving each of these edges the label FOCUS. We
require that root have at most one outgoing FO-
CUS edge. Our system has two feature types for
this edge: the concept it points to, and the shortest
dependency path from a word in the span to the
root of the dependency tree.
5 Automatic Alignments
In order to train the parser, we need alignments be-
tween sentences in the training data and their an-
notated AMR graphs. More specifically, we need
to know which spans of words invoke which con-
cept fragments in the graph. To do this, we built
an automatic aligner and tested its performance on
a small set of alignments we annotated by hand.
The automatic aligner uses a set of rules to
greedily align concepts to spans. The list of rules
is given in Table 2. The aligner proceeds down
the list, first aligning named-entities exactly, then
fuzzy matching named-entities, then date-entities,
etc. For each rule, an entire pass through the AMR
graph is done. The pass considers every concept in
the graph and attempts to align a concept fragment
rooted at that concept if the rule can apply. Some
rules only apply to a particular type of concept
fragment, while others can apply to any concept.
For example, rule 1 can apply to any NAME con-
cept and its OP children. It searches the sentence
1431
for a sequence of words that exactly matches its
OP children and aligns them to the NAME and OP
children fragment.
Concepts are considered for alignment in the or-
der they are listed in the AMR annotation (left to
right, top to bottom). Concepts that are not aligned
in a particular pass may be aligned in subsequent
passes. Concepts are aligned to the first match-
ing span, and alignments are mutually exclusive.
Once aligned, a concept in a fragment is never re-
aligned.
7
However, more concepts can be attached
to the fragment by rules 8?14.
We use WordNet to generate candidate lemmas,
and we also use a fuzzy match of a concept, de-
fined to be a word in the sentence that has the
longest string prefix match with that concept?s la-
bel, if the match length is ? 4. If the match length
is < 4, then the concept has no fuzzy match. For
example the fuzzy match for ACCUSE-01 could be
?accusations? if it is the best match in the sen-
tence. WordNet lemmas and fuzzy matches are
only used if the rule explicitly uses them. All to-
kens and concepts are lowercased before matches
or fuzzy matches are done.
On the 200 sentences of training data we
aligned by hand, the aligner achieves 92% preci-
sion, 89% recall, and 90% F
1
for the alignments.
6 Training
We now describe how to train the two stages of the
parser. The training data for the concept identifi-
cation stage consists of (X,Y ) pairs:
? Input: X , a sentence annotated with named
entities (person, organization, location, mis-
ciscellaneous) from the Illinois Named Entity
Tagger (Ratinov and Roth, 2009), and part-of-
speech tags and basic dependencies from the
Stanford Parser (Klein and Manning, 2003; de
Marneffe et al, 2006).
? Output: Y , the sentence labeled with concept
subgraph fragments.
The training data for the relation identification
stage consists of (X,Y ) pairs:
7
As an example, if ?North Korea? shows up twice in
the AMR graph and twice in the input sentence, then the
first ?North Korea? concept fragment listed in the AMR gets
aligned to the first ?North Korea? mention in the sentence,
and the second fragment to the second mention (because the
first span is already aligned when the second ?North Korea?
concept fragment is considered, so it is aligned to the second
matching span).
1. (Named Entity) Applies to name concepts and their
opn children. Matches a span that exactly matches its
opn children in numerical order.
2. (Fuzzy Named Entity) Applies to name concepts and
their opn children. Matches a span that matches the
fuzzy match of each child in numerical order.
3. (Date Entity) Applies to date-entity concepts
and their day, month, year children (if exist).
Matches any permutation of day, month, year, (two digit
or four digit years), with or without spaces.
4. (Minus Polarity Tokens) Applies to - concepts, and
matches ?no?, ?not?, ?non.?
5. (Single Concept) Applies to any concept. Strips
off trailing ?-[0-9]+? from the concept (for example
run-01 ? run), and matches any exact matching
word or WordNet lemma.
6. (Fuzzy Single Concept) Applies to any concept.
Strips off trailing ?-[0-9]+?, and matches the fuzzy match
of the concept.
7. (U.S.) Applies to name if its op1 child is united
and its op2 child is states. Matches a word that
matches ?us?, ?u.s.? (no space), or ?u. s.? (with space).
8. (Entity Type) Applies to concepts with an outgoing
name edge whose head is an aligned fragment. Up-
dates the fragment to include the unaligned concept.
Ex: continent in (continent :name (name
:op1 "Asia")) aligned to ?asia.?
9. (Quantity) Applies to .
*
-quantity concepts with
an outgoing unit edge whose head is aligned. Up-
dates the fragment to include the unaligned concept. Ex:
distance-quantity in (distance-quantity
:unit kilometer) aligned to ?kilometres.?
10. (Person-Of, Thing-Of) Applies to person and
thing concepts with an outgoing .
*
-of edge whose
head is aligned. Updates the fragment to include
the unaligned concept. Ex: person in (person
:ARG0-of strike-02) aligned to ?strikers.?
11. (Person) Applies to person concepts with a sin-
gle outgoing edge whose head is aligned. Updates
the fragment to include the unaligned concept. Ex:
person in (person :poss (country :name
(name :op1 "Korea")))
12. (Goverment Organization) Applies to concepts
with an incoming ARG.
*
-of edge whose tail is an
aligned government-organization concept. Up-
dates the fragment to include the unaligned concept. Ex:
govern-01 in (government-organization
:ARG0-of govern-01) aligned to ?government.?
13. (Minus Polarity Prefixes) Applies to - concepts
with an incoming polarity edge whose tail is aligned
to a word beginning with ?un?, ?in?, or ?il.? Up-
dates the fragment to include the unaligned concept.
Ex: - in (employ-01 :polarity -) aligned to
?unemployment.?
14. (Degree) Applies to concepts with an incoming
degree edge whose tail is aligned to a word ending
is ?est.? Updates the fragment to include the unaligned
concept. Ex: most in (large :degree most)
aligned to ?largest.?
Table 2: Rules used in the automatic aligner.
1432
? Input: X , the sentence labeled with graph frag-
ments, as well as named enties, POS tags, and
basic dependencies as in concept identification.
? Output: Y , the sentence with a full AMR
parse.
8
Alignments are used to induce the concept label-
ing for the sentences, so no annotation beyond the
automatic alignments is necessary.
We train the parameters of the stages separately
using AdaGrad (Duchi et al, 2011) with the per-
ceptron loss function (Rosenblatt, 1957; Collins,
2002). We give equations for concept identifica-
tion parameters ? and features f(X,Y ). For a
sentence of length k, and spans b labeled with a
sequence of concept fragments c, the features are:
f(X,Y ) =
?
k
i=1
f(w
b
i?1
:b
i
, b
i?1
, b
i
, c
i
)
To train with AdaGrad, we process examples in
the training data ((X
1
, Y
1
), . . . , (X
N
, Y
N
)) one
at a time. At time t, we decode (?3) to get
?
Y
t
and
compute the subgradient:
s
t
= f(X
t
,
?
Y
t
)? f(X
t
, Y
t
)
We then update the parameters and go to the next
example. Each component i of the parameter vec-
tor gets updated like so:
?
t+1
i
= ?
t
i
?
?
?
?
t
t
?
=1
s
t
?
i
s
t
i
? is the learning rate which we set to 1. For
relation identification training, we replace ? and
f(X,Y ) in the above equations with ? and
g(X,Y ) =
?
e?E
G
g(e).
We ran AdaGrad for ten iterations for concept
identification, and five iterations for relation iden-
tification. The number of iterations was chosen by
early stopping on the development set.
7 Experiments
We evaluate our parser on the newswire section
of LDC2013E117 (deft-amr-release-r3-proxy.txt).
Statistics about this corpus and our train/dev./test
splits are given in Table 3.
8
Because the alignments are automatic, some concepts
may not be aligned, so we cannot compute their features. We
remove the unaligned concepts and their edges from the full
AMR graph for training. Thus some graphs used for training
may in fact be disconnected.
Split Document Years Sentences Tokens
Train 1995-2006 4.0k 79k
Dev. 2007 2.1k 40k
Test 2008 2.1k 42k
Table 3: Train/dev./test split.
Train Test
P R F
1
P R F
1
.92 .90 .91 .90 .79 .84
Table 4: Concept identification performance.
For the performance of concept identification,
we report precision, recall, and F
1
of labeled spans
using the induced labels on the training and test
data as a gold standard (Table 4). Our concept
identifier achieves 84% F
1
on the test data. Pre-
cision is roughly the same between train and test,
but recall is worse on test, implicating unseen con-
cepts as a significant source of errors on test data.
We evaluate the performance of the full parser
using Smatch v1.0 (Cai and Knight, 2013), which
counts the precision, recall and F
1
of the concepts
and relations together. Using the full pipeline
(concept identification and relation identification
stages), our parser achieves 58% F
1
on the test
data (Table 5). Using gold concepts with the re-
lation identification stage yields a much higher
Smatch score of 80% F
1
. As a comparison, AMR
Bank annotators have a consensus inter-annotator
agreement Smatch score of 83% F
1
. The runtime
of our system is given in Figure 3.
The large drop in performance of 22% F
1
when
moving from gold concepts to system concepts
suggests that joint inference and training for the
two stages might be helpful.
8 Related Work
Our approach to relation identification is inspired
by graph-based techniques for non-projective syn-
tactic dependency parsing. Minimum span-
ning tree algorithms?specifically, the optimum
branching algorithm of Chu and Liu (1965) and
Edmonds (1967)?were first used for dependency
parsing by McDonald et al (2005). Later ex-
Train Test
concepts P R F
1
P R F
1
gold .85 .95 .90 .76 .84 .80
automatic .69 .78 .73 .52 .66 .58
Table 5: Parser performance.
1433
0 10 20 30 400.0
0.1
0.2
0.3
0.4
0.5
sentence length (words)
aver
age r
untim
e (seco
nds)
Figure 3: Runtime of JAMR (all stages).
tensions allow for higher-order (non?edge-local)
features, often making use of relaxations to solve
the NP-hard optimization problem. Mcdonald and
Pereira (2006) incorporated second-order features,
but resorted to an approximate algorithm. Oth-
ers have formulated the problem as an integer lin-
ear program (Riedel and Clarke, 2006; Martins et
al., 2009). TurboParser (Martins et al, 2013) uses
AD
3
(Martins et al, 2011), a type of augmented
Lagrangian relaxation, to integrate third-order fea-
tures into a CLE backbone. Future work might ex-
tend JAMR to incorporate additional linguistically
motivated constraints and higher-order features.
The task of concept identification is similar in
form to the problem of Chinese word segmenta-
tion, for which semi-Markov models have success-
fully been used to incorporate features based on
entire spans (Andrew, 2006).
While all semantic parsers aim to transform nat-
ural language text to a formal representation of
its meaning, there is wide variation in the mean-
ing representations and parsing techniques used.
Space does not permit a complete survey, but we
note some connections on both fronts.
Interlinguas (Carbonell et al, 1992) are an im-
portant precursor to AMR. Both formalisms are
intended for use in machine translation, but AMR
has an admitted bias toward the English language.
First-order logic representations (and exten-
sions using, e.g., the ?-calculus) allow variable
quantification, and are therefore more power-
ful. In recent research, they are often associ-
ated with combinatory categorial grammar (Steed-
man, 1996). There has been much work on sta-
tistical models for CCG parsing (Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Kwiatkowski et al, 2010, inter alia), usually using
chart-based dynamic programming for inference.
Natural language interfaces for querying
databases have served as another driving applica-
tion (Zelle and Mooney, 1996; Kate et al, 2005;
Liang et al, 2011, inter alia). The formalisms
used here are richer in logical expressiveness than
AMR, but typically use a smaller set of concept
types?only those found in the database.
In contrast, semantic dependency parsing?in
which the vertices in the graph correspond to the
words in the sentence?is meant to make semantic
parsing feasible for broader textual domains. Al-
shawi et al (2011), for example, use shift-reduce
parsing to map sentences to natural logical form.
AMR parsing also shares much in common
with tasks like semantic role labeling and frame-
semantic parsing (Gildea and Jurafsky, 2002; Pun-
yakanok et al, 2008; Das et al, 2014, inter alia).
In these tasks, predicates are often disambiguated
to a canonical word sense, and roles are filled
by spans (usually syntactic constituents). They
consider each predicate separately, and produce
a disconnected set of shallow predicate-argument
structures. AMR, on the other hand, canonical-
izes both predicates and arguments to a common
concept label space. JAMR reasons about all con-
cepts jointly to produce a unified representation of
the meaning of an entire sentence.
9 Conclusion
We have presented the first published system for
automatic AMR parsing, and shown that it pro-
vides a strong baseline based on the Smatch eval-
uation metric. We also present an algorithm for
finding the maximum, spanning, connected sub-
graph and show how to incorporate extra con-
straints with Lagrangian relaxation. Our feature-
based learning setup allows the system to be easily
extended by incorporating new feature sources.
Acknowledgments
The authors gratefully acknowledge helpful cor-
respondence from Kevin Knight, Ulf Hermjakob,
and Andr?e Martins, and helpful feedback from
Nathan Schneider, Brendan O?Connor, Waleed
Ammar, and the anonymous reviewers. This
work was sponsored by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533
and DARPA grant FA8750-12-2-0342 funded un-
der the DEFT program.
1434
References
Hiyan Alshawi, Pi-Chuan Chang, and Michael Ring-
gaard. 2011. Deterministic statistical mapping of
sentences to underspecified semantics. In Proc. of
ICWS.
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Proc. of EMNLP.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proc. of the Linguistic Annota-
tion Workshop and Interoperability with Discourse.
Shu Cai and Kevin Knight. 2013. Smatch: an eval-
uation metric for semantic feature structures. In
Proc. of ACL.
Jaime G. Carbonell, Teruko Mitamura, and Eric H. Ny-
berg. 1992. The KANT perspective: A critique
of pure transfer (and pure interlingua, pure trans-
fer, . . . ). In Proc. of the Fourth International Con-
ference on Theoretical and Methodological Issues
in Machine Translation: Empiricist vs. Rationalist
Methods in MT.
David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proc. of ACL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Dipanjan Das, Andr?e F. T. Martins, and Noah A. Smith.
2012. An exact dual decomposition algorithm for
shallow semantic parsing with constraints. In Proc.
of the Joint Conference on Lexical and Computa-
tional Semantics.
Dipanjan Das, Desai Chen, Andr?e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics, 40(1):9?56.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, pages 81?120. Univ. of Pitts-
burgh Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of LREC.
Bonnie Dorr, Nizar Habash, and David Traum. 1998.
A thematic hierarchy for efficient generation from
lexical-conceptual structure. In David Farwell, Lau-
rie Gerber, and Eduard Hovy, editors, Machine
Translation and the Information Soup: Proc. of
AMTA.
Frank Drewes, Hans-J?org Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Handbook of Graph Grammars, pages 95?
162. World Scientific.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Jack Edmonds. 1967. Optimum branchings. National
Bureau of Standards.
Marshall L. Fisher. 2004. The Lagrangian relaxation
method for solving integer programming problems.
Management Science, 50(12):1861?1871.
Arthur M Geoffrion. 1974. Lagrangean relaxation for
integer programming. Springer.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jacques Janssen and Nikolaos Limnios. 1999. Semi-
Markov Models and Applications. Springer, Octo-
ber.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. of COLING.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to
formal languages. In Proc. of AAAI.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL.
Joseph B. Kruskal. 1956. On the shortest spanning
subtree of a graph and the traveling salesman prob-
lem. Proc. of the American Mathematical Society,
7(1):48.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proc. of EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proc. of ACL.
Andr?e F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proc. of ACL.
1435
Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M?ario A. T. Figueiredo. 2011. Dual
decomposition with many overlapping components.
In Proc. of EMNLP.
Andr?e F. T. Martins, Miguel Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective Turbo parsers. In Proc. of ACL.
Ryan Mcdonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of EACL, page 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of
EMNLP.
Terence Parsons. 1990. Events in the Semantics of En-
glish: A study in subatomic semantics. MIT Press.
Robert C. Prim. 1957. Shortest connection networks
and some generalizations. Bell System Technology
Journal, 36:1389?1401.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of CoNLL.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP.
Frank Rosenblatt. 1957. The perceptron?a perceiving
and recognizing automaton. Technical Report 85-
460-1, Cornell Aeronautical Laboratory.
Alexander M. Rush and Michael Collins. 2012. A
tutorial on dual decomposition and Lagrangian re-
laxation for inference in natural language process-
ing. Journal of Artificial Intelligence Research,
45(1):305?-362.
Mark Steedman. 1996. Surface structure and interpre-
tation. Linguistic inquiry monographs. MIT Press.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of AAAI.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of UAI.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In In Proc. of EMNLP-CoNLL.
1436
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 605?610,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Unsupervised Alignment of Privacy Policies using Hidden Markov Models
Rohan Ramanath Fei Liu Norman Sadeh Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{rrohan,feiliu,sadeh,nasmith}@cs.cmu.edu
Abstract
To support empirical study of online pri-
vacy policies, as well as tools for users
with privacy concerns, we consider the
problem of aligning sections of a thousand
policy documents, based on the issues they
address. We apply an unsupervised HMM;
in two new (and reusable) evaluations, we
find the approach more effective than clus-
tering and topic models.
1 Introduction
Privacy policy documents are verbose, often eso-
teric legal documents that many people encounter
as clients of companies that provide services on
the web. McDonald and Cranor (2008) showed
that, if users were to read the privacy policies of
every website they access during the course of a
year, they would end up spending a substantial
amount of their time doing just that and would
often still not be able to answer basic questions
about what these policies really say. Unsurpris-
ingly, many people do not read them (Federal
Trade Commission, 2012).
Such policies therefore offer an excellent op-
portunity for NLP tools that summarize or ex-
tract key information that (i) helps users under-
stand the implications of agreeing to these poli-
cies and (ii) helps legal analysts understand the
contents of these policies and make recommenda-
tions on how they can be improved or made more
clear. Past applications of NLP have sought to
parse privacy policies into machine-readable rep-
resentations (Brodie et al, 2006) or extract sub-
policies from larger documents (Xiao et al, 2012).
Machine learning has been applied to assess cer-
tain attributes of policies (Costante et al, 2012;
Ammar et al, 2012; Costante et al, 2013; Zim-
meck and Bellovin, 2013).
This paper instead analyzes policies in aggre-
gate, seeking to align sections of policies. This
task is motivated by an expectation that many poli-
cies will address similar issues,
1
such as collec-
tion of a user?s contact, location, health, and fi-
nancial information, sharing with third parties, and
deletion of data. This expectation is supported
by recommendation by privacy experts (Gellman,
2014) and policymakers (Federal Trade Commis-
sion, 2012); in the financial services sector, the
Gramm-Leach-Bliley Act requires these institu-
tions to address a specific set of issues. Aligning
policy sections is a first step toward our aforemen-
tioned summarization and extraction goals.
We present the following contributions:
? A new corpus of over 1,000 privacy policies
gathered from widely used websites, manually
segmented into subtitled sections by crowdwork-
ers (?2).
? An unsupervised approach to aligning the policy
sections based on the issues they discuss. For
example, sections that discuss ?user data on the
company?s server? should be grouped together.
The approach is inspired by the application of
hidden Markov models to sequence alignment in
computational biology (Durbin et al, 1998; ?3).
? Two reusable evaluation benchmarks for the re-
sulting alignment of policy sections (?4). We
demonstrate that our approach outperforms na??ve
methods (?5).
Our corpus and benchmarks are available at
http://usableprivacy.org/data.
2 Data Collection
We collected 1,010 unique privacy policy
documents from the top websites ranked by
Alexa.com.
2
These policies were collected during
a period of six weeks during December 2013 and
January 2014. They are a snapshot of privacy
policies of mainstream websites covering fifteen
1
Personal communication, Joel Reidenberg.
2
http://www.alexa.com
605
Business Computers Games Health
Home News Recreation Shopping
Arts Kids and Teens Reference Regional
Science Society Sports
Table 1: Fifteen website categories provided by Alexa.com.
We collect privacy policies from the top 100 websites in each.
of Alexa.com?s seventeen categories (Table 1).
3
Finding a website?s policy is not trivial. Though
many well-regulated commercial websites provide
a ?privacy? link on their homepages, not all do.
We found university websites to be exceptionally
unlikely to provide such a link. Even once the pol-
icy?s URL is identified, extracting the text presents
the usual challenges associated with scraping doc-
uments from the web. Since every site is differ-
ent in its placement of the document (e.g., buried
deep within the website, distributed across several
pages, or mingled together with Terms of Service)
and format (e.g., HTML, PDF, etc.), and since we
wish to preserve as much document structure as
possible (e.g., section labels), full automation was
not a viable solution.
We therefore crowdsourced the privacy policy
document collection using Amazon Mechanical
Turk. For each website, we created a HIT in
which a worker was asked to copy and paste the
following privacy policy-related information into
text boxes: (i) privacy policy URL; (ii) last up-
dated date (or effective date) of the current privacy
policy; (iii) privacy policy full text; and (iv) the
section subtitles in the top-most layer of the pri-
vacy policy. To identify the privacy policy URL,
workers were encouraged to go to the website and
search for the privacy link. Alternatively, they
could form a search query using the website name
and ?privacy policy? (e.g., ?Amazon.com privacy
policy?) and search in the returned results for the
most appropriate privacy policy URL. Given the
privacy policy full text and the section subtitles,
we partition the full privacy document into differ-
ent sections, delimited by the section subtitles. A
privacy policy is then converted into XML.
Each HIT was completed by three workers, paid
$0.05, for a total cost of $380 (including Ama-
zon?s surcharge).
3
The ?Adult? category was excluded; the ?World? cate-
gory was excluded since it contains mainly popular websites
in different languages, and we opted to focus on policies in
English in this first stage of research, though mulitlingual pol-
icy analysis presents interesting challenges for future work.
3 Approach
Given the corpus of privacy policies described in
?2, we designed a model to efficiently infer an
alignment of policy sections. While we expect that
different kinds of websites will likely address dif-
ferent privacy issues, we believe that many poli-
cies will discuss roughly the same set of issues.
Aligning the policies is a first step in a larger effort
to (i) automatically analyze policies to make them
less opaque to users and (ii) support legal experts
who wish to characterize the state of privacy on-
line and make recommendations (Costante et al,
2012; Ammar et al, 2012; Costante et al, 2013).
We are inspired by multiple sequence alignment
methods in computational biology (Durbin et al,
1998) and by Barzilay and Lee (2004), who de-
scribed a hidden Markov model (HMM) for doc-
ument content where each state corresponds to a
distinct topic and generates sentences relevant to
that topic according to a language model. We
estimate an HMM-like model on our corpus, ex-
ploiting similarity across privacy policies to the
extent it is evident in the data. In our formula-
tion, each hidden state corresponds to an issue or
topic, characterized by a distribution over words
and bigrams appearing in privacy policy sections
addressing that issue. The transition distribution
captures tendencies of privacy policy authors to
organize these sections in similar orders, though
with some variation.
The generative story for our model is as follows.
Let S denote the set of hidden states.
1. Choose a start state y
1
from S according to the
start-state distribution.
2. For t = 1, 2, . . ., until y
t
is the stopping state:
(a) Sample the tth section of the document by
drawing a bag of terms, o
t
, according to the
emission multinomial distribution for state y
t
.
Note the difference from traditional HMMs, in
which a single observation symbol is drawn
at each time step. o
t
is generated by repeat-
edly sampling from a distribution over terms
that includes all unigrams and bigrams except
those that occur in fewer than 5% of the doc-
uments and in more than 98% of the docu-
ments. This filtering rule was designed to
eliminate uninformative stopwords as well as
company-specific terms (e.g., the name of the
company).
4
4
The emission distributions are not a proper language
606
Websites with Unique privacy Unique privacy Ave. sections Ave. tokens
Category privacy URL policies policies w/ date per policy per policy
Arts 94 80 72 11.1 (? 3.8) 2894 (? 1815)
Business 100 95 75 10.1 (? 4.9) 2531 (? 1562)
Computers 100 78 62 10.7 (? 4.9) 2535 (? 1763)
Games 92 80 51 10.2 (? 4.9) 2662 (? 2267)
Health 92 86 57 10.0 (? 4.4) 2325 (? 1891)
Home 100 84 68 11.5 (? 3.8) 2493 (? 1405)
Kids and Teens 96 86 62 10.3 (? 4.5) 2683 (? 1979)
News 96 91 68 10.7 (? 3.9) 2588 (? 2493)
Recreation 98 97 67 11.9 (? 4.5) 2678 (? 1421)
Reference 84 86 55 9.9 (? 4.1) 2002 (? 1454)
Regional 98 91 72 11.2 (? 4.2) 2557 (? 1359)
Science 71 75 49 9.2 (? 4.1) 1705 (? 1136)
Shopping 100 99 84 12.0 (? 4.1) 2683 (? 1154)
Society 96 94 65 10.2 (? 4.6) 2505 (? 1587)
Sports 96 62 38 10.9 (? 4.0) 2222 (? 1241)
Average 94.2 85.6 63.0 10.7 (? 4.3) 2471 (? 1635)
Table 2: Statistics of each website category, including (i) the number of websites with an identified privacy policy link; (ii)
number of unique privacy policies in each category (note that in rare cases, multiple unique privacy policies were identified
for the same website, e.g., a website that contains links to both new and old versions of its privacy policy); (iii) number of
websites with an identified privacy modification date; (iv) average number of sections per policy; (v) average number of tokens
per policy.
(b) Sample the next state, y
t+1
, according to the
transition distribution over S.
This model can nearly be understood as a hid-
den semi-Markov model (Baum and Petrie, 1966),
though we treat the section lengths as observable.
Indeed, our model does not even generate these
lengths, since doing so would force the states to
?explain? the length of each section, not just its
content. The likelihood function for the model is
shown in Figure 1.
The parameters of the model are almost iden-
tical to those of a classic HMM (start state dis-
tribution, emission distributions, and transition
distributions), except that emissions are char-
acterized by multinomial rather than a cate-
gorical distributions. These are learned us-
ing Expectation-Maximization, with a forward-
backward algorithm to calculate marginals (E-
step) and smoothed maximum likelihood estima-
tion for the M-step (Rabiner, 1989). After learn-
ing, the most probable assignment of a policy?s
sections to states can be recovered using a variant
of the Viterbi algorithm.
We consider three HMM variants. ?Vanilla? al-
lows all transitions. The other two posit an order-
ing on the states S = {s
1
, s
2
, . . . , s
K
}, and re-
strict the set of transitions that are possible, impos-
ing bias on the learner. ?All Forward? only allows
models (e.g., a bigram may be generated by as many as three
draws from the emission distribution: once for each unigram
it contains and once for the bigram).
s
k
to transition to {s
k
, s
k+1
, . . . , s
K
}. ?Strict For-
ward? only allows s
k
to transition to s
k
or s
k+1
.
4 Evaluation
Developing a gold-standard alignment of privacy
policies would either require an interface that al-
lows each annotator to interact with the entire cor-
pus of previously aligned documents while read-
ing the one she is annotating, or the definition (and
likely iterative refinement) of a set of categories
for manually labeling policy sections. These were
too costly for us to consider, so we instead pro-
pose two generic methods to evaluate models
for sequence alignment of a collection of docu-
ments with generally similar content. Though our
model (particularly the restricted variants) treats
the problem as one of alignment, our evaluations
consider groupings of policy sections. In the se-
quel, a grouping on a set X is defined as a collec-
tion of subsets X
i
? X; these may overlap (i.e.,
there might be x ? X
i
?X
j
) and need not be ex-
haustive (i.e., there might be x ? X \
?
i
X
i
).
4.1 Evaluation by Human QA
This study was carried out as part of a larger col-
laboration with legal scholars who study privacy.
In that work, we have formulated a set of nine mul-
tiple choice questions about a single policy that
ask about collection of contact, location, health,
and financial information, sharing of each with
607
Ppi,?,? (?yt,ot?
n
t=1
| ?`
t
?
n
t=1
) = pi(y
1
)
n
?
t=1
(
`
t
?
i=1
?(o
t,i
| y
i
)
)
?(y
t+1
| y
t
)
Figure 1: The likelihood function for the alignment model (one privacy policy). y
t
is the hidden state for the tth section, o
t
is
the bag of unigram and bigram terms observed in that section, and `
t
is the size of the bag. Start-state, emission, and transition
distributions are denoted respectively by pi, ?, and ?. y
n+1
is the silent stopping state.
third parties, and deletion of data.
5
The questions
were inspired primarily by the substantive interest
of these domain experts?not by this particular al-
gorithmic study.
For thirty policies, we obtained answers from
each of six domain experts who were not involved
in designing the questions. For the purposes of this
study, the experts? answers are not important. In
addition to answering each question for each pol-
icy, we also asked each expert to copy and paste
the text of the policy that contains the answer.
Experts were allowed to select as many sections
for each question as they saw fit, since answering
some questions may require synthesizing informa-
tion from different sections.
For each of the nine questions, we take the
union of all policy sections that contain text se-
lected by any annotator as support for her answer.
This results in nine groups of policy sections,
which we call answer-sets denoted A
1
, . . . , A
9
.
Our method allows these to overlap (63% of the
sections in any A
i
occurred in more than one A
i
),
and they are not exhaustive (since many sections
of the policies were not deemed to contain answers
to any of the nine questions by any expert).
Together, these can be used as a gold standard
grouping of policy sections, against which we can
compare our system?s output. To do this, we define
the set of section pairs that are grouped together
in answer sets, G = |{?a, b? | ?A
i
3 a, b}|, and
a similar set of pairs H from a model?s grouping.
From these sets, we calculate estimates of preci-
sion (|G ?H|/|H|) and recall (|G ?H|/|G|).
One shortcoming of this approach, for which
the second evaluation seeks to compensate, is that
a very small, and likely biased, subset of the policy
sections is considered.
4.2 Evaluation by Direct Judgment
We created a separate gold standard of judgments
of pairs of privacy policy sections. The data se-
lected for judgment was a sample of pairs stratified
5
The questions are available in an online appendix at
http://usableprivacy.org/data.
by a simple measure of text similarity. We derived
unigram tfidf vectors for each section in each of
50 randomly sampled policies per category. We
then binned pairs of sections by cosine similarity
(into four bins bounded by 0.25, 0.5, and 0.75).
We sampled 994 section pairs uniformly across the
15 categories? four bins each.
Crowdsourcing was used to determine, for each
pair, whether the two sections should be grouped
together. A HIT consisted of a pair of policy sec-
tions and a multiple choice question, ?After read-
ing the two sections given below, would you say
that they broadly discuss the same topic?? The
possible answers were:
1. Yes, both the sections essentially convey the
same message in a privacy policy.
2. Although, the sections do not convey the same
message, the broadly discuss the same topic.
(For ease of understanding, some examples of
content on ?the same topic? were included.)
3. No, the sections discuss two different topics.
The first two options were considered a ?yes? for
the majority voting and for defining a gold stan-
dard. Every section-pair was annotated by at least
three annotators (as many as 15, increased until
an absolute majority was reached). Turkers with
an acceptance rate greater than 95% with an ex-
perience of at least 100 HITs were allowed and
paid $0.03 per annotation. The total cost includ-
ing some initial trials was $130. 535 out of the
994 pairs were annotated to be similar in topic. An
example is shown in Figure 2.
As in ?4.1, we calculate precision and recall on
pairs. This does not penalize the model for group-
ing together a ?no? pair; we chose it nonetheless
because it is interpretable.
5 Experiment
In this section, we evaluate the three HMM vari-
ants described in ?3, and two baselines, using the
methods in ?4. All of the methods require the
specification of the number of groups or hidden
states, which we fix to ten, the average number of
sections per policy.
608
Section 5 of classmates.com:
[46 words] . . . You may also be required to use a password to access certain pages on the Services where certain
types of your personal information can be changed or deleted. . . . [113 words]
Section 2 of 192.com:
[50 words] . . . This Policy sets out the means by which You can have Your Personal Information removed from
the Service. 192.com is also committed to keeping Personal Information of users of the Service secure and only to
use it for the purposes set out in this Policy and as agreed by You. . . . [24 words]
Figure 2: Selections from sections that discuss the issue of ?deletion of personal information? and were labeled as discussing
the same issue by crowdworkers. Both na??ve grouping and LDA put them in two different groups, but the Strict Forward variant
of our model correctly groups them together.
Precision Recall F
1
Mean S.D. Mean S.D. Mean S.D.
Clust. 0.63 ? 0.30 ? 0.40 ?
LDA 0.56 0.03 0.20 0.05 0.29 0.06
Vanilla 0.62 0.04 0.41 0.04 0.49 0.03
All F. 0.63 0.03 0.47 0.12 0.53 0.06
Strict F. 0.62 0.05 0.46 0.18 0.51 0.07
Clust. 0.62 ? 0.23 ? 0.34 ?
LDA 0.57 0.03 0.18 0.01 0.28 0.02
Vanilla 0.57 0.01 0.30 0.03 0.39 0.02
All F. 0.58 0.02 0.32 0.06 0.41 0.04
Strict F. 0.58 0.03 0.32 0.14 0.40 0.08
Table 3: Evaluation by human QA (above) and direct judg-
ment (below), aggregated across ten independent runs where
appropriate (see text). Vanilla, All F(orward), and Strict
F(orward) are three variants of our HMM.
Baselines. Our first baseline is a greedy divisive
clustering algorithm
6
to partition the policy sec-
tions into ten clusters. In this method, the de-
sired K-way clustering solution is computed by
performing a sequence of bisections. The imple-
mentation uses unigram features and cosine simi-
larity. Our second baseline is latent Dirichlet alo-
cation (LDA; Blei et al, 2003), with ten topics and
online variational Bayes for inference (Hoffman et
al., 2010).
7
To more closely match our models,
LDA is given access to the same unigram and bi-
gram tokens.
Results. Table 3 shows the results. For LDA
and the HMM variants (which use random initial-
ization), we report mean and standard deviation
across ten independent runs. All three variants
of the HMM improve over the baselines on both
tasks, in terms of F
1
. In the human QA evalu-
ation, this is mostly due to recall improvements
(i.e., more pairs of sections relevant to the same
policy question were grouped together).
The three variants of the model performed sim-
ilarly on average, though Strict Forward had very
high variance. Its maximum performance across
6
As implemented in CLUTO, http://glaros.dtc.
umn.edu/gkhome/cluto/cluto/overview
7
As implemented in gensim (
?
Reh?u?rek and Sojka, 2010).
ten runs was very high (67% and 53% F
1
on the
two tasks), suggesting the potential benefits of
good initialization or model selection.
6 Conclusion
We considered the task of aligning sections of
a collection of roughly similarly-structured legal
documents, based on the issues they address. We
introduced an unsupervised model for this task
along with two new (and reusable) evaluations.
Our experiments show the approach to be more ef-
fective than clustering and topic models. The cor-
pus and evaluation data have been made available
at http://usableprivacy.org/data . In
future work, policy section alignments will be
used in automated analysis to extract useful infor-
mation for users and privacy scholars.
Acknowledgments
The authors gratefully acknowledge helpful com-
ments from Lorrie Cranor, Joel Reidenberg, Flo-
rian Schaub, and several anonymous reviewers.
This research was supported by NSF grant SaTC-
1330596.
References
Waleed Ammar, Shomir Wilson, Norman Sadeh, and
Noah A. Smith. 2012. Automatic categorization of
privacy policies: A pilot study. Technical Report
CMU-LTI-12-019, Carnegie Mellon University.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proc. of HLT-
NAACL.
Leonard E. Baum and Ted Petrie. 1966. Statistical
inference for probabilistic functions of finite state
Markov chains. Annals of Mathematical Statistics,
37:1554?1563.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet alocation. the Journal of
machine Learning research, 3:993?1022.
609
Carolyn A. Brodie, Clare-Marie Karat, and John Karat.
2006. An empirical study of natural language pars-
ing of privacy policy rules using the SPARCLE pol-
icy workbench. In Proc. of the Symposium on Us-
able Privacy and Security.
Elisa Costante, Yuanhao Sun, Milan Petkovi?c, and
Jerry den Hartog. 2012. A machine learning solu-
tion to assess privacy policy completeness. In Proc.
of the ACM Workshop on Privacy in the Electronic
Society.
Elisa Costante, Jerry Hartog, and Milan Petkovi.
2013. What websites know about you. In Roberto
Pietro, Javier Herranz, Ernesto Damiani, and Radu
State, editors, Data Privacy Management and Au-
tonomous Spontaneous Security, volume 7731 of
Lecture Notes in Computer Science, pages 146?159.
Springer Berlin Heidelberg.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Federal Trade Commission. 2012. Protecting con-
sumer privacy in an era of rapid change: Recom-
mendations for businesses and policymakers.
Robert Gellman. 2014. Fair information prac-
tices: a basic history (v. 2.11). Available at
http://www.bobgellman.com/rg-docs/
rg-FIPShistory.pdf.
Matthew D Hoffman, David M Blei, and Francis R
Bach. 2010. Online learning for latent Dirichlet al
location. In NIPS.
Aleecia M. McDonald and Lorrie Faith Cranor. 2008.
The cost of reading privacy policies. I/S: A Journal
of Law and Policy for the Information Society, 4(3).
Lawrence Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In
Proc. of the LREC Workshop on New Challenges for
NLP Frameworks.
Xusheng Xiao, Amit Paradkar, Suresh Thum-
malapenta, and Tao Xie. 2012. Automated ex-
traction of security policies from natural-language
software documents. In Proc. of the ACM SIGSOFT
International Symposium on the Foundations of
Software Engineering.
Sebastian Zimmeck and Steven M. Bellovin. 2013.
Machine learning for privacy policy.
610
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828?834,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Distributed Representations of Geographically Situated Language
David Bamman Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dbamman,cdyer,nasmith}@cs.cmu.edu
Abstract
We introduce a model for incorporating
contextual information (such as geogra-
phy) in learning vector-space representa-
tions of situated language. In contrast to
approaches to multimodal representation
learning that have used properties of the
object being described (such as its color),
our model includes information about the
subject (i.e., the speaker), allowing us to
learn the contours of a word?s meaning
that are shaped by the context in which
it is uttered. In a quantitative evaluation
on the task of judging geographically in-
formed semantic similarity between repre-
sentations learned from 1.1 billion words
of geo-located tweets, our joint model out-
performs comparable independent models
that learn meaning in isolation.
1 Introduction
The vast textual resources used in NLP ?
newswire, web text, parliamentary proceedings ?
can encourage a view of language as a disembod-
ied phenomenon. The rise of social media, how-
ever, with its large volume of text paired with in-
formation about its author and social context, re-
minds us that each word is uttered by a particular
person at a particular place and time. In short: lan-
guage is situated.
The coupling of text with demographic infor-
mation has enabled computational modeling of
linguistic variation, including uncovering words
and topics that are characteristic of geographical
regions (Eisenstein et al, 2010; O?Connor et al,
2010; Hong et al, 2012; Doyle, 2014), learning
correlations between words and socioeconomic
variables (Rao et al, 2010; Eisenstein et al, 2011;
Pennacchiotti and Popescu, 2011; Bamman et al,
2014); and charting how new terms spread geo-
graphically (Eisenstein et al, 2012). These models
can tell us that hella was (at one time) used most
often by a particular demographic group in north-
ern California, echoing earlier linguistic studies
(Bucholtz, 2006), and that wicked is used most
often in New England (Ravindranath, 2011); and
they have practical applications, facilitating tasks
like text-based geolocation (Wing and Baldridge,
2011; Roller et al, 2012; Ikawa et al, 2012).
One desideratum that remains, however, is how the
meaning of these terms is shaped by geographical
influences ? while wicked is used throughout the
United States to mean bad or evil (?he is a wicked
man?), in New England it is used as an adverbial
intensifier (?my boy?s wicked smart?). In lever-
aging grounded social media to uncover linguistic
variation, what we want to learn is how a word?s
meaning is shaped by its geography.
In this paper, we introduce a method that ex-
tends vector-space lexical semantic models to
learn representations of geographically situated
language. Vector-space models of lexical seman-
tics have been a popular and effective approach
to learning representations of word meaning (Lin,
1998; Turney and Pantel, 2010; Reisinger and
Mooney, 2010; Socher et al, 2013; Mikolov et al,
2013, inter alia). In bringing in extra-linguistic in-
formation to learn word representations, our work
falls into the general domain of multimodal learn-
ing; while other work has used visual informa-
tion to improve distributed representations (An-
drews et al, 2009; Feng and Lapata, 2010; Bruni
et al, 2011; Bruni et al, 2012a; Bruni et al,
2012b; Roller and im Walde, 2013), this work
generally exploits information about the object be-
ing described (e.g., strawberry and a picture of a
strawberry); in contrast, we use information about
the speaker to learn representations that vary ac-
cording to contextual variables from the speaker?s
perspective. Unlike classic multimodal systems
that incorporate multiple active modalities (such
as gesture) from a user (Oviatt, 2003; Yu and
828
...
W
X
Main
Alabama Alaska
Arizona
Arkansas
h
o
Figure 1: Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item
#2) spoken in Alaska, along with a single output. Parameter matrixW consists of the learned low-dimensional embeddings.
Ballard, 2004), our primary input is textual data,
supplemented with metadata about the author and
the moment of authorship. This information en-
ables learning models of word meaning that are
sensitive to such factors, allowing us to distin-
guish, for example, between the usage of wicked
in Massachusetts from the usage of that word else-
where, and letting us better associate geographi-
cally grounded named entities (e.g, Boston) with
their hypernyms (city) in their respective regions.
2 Model
The model we introduce is grounded in the distri-
butional hypothesis (Harris, 1954), that two words
are similar by appearing in the same kinds of con-
texts (where ?context? itself can be variously de-
fined as the bag or sequence of tokens around a tar-
get word, either by linear distance or dependency
path). We can invoke the distributional hypothe-
sis for many instances of regional variation by ob-
serving that such variants often appear in similar
contexts. For example:
? my boy?s wicked smart
? my boy?s hella smart
? my boy?s very smart
Here, all three variants can often be seen in an im-
mediately pre-adjectival position (as is common
with intensifying adverbs).
Given the empirical success of vector-space rep-
resentations in capturing semantic properties and
their success at a variety of NLP tasks (Turian et
al., 2010; Socher et al, 2011; Collobert et al,
2011; Socher et al, 2013), we use a simple, but
state-of-the-art neural architecture (Mikolov et al,
2013) to learn low-dimensional real-valued repre-
sentations of words. The graphical form of this
model is illustrated in figure 1.
This model corresponds to an extension of
the ?skip-gram? language model (Mikolov et al,
2013) (hereafter SGLM). Given an input sentence
s and a context window of size t, each word s
i
is
conditioned on in turn to predict the identities of
all of the tokens within t words around it. For a
vocabulary V , each input word s
i
is represented
as a one-hot vector w
i
of length |V |. The SGLM
has two sets of parameters. The first is the rep-
resentation matrix W ? R
|V |?k
, which encodes
the real-valued embeddings for each word in the
vocabulary. A matrix multiply h = w
>
W,? R
k
serves to index the particular embedding for word
w, which constitutes the model?s hidden layer. To
predict the value of the context word y (again, a
one-hot vector of dimensionality |V |), this hidden
representation h is then multiplied by a second pa-
rameter matrix X ? R
|V |?k
. The final prediction
over the output vocabulary is then found by pass-
ing this resulting vector through the softmax func-
tion o = softmax(Xh), giving a vector in the |V |-
dimensional unit simplex. Backpropagation using
(input x, output y) word tuples learns the values
of W (the embeddings) and X (the output param-
eter matrix) that maximize the likelihood of y (i.e.,
the context words) conditioned on x (i.e., the s
i
?s).
During backpropagation, the errors propagated are
the difference between o (a probability distribu-
tion with k outcomes) and the true (one-hot) out-
put y.
Let us define a set of contextual variables
C; in the experiments that follow, C is com-
prised solely of geographical state C
state
=
{AK,AL, . . . ,WY}) but could in principle in-
clude any number of features, such as calendar
829
month, day of week, or other demographic vari-
ables of the speaker. Let |C| denote the sum of the
cardinalities of all variables in C (i.e., 51 states,
including the District of Columbia). Rather than
using a single embedding matrix W that contains
low-dimensional representations for every word in
the vocabulary, we define a global embedding ma-
trix W
main
? R
|V |?k
and an additional |C| such
matrices (each again of size |V | ? k, which cap-
ture the effect that each variable value has on each
word in the vocabulary. Given an input word w
and set of active variable values A (e.g., A =
{state = MA}), we calculate the hidden layer
h as the sum of these independent embeddings:
h = w
>
W
main
+
?
a?A
w
>
W
a
. While the word
wicked has a common low-dimensional represen-
tation in W
main,wicked
that is invoked for every
instance of its use (regardless of the place), the
corresponding vector W
MA,wicked
indicates how
that common representation should shift in k-
dimensional space when used in Massachusetts.
Backpropagation functions as in standard SGLM,
with gradient updates for each training example
{x, y} touching not onlyW
main
(as in SGLM), but
all active W
A
as well.
The additional W embeddings we add lead to
an increase in the number of total parameters by
a factor of |C|. To control for the extra degrees
of freedom this entails, we add squared `
2
regu-
larization to all parameters, using stochastic gra-
dient descent for backpropagation with minibatch
updates for the regularization term. As in Mikolov
et al (2013), we speed up computation using the
hierarchical softmax (Morin and Bengio, 2005) on
the output matrix X .
This model defines a joint parameterization over
all variable values in the data, where information
from data originating in California, for instance,
can influence the representations learned for Wis-
consin; a naive alternative would be to simply train
individual models on each variable value (a ?Cal-
ifornia? model using data only from California,
etc.). A joint model has three a priori advantages
over independent models: (i) sharing data across
variable values encourages representations across
those values to be similar; e.g., while city may be
closer to Boston in Massachusetts and Chicago in
Illinois, in both places it still generally connotes
a municipality; (ii) such sharing can mitigate data
sparseness for less-witnessed areas; and (iii) with
a joint model, all representations are guaranteed to
be in the same vector space and can therefore be
compared to each other; with individual models
(each with different initializations), word vectors
across different states may not be directly com-
pared.
3 Evaluation
We evaluate our model by confirming its face
validity in a qualitative analysis and estimating
its accuracy at the quantitative task of judging
geographically-informed semantic similarity. We
use 1.1 billion tokens from 93 million geolocated
tweets gathered between September 1, 2011 and
August 30, 2013 (approximately 127,000 tweets
per day evenly sampled over those two years).
This data only includes tweets that have been ge-
olocated to state-level granularity in the United
States using high-precision pattern matching on
the user-specified location field (e.g., ?new york
ny? ? NY, ?chicago? ? IL, etc.). As a pre-
processing step, we identify a set of target mul-
tiword expressions in this corpus as the maximal
sequence of adjectives + nouns with the highest
pointwise mutual information; in all experiments
described below, we define the vocabulary V as
the most frequent 100,000 terms (either unigrams
or multiword expressions) in the total data, and set
the dimensionality of the embedding k = 100. In
all experiments, the contextual variable is the ob-
served US state (including DC), so that |C| = 51;
the vector space representation of word w in state
s is w
>
W
main
+ w
>
W
s
.
3.1 Qualitative Evaluation
To illustrate how the model described above can
learn geographically-informed semantic represen-
tations of words, table 1 displays the terms with
the highest cosine similarity to wicked in Kansas
and Massachusetts after running our joint model
on the full 1.1 billion words of Twitter data; while
wicked in Kansas is close to other evaluative terms
like evil and pure and religious terms like gods and
spirit, in Massachusetts it is most similar to other
intensifiers like super, ridiculously and insanely.
Table 2 likewise presents the terms with the
highest cosine similarity to city in both Califor-
nia and New York; while the terms most evoked
by city in California include regional locations
like Chinatown, Los Angeles? South Bay and San
Francisco?s East Bay, in New York the most sim-
ilar terms include hamptons, upstate and borough
830
Kansas Massachusetts
term cosine term cosine
wicked 1.000 wicked 1.000
evil 0.884 super 0.855
pure 0.841 ridiculously 0.851
gods 0.841 insanely 0.820
mystery 0.830 extremely 0.793
spirit 0.830 goddamn 0.781
king 0.828 surprisingly 0.774
above 0.825 kinda 0.772
righteous 0.823 #sarcasm 0.772
magic 0.822 sooooooo 0.770
Table 1: Terms with the highest cosine similarity to wicked
in Kansas and Massachusetts.
California New York
term cosine term cosine
city 1.000 city 1.000
valley 0.880 suburbs 0.866
bay 0.874 town 0.855
downtown 0.873 hamptons 0.852
chinatown 0.854 big city 0.842
south bay 0.854 borough 0.837
area 0.851 neighborhood 0.835
east bay 0.845 downtown 0.827
neighborhood 0.843 upstate 0.826
peninsula 0.840 big apple 0.825
Table 2: Terms with the highest cosine similarity to city in
California and New York.
(New York City?s term of administrative division).
3.2 Quantitative Evaluation
As a quantitative measure of our model?s perfor-
mance, we consider the task of judging semantic
similarity among words whose meanings are likely
to evoke strong geographical correlations. In the
absence of a sizable number of linguistically in-
teresting terms (like wicked) that are known to be
geographically variable, we consider the proxy of
estimating the named entities evoked by specific
terms in different geographical regions. As noted
above, geographic terms like city provide one such
example: in Massachusetts we expect the term city
to be more strongly connected to grounded named
entities like Boston than to other US cities. We
consider seven categories for which we can rea-
sonably expect the connotations of each term to
vary by geography; in each case, we calculate the
distance between two terms x and y using repre-
sentations learned for a given state (?
state
(x, y)).
1. city. For each state, we measure the distance
between the word city and the state?s most
populous city; e.g., ?
AZ
(city , phoenix ).
2. state. For each state, the distance between
the word state and the state?s name; e.g.,
?
WI
(state,wisconsin).
3. football. For all NFL teams, the distance be-
tween the word football and the team name;
e.g., ?
IL
(football , bears).
4. basketball. For all NBA teams from
a US state, the distance between the
word basketball and the team name; e.g.,
?
FL
(basketball , heat).
5. baseball. For all MLB teams from a US
state, the distance between the word baseball
and the team name; e.g., ?
IL
(baseball , cubs),
?
IL
(baseball ,white sox ).
6. hockey. For all NHL teams from a US state,
the distance between the word hockey and the
team name; e.g., ?
PA
(hockey , penguins).
7. park. For all US national parks, the distance
between the word park and the park name;
e.g., ?
AK
(park , denali).
Each of these questions asks the following:
what words are evoked for a given target word
(like football)? While football may everywhere
evoke similar sports like baseball or soccer or
more specific football-related terms like touch-
down or field goal, we expect that particular sports
teams will be evoked more strongly by the word
football in their particular geographical region: in
Wisconsin, football should evoke packers, while
in Pennsylvania, football evokes steelers. Note
that this is not the same as simply asking which
sports team is most frequently (or most character-
istically) mentioned in a given area; by measuring
the distance to a target word (football), we are at-
tempting to estimate the varying strengths of asso-
ciation between concepts in different regions.
For each category, we measure similarity as the
average cosine similarity between the vector for
the target word for that category (e.g., city) and the
corresponding vector for each state-specific an-
swer (e.g., chicago for IL; boston for MA). We
compare three different models:
1. JOINT. The full model described in section
2, in which we learn a global representation
for each word along with deviations from that
common representation for each state.
2. INDIVIDUAL. For comparison, we also parti-
tion the data among all 51 states, and train a
single model for each state using only data
from that state. In this model, there is no
sharing among states; California has the most
831
0.00
0.25
0.50
0.75
city state baseball basketball football hockey park
sim
ila
rity
Model
Joint
Individual
?Geo
Figure 2: Average cosine similarity for all models across all categories, with 95% confidence intervals on the mean.
data with 11,604,637 tweets; Wyoming has
the least with 47,503 tweets.
3. ?GEO. We also train a single model on all of
the training data, but ignore any state meta-
data. In this case the distance ? between two
terms is their overall distance within the en-
tire United States.
As one concrete example of these differences
between individual data points, the cosine similar-
ity between city and seattle in the ?GEO model
is 0.728 (seattle is ranked as the 188th most sim-
ilar term to city overall); in the INDIVIDUAL
model using only tweets from Washington state,
?
WA
(city, seattle) = 0.780 (rank #32); and in
the JOINT model, using information from the en-
tire United States with deviations for Washington,
?
WA
(city, seattle) = 0.858 (rank #6). The over-
all similarity for the city category of each model is
the average of 51 such tests (one for each city).
Figure 2 present the results of the full evalua-
tion, including 95% confidence intervals for each
mean. While the two models that include ge-
ographical information naturally outperform the
model that does not, the JOINT model generally
far outperforms the INDIVIDUAL models trained
on state-specific subsets of the data.
1
A model that
can exploit all of the information in the data, learn-
ing core vector-space representations for all words
along with deviations for each contextual variable,
is able to learn more geographically-informed rep-
resentations for this task than strict geographical
models alone.
1
This result is robust to the choice of distance metric; an
evaluation measuring the Euclidean distance between vectors
shows the JOINT model to outperform the INDIVIDUAL and
?GEO models across all seven categories.
4 Conclusion
We introduced a model for leveraging situational
information in learning vector-space representa-
tions of words that are sensitive to the speaker?s
social context. While our results use geographical
information in learning low-dimensional represen-
tations, other contextual variables are straightfor-
ward to include as well; incorporating effects for
time ? such as time of day, month of year and ab-
solute year ? may be a powerful tool for reveal-
ing periodic and historical influences on lexical se-
mantics.
Our approach explores the degree to which ge-
ography, and other contextual factors, influence
word meaning in addition to frequency of usage.
By allowing all words in different regions (or more
generally, with different metadata factors) to ex-
ist in the same vector space, we are able com-
pare different points in that space ? for example,
to ask what terms used in Chicago are most simi-
lar to hot dog in New York, or what word groups
shift together in the same region in comparison
to the background (indicating the shift of an en-
tire semantic field). All datasets and software to
support these geographically-informed represen-
tations can be found at: http://www.ark.
cs.cmu.edu/geoSGLM.
5 Acknowledgments
The research reported in this article was supported
by US NSF grants IIS-1251131 and CAREER IIS-
1054319, and by an ARCS scholarship to D.B.
This work was made possible through the use of
computing resources made available by the Open
Cloud Consortium, Yahoo and the Pittsburgh Su-
percomputing Center.
832
References
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463?498.
David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014. Gender identity and lexical variation
in social media. Journal of Sociolinguistics, 18(2).
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proc. of the Workshop on Geometrical Models of
Natural Language Semantics.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012a. Distributional semantics in
technicolor. In Proc. of ACL.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes:
Using image analysis to improve computational rep-
resentations of word meaning. In Proc. of the ACM
International Conference on Multimedia.
Mary Bucholtz. 2006. Word up: Social meanings of
slang in California youth culture. In Jane Goodman
and Leila Monaghan, editors, A Cultural Approach
to Interpersonal Communication: Essential Read-
ings, Malden, MA. Blackwell.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In Proc. of EACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. of ACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words. arXiv, abs/1210.5268.
Yansong Feng and Mirella Lapata. 2010. Visual in-
formation in semantic representation. In Proc. of
NAACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the Twit-
ter stream. In Proc. of WWW.
Yohei Ikawa, Miki Enoki, and Michiaki Tatsubori.
2012. Location inference using microblog mes-
sages. In Proc. of WWW.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLING-ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proc. of AISTATS.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. Discovering demographic
language variation. In NIPS Workshop on Machine
Learning and Social Computing.
Sharon Oviatt. 2003. Multimodal interfaces.
In Julie A. Jacko and Andrew Sears, editors,
The Human-computer Interaction Handbook, pages
286?304, Hillsdale, NJ, USA. L. Erlbaum Asso-
ciates Inc.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proc. of KDD.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. of the Workshop on
Search and Mining User-generated Contents.
Maya Ravindranath. 2011. A wicked good reason to
study intensifiers in New Hampshire. In NWAV 40.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proc. of NAACL.
Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal LDA model integrating textual, cogni-
tive and visual modalities. In Proc. of EMNLP.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proc. of EMNLP-CoNLL.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proc. of EMNLP.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188, January.
833
Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proc. of ACL.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perception, 1(1):57?80.
834
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121?126,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Simplified Dependency Annotations with GFL-Web
Michael T. Mordowanec Nathan Schneider Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
michael.mordowanec@gmail.com, {nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present GFL-Web, a web-based in-
terface for syntactic dependency annota-
tion with the lightweight FUDG/GFL for-
malism. Syntactic attachments are spec-
ified in GFL notation and visualized as
a graph. A one-day pilot of this work-
flow with 26 annotators established that
even novices were, with a bit of training,
able to rapidly annotate the syntax of En-
glish Twitter messages. The open-source
tool is easily installed and configured; it
is available at: https://github.com/
Mordeaux/gfl_web
1 Introduction
High-quality syntactic annotation of natural lan-
guage is expensive to produce. Well-known large-
scale syntactic annotation projects, such as the
Penn Treebank (Marcus et al., 1993), the En-
glish Web Treebank (Bies et al., 2012), the Penn
Arabic Treebank (Maamouri et al., 2004), and
the Prague dependency treebanks (Haji
?
c, 1998;
?
Cmejrek et al., 2005), have relied on expert lin-
guists to produce carefully-controlled annotated
data. Because this process is costly, such anno-
tation projects have been undertaken for only a
handful of important languages. Therefore, devel-
oping syntactic resources for less-studied, lower-
resource, or politically less important languages
and genres will require alternative methods. To
address this, simplified annotation schemes that
trade cost for detail have been proposed (Habash
and Roth, 2009).
1
1
These can be especially effective when some details of
the syntax can be predicted automatically with high accuracy
(Alkuhlani et al., 2013).
The Fragmentary Unlabeled Dependency
Grammar (FUDG) formalism (Schneider et al.,
2013) was proposed as a simplified framework for
annotating dependency syntax. Annotation effort
is reduced by relaxing a number of constraints
placed on traditional annotators: partial fragments
can be specified where the annotator is uncertain
of part of the structure or wishes to focus only
on certain phenomena (such as verbal argument
structure). FUDG also offers mechanisms for
excluding extraneous tokens from the annotation,
for marking multiword units, and for describing
coordinate structures. FUDG is written in an
ASCII-based notation for annotations called
Graph Fragment Language (GFL), and text-based
tools for verifying, converting, and rendering GFL
annotations are provided.
Although GFL offers a number of conveniences
to annotators, the text-based UI is limiting: the
existing tools require constant switching between
a text editor and executing commands, and there
are no tools for managing a large-scale annotation
effort. Additionally, user interface research has
found marked preferences for and better perfor-
mance with graphical tools relative to text-based
interfaces?particularly for less computer-savvy
users (Staggers and Kobus, 2000). In this paper,
we present the GFL-Web tool, a web-based inter-
face for FUDG/GFL annotation. The simple inter-
face provides instantaneous feedback on the well-
formedness of a GFL annotation, and by wrapping
Schneider et al.?s notation parsing and rendering
software, gives a user-friendly visualization of the
annotated sentence. The tool itself is lightweight,
multi-user, and easily deployed with few software
dependencies. Sentences are assigned to anno-
tators via an administrative interface, which also
records progress and provides for a text dump of
121
(a) @Bryan_wright11 i lost all my contacts , smh .
(b) Texas Rangers are in the World Series ! Go
Rangers !!!!!!!!! http://fb.me/D2LsXBJx
Figure 1: FUDG annotation graphs for two tweets.
all annotations. The interface for annotators is de-
signed to be as simple as possible.
We provide an overview of the FUDG/GFL
framework (?2), detail how the tool is set up and
utilized (?3), and discuss a pilot exercise in which
26 users provided nearly 1,000 annotations of En-
glish Twitter messages (?4). Finally, we note some
of the technical aspects of the tool (?5) and related
syntactic annotation software (?6).
2 Background
GFL-Web is designed to simplify the creation
of dependency treebanks from noisy or under-
resourced data; to that end, it exploits the
lightweight FUDG/GFL framework of Schneider
et al. (2013). Here we outline how FUDG differs
from traditional Dependency Grammar (?2.1) and
detail major aspects of GFL (?2.2).
2.1 FUDG
Figure 1 displays two FUDG graphs of annota-
tions of Twitter messages (?tweets?, shown below
in tokenized form). Arrows point upwards from
dependents to their heads. These tweets illustrate
several characteristics of the formalism, including:
? The input may contain multiple independent
syntactic units, or ?utterances?; the annotation
indicates these by attaching their heads to a spe-
cial root node called **.
? Some input tokens are omitted if deemed ex-
trinsic to the syntax; by convention, these in-
clude most punctuation, hashtags, usernames,
and URLs.
? Multiword units may be joined to form com-
posite lexical nodes (e.g., World_Series in fig-
ure 1b). These nodes are not annotated with any
internal syntactic structure.
? Tokens that are used in the FUDG parse must be
unambiguous. If a word appears multiple times
in the input, it is disambiguated with ~ and an
index (e.g., Rangers~2 in figure 1b).
(Some of the other mechanisms in FUDG, such as
coordinate structures and underspecification, are
not shown here; they are not important for pur-
poses of this paper.)
2.2 GFL
The Graph Fragment Language is a simple ASCII-
based language for FUDG annotations. Its norms
are designed to be familiar to users with basic pro-
gramming language proficiency, and they are intu-
itive and easy to learn even for those without. The
annotation in figure 1a may be expressed in GFL
as:
2
i > lost** < ({all my} > contacts)
smh**
In GFL, angle brackets point from a dependent
(child) to its head (parent). Parentheses group
nodes together; the head of this group is then at-
tached to another node. The double asterisk (**)
marks a root node in an annotations containing
multiple utterances. Curly braces group nodes that
modify the same head.
GFL corresponding to Figure 1b is:
2
The abbreviation smh stands for shaking my head.
122
Sentence: Texas Rangers are in the World Series ! Go Rangers !!!!!!!!! http://fb.me/D2LsXBJx
Input format:
---
% ID data_set_name:417
% TEXT
Texas Rangers~1 are in the World Series ! Go Rangers~2 !!!!!!!!!
http://fb.me/D2LsXBJx
% ANNO
Texas Rangers~1 are in the World Series Go Rangers~2
http://fb.me/D2LsXBJx
Figure 2: Illustration of the GFL-Web input format for a tweet. The ANNO section will be shown to the user as the default
annotation; punctuation has been stripped out automatically to save time.
Figure 3: User home screen showing assigned batches for annotation, with links to the training set and blank annotation form.
[Texas Rangers~1] > are** < in
in < (the > [World Series])
Go** < Rangers~2
This uses square brackets for multiword expres-
sions. Similar to a programming language, there
are often many equivalent GFL annotation options
for a given sentence. The annotation can be split
across multiple lines so that annotators can ap-
proach smaller units first and then link them to-
gether.
3 Using GFL-Web
The GFL-Web tool uses the Python programming
language?s Flask microframework for server-side
scripting. This allows it to be deployed on a web
server, locally or via the Internet. This also en-
ables the interface to rely on scripts previously
created for analyzing GFL. Once installed, the re-
searcher need only configure a few settings and be-
gin entering data to be annotated.
3.1 Setup
There are a few simple configuration options. The
most useful of these options specify how many
sentences should be in each batch that is assigned
to an annotator, and how many sentences in each
batch should be doubly annotated, for the purpose
of assessing inter-annotator agreement. By de-
fault, the batch size is 10, and the first 2 sentences
of each batch overlap with the previous batch, so
4/10 of the sentences in the batch will be annotated
by someone else (assuming no two consecutive
batches are assigned to the same user). The pro-
gram requires tokenized input, with indices added
to distinguish between words that appear twice
(easily automated). The input format, figure 2, al-
lows for easy editing with a text editor if so de-
sired.
Once the input files have been placed in a des-
ignated directory, an admin interface can be used
to assign batches of data to specific users (annota-
tors).
3.2 Annotation
Annotators log in with their username and see a
home screen, figure 3. The home screen always
offers links to a training set to get them up to
speed, as well as a blank annotation form into
which they can enter and annotate any sentence.
Beneath these is a table of batches of sentences
which have been assigned to the user. Clicking
123
Figure 4: A well-formed GFL annotation is indicated by a
green background and visualization of the analysis graph.
any of these will take the annotator to an annota-
tion page, which displays the text to be annotated,
an input field, and a comments box.
The annotation interface is simple and intuitive
and provides instant feedback, preventing the an-
notator from submitting ill-formed annotations.
Annotators press the Analyze button and receive
feedback before submitting annotations (figure 4).
Common GFL errors such as unbalanced paren-
theses are caught by the program and brought to
the attention of the annotator with an informative
error message (figure 5). The annotator can then
fix the error, and will be able to submit once all
errors are resolved.
The training set consists of 15 sentences se-
lected from Rossman and Mills (1922), shown in
the same annotation interface. Examples become
increasingly more complicated in order to famil-
iarize the user with different syntactic phenomena
and the entry-analyze-review workflow. A button
displays the FUDG graph from an expert annota-
tion so the novice can compare it to her own and
consult the guidelines (or ask for help) where the
two graphs deviate.
4 Pilot User Study
We conducted a pilot annotation project in which
26 annotators were trained on GFL-Web and asked
to annotate English Twitter messages from the
daily547 and oct27 Twitter datasets of Gimpel
et al. (2011). The overwhelming majority were all
trained on the same day, having no prior knowl-
edge of GFL. Most, but not all, were native speak-
ers of English. Those who had no prior knowl-
edge of dependency grammar in general received
a short tutorial on the fundamentals before being
introduced to the annotation workflow. All par-
ticipants who were new to FUDG/GFL worked
through the training set before moving on to the
Twitter data. Annotators were furnished with the
English annotation guidelines of Schneider et al.
(2013).
3
4.1 Results
In the one-day event, 906 annotations were gen-
erated. Inter-annotator agreement was high?.9
according to the softComPrec measure (Schnei-
der et al., 2013)?and an expert?s examination of a
sample of the annotations found that 75% of con-
tained no major error.
Annotators used the analysis feature of the
interface?which displays either a visual represen-
tation of the tree or an error message?an aver-
age of 3.06 times per annotation. The interface re-
quires they analyze each annotation at least once.
Annotators have the ability to resubmit annota-
tions if they later realize they made an error, and
each annotation was submitted an average of 1.16
times. Disregarding instances that took over 1,000
seconds (under the assumption that these repre-
sent annotators taking breaks), the median time
between the first analysis and the first submission
of an annotation was 30 seconds. We take this
as evidence that annotators found the instant feed-
back features useful in refining their annotations.
4.2 Post-Pilot Improvements
Annotator feedback prompted some changes to the
interface. The annotation input box was changed
to incorporate bracket-matching. The graph visu-
alization for a correct annotation was added for
each example in the training set so new annota-
tors could compare their tree to an example. Pre-
sumably these changes would further reduce anno-
tators? training time and improve their efficiency.
Progress bars were added to the user home screen
to show per-batch completion information.
4.3 Other Languages
In addition to English, guidelines for Swahili,
Zulu, and Mandarin are currently in development.
3https://github.com/brendano/gfl_syntax/
blob/master/guidelines/guidelines.md
124
Figure 5: An example error notification. The red background indicates an error, and the cause of the error is displayed at the
bottom of the screen.
5 Technical Architecture
GFL-Web and its software dependencies for ana-
lyzing and visualizing GFL are largely written in
Python. The tool is built with Flask, a Python
framework for web applications. Data is stored
and transmitted to and from the browser in the
Javascript Object Notation (JSON) format, which
is supported by libraries in most programming lan-
guages. The browser interface uses AJAX tech-
niques to interact dynamically with the server.
GFL-Web is written for Python version 2.7.
It wraps scripts previously written for the analy-
sis and visualization of GFL (Schneider et al.,
2013). These in turn require Graphviz (Ellson
et al., 2002), which is freely available.
Flask provides a built-in server, but can also be
deployed in Apache, via WSGI or CGI, etc.
6 Other Tools
In treebanking, a good user interface is essen-
tial for annotator productivity and accuracy. Sev-
eral existing tools support dependency annota-
tion; GFL-Web is the first designed specifi-
cally for the FUDG/GFL framework. Some,
including WebAnno (Yimam et al., 2013) and
brat (Stenetorp et al., 2012), are browser-based,
while WordFreak (Morton and LaCivita, 2003),
Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pa-
jas and Fabian, 2000?2011) are client-side appli-
cations. All offer tree visualizations; to the best
of our knowledge, ours is the only dependency
annotation interface that has text as the exclu-
sive mode of entry. Some, such as WebAnno
and brat, aim to be fairly general-purpose, sup-
porting a wide range of annotation schemes; by
contrast, GFL-Web supports a single annotation
scheme, which keeps the configuration (and code-
base) simple. In the future, GFL-Web might in-
corporate elements of monitoring progress, such
as display of evaluation measures computed with
existing FUDG/GFL scripts.
Certain elements of the FUDG/GFL framework
can be found in other annotation systems, such
as the PASSAGE syntactic representation (Vilnat
et al., 2010), which allows for grouping of words
into units, but still requires dependency relations
to be labeled.
Finally, we note that new approaches to corpus
annotation of semantic dependencies also come
with rich browser-based annotation interfaces (Ba-
narescu et al., 2013; Abend and Rappoport, 2013).
7 Conclusion
While the creation of high-quality, highly speci-
fied, syntactically annotated corpora is a goal that
is out of reach for most languages and genres,
GFL-Web facilitates a rapid annotation workflow
within a simple framework for dependency syn-
tax. More information on FUDG/GFL is avail-
able at http://www.ark.cs.cmu.edu/FUDG/,
and the source code for GFL-Web is available at
https://github.com/Mordeaux/gfl_web.
125
Acknowledgments
The authors thank Archna Bhatia, Lori Levin, Ja-
son Baldridge, Dan Garrette, Jason Mielens, Liang
Sun, Shay Cohen, Spencer Onuffer, Nora Ka-
zour, Manaal Faruqui, Wang Ling, Waleed Am-
mar, David Bamman, Dallas Card, Jeff Flani-
gan, Lingpeng Kong, Bill McDowell, Brendan
O?Connor, Tobi Owoputi, Yanchuan Sim, Swabha
Swayamdipta, and Dani Yogatama for annotating
data, and anonymous reviewers for helpful feed-
back. This research was supported by NSF grant
IIS-1352440.
References
Omri Abend and Ari Rappoport. 2013. Universal Con-
ceptual Cognitive Annotation (UCCA). In Proc. of
ACL, pages 228?238. Sofia, Bulgaria.
Sarah Alkuhlani, Nizar Habash, and Ryan Roth. 2013.
Automatic morphological enrichment of a mor-
phologically underspecified treebank. In Proc. of
NAACL-HLT, pages 460?470. Atlanta, Georgia.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for sembanking. In Proc. of the 7th Linguistic An-
notation Workshop and Interoperability with Dis-
course, pages 178?186. Sofia, Bulgaria.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Martin
?
Cmejrek, Jan Cu
?
r?n, Jan Haji
?
c, and Ji
?
r? Havelka.
2005. Prague Czech-English Dependency Treebank:
resource for structure-based MT. In Proc. of EAMT,
pages 73?78. Budapest, Hungary.
John Ellson, Emden Gansner, Lefteris Koutsofios,
Stephen C. North, and Gordon Woodhull. 2002.
Graphviz?open source graph drawing tools. In Pe-
tra Mutzel, Michael J?nger, and Sebastian Leipert,
editors, Graph Drawing, pages 483?484. Springer,
Berlin.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT, pages 42?47. Portland, Ore-
gon.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proc. of ACL-
IJCNLP, pages 221?224. Suntec, Singapore.
Jan Haji
?
c. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Haji?cov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Arantza D?az De Ilarraza, Aitzpea Garmendia, and
Maite Oronoz. 2004. Abar-Hitz: An annotation tool
for the Basque dependency treebank. In Proc. of
LREC, pages 251?254. Lisbon, Portugal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Tree-
bank: building a large-scale annotated Arabic cor-
pus. In NEMLAR Conference on Arabic Language
Resources and Tools, pages 102?109. Cairo.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Thomas Morton and Jeremy LaCivita. 2003.
WordFreak: An open tool for linguistic anno-
tation. In Proc. of HLT-NAACL: Demonstrations,
pages 17?18. Edmonton, Canada.
Petr Pajas and Peter Fabian. 2000?2011. Tree Editor
TrEd 2.0. http://ufal.mff.cuni.cz/tred/.
Mary Blanche Rossman and Mary Wilda Mills. 1922.
Graded Sentences for Analysis, Selected from the
Best Literature and Systematically Graded for Class
Use. L. A. Noble.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse, pages 51?60. Sofia, Bulgaria.
Nancy Staggers and David Kobus. 2000. Comparing
response time, errors, and satisfaction between text-
based and graphical user interfaces during nursing
order tasks. Journal of the American Medical Infor-
matics Association, 7(2):164?176.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi
?
c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proc. of EACL: Demonstrations,
pages 102?107. Avignon, France.
Anne Vilnat, Patrick Paroubek, Eric Villemonte
de la Clergerie, Gil Francopoulo, and Marie-Laure
Gu?not. 2010. PASSAGE syntactic representation:
a minimal common ground for evaluation. In Proc.
of LREC, pages 2478?2485. Valletta, Malta.
Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A flexible, web-based and visually
supported system for distributed annotations. In
Proc. of ACL: Demonstrations, pages 1?6. Sofia,
Bulgaria.
126
Dynamic Language Models for Streaming Text
Dani Yogatama? Chong Wang? Bryan R. Routledge? Noah A. Smith? Eric P. Xing?
?School of Computer Science
?Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
?{dyogatama,chongw,nasmith,epxing}@cs.cmu.edu, ?routledge@cmu.edu
Abstract
We present a probabilistic language model that
captures temporal dynamics and conditions on
arbitrary non-linguistic context features. These
context features serve as important indicators
of language changes that are otherwise difficult
to capture using text data by itself. We learn
our model in an efficient online fashion that is
scalable for large, streaming data. With five
streaming datasets from two different genres?
economics news articles and social media?we
evaluate our model on the task of sequential
language modeling. Our model consistently
outperforms competing models.
1 Introduction
Language models are a key component in many NLP
applications, such as machine translation and ex-
ploratory corpus analysis. Language models are typi-
cally assumed to be static?the word-given-context
distributions do not change over time. Examples
include n-gram models (Jelinek, 1997) and proba-
bilistic topic models like latent Dirichlet allocation
(Blei et al., 2003); we use the term ?language model?
to refer broadly to probabilistic models of text.
Recently, streaming datasets (e.g., social media)
have attracted much interest in NLP. Since such data
evolve rapidly based on events in the real world, as-
suming a static language model becomes unrealistic.
In general, more data is seen as better, but treating all
past data equally runs the risk of distracting a model
with irrelevant evidence. On the other hand, cau-
tiously using only the most recent data risks overfit-
ting to short-term trends and missing important time-
insensitive effects (Blei and Lafferty, 2006; Wang
et al., 2008). Therefore, in this paper, we take steps
toward methods for capturing long-range temporal
dynamics in language use.
Our model also exploits observable context vari-
ables to capture temporal variation that is otherwise
difficult to capture using only text. Specifically for
the applications we consider, we use stock market
data as exogenous evidence on which the language
model depends. For example, when an important
company?s price moves suddenly, the language model
should be based not on the very recent history, but
should be similar to the language model for a day
when a similar change happened, since people are
likely to say similar things (either about that com-
pany, or about conditions relevant to the change).
Non-linguistic contexts such as stock price changes
provide useful auxiliary information that might indi-
cate the similarity of language models across differ-
ent timesteps.
We also turn to a fully online learning framework
(Cesa-Bianchi and Lugosi, 2006) to deal with non-
stationarity and dynamics in the data that necessitate
adaptation of the model to data in real time. In on-
line learning, streaming examples are processed only
when they arrive. Online learning also eliminates
the need to store large amounts of data in memory.
Strictly speaking, online learning is distinct from
stochastic learning, which for language models built
on massive datasets has been explored by Hoffman
et al. (2013) and Wang et al. (2011). Those tech-
niques are still for static modeling. Language model-
ing for streaming datasets in the context of machine
translation was considered by Levenberg and Os-
borne (2009) and Levenberg et al. (2010). Goyal
et al. (2009) introduced a streaming algorithm for
large scale language modeling by approximating n-
gram frequency counts. We propose a general online
learning algorithm for language modeling that draws
inspiration from regret minimization in sequential
predictions (Cesa-Bianchi and Lugosi, 2006) and on-
181
Transactions of the Association for Computational Linguistics, 2 (2014) 181?192. Action Editor: Eric Fosler-Lussier.
Submitted 10/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
line variational algorithms (Sato, 2001; Honkela and
Valpola, 2003).
To our knowledge, our model is the first to bring
together temporal dynamics, conditioning on non-
linguistic context, and scalable online learning suit-
able for streaming data and extensible to include
topics and n-gram histories. The main idea of our
model is independent of the choice of the base lan-
guage model (e.g., unigrams, bigrams, topic models,
etc.). In this paper, we focus on unigram and bi-
gram language models in order to evaluate the basic
idea on well understood models, and to show how it
can be extended to higher-order n-grams. We leave
extensions to topic models for future work.
We propose a novel task to evaluate our proposed
language model. The task is to predict economics-
related text at a given time, taking into account the
changes in stock prices up to the corresponding day.
This can be seen an inverse of the setup considered
by Lavrenko et al. (2000), where news is assumed
to influence stock prices. We evaluate our model
on economics news in various languages (English,
German, and French), as well as Twitter data.
2 Background
In this section, we first discuss the background for
sequential predictions then describe how to formulate
online language modeling as sequential predictions.
2.1 Sequential Predictions
Let w1, w2, . . . , wT be a sequence of response vari-
ables, revealed one at a time. The goal is to design
a good learner to predict the next response, given
previous responses and additional evidence which
we denote by xt ? RM (at time t). Throughout this
paper, we use the term features for x. Specifically, at
each round t, the learner receives xt and makes a pre-
diction w?t, by choosing a parameter vector?t ? RM .
In this paper, we refer to ? as feature coefficients.
There has been an enormous amount of work on
online learning for sequential predictions, much of it
building on convex optimization. For a sequence of
loss functions `1, `2, . . . , `T (parameterized by ?),
an online learning algorithm is a strategy to minimize
the regret, with respect to the best fixed ?? in hind-
sight.1 Regret guarantees assume a Lipschitz con-
1Formally, the regret is defined as RegretT (??) =
dition on the loss function ` that can be prohibitive
for complex models. See Cesa-Bianchi and Lugosi
(2006), Rakhlin (2009), Bubeck (2011), and Shalev-
Shwartz (2012) for in-depth discussion and review.
There has also been work on online and stochastic
learning for Bayesian models (Sato, 2001; Honkela
and Valpola, 2003; Hoffman et al., 2013), based on
variational inference. The goal is to approximate pos-
terior distributions of latent variables when examples
arrive one at a time.
In this paper, we will use both kinds of techniques
to learn language models for streaming datasets.
2.2 Problem Formulation
Consider an online language modeling problem, in
the spirit of sequential predictions. The task is to
build a language model that accurately predicts the
texts generated on day t, conditioned on observ-
able features up to day t, x1:t. Every day, after
the model makes a prediction, the actual texts wt
are revealed and we suffer a loss. The loss is de-
fined as the negative log likelihood of the model
`t = ? log p(wt | ?,?1:t?1,x1:t?1,n1:t?1), where
? and ?1:T are the model parameters andn is a back-
ground distribution (details are given in ?3.2). We
can then update the model and proceed to day t+ 1.
Notice the similarity to the sequential prediction de-
scribed above. Importantly, this is a realistic setup for
building evolving language models from large-scale
streaming datasets.
3 Model
3.1 Notation
We index timesteps by t ? {1, . . . , T} and word
types by v ? {1, . . . , V }, both are always given as
subscripts. We denote vectors in boldface and use
1 : T as a shorthand for {1, 2, . . . , T}. We assume
words of the form {wt}Tt=1 for wt ? RV , which is
the vector of word frequences at timetstep t. Non-
linguistic context features are {xt}Tt=1 for xt ? RM .
The goal is to learn parameters ? and ?1:T , which
will be described in detail next.
3.2 Generative Story
The main idea of our model is illustrated by the fol-
lowing generative story for the unigram language
PT
t=1 `t(xt,?t, wt)? inf??
PT
t=1 `t(xt,??, wt).
182
model. (We will discuss the extension to higher-order
language models later.) A graphical representation
of our proposed model is given in Figure 1.
1. Draw feature coefficients ? ? N(0, ?I).2 Here
? is a vector in RM , where M is the dimension-
ality of the feature vector.
2. For each timestep t:
(a) Observe non-linguistic context features xt.
(b) Draw ?t ?
N
(?t?1
k=1 ?k
exp(?>f(xt,xk))Pt?1
j=1 ?j exp(?>f(xt,xj))
?k, ?I
)
.
Here, ?t is a vector in RV , where V is
the size of the word vocabulary, ? is
the variance parameter and ?k is a fixed
hyperparameter; we discuss them below.
(c) For each word wt,v, draw wt,v ?
Categorical
(
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
)
.
In the last step, ?t and n are mapped to the V -
dimensional simplex, forming a distribution over
words. n1:t?1 ? RV is a background (log) distri-
bution, inspired by a similar idea in Eisenstein et al.
(2011). In this paper, we set n1:t?1,v to be the log-
frequency of v up to time t? 1. We can interpret ?
as a time-dependent deviation from the background
log-frequencies that incorporates world-context. This
deviation comes in the form of a weighted average of
earlier deviation vectors.
The intuition behind the model is that the probabil-
ity of a word appearing at day t depends on the back-
ground log-frequencies, the deviation coefficients of
the word at previous timesteps ?1:t?1, and the sim-
ilarity of current conditions of the world (based on
observable features x) to previous timesteps through
f(xt,xk). That is, f is a function that takes d-
dimensional feature vectors at two timesteps xt and
xk and returns a similarity vector f(xt,xk) ? RM
(see ?6.1.1 for an example of f that we use in our
experiments). The similarity is parameterized by ?,
and decays over time with rate ?k. In this work, we
assume a fixed window size c (i.e., we consider c
most recent timesteps), so that ?1:t?c?1 = 0 and
?t?c:t?1 = 1. This allows up to cth order depen-
dencies.3 Setting ? this way allows us to bound the
2Feature coefficients ? can be also drawn from other distri-
butions such as ? ? Laplace(0, ?).
3In online Bayesian learning, it is known that forgetting
inaccurate estimates from earlier timesteps is important (Sato,
 
xtxsxrxq
wq wr ws wt
 t s r q
?
NrNq Ns Nt
T
Figure 1: Graphical representation of the model. The
subscript indices q, r, s are shorthands for the previ-
ous timesteps t ? 3, t ? 2, t ? 1. Only four timesteps
are shown here. There are arrows from previous
?t?4,?t?5, . . . ,?t?c to ?t, where c is the window size
as described in ?3.2. They are not shown here, for read-
ability.
number of past vectors ? that need to be kept in
memory. We set ?0 to 0.
Although the generative story described above
is for unigram language models, extensions can be
made to more complex models (e.g., mixture of un-
igrams, topic models, etc.) and to longer n-gram
contexts. In the case of topic models, the model
will be related to dynamic topic models (Blei and
Lafferty, 2006) augmented by context features, and
the learning procedure in ?4 can be used to perform
online learning of dynamic topic models. However,
our model captures longer-range dependencies than
dynamic topic models, and can condition on non-
linguistic features or metadata. In the case of higher-
order n-grams, one simple way is to draw more ?,
one for each history. For example, for a bigram
model, ? is in RV 2 , rather than RV in the unigram
model. We consider both unigram and bigram lan-
guage models in our experiments in ?6. However, the
main idea presented in this paper is largely indepen-
dent of the base model.
Related work. Mimno and McCallum (2008) and
Eisenstein et al. (2010) similarly conditioned text on
2001; Honkela and Valpola, 2003). Since we set ?1:t?c?1 = 0,
at every timestep t, ?k leads to forgetting older examples.
183
observable features (e.g., author, publication venue,
geography, and other document-level metadata), but
conducted inference in a batch setting, thus their ap-
proaches are not suitable for streaming data. It is not
immediately clear how to generalize their approach to
dynamic settings. Algorithmically, our work comes
closest to the online dynamic topic model of Iwata
et al. (2010), except that we also incorporate context
features.
4 Learning and Inference
The goal of the learning procedure is to minimize the
overall negative log likelihood,
? logL(D) =
? log
?
d?1:T p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n).
However, this quantity is intractable. Instead, we
derive an upper bound for this quantity and minimize
that upper bound. Using Jensen?s inequality, the vari-
ational upper bound on the negative log likelihood
is:
? logL(D) ? ?
?
d?1:T q(?1:T | ?1:T ) (4)
log p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n)q(?1:T | ?1:T )
.
Specifically, we use mean-field variational inference
where the variables in the variational distribution q
are completely independent. We use Gaussian distri-
butions as our variational distributions for ?, denoted
by ? in the bound in Eq. 4. We denote the parameters
of the Gaussian variational distribution for ?t,v (word
v at timestep t) by ?t,v (mean) and ?t,v (variance).
Figure 2 shows the functional form of the varia-
tional bound that we seek to minimize, denoted by B?.
The two main steps in the optimization of the bound
are inferring ?t and updating feature coefficients ?.
We next describe each step in detail.
4.1 Learning
The goal of the learning procedure is to minimize the
upper bound in Figure 2 with respect to ?. However,
since the data arrives in an online fashion, and speed
is very important for processing streaming datasets,
the model needs to be updated at every timestep t (in
our experiments, daily).
Notice that at timestep t, we only have access
to x1:t and w1:t, and we perform learning at every
timestep after the text for the current timestep wt
is revealed. We do not know xt+1:T and wt+1:T .
Nonetheless, we want to update our model so that
it can make a better prediction at t + 1. Therefore,
we can only minimize the bound until timestep t.
Let Ck , exp(?>f(xt,xk))Pt?1
j=t?c exp(?>f(xt,xj))
. Our learning al-
gorithm is a variational Expectation-Maximization
algorithm (Wainwright and Jordan, 2008).
E-step Recall that we use variational inference and
the variational parameters for ? are ? and ?. As
shown in Figure 2, since the log-sum-exp in the last
term of B is problematic, we introduce additional
variational parameters ? to simplify B and obtain
B? (Eqs. 2?3). The E-step deals with all the local
variables ?, ?, and ?.
Fixing other variables and taking the derivative
of the bound B? w.r.t. ?t and setting it to zero,
we obtain the closed-form update for ?t: ?t =?
v?V exp (n1:t?1,v) exp
(
?t,v + ?t,v2
).
To minimize with respect to ?t and ?t, we apply
gradient-based methods since there are no closed-
form solutions. The derivative w.r.t. ?t,v is:
?B?
??t,v
=?t,v ? Ck?k,v?
? nt,v +
nt
?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
,
where nt =?v?V nt,v.
The derivative w.r.t. ?t,v is:
?B?
??t,v
= 12?t,v
+ 12? +
nt
2?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
.
Although we require iterative methods in the E-step,
we find it to be reasonably fast in practice.4 Specifi-
cally, we use the L-BFGS quasi-Newton algorithm
(Liu and Nocedal, 1989).
We can further improve the bound by updating
the variational parameters for timestep 1 : t? 1, i.e.,
?1:t?1 and?1:t?1, as well. However, this will require
storing the texts from previous timesteps. Addition-
ally, this will complicate the M-step update described
4Approximately 16.5 seconds/day (walltime) to learn the
model on the EN:NA dataset on a 2.40GHz CPU with 24GB
memory.
184
B =?
T?
t=1
Eq[log p(?t | ?k,?,xt)]?
T?
t=1
Eq[log p(wt | ?t,nt)]?H(q) (1)
=
T?
t=1
?
??
??
1
2
?
j?V
log ?t,j? ? Eq
?
???
(
?t ?
?t?1
k=t?c Ck?k
)2
2?
?
??? Eq
?
??
v?wt
n1:t?1,v + ?t,v ? log
?
j?V
exp(n1:t?1,j + ?t,j)
?
?
?
??
??
(2)
?
T?
t=1
?
??
??
1
2
?
j?V
log ?t,v? +
(
?t ?
?t?1
k=t?c Ck?k
)2
2? +
?t +
?t?1
k=t?c C2k?k
2?
?
?
v?wt
?
??t,v ? log ?t ?
1
?t
?
j?V
exp (n1:t?1,j) exp
(
?t,j +
?t,j
2
)
?
?
?
??
??
+ const (3)
Figure 2: The variational bound that we seek to minimize, B. H(q) is the entropy of the variational distribution q. The
derivation from line 1 to line 2 is done by replacing the probability distributions p(?t | ?k,?,xt) and p(wt | ?t,nt)
by their respective functional forms. Notice that in line 3 we compute the expectations under the variational distributions
and further bound B by introducing additional variational parameters ? using Jensen?s inequality on the log-sum-exp in
the last term. We denote the new bound B?.
below. Therefore, for each s < t, we choose to fix
?s and ?s once they are learned at timestep s.
M-step In the M-step, we update the global pa-
rameter ?, fixing ?1:t. Fixing other parameters and
taking the derivative of B? w.r.t. ?, we obtain:5
?B?
?? =
(?t ?
?t?1
k=t?cCk?k)(?
?t?1
k=t?c
?Ck
?? )
?
+
?t?1
k=t?cCk?k ?Ck??
? ,
where:
?Ck
?? =Ckf(xt,xk)
?Ck
?t?1
s=t?c f(xt,xs) exp(?>f(xt,xs))?t?1
s=t?c exp(?>f(xt,xs))
.
We follow the convex optimization strategy and sim-
ply perform a stochastic gradient update: ?t+1 =
?t + ?t ?B???t (Zinkevich, 2003). While the variational
bound B? is not convex, given the local variables ?1:t
5In our implementation, we augment ? with a squared L2
regularization term (i.e., we assume that ? is drawn from a
normal distribution with mean zero and variance ?) and use the
FOBOS algorithm (Duchi and Singer, 2009). The derivative
of the regularization term is simple and is not shown here. Of
course, other regularizers (e.g., the L1-norm, which we use for
other parameters, or the L1/?-norm) can also be explored.
and ?1:t, optimizing ? at timestep t without know-
ing the future becomes a convex problem.6 Since
we do not reestimate ?1:t?1 and ?1:t?1 in the E-step,
the choice to perform online gradient descent instead
of iteratively performing batch optimization at every
timestep is theoretically justified.
Notice that our overall learning procedure is still
to minimize the variational upper bound B?. All these
choices are made to make the model suitable for
learning in real time from large streaming datasets.
Preliminary experiments showed that performing
more than one EM iteration per day does not consid-
erably improve performance, so in our experiments
we perform one EM iteration per day.
To learn the parameters of the model, we rely on
approximations and optimize an upper bound B?. We
have opted for this approach over alternatives (such
as MCMC methods) because of our interest in the
online, large-data setting. Our experiments show that
we are still able to learn reasonable parameter esti-
mates by optimizing B?. Like online variational meth-
ods for other latent-variable models such as LDA
(Sato, 2001; Hoffman et al., 2013), open questions re-
main about the tightness of such approximations and
the identifiability of model parameters. We note, how-
6As a result, our algorithm is Hannan consistent w.r.t. the
best fixed ? (for B?) in hindsight; i.e., the average regret goes to
zero as T goes to?.
185
ever, that our model does not include latent mixtures
of topics and may be generally easier to estimate.
5 Prediction
As described in ?2.2, our model is evaluated by the
loss suffered at every timestep, where the loss is
defined as the negative log likelihood of the model
on text at timestep wt. Therefore, at each timestep t,
we need to predict (the distribution of)wt. In order
to do this, for each word v ? V , we simply compute
the deviation means ?t,v as weighted combinations
of previous means, where the weights are determined
by the world-context similarity encoded in x:
Eq[?t,v | ?t,v] =
t?1?
k=t?c
exp(?>f(xt,xk))?t?1
j=t?c exp(?>f(xt,xj))
?k,v.
Recall that the word distribution that we use for
prediction is obtained by applying the operator pi
that maps ?t and n to the V -dimensional simplex,
forming a distribution over words: pi(?t,n1:t?1)v =
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
, where n1:t?1,v ? RV is a
background distribution (the log-frequency of word
v observed up to time t? 1).
6 Experiments
In our experiments, we consider the problem of pre-
dicting economy-related text appearing in news and
microblogs, based on observable features that reflect
current economic conditions in the world at a given
time. In the following, we describe our dataset in de-
tail, then show experimental results on text prediction.
In all experiments, we set the window size c = 7 (one
week) or c = 14 (two weeks), ? = 12|V | (V is thesize of vocabulary of the dataset under consideration),
and ? = 1.
6.1 Dataset
Our data contains metadata and text corpora. The
metadata is used as our features, whereas the text
corpora are used for learning language models and
predictions. The dataset (excluding Twitter) can
be downloaded at http://www.ark.cs.cmu.
edu/DynamicLM.
6.1.1 Metadata
We use end-of-day stock prices gathered from
finance.yahoo.com for each stock included in
the Standard & Poor?s 500 index (S&P 500). The
index includes large (by market value) companies
listed on US stock exchanges.7 We calculate daily
(continuously compounded) returns for each stock, o:
ro,t = logPo,t? logPo,t?1, where Po,t is the closing
stock price.8 We make a simplifying assumption that
text for day t is generated after Po,t is observed.9
In general, stocks trade Monday to Friday (except
for federal holidays and natural disasters). For days
when stocks do not trade, we set ro,t = 0 for all
stocks since any price change is not observed.
We transform returns into similarity values as fol-
lows: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k)
and 0 otherwise. While this limits the model by ig-
noring the magnitude of price changes, it is still rea-
sonable to capture the similarity between two days.10
There are 500 stocks in the S&P 500, so xt ? R500
and f(xt,xk) ? R500.
6.1.2 Text data
We have five streams of text data. The first four
corpora are news streams tracked through Reuters.11
Two of them are written in English, North American
Business Report (EN:NA) and Japanese Investment
News (EN:JP). The remaining two are German Eco-
nomic News Service (DE, in German) and French
Economic News Service (FR, in French). For all four
of the Reuters streams, we collected news data over
a period of thirteen months (392 days), 2012-05-26
to 2013-06-21. See Table 1 for descriptive statistics
of these datasets. Numerical terms are mapped to a
single word, and all letters are downcased.
The last text stream comes from the Deca-
hose/Gardenhose stream from Twitter. We collected
public tweets that contain ticker symbols (i.e., sym-
bols that are used to denote stocks of a particular
company in a stock market), preceded by the dollar
7For a list of companies listed in the S&P 500 as of
2012, see http://en.wikipedia.org/wiki/List_
of_S\%26P_500_companies. This set was fixed during
the time periods of all our experiments.
8We use the ?adjusted close? on Yahoo that includes interim
dividend cash flows and also adjusts for ?splits? (changes in the
number of outstanding shares).
9This is done in order to avoid having to deal with hourly
timesteps. In addition, intraday price data is only available
through commercial data provided.
10Note that daily stock returns are equally likely to be positive
or negative and display little serial correlation.
11http://www.reuters.com
186
Dataset Total # Doc. Avg. # Doc. #Days Unigrams BigramsTotal # Tokens Size Vocab. Total # Tokens Size Vocab.
EN:NA 86,683 223 392 28,265,550 10,000 11,804,201 5,000
EN:JP 70.807 182 392 16,026,380 10,000 7,047,095 5,000
FR 62,355 160 392 11,942,271 10,000 3,773,517 5,000
DE 51,515 132 392 9,027,823 10,000 3,499,965 5,000
Twitter 214,794 336 639 1,660,874 10,000 551,768 5,000
Table 1: Statistics about the datasets. Average number of documents (third column) is per day.
sign $ (e.g., $GOOG, $MSFT, $AAPL, etc.). These
tags are generally used to indicate tweets about the
stock market. We look at tweets from the period
2011-01-01 to 2012-09-30 (639 days). As a result,
we have approximately 100?800 tweets per day. We
tokenized the tweets using the CMU ARK TweetNLP
tools,12 numerical terms are mapped to a single word,
and all letters are downcased.
We perform two experiments using unigram and
bigram language models as the base models. For
each dataset, we consider the top 10,000 unigrams
after removing corpus-specific stopwords (the top
100 words with highest frequencies). For the bigram
experiments, we only use 5,000 words to limit the
number of unique bigrams so that we can simulate
experiments for the entire time horizon in a reason-
able amount of time. In standard open-vocabulary
language modeling experiments, the treatment of un-
known words deserves care. We have opted for a
controlled, closed-vocabulary experiment, since stan-
dard smoothing techniques will almost surely interact
with temporal dynamics and context in interesting
ways that are out of scope in the present work.
6.2 Baselines
Since this is a forecasting task, at each timestep, we
only have access to data from previous timesteps.
Our model assumes that all words in all documents
in a corpus come from a single multinomial distri-
bution. Therefore, we compare our approach to the
corresponding base models (standard unigram and bi-
gram language models) over the same vocabulary (for
each stream). The first one maintains counts of every
word and updates the counts at each timestep. This
corresponds to a base model that uses all of the avail-
able data up to the current timestep (?base all?). The
second one replaces counts of every word with the
12https://www.ark.cs.cmu.edu/TweetNLP
counts from the previous timestep (?base one?). Ad-
ditionally, we also compare with a base model whose
counts decay exponentially (?base exp?). That is, the
counts from previous timesteps decay by exp(??s),
where s is the distance between previous timesteps
and the current timestep and ? is the decay constant.
We set the decay constant ? = 1. We put a symmetric
Dirichlet prior on the counts (?add-one? smoothing);
this is analogous to our treatment of the background
frequencies n in our model. Note that our model,
similar to ?base all,? uses all available data up to
timestep t? 1 when making predictions for timestep
t. The window size c only determines which previ-
ous timesteps? models can be chosen for making a
prediction today. The past models themselves are es-
timated from all available data up to their respective
timesteps.
We also compare with two strong baselines: a lin-
ear interpolation of ?base one? models for the past
week (?int. week?) and a linear interpolation of ?base
all? and ?base one? (?int one all?). The interpolation
weights are learned online using the normalized expo-
nentiated gradient algorithm (Kivinen and Warmuth,
1997), which has been shown to enjoy a stronger
regret guarantee compared to standard online gra-
dient descent for learning a convex combination of
weights.
6.3 Results
We evaluate the perplexity on unseen dataset to eval-
uate the performance of our model. Specifically, we
use per-word predictive perplexity:
perplexity = exp
(
?
?T
t=1 log p(wt | ?,x1:t,n1:t?1)?T
t=1
?
j?V wt,j
)
.
Note that the denominator is the number of tokens
up to timestep T . Lower perplexity is better.
Table 2 and Table 3 show the perplexity results for
187
Dataset base all base one base exp int. week int. one all c = 7 c = 14
EN:NA 3,341 3,677 3,486 3,403 3,271 3,262 3,285
EN:JP 2,802 3,212 2,750 2,949 2,708 2,656 2,689
FR 3,603 3,910 3,678 3,625 3,416 3,404 3,438
DE 3,789 4,199 3,979 3,926 3,634 3,649 3,687
Twitter 3,880 6,168 5,133 5,859 4,047 3,801 3,819
Table 2: Perplexity results for our five data streams in the unigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are unigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost two columns are versions of
our model. Best results are highlighted in bold.
Dataset base all base one base exp int. week int. one all c = 7
EN:NA 242 2,229 1,880 2,200 244 223
EN:JP 185 2,101 1,726 2,050 189 167
FR 159 2,084 1,707 2,068 166 139
DE 268 2,634 2,267 2,644 282 243
Twitter 756 4,245 4,253 5,859 4,046 739
Table 3: Perplexity results for our five data streams in the bigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are bigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost column is a version of our
model with c = 7. Best results are highlighted in bold.
each of the datasets for unigram and bigram experi-
ments respectively. Our model outperformed other
competing models in all cases but one. Recall that we
only define the similarity function of world context
as: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k) and
0 otherwise. A better similarity function (e.g., one
that takes into account market size of the company
and the magnitude of increase or decrease in the stock
price) might be able to improve the performance fur-
ther. We leave this for future work. Furthermore,
the variations can be captured using models from the
past week. We discuss why increasing c from 7 to 14
did not improve performance of the model in more
detail in ?6.4.
We can also see how the models performed over
time. Figure 4 traces perplexity for four Reuters news
stream datasets.13 We can see that in some cases the
performance of the ?base all? model degraded over
time, whereas our model is more robust to temporal
13In both experiments, in order to manage the time and space
complexities of updating ?, we apply a sparsity shrinkage tech-
nique by using OWL-QN (Andrew and Gao, 2007) when maxi-
mizing it, with regularization constant set to 1. Intuitively, this
is equivalent to encouraging the deviation vector to be sparse
(Eisenstein et al., 2011).
shifts.
In the bigram experiments, we only ran our model
with c = 7, since we need to maintain ? in RV 2 ,
instead of RV in the unigram model. The goal of
this experiment is to determine whether our method
still adds benefit to more expressive language mod-
els. Note that the weights of the linear interpolation
models are also learned in an online fashion since
there are no classical training, development, and test
sets in our setting. Since the ?base one? model per-
formed poorly in this experiment, the performance of
the interpolated models also suffered. For example,
the ?int. one all? model needed time to learn that the
?base one? model has to be downweighted (we started
with all interpolated models having uniform weights),
so it was not able to outperform even the ?base all?
model.
6.4 Analysis and Discussion
It should not be surprising that conditioning on
world-context reduces perplexity (Cover and Thomas,
1991). A key attraction of our model, we believe, lies
in the ability to inspect its parameters.
Deviation coefficients. Inspecting the model al-
lows us to gain insight into temporal trends. We
188
Twitter:Google
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
2.0
goog
@google
google+
#goog
r
GOOG
Twitter:Microsoft
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
microsoft
msft
#microsoft
r
MSFT
Figure 3: Deviation coefficients ? over time for Google- and Microsoft-related words on Twitter with unigram base
model (c = 7). Significant changes (increases or decreases) in the returns of Google and Microsoft stocks are usually
followed by increases in ? of related words.
investigate the deviations learned by our model on the
Twitter dataset. Examples are shown in Figure 3. The
left plot shows ? for four words related to Google:
goog, #goog, @google, google+. For compari-
son, we also show the return of Google stock for the
corresponding timestep (scaled by 50 and centered at
0.5 for readability, smoothed using loess (Cleveland,
1979), denoted by rGOOG in the plot). We can see
that significant changes of return of Google stocks
(e.g., the rGOOG spikes between timesteps 50?100,
150?200, 490?550 in the plot) occurred alongside
an increase in ? of Google-related words. Similar
trends can also be observed for Microsoft-related
words in the right plot. The most significant loss of
return of Microsoft stocks (the downward spike near
timestep 500 in the plot) is followed by a sudden
sharp increase in ? of the words #microsoft and
microsoft.
Feature coefficients. We can also inspect the
learned feature coefficients ? to investigate which
stocks have higher associations with the text that
is generated. Our feature coefficients are designed
to reflect which changes (or lack of changes) in
stock prices influence the word distribution more,
not which stocks are talked about more often. We
find that the feature coefficients do not correlate with
obvious company characteristics like market capi-
talization (firm size). For example, on the Twitter
dataset with bigram base models, the five stocks with
the highest weights are: ConAgra Foods Inc., Intel
Corp., Bristol-Myers Squibb, Frontier Communica-
tions Corp., and Amazon.com Inc. Strongly negative
weights tended to align with streams with less activ-
time lags
frequ
ency
0
20
40
60
80
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Figure 5: Distributions of the selection probabilities of
models from the previous c = 14 timesteps, on the EN:NA
dataset with unigram base model. For simplicity, we show
E-step modes. The histogram shows that the model tends
to favor models from days closer to the current date.
ity, suggesting that these were being used to smooth
across all c days of history. A higher weight for stock
o implies an increase in probability of choosing mod-
els from previous timesteps s, when the state of the
world for the current timestep t and timestep s is the
same (as represented by our similarity function) with
respect to stock o (all other things being equal), and
a decrease in probability for a lower weight.
Selected models. Besides feature coefficients, our
model captures temporal shift by modeling similar-
ity across the most recent c days. During inference,
our model weights different word distributions from
the past. The similarity is encoded in the pairwise
features f(xt,xk) and the parameters ?. Figure 5
shows the distributions of the strongest-posterior
models from previous timesteps, based on how far
189
EN:NA
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
EN:JP
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
FR
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
DE
timestep
perpl
exity
0 50 100 150 200 250 300 350
300
500
700
base all
complete
int. one all
Figure 4: Perplexity over time for four Reuters news streams (c = 7) with bigram base models.
190
in the past they are at the time of use, aggregated
across rounds on the EN:NA dataset, for window size
c = 14. It shows that the model tends to favor models
from days closer to the current date, with the t ? 1
models selected the most, perhaps because the state
of the world today is more similar to dates closer to
today compare to more distant dates. The plot also
explains why increasing c from 7 to 14 did not im-
prove performance of the model, since most of the
variation in our datasets can be captured with models
from the past week.
Topics. Latent topic variables have often figured
heavily in approaches to dynamic language model-
ing. In preliminary experiments incorporating single-
membership topic variables (i.e., each document be-
longs to a single topic, as in a mixture of unigrams),
we saw no benefit to perplexity. Incorporating top-
ics also increases computational cost, since we must
maintain and estimate one language model per topic,
per timestep. It is straightforward to design mod-
els that incorporate topics with single- or mixed-
membership as in LDA (Blei et al., 2003), an in-
teresting future direction.
Potential applications. Dynamic language models
like ours can be potentially useful in many applica-
tions, either as a standalone language model, e.g.,
predictive text input, whose performance may de-
pend on the temporal dimension; or as a component
in applications like machine translation or speech
recognition. Additionally, the model can be seen as
a step towards enhancing text understanding with
numerical, contextual data.
7 Conclusion
We presented a dynamic language model for stream-
ing datasets that allows conditioning on observable
real-world context variables, exemplified in our ex-
periments by stock market data. We showed how to
perform learning and inference in an online fashion
for this model. Our experiments showed the predic-
tive benefit of such conditioning and online learning
by comparing to similar models that ignore temporal
dimensions and observable variables that influence
the text.
Acknowledgements
The authors thank several anonymous reviewers for help-
ful feedback on earlier drafts of this paper and Brendan
O?Connor for help with collecting Twitter data. This re-
search was supported in part by Google, by computing
resources at the Pittsburgh Supercomputing Center, by
National Science Foundation grant IIS-1111142, AFOSR
grant FA95501010247, ONR grant N000140910758, and
by the Intelligence Advanced Research Projects Activ-
ity via Department of Interior National Business Center
contract number D12PC00347. The U.S. Government is
authorized to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as nec-
essarily representing the official policies or endorsements,
either expressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of l1-regularized log-linear models. In Proc. of ICML.
David M. Blei and John D. Lafferty. 2006. Dynamic topic
models. In Proc. of ICML.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Se?bastien Bubeck. 2011. Introduction to online opti-
mization. Technical report, Department of Operations
Research and Financial Engineering, Princeton Univer-
sity.
Nicolo` Cesa-Bianchi and Ga?bor Lugosi. 2006. Prediction,
Learning, and Games. Cambridge University Press.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American Statistical Association, 74(368):829?836.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of
Information Theory. John Wiley & Sons.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10(7):2899?
2934.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc. of
ICML.
Amit Goyal, Hal Daume III, and Suresh Venkatasubrama-
nian. 2009. Streaming for large scale NLP: Language
modeling. In Proc. of HLT-NAACL.
191
Matt Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic variational inference. Jour-
nal of Machine Learning Research, 14:1303?1347.
Antti Honkela and Harri Valpola. 2003. On-line varia-
tional Bayesian learning. In Proc. of ICA.
Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and
Naonori Ueda. 2010. Online multiscale dynamic topic
models. In Proc. of KDD.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1?63.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Mining
of concurrent text and time series. In Proc. of KDD
Workshop on Text Mining.
Abby Levenberg and Miles Osborne. 2009. Stream-based
randomised language models for SMT. In Proc. of
EMNLP.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for sta-
tistical machine translation. In Proc. of HLT-NAACL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
David Mimno and Andrew McCallum. 2008. Topic mod-
els conditioned on arbitrary features with Dirichlet-
multinomial regression. In Proc. of UAI.
Alexander Rakhlin. 2009. Lecture notes on online learn-
ing. Technical report, Department of Statistics, The
Wharton School, University of Pennsylvania.
Masaaki Sato. 2001. Online model selection based on the
variational bayes. Neural Computation, 13(7):1649?
1681.
Shai Shalev-Shwartz. 2012. Online learning and online
convex optimization. Foundations and Trends in Ma-
chine Learning, 4(2):107?194.
Martin J. Wainwright and Michael I. Jordan. 2008. Graph-
ical models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1(1?2):1?305.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Proc.
of UAI.
Chong Wang, John Paisley, and David M. Blei. 2011. On-
line variational inference for the hierarchical Dirichlet
process. In Proc. of AISTATS.
Martin Zinkevich. 2003. Online convex programming
and generalized infinitesimal gradient ascent. In Proc.
of ICML.
192
Discriminative Lexical Semantic Segmentation with Gaps:
Running the MWE Gamut
Nathan Schneider Emily Danchik Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nschneid,emilydan,cdyer,nasmith}@cs.cmu.edu
Abstract
We present a novel representation, evaluation
measure, and supervised models for the task of
identifying the multiword expressions (MWEs)
in a sentence, resulting in a lexical seman-
tic segmentation. Our approach generalizes
a standard chunking representation to encode
MWEs containing gaps, thereby enabling effi-
cient sequence tagging algorithms for feature-
rich discriminative models. Experiments on a
new dataset of English web text offer the first
linguistically-driven evaluation of MWE iden-
tification with truly heterogeneous expression
types. Our statistical sequence model greatly
outperforms a lookup-based segmentation pro-
cedure, achieving nearly 60% F1 for MWE
identification.
1 Introduction
Language has a knack for defying expectations when
put under the microscope. For example, there is the
notion?sometimes referred to as compositionality?
that words will behave in predictable ways, with indi-
vidual meanings that combine to form complex mean-
ings according to general grammatical principles. Yet
language is awash with examples to the contrary:
in particular, idiomatic expressions such as awash
with NP, have a knack for VP-ing, to the contrary, and
defy expectations. Thanks to processes like metaphor
and grammaticalization, these are (to various degrees)
semantically opaque, structurally fossilized, and/or
statistically idiosyncratic. In other words, idiomatic
expressions may be exceptional in form, function,
or distribution. They are so diverse, so unruly, so
1. MW named entities: Prime Minister Tony Blair
2. MW compounds: hot air balloon, skinny dip
3. conventionally SW compounds: somewhere
4. verb-particle: pick up, dry out, take over, cut short
5. verb-preposition: refer to, depend on, look for
6. verb-noun(-preposition): pay attention (to)
7. support verb: make decisions, take pictures
8. other phrasal verb: put up with, get rid of
9. PP modifier: above board, at all, from time to time
10. coordinated phrase: cut and dry, more or less
11. connective: as well as, let alone, in spite of
12. semi-fixed VP: pick up where <one> left off
13. fixed phrase: scared to death, leave of absence
14. phatic: You?re welcome. Me neither!
15. proverb: Beggars can?t be choosers.
Figure 1: Some of the classes of idioms in English.
The examples included here contain multiple lexicalized
words?with the exception of those in (3), if the conven-
tional single-word (SW) spelling is used.
difficult to circumscribe, that entire theories of syn-
tax are predicated on the notion that constructions
with idiosyncratic form-meaning mappings (Fillmore
et al., 1988; Goldberg, 1995) or statistical properties
(Goldberg, 2006) offer crucial evidence about the
grammatical organization of language.
Here we focus on multiword expressions
(MWEs): lexicalized combinations of two or more
words that are exceptional enough to be considered
as single units in the lexicon. As figure 1 illus-
trates, MWEs occupy diverse syntactic and semantic
functions. Within MWEs, we distinguish (a) proper
names and (b) lexical idioms. The latter have proved
themselves a ?pain in the neck for NLP? (Sag et al.,
2002). Automatic and efficient detection of MWEs,
though far from solved, would have diverse appli-
193
Transactions of the Association for Computational Linguistics, 2 (2014) 193?206. Action Editor: Joakim Nivre.
Submitted 12/2013; Revised 1/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
cations including machine translation (Carpuat and
Diab, 2010), information retrieval (Newman et al.,
2012), opinion mining (Berend, 2011), and second
language learning (Ellis et al., 2008).
It is difficult to establish any comprehensive tax-
onomy of multiword idioms, let alone develop lin-
guistic criteria and corpus resources that cut across
these types. Consequently, the voluminous litera-
ture on MWEs in computational linguistics?see ?7,
Baldwin and Kim (2010), and Ramisch (2012) for
surveys?has been fragmented, looking (for exam-
ple) at subclasses of phrasal verbs or nominal com-
pounds in isolation. To the extent that MWEs have
been annotated in existing corpora, it has usually
been as a secondary aspect of some other scheme.
Traditionally, such resources have prioritized certain
kinds of MWEs to the exclusion of others, so they
are not appropriate for evaluating general-purpose
identification systems.
In this article, we briefly review a shallow form
of analysis for MWEs that is neutral to expression
type, and that facilitates free text annotation with-
out requiring a prespecified MWE lexicon (?2). The
scheme applies to gappy (discontinuous) as well as
contiguous expressions, and allows for a qualitative
distinction of association strengths. In Schneider
et al. (2014) we have applied this scheme to fully an-
notate a 55,000-word corpus of English web reviews
(Bies et al., 2012a), a conversational genre in which
colloquial idioms are highly salient. This article?s
main contribution is to show that the representation?
constrained according to linguistically motivated as-
sumptions (?3)?can be transformed into a sequence
tagging scheme that resembles standard approaches
in named entity recognition and other text chunking
tasks (?4). Along these lines, we develop a discrim-
inative, structured model of MWEs in context (?5)
and train, evaluate, and examine it on the annotated
corpus (?6). Finally, in ?7 and ?8 we comment on
related work and future directions.
2 Annotated Corpus
To build and evaluate a multiword expression ana-
lyzer, we use the MWE-annotated corpus of Schnei-
der et al. (2014). It consists of informal English web
text that has been specifically and completely anno-
tated for MWEs, without reference to any particular
lexicon. To the best of our knowledge, this corpus
is the first to be freely annotated for many kinds of
MWEs (without reference to a lexicon), and is also
the first dataset of social media text with MWE an-
notations beyond named entities. This section gives
a synopsis of the annotation conventions used to de-
velop that resource, as they are important to under-
standing our models and evaluation.
Rationale. The multiword expressions community
has lacked a canonical corpus resource comparable
to benchmark datasets used for problems such as
NER and parsing. Consequently, the MWE litera-
ture has been driven by lexicography: typically, the
goal is to acquire an MWE lexicon with little or no
supervision, or to apply such a lexicon to corpus
data. Studies of MWEs in context have focused on
various subclasses of constructions in isolation, ne-
cessitating special-purpose datasets and evaluation
schemes. By contrast, Schneider et al.?s (2014) cor-
pus creates an opportunity to tackle general-purpose
MWE identification, such as would be desirable for
use by high-coverage downstream NLP systems. It is
used to train and evaluate our models below. The cor-
pus is publicly available as a benchmark for further
research.1
Data. The documents in the corpus are online user
reviews of restaurants, medical providers, retailers,
automotive services, pet care services, etc. Marked
by conversational and opinionated language, this
genre is fertile ground for colloquial idioms (Nunberg
et al., 1994; Moon, 1998). The 723 reviews (55,000
words, 3,800 sentences) in the English Web Tree-
bank (WTB; Bies et al., 2012b) were collected by
Google, tokenized, and annotated with phrase struc-
ture trees in the style of the Penn Treebank (Marcus
et al., 1993). MWE annotators used the sentence and
word tokenizations supplied by the treebank.2
Annotation scheme. The annotation scheme itself
was designed to be as simple as possible. It consists
of grouping together the tokens in each sentence that
belong to the same MWE instance. While annotation
guidelines provide examples of MWE groupings in
a wide range of constructions, the annotator is not
1http://www.ark.cs.cmu.edu/LexSem/
2Because we use treebank data, syntactic parses are available
to assist in post hoc analysis. Syntactic information was not
shown to annotators.
194
# of constituent tokens
2 3 ?4 total
strong 2257 595 172 3024
weak 269 121 69 459
2526 716 241 3483
# of gaps
0 1 2
2626 394 4
322 135 2
2948 529 6
Table 1: Counts in the MWE corpus.
tied to any particular taxonomy or syntactic structure.
This simplifies the number of decisions that have to
be made for each sentence, even if some are difficult.
Further instructions to annotators included:
? Groups should include only the lexically fixed parts
of an expression (modulo inflectional morphology);
this generally excludes determiners and pronouns:
made the mistake, pride themselves on.
? Multiword proper names count as MWEs.
? Misspelled or unconventionally spelled tokens are
interpreted according to the intended word if clear.
? Overtokenized words (spelled as two tokens, but
conventionally one word) are joined as multiwords.
Clitics separated by the tokenization in the corpus?
negative n?t, possessive ?s, etc.?are joined if func-
tioning as a fixed part of a multiword (e.g., T ?s
Cafe), but not if used productively.
Gaps. There are, broadly speaking, three reasons
to group together tokens that are not fully contigu-
ous. Most commonly, gaps contain internal modifiers,
such as good in make good decisions. Syntactic con-
structions such as the passive can result in gaps that
might not otherwise be present: in good decisions
were made, there is instead a gap filled by the pas-
sive auxiliary. Finally, some MWEs may take internal
arguments: they gave me a break. Figure 1 has addi-
tional examples. Multiple gaps can occur even within
the same expression, though it is rare: they agreed to
give Bob a well-deserved break.
Strength. The annotation scheme has two
?strength? levels for MWEs. Clearly idiomatic ex-
pressions are marked as strong MWEs, while mostly
compositional but especially frequent collocations/
phrases (e.g., abundantly clear and patently obvious)
are marked as weak MWEs. Weak multiword groups
are allowed to include strong MWEs as constituents
(but not vice versa). Strong groups are required to
cohere when used inside weak groups: that is, a weak
group cannot include only part of a strong group.
For purposes of annotation, there were no constraints
hinging on the ordering of tokens in the sentence.
Process. MWE annotation proceeded one sentence
at a time. The 6 annotators referred to and improved
the guidelines document on an ongoing basis. Every
sentence was seen independently by at least 2 an-
notators, and differences of opinion were discussed
and resolved (often by marking a weak MWE as a
compromise). See Schneider et al. (2014) for details.
Statistics. The annotated corpus consists of 723
documents (3,812 sentences). MWEs are frequent
in this domain: 57% of sentences (72% of sentences
over 10 words long) and 88% of documents contain
at least one MWE. 8,060/55,579=15% of tokens
belong to an MWE; in total, there are 3,483 MWE
instances. 544 (16%) are strong MWEs containing a
gold-tagged proper noun?most are proper names. A
breakdown appears in table 1.
3 Representation and Task Definition
We define a lexical segmentation of a sentence as a
partitioning of its tokens into segments such that each
segment represents a single unit of lexical meaning.
A multiword lexical expression may contain gaps,
i.e. interruptions by other segments. We impose two
restrictions on gaps that appear to be well-motivated
linguistically:
? Projectivity: Every expression filling a gap must
be completely contained within that gap; gappy
expressions may not interleave.
? No nested gaps: A gap in an expression may be
filled by other single- or multiword expressions, so
long as those do not themselves contain gaps.
Formal grammar. Our scheme corresponds to the
following extended CFG (Thatcher, 1967), where S
is the full sentence and terminals w are word tokens:
S ? X+
X ? w+ (Y+ w+)?
Y ? w+
Each expression X or Y is lexicalized by the words in
one or more underlined variables on the right-hand
side. An X constituent may optionally contain one or
more gaps filled by Y constituents, which must not
contain gaps themselves.3
3MWEs with multiple gaps are rare but attested in data: e.g.,
putting me at my ease. We encountered one violation of the gap
nesting constraint in the reviews data: I have21 nothing21 but21
fantastic things2 to21 say21 . Additionally, the interrupted phrase
195
Denoting multiword groupings with subscripts, My
wife had taken1 her ?072 Ford2 Fusion2 in1 for a
routine oil3 change3 contains 3 multiword groups?{taken, in}, {?07, Ford, Fusion}, {oil, change}?and
7 single-word groups. The first MWE is gappy (ac-
centuated by the box); a single word and a contiguous
multiword group fall within the gap. The projectivity
constraint forbids an analysis like taken1 her ?072
Ford1 Fusion2, while the gap nesting constraint for-
bids taken1 her2 ?07 Ford2 Fusion2 in1.
3.1 Two-level Scheme: Strong vs. Weak MWEs
Our annotated data distinguish two strengths of
MWEs as discussed in ?2. Augmenting the gram-
mar of the previous section, we therefore designate
nonterminals as strong (X , Y ) or weak (X? , Y? ):
S ? X?+
X? ? X+ (Y?+ X+)?
X ? w+ (Y?+ w+)?
Y? ? Y+
Y ? w+
A weak MWE may be lexicalized by single words
and/or strong multiwords. Strong multiwords cannot
contain weak multiwords except in gaps. Further, the
contents of a gap cannot be part of any multiword
that extends outside the gap.4
For example, consider the segmentation: he was
willing to budge1 a2 little2 on1 the price which
means4 a43 lot43 to4 me4. Subscripts denote strong
MW groups and superscripts weak MW groups; un-
marked tokens serve as single-word expressions. The
MW groups are thus {budge, on}, {a, little}, {a, lot},
and {means, {a, lot}, to, me}. As should be evident
from the grammar, the projectivity and gap-nesting
constraints apply here just as in the 1-level scheme.
3.2 Evaluation
Matching criteria. Given that most tokens do not
belong to an MWE, to evaluate MWE identification
we adopt a precision/recall-based measure from the
coreference resolution literature. The MUC criterion
(Vilain et al., 1995) measures precision and recall
great gateways never1 before1 , so23 far23 as23 Hudson knew2 ,
seen1 by Europeans was annotated in another corpus.
4This was violated 6 times in our annotated data: modifiers
within gaps are sometimes collocated with the gappy expression,
as in on12 a12 tight1 budget12 and have12 little1 doubt12.
of links in terms of groups (units) implied by the
transitive closure over those links.5 It can be defined
as follows:
Let a ? b denote a link between two elements
in the gold standard, and a??b denote a link in the
system prediction. Let the ? operator denote the tran-
sitive closure over all links, such that ?a??b? is 1 if
a and b belong to the same (gold) set, and 0 other-
wise. Assuming there are no redundant6 links within
any annotation (which in our case is guaranteed by
linking consecutive words in each MWE), we can
write the MUC precision and recall measures as:
P = ?a,b?a??b ?a??b??a,b?a??b 1 R =
?a,b?a?b ?a???b??a,b?a?b 1
This awards partial credit when predicted and gold
expressions overlap in part. Requiring full MWEs to
match exactly would arguably be too stringent, over-
penalizing larger MWEs for minor disagreements.
We combine precision and recall using the standard
F1 measure of their harmonic mean. This is the link-
based evaluation used for most of our experiments.
For comparison, we also report some results with
a more stringent exact match evaluation where the
span of the predicted MWE must be identical to the
span of the gold MWE for it to count as correct.
Strength averaging. Recall that the 2-level
scheme (?3.1) distinguishes strong vs. weak links/
groups, where the latter category applies to reason-
ably compositional collocations as well as ambigu-
ous or difficult cases. If where one annotation uses
a weak link the other has a strong link or no link at
all, we want to penalize the disagreement less than
if one had a strong link and the other had no link.
To accommodate the 2-level scheme, we therefore
average F?1 , in which all weak links have been con-
verted to strong links, and F?1 , in which they have
been removed: F1 = 12(F?1 +F?1 ).7 If neither annota-tion contains any weak links, this equals the MUC
5As a criterion for coreference resolution, the MUC measure
has perceived shortcomings which have prompted several other
measures (see Recasens and Hovy, 2011 for a review). It is not
clear, however, whether any of these criticisms are relevant to
MWE identification.
6A link between a and b is redundant if the other links already
imply that a and b belong to the same set. A set of N elements is
expressed non-redundantly with exactly N ?1 links.
7Overall precision and recall are likewise computed by aver-
aging ?strengthened? and ?weakened? measurements.
196
no gaps,
1-level
he
O
was
O
willing
O
to
O
budge
O
a
B
little
I
on
O
the
O
price
O
which
O
means
B
a
I
lot
I
to
I
me
I
.
O
(O?BI+)+
no gaps,
2-level
he
O
was
O
willing
O
to
O
budge
O
a
B
little
I?
on
O
the
O
price
O
which
O
means
B
a
I?
lot
I?
to
I?
me
I?
.
O
(O?B[I?I?]+)+
gappy,
1-level
he
O
was
O
willing
O
to
O
budge
B
a
b
little
i
on
I
the
O
price
O
which
O
means
B
a
I
lot
I
to
I
me
I
.
O
(O?B(o?bi+?I)?I+)+
gappy,
2-level
he
O
was
O
willing
O
to
O
budge
B
a
b
little
??
on
I?
the
O
price
O
which
O
means
B
a
I?
lot
I?
to
I?
me
I?
.
O
(O?B(o?b[????]+?[I?I?])?[I?I?]+)+
Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and
weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications.
score because F1 = F?1 = F?1 . This method appliesto both the link-based and exact match evaluation
criteria.
4 Tagging Schemes
Following (Ramshaw and Marcus, 1995), shallow an-
alysis is often modeled as a sequence-chunking task,
with tags containing chunk-positional information.
The BIO scheme and variants (e.g., BILOU; Ratinov
and Roth, 2009) are standard for tasks like named
entity recognition, supersense tagging, and shallow
parsing.
The language of derivations licensed by the gram-
mars in ?3 allows for a tag-based encoding of MWE
analyses with only bigram constraints. We describe
4 tagging schemes for MWE identification, starting
with BIO and working up to more expressive variants.
They are depicted in figure 2.
No gaps, 1-level (3 tags). This is the standard con-
tiguous chunking representation from Ramshaw and
Marcus (1995) using the tags {O B I}. O is for to-
kens outside any chunk; B marks tokens beginning
a chunk; and I marks other tokens inside a chunk.
Multiword chunks will thus start with B and then I.
B must always be followed by I; I is not allowed at
the beginning of the sentence or following O.
No gaps, 2-level (4 tags). We can distinguish
strength levels by splitting I into two tags: I? for
strong expressions and I? for weak expressions. To
express strong and weak contiguous chunks requires
4 tags: {O B I? I?}. (Marking B with a strength as well
would be redundant because MWEs are never length-
one chunks.) The constraints on I? and I? are the same
as the constraints on I in previous schemes. If I? and
I? occur next to each other, the strong attachment will
receive higher precedence, resulting in analysis of
strong MWEs as nested within weak MWEs.
Gappy, 1-level (6 tags). Because gaps cannot
themselves contain gappy expressions (we do not
support full recursivity), a finite number of additional
tags are sufficient to encode gappy chunks. We there-
fore add lowercase tag variants representing tokens
within a gap: {O o B b I i}. In addition to the con-
straints stated above, no within-gap tag may occur at
the beginning or end of the sentence or immediately
following or preceding O. Within a gap, b, i, and o
behave like their out-of-gap counterparts.
Gappy, 2-level (8 tags). 8 tags are required to en-
code the 2-level scheme with gaps: {O o B b I? ?? I? ??}.
Variants of the inside tag are marked for strength of
the incoming link?this applies gap-externally (capi-
talized tags) and gap-internally (lowercase tags). If I?
or I? immediately follows a gap, its diacritic reflects
the strength of the gappy expression, not the gap?s
contents.
5 Model
With the above representations we model MWE iden-
tification as sequence tagging, one of the paradigms
that has been used previously for identifying con-
tiguous MWEs (Constant and Sigogne, 2011, see
?7).8 Constraints on legal tag bigrams are sufficient
to ensure the full tagging is well-formed subject to
the regular expressions in figure 2; we enforce these
8Hierarchical modeling based on our representations is left
to future work.
197
constraints in our experiments.9
In NLP, conditional random fields (Lafferty et al.,
2001) and the structured perceptron (Collins, 2002)
are popular techniques for discriminative sequence
modeling with a convex loss function. We choose
the second approach for its speed: learning and in-
ference depend mainly on the runtime of the Viterbi
algorithm, whose asymptotic complexity is linear in
the length of the input and (with a first-order Markov
assumption) quadratic in the number of tags. Below,
we review the structured perceptron and discuss our
cost function, features, and experimental setup.
5.1 Cost-Augmented Structured Perceptron
The structured perceptron?s (Collins, 2002) learn-
ing procedure, algorithm 1, generalizes the classic
perceptron algorithm (Freund and Schapire, 1999) to
incorporate a structured decoding step (for sequences,
the Viterbi algorithm) in the inner loop. Thus, train-
ing requires only max inference, which is fast with a
first-order Markov assumption. In training, features
are adjusted where a tagging error is made; the pro-
cedure can be viewed as optimizing the structured
hinge loss. The output of learning is a weight vector
that parametrizes a feature-rich scoring function over
candidate labelings of a sequence.
To better align the learning algorithm with our
F-score?based MWE evaluation (?3.2), we use a
cost-augmented version of the structured perceptron
that is sensitive to different kinds of errors during
training. When recall is the bigger obstacle, we can
adopt the following cost function: given a sentence
x, its gold labeling y?, and a candidate labeling y?,
cost(y?,y?,x) = ?y???
j=1c(y?j ,y?j) where
c(y?,y?) = ?y? ? y??+??y? ? {B,b}?y? ? {O,o}?
A single nonnegative hyperparameter, ? , controls
the tradeoff between recall and accuracy; higher ?
biases the model in favor of recall (possibly hurt-
ing accuracy and precision). This is a slight variant
of the recall-oriented cost function of Mohit et al.
(2012). The difference is that we only penalize
beginning-of-expression recall errors. Preliminary
9The 8-tag scheme licenses 42 tag bigrams: sequences such
as B O and o ?? are prohibited. There are also constraints on the
allowed tags at the beginning and end of the sequence.
Input: data ??x(n),y(n)??Nn=1; number of iterations Mw? 0
w? 0
t ? 1
for m = 1 to M do
for n = 1 to N do?x,y?? ?x(n),y(n)?
y?? argmaxy? (w?g(x,y?)+cost(y,y?,x))
if y? ? y then
w?w+g(x,y)?g(x, y?)
w?w+ tg(x,y)? tg(x, y?)
end
t ? t +1end
end
Output: w?(w/t)
Algorithm 1: Training with the averaged perceptron.
(Adapted from Daum?, 2006, p. 19.)
experiments showed that a cost function penalizing
all recall errors?i.e., with ??y? ? O?y? = O? as the
second term, as in Mohit et al.?tended to append
additional tokens to high-confidence MWEs (such
as proper names) rather than encourage new MWEs,
which would require positing at least two new non-
outside tags.
5.2 Features
Basic features. These are largely based on those
of Constant et al. (2012): they look at word unigrams
and bigrams, character prefixes and suffixes, and POS
tags, as well as lexicon entries that match lemmas10
of multiple words in the sentence. Appendix A lists
the basic features in detail.
Some of the basic features make use of lexicons.
We use or construct 10 lists of English MWEs: all
multiword entries in WordNet (Fellbaum, 1998); all
multiword chunks in SemCor (Miller et al., 1993);
all multiword entries in English Wiktionary;11 the
WikiMwe dataset mined from English Wikipedia
(Hartmann et al., 2012); the SAID database of
phrasal lexical idioms (Kuiper et al., 2003); the
named entities and other MWEs in the WSJ corpus
on the English side of the CEDT (Hajic? et al., 2012);
10The WordNet API in NLTK (Bird et al., 2009) was used for
lemmatization.
11http://en.wiktionary.org; data obtained from
https://toolserver.org/~enwikt/definitions/
enwikt-defs-20130814-en.tsv.gz
198
LOOKUP SUPERVISED MODEL
preexising lexicons entries max gap
length
P R F1 ? P R F1 ?
none 0 74.39 44.43 55.57 2.19
WordNet + SemCor 71k 0 46.15 28.41 35.10 2.44 74.51 45.79 56.64 1.90
6 lexicons 420k 0 35.05 46.76 40.00 2.88 76.08 52.39 61.95 1.67
10 lexicons 437k 0 33.98 47.29 39.48 2.88 75.95 51.39 61.17 2.30
best configuration with
in-domain lexicon
1 46.66 47.90 47.18 2.31 76.64 51.91 61.84 1.65
2 lexicons + MWtypes(train)?1 6 lexicons + MWtypes(train)?2
Table 2: Use of lexicons for lookup-based vs. statistical segmentation. Supervised learning used only basic features
and the structured perceptron, with the 8-tag scheme. Results are with the link-based matching criterion for evaluation.
Top: Comparison of preexisting lexicons. ?6 lexicons? refers to WordNet and SemCor plus SAID, WikiMwe, Phrases.net,
and English Wiktionary; ?10 lexicons? adds MWEs from CEDT, VNC, LVC, and Oyz. (In these lookup-based
configurations, allowing gappy MWEs never helps performance.)
Bottom: Combining preexisting lexicons with a lexicon derived from MWEs annotated in the training portion of each
cross-validation fold at least once (lookup) or twice (model).
All precision, recall, and F1 percentages are averaged across 8 folds of cross-validation on train; standard deviations
are shown for the F1 score. In each column, the highest value using only preexisting lexicons is underlined, and the
highest overall value is bolded. The boxed row indicates the configuration used as the basis for subsequent experiments.
the verb-particle constructions (VPCs) dataset of
(Baldwin, 2008); a list of light verb constructions
(LVCs) provided by Claire Bonial; and two idioms
websites.12 After preprocessing, each lexical entry
consists of an ordered sequence of word lemmas,
some of which may be variables like <something>.
Given a sentence and one or more of the lexicons,
lookup proceeds as follows: we enumerate entries
whose lemma sequences match a sequence of lemma-
tized tokens, and build a lattice of possible analyses
over the sentence. We find the shortest path (i.e.,
using as few expressions as possible) with dynamic
programming, allowing gaps of up to length 2.13
Unsupervised word clusters. Distributional clus-
tering on large (unlabeled) corpora can produce lexi-
cal generalizations that are useful for syntactic and
semantic analysis tasks (e.g.: Miller et al., 2004; Koo
et al., 2008; Turian et al., 2010; Owoputi et al., 2013;
Grave et al., 2013). We were interested to see whether
a similar pattern would hold for MWE identification,
given that MWEs are concerned with what is lexi-
cally idiosyncratic?i.e., backing off from specific
lexemes to word classes may lose the MWE-relevant
information. Brown clustering14 (Brown et al., 1992)
12http://www.phrases.net/ and http://home.
postech.ac.kr/~oyz/doc/idiom.html
13Each top-level lexical expression (single- or multiword)
incurs a cost of 1; each expression within a gap has cost 1.25.
14With Liang?s (2005) implementation: https://github.
com/percyliang/brown-cluster. We obtain 1,000 clusters
on the 21-million-word Yelp Academic Dataset15
(which is similar in genre to the annotated web re-
views data) gives us a hard clustering of word types.
To our tagger, we add features mapping the previ-
ous, current, and next token to Brown cluster IDs.
The feature for the current token conjoins the word
lemma with the cluster ID.
Part-of-speech tags. We compared three PTB-
style POS taggers on the full REVIEWS subcor-
pus (train+test). The Stanford CoreNLP tagger16
(Toutanova et al., 2003) yields an accuracy of 90.4%.
The ARK TweetNLP tagger v. 0.3.2 (Owoputi et al.,
2013) achieves 90.1% with the model17 trained on the
Twitter corpus of Ritter et al. (2011), and 94.9% when
trained on the ANSWERS, EMAIL, NEWSGROUP, and
WEBLOG subcorpora of WTB. We use this third con-
figuration to produce automatic POS tags for training
and testing our MWE tagger. (A comparison condi-
tion in ?6.3 uses oracle POS tags.)
5.3 Experimental Setup
The corpus of web reviews described in ?2 is used
for training and evaluation. 101 arbitrarily chosen
documents (500 sentences, 7,171 words) were held
from words appearing at least 25 times.
15https://www.yelp.com/academic_dataset
16v. 3.2.0, with english-bidirectional-distsim
17http://www.ark.cs.cmu.edu/TweetNLP/model.
ritter_ptb_alldata_fixed.20130723
199
LINK-BASED EXACT MATCH
configuration M ? ?w? P R F1 P R F1
base model 5 ? 1,765k 69.27 50.49 58.35 60.99 48.27 53.85+ recall cost 4 150 1,765k 61.09 57.94 59.41 53.09 55.38 54.17+ clusters 3 100 2,146k 63.98 55.51 59.39 56.34 53.24 54.70+ oracle POS 4 100 2,145k 66.19 59.35 62.53 58.51 57.00 57.71
Table 3: Comparison of supervised models on test (using the 8-tag scheme). The base model corresponds to the boxed
result in table table 2, but here evaluated on test. For each configuration, the number of training iterations M and (except
for the base model) the recall-oriented hyperparameter ? were tuned by cross-validation on train.
out as a final test set. This left 3,312 sentences/
48,408 words for training/development (train). Fea-
ture engineering and hyperparameter tuning were
conducted with 8-fold cross-validation on train. The
8-tag scheme is used except where otherwise noted.
In learning with the structured perceptron (algo-
rithm 1), we employ two well-known techniques that
can both be viewed as regularization. First, we use
the average of parameters over all timesteps of learn-
ing. Second, within each cross-validation fold, we de-
termine the number of training iterations (epochs) M
by early stopping?that is, after each iteration, we use
the model to decode the held-out data, and when that
accuracy ceases to improve, use the previous model.
The two hyperparameters are the number of iterations
and the value of the recall cost hyperparameter (?).
Both are tuned via cross-validation on train; we use
the multiple of 50 that maximizes average link-based
F1. The chosen values are shown in table 3. Experi-
ments were managed with the ducttape tool.18
6 Results
We experimentally address the following questions
to probe and justify our modeling approach.
6.1 Is supervised learning necessary?
Previous MWE identification studies have found
benefit to statistical learning over heuristic lexicon
lookup (Constant and Sigogne, 2011; Green et al.,
2012). Our first experiment tests whether this holds
for comprehensive MWE identification: it compares
our supervised tagging approach with baselines of
heuristic lookup on preexisting lexicons. The base-
lines construct a lattice for each sentence using the
same method as lexicon-based model features (?5.2).
If multiple lexicons are used, the union of their en-
18https://github.com/jhclark/ducttape/
tries is used to construct the lattice. The resulting
segmentation?which does not encode a strength
distinction?is evaluated against the gold standard.
Table 2 shows the results. Even with just the la-
beled training set as input, the supervised approach
beats the strongest heuristic baseline (that incorpo-
rates in-domain lexicon entries extracted from the
training data) by 30 precision points, while achieving
comparable recall. For example, the baseline (but not
the statistical model) incorrectly predicts an MWE in
places to eat in Baltimore (because eat in, meaning
?eat at home,? is listed in WordNet). The supervised
approach has learned not to trust WordNet too much
due to this sort of ambiguity. Downstream applica-
tions that currently use lexicon matching for MWE
identification (e.g., Ghoneim and Diab, 2013) likely
stand to benefit from our statistical approach.
6.2 How best to exploit MWE lexicons
(type-level information)?
For statistical tagging (right portion of table 2), using
more preexisting (out-of-domain) lexicons generally
improves recall; precision also improves a bit.
A lexicon of MWEs occurring in the non-held-out
training data at least twice19 (table 2, bottom right) is
marginally worse (better precision/worse recall) than
the best result using only preexisting lexicons.
6.3 Variations on the base model
We experiment with some of the modeling alterna-
tives discussed in ?5. Results appear in table 3 under
both the link-based and exact match evaluation cri-
teria. We note that the exact match scores are (as
expected) several points lower.
19If we train with access to the full lexicon of training
set MWEs, the learner credulously overfits to relying on that
lexicon?after all, it has perfect coverage of the training data!?
which proves fatal for the model at test time.
200
Recall-oriented cost. The recall-oriented cost
adds about 1 link-based F1 point, sacrificing precision
in favor of recall.
Unsupervised word clusters. When combined
with the recall-oriented cost, these produce a slight
improvement to precision/degradation to recall, im-
proving exact match F1 but not affecting link-based
F1. Only a few clusters receive high positive weight;
one of these consists of matter, joke, biggie, pun,
avail, clue, corkage, frills, worries, etc. These words
are diverse semantically, but all occur in collocations
with no, which is what makes the cluster coherent
and useful to the MWE model.
Oracle part-of-speech tags. Using human-
annotated rather than automatic POS tags improves
MWE identification by about 3 F1 points on test
(similar differences were observed in development).
6.4 What are the highest-weighted features?
An advantage of the linear modeling framework is
that we can examine learned feature weights to gain
some insight into the model?s behavior.
In general, the highest-weighted features are the
lexicon matching features and features indicative of
proper names (POS tag of proper noun, capitalized
word not at the beginning of the sentence, etc.).
Despite the occasional cluster capturing colloca-
tional or idiomatic groupings, as described in the
previous section, the clusters appear to be mostly
useful for identifying words that tend to belong (or
not) to proper names. For example, the cluster with
street, road, freeway, highway, airport, etc., as well
as words outside of the cluster vocabulary, weigh
in favor of an MWE. A cluster with everyday desti-
nations (neighborhood, doctor, hotel, bank, dentist)
prefers non-MWEs, presumably because these words
are not typically part of proper names in this corpus.
This was from the best model using non-oracle POS
tags, so the clusters are perhaps useful in correct-
ing for proper nouns that were mistakenly tagged
as common nouns. One caveat, though, is that it is
hard to discern the impact of these specific features
where others may be capturing essentially the same
information.
6.5 How heterogeneous are learned MWEs?
On test, the final model (with automatic POS tags)
predicts 365 MWE instances (31 are gappy; 23 are
POS pattern # examples (lowercased lemmas)
NOUN NOUN 53 customer service, oil change
VERB PREP 36 work with, deal with, yell at
PROPN PROPN 29 eagle transmission, comfort zone
ADJ NOUN 21 major award, top notch, mental health
VERB PART 20 move out, end up, pick up, pass up
VERB ADV 17 come back, come in, come by, stay away
PREP NOUN 12 on time, in fact, in cash, for instance
VERB NOUN 10 take care, make money, give crap
VERB PRON 10 thank you, get it
PREP PREP 8 out of, due to, out ta, in between
ADV ADV 6 no matter, up front, at all, early on
DET NOUN 6 a lot, a little, a bit, a deal
VERB DET NOUN 6 answer the phone, take a chance
NOUN PREP 5 kind of, care for, tip on, answer to
Table 4: Top predicted POS patterns and frequencies.
weak). There are 298 unique MWE types.
Organizing the predicted MWEs by their coarse
POS sequence reveals that the model is not too preju-
diced in the kinds of expressions it recognizes: the
298 types fall under 89 unique POS+strength patterns.
Table 4 shows the 14 POS sequences predicted 5 or
more times as strong MWEs. Some of the examples
(major award, a deal, tip on) are false positives, but
most are correct. Singleton patterns include PROPN
VERB (god forbid), PREP DET (at that), ADJ PRON
(worth it), and PREP VERB PREP (to die for).
True positive MWEs mostly consist of (a) named
entities, and (b) lexical idioms seen in training and/or
listed in one of the lexicons. Occasionally the sys-
tem correctly guesses an unseen and OOV idiom
based on features such as hyphenation (walk - in) and
capitalization/OOV words (Chili Relleno, BIG MIS-
TAKE). On test, 244 gold MWE types were unseen
in training; the system found 93 true positives (where
the type was predicted at least once), 109 false posi-
tives, and 151 false negatives?an unseen type recall
rate of 38%. Removing types that occurred in lexi-
cons leaves 35 true positives, 61 false positives, and
111 false negatives?a unseen and OOV type recall
rate of 24%.
6.6 What kinds of mismatches occur?
Inspection of the output turns up false positives due
to ambiguity (e.g., Spongy and sweet bread); false
negatives (top to bottom); and overlap (get high qual-
ity service, gold get high quality service; live up to,
gold live up to). A number of the mismatches turn
201
scheme ?Y ? ? M ?w? P R F1
no gaps, 1-level 3 100 2.1 733k 73.33 55.72 63.20
no gaps, 2-level 4 150 3.3 977k 72.60 59.11 65.09
gappy, 1-level 6 200 1.6 1,466k 66.48 61.26 63.65
gappy, 2-level 8 100 3.5 1,954k 73.27 60.44 66.15
Table 5: Training with different tagging schemes. Results
are cross-validation averages on train. All schemes are
evaluated against the full gold standard (8 tags).
out to be problems with the gold standard, like hav-
ing our water shut off (gold having our water shut
off ). This suggests that even noisy automatic taggers
might help identify annotation inconsistencies and
errors for manual correction.
6.7 Are gappiness and the strength distinction
learned in practice?
Three quarters of MWEs are strong and contain no
gaps. To see whether our model is actually sensi-
tive to the phenomena of gappiness and strength,
we train on data simplified to remove one or both
distinctions?as in the first 3 labelings in figure 2?
and evaluate against the full 8-tag scheme. For the
model with the recall cost, clusters, and oracle POS
tags, we evaluate each of these simplifications of
the training data in table 5. The gold standard for
evaluation remains the same across all conditions.
If the model was unable to recover gappy expres-
sions or the strong/weak distinction, we would expect
it to do no better when trained with the full tagset than
with the simplified tagset. However, there is some
loss in performance as the tagset for learning is sim-
plified, which suggests that gappiness and strength
are being learned to an extent.
7 Related Work
Our annotated corpus (Schneider et al., 2014) joins
several resources that indicate certain varieties of
MWEs: lexicons such as WordNet (Fellbaum, 1998),
SAID (Kuiper et al., 2003), and WikiMwe (Hartmann
et al., 2012); targeted lists (Baldwin, 2005, 2008;
Cook et al., 2008; Tu and Roth, 2011, 2012); web-
sites like Wiktionary and Phrases.net; and large-scale
corpora such as SemCor (Miller et al., 1993), the
French Treebank (Abeill? et al., 2003), the Szeged-
ParalellFX corpus (Vincze, 2012), and the Prague
Czech-English Dependency Treebank (C?mejrek et al.,
2005). The difference is that Schneider et al. (2014)
pursued a comprehensive annotation approach rather
than targeting specific varieties of MWEs or relying
on a preexisting lexical resource. The annotations
are shallow, not relying explicitly on syntax (though
in principle they could be mapped onto the parses in
the Web Treebank).
In terms of modeling, the use of machine learn-
ing classification (Hashimoto and Kawahara, 2008;
Shigeto et al., 2013) and specifically BIO sequence
tagging (Diab and Bhutada, 2009; Constant and Si-
gogne, 2011; Constant et al., 2012; Vincze et al.,
2013) for contextual recognition of MWEs is not
new. Lexical semantic classification tasks like named
entity recognition (e.g., Ratinov and Roth, 2009), su-
persense tagging (Ciaramita and Altun, 2006; Paa?
and Reichartz, 2009), and index term identification
(Newman et al., 2012) also involve chunking of cer-
tain MWEs. But our discriminative models, facili-
tated by the new corpus, broaden the scope of the
MWE identification task to include many varieties of
MWEs at once, including explicit marking of gaps
and a strength distinction. By contrast, the afore-
mentioned identification systems, as well as some
MWE-enhanced syntactic parsers (e.g., Green et al.,
2012), have been restricted to contiguous MWEs.
However, Green et al. (2011) allow gaps to be de-
scribed as constituents in a syntax tree. Gimpel and
Smith?s (2011) shallow, gappy language model al-
lows arbitrary token groupings within a sentence,
whereas our model imposes projectivity and nest-
ing constraints (?3). Blunsom and Baldwin (2006)
present a sequence model for HPSG supertagging,
and evaluate performance on discontinuous MWEs,
though the sequence model treats the non-adjacent
component supertags like other labels?it cannot en-
force that they mutually require one another, as we
do via the gappy tagging scheme (?3.1). The lexicon
lookup procedures of Bejc?ek et al. (2013) can match
gappy MWEs, but are nonstatistical and extremely
error-prone when tuned for high oracle recall.
Another major thread of research has pursued un-
supervised discovery of multiword types from raw
corpora, such as with statistical association measures
(Church et al., 1991; Pecina, 2010; Ramisch et al.,
2012, inter alia), parallel corpora (Melamed, 1997;
Moir?n and Tiedemann, 2006; Tsvetkov and Wint-
ner, 2010), or a combination thereof (Tsvetkov and
202
Wintner, 2011); this may be followed by a lookup-
and-classify approach to contextual identification
(Ramisch et al., 2010). Though preliminary experi-
ments with our models did not show benefit to incor-
porating such automatically constructed lexicons, we
hope these two perspectives can be brought together
in future work.
8 Conclusion
This article has presented the first supervised model
for identifying heterogeneous multiword expressions
in English text. Our feature-rich discriminative se-
quence tagger performs shallow chunking with a
novel scheme that allows for MWEs containing gaps,
and includes a strength distinction to separate highly
idiomatic expressions from collocations. It is trained
and evaluated on a corpus of English web reviews
that are comprehensively annotated for multiword
expressions. Beyond the training data, its features in-
corporate evidence from external resources?several
lexicons as well as unsupervised word clusters; we
show experimentally that this statistical approach is
far superior to identifying MWEs by heuristic lexicon
lookup alone. Future extensions might integrate addi-
tional features (e.g., exploiting statistical association
measures computed over large corpora), enhance the
lexical representation (e.g., by adding semantic tags),
improve the expressiveness of the model (e.g., with
higher-order features and inference), or integrate the
model with other tasks (such as parsing and transla-
tion).
Our data and open source software are released at
http://www.ark.cs.cmu.edu/LexSem/.
Acknowledgments
This research was supported in part by NSF CA-
REER grant IIS-1054319, Google through the Read-
ing is Believing project at CMU, and DARPA grant
FA8750-12-2-0342 funded under the DEFT program.
We are grateful to Kevin Knight, Martha Palmer,
Claire Bonial, Lori Levin, Ed Hovy, Tim Baldwin,
Omri Abend, members of JHU CLSP, the NLP group
at Berkeley, and the Noah?s ARK group at CMU, and
anonymous reviewers for valuable feedback.
A Basic Features
All are conjoined with the current label, yi.
Label Features
1. previous label (the only first-order feature)
Token Features
Original token
2. i = {1,2}
3. i = ?w??{0,1}
4. capitalized ? ?i = 0?
5. word shape
Lowercased token
6. prefix: [wi]k1 ?4k=1
7. suffix: [wi]?w?j ??w?j=?w??3
8. has digit
9. has non-alphanumeric c
10. context word: w j ?i+2j=i?2
11. context word bigram: w j+1j ?i+1j=i?2
Lemma Features
12. lemma + context lemma if one of them is a verb and the other
is a noun, verb, adjective, adverb, preposition, or particle: ?i ?
? j ?i+2j=i?2
Part-of-speech Features
13. context POS: pos j ?i+2j=i?2
14. context POS bigram: pos j+1j ?i+1j=i?2
15. word + context POS: wi?posi?1
16. context word + POS: wi?1?posi
Lexicon Features (unlexicalized)
WordNet only
17. OOV: ?i is not in WordNet as a unigram lemma ? posi
18. compound: non-punctuation lemma ?i and the {previous,
next} lemma in the sentence (if it is non-punctuation; an inter-
vening hyphen is allowed) form an entry in WordNet, possibly
separated by a hyphen or space
19. compound-hyphen: posi = HYPH ? previous and next tokens
form an entry in WordNet, possibly separated by a hyphen or
space
20. ambiguity class: if content word unigram ?i is in WordNet,
the set of POS categories it can belong to; else posi if not a
content POS ? the POS of the longest MW match to which ?i
belongs (if any) ? the position in that match (B or I)
For each multiword lexicon
21. lexicon name ? status of token i in the shortest path segmen-
tation (O, B, or I) ? subcategory of lexical entry whose match
includes token i, if matched ? whether the match is gappy
22. the above ? POS tags of the first and last matched tokens in
the expression
Over all multiword lexicons
23. at least k lexicons contain a match that includes this token (if
n ? 1 matches, n active features)
24. at least k lexicons contain a match that includes this token,
starts with a given POS, and ends with a given POS
203
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?
and Nancy Ide, editors, Treebanks, volume 20 of Text,
Speech and Language Technology, pages 165?187.
Kluwer Academic Publishers, Dordrecht, The Nether-
lands.
Timothy Baldwin. 2005. Looking for prepositional verbs
in corpus data. In Proc. of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics Formalisms
and Applications, pages 115?126. Colchester, UK.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proc. of MWE, pages 1?2. Marrakech,
Morocco.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, Florida, USA.
Eduard Bejc?ek, Pavel Stran??k, and Pavel Pecina. 2013.
Syntactic identification of occurrences of multiword
expressions in text using a lexicon with dependency
structures. In Proc. of the 9th Workshop on Multiword
Expressions, pages 106?115. Atlanta, Georgia, USA.
G?bor Berend. 2011. Opinion expression mining by ex-
ploiting keyphrase extraction. In Proc. of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1162?1170. Chiang Mai, Thailand.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012a. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012b. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natu-
ral Language Processing with Python: Analyzing Text
with the Natural Language Toolkit. O?Reilly Media,
Inc., Sebastopol, California, USA.
Phil Blunsom and Timothy Baldwin. 2006. Multilingual
deep lexical acquisition for HPSGs via supertagging.
In Proc. of EMNLP, pages 164?171. Sydney, Australia.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in sta-
tistical machine translation. In Proc. of NAACL-HLT,
pages 242?245. Los Angeles, California, USA.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analysis.
In Uri Zernik, editor, Lexical acquisition: exploiting
on-line resources to build a lexicon, pages 115?164.
Lawrence Erlbaum Associates, Hillsdale, New Jersey,
USA.
Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extrac-
tion with a supersense sequence tagger. In Proc. of
EMNLP, pages 594?602. Sydney, Australia.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models: theory and experiments
with perceptron algorithms. In Proc. of EMNLP, pages
1?8. Philadelphia, Pennsylvania, USA.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to the
Real World, pages 49?56. Portland, Oregon, USA.
Matthieu Constant, Anthony Sigogne, and Patrick Watrin.
2012. Discriminative strategies to integrate multiword
expression recognition and parsing. In Proc. of ACL,
pages 204?212. Jeju Island, Korea.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008.
The VNC-Tokens dataset. In Proc. of MWE, pages
19?22. Marrakech, Morocco.
Hal Daum?, III. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. disserta-
tion, University of Southern California, Los Angeles,
California, USA. URL http://hal3.name/docs/
daume06thesis.pdf.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction MWE token classification. In Proc. of MWE,
pages 17?22. Suntec, Singapore.
Nick C. Ellis, Rita Simpson-Vlach, and Carson Maynard.
2008. Formulaic language in native and second lan-
guage speakers: psycholinguistics, corpus linguistics,
and TESOL. TESOL Quarterly, 42(3):375?396.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Charles J. Fillmore, Paul Kay, and Mary Catherine
O?Connor. 1988. Regularity and idiomaticity in gram-
matical constructions: the case of ?let alone?. Language,
64(3):501?538.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
Mahmoud Ghoneim and Mona Diab. 2013. Multiword
expressions in the context of statistical machine trans-
204
lation. In Proc. of IJCNLP, pages 1181?1187. Nagoya,
Japan.
Kevin Gimpel and Noah A. Smith. 2011. Generative
models of monolingual and bilingual gappy patterns.
In Proc. of WMT, pages 512?522. Edinburgh, Scotland,
UK.
Adele E. Goldberg. 1995. Constructions: a construction
grammar approach to argument structure. University
of Chicago Press, Chicago, Illinois, USA.
Adele E. Goldberg. 2006. Constructions at work: the
nature of generalization in language. Oxford University
Press, Oxford, UK.
Edouard Grave, Guillaume Obozinski, and Francis Bach.
2013. Hidden Markov tree models for semantic class
induction. In Proc. of CoNLL, pages 94?103. Sofia,
Bulgaria.
Spence Green, Marie-Catherine de Marneffe, John Bauer,
and Christopher D. Manning. 2011. Multiword expres-
sion identification with tree substitution grammars: a
parsing tour de force with French. In Proc. of EMNLP,
pages 725?735. Edinburgh, Scotland, UK.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2012. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Jan Hajic?, Eva Hajic?ov?, Jarmila Panevov?, Petr Sgall,
Silvie Cinkov?, Eva Fuc??kov?, Marie Mikulov?, Petr
Pajas, Jan Popelka, Jir?? Semeck?, Jana ?indlerov?, Jan
?te?p?nek, Josef Toman, Zden?ka Ure?ov?, and Zdene?k
?abokrtsk?. 2012. Prague Czech-English Dependency
Treebank 2.0. Technical Report LDC2012T08, Linguis-
tic Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
Silvana Hartmann, Gy?rgy Szarvas, and Iryna Gurevych.
2012. Mining multiword terms from Wikipedia. In
Maria Teresa Pazienza and Armando Stellato, editors,
Semi-Automatic Ontology Development. IGI Global,
Hershey, Pennsylvania, USA.
Chikara Hashimoto and Daisuke Kawahara. 2008. Con-
struction of an idiom corpus and its application to id-
iom identification based on WSD incorporating idiom-
specific features. In Proc. of EMNLP, pages 992?1001.
Honolulu, Hawaii, USA.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-08: HLT, pages 595?603. Columbus, Ohio.
Koenraad Kuiper, Heather McCann, Heidi Quinn,
Therese Aitchison, and Kees van der Veer. 2003.
SAID. Technical Report LDC2003T10, Linguistic
Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: probabilistic
models for segmenting and labeling sequence data. In
Proc. of ICML, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts In-
stitute of Technology, Cambridge, Massachusetts,
USA. URL http://people.csail.mit.edu/
pliang/papers/meng-thesis.pdf.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc.
of EMNLP, pages 97?108. Providence, Rhode Island,
USA.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of HLT, pages 303?308. Plainsboro, New Jersey,
USA.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative
training. In Proc. of HLT-NAACL, pages 337?342.
Boston, Massachusetts, USA.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Ke-
mal Oflazer, and Noah A. Smith. 2012. Recall-oriented
learning of named entities in Arabic Wikipedia. In
Proc. of EACL, pages 162?173. Avignon, France.
Begona Villada Moir?n and J?rg Tiedemann. 2006. Iden-
tifying idiomatic expressions using automatic word-
alignment. In Proc. of the EACL 2006 Workshop
on Multi-word Expressions in a Multilingual Context,
pages 33?40. Trento, Italy.
Rosamund Moon. 1998. Fixed expressions and idioms
in English: a corpus-based approach. Oxford Stud-
ies in Lexicography and Lexicology. Clarendon Press,
Oxford, UK.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmentation
for index term identification and keyphrase extraction.
In Proc. of COLING 2012, pages 2077?2092. Mumbai,
India.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow. 1994.
Idioms. Language, 70(3):491?538.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL-HLT,
pages 380?390. Atlanta, Georgia, USA.
Gerhard Paa? and Frank Reichartz. 2009. Exploiting
205
semantic constraints for estimating supersenses with
CRFs. In Proc. of the Ninth SIAM International Confer-
ence on Data Mining, pages 485?496. Sparks, Nevada,
USA.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Evalu-
ation, 44(1):137?158.
Carlos Ramisch. 2012. A generic and open
framework for multiword expressions treatment:
from acquisition to applications. Ph.D. disser-
tation, University of Grenoble and Federal Uni-
versity of Rio Grande do Sul, Grenoble, France.
URL http://www.inf.ufrgs.br/~ceramisch/
download_files/thesis-getalp.pdf.
Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio.
2012. A broad evaluation of techniques for automatic
acquisition of multiword expressions. In Proc. of ACL
2012 Student Research Workshop, pages 1?6. Jeju Is-
land, Korea.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010. mwetoolkit: a framework for multiword expres-
sion identification. In Proc. of LREC, pages 662?669.
Valletta, Malta.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Proc.
of the Third ACL Workshop on Very Large Corpora,
pages 82?94. Cambridge, Massachusetts, USA.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of CoNLL, pages 147?155. Boulder, Colorado, USA.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(04):485?510.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proc. of EMNLP, pages 1524?1534. Edin-
burgh, Scotland, UK.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
a pain in the neck for NLP. In Alexander Gelbukh,
editor, Computational Linguistics and Intelligent Text
Processing, volume 2276 of Lecture Notes in Computer
Science, pages 189?206. Springer, Berlin, Germany.
Nathan Schneider, Spencer Onuffer, Nora Kazour, Emily
Danchik, Michael T. Mordowanec, Henrietta Conrad,
and Noah A. Smith. 2014. Comprehensive annotation
of multiword expressions in a social web corpus. In
Proc. of LREC. Reykjav?k, Iceland.
Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, Shuhei
Kondo, Tomoya Kouse, Keisuke Sakaguchi, Akifumi
Yoshimoto, Frances Yung, and Yuji Matsumoto. 2013.
Construction of English MWE dictionary and its appli-
cation to POS tagging. In Proc. of the 9th Workshop
on Multiword Expressions, pages 139?144. Atlanta,
Georgia, USA.
James W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization of
finite automata theory. Journal of Computer and System
Sciences, 1(4):317?322.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180. Edmonton, Alberta,
Canada.
Yulia Tsvetkov and Shuly Wintner. 2010. Extraction of
multi-word expressions from small parallel corpora.
In Coling 2010: Posters, pages 1256?1264. Beijing,
China.
Yulia Tsvetkov and Shuly Wintner. 2011. Identification
of multi-word expressions by combining multiple lin-
guistic information sources. In Proc. of EMNLP, pages
836?845. Edinburgh, Scotland, UK.
Yuancheng Tu and Dan Roth. 2011. Learning English
light verb constructions: contextual or statistical. In
Proc. of the Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages 31?39.
Portland, Oregon, USA.
Yuancheng Tu and Dan Roth. 2012. Sorting out the most
confusing English phrasal verbs. In Proc. of *SEM,
pages 65?69. Montr?al, Quebec, Canada.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proc. of ACL,
pages 384?394. Uppsala, Sweden.
Martin C?mejrek, Jan Cur??n, Jan Hajic?, and Jir?? Havelka.
2005. Prague Czech-English Dependency Treebank:
resource for structure-based MT. In Proc. of EAMT,
pages 73?78. Budapest, Hungary.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proc. of MUC-6, pages
45?52. Columbia, Maryland, USA.
Veronika Vincze. 2012. Light verb constructions in the
SzegedParalellFX English-Hungarian parallel corpus.
In Proc. of LREC. Istanbul, Turkey.
Veronika Vincze, Istv?n Nagy T., and J?nos Zsibrita. 2013.
Learning to detect English and Hungarian light verb
constructions. ACM Transactions on Speech and Lan-
guage Processing, 10(2):6:1?6:25.
206
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 264?267,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SEMAFOR: Frame Argument Resolution with Log-Linear Models
Desai Chen Nathan Schneider Dipanjan Das Noah A. Smith
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
{desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.edu
Abstract
This paper describes the SEMAFOR sys-
tem?s performance in the SemEval 2010
task on linking events and their partici-
pants in discourse. Our entry is based
upon SEMAFOR 1.0 (Das et al, 2010a),
a frame-semantic probabilistic parser built
from log-linear models. The extended sys-
tem models null instantiations, including
non-local argument reference. Performance
is evaluated on the task data with and with-
out gold-standard overt arguments. In both
settings, it fares the best of the submitted
systems with respect to recall and F
1
.
1 Introduction
The theory of frame semantics (Fillmore, 1982)
holds that meaning is largely structured by holis-
tic units of knowledge, called frames. Each frame
encodes a conventionalized gestalt event or sce-
nario, often with conceptual dependents (partic-
ipants, props, or attributes) filling roles to elab-
orate the specific instance of the frame. In the
FrameNet lexicon (Fillmore et al, 2003), each
frame defines core roles tightly coupled with
the particular meaning of the frame, as well as
more generic non-core roles (Ruppenhofer et al,
2006). Frames can be evoked with linguistic pred-
icates, known as lexical units (LUs); role fillers
can be expressed overtly and linked to the frame
via (morpho)syntactic constructions. However, a
great deal of conceptually-relevant content is left
unexpressed or is not explicitly linked to the frame
via linguistic conventions; rather, it is expected
that the listener will be able to infer the appro-
priate relationships pragmatically. Certain types
of implicit content and implicit reference are for-
malized in the theory of null instantiations (NIs)
(Fillmore, 1986; Ruppenhofer, 2005). A complete
frame-semantic analysis of text thus incorporates
covert and overt predicate-argument information.
In this paper, we describe a system for frame-
semantic analysis, evaluated on a semantic role
labeling task for explicit and implicit arguments
(?2). Extending the SEMAFOR 1.0 frame-
semantic parser (Das et al, 2010a; outlined in ?3),
we detect null instantiations via a simple two-stage
pipeline: the first stage predicts whether a given
role is null-instantiated, and the second stage (?4)
predicts how it is null-instantiated, if it is not overt.
We report performance on the SemEval 2010 test
set under the full-SRL and NI-only conditions.
2 Data
The SemEval 2007 task on frame-semantic pars-
ing (Baker et al, 2007) provided a small (about
50,000 words and 2,000 sentences) dataset of
news text, travel guides, and bureaucratic accounts
of weapons stockpiles. Sentences in this dataset
were fully annotated with frames and their argu-
ments. The SemEval 2010 task (Ruppenhofer et
al., 2010) adds annotated data in the fiction do-
main: parts of two Sherlock Holmes stories by
Arthur Conan Doyle. The SemEval 2010 train-
ing set consists of the SemEval 2007 data plus
one document from the new domain. This doc-
ument has about 7800 words in 438 sentences;
it has 1492 annotated frame instances, including
3169 (overt and null-instantiated) argument anno-
tations. The test set consists of two chapters from
another story: Chapter 13 contains about 4000
words, 249 sentences, and 791 frames; Chapter 14
contains about 5000 words, 276 sentences, and
941 frames (see also Table 3). Figure 1 shows
two annotated test sentences. All data released for
the 2010 task include part-of-speech tags, lemmas,
and phrase-structure trees from a parser, with head
annotations for constituents.
3 Argument identification
Our starting point is SEMAFOR 1.0 (Das et
al., 2010a), a discriminative probabilistic frame-
semantic parsing model that operates in three
stages: (a) rule-based target selection, (b) proba-
bilistic disambiguation that resolves each target to
a FrameNet frame, and (c) joint selection of text
spans to fill the roles of each target through a sec-
ond probabilistic model.
1
1
Das et al (2010a) report the performance of this system
on the complete SemEval 2007 task at 46.49% F
1
.
264
`` I         THINK   that I shall be in a position to     MAKE   the situation rather   more   CLEAR to you before long .
It has been an exceedingly DIFFICULT and most complicated  business .
DIFFICULTY
difficult.a
Degree
Activity
OPINION
think.v
CAUSATION
make.v
OBVIOUSNESS
clear.n
Experiencer
Cognizer Opinion
Actor Effect
Experiencer
Degree AttributePhenomenon
Figure 1. Two consecutive sentences
in the test set, with frame-semantic an-
notations. Shaded regions represent
frames: they include the target word in
the sentence, the corresponding frame
name and lexical unit, and arguments.
Horizontal bars mark gold argument
spans?white bars are gold annotations
and black bars show mistakes of our
NI-only system.
Chapter 13 Chapter 14
Training Data Prec. Rec. F
1
Prec. Rec. F
1
SemEval 2010 data (includes SemEval 2007 data) 0.69 0.50 0.58 0.66 0.48 0.56
SemEval 2007 data + 50% new, in-domain data 0.68 0.47 0.55 0.66 0.45 0.54
SemEval 2007 data only 0.67 0.41 0.50 0.64 0.40 0.50
Table 1. Overt
argument labeling
performance.
Stage (c), known as argument identification or
SRL, is most relevant here. In this step, the system
takes the target (frame-evoking) phrase t and cor-
responding frame type f predicted by the previous
stages, and independently fills each role of f with
a word or phrase from the sentence, or the sym-
bol OTHER to indicate that the role has no (local)
overt argument. Features used to inform this de-
cision include aspects of the syntactic dependency
parse (e.g. the path in the parse from the target
to the argument); voice; word overlap of the argu-
ment with respect to the target; and part-of-speech
tags within and around the argument. SEMAFOR
as described in (Das et al, 2010a) does not dis-
tinguish between different types of null instantia-
tions or find non-local referents. Given perfect
input to stage (c), the system achieved 68.5% F
1
on the SemEval 2007 data (exact match, evaluat-
ing overt arguments only). The only difference
in our use of SEMAFOR?s argument identification
module is in preprocessing the training data: we
use dependency parses transformed from the head-
augmented phrase-structure parses in the task data.
Table 1 shows the performance of our argument
identification model on this task?s test data. The
SRL systems compared in (Ruppenhofer et al,
2010) all achieved precision in the mid 60% range,
but SEMAFOR achieved substantially higher re-
call, F
1
, and label accuracy on this subtask. (The
table also shows how performance of our model
degrades when half or all of the new data are not
used for training; the 9% difference in recall sug-
gests the importance of in-domain training data.)
4 Null instantiation detection
In this subtask, which follows the argument iden-
tification subtask (?3), our system seeks to char-
acterize non-overt core roles given gold standard
local frame-argument annotations. Consider the
following passage from the test data:
?That?s lucky for him?in fact, it?s lucky for all
of you, since you are all on the wrong side of the
law in this matter. I am not sure that as a consci-
entious detective [
Authorities
my] first duty is not to
arrest [
Suspect
the whole household]. [
DNI
Charges
?]
The frame we are interested in, ARREST, has four
core roles, two of which (Authorities and Sus-
pect) have overt (local) arguments. The third core
role, Charges, is annotated as having anaphoric
or definite null instantiation (DNI). ?Definite?
means that the discourse implies a specific referent
that should be recoverable from context, without
marking that referent linguistically. Some DNIs in
the data are linked to phrases in syntactically non-
local positions, such as in another sentence (see
Figure 1). This one is not (though our model in-
correctly labels this matter from the previous sen-
tence as a DNI referent for this role). The fourth
core role, Offense, is not annotated as a null in-
stantiation because it belongs to the same CoreSet
as Charges?which is to say they are relevant in
a similar way to the frame as a whole (both pertain
to the rationale for the arrest) and only one is typ-
ically expressed.
2
We will use the term masked
to refer to any non-overt core role which does not
need to be specified as null-instantiated due to a
structural connection to another role in its frame.
The typology of NIs given in Ruppenhofer
(2005) and employed in the annotation distin-
guishes anaphoric/definite NIs from existential or
indefinite null instantiations (INIs). Rather than
having a specific referent accessible in the dis-
course, INIs are left vague or deemphasized, as in
2
If the FrameNet lexicon marks a pair of roles within a
frame as being in a CoreSet or Excludes relationship, then
filling one of them satisfies the requirement that the other be
(expressly or implicitly) present in the use of the frame.
265
Chapter 13 Chapter 14
Training Data Prec. Rec. F
1
Prec. Rec. F
1
N
I
-
o
n
l
y
SemEval 2010 new: 100% 0.40 0.64 0.50 0.53 0.60 0.56
SemEval 2010 new: 75% 0.66 0.37 0.50 0.70 0.37 0.48
SemEval 2010 new: 50% 0.73 0.38 0.51 0.75 0.35 0.48
Full All 0.35 0.55 0.43 0.56 0.49 0.52
Table 2. Performance on the
full task and the NI-only task.
The NI model was trained on the
new SemEval 2010 document, ?The
Tiger of San Pedro? (data from the
2007 task was excluded because
none of the null instantiations in that
data had annotated referents).
Predicted
overt DNI INI masked inc. total
G
o
l
d
overt 2068 (1630) 5 362 327 0 2762
DNI 64 12 (3) 182 90 0 348
INI 41 2 214 96 0 353
masked 73 0 240 1394 0 1707
inc. 12 2 55 2 0 71
total 2258 21 1053 1909 0 3688 correct
Table 3. Instantiation type confusion ma-
trix for the full model (argument identifi-
cation plus NI detection). Parenthesized
numbers count the predictions of the cor-
rect type which also predicted the same
(argument or referent) span. On the NI-
only task, our system has a similar distri-
bution of NI detection errors.
the thing(s) eaten in the sentence We ate.
The problem can be decomposed into two steps:
(a) classifying each null instantiation as definite,
indefinite, or masked; and (b) resolving the DNIs,
which entails finding referents in the non-local
context. Instead, our model makes a single NI pre-
diction for any role that received no local argument
(OTHER) in the argument identification phase (?3),
thereby combining classification and resolution.
3
4.1 Model
Our model for this subtask is analogous to the ar-
gument identification model: it chooses one from
among many possible fillers for each role. How-
ever, whereas the argument identification model
considers parse constituents as potential local
fillers (which might constitute an overt argument
within the sentence) along with a special category,
OTHER, here the set of candidate fillers consists of
phrases from outside the sentence, along with spe-
cial categories INI or MASKED. When selected, a
non-local phrase will be interpreted as a non-local
argument and labeled as a DNI referent.
These non-local candidate fillers are handled
differently from candidates within the sentence
considered in the argument identification model:
they are selected using more restrictive criteria,
and are associated with a different set of features.
Restricted search space for DNI referents. We
consider nouns, pronouns, and noun phrases from
the previous three sentences as candidate DNI ref-
erents. This narrows the search space considerably
to make learning tractable, but at a cost: many
gold DNI referents will not even be considered.
In the training data, there are about 250 DNI in-
stances with explicit referents; their distribution is
3
Investigation of separate modeling is left to future work.
chaotic.
4
Judging by the training data, our heuris-
tics thus limit oracle recall to about 20% of DNIs.
5
Modified feature set. Since it is not obvious how
to calculate a syntactic path between two words
in different sentences, we replaced dependency
path features with simpler features derived from
FrameNet?s lexicographic exemplar annotations.
For each candidate span, we use two types of fea-
tures to model the affinity between the head word
and the role. The first indicates whether the head
word is used as a filler for this role in at least
one of the lexicographic exemplars. The second
encodes the maximum distributional similarity to
any word heading a filler of that role in the ex-
emplars.
6
In practice, we found that these fea-
tures received negligible weight and had virtually
no effect on performance, possibly due to data
sparseness. An additional change in the feature
set is that ordering/distance features (Das et al,
2010b, p. 13) were replaced with a feature indicat-
ing the number of sentences away the candidate
is from the target.
7
Otherwise, the null identifica-
4
91 DNI referents are found no more than three sentences
prior; another 90 are in the same sentence as the target. 20
DNIs have referents which are not noun phrases. Six appear
after the sentence containing its frame target; 28 appear at
least 25 sentences prior. 60 have no referent.
5
Our system ignores DNIs with no referent or with a ref-
erent in the same sentence as the target. Experiments with
variants on these assumptions show that the larger the search
space (i.e. the more candidate DNI referents are under con-
sideration), the worse the trained model performs at distin-
guishing NIs from non-NIs (though DNI vs. INI precision
improves). This suggests that data sparseness is hindering
our system?s ability to learn useful generalizations about NIs.
6
Distributional similarity scores are obtained
from D. Lin?s Proximity-based Thesaurus (http:
//webdocs.cs.ualberta.ca/~lindek/
Downloads/sims.lsp.gz) and quantized into bi-
nary features for intervals: [0, .03), [.03, .06), [.06, .08),
[.08,?).
7
All of the new features are instantiated in three forms:
266
tion model uses the same features as the argument
identification model.
The theory of null instantiations holds that the
grammaticality of lexically-licensed NI for a role
in a given frame depends on the LU: for exam-
ple, the verbs buy and sell share the same frame
but differ as to whether the Buyer or Seller role
may be lexically null-instantiated. Our model?s
feature set is rich enough to capture this in a soft
way, with lexicalized features that fire, e.g., when
the Seller role is null-instantiated and the target
is buy. Moreover, (Ruppenhofer, 2005) hypoth-
esizes that each role has a strong preference for
one interpretation (INI or DNI) when it is lexically
null-instantiated, regardless of LU. This, too, is
modeled in our feature set. In theory these trends
should be learnable given sufficient data, though it
is doubtful that there are enough examples of null
instantiations in the currently available dataset for
this learning to take place.
4.2 Evaluation
We trained the model on the non-overt arguments
in the new SemEval 2010 training document,
which has 580 null instantiations?303 DNIs and
277 INIs.
8,9
Then we used the task scoring proce-
dure to evaluate the NI detection subtask in isola-
tion (given gold-standard overt arguments) as well
as the full task (when this module is combined in a
pipeline with argument identification). Results are
shown in Table 2.
10
Table 3 provides a breakdown of our sys-
tem?s predictions on the test data by instantiation
type: overt local arguments, DNIs, INIs, and the
MASKED category (marking the role as redundant
or irrelevant for the particular use of the frame,
given the other arguments). It also shows counts
for incorporated (?inc.?) roles, which are filled by
the frame-evoking target, e.g. clear in Figure 1.
11
This table shows that the system is reasonably ef-
fective at discriminating NIs from masked roles,
one specific to the frame and the role, one specific to the role
name only, and one to learn the overall bias of the data.
8
For feature engineering we held out the last 25% of sen-
tences from the new training document as development data,
retraining on the full training set for final evaluation.
9
We used Nils Reiter?s FrameNet API, version 0.4
(http://www.cl.uni-heidelberg.de/trac/
FrameNetAPI) in processing the data.
10
The other system participating in the NI-only subtask
had much lower NI recall of 8% (Ruppenhofer et al, 2010).
11
We do not predict any DNIs without referents or in-
corporated roles, though the evaluation script gives us credit
when we predict INI for these cases.
but DNI identification suffers from low recall and
INI identification from low precision. Data sparse-
ness is likely the biggest obstacle here. To put this
in perspective, there are over 20,000 training ex-
amples of overt arguments, but fewer than 600 ex-
amples of null instantiations, two thirds of which
do not have referents. Without an order of mag-
nitude more NI data (at least), it is unlikely that
a supervised learner could generalize well enough
to recognize on new data null instantiations of the
over 7000 roles in the lexicon.
5 Conclusion
We have described a system that implements a
clean probabilistic model of frame-semantic struc-
ture, considering overt arguments as well as var-
ious forms of null instantion of roles. The sys-
tem was evaluated on SemEval 2010 data, with
mixed success at detecting null instantiations. We
believe in-domain data sparseness is the predom-
inant factor limiting the robustness of our super-
vised model.
Acknowledgments
This work was supported by DARPA grant
NBCH-1080004 and computational resources
provided by Yahoo. We thank the task organizers for
providing data and conducting the evaluation, and two
reviewers for their comments.
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-2007
Task 19: Frame Semantic Structure Extraction. In Proc.
of SemEval.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of NAACL-
HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b.
SEMAFOR 1.0: A probabilistic frame-semantic parser.
Technical Report CMU-LTI-10-001, Carnegie Mellon
University.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of Lexi-
cography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in the
Morning Calm, pages 111?137. Hanshin Publishing Co.,
Seoul, South Korea.
C. J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proc. of Berkeley Linguistics Society, pages
95?107, Berkeley, CA.
J. Ruppenhofer, M. Ellsworth, M. R.L. Petruck, C. R. John-
son, and J. Scheffczyk. 2006. FrameNet II: extended the-
ory and practice.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker, and
M. Palmer. 2010. SemEval-2010 Task 10: Linking
Events and Their Participants in Discourse. In Proc. of
SemEval.
J. Ruppenhofer. 2005. Regularities in null instantiation.
267
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 209?217,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
An Exact Dual Decomposition Algorithm
for Shallow Semantic Parsing with Constraints
Dipanjan Das? Andre? F. T. Martins?? Noah A. Smith?
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{dipanjan,afm,nasmith}@cs.cmu.edu
Abstract
We present a novel technique for jointly predict-
ing semantic arguments for lexical predicates. The
task is to find the best matching between seman-
tic roles and sentential spans, subject to struc-
tural constraints that come from expert linguistic
knowledge (e.g., in the FrameNet lexicon). We
formulate this task as an integer linear program
(ILP); instead of using an off-the-shelf tool to
solve the ILP, we employ a dual decomposition
algorithm, which we adapt for exact decoding via
a branch-and-bound technique. Compared to a
baseline that makes local predictions, we achieve
better argument identification scores and avoid all
structural violations. Runtime is nine times faster
than a proprietary ILP solver.
1 Introduction
Semantic knowledge is often represented declara-
tively in resources created by linguistic experts. In
this work, we strive to exploit such knowledge in
a principled, unified, and intuitive way. An ex-
ample resource where a wide variety of knowledge
has been encoded over a long period of time is the
FrameNet lexicon (Fillmore et al, 2003),1 which
suggests an analysis based on frame semantics (Fill-
more, 1982). This resource defines hundreds of
semantic frames. Each frame represents a gestalt
event or scenario, and is associated with several se-
mantic roles, which serve as participants in the event
that the frame signifies (see Figure 1 for an exam-
ple). Along with storing the above data, FrameNet
also provides a hierarchy of relationships between
frames, and semantic relationships between pairs of
roles. In prior NLP research using FrameNet, these
interactions have been largely ignored, though they
1http://framenet.icsi.berkeley.edu
have the potential to improve the quality and consis-
tency of semantic analysis.
In this paper, we present an algorithm that finds
the full collection of arguments of a predicate given
its semantic frame. Although we work within the
conventions of FrameNet, our approach is general-
izable to other semantic role labeling (SRL) frame-
works. We model this argument identification task
as constrained optimization, where the constraints
come from expert knowledge encoded in a lexi-
con. Following prior work on PropBank-style SRL
(Kingsbury and Palmer, 2002) that dealt with simi-
lar constrained problems (Punyakanok et al, 2004;
Punyakanok et al, 2008, inter alia), we incorporate
this declarative knowledge in an integer linear pro-
gram (ILP).
Because general-purpose ILP solvers are propri-
etary and do not fully exploit the structure of the
problem, we turn to a class of optimization tech-
niques called dual decomposition (Komodakis et
al., 2007; Rush et al, 2010; Martins et al, 2011a).
We derive a modular, extensible, parallelizable ap-
proach in which semantic constraints map not just
to declarative components of the algorithm, but also
to procedural ones, in the form of ?workers.? While
dual decomposition algorithms only solve a relax-
ation of the original problem, we make a novel con-
tribution by wrapping the algorithm in a branch-and-
bound search procedure, resulting in exact solutions.
We experimentally find that our algorithm
achieves accuracy comparable to a state-of-the-art
system, while respecting all imposed linguistic con-
straints. In comparison to inexact beam search that
violates many of these constraints, our exact decoder
has less than twice the runtime; furthermore, it de-
codes nine times faster than CPLEX, a state-of-the-
art, proprietary, general-purpose exact ILP solver.
209
Austria , once expected to waltz smoothly into the European Union , is elbowing its partners , 
treading on toes and pogo-dancing in a most un-Viennese manner .
SELF_MOTION COLLABORATION
CONDUCTGoalManner Partner_1 Partner_2MannerAgentSelf_mover
Figure 1: An example sentence from the annotations released as part of FrameNet 1.5 with three predicates marked in
bold. Each predicate has its evoked semantic frame marked above it, in a distinct color. For each frame, its semantic
roles are shown in the same color, and the spans fulfilling the roles are underlined. For example, manner evokes the
CONDUCT frame, and has the Agent and Manner roles fulfilled by Austria and most un-Viennese respectively.
2 Collective Argument Identification
Here, we take a declarative approach to modeling
argument identification using an ILP and relate our
formulation to prior work in shallow semantic pars-
ing. We show how knowledge specified in a lin-
guistic resource can be used to derive the constraints
used in our ILP. Finally, we draw connections of our
specification to graphical models, a popular formal-
ism in AI, and describe how the constraints can be
treated as factors in a factor graph.
2.1 Declarative Specification
Let us denote a predicate by t and the semantic
frame it evokes within a sentence x by f . In this
work, we assume that the semantic frame f is given,
which is traditionally the case in controlled exper-
iments used to evaluate SRL systems (Ma`rquez et
al., 2008). Given the semantic frame of a predicate,
the semantic roles that might be filled are assumed
to be given by the lexicon (as in PropBank and
FrameNet). Let the set of roles associated with the
frame f be Rf . In sentence x, the set of candidate
spans of words that might fill each role is enumer-
ated, usually following an overgenerating heuristic;2
let this set of spans be St. We include the null span ?
in St; connecting it to a role r ? Rf denotes that the
role is not overt. Our approach assumes a scoring
function that gives a strength of association between
roles and candidate spans. For each role r ? Rf and
span s ? St, this score is parameterized as:
c(r, s) = ? ? h(t, f,x, r, s), (1)
where ? are model weights and h is a feature func-
tion that looks at the predicate t, the evoked frame
f , sentence x, and its syntactic analysis, along with
2Here, as in most SRL literature, role fillers are assumed to be
expressed as contiguous spans, though such an assumption is
easy to relax in our framework.
r and s. The SRL literature provides many feature
functions of this form and many ways to use ma-
chine learning to acquire ?. Our presented method
does not make any assumptions about the score ex-
cept that it has the form in Eq. 1.
We define a vector z of binary variables zr,s ?
{0, 1} for every role and span pair. We have that:
z ? {0, 1}d, where d = |Rf | ? |St|. zr,s = 1 means
that role r is filled by span s. Given the binary z vec-
tor, it is straightforward to recover the collection of
arguments by checking which components zr,s have
an assignment of 1; we use this strategy to find argu-
ments, as described in ?4.2 (strategies 4 and 6). The
joint argument identification task can be represented
as a constrained optimization problem:
maximize
?
r?Rf
?
s?St c(r, s)? zr,s
with respect to z ? {0, 1}d
such that Az ? b. (2)
The last line imposes constraints on the mapping be-
tween roles and spans; these are motivated on lin-
guistic grounds and are described next.3
Uniqueness: Each role r is filled by at most one
span in St. This constraint can be expressed by:
?r ? Rf ,
?
s?St zr,s = 1. (3)
There are O(|Rf |) such constraints. Note that since
St contains the null span ?, non-overt roles are also
captured using the above constraints. Such a con-
straint is used extensively in prior literature (Pun-
yakanok et al, 2008, ?3.4.1).
Overlap: SRL systems commonly constrain roles
to be filled by non-overlapping spans. For example,
Toutanova et al (2005) used dynamic programming
over a phrase structure tree to prevent overlaps be-
tween arguments, and Punyakanok et al (2008) used
3Note that equality constraints a ?z = b can be transformed into
double-side inequalities a ? z ? b and ?a ? z ? ?b.
210
constraints in an ILP to respect this requirement. In-
spired by the latter, we require that each input sen-
tence position of x be covered by at most one argu-
ment. For each role r ? Rf , we define:
Gr(i) = {s | s ? St, s covers position i in x}. (4)
We can define our overlap constraints in terms of Gr
as follows, for every sentence position i:
?i ? {1, . . . , |x|},
?
r?Rf
?
s?Gr(i) zr,s ? 1, (5)
This gives us O(|x|) constraints.
Pairwise ?Exclusions?: For many predicate
classes, there are pairs of roles forbidden to appear
together in the analysis of a single predicate token.
Consider the following two sentences:
A blackberry
Entity 1
resembles a loganberry
Entity 2
. (6)
Most berries
Entities
resemble each other. (7)
Consider the uninflected predicate resemble in
both sentences, evoking the same meaning. In exam-
ple 6, two roles, which we call Entity 1 and Entity 2
describe two entities that are similar to each other.
In the second sentence, a phrase fulfills a third role,
called Entities, that collectively denotes some ob-
jects that are similar. It is clear that the roles Entity 1
and Entities cannot be overt for the same predicate
at once, because the latter already captures the func-
tion of the former; a similar argument holds for the
Entity 2 and Entities roles. We call this phenomenon
the ?excludes? relationship. Let us define a set of
pairs fromRf that have this relationship:
Exclf = {(ri, rj) | ri and rj exclude each other}
Using the above set, we define the constraint:
?(ri, rj) ? Exclf , zri,? + zrj ,? ? 1 (8)
In English: if both roles are overt in a parse, this
constraint will be violated, and we will not respect
the ?excludes? relationship between the pair. If nei-
ther or only one of the roles is overt, the constraint
is satisfied. The total number of such constraints is
O(|Exclf |), which is the number of pairwise ?ex-
cludes? relationships of a given frame.
Pairwise ?Requirements?: The sentence in exam-
ple 6 illustrates another kind of constraint. The pred-
icate resemble cannot have only one of Entity 1 and
Entity 2 as roles in text. For example,
* A blackberry
Entity 1
resembles. (9)
Enforcing the overtness of two roles sharing this
?requires? relationship is straightforward. We define
the following set for a frame f :
Reqf = {(ri, rj) | ri and rj require each other}
This leads to constraints of the form
?(ri, rj) ? Reqf , zri,? ? zrj ,? = 0 (10)
If one role is overt (or absent), so must the other
be. A related constraint has been used previously
in the SRL literature, enforcing joint overtness re-
lationships between core arguments and referential
arguments (Punyakanok et al, 2008, ?3.4.1), which
are formally similar to the example above.4
Integer Linear Program and Relaxation: Plug-
ging the constraints in Eqs. 3, 5, 8 and 10 into the
last line of Eq. 2, we have the argument identifica-
tion problem expressed as an ILP, since the indica-
tor variables z are binary. In this paper, apart from
the ILP formulation, we will consider the follow-
ing relaxation of Eq. 2, which replaces the binary
constraint z ? {0, 1}d by a unit interval constraint
z ? [0, 1]d, yielding a linear program:
maximize
?
r?Rf
?
s?St c(r, s)? zr,s
with respect to z ? [0, 1]d
such that Az ? b. (11)
There are several LP and ILP solvers available,
and a great deal of effort has been spent by the
optimization community to devise efficient generic
solvers. An example is CPLEX, a state-of-the-art
solver for mixed integer programming that we em-
ploy as a baseline to solve the ILP in Eq. 2 as well
as its LP relaxation in Eq. 11. Like many of the best
implementations, CPLEX is proprietary.
4 We noticed in the annotated data, in some cases, the ?requires?
constraint is violated by the FrameNet annotators. This hap-
pens mostly when one of the required roles is absent in the
sentence containing the predicate, but is rather instantiated in
an earlier sentence; see Gerber and Chai (2010). We apply the
hard constraint in Eq. 10, though extending our algorithm to
seek arguments outside the sentence is straightforward (Chen
et al, 2010).
211
2.2 Linguistic Constraints from FrameNet
Although enforcing the four different sets of con-
straints above is intuitive from a general linguistic
perspective, we ground their use in definitive lin-
guistic information present in the FrameNet lexicon
(Fillmore et al, 2003). FrameNet, along with lists
of semantic frames, associated semantic roles, and
predicates that could evoke the frames, gives us a
small number of annotated sentences with frame-
semantic analysis. From the annotated data, we
gathered that only 3.6% of the time is a role instanti-
ated multiple times by different spans in a sentence.
This justifies the uniqueness constraint enforced by
Eq. 3. Use of such a constraint is also consistent
with prior work in frame-semantic parsing (Johans-
son and Nugues, 2007; Das et al, 2010a). Similarly,
we found that in the annotations, no arguments over-
lapped with each other for a given predicate. Hence,
the overlap constraints in Eq. 5 are also justified.
Our third and fourth sets of constraints, presented
in Eqs. 8 and 10, come from FrameNet, too; more-
over, they are explicitly mentioned in the lexicon.
Examples 6?7 are instances where the predicate re-
semble evokes the SIMILARITY frame, which is de-
fined in FrameNet as: ?Two or more distinct en-
tities, which may be concrete or abstract objects
or types, are characterized as being similar to each
other. Depending on figure/ground relations, the
entities may be expressed in two distinct frame el-
ements and constituents, Entity 1 and Entity 2, or
jointly as a single frame element and constituent,
Entities.?
For this frame, the lexicon lists several roles other
than the three roles we have already observed, such
as Dimension (the dimension along which the enti-
ties are similar), Differentiating fact (a fact that re-
veals how the concerned entities are similar or dif-
ferent), and so forth. Along with the roles, FrameNet
also declares the ?excludes? and ?requires? relation-
ships noted in our discussion in Section 2.1. The
case of the SIMILARITY frame is not unique; in Fig. 1,
the frame COLLABORATION, evoked by the predicate
partners, also has two roles Partner 1 and Partner 2
that share the ?requires? relationship. In fact, out
of 877 frames in FrameNet 1.5, the lexicon?s latest
edition, 204 frames have at least a pair of roles that
share the ?excludes? relationship, and 54 list at least
a pair of roles that share the ?requires? relationship.
2.3 Constraints as Factors in a Graphical Model
The LP in Eq. 11 can be represented as a maxi-
mum a posteriori (MAP) inference problem in an
undirected graphical model. In the factor graph,
each component of z corresponds to a binary vari-
able, and each instantiation of a constraint in
Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith
and Eisner (2008) and Martins et al (2010) used
such a representation to impose constraints in a de-
pendency parsing problem; the latter discussed the
equivalence of linear programs and factor graphs for
representing discrete optimization problems. Each
of our constraints take standard factor forms we can
describe using the terminology of Smith and Eisner
(2008) and Martins et al (2010). The uniqueness
constraint in Eq. 3 corresponds to an XOR factor,
while the overlap constraint in Eq. 5 corresponds to
an ATMOSTONE factor. The constraints in Eq. 8
enforcing the ?excludes? relationship can be repre-
sented with an OR factor. Finally, each ?requires?
constraints in Eq. 10 is equivalent to an XORWITH-
OUTPUT factor.
In the following section, we describe how we ar-
rive at solutions for the LP in Eq. 11 using dual de-
composition, and how we adapt it to efficiently re-
cover the exact solution of the ILP (Eq. 2), without
the need of an off-the-shelf ILP solver.
3 ?Augmented? Dual Decomposition
Dual decomposition methods address complex op-
timization problems in the dual, by dividing them
into simple worker problems, which are repeat-
edly solved until a consensus is reached. The
most simple technique relies on the subgradient
algorithm (Komodakis et al, 2007; Rush et al,
2010); as an alternative, an augmented Lagrangian
technique was proposed by Martins et al (2011a,
2011b), which is more suitable when there are many
small components?commonly the case in declara-
tive constrained problems, such as the one at hand.
Here, we present a brief overview of the latter, which
is called Dual Decomposition with the Alternating
Direction Method of Multipliers (AD3).
Let us start by establishing some notation. Let
m ? {1, . . . ,M} index a factor, and denote by i(m)
212
the vector of indices of variables linked to that fac-
tor. (Recall that each factor represents the instantia-
tion of a constraint.) We introduce a new set of vari-
ables, u ? Rd, called the ?witness? vector. We split
the vector z into M overlapping pieces z1, . . . , zM ,
where each zm ? [0, 1]|i(m)|, and add M constraints
zm = ui(m) to impose that all the pieces must agree
with the witness (and therefore with each other).
Each of the M constraints described in ?2 can be
encoded with its own matrix Am and vector bm
(which jointly define A and b in Eq. 11). For conve-
nience, we denote by c ? Rd the score vector, whose
components are c(r, s), for each r ? Rf and s ? St
(Eq. 1), and define the following scores for the mth
subproblem:
cm(r, s) = ?(r, s)
?1c(r, s), ?(r, s) ? i(m), (12)
where ?(r, s) is the number of constraints that in-
volve role r and span s. Note that according to this
definition, c ? z =
?M
m=1 cm ? zm. We can rewrite
the LP in Eq. 11 in the following equivalent form:
maximize
M?
m=1
cm ? zm
with respect to u ? Rd, zm ? [0, 1]i(m), ?m
such that Amzm ? bm, ?m
zm = ui(m), ?m. (13)
We next augment the objective with a quadratic
penalty term ?2
?M
m=1 ?zm?ui(m)?
2 (for some ? >
0). This does not affect the solution of the problem,
since the equality constraints in the last line force
this penalty to vanish. However, as we will see, this
penalty will influence the workers and will lead to
faster consensus. Next, we introduce Lagrange mul-
tipliers ?m for those equality constraints, so that the
augmented Lagrangian function becomes:
L?(z,u,?) =
M?
m=1
(cm + ?m) ? zm ? ?m ? ui(m)
?
?
2
?zm ? ui(m)?
2. (14)
The AD3 algorithm seeks a saddle point of L? by
performing alternating maximization with respect to
z and u, followed by a gradient update of ?. The re-
sult is shown as Algorithm 1. Like dual decomposi-
tion approaches, it repeatedly performs a broadcast
operation (the zm-updates, which can be done in pa-
Algorithm 1 AD3 for Argument Identification
1: input:
? role-span matching scores c := ?c(r, s)?r,s,
? structural constraints ?Am,bm?Mm=1,
? penalty ? > 0
2: initialize u uniformly (i.e., u(r, s) = 0.5, ?r, s)
3: initialize each ?m = 0, ?m ? {1, . . . ,M}
4: initialize t? 1
5: repeat
6: for each m = 1, . . . ,M do
7: make a zm-update by finding the best scoring
analysis for the mth constraint, with penalties
for deviating from the consensus u:
zt+1m ? argmax
Amzm?bm
(cm+?m)?zm?
?
2
?zm?ui(m)?
2
8: end for
9: make a u-update by updating the consensus solu-
tion, averaging z1, . . . , zm:
ut+1(r, s)?
1
?(r, s)
?
m:(r,s)?i(m)
zt+1m (r, s)
10: make a ?-update:
?t+1m ? ?
t
m ? ?(z
(t+1)
m ? u
(t+1)
i(m) ), ?m
11: t? t+ 1
12: until convergence.
13: output: relaxed primal solution u? and dual solution
??. If u? is integer, it will encode an assignment of
spans to roles. Otherwise, it will provide an upper
bound of the true optimum.
-rallel, one constraint per ?worker?) and a gather op-
eration (the u- and ?-updates). Each u-operation
can be seen as an averaged voting which takes into
consideration each worker?s results.
Like in the subgradient method, the?-updates can
be regarded as price adjustments, which will affect
the next round of zm-updates. The only difference
with respect to the subgradient method (Rush et al,
2010) is that each subproblem involved in a zm-
update also has a quadratic penalty that penalizes de-
viations from the previous average voting; it is this
term that accelerates consensus and therefore con-
vergence. Martins et al (2011b) also provide stop-
ping criteria for the iterative updates using primal
and dual residuals that measure convergence; we re-
fer the reader to that paper for details.
A key attraction of this algorithm is all the com-
ponents of the declarative specification remain intact
213
in the procedural form. Each worker corresponds
exactly to one constraint in the ILP, which corre-
sponds to one linguistic constraint. There is no need
to work out when, during the procedure, each con-
straint might have an effect, as in beam search.
Solving the subproblems. In a different appli-
cation, Martins et al (2011b, ?4) showed how
to solve each zm-subproblem associated with the
XOR, XORWITHOUTPUT and OR factors in runtime
O(|i(m)| log |i(m)|). The only subproblem that re-
mains is that of the ATMOSTONE factor, to which
we now turn. The problem can be transformed into
that of projecting a point (a1, . . . , ak) onto the set
Sm =
{
zm ? [0, 1]|i(m)|
?
?
?|i(m)|
j=1 zm,j ? 1
}
.
This projection can be computed as follows:
1. Clip each aj into the interval [0, 1] (i.e., set
a?j = min{max{aj , 0}, 1}). If the result satisfies
?k
j=1 a
?
j ? 1, then return (a
?
1, . . . , a
?
k).
2. Otherwise project (a1, . . . , ak) onto the probabil-
ity simplex:
{
zm ? [0, 1]|i(m)|
?
?
?|i(m)|
j=1 zm,j = 1
}
.
This is precisely the XOR subproblem and can be
solved in time O(|i(m)| log |i(m)|).
Caching. As mentioned by Martins et al (2011b),
as the algorithm comes close to convergence, many
subproblems become unchanged and their solutions
can be cached. By caching the subproblems, we
managed to reduce runtime by about 60%.
Exact decoding. Finally, it is worth recalling that
AD3, like other dual decomposition algorithms,
solves a relaxation of the actual problem. Although
we have observed that the relaxation is often tight?
cf. ?4?this is not always the case. Specifically, a
fractional solution may be obtained, which is not in-
terpretable as an argument, and therefore it is de-
sirable to have a strategy to recover the exact solu-
tion. Two observations are noteworthy. First, the
optimal value of the relaxed problem (Eq. 11) pro-
vides an upper bound to the original problem (Eq. 2).
This is because Eq. 2 has the additional integer con-
straint on the variables. In particular, any feasible
dual point provides an upper bound to the original
problem?s optimal value. Second, during execution
of the AD3 algorithm, we always keep track of a se-
quence of feasible dual points. Therefore, each it-
eration constructs tighter and tighter upper bounds.
With this machinery, we have all that is necessary for
implementing a branch-and-bound search that finds
the exact solution of the ILP. The procedure works
recursively as follows:
1. Initialize L = ?? (our best value so far).
2. Run Algorithm 1. If the solution u? is integer, re-
turn u? and set L to the objective value. If along
the execution we obtain an upper bound less than
L, then Algorithm 1 can be safely stopped and
return ?infeasible??this is the bound part. Oth-
erwise (if u? is fractional) go to step 3.
3. Find the ?most fractional? component of u? (call
it u?j ) and branch: constrain uj = 0 and go to
step 2, eventually obtaining an integer solution u?0
or infeasibility; and then constrain uj = 1 and do
the same, obtaining u?1. Return the u
? ? {u?0,u
?
1}
that yields the largest objective value.
Although this procedure may have worst-case expo-
nential runtime, we found it empirically to rapidly
obtain the exact solution in all test cases.
4 Experiments and Results
4.1 Dataset, Preprocessing, and Learning
In our experiments, we use FrameNet 1.5, which
contains a lexicon of 877 frames and 1,068 role
labels, and 78 documents with multiple predicate-
argument annotations (a superset of the SemEval
shared task dataset; Baker et al, 2007). We used the
same split as Das and Smith (2011), with 55 doc-
uments for training (containing 19,582 frame anno-
tations) and 23 for testing (with 4,458 annotations).
We randomly selected 4,462 predicates in the train-
ing set as development data. The raw sentences in all
the training and test documents were preprocessed
using MXPOST (Ratnaparkhi, 1996) and the MST
dependency parser (McDonald et al, 2005).
The state-of-the-art system for this task is SE-
MAFOR, an open source tool (Das et al, 2010a)5
that provides a baseline benchmark for our new al-
gorithm. We use the components of SEMAFOR
as-is to define the features h and train the weights
? used in the scoring function c. We also use its
5http://www.ark.cs.cmu.edu/SEMAFOR
214
heuristic mechanism to find potential spans St for a
given predicate t. SEMAFOR learns weights using
`2-penalized log-likelihood; we augmented its dev
set-tuning procedure to tune both the regularization
strength and the AD3 penalty strength ?. We ini-
tialize ? = 0.1 and follow Martins et al (2011b)
in dynamically adjusting it. Note that we do not use
SEMAFOR?s automatic frame identification compo-
nent in our presented experiments, as we assume that
we have gold frames on each predicate. This lets us
compare the different argument identification meth-
ods in a controlled fashion.
4.2 Decoding Strategies
We compare the following algorithms:
1. Local: this is a na??ve argument identification
strategy that selects the best span for each role r,
according to the score function c(r, s). It ignores
all constraints except ?uniqueness.?
2. SEMAFOR: this strategy employs greedy beam
search to eliminate overlaps between predicted ar-
guments (Das et al, 2010b, Algorithm 1). Note
that it does not try to respect the ?excludes? and
?requires? constraints between pairs of roles. The
default size of the beam in SEMAFOR was a safe
10,000; this resulted in extremely slow decoding
times. We also tried beam sizes of 100 and 2
(the latter being the smallest size that achieves the
same F1 score on the dev set as beam width 100.)
3. CPLEX, LP: this uses CPLEX to solve the re-
laxed LP in Eq. 11. To handle fractional z, for
each role r, we choose the best span s?, such that
s? = argmaxs?Sr zr,s, solving ties arbitrarily.
4. CPLEX, exact: this tackles the actual ILP (Eq. 2)
with CPLEX.
5. AD3, LP: this is the counterpart of the LP version
of CPLEX, where the relaxed problem is solved
using AD3. We choose the spans for each role in
the same way as in strategy 3.
6. AD3, exact: this couples AD3 with branch-and-
bound search to get the exact integer solution.
4.3 Results
Table 1 shows performance of the different decoding
strategies on the test set. We report precision, recall,
and F1 scores.6 Since these scores do not penal-
6We use the evaluation script from SemEval 2007 shared task,
modified to evaluate only the argument identification output.
ize structural violations, we also report the number
of overlap, ?excludes,? and ?requires? constraints
that were violated in the test set. Finally, we tab-
ulate each setting?s decoding time in seconds on the
whole test set averaged over 5 runs.7 The Local
model is very fast but suffers degradation in pre-
cision and violates one constraint roughly per nine
predicates. SEMAFOR used a default beam size of
10,000, which is extremely slow; a faster version of
beam size 100 results in the same precision and re-
call values, but is 15 times faster. Beam size 2 results
in slightly worse precision and recall values, but is
even faster. All of these, however, result in many
constraint violations. Strategies involving CPLEX
and AD3 perform similarly to each other and SE-
MAFOR on precision and recall, but eliminate most
or all of the constraint violations. SEMAFOR with
beam size 2 is 11-16 times faster than the CPLEX
strategies, but is only twice as fast than AD3, and re-
sults in significantly more structural violations. The
exact algorithms are slower than the LP versions, but
compared to CPLEX, AD3 is significantly faster and
has a narrower gap between its exact and LP ver-
sions. We found that relaxation was tight 99.8% of
the time on the test examples.
The example in Fig. 1 is taken from our test set,
and shows an instance where two roles, Partner 1
and Partner 2 share the ?requires? relationship; for
this example, the beam search decoder misses the
Partner 2 role, which is a violation, while our AD3
decoder identifies both arguments correctly. Note
that beam search makes plenty of linguistic viola-
tions, but has precision and recall values that are
marginally better than AD3. We found that beam
search, when violating many ?requires? constraints,
often finds one role in the pair, which increases its
recall. AD3 is sometimes more conservative in such
cases, predicting neither role. A second issue, as
noted in footnote 4, is that the annotations some-
times violate these constraints. Overall, we found
it interesting that imposing the constraints did not
have much effect on standard measures of accuracy.
7We used a 64-bit machine with 2 2.6GHz dual-core CPUs (i.e.,
4 processors in all) with a total of 8GB of RAM. The work-
ers in AD3 were not parallelized, while CPLEX automatically
parallelized execution.
215
Violations
Method P R F1 Overlap Requires Excludes Time in Secs.
Local 67.69 59.76 63.48 441 45 15 1.26 ? 0.01
SEMAFOR (beam = 2) 70.18 59.54 64.42 0 49 0 2.74 ? 0.10
SEMAFOR (beam = 100) 70.43 59.64 64.59 0 50 1 29.00 ? 0.25
SEMAFOR (beam = 10000) 70.43 59.64 64.59 0 50 1 440.67 ? 5.53
CPLEX, LP 70.34 59.43 64.43 0 1 0 32.67 ? 1.29
CPLEX, exact 70.31 59.45 64.43 0 0 0 43.12 ? 1.26
AD3, LP 70.30 59.45 64.42 2 2 0 4.17 ? 0.01
AD3, exact 70.31 59.45 64.43 0 0 0 4.78 ? 0.04
Table 1: Comparison of decoding strategies in ?4.2. We evaluate in terms of precision, recall and F1 score on a test
set containing 4,458 predicates. We also compute the number of structural violations each model makes: number
of overlapping arguments and violations of the ?requires? and ?excludes? constraints of ?2. Finally decoding time
(without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs.
5 Related Work
Semantic role labeling: Most SRL systems use
conventions from PropBank (Kingsbury and Palmer,
2002) and NomBank (Meyers et al, 2004), which
store information about verbal and nominal pred-
icates and corresponding symbolic and meaning-
specific semantic roles. A separate line of work,
including this paper, investigates SRL systems that
use FrameNet conventions; while less popular, these
systems, pioneered by Gildea and Jurafsky (2002),
consider predicates of a wider variety of syntactic
categories, use semantic frame abstractions, and em-
ploy explicit role labels. A common trait in prior
work has been the use of a two-stage model that
identifies arguments first, then labels them. They are
treated jointly here, unlike what has typically been
done in PropBank-style SRL (Ma`rquez et al, 2008).
Dual decomposition: Rush et al (2010) proposed
subgradient-based dual decomposition as a way of
combining models which are tractable individually,
but not jointly, by solving a relaxation of the origi-
nal problem. This was followed by work adopting
this method for syntax and translation (Koo et al,
2010; Auli and Lopez, 2011; DeNero and Macherey,
2011; Rush and Collins, 2011; Chang and Collins,
2011). Recently, Martins et al (2011b) showed that
the success of subgradient-based dual decomposi-
tion strongly relies on breaking down the original
problem into a ?good? decomposition, i.e., one with
few overlapping components. This leaves out many
declarative constrained problems, for which such a
good decomposition is not readily available. For
those, Martins et al (2011b) proposed the AD3 al-
gorithm, which retains the modularity of previous
methods, but can handle thousands of small over-
lapping components.
Exact decoding: This paper contributes an exact
branch-and-bound technique wrapped around AD3.
A related line of research is that of Rush and Collins
(2011), who proposed a tightening procedure for
dual decomposition, which can be seen as a cutting
plane method (another popular approach in combi-
natorial optimization).
6 Conclusion
We presented a novel algorithm for incorporating
declarative linguistic knowledge as constraints in
shallow semantic parsing. It outperforms a na??ve
baseline that is oblivious to the constraints. Further-
more, it is significantly faster than a decoder em-
ploying a state-of-the-art proprietary solver, and less
than twice as slow as beam search, which is inexact
and does not respect all linguistic constraints. Our
method is easily amenable to the inclusion of more
constraints, which would require minimal program-
ming effort. Our implementation of AD3 within
SEMAFOR will be publicly released at http://
www.ark.cs.cmu.edu/SEMAFOR.
Acknowledgments
We thank the three anonymous reviewers for their valu-
able feedback. This material is based upon work sup-
ported by NSF grant IIS-1054319, Google?s support
of the Wordly Knowledge Project, a FCT/ICTI grant
through the CMU-Portugal Program, and by Priberam,
through the Discooperio project, contract 2011/18501 of
the EU/FEDER program.
216
References
M. Auli and A. Lopez. 2011. A comparison of loopy be-
lief propagation and dual decomposition for integrated
ccg supertagging and parsing. In Proc. of ACL.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction. In
Proc. of SemEval.
Y.-W. Chang and Michael Collins. 2011. Exact decoding
of Phrase-Based translation models through lagrangian
relaxation. In Proc. of EMNLP. Association for Com-
putational Linguistics.
D. Chen, N. Schneider, D. Das, and N. A. Smith.
2010. SEMAFOR: Frame argument resolution with
log-linear models. In Proc. of SemEval.
D. Das and N. A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Proc. of
ACL.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of
NAACL-HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b.
SEMAFOR 1.0: a probabilistic frame-semantic parser.
Technical report, CMU-LTI-10-001.
J. DeNero and K. Macherey. 2011. Model-based aligner
combination using dual decomposition. In Proc. of
ACL.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame Semantics. In Linguistics in
the Morning Calm. Hanshin.
M. Gerber and J. Y. Chai. 2010. Beyond nombank: A
study of implicit arguments for nominal predicates. In
ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
L. Ma`rquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate variational infer-
ence. In EMNLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011a. An augmented
Lagrangian approach to constrained MAP inference.
In Proc. of ICML.
A F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011b. Dual decomposition with
many overlapping components. In Proc. of EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The
NomBank project: An interim report. In Proc. of
NAACL/HLT Workshop on Frontiers in Corpus Anno-
tation.
V. Punyakanok, D. Roth, W.-T. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear programming
inference. In Proc. of COLING.
V. Punyakanok, D. Roth, and W Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34:257?287.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In Proc. of ACL.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear programming
relaxations for natural language processing. In Pro-
ceedings of EMNLP.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
K. Toutanova, A. Haghighi, and C. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of
ACL.
217
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 176?180,
Dublin, Ireland, August 23-24, 2014.
CMU: Arc-Factored, Discriminative Semantic Dependency Parsing
Sam Thomson Brendan O?Connor Jeffrey Flanigan David Bamman
Jesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sthomson,brenocon,jflanigan,dbamman,jessed,
swabha,nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present an arc-factored statistical model
for semantic dependency parsing, as de-
fined by the SemEval 2014 Shared Task 8
on Broad-Coverage Semantic Dependency
Parsing. Our entry in the open track placed
second in the competition.
1 Introduction
The task of broad coverage semantic dependency
parsing aims to provide a shallow semantic analysis
of text not limited to a specific domain. As distinct
from deeper semantic analysis (e.g., parsing to a
full lambda-calculus logical form), shallow seman-
tic parsing captures relationships between pairs
of words or concepts in a sentence, and has wide
application for information extraction, knowledge
base population, and question answering (among
others).
We present here two systems that produce seman-
tic dependency parses in the three formalisms of the
SemEval 2014 Shared Task 8 on Broad-Coverage
Semantic Dependency Parsing (Oepen et al., 2014).
These systems generate parses by extracting fea-
tures for each potential dependency arc and learn-
ing a statistical model to discriminate between good
arcs and bad; the first treats each labeled edge de-
cision as an independent multiclass logistic regres-
sion (?3.2.1), while the second predicts arcs as part
of a graph-based structured support vector machine
(?3.2.2). Common to both models is a rich set of
features on arcs, described in ?3.2.3. We include a
discussion of features found to have no discernable
effect, or negative effect, during development (?4).
Our system placed second in the open track of
the Broad-Coverage Semantic Dependency Parsing
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
Figure 1: Example annotations for DM (top), PAS (middle),
and PCEDT (bottom).
task (in which output from syntactic parsers and
other outside resources can be used). We present
our results in ?5.
2 Formalisms
The Shared Task 8 dataset consists of annota-
tions of the WSJ Corpus in three different se-
mantic dependency formalisms. DM is derived
from LinGO English Resource Grammar (ERG)
annotations in DeepBank (Flickinger et al., 2012).
PAS is derived from the Enju HPSG treebank us-
ing the conversion rules of Miyao et al. (2004).
PCEDT is derived from the tectogrammatical layer
of the Prague Czech-English Dependency Treebank
(Haji?c, 1998). See Figure 1 for an example.
The three formalisms come from very different
linguistic theories, but all are represented as labeled
directed graphs, with words as vertices, and all
have ?top? annotations, corresponding roughly to
the semantic focus of the sentence. (A ?top? need
not be a root of the graph.) This allows us to use
the same machinery (?3) for training and testing
statistical models for the three formalisms.
3 Models
We treat the problem as a three-stage pipeline. The
first stage prunes words by predicting whether they
have any incoming or outgoing edges at all (?3.1);
if a word does not, then it is not considered for
any attachments in later stages. The second stage
176
predicts where edges are present, and their labels
(?3.2). The third stage predicts whether a predicate
word is a top or not (?3.3). Formalisms sometimes
annotate more than one ?top? per sentence, but we
found that we achieve the best performance on all
formalisms by predicting only the one best-scoring
?top? under the model.
3.1 Singleton Classification
For each formalism, we train a classifier to rec-
ognize singletons, nodes that have no parents or
children. (For example, punctuation tokens are of-
ten singletons.) This makes the system faster with-
out affecting accuracy. For singleton prediction,
we use a token-level logistic regression classifier,
with features including the word, its lemma, and
its part-of-speech tag. If the classifier predicts a
probability of 99% or higher the token is pruned;
this removes around 10% of tokens. (The classi-
fier performs differently on different formalisms;
on PAS it has perfect accuracy, while on DM and
PCEDT accuracy is in the mid-90?s.)
3.2 Edge Prediction
In the second stage of the pipeline, we predict the
set of labeled directed edges in the graph. We use
the same set of edge-factored features (?3.2.3) in
two alternative models: an edge-independent mul-
ticlass logistic regression model (LOGISTICEDGE,
?3.2.1); and a structured SVM (Taskar et al., 2003;
Tsochantaridis et al., 2004) that enforces a deter-
minism constraint for certain labels, which allows
each word to have at most one outgoing edge with
that label (SVMEDGE, ?3.2.2). For each formalism,
we trained both models with varying features en-
abled and hyperparameter settings and submitted
the configuration that produced the best labeled F
1
on the development set. For DM and PCEDT, this
was LOGISTICEDGE; for PAS, this was SVMEDGE.
We report results only for the submitted configu-
rations, with different features enabled. Due to
time constraints, full hyperparameter sweeps and
comparable feature sweeps were not possible.
3.2.1 LOGISTICEDGE Parser
The LOGISTICEDGE model considers only token
index pairs (i, j) where |i ? j| ? 10, i 6= j,
and both t
i
and t
j
have been predicted to be non-
singletons by the first stage. Although this prunes
some gold edges, among the formalisms, 95%?97%
of all gold edges are between tokens of distance
10 or less. Both directions i ? j and j ? i are
considered between every pair.
Let L be the set of K + 1 possible output labels:
the formalism?s original K edge labels, plus the
additional label NOEDGE, which indicates that no
edge exists from i to j. The model treats every pair
of token indices (i, j) as an independent multiclass
logistic regression over output space L. Let x be
an input sentence. For candidate parent index i,
child index j, and edge label `, we extract a feature
vector f(x, i, j, `), where ` is conjoined with every
feature described in ?3.2.3. The multiclass logis-
tic regression model defines a distribution over L,
parametrized by weights ?:
P (` | ?, x, i, j) =
exp{? ? f(x, i, j, `)}
?
`
?
?L
exp{? ? f(x, i, j, `
?
)}
.
? is learned by minimizing total negative log-
likelihood of the above (with weighting; see be-
low), plus `
2
regularization. AdaGrad (Duchi et al.,
2011) is used for optimization. This seemed to opti-
mize faster than L-BFGS (Liu and Nocedal, 1989),
at least for earlier iterations, though we did no sys-
tematic comparison. Stochastic gradient steps are
applied one at a time from individual examples,
and a gradient step for the regularizer is applied
once per epoch.
The output labels have a class imbalance; in all
three formalisms, there are many more NOEDGE
examples than true edge examples. We improved
F
1
performance by downweighting NOEDGE
examples through a weighted log-likelihood
objective,
?
i,j
?
`
w
`
logP (` |?, x, i, j), with
w
NOEDGE
= 0.3 (selected on development set) and
w
`
= 1 otherwise.
Decoding: To predict a graph structure at test-time
for a new sentence, the most likely edge label is pre-
dicted for every candidate (i, j) pair of unpruned
tokens. If an edge is predicted for both directions
for a single (i, j) pair, only the edge with the higher
score is chosen. (There are no such bidirectional
edges in the training data.) This post-processing ac-
tually did not improve accuracy on DM or PCEDT;
it did improve PAS by ?0.2% absolute F
1
, but we
did not submit LOGISTICEDGE for PAS.
3.2.2 SVMEDGE Parser
In the SVMEDGE model, we use a structured SVM
with a determinism constraint. This constraint en-
sures that each word token has at most one outgoing
edge for each label in a set of deterministic labels
L
d
. For example, in DM a predicate never has more
177
than one child with edge label ?ARG1.? L
d
was
chosen to be the set of edges that were > 99.9%
deterministic in the training data.
1
Consider the fully dense graph of all edges be-
tween all words predicted as not singletons by the
singleton classifier ?3.1 (in all directions with all
possible labels). Unlike LOGISTICEDGE, the la-
bel set L does not include an explicit NOEDGE
label. If ? denotes the model weights, and f de-
notes the features, then an edge from i to j with
label ` in the dense graph has a weight c(i, j, `)
assigned to it using the linear scoring function
c(i, j, `) = ? ? f(x, i, j, `).
Decoding: For each node and each label `, if ` ?
L
d
, the decoder adds the highest scoring outgoing
edge, if its weight is positive. For ` 6? L
d
, every
outgoing edge with positive weight is added. This
procedure is guaranteed to find the highest scoring
subgraph (largest sum of edge weights) of the dense
graph subject to the determinism constraints. Its
runtime is O(n
2
).
The model weights are trained using the struc-
tured SVM loss. If x is a sentence and y is a
graph over that sentence, let the features be de-
noted f(x, y) =
?
(i,j,`)?y
f(x, i, j, `). The SVM
loss for each training example (x
i
, y
i
) is:
??
>
f(x
i
, y
i
)+max
y
?
>
f(x
i
, y)+cost(y, y
i
)
where cost(y, y
i
) = ?|y \ y
i
| + ?|y
i
\ y|. ? and
? trade off between precision and recall for the
edges (Gimpel and Smith, 2010). The loss is min-
imized with AdaGrad using early-stopping on a
development set.
3.2.3 Edge Features
Table 1 describes the features we used for predict-
ing edges. These features were computed over an
edge e with parent token s at index i and child
token t at index j. Unless otherwise stated, each
feature template listed has an indicator feature that
fires for each value it can take on. For the sub-
mitted results, LOGISTICEDGE uses all features
except Dependency Path v2, POS Path, and Dis-
tance Thresholds, and SVMEDGE uses all features
except Dependency Path v1. This was due to
SVMEDGE being faster to train than LOGISTIC-
EDGE when including POS Path features, and due
1
By this we mean that of the nodes that have at least
one outgoing ` edge, 99.9% of them have only one outgo-
ing ` edge. For DM, L
d
= L\{? and c,? ? or c,? ? then c,?
?loc,? ?mwe,? ?subord?}; for PAS, L
d
= L; and for PCEDT,
L
d
={?DPHR,? ?INTF,? ?VOCAT?}.
Tokens: The tokens s and t themselves.
Lemmas: Lemmas of s and t.
POS tags: Part of speech tags of s and t.
Linear Order: Fires if i < j.
Linear Distance: i? j.
Dependency Path v1 (LOGISTICEDGE only): The
concatenation of all POS tags, arc labels and up/down
directions on the path in the syntactic dependency tree
from s to t. Conjoined with s, with t, and without either.
Dependency Path v2 (SVMEDGE only): Same as De-
pendency Path v1, but with the lemma of s or t instead
of the word, and substituting the token for any ?IN? POS
tag.
Up/Down Dependency Path: The sequence of upward
and downward moves needed to get from s to t in the
syntactic dependency tree.
Up/Down/Left/Right Dependency Path: The unla-
beled path through the syntactic dependency tree from s
to t, annotated with whether each step through the tree
was up or down, and whether it was to the right or left in
the sentence.
Is Parent: Fires if s is the parent of t in the syntactic
dependency parse.
Dependency Path Length: Distance between s and t in
the syntactic dependency parse.
POS Context: Concatenated POS tags of tokens at i?1,
i, i+ 1, j ? 1, j, and j + 1. Concatenated POS tags of
tokens at i? 1, i, j ? 1, and j. Concatenated POS tags
of tokens at i, i+ 1, j, and j + 1.
Subcategorization Sequence: The sequence of depen-
dency arc labels out of s, ordered by the index of the
child. Distinguish left children from right children. If t
is a direct child of s, distinguish its arc label with a ?+?.
Conjoin this sequence with the POS tag of s.
Subcategorization Sequence with POS: As above, but
add the POS tag of each child to its arc label.
POS Path (SVMEDGE only): Concatenated POS tags
between and including i and j. Conjoined with head
lemma, with dependent lemma, and without either.
Distance Thresholds (SVMEDGE only): Fires for ev-
ery integer between 1 and blog(|i? j|+1)/ log(1.39)c
inclusive.
Table 1: Features used in edge prediction
to time constraints for the submission we were un-
able to retrain LOGISTICEDGE with these features.
3.2.4 Feature Hashing
The biggest memory usage was in the map from
feature names to integer indices during feature
extraction. For experimental expedience, we im-
plemented multitask feature hashing (Weinberger
et al., 2009), which hashes feature names to indices,
under the theory that errors due to collisions tend
to cancel. No drop in accuracy was observed.
3.3 Top Prediction
We trained a separate token-level binary logistic
regression model to classify whether a token?s node
had the ?top? attribute or not. At decoding time, all
predicted predicates (i.e., nodes where there is at
178
least one outbound edge) are possible candidates
to be ?top?; the classifier probabilities are evalu-
ated, and the highest-scoring node is chosen to be
?top.? This is suboptimal, since some graphs have
multiple tops (in PCEDT this is more common);
but selection rules based on probability thresholds
gave worse F
1
performance on the dev set. For a
given token t at index i, the top classifier?s features
included t?s POS tag, i, those two conjoined, and
the depth of t in the syntactic dependency tree.
4 Negative Results
We followed a forward-selection process during
feature engineering. For each potential feature,
we tested the current feature set versus the current
feature set plus the new potential feature. If the
new feature did not improve performance, we did
not add it. We list in table 2 some of the features
which we tested but did not improve performance.
In order to save time, we ran these feature se-
lection experiments on a subsample of the training
data, for a reduced number of iterations. These re-
sults thus have a strong caveat that the experiments
were not exhaustive. It may be that some of these
features could help under more careful study.
5 Experimental Setup
We participated in the Open Track, and used the
syntactic dependency parses supplied by the orga-
nizers. Feature engineering was performed on a
development set (?20), training on ??00?19. We
evaluate labeled precision (LP), labeled recall (LR),
labeled F
1
(LF), and labeled whole-sentence match
(LM) on the held-out test data using the evaluation
script provided by the organizers. LF was aver-
aged over the formalisms to determine the winning
system. Table 3 shows our scores.
6 Conclusion and Future Work
We found that feature-rich discriminative models
perform well at the task of mapping from sentences
to semantic dependency parses. While our final
approach is fairly standard for work in parsing,
we note here additional features and constraints
which did not appear to help (contrary to expecta-
tion). There are a number of clear extensions to
this work that could improve performance. While
an edge-factored model allows for efficient infer-
ence, there is much to be gained from higher-order
features (McDonald and Pereira, 2006; Martins
et al., 2013). The amount of information shared
Word vectors: Features derived from 64-dimensional
vectors from (Faruqui and Dyer, 2014), including the
concatenation, difference, inner product, and element-
wise multiplication of the two vectors associated with
a parent-child edge. We also trained a Random Forest
on the word vectors using Liaw and Wiener?s (2002) R
implementation. The predicted labels were then used as
features in LOGISTICEDGE.
Brown clusters Features derived from Brown clusters
(Brown et al., 1992) trained on a large corpus of web data.
Parent, child, and conjoined parent-child edge features
from cluster prefixes of length 2, 4, 6, 8, 10, and 12.
Conjunctions of those features with the POS tags of the
parent and child tokens.
Active/passive: Active/passive voice feature (as in Jo-
hansson and Nugues (2008)) conjoined with both the
Linear Distance features and the Subcategorization Se-
quence features. Voice information may already be cap-
tured by features from the Stanford dependency?style
parses, which include passivization information in arc
labels such as nsubjpass and auxpass (de Marneffe and
Manning, 2008).
Connectivity constraint: Enforcing that the graph is
connected (ignoring singletons), similar to Flanigan et al.
(2014). Almost all semantic dependency graphs in the
training data are connected (ignoring singletons), but
we found that enforcing this constraint significantly hurt
precision.
Tree constraint: Enforces that the graph is a tree. Un-
surprisingly, we found that enforcing a tree constraint
hurt performance.
Table 2: Features and constraints giving negative results.
LP LR LF LM
DM 0.8446 0.8348 0.8397 0.0875
PAS 0.9078 0.8851 0.8963 0.2604
PCEDT 0.7681 0.7072 0.7364 0.0712
Average 0.8402 0.8090 0.8241 0.1397
Table 3: Labeled precision (LP), recall (LR), F
1
(LF), and
whole-sentence match (LM) on the held-out test data.
between the three formalisms suggests that a multi-
task learning (Evgeniou and Pontil, 2004) frame-
work could lead to gains. And finally, there is
additional structure in the formalisms which could
be exploited (such as the deterministic processes
by which an original PCEDT tree annotation was
converted into a graph); formulating more subtle
graph constraints to capture this a priori knowl-
edge could lead to improved performance. We
leave such explorations to future work.
Acknowledgements
We are grateful to Manaal Faruqui for his help in word vector
experiments, and to reviewers for helpful comments. The re-
search reported in this paper was sponsored by the U.S. Army
Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533, DARPA
grant FA8750-12-2-0342 funded under the DEFT program,
U.S. NSF grants IIS-1251131 and IIS-1054319, and Google?s
support of the Reading is Believing project at CMU.
179
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Lin-
guistics, 18(4):467?479.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
Coling 2008: Proc. of the Workshop on Cross-Framework
and Cross-Domain Parser Evaluation, pages 1?8. Manch-
ester, UK.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adap-
tive subgradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Research,
12:2121?2159.
Theodoros Evgeniou and Massimiliano Pontil. 2004. Regular-
ized multitask learning. In Proc. of KDD, pages 109?117.
Seattle, WA, USA.
Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correlation.
In Proc. of EACL, pages 462?471. Gothenburg, Sweden.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer,
and Noah A. Smith. 2014. A discriminative graph-based
parser for the Abstract Meaning Representation. In Proc.
of ACL, pages 1426?1436. Baltimore, MD, USA.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. Deep-
Bank: a dynamically annotated treebank of the Wall Street
Journal. In Proc. of the Eleventh International Workshop on
Treebanks and Linguistic Theories, pages 85?96. Lisbon,
Portugal.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity. URL http://lti.cs.cmu.edu/sites/
default/files/research/reports/2010/
cmulti10008.pdf.
Jan Haji?c. 1998. Building a syntactically annotated corpus:
the Prague Dependency Treebank. In Eva Haji?cov?a, ed-
itor, Issues of Valency and Meaning. Studies in Honour
of Jarmila Panevov?a, pages 106?132. Prague Karolinum,
Charles University Press, Prague.
Richard Johansson and Pierre Nugues. 2008. Dependency-
based semantic role labeling of PropBank. In Proc. of
EMNLP, pages 69?78. Honolulu, HI, USA.
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomForest. R News, 2(3):18?
22. URL http://cran.r-project.org/web/
packages/randomForest/.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45(3):503?528.
Andr?e F. T. Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL, pages 617?622. Sofia,
Bulgaria.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proc. of
EACL, pages 81?88. Trento, Italy.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
head-driven phrase structure grammar from the Penn Tree-
bank. In Proc. of IJCNLP, pages 684?693. Hainan Island,
China.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel
Zeman, Dan Flickinger, Jan Haji?c, Angelina Ivanova, and
Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage
semantic dependency parsing. In Proc. of SemEval. Dublin,
Ireland.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin Markov networks. In Proc. of NIPS, pages 25?32.
Vancouver, British Columbia, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
of ICML, pages 104?111. Banff, Alberta, Canada.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In Proc. of ICML, pages
1113?1120. Montreal, Quebec, Canada.
180
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 35?40,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Rating Computer-Generated Questions with Mechanical Turk
Michael Heilman
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
mheilman@cs.cmu.edu
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We use Amazon Mechanical Turk to rate
computer-generated reading comprehension
questions about Wikipedia articles. Such
application-specific ratings can be used to
train statistical rankers to improve systems?
final output, or to evaluate technologies that
generate natural language. We discuss the
question rating scheme we developed, assess
the quality of the ratings that we gathered
through Amazon Mechanical Turk, and show
evidence that these ratings can be used to im-
prove question generation.
1 Introduction
This paper discusses the use of Amazon Mechani-
cal Turk (MTurk) to rate computer-generated read-
ing comprehension questions about Wikipedia arti-
cles.
We have developed a question generation sys-
tem (Heilman and Smith, 2009; Heilman and
Smith, 2010) that uses the overgenerate-and-rank
paradigm (Langkilde and Knight, 1998). In the
the overgenerate-and-rank approach, many system-
generated outputs are ranked in order to select higher
quality outputs. While the approach has had con-
siderable success in natural language generation
(Langkilde and Knight, 1998; Walker et al, 2001),
it often requires human labels on system output for
the purpose of learning to rank. We employ MTurk
to reduce the time and cost of acquiring these labels.
For many problems, large labeled datasets do not
exist. One alternative is to build rule-based sys-
tems, but it is often difficult and time-consuming
to accurately encode relevant linguistic knowledge
in rules. Another alternative, unsupervised or semi-
supervised learning, usually requires clever formu-
lations of bias that guide the learning process (Car-
roll and Charniak, 1992; Yarowsky, 1995); such
intuitions are not always available. Thus, small,
application-specific labeled datasets, which can be
cheaply constructed using MTurk, may provide con-
siderable benefits by enabling the use of supervised
learning.
In addition to using MTurk ratings to train a
learned ranking component, we could also use
MTurk ratings to evaluate the final top-ranked out-
put of our system. More generally, MTurk can be a
useful evaluation tool for systems that output natu-
ral language (e.g., systems for natural language gen-
eration, summarization, translation). For example,
Callison-Burch (2009) used MTurk to evaluate ma-
chine translations. MTurk facilitates the efficient
measurement and understanding of errors made by
such technologies, and could be used to complement
automatic evaluation metrics such as BLEU (Pap-
ineni et al, 2002) and ROUGE (Lin, 2004).
It is true that, for our task, MTurk workers
annotate computer-generated rather than human-
generated natural language. Thus, the data will
not be as generally useful as other types of anno-
tations, such as parse trees, which could be used to
build general purpose syntactic parsers. However,
for the reasons described above, we believe the use
of MTurk to rate computer-generated output can be
useful for the training, development, and evaluation
of language technologies.
The remainder of the paper is organized as fol-
lows: ?2 and ?3 briefly describe the question gener-
ation system and corpora used in our experiments.
?4 provides the details of our rating scheme. ?5 dis-
cusses the quantity, cost, speed, and quality of the
ratings we gathered. ?6 presents preliminary experi-
ments showing that theMTurk ratings improve ques-
tion ranking. Finally, in ?7, we conclude.
35
2 Question Generation System
We use MTurk to improve and evaluate a system
for automatic question generation (QG). In our QG
approach, hand-crafted rules transform declarative
sentences from an input text into a large set of ques-
tions (i.e., hundreds per page). This rule system is
complemented by a statistical ranker, which ranks
questions according to their quality. Currently, we
focus on basic linguistic issues and the goal of pro-
ducing acceptable questions?that is, questions that
are grammatical, make sense, and are not vague. We
believe an educator could select and revise output
from the system in order to produce a final set of
high-quality, challenging questions.
Our system is described by Heilman and Smith
(2010). In that work, we employed a differ-
ent scheme involving binary judgments of question
quality according to various factors such as gram-
maticality, vagueness, and others. We also employed
university students as novice annotators. For the
training dataset, only one human rated each ques-
tion. See Heilman and Smith (2009) for more de-
tails.1
3 Corpora
In our experiments, we generated questions from
60 articles sampled from the ?featured? articles in
the English Wikipedia2 that have between 250 and
2,000 word tokens. This collection provides expos-
itory texts written at an adult reading level from a
variety of domains, which roughly approximates the
prose that a secondary or post-secondary level stu-
dent would encounter. By choosing from the fea-
tured articles, we intended to select well-edited ar-
ticles about topics of general interest. We then ran-
domly selected 20 questions from each of 60 articles
for labeling with MTurk.3
1We also generated some questions using a technique that
replaces pronouns and underspecified noun phrases with an-
tecedent mentions identified by a coreference resolver. We will
not provide details about this component here because they are
not relevant to our use of MTurk to rate questions. A forthcom-
ing paper will describe these additions.
2The English Wikipedia data were downloaded on Decem-
ber 16, 2008 from http://en.wikipedia.org
3Five questions were later eliminated from this set due to
minor implementation changes, the details of which are unin-
teresting. The final set contained 1,195 questions.
Rating Details
1 Bad The question has major prob-
lems.
2 Unacceptable The question definitely has a
minor problem.
3 Borderline The question might have a
problem, but I?m not sure.
4 Acceptable The question does not have
problems.
5 Good The question is as good as one
that a human teacher might
write for a reading quiz.
Table 1: The five-point question rating scale.
4 Rating Scheme
This section describes the rating scheme we de-
veloped for evaluating the quality of computer-
generated questions on MTurk.
Questions were presented independently as sin-
gle human intelligence tasks (HITs). At the top of
the page, raters were given the instructions shown
in Figure 1 along with 7 examples of good and bad
questions with their appropriate ratings. Below the
instructions and examples was an excerpt from the
source text consisting of up to 5 sentences of con-
text, ending with the primary sentence that the ques-
tion was generated from. The question to be rated
then followed.
Below each question was the five-point rating
scale shown in Table 1. Workers were required to
select a single rating by clicking a radio button. At
the bottom of the page, the entire source article text
was given, in case the worker felt it was necessary
to refer back to more context.
We paid 5 cents per rating,4 and each question was
rated by five workers. With the 10% commission
charge by Amazon, each question cost 27.5 cents.
The final rating value was computed by taking
the arithmetic mean of the ratings. Table 2 provides
some examples of questions and their mean ratings.
4.1 Monitoring Turker Ratings
During some pilot tests, we found that it was par-
ticularly important to set some qualification criteria
for workers. Specifically, we only allowed workers
4Given the average time spent per HIT, the pay rate can be
extrapolated to $5?10 per hour.
36
Figure 1: A screenshot of the instructions given to workers.
who had completed at least 50 previously accepted
HITs. We also required that at least 95% of workers?
previous submissions had been accepted.
We also submitted HITs in batches of 100 to 500
so that we could more closely monitor the process.
In addition, we performed a limited amount of
semi-automated monitoring of the ratings, and re-
jected work from workers who were clearly ran-
domly clicking on answers or not following the rat-
ing scheme properly. We tried to err on the side of
accepting bad work. After all ratings for a batch
of questions were received, we calculated for each
worker the number of ratings submitted, the aver-
age time spent on each question, the average rating,
and the correlation of the worker?s rating with the
mean of the other 4 ratings. We used a combination
of these statistics to identify extremely bad workers
(e.g., ones who had negative correlations with other
workers and spent less than 10 seconds per ques-
tion). If some of the ratings for a question were
rejected, then the HIT was ?extended? in order to
receive 5 ratings.
5 Quantity, Cost, Speed, and Quality
This section discusses the quantity and quality of the
question ratings we received from MTurk.
5.1 Quantity and Cost of Ratings
We received 5 ratings each for 1,200 questions, cost-
ing a total of $330. 178 workers participated. Work-
ers submitted 33.9 ratings on average (s.d. = 58.0).
The distribution of ratings per worker was highly
skewed, such that a handful of workers submitted
100 or more ratings (max = 395). The ratings from
these who submitted more than 100 ratings seemed
to be slightly lower in quality but still acceptable.
The median number of ratings per worker was 11.
5.2 Speed of Ratings
Ratings were received very quickly once the HITs
were submitted. Figure 2 shows the cumulative
number of ratings received for a batch of questions,
37
Source Text Excerpt Question Rating
MD 36 serves as the main road through the Georges Creek
Valley, a region which is historically known for coal mining,
and has been designated by MDSHA as part of the Coal Her-
itage Scenic Byway.
Which part has MD 36 been desig-
nated by MDSHA as?
1.4
He worked further on the story with the Soviet author Isaac
Babel, but no material was ever published or released from
their collaboration, and the production of Bezhin Meadow
came to an end.
What did the production of Bezhin
Meadow come to?
2.0
The design was lethal, successful and much imitated, and
remains one of the definitive weapons of World War II.
Does the design remain one of the
definitive weapons of World War II?
2.8
Francium was discovered by Marguerite Perey in France
(from which the element takes its name) in 1939.
Where was Francium discovered by
Marguerite Perey in 1939?
3.8
Lazare Ponticelli was the longest-surviving officially recog-
nized veteran. . . Although he attempted to remain with his
French regiment, he eventually enlisted in. . .
Did Lazare Ponticelli attempt to re-
main with his French regiment?
4.4
Table 2: Example computer-generated questions, along with their mean ratings from Mechanical Turk.
1000150020002500
Cumulative # Ratings
05001000150020002500
010
203
040
506
070
Minut
es Elap
sed
05001000150020002500
010
203
040
506
070
Minut
es Elap
sed
Figure 2: The cumulative number of ratings submitted by
MTurk workers over time, for a batch of 497 questions
posted simultaneously (there are 5 ratings per question).
indicating that more than 1,000 ratings were re-
ceived per hour.
5.3 Quality of Ratings
We evaluated inter-rater agreement by having the
first author and an independent judge rate a random
sample of 40 questions from 4 articles. The indepen-
dent judge was a computational linguist. The Pear-
son correlation coefficient between the first author?s
ratings and the mean ratings from MTurk work-
ers was r = 0.79, which is fairly strong though
not ideal. The correlation between the independent
judge?s ratings and the MTurk workers was r =
0.74. These fairly strong positive correlations be-
tween the MTurk ratings and the two human judges
provide evidence that the rating scheme is consis-
tent and well-defined. The results also agree with
Snow et al (2008), who found that aggregating la-
bels from 3 to 7 workers often provides expert lev-
els of agreement. Interestingly, the agreement be-
tween the two human raters was somewhat lower
(r = 0.65), suggesting that aggregated labels from a
crowd of MTurk workers can be more reliable than
individual humans.5
6 Using Labeled Data to Improve Question
Ranking
In this section, we provide some preliminary results
to demonstrate that MTurk ratings can be used for
learning to rank QG output.
First, we briefly characterize the quality of un-
ranked output. Figure 3 shows a histogram of the
mean MTurk ratings for the 1,195 questions, show-
ing that only a relatively small fraction of the ques-
tions created by the overgenerating steps of our sys-
tem are acceptable: 12.9% when using 3.5 as the
threshold for acceptability.
However, ranking can lead to substantially higher
levels of quality in the top-ranked questions, which
5We also converted the ratings into binary values based on
whether they exceeded a threshold of 3.5. After this conversion
to a nominal scale, we computed a Cohen?s ? of 0.54, which
indicates ?moderate? agreement (Landis and Koch, 1977).
38
5%10%15%20%25%
Percent of Questions
0%5%10%15%20%25%
Mean
 
Ratin
g Ran
ge
Figure 3: The distribution of the 1,195 question ratings.
might be presented first in a user interface. There-
fore, we investigated how many MTurk-rated ques-
tions are needed to train an effective statistical ques-
tion ranker. Our ranking model is essentially the
same as the one used by Heilman and Smith (2010).
Rather than logistic regression, which we used pre-
viously, here we use a linear regression with `2 reg-
ularization to account for the ordinal scale of the av-
eraged question ratings. We set the regularization
parameter through cross-validation with the training
data.
The regression includes all of the features de-
scribed by Heilman and Smith (2010). It includes
features for sentence lengths, whether the question
includes various WH words, whether certain syntac-
tic transformations performed during QG, whether
negation words are present in questions, how many
times various parts of speech appeared, and others.
It also includes some additional coreference features
for parts of speech and lengths of noun phrase men-
tions and their antecedents.6 In all, the ranker in-
cludes 326 features.
For our experiments, we set aside a randomly cho-
sen 200 of the 1,195 rated questions as a test set.
We then trained statistical rankers on randomly sam-
pled subsets of the remaining questions, from size
N = 50 up to N = 995. For each value of N ,
we used the ranker trained on that amount of data
to rank the 200 test questions. We then computed
6Since these additional coreference features are not immedi-
ately relevant to this work, we will not describe them fully here.
A forthcoming paper will describe them in more detail.
0.40.50.6
Acceptability of 
Ranked Fifth
0.20.30.40.50.6 0
250
500
750
1000
Top-
Traini
ng Set 
Size
0.20.30.40.50.6 0
250
500
750
1000
Traini
ng Set 
Size
Figure 4: A graph of the acceptability of top-ranked ques-
tions when datasets of increasing size are used to train a
statistical question ranker. Error bars show 95% confi-
dence intervals computed from the 10 runs of the sam-
pling process.
the percentage of the top fifth of the ranked test set
questions with a mean rating above 3.5. For each
N less than 995, we repeated the entire sampling,
training, and ranking process 10 times and averaged
the results. (We used the same 200 question test set
throughout the process.)
Figure 4 presents the results, with the acceptabil-
ity of unranked questions (23%) included at N = 0
for comparison. We see that ranking more than dou-
bles the acceptability of the top-ranked questions,
consistent with findings from Heilman and Smith
(2010). It appears that ranking performance im-
proves as more training data are used. When 650 ex-
amples were used, 49% of the top-ranked questions
were acceptable. Ranking performance appears to
level off somewhat when more than 650 training ex-
amples are used. However, we speculate that if the
model included more fine-grained features, the value
of additional labeled data might increase.7
7 Conclusion
In this paper, we used MTurk to gather quality rat-
ings for computer-generated questions. We pre-
7To directly compare the ranker?s predictions to the correla-
tions presented in ?5.3, we computed a correlation coefficient
between the test set ratings from MTurk and the ratings pre-
dicted by the ranker when it was trained on all 995 training ex-
amples. The coefficient was r = 0.36, which is statistically sig-
nificant (p < .001) but suggests that there is substantial room
for improvement in the ranking model.
39
sented a question rating scheme, and found high lev-
els of inter-rater agreement (r ? 0.74) between rat-
ings from reliable humans and ratings from MTurk.
We also showed that ratings can be gathered from
MTurk quickly (more than 1,000 per hour) and
cheaply (less than 30 cents per question).
While ratings of computer-generated language are
not as generally useful as, for example, annotations
of the syntactic structure of human-generated lan-
guage, many research paradigms involving the auto-
matic generation of language may be able to benefit
from using MTurk to quickly and cheaply evaluate
ongoing work. Also, we demonstrated that such rat-
ings can be used in an overgenerate-and-rank strat-
egy to greatly improve the quality of a system?s top-
ranked output.
References
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. of EMNLP.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
M. Heilman and N. A. Smith. 2009. Question gener-
ation via overgenerating transformations and ranking.
Technical Report CMU-LTI-09-013, Language Tech-
nologies Institute, Carnegie Mellon University.
M. Heilman and N. A. Smith. 2010. Good question!
statistical ranking for question generation. In Proc. of
NAACL-HLT.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33.
I. Langkilde and Kevin Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Proc.
of ACL.
C. Lin. 2004. ROUGE: a package for automatic eval-
uation of summaries. In Proc. of Workshop on Text
Summarization.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Snow, B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast ? but is it good? evaluating non-expert
annotations for natural language tasks. In Proc. of
EMNLP.
M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proc. of NAACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL.
40
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 152?158,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Shedding (a Thousand Points of) Light on Biased Language
Tae Yano
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
taey@cs.cmu.edu
Philip Resnik
Department of Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umiacs.umd.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
This paper considers the linguistic indicators of bias
in political text. We used Amazon Mechanical Turk
judgments about sentences from American political
blogs, asking annotators to indicate whether a sen-
tence showed bias, and if so, in which political di-
rection and through which word tokens. We also
asked annotators questions about their own political
views. We conducted a preliminary analysis of the
data, exploring how different groups perceive bias in
different blogs, and showing some lexical indicators
strongly associated with perceived bias.
1 Introduction
Bias and framing are central topics in the study of com-
munications, media, and political discourse (Scheufele,
1999; Entman, 2007), but they have received relatively
little attention in computational linguistics. What are the
linguistic indicators of bias? Are there lexical, syntactic,
topical, or other clues that can be computationally mod-
eled and automatically detected?
Here we use Amazon Mechanical Turk (MTurk) to en-
gage in a systematic, empirical study of linguistic indi-
cators of bias in the political domain, using text drawn
from political blogs. Using the MTurk framework, we
collected judgments connected with the two dominant
schools of thought in American politics, as exhibited in
single sentences. Since no one person can claim to be an
unbiased judge of political bias in language, MTurk is an
attractive framework that lets us measure perception of
bias across a population.
2 Annotation Task
We drew sentences from a corpus of American political
blog posts from 2008. (Details in Section 2.1.) Sentences
were presented to participants one at a time, without con-
text. Participants were asked to judge the following (see
Figure 1 for interface design):
? To what extent a sentence or clause is biased (none,
somewhat, very);
? The nature of the bias (very liberal, moderately lib-
eral, moderately conservative, very conservative, bi-
ased but not sure which direction); and
? Which words in the sentence give away the author?s
bias, similar to ?rationale? annotations in Zaidan et
al. (2007).
For example, a participant might identify a moderate
liberal bias in this sentence,
Without Sestak?s challenge, we would have
Specter, comfortably ensconced as a Democrat
in name only.
adding checkmarks on the underlined words. A more
neutral paraphrase is:
Without Sestak?s challenge, Specter would
have no incentive to side more frequently with
Democrats.
It is worth noting that ?bias,? in the sense we are us-
ing it here, is distinct from ?subjectivity? as that topic
has been studied in computational linguistics. Wiebe
et al (1999) characterize subjective sentences as those
that ?are used to communicate the speaker?s evaluations,
opinions, and speculations,? as distinguished from sen-
tences whose primary intention is ?to objectively com-
municate material that is factual to the reporter.? In con-
trast, a biased sentence reflects a ?tendency or preference
towards a particular perspective, ideology or result.?1 A
subjective sentence can be unbiased (I think that movie
was terrible), and a biased sentence can purport to com-
municate factually (Nationalizing our health care system
1http://en.wikipedia.org/wiki/Bias as of 13 April,
2010.
152
is a point of no return for government interference in the
lives of its citizens2).
In addition to annotating sentences, each participant
was asked to complete a brief questionnaire about his or
her own political views. The survey asked:
1. Whether the participant is a resident of the United
States;
2. Who the participant voted for in the 2008 U.S.
presidential election (Barack Obama, John McCain,
other, decline to answer);
3. Which side of political spectrum he/she identified
with for social issues (liberal, conservative, decline
to answer); and
4. Which side of political spectrum he/she identified
with for fiscal/economic issues (liberal, conserva-
tive, decline to answer).
This information was gathered to allow us to measure
variation in bias perception as it relates to the stance of
the annotator, e.g., whether people who view themselves
as liberal perceive more bias in conservative sources, and
vice versa.
2.1 Dataset
We extracted our sentences from the collection of blog
posts in Eisenstein and Xing (2010). The corpus con-
sists of 2008 blog posts gathered from six sites focused
on American politics:
? American Thinker (conservative),3
? Digby (liberal),4
? Hot Air (conservative),5
? Michelle Malkin (conservative),6
? Think Progress (liberal),7 and
? Talking Points Memo (liberal).8
13,246 posts were gathered in total, and 261,073 sen-
tences were extracted using WebHarvest9 and OpenNLP
1.3.0.10 Conservative and liberal sites are evenly rep-
resented (130,980 sentences from conservative sites,
130,093 from liberal sites). OpenNLP was also used for
tokenization.
2Sarah Palin, http://www.facebook.com/note.php?
note_id=113851103434, August 7, 2009.
3http://www.americanthinker.com
4http://digbysblog.blogspot.com
5http://hotair.com
6http://michellemalkin.com
7http://thinkprogress.org
8http://www.talkingpointsmemo.com
9http://web-harvest.sourceforge.net
10http://opennlp.sourceforge.net
Liberal Conservative
thinkprogress org exit question
video thinkprogress hat tip
et rally ed lasky
org 2008 hot air
gi bill tony rezko
wonk room ed morrissey
dana perino track record
phil gramm confirmed dead
senator mccain american thinker
abu ghraib illegal alien
Table 1: Top ten ?sticky? partisan bigrams for each side.
2.2 Sentence Selection
To support exploratory data analysis, we sought a di-
verse sample of sentences for annotation, but we were
also guided by some factors known or likely to correlate
with bias. We extracted sentences from our corpus that
matched at least one of the categories below, filtering to
keep those of length between 8 and 40 tokens. Then, for
each category, we first sampled 100 sentences without re-
placement. We then randomly extracted sentences up to
1,100 from the remaining pool. We selected the sentences
this way so that the collection has variety, while including
enough examples for individual categories. Our goal was
to gather at least 1,000 annotated sentences; ultimately
we collected 1,041. The categories are as follows.
?Sticky? partisan bigrams. One likely indicator of
bias is the use of terms that are particular to one side or
the other in a debate (Monroe et al, 2008). In order to
identify such terms, we independently created two lists
of ?sticky? (i.e., strongly associated) bigrams in liberal
and conservative subcorpora, measuring association us-
ing the log-likelihood ratio (Dunning, 1993) and omitting
bigrams containing stopwords.11 We identified a bigram
as ?liberal? if it was among the top 1,000 bigrams from
the liberal blogs, as measured by strength of association,
and was also not among the top 1,000 bigrams on the con-
servative side. The reverse definition yielded the ?conser-
vative? bigrams. The resulting liberal list contained 495
bigrams, and the conservative list contained 539. We then
manually filtered cases that were clearly remnant HTML
tags and other markup, arriving at lists of 433 and 535,
respectively. Table 1 shows the strongest weighted bi-
grams.
As an example, consider this sentence (with a preced-
ing sentence of context), which contains gi bill. There is
no reason to think the bigram itself is inherently biased
(in contrast to, for example, death tax, which we would
11We made use of Pedersen?s N -gram Statistics Package (Banerjee
and Pedersen, 2003).
153
perceive as biased in virtually any unquoted context), but
we do perceive bias in the full sentence.
Their hard fiscal line softens in the face of
American imperialist adventures. According to
CongressDaily the Bush dogs are also whining
because one of their members, Stephanie Her-
seth Sandlin, didn?t get HERGI Bill to the floor
in favor of Jim Webb?s .
Emotional lexical categories. Emotional words might
be another indicator of bias. We extracted four categories
of words from Pennebaker?s LIWC dictionary: Nega-
tive Emotion, Positive Emotion, Causation, and Anger.12
The following is one example of a biased sentence in our
dataset that matched these lexicons, in this case the Anger
category; the match is in bold.
A bunch of ugly facts are nailing the biggest
scare story in history.
The five most frequent matches in the corpus for each
category are as follows.13
Negative Emotion: war attack* problem* numb* argu*
Positive Emotion: like well good party* secur*
Causation: how because lead* make why
Anger: war attack* argu* fight* threat*
Kill verbs. Greene and Resnik (2009) discuss the rel-
evance of syntactic structure to the perception of senti-
ment. For example, their psycholinguistic experiments
would predict that when comparing Millions of people
starved under Stalin (inchoative) with Stalin starved mil-
lions of people (transitive), the latter will be perceived as
more negative toward Stalin, because the transitive syn-
tactic frame tends to be connected with semantic prop-
erties such as intended action by the subject and change
of state in the object. ?Kill verbs? provide particularly
strong examples of such phenomena, because they ex-
hibit a large set of semantic properties canonically as-
sociated with the transitive frame (Dowty, 1991). The
study by Greene and Resnik used 11 verbs of killing and
similar action to study the effect of syntactic ?packag-
ing? on perceptions of sentiment.14 We included mem-
bership on this list (in any morphological form) as a se-
lection criterion, both because these verbs may be likely
12http://www.liwc.net. See Pennebaker et al (2007) for de-
tailed description of background theory, and how these lexicons were
constructed. Our gratitude to Jamie Pennebaker for the use of this dic-
tionary.
13Note that some LIWC lexical entries are specified as pre-
fixes/stems, e.g. ugl*, which matches ugly uglier, etc.
14The verbs are: kill, slaughter, assassinate, shoot, poison, strangle,
smother, choke, drown, suffocate, and starve.
to appear in sentences containing bias (they overlap sig-
nificantly with Pennebaker?s Negative Emotion list), and
because annotation of bias will provide further data rel-
evant to Greene and Resnik?s hypothesis about the con-
nections among semantic propeties, syntactic structures,
and positive or negative perceptions (which are strongly
connected with bias).
In our final 1,041-sentence sample, ?sticky bigrams?
occur 235 times (liberal 113, conservative 122), the lexi-
cal category features occur 1,619 times (Positive Emotion
577, Negative Emotion 466, Causation 332, and Anger
244), and ?kill? verbs appear as a feature in 94 sentences.
Note that one sentence often matches multiple selection
criteria. Of the 1,041-sentence sample, 232 (22.3%) are
from American Thinker, 169 (16.2%) from Digby, 246
(23.6%) from Hot Air, 73 (7.0%) from Michelle Malkin,
166 (15.9%) from Think Progress, and 155 (14.9%) from
Talking Points Memo.
3 Mechanical Turk Experiment
We prepared 1,100 Human Intelligence Tasks (HITs),
each containing one sentence annotation task. 1,041 sen-
tences were annotated five times each (5,205 judgements
total). One annotation task consists of three bias judge-
ment questions plus four survey questions. We priced
each HIT between $0.02 and $0.04 (moving from less
to more to encourage faster completion). The total cost
was $212.15 We restricted access to our tasks to those
who resided in United States and who had above 90% ap-
proval history, to ensure quality and awareness of Amer-
ican political issues. We also discarded HITs annotated
by workers with particularly low agreement scores. The
time allowance for each HIT was set at 5 minutes.
3.1 Annotation Results
3.1.1 Distribution of Judgments
Overall, more than half the judgments are ?not biased,?
and the ?very biased? label is used sparingly (Table 2).
There is a slight tendency among the annotators to assign
the ?very conservative? label, although moderate bias is
distributed evenly on both side (Table 3). Interestingly,
there are many ?biased, but not sure? labels, indicating
that the annotators are capable of perceiving bias (or ma-
nipulative language), without fully decoding the intent of
the author, given sentences out of context.
Bias 1 1.5 2 2.5 3
% judged 36.0 26.6 25.5 9.4 2.4
Table 2: Strength of perceived bias per sentence, averaged over
the annotators (rounded to nearest half point). Annotators rate
bias on a scale of 1 (no bias), 2 (some bias), and 3 (very biased).
15This includes the cost for the discarded annotations.
154
Figure 1: HIT: Three judgment questions. We first ask for the strength of bisa, then the direction. For the word-level annotation
question (right), workers are asked to check the box to indicate the region which ?give away? the bias.
Bias type VL ML NB MC VC B
% judged 4.0 8.5 54.8 8.2 6.7 17.9
Table 3: Direction of perceived bias, per judgment (very lib-
eral, moderately liberal, no bias, moderately conservative, very
conservative, biased but not sure which).
Economic
L M C NA
So
ci
al
L 20.1 10.1 4.9 0.7
M 0.0 21.9 4.7 0.0
C 0.1 0.4 11.7 0.0
NA 0.1 0.0 11.2 14.1
Table 4: Distribution of judgements by annotators? self-
identification on social issues (row) and fiscal issue (column);
{L, C, M, NA} denote liberal, conservative, moderate, and de-
cline to answer, respectively.
3.1.2 Annotation Quality
In this study, we are interested in where the wisdom of
the crowd will take us, or where the majority consensus
on bias may emerge. For this reason we did not contrive a
gold standard for ?correct? annotation. We are, however,
mindful of its overall quality?whether annotations have
reasonable agreement, and whether there are fraudulent
responses tainting the results.
To validate our data, we measured the pair-wise Kappa
statistic (Cohen, 1960) among the 50 most frequent work-
ers16 and took the average over all the scores.17. The
average of the agreement score for the first question is
0.55, and the second 0.50. Those are within the range of
reasonable agreement for moderately difficult task. We
also inspected per worker average scores for frequent
workers18 and found one with consistently low agreement
scores. We discarded all the HITs by this worker from our
results. We also manually inspected the first 200 HITs for
apparent frauds. The annotations appeared to be consis-
tent. Often annotators agreed (many ?no bias? cases were
unanimous), or differed in only the degree of strength
(?very biased? vs. ?biased?) or specificity (?biased but I
am not sure? vs. ?moderately liberal?). The direction of
bias, if specified, was very rarely inconsistent.
Along with the annotation tasks, we asked workers
how we could improve our HITs. Some comments were
16258 workers participated; only 50 of them completed more than 10
annotations.
17Unlike traditional subjects for a user-annotation study, our annota-
tors have not judged all the sentences considered in the study. There-
fore, to compute the agreement, we considered only the case where two
annotators share 20 or more sentences.
18We consider only those with 10 or more annotations.
155
insightful for our study (as well as for the interface de-
sign). A few pointed out that an impolite statement or
a statement of negative fact is not the same as bias, and
therefore should be marked separately from bias. Others
mentioned that some sentences are difficult to judge out
of context. These comments will be taken into account in
future research.
4 Analysis and Significance
In the following section we report some of the interesting
trends we found in our annotation results. We consider a
few questions and report the answers the data provide for
each.
4.1 Is a sentence from a liberal blog more likely be
seen as liberal?
In our sample sentence pool, conservatives and liberals
are equally represented, though each blog site has a dif-
ferent representation.19 We grouped sentences by source
site, then computed the percentage representation of each
site within each bias label; see Table 5. In the top row,
we show the percentage representation of each group in
overall judgements.
In general, a site yields more sentences that match its
known political leanings. Note that in our annotation
task, we did not disclose the sentence?s source to the
workers. The annotators formed their judgements solely
based on the content of the sentence. This result can
be taken as confirming people?s ability to perceive bias
within a sentence, or, conversely, as confirming our a pri-
ori categorizations of the blogs.
at ha mm db tp tpm
Overall 22.3 23.6 7.0 16.2 15.9 14.9
NB 23.7 22.3 6.1 15.7 17.0 15.3
VC 24.8 32.3 19.3 6.9 7.5 9.2
MC 24.4 33.6 8.0 8.2 13.6 12.2
ML 16.6 15.2 3.4 21.1 22.9 20.9
VL 16.7 9.0 4.3 31.0 22.4 16.7
B 20.1 25.4 7.2 19.5 12.3 13.7
Table 5: Percentage representation of each site within bias label
pools from question 2 (direction of perceived bias): very liberal,
moderately liberal, no bias, moderately conservative, very con-
servative, biased but not sure which. Rows sum to 100. Bold-
face indicates rates higher than the site?s overall representation
in the pool.
4.2 Does a liberal leaning annotator see more
conservative bias?
In Table 5, we see that blogs are very different from each
other in terms of the bias annotators perceive in their lan-
19Posts appear on different sites at different rates.
1
3
10
32
100
very conservative no bias very liberal
not sure
LL
MM
CC
Overall
Figure 2: Distribution of bias labels (by judgment) for social
and economic liberals (LL), social and economic moderates
(MM), and social and economic conservatives (CC), and over-
all. Note that this plot uses a logarithmic scale, to tease apart
the differences among groups.
guage. In general, conservative sites seemingly produced
much more identifiable partisan bias than liberal sites.20
This impression, however, might be an artifact of the
distribution of the annotators? own bias. As seen in Ta-
ble 4, a large portion of our annotators identified them-
selves as liberal in some way. People might call a state-
ment biased if they disagree with it, while showing le-
niency toward hyperbole more consistent with their opin-
ions.
To answer this question, we break down the judgement
labels by the annotators? self-identification, and check
the percentage of each bias type within key groups (see
Figure 2). In general, moderates perceive less bias than
partisans (another useful reality check, in the sense that
this is to be expected), but conservatives show a much
stronger tendency to label sentences as biased, in both
directions. (We caution that the underrepresentation of
self-identifying conservatives in our worker pool means
that only 608 judgments from 48 distinct workers were
used to estimate these statistics.) Liberals in this sample
are less balanced, perceiving conservative bias at double
the rate of liberal bias.
4.3 What are the lexical indicators of perceived
bias?
For a given word type w, we calculate the frequency that
it was marked as indicating bias, normalized by its total
number of occurrences. To combine the judgments of dif-
ferent annotators, we increment w?s count by k/n when-
ever k judgments out of n marked the word as showing
bias. We perform similar calculations with a restriction
to liberal and conservative judgments on the sentence as a
20Liberal sites cumulatively produced 64.9% of the moderately lib-
eral bias label and 70.1 % of very liberal, while conservative sites pro-
duced 66.0% of moderately conservative and 76.4% of very conserva-
tive, respectively.
156
Overall Liberal Conservative Not Sure Which
bad 0.60 Administration 0.28 illegal 0.40 pass 0.32
personally 0.56 Americans 0.24 Obama?s 0.38 bad 0.32
illegal 0.53 woman 0.24 corruption 0.32 sure 0.28
woman 0.52 single 0.24 rich 0.28 blame 0.28
single 0.52 personally 0.24 stop 0.26 they?re 0.24
rich 0.52 lobbyists 0.23 tax 0.25 happen 0.24
corruption 0.52 Republican 0.22 claimed 0.25 doubt 0.24
Administration 0.52 union 0.20 human 0.24 doing 0.24
Americans 0.51 torture 0.20 doesn?t 0.24 death 0.24
conservative 0.50 rich 0.20 difficult 0.24 actually 0.24
doubt 0.48 interests 0.20 Democrats 0.24 exactly 0.22
torture 0.47 doing 0.20 less 0.23 wrong 0.22
Table 6: Most strongly biased words, ranked by relative frequency of receiving a bias mark, normalized by total frequency. Only
words appearing five times or more in our annotation set are ranked.
whole. Top-ranked words for each calculation are shown
in Table 6.
Some of the patterns we see are consistent with what
we found in our automatic method for proposing biased
bigrams. For example, the bigrams tended to include
terms that refer to members or groups on the opposing
side. Here we find that Republican and Administration
(referring in 2008 to the Bush administration) tends to
show liberal bias, while Obama?s and Democrats show
conservative bias.
5 Discussion and Future Work
The study we have conducted here represents an initial
pass at empirical, corpus-driven analysis of bias using the
methods of computational linguistics. The results thus far
suggest that it is possible to automatically extract a sam-
ple that is rich in examples that annotators would con-
sider biased; that na??ve annotators can achieve reason-
able agreement with minimal instructions and no train-
ing; and that basic exploratory analysis of results yields
interpretable patterns that comport with prior expecta-
tions, as well as interesting observations that merit further
investigation.
In future work, enabled by annotations of biased and
non-biased material, we plan to delve more deeply into
the linguistic characteristics associated with biased ex-
pression. These will include, for example, an analysis
of the extent to which explicit ?lexical framing? (use of
partisan terms, e.g., Monroe et al, 2008) is used to con-
vey bias, versus use of more subtle cues such as syntactic
framing (Greene and Resnik, 2009). We will also explore
the extent to which idiomatic usages are connected with
bias, with the prediction that partisan ?memes? tend to be
more idiomatic than compositional in nature.
In our current analysis, the issue of subjectivity was not
directly addressed. Previous work has shown that opin-
ions are closely related to subjective language (Pang and
Lee, 2008). It is possible that asking annotators about
sentiment while asking about bias would provide a deeper
understanding of the latter. Interestingly, annotator feed-
back included remarks that mere negative ?facts? do not
convey an author?s opinion or bias. The nature of subjec-
tivity as a factor in bias perception is an important issue
for future investigation.
6 Conclusion
This paper considered the linguistic indicators of bias in
political text. We used Amazon Mechanical Turk judg-
ments about sentences from American political blogs,
asking annotators to indicate whether a sentence showed
bias, and if so, in which political direction and through
which word tokens; these data were augmented by a po-
litical questionnaire for each annotator. Our preliminary
analysis suggests that bias can be annotated reasonably
consistently, that bias perception varies based on personal
views, and that there are some consistent lexical cues for
bias in political blog data.
Acknowledgments
The authors acknowledge research support from HP
Labs, help with data from Jacob Eisenstein, and help-
ful comments from the reviewers, Olivia Buzek, Michael
Heilman, and Brendan O?Connor.
References
Satanjeev Banerjee and Ted Pedersen. 2003. The design, implementa-
tion and use of the ngram statistics package. In the Fourth Interna-
tional Conference on Intelligent Text Processing and Computational
Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1):37?46.
David Dowty. 1991. Thematic Proto-Roles and Argument Selection.
Language, 67:547?619.
157
Ted Dunning. 1993. Accurate methods for the statistics of surprise and
coincidence. Computational Linguistics, 19(1):61?74.
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog
corpus. Technical report CMU-ML-10-101.
Robert M. Entman. 2007. Framing bias: Media in the distribution of
power. Journal of Communication, 57(1):163?173.
Stephan Greene and Philip Resnik. 2009. More than words: Syntactic
packaging and implicit sentiment. In NAACL, pages 503?511, June.
Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn. 2008.
Fightin? words: Lexical feature selection and evaluation for identi-
fying the content of political conflict. Political Analysis, 16(4):372?
403, October.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis.
Foundations and Trends in Information Retrieval, 2(1-2):1?135.
J.W. Pennebaker, C.K Chung, M. Ireland, A Gonzales, and R J. Booth,
2007. The development and psychometric properties of LIWC2007.
Dietram A. Scheufele. 1999. Framing as a theory of media effects.
Journal of Communication, 49(1):103?122.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O?Hara. 1999.
Development and use of a gold standard data set for subjectivity
classifications. In Proceedings of the Association for Computational
Linguistics (ACL), pages 246?253.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using ?anno-
tator rationales? to improve machine learning for text categorization.
In NAACL, pages 260?267, April.
158
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 213?222,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Distributed Asynchronous Online Learning
for Natural Language Processing
Kevin Gimpel Dipanjan Das Noah A. Smith
Language Technologies Institute
Carnegie Mellon Univeristy
Pittsburgh, PA 15213, USA
{kgimpel,dipanjan,nasmith}@cs.cmu.edu
Abstract
Recent speed-ups for training large-scale
models like those found in statistical NLP
exploit distributed computing (either on
multicore or ?cloud? architectures) and
rapidly converging online learning algo-
rithms. Here we aim to combine the two.
We focus on distributed, ?mini-batch?
learners that make frequent updates asyn-
chronously (Nedic et al, 2001; Langford
et al, 2009). We generalize existing asyn-
chronous algorithms and experiment ex-
tensively with structured prediction prob-
lems from NLP, including discriminative,
unsupervised, and non-convex learning
scenarios. Our results show asynchronous
learning can provide substantial speed-
ups compared to distributed and single-
processor mini-batch algorithms with no
signs of error arising from the approximate
nature of the technique.
1 Introduction
Modern statistical NLP models are notoriously
expensive to train, requiring the use of general-
purpose or specialized numerical optimization al-
gorithms (e.g., gradient and coordinate ascent al-
gorithms and variations on them like L-BFGS and
EM) that iterate over training data many times.
Two developments have led to major improve-
ments in training time for NLP models:
? online learning algorithms (LeCun et al, 1998;
Crammer and Singer, 2003; Liang and Klein,
2009), which update the parameters of a model
more frequently, processing only one or a small
number of training examples, called a ?mini-
batch,? between updates; and
? distributed computing, which divides training
data among multiple CPUs for faster processing
between updates (e.g., Clark and Curran, 2004).
Online algorithms offer fast convergence rates
and scalability to large datasets, but distributed
computing is a more natural fit for algorithms that
require a lot of computation?e.g., processing a
large batch of training examples?to be done be-
tween updates. Typically, distributed online learn-
ing has been done in a synchronous setting, mean-
ing that a mini-batch of data is divided among
multiple CPUs, and the model is updated when
they have all completed processing (Finkel et al,
2008). Each mini-batch is processed only after the
previous one has completed.
Synchronous frameworks are appealing in that
they simulate the same algorithms that work on
a single processor, but they have the drawback
that the benefits of parallelism are only obtainable
within one mini-batch iteration. Moreover, empir-
ical evaluations suggest that online methods only
converge faster than batch algorithms when using
very small mini-batches (Liang and Klein, 2009).
In this case, synchronous parallelization will not
offer much benefit.
In this paper, we focus our attention on asyn-
chronous algorithms that generalize those pre-
sented by Nedic et al (2001) and Langford et al
(2009). In these algorithms, multiple mini-batches
are processed simultaneously, each using poten-
tially different and typically stale parameters. The
key advantage of an asynchronous framework is
that it allows processors to remain in near-constant
use, preventing them from wasting cycles wait-
ing for other processors to complete their por-
tion of the current mini-batch. In this way, asyn-
chronous algorithms allow more frequent parame-
ter updates, which speeds convergence.
Our contributions are as follows:
? We describe a framework for distributed asyn-
chronous optimization (?5) similar to those de-
scribed by Nedic et al (2001) and Langford et
al. (2009), but permitting mini-batch learning.
The prior work contains convergence results for
asynchronous online stochastic gradient descent
213
for convex functions (discussed in brief in ?5.2).
? We report experiments on three structured NLP
tasks, including one problem that matches
the conditions for convergence (named entity
recognition; NER) and two that depart from the-
oretical foundations, namely the use of asyn-
chronous stepwise EM (Sato and Ishii, 2000;
Cappe? and Moulines, 2009; Liang and Klein,
2009) for both convex and non-convex opti-
mization.
? We directly compare asynchronous algorithms
with multiprocessor synchronous mini-batch al-
gorithms (e.g., Finkel et al, 2008) and tradi-
tional batch algorithms.
? We experiment with adding artificial delays to
simulate the effects of network or hardware traf-
fic that could cause updates to be made with ex-
tremely stale parameters.
? Our experimental settings include both indi-
vidual 4-processor machines as well as large
clusters of commodity machines implementing
the MapReduce programming model (Dean and
Ghemawat, 2004). We also explore effects of
mini-batch size.
Our main conclusion is that, when small mini-
batches work well, asynchronous algorithms of-
fer substantial speed-ups without introducing er-
ror. When large mini-batches work best, asyn-
chronous learning does not hurt.
2 Optimization Setting
We consider the problem of optimizing a function
f : Rd ? R with respect to its argument, denoted
? = ??1, ?2, . . . , ?d?. We assume that f is a sum
of n convex functions (hence f is also convex):1
f(?) =
?n
i=1 fi(?) (1)
We initially focus our attention on functions that
can be optimized using gradient or subgradient
methods. Log-likelihood for a probabilistic model
with fully observed training data (e.g., conditional
random fields; Lafferty et al, 2001) is one exam-
ple that frequently arises in NLP, where the fi(?)
each correspond to an individual training exam-
ple and the ? are log-linear feature weights. An-
other example is large-margin learning for struc-
tured prediction (Taskar et al, 2005; Tsochan-
1We use ?convex? to mean convex-up when minimizing
and convex-down, or concave, when maximizing.
taridis et al, 2005), which can be solved by sub-
gradient methods (Ratliff et al, 2006).
For concreteness, we discuss the architecture
in terms of gradient-based optimization, using the
following gradient descent update rule (for mini-
mization problems):2
?(t+1) ? ?(t) ? ?(t)g(?(t)) (2)
where ?(t) is the parameter vector on the tth iter-
ation, ?(t) is the step size on the tth iteration, and
g : Rd ? Rd is the vector function of first deriva-
tives of f with respect to ?:
g(?) =
?
?f
??1
(?), ?f??2 (?), . . . ,
?f
??d
(?)
?
(3)
We are interested in optimizing such functions
using distributed computing, by which we mean to
include any system containing multiple processors
that can communicate in order to perform a single
task. The set of processors can range from two
cores on a single machine to a MapReduce cluster
of thousands of machines.
Note our assumption that the computation re-
quired to optimize f with respect to ? is, essen-
tially, the gradient vector g(?(t)), which serves
as the descent direction. The key to distribut-
ing this computation is the fact that g(?(t)) =
?n
i=1 gi(?
(t)), where gi(?) denotes the gradient
of fi(?) with respect to ?. We now discuss several
ways to go about distributing such a problem, cul-
minating in the asynchronous mini-batch setting.
3 Distributed Batch Optimization
Given p processors plus a master processor, the
most straightforward way to optimize f is to par-
tition the fi so that for each i ? {1, 2, . . . , n},
gi is computed on exactly one ?slave? processor.
Let Ij denote the subset of examples assigned to
the jth slave processor (
?p
j=1 Ij = {1, . . . , n}
and j 6= j? ? Ij ? Ij? = ?). Processor j re-
ceives the examples in Ij along with the neces-
sary portions of ?(t) for calculating gIj (?
(t)) =
?
i?Ij
gi(?
(t)). The result of this calculation is
returned to the master processor, which calculates
g(?(t)) =
?
j gIj (?
(t)) and executes Eq. 2 (or
something more sophisticated that uses the same
information) to obtain a new parameter vector.
It is natural to divide the data so that each pro-
cessor is assigned approximately n/p of the train-
ing examples. Because of variance in the expense
2We use the term ?gradient? for simplicity, but subgradi-
ents are sufficient throughout.
214
of calculating the different gi, and because of un-
predictable variation among different processors?
speed (e.g., variation among nodes in a cluster,
or in demands made by other users), there can be
variation in the observed runtime of different pro-
cessors on their respective subsamples. Each it-
eration of calculating g will take as long as the
longest-running among the processors, whatever
the cause of that processor?s slowness. In comput-
ing environments where the load on processors is
beyond the control of the NLP researcher, this can
be a major bottleneck.
Nonetheless, this simple approach is widely
used in practice; approaches in which the gradient
computation is distributed via MapReduce have
recently been described in machine learning and
NLP (Chu et al, 2006; Dyer et al, 2008; Wolfe et
al., 2008). Mann et al (2009) compare this frame-
work to one in which each processor maintains a
separate parameter vector which is updated inde-
pendently of the others. At the end of learning, the
parameter vectors are averaged or a vote is taken
during prediction. A similar parameter-averaging
approach was taken by Chiang et al (2008) when
parallelizing MIRA (Crammer et al, 2006). In
this paper, we restrict our attention to distributed
frameworks which maintain and update a single
copy of the parameters ?. The use of multiple
parameter vectors is essentially orthogonal to the
framework we discuss here and we leave the inte-
gration of the two ideas for future exploration.
4 Distributed Synchronous Mini-Batch
Optimization
Distributed computing can speed up batch algo-
rithms, but we would like to transfer the well-
known speed-ups offered by online and mini-batch
algorithms to the distributed setting as well. The
simplest way to implement mini-batch stochastic
gradient descent (SGD) in a distributed computing
environment is to divide each mini-batch (rather
than the entire batch) among the processors that
are available and to update the parameters once the
gradient from the mini-batch has been computed.
Finkel et al (2008) used this approach to speed
up training of a log-linear model for parsing. The
interaction between the master processor and the
distributed computing environment is nearly iden-
tical to the distributed batch optimization scenario.
Where M (t) is the set of indices in the mini-batch
processed on iteration t, the update is:
?(t+1) ? ?(t) ? ?(t)
?
i?M(t) gi(?
(t)) (4)
The distributed synchronous framework can
provide speed-ups over a single-processor imple-
mentation of SGD, but inevitably some processors
will end up waiting for others to finish processing.
This is the same bottleneck faced by the batch ver-
sion in ?3. While the time for each mini-batch is
shorter than the time for a full batch, mini-batch
algorithms make far more updates and some pro-
cessor cycles will be wasted in computing each
one. Also, more mini-batches imply that more
time will be lost due to per-mini-batch overhead
(e.g., waiting for synchronization locks in shared-
memory systems, or sending data and ? to the pro-
cessors in systems without shared memory).
5 Distributed Asynchronous Mini-Batch
Optimization
An asynchronous framework may use multiple
processors more efficiently and minimize idle time
(Nedic et al, 2001; Langford et al, 2009). In this
setting, the master sends ? and a mini-batchMk to
each slave k. Once slave k finishes processing its
mini-batch and returns gMk(?), the master imme-
diately updates ? and sends a new mini-batch and
the new ? to the now-available slave k. As a result,
slaves stay occupied and never need to wait on oth-
ers to finish. However, nearly all gradient com-
ponents are computed using slightly stale parame-
ters that do not take into account the most recent
updates. Nedic et al (2001) proved that conver-
gence is still guaranteed under certain conditions,
and Langford et al (2009) obtained convergence
rate results. We describe these results in more de-
tail in ?5.2.
The update takes the following form:
?(t+1) ? ?(t) ? ?(t)
?
i?M(?(t)) gi(?
(?(t))) (5)
where ?(t) ? t is the start time of the mini-batch
used for the tth update. Since we started pro-
cessing the mini-batch at time ?(t) (using param-
eters ?(?(t))), we denote the mini-batch M (?(t)). If
?(t) = t, then Eq. 5 is identical to Eq. 4. That is,
t? ?(t) captures the ?staleness? of the parameters
used to compute the gradient for the tth update.
Asynchronous frameworks do introduce error
into the training procedure, but it is frequently
the case in NLP problems that only a small frac-
tion of parameters is needed for each mini-batch
215
Input: number of examples n, mini-batch size m,
random seed r
?` ? ?;
seedRandomNumberGenerator (r);
while converged (?) = false do
g ? 0;
for j ? 1 to m do
k ? Uniform({1, . . . , n});
g ? g + gk(?`);
end
acquireLock (?);
? ? updateParams (?, g);
?` ? ?;
releaseLock (?);
end
Algorithm 1: Procedure followed by each thread for multi-
core asynchronous mini-batch optimization. ? is the single
copy of the parameters shared by all threads. The conver-
gence criterion is left unspecified here.
of training examples. For example, for simple
word alignment models like IBM Model 1 (Brown
et al, 1993), only parameters corresponding to
words appearing in the particular subsample of
sentence pairs are needed. The error introduced
when making asynchronous updates should intu-
itively be less severe in these cases, where dif-
ferent mini-batches use small and mostly non-
overlapping subsets of ?.
5.1 Implementation
The algorithm sketched above is general enough
to be suitable for any distributed system, but when
using a system with shared memory (e.g., a single
multiprocessor machine) a more efficient imple-
mentation is possible. In particular, we can avoid
the master/slave architecture and simply start p
threads that each compute and execute updates in-
dependently, with a synchronization lock on ?. In
our single-machine experiments below, we use Al-
gorithm 1 for each thread. A different random seed
(r) is passed to each thread so that they do not all
process the same sequence of examples. At com-
pletion, the result is contained in ?.
5.2 Convergence Results
We now briefly summarize convergence results
from Nedic et al (2001) and Langford et al
(2009), which rely on the following assumptions:
(i) The function f is convex. (ii) The gradients
gi are bounded, i.e., there exists C > 0 such that
?gi(?
(t))? ? C. (iii) ? (unknown) D > 0 such
that t ? ?(t) < D. (iv) The stepsizes ?(t) satisfy
certain standard conditions.
In addition, Nedic et al require that all func-
tion components are used with the same asymp-
totic frequency (as t ? ?). Their results are
strongest when choosing function components in
each mini-batch using a ?cyclic? rule: select func-
tion fi for the kth time only after all functions have
been selected k ? 1 times. For a fixed step size
?, the sequence of function values f(?(t)) con-
verges to a region of the optimum that depends
on ?, the maximum norm of any gradient vector,
and the maximum delay for any mini-batch. For
a decreasing step size, convergence is guaranteed
to the optimum. When choosing components uni-
formly at random, convergence to the optimum is
again guaranteed using a decreasing step size for-
mula, but with slightly more stringent conditions
on the step size.
Langford et al (2009) present convergence rates
via regret bounds, which are linear in D. The con-
vergence rate of asynchronous stochastic gradient
descent is O(
?
TD), where T is the total number
of updates made. In addition to the situation in
which function components are chosen uniformly
at random, Langford et al provide results for sev-
eral other scenarios, including the case in which an
adversary supplies the training examples in what-
ever ordering he chooses.
Below we experiment with optimization of both
convex and non-convex functions, using fixed step
sizes and decreasing step size formulas, and con-
sider several values of D. Even when exploring
regions of the experimental space that are not yet
supported by theoretical results, asynchronous al-
gorithms perform well empirically in all settings.
5.3 Gradients and Expectation-Maximization
The theory applies when using first-order methods
to optimize convex functions. Though the function
it is optimizing is not usually convex, the EM algo-
rithm can be understood as a hillclimber that trans-
forms the gradient to keep ? feasible; it can also
be understood as a coordinate ascent algorithm.
Either way, the calculations during the E-step re-
semble g(?). Several online or mini-batch vari-
ants of the EM algorithm have been proposed, for
example incremental EM (Neal and Hinton, 1998)
and online EM (Sato and Ishii, 2000; Cappe? and
Moulines, 2009), and we follow Liang and Klein
(2009) in referring to this latter algorithm as step-
wise EM. Our experiments with asynchronous
minibatch updates include a case where the log-
likelihood f is convex and one where it is not.
216
task data n # params. eval. method convex?
?6.1 named entity
recognition (CRF;
Lafferty et al, 2001)
CoNLL 2003 English
(Tjong Kim Sang and De
Meulder, 2003)
14,987
sents.
1.3M F1 SGD yes
?6.2 word alignment (Model
1, both directions;
Brown et al, 1993)
NAACL 2003 parallel text
workshop (Mihalcea and
Pedersen, 2003)
300K
pairs
14.2M ?2
(E?F +
F?E)
AER EM yes
S6.3 unsupervised POS
(bigram HMM)
Penn Treebank ?1?21
(Marcus et al, 1993)
41,825
sents.
2,043,226 (Johnson,
2007)
EM no
Table 1: Our experiments consider three tasks.
0 2 4 6 8 10 12
84
86
88
90
Wall clock time (hours)
F1
 
 
Asynchronous (4 processors)Synchronous (4 processors)Single?processor
Figure 1: NER: Synchronous
mini-batch SGD converges faster
in F1 than the single-processor
version, and the asynchronous
version converges faster still. All
curves use a mini-batch size of 4.
6 Experiments
We performed experiments to measure speed-ups
obtainable through distributed online optimiza-
tion. Since we will be considering different opti-
mization algorithms and computing environments,
we will primarily be interested in the wall-clock
time required to obtain particular levels of perfor-
mance on metrics appropriate to each task. We
consider three tasks, detailed in Table 1.
For experiments on a single node, we used a
64-bit machine with two 2.6GHz dual-core CPUs
(i.e., 4 processors in all) with a total of 8GB of
RAM. This was a dedicated machine that was not
available for any other jobs. We also conducted
experiments using a cluster architecture running
Hadoop 0.20 (an implementation of MapReduce),
consisting of 400 machines, each having 2 quad-
core 1.86GHz CPUs with a total of 6GB of RAM.
6.1 Named Entity Recognition
Our NER CRF used a standard set of features, fol-
lowing Kazama and Torisawa (2007), along with
token shape features like those in Collins (2002)
and simple gazetteer features; a feature was in-
cluded if and only it occurred at least once in train-
ing data (total 1.3M).We used a diagonal Gaussian
prior with a variance of 1.0 for each weight.
We compared SGD on a single processor to dis-
tributed synchronous SGD and distributed asyn-
chronous SGD. For all experiments, we used a
fixed step size of 0.01 and chose each training ex-
ample for each mini-batch uniformly at random
from the full data set.3 We report performance by
3In preliminary experiments, we experimented with vari-
0 2 4 6 8 10
86
88
90
F1
 
 
Synchronous (4 processors)Synchronous (2 processors)Single?processor
0 2 4 6 8 10
86
88
90
Wall clock time (hours)
F1
 
 
Asynchronous (4 processors)
Asynchronous (2 processors)Single?processor
Figure 2: NER: (Top) Synchronous optimization improves
very little when moving from 2 to 4 processors due to the
need for load-balancing, leaving some processors idle for
stretches of time. (Bottom) Asynchronous optimization does
not require load balancing and therefore improves when mov-
ing from 2 to 4 processors because each processor is in near-
constant use. All curves use a mini-batch size of 4 and the
?Single-processor? curve is identical in the two plots.
plotting test-set accuracy against wall-time over
12 hours.4
Comparing Synchronous and Asynchronous
Algorithms Figure 1 shows our primary result
for the NER experiments. When using all four
available processors, the asynchronous algorithm
converges faster than the other two algorithms. Er-
ror due to stale parameters during gradient com-
putation does not appear to cause any more varia-
ous fixed step sizes and decreasing step size schedules, and
found a fixed step size to work best for all settings.
4Decoding was performed offline (so as not to affect mea-
surments) with models sampled every ten minutes.
217
tion in performance than experienced by the syn-
chronous mini-batch algorithm. Note that the dis-
tributed synchronous algorithm and the single-
processor algorithm make identical sequences of
parameter updates; the only difference is the
amount of time between each update. Since we
save models every ten minutes and not every ith
update, the curves have different shapes. The se-
quence of updates for the asynchronous algorithm,
on the other hand, actually depends on the vagaries
of the computational environment. Nonetheless,
the asynchronous algorithm using 4 processors has
nearly converged after only 2 hours, while the
single-processor algorithm requires 10?12 hours
to reach the same F1.
Varying the Number of Processors Figure 2
shows the improvement in convergence time by
using 4 vs. 2 processors for the synchronous (top)
and asynchronous (bottom) algorithms. The ad-
ditional two processors help the asynchronous al-
gorithm more than the synchronous one. This
highlights the key advantage of asynchronous al-
gorithms: it is easier to keep all processors in
constant use. Synchronous algorithms might be
improved through load-balancing; in our experi-
ments here, we simply assigned m/p examples to
each processor, wherem is the mini-batch size and
p is the number of processors. When m = p, as in
the 4-processor curve in the upper plot of Figure 2,
we assign a single example to each processor; this
is optimal in the sense that no other scheduling
strategy will process the mini-batch faster. There-
fore, the fact that the 2-processor and 4-processor
curves are so close suggests that the extra two pro-
cessors are not being fully exploited, indicating
that the optimal load balancing strategy for a small
mini-batch still leaves processors under-used due
to the synchronous nature of the updates.
The only bottleneck in the asynchronous algo-
rithm is the synchronization lock during updating,
required since there is only one copy of ?. For
CRFs with a few million weights, the update is
typically much faster than processing a mini-batch
of examples; furthermore, when using small mini-
batches, the update vector is typically sparse.5 For
all experimental results presented thus far, we used
a mini-batch size of 4. We experimented with ad-
5In a standard implementation, the sparsity of the update
will be nullified by regularization, but to improve efficiency
in practice the regularization penalty can be accumulated and
applied less frequently than every update.
0 2 4 6 8 10 1285
86
87
88
89
90
91
Wall clock time (hours)
F1
 
 
Asynchronous, no delay
Asynchronous, ? = 5
Single?processor, no delay
Asynchronous, ? = 10
Asynchronous, ? = 20
Figure 3: NER: Convergence curves when a delay is incurred
with probability 0.25 after each mini-batch is processed. The
delay durations (in seconds) are sampled fromN(?, (?/5)2),
for several means ?. Each mini-batch (size = 4) takes less
than a second to process, so if the delay is substantially longer
than the time required to process a mini-batch, the single-
node version converges faster. While curves with ? = 10 and
20 appear less smooth than the others, they are still heading
steadily toward convergence.
ditional mini-batch sizes of 1 and 8, but there was
very little difference in the resulting curves.
Artificial Delays We experimented with adding
artificial delays to the algorithm to explore how
much overhead would be tolerable before paral-
lelized computation becomes irrelevant. Figure 3
shows results when each processor sleeps with
0.25 probability for a duration of time between
computing the gradient on its mini-batch of data
and updating the parameters. The delay length is
chosen from a normal distribution with the means
(in seconds) shown and ? = ?/5 (truncated at
zero). Since only one quarter of the mini-batches
have an artificial delay, increasing ? increases the
average parameter ?staleness?, letting us see how
the asynchronous algorithm fares with extremely
stale parameters.
The average time required to compute the gradi-
ent for a mini-batch of 4 is 0.62 seconds. When the
average delay is 1.25 seconds (? = 5), twice the
average time for a mini-batch, the asynchronous
algorithm still converges faster than the single-
node algorithm. In addition, even with substan-
tial delays of 5?10 times the processing time for a
mini-batch, the asynchronous algorithm does not
fail but proceeds steadily toward convergence.
The practicality of using the asynchronous algo-
rithm depends on the average duration for a mini-
batch and the amount of expected additional over-
head. We attempted to run these experiments on
218
AER Time (h:m)
Single machine:
Asynch. stepwise EM 0.274 1:58
Synch. stepwise EM (4 proc.) 0.274 2:08
Synch. stepwise EM (1 proc.) 0.272 6:57
Batch EM 0.276 2:15
MapReduce:
Asynch. stepwise EM 0.281 5:41
Synch. stepwise EM 0.273 27:03
Batch EM 0.276 8:35
Table 2: Alignment error rates and wall time after 20 itera-
tions of EM for various settings. See text for details.
a large MapReduce cluster, but the overhead re-
quired for each MapReduce job was too large to
make this viable (30?60 seconds).
6.2 Word Alignment
We trained IBM Model 1 in both directions. To
align test data, we symmetrized both directional
Viterbi alignments using the ?grow-diag-final?
heuristic (Koehn et al, 2003). We evaluated our
models using alignment error rate (AER).
Experiments on a Single Machine We fol-
lowed Liang and Klein (2009) in using syn-
chronous (mini-batch) stepwise EM on a single
processor for this task. We used the same learning
rate formula (?(t) = (t+2)?q, with 0.5 < q ? 1).
We also used asynchronous stepwise EM by using
the same update rule, but gathered sufficient statis-
tics on 4 processors of a single machine in paral-
lel, analogous to our asynchronous method from
?5. Whenever a processor was done gathering the
expected counts for its mini-batch, it updated the
sufficient statistics vector and began work on the
next mini-batch.
We used the sparse update described by Liang
and Klein, which allows each thread to make
additive updates to the parameter vector and
to separately-maintained normalization constants
without needing to renormalize after each update.
When probabilities are needed during inference,
normalizers are divided out on-the-fly as needed.
We made 10 passes of asynchronous stepwise
EM to measure its sensitivity to q and the mini-
batch size m, using different values of these
hyperparameters (q ? {0.5, 0.7, 1.0}; m ?
{5000, 10000, 50000}), and selected values that
maximized log-likelihood (q = 0.7, m = 10000).
Experiments on MapReduce We implemented
the three techniques in a MapReduce framework.
We implemented batch EM on MapReduce by
converting each EM iteration into two MapRe-
duce jobs: one for the E-step and one for the M-
step.6 For the E-step, we divided our data into
24 map tasks, and computed expected counts for
the source-target parameters at each mapper. Next,
we summed up the expected counts in one reduce
task. For the M-step, we took the output from
the E-step, and in one reduce task, normalized
each source-target parameter by the total count for
the source word.7 To gather sufficient statistics
for synchronous stepwise EM, we used 6 mappers
and one reducer for a mini-batch of size 10000.
For the asynchronous version, we ran four parallel
asynchronous mini-batches, the sufficient statis-
tics being gathered using MapReduce again for
each mini-batch with 6 map tasks and one reducer.
Results Figure 4 shows log-likelihood for the
English?French direction during the first 80 min-
utes of optimization. Similar trends were observed
for the French?English direction as well as for
convergence in AER. Table 2 shows the AER at
the end of 20 iterations of EM for the same set-
tings.8 It takes around two hours to finish 20 iter-
ations of batch EM on a single machine, while it
takes more than 8 hours to do so on MapReduce.
This is because of the extra overhead of transfer-
ring ? from a master gateway machine to mappers,
from mappers to reducers, and from reducers back
to the master. Synchronous and asynchronous EM
suffer as well.
From Figure 4, we see that synchronous and
asynchronous stepwise EM converge at the same
rate when each is given 4 processors. The main
difference between this task and NER is the size
of the mini-batch used, so we experimented with
several values for the mini-batch size m. Fig-
ure 5 shows the results. As m decreases, a larger
fraction of time is spent updating parameters; this
slows observed convergence time even when us-
ing the sparse update rule. It can be seen that,
though synchronous and asynchronous stepwise
EM converge at the same rate with a large mini-
batch size (m = 10000), asynchronous stepwise
6The M-step could have been performed without MapRe-
duce by storing all the parameters in memory, but memory
restrictions on the gateway node of our cluster prevented this.
7For the reducer in the M-step, the source served as the
key, and the target appended by the parameter?s expected
count served as the value.
8Note that for wall time comparison, we sample models
every five minutes. The time taken to write these models
ranges from 30 seconds to a minute, thus artificially elon-
gating the total time for all iterations.
219
10 20 30 40 50 60 70 80
?40
?35
?30
?25
?20
Log
?Li
keli
hoo
d
 
 
Asynch. Stepwise EM (4 processors)
Synch. Stepwise EM (4 processors)
Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
10 20 30 40 50 60 70 80
?40
?35
?30
?25
?20
Wall clock time (minutes)
Log
?Li
keli
hoo
d
 
 
Asynch. Stepwise EM (MapReduce)
Synch. Stepwise EM (MapReduce)
Batch EM (MapReduce)
Figure 4: English?French log-likelihood vs. wall clock time
in minutes on both a single machine (top) and on a large
MapReduce cluster (bottom), shown on separate plots for
clarity, though axis scales are identical. We show runs of
each setting for the first 80 minutes, although EM was run
for 20 passes through the data in all cases (Table 2). Fastest
convergence is obtained by synchronous and asynchronous
stepwise EM using 4 processors on a single node. While the
algorithms converge more slowly on MapReduce due to over-
head, the asynchronous algorithm converges the fastest. We
observed similar trends for the French?English direction.
EM converges faster as m decreases. With large
mini-batches, load-balancing becomes less impor-
tant as there will be less variation in per-mini-
batch observed runtime. These results suggest that
asynchronous mini-batch algorithms will be most
useful for learning problems in which small mini-
batches work best. Fortunately, however, we do
not see any problems stemming from approxima-
tion errors due to the use of asynchronous updates.
6.3 Unsupervised POS Tagging
Our unsupervised POS experiments use the same
task and approach of Liang and Klein (2009) and
so we fix hyperparameters for stepwise EM based
on their findings (learning rate ?(t) = (t+2)?0.7).
The asynchronous algorithm uses the same learn-
ing rate formula as the single-processor algorithm.
There is only a single t that is maintained and gets
incremented whenever any thread updates the pa-
rameters. Liang and Klein used a mini-batch size
of 3, but we instead use a mini-batch size of 4 to
better suit our 4-processor synchronous and asyn-
chronous architectures.
Like NER, we present results for unsupervised
tagging experiments on a single machine only, i.e.,
not using a MapReduce cluster. For tasks like POS
tagging that have been shown to work best with
small mini-batches (Liang and Klein, 2009), we
10 20 30 40 50 60 70 80
?35
?30
?25
?20
Wall clock time (minutes)
Log
?L
ike
liho
od
 
 
Asynch. (m = 10,000)
Synch. (m = 10,000)
Asynch. (m = 1,000)
Synch. (m = 1,000)
Asynch. (m = 100)
Synch. (m = 100)
Figure 5: English?French log-likelihood vs. wall clock time
in minutes for stepwise EM with 4 processors for various
mini-batch sizes (m). The benefits of asynchronous updat-
ing increase as m decreases.
did not conduct experiments with MapReduce due
to high overhead per mini-batch.
For initialization, we followed Liang and Klein
by initializing each parameter as ?i ? e1+ai ,
ai ? Uniform([0, 1]). We generated 5 random
models using this procedure and used each to ini-
tialize each algorithm. We additionally used 2
random seeds for choosing the ordering of exam-
ples,9 resulting in a total of 10 runs for each al-
gorithm. We ran each for six hours, saving mod-
els every five minutes. After training completed,
using each model we decoded the entire training
data using posterior decoding and computed the
log-likelihood. The results for 5 initial models and
two example orderings are shown in Figure 6. We
evaluated tagging performance using many-to-1
accuracy, which is obtained by mapping the HMM
states to gold standard POS tags so as to maximize
accuracy, where multiple states can be mapped to
the same tag. This is the metric used by Liang and
Klein (2009) and Johnson (2007), who report fig-
ures comparable to ours. The asynchronous algo-
rithm converges much faster than the single-node
algorithm, allowing a tagger to be trained from
the Penn Treebank in less than two hours using
a single machine. Furthermore, the 4-processor
synchronous algorithm improves only marginally
9We ensured that the examples processed in the sequence
of mini-batches were identical for the 1-processor and 4-
processor versions of synchronous stepwise EM, but the
asynchronous algorithm requires a different seed for each
processor and, furthermore, the actual order of examples pro-
cessed depends on wall times and cannot be controlled for.
Nonetheless, we paired a distinct set of seeds for the asyn-
chronous algorithm with each of the two seeds used for the
synchronous algorithms.
220
0 1 2 3 4 5 6
?7.5
?7
?6.5
?6 x 10
6
Log
?Li
keli
hoo
d
0 1 2 3 4 5 6
50
55
60
65
Wall clock time (hours)
Acc
urac
y (%
)
 
 
Asynch. Stepwise EM (4 processors)Synch. Stepwise EM (4 processors)Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
0 1 2 3 4 5 6
?7.5
?7
?6.5
?6 x 10
6
Log
?Li
keli
hoo
d
0 1 2 3 4 5 6
50
55
60
65
Wall clock time (hours)
Acc
urac
y (%
)
 
 
Asynch. Stepwise EM (4 processors)Synch. Stepwise EM (4 processors)
Synch. Stepwise EM (1 processor)
Batch EM (1 processor)
Figure 6: POS: Asynchronous stepwise EM converges faster in log-likelihood and accuracy than the synchronous versions.
Curves are shown for each of 5 random initial models. One example ordering random seed is shown on the left, another on the
right. The accuracy curves for batch EM do not appear because the highest accuracy reached is only 40.7% after six hours.
over the 1-processor baseline.
The accuracy of the asynchronous curves of-
ten decreases slightly after peaking. We can sur-
mise from the log-likelihood plot that the drop
in accuracy is not due to the optimization be-
ing led astray, but probably rather due to the
complex relationship between likelihood and task-
specific evaluation metrics in unsupervised learn-
ing (Merialdo, 1994). In fact, when we exam-
ined the results of synchronous stepwise EM be-
tween 6 and 12 hours of execution, we found sim-
ilar drops in accuracy as likelihood continued to
improve. From Figure 6, we conclude that the
asynchronous algorithm has no harmful effect on
learned model?s accuracy beyond the choice to op-
timize log-likelihood.
While there are currently no theoretical conver-
gence results for asynchronous optimization algo-
rithms for non-convex functions, our results are
encouraging for the prospects of establishing con-
vergence results for this setting.
7 Discussion
Our best results were obtained by exploiting mul-
tiple processors on a single machine, while exper-
iments using a MapReduce cluster were plagued
by communication and framework overhead.
Since Moore?s Law predicts a continual in-
crease in the number of cores available on a sin-
gle machine but not necessarily an increase in the
speed of those cores, we believe that algorithms
that can effectively exploit multiple processors on
a single machine will be increasingly useful. Even
today, applications in NLP involving rich-feature
structured prediction, such as parsing and transla-
tion, typically use a large portion of memory for
storing pre-computed data structures, such as lex-
icons, feature name mappings, and feature caches.
Frequently these are large enough to prevent the
multiple cores on a single machine from being
used for multiple experiments, leaving some pro-
cessors unused. However, using multiple threads
in a single program allows these large data struc-
tures to be shared and allows the threads to make
use of the additional processors.
We found the overhead incurred by the MapRe-
duce programming model, as implemented in
Hadoop 0.20, to be substantial. Nonetheless,
we found that asynchronously running multiple
MapReduce calls at the same time, rather than
pooling all processors into a single MapReduce
call, improves observed convergence with negli-
gible effects on performance.
8 Conclusion
We have presented experiments using an asyn-
chronous framework for distributed mini-batch
optimization that show comparable performance
of trained models in significantly less time than
traditional techniques. Such algorithms keep pro-
cessors in constant use and relieve the programmer
from having to implement load-balancing schemes
for each new problem encountered. We expect
asynchronous learning algorithms to be broadly
applicable to training NLP models.
Acknowledgments The authors thank Qin Gao, Garth Gib-
son, Andre? Martins, Brendan O?Connor, Stephan Vogel, and
the reviewers for insightful comments. This work was sup-
ported by awards from IBM, Google, computing resources
from Yahoo, and NSF grants 0836431 and 0844507.
221
References
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
O. Cappe? and E. Moulines. 2009. Online EM algo-
rithm for latent data models. Journal of the Royal
Statistics Society: Series B (Statistical Methodol-
ogy), 71.
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
line large-margin training of syntactic and structural
translation features. In Proc. of EMNLP.
C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and
K. Olukotun. 2006. Map-Reduce for machine learn-
ing on multicore. In NIPS.
S. Clark and J.R. Curran. 2004. Log-linear models for
wide-coverage CCG parsing. In Proc. of EMNLP.
M. Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In Proc. of ACL.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. Dean and S. Ghemawat. 2004. MapReduce: Sim-
plified data processing on large clusters. In Sixth
Symposium on Operating System Design and Imple-
mentation.
C. Dyer, A. Cordova, A. Mont, and J. Lin. 2008. Fast,
easy, and cheap: Construction of statistical machine
translation models with MapReduce. In Proc. of the
Third Workshop on Statistical Machine Translation.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In Proc. of ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
J. Kazama and K. Torisawa. 2007. A new perceptron
algorithm for sequence labeling with non-local fea-
tures. In Proc. of EMNLP-CoNLL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. Langford, A. J. Smola, and M. Zinkevich. 2009.
Slow learners are fast. In NIPS.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278?2324.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL-HLT.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models.
In NIPS.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?172.
R. Mihalcea and T. Pedersen. 2003. An evaluation
exercise for word alignment. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond.
R. Neal and G. E. Hinton. 1998. A view of the EM al-
gorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models.
A. Nedic, D. P. Bertsekas, and V. S. Borkar. 2001.
Distributed asynchronous incremental subgradient
methods. In Proc. of the March 2000 Haifa Work-
shop: Inherently Parallel Algorithms in Feasibility
and Optimization and Their Applications.
N. Ratliff, J. Bagnell, and M. Zinkevich. 2006. Sub-
gradient methods for maximum margin structured
learning. In ICML Workshop on Learning in Struc-
tured Outputs Spaces.
M. Sato and S. Ishii. 2000. On-line EM algorithm for
the normalized Gaussian network. Neural Compu-
tation, 12(2).
B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin.
2005. Learning structured prediction models: A
large margin approach. In Proc. of ICML.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453?1484.
J. Wolfe, A. Haghighi, and D. Klein. 2008. Fully
distributed EM for very large datasets. In Proc. of
ICML.
222
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115?123,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Author Age Prediction from Text using Linear Regression
Dong Nguyen Noah A. Smith Carolyn P. Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dongn,nasmith,cprose}@cs.cmu.edu
Abstract
While the study of the connection between
discourse patterns and personal identification
is decades old, the study of these patterns us-
ing language technologies is relatively recent.
In that more recent tradition we frame author
age prediction from text as a regression prob-
lem. We explore the same task using three
very different genres of data simultaneously:
blogs, telephone conversations, and online fo-
rum posts. We employ a technique from do-
main adaptation that allows us to train a joint
model involving all three corpora together as
well as separately and analyze differences in
predictive features across joint and corpus-
specific aspects of the model. Effective fea-
tures include both stylistic ones (such as POS
patterns) as well as content oriented ones. Us-
ing a linear regression model based on shallow
text features, we obtain correlations up to 0.74
and mean absolute errors between 4.1 and 6.8
years.
1 Introduction
A major thrust of research in sociolinguistics is to
understand the connection between the way peo-
ple use language and their community membership,
where community membership can be construed
along a variety of dimensions, including age, gen-
der, socioeconomic status and political affiliation. A
person is a member of a multiplicity of communi-
ties, and thus the person?s identity and language are
influenced by many factors.
In this paper we focus on the relationship between
age and language use. Recently, machine learning
methods have been applied to determine the age of
persons based on the language that they utter. Stud-
ies of the stylistic and content-based features that
predict age or other personal characteristics yield
new insights into the connection between discourse
and identity. However, that connection is known to
be highly contextual, such as whether the data were
collected synchronously or asynchronously, through
typed or spoken interaction, or whether participants
can see one another or not. Recent work in the area
of domain adaptation raises awareness about the ef-
fect of contextual factors on the generality of text
prediction models.
Our first contribution to this literature is an in-
vestigation of age prediction using a multi-corpus
approach. We present results and analysis across
three very different corpora: a blog corpus (Schler
et al, 2006), a transcribed telephone speech corpus
(Cieri et al, 2004) and posts from an online forum
on breast cancer. By using the domain adaptation
approach of Daume? III (2007), we train a model on
all these corpora together and separate the global
features from corpus-specific features that are asso-
ciated with age.
A second contribution is the investigation of age
prediction with age modeled as a continuous vari-
able rather than as a categorical variable. Most
prior research on age prediction has framed this as a
two-class or three-class classification problem (e.g.,
Schler et al, 2006 and Garera and Yarowsky, 2009).
In our work, modeling age as a continuous variable
is interesting not only as a more realistic representa-
tion of age, but also for practical benefits of joint
modeling of age across corpora since the bound-
115
aries for discretizing age into a categorical variable
in prior work have been chosen heuristically and in
a corpus-dependent way, making it hard to compare
performance across different kinds of data.
In the remainder of the paper, we first discuss re-
lated work and present and compare the different
datasets. We then outline our approach and results.
We conclude with discussion and future work.
2 Related work
Time is an important factor in sociolinguistic analy-
sis of language variation. While a thorough review
of this work is beyond the scope of this paper, Eckert
(1996) gives an overview of the literature on age as
a sociolinguistic variable. Linguistic variation can
occur as an individual moves through life, or as a re-
sult of changes in the community itself as it moves
through time. As an added complexity, Argamon et
al. (2007) found connections between language vari-
ation and age and gender. Features that were used
with increasing age were also used more by males
for any age. Features that were used with decreas-
ing age were used more by females. In other work,
the same features that distinguish male and female
writing also distinguish non-fiction and fiction (Arg-
amon et al, 2003). Thus, the separate effects of age,
time period, gender, topic, and genre may be diffi-
cult to tease apart in naturalistic data where many of
these variables are unknown.
Recently, machine learning approaches have been
explored to estimate the age of an author or speaker
using text uttered or written by the person. This
has been modeled as a classification problem, in a
similar spirit to sociolinguistic work where age has
been investigated in terms of differences in distri-
butions of characteristics between cohorts. In the
sociolinguistic literature, cohorts such as these are
determined either etically (arbitrary, but equal age
spans such as decades) or emically (related to life
stage, such as adolescence etc.). In machine learn-
ing research, these cohorts have typically been deter-
mined for practical reasons relating to distribution of
age groups within a corpus, although the boundaries
sometimes have also made sense from a life stage
perspective. For example, researchers have mod-
eled age as a two-class classification problem with
boundaries at age 40 (Garera and Yarowsky, 2009)
or 30 (Rao et al, 2010). Another line of work has
looked at modeling age estimation as a three-class
classification problem (Schler et al, 2006; Goswami
et al, 2009), with age groups of 13-17, 23-27 and
33-42. In addition to machine learning experiments,
other researchers have published statistical analyses
of differences in distribution related to age and lan-
guage and have found similar patterns.
As an example of one of these studies, Pen-
nebaker and Stone (2003) analyzed the relationship
between language use and aging by collecting data
from a large number of previous studies. They
used LIWC (Pennebaker et al, 2001) for analysis.
They found that with increasing age, people tend to
use more positive and fewer negative affect words,
more future-tense and less past-tense, and fewer
self-references. Furthermore, a general pattern of
increasing cognitive complexity was seen. Barbieri
(2008) uses key word analysis to analyze language
and age. Two groups (15?25 and 35?60) were com-
pared. Analysis showed that younger speakers? talk
is characterized by slang and swear words, indica-
tors of speaker stance and emotional involvement,
while older people tend to use more modals.
Age classification experiments have been con-
ducted on a wide range of types of data, in-
cluding blogs (Schler et al, 2006; Goswami et
al., 2009), telephone conversations (Garera and
Yarowsky, 2009), and recently Twitter (Rao et al,
2010). Effective features were both content fea-
tures (such as unigrams, bigrams and word classes)
as well as stylistic features (such as part-of-speech,
slang words and average sentence length). These
separate published studies present some common-
alities of findings. However, based on these re-
sults from experiments conducted on very different
datasets, it is not possible to determine how gener-
alizable the models are. Thus, there is a need for an
investigation of generalizability specifically in the
modeling of linguistic variation related to age, which
we present in this paper.
Age classification from speech data has been of
interest for many years. Recently, age regression us-
ing speech features has been explored (Spiegl et al,
2009). Spiegel?s system obtained a mean absolute
error of approximately 10 years using support vec-
tor regression. Van Heerden et al (2010) explore
combining regression estimates to improve age clas-
116
sification. As far as we are aware, we are the first to
publish results from a regression model that directly
predicts age using textual features.
3 Data description
We explore three datasets with different characteris-
tics. The data was divided into a training, develop-
ment and test set. Statistics are listed in Table 1.
3.1 Blog corpus
In August 2004 Schler et al (2006) crawled blogs
from blogger.com. Information such as gen-
der and age were provided by the users in their re-
spective profiles. Users were divided into three age
groups, and each group had an equal number of fe-
male and male bloggers. In our experiments, ev-
ery document consists of all posts from a particular
blogger.
3.2 Fisher telephone corpus
The Fisher corpus (Cieri et al, 2004) contains tran-
scripts of telephone conversations. People were ran-
domly assigned to pairs, and for (almost) every per-
son, characteristics such as gender and age were
recorded. Furthermore, for each conversation a topic
was assigned. The data was collected beginning De-
cember 2002 and continued for nearly one year. In
our experiments, we aggregate the data for each per-
son.
3.3 Breast cancer forum
We drew data from one of the most active online fo-
rums for persons with breast cancer.1 All posts and
user profiles of the forum were crawled in January
2011. Only a small proportion of users had indicated
their age in their profile. We manually annotated the
age of approximately 200 additional users with less
common ages by looking manually at their posts. An
author?s age can often be annotated because users
tend to make references to their age when they intro-
duce themselves or when telling their treatment his-
tory (e.g., I was diagnosed 2 years ago when I was
just 38). Combining this with the date of the specific
post, a birth year can be estimated. Because a per-
son?s data can span multiple years, we aggregate all
the data per year for each person. Each person was
1http://community.breastcancer.org
Figure 1: Comparison of age frequency in datasets.
0
500
1000
1500
2000
2500
10 20 30 40 50 60 70 80 90
F
re
qu
en
cy
Age
Blogs
Fisher
Cancer
assigned randomly to one of the data splits, to make
sure all documents representing the same person ap-
peared in only one split. The dataset contains posts
from October 2002 until January 2011.
3.4 Dataset comparison and statistics
The datasets differ in several respects: specificity
(general topics versus breast cancer), modality of in-
teraction (telephone conversations versus online fo-
rum versus blog post), age distribution, and amount
of data per person. The blog and Fisher dataset con-
tain approximately equal amounts of males and fe-
males, while the breast cancer dataset is heavily bi-
ased towards women.
A comparison of the age distributions of the three
corpora is given in Figure 1. The Fisher dataset
has the most uniform distribution across the ages,
while the blog data has a lot of young persons and
the breast cancer forum has a lot of older people.
The youngest person in our dataset is 13 years old
and the oldest is 88. Note that our blog corpus con-
tains gaps between different age categories, which
is an artifact of the experimental approach used by
the people who released this dataset (Schler et al,
2006).
Because all datasets were created between 2002
and 2011, we are less likely to observe results due to
cohort effects (changes that occur because of collec-
tive changes in culture, such as use of the Internet).
117
Table 1: Datasets statistics.
Blogs Fisher Cancer
Data #docs avg #tokens #docs avg #tokens #docs avg #tokens #persons
Training 9,660 13,042 5,957 3,409 2,330 22,719 1,269
Development 4,830 13,672 2,977 3,385 747 32,239 360
Test 4,830 13,206 2,980 3,376 797 26,952 368
4 Experimental setup
4.1 Linear regression
Given an input vector x ? Rm, where x1, . . . , xm
represent features (also called independent variables
or predictors), we find a prediction y? ? R for the age
of a person y ? R using a linear regression model:
y? = ?0 + x>? where ?0 and ? are the parame-
ters to estimate. Usually, the parameters are learned
by minimizing the sum of squared errors. In order
to strive for a model with high explanatory value,
we use a linear regression model with Lasso (also
called L1) regularization (Tibshirani, 1996). This
minimizes the sum of squared errors, but in addition
adds a penalty term ?
?m
j=1 |?j |. ? is a constant and
can be found by optimizing over the development
data. As a result, this method delivers sparse mod-
els. We use OWLQN to optimize the regularized
empirical risk (Andrew and Gao, 2007; Gao et al,
2007). We evaluate the models by reporting the cor-
relation and mean absolute error (MAE).
4.2 Joint model
To discover which features are important across
datasets and which are corpus-specific, we train a
model on the data of all corpora using the feature
representation proposed by Daume? III (2007). Using
this model, the original feature space is augmented
by representing each individual feature as 4 new fea-
tures: a global feature and three corpus-specific fea-
tures, specifically one for each dataset. Thus for ev-
ery feature f , we now have fglobal , fblogs , ffisher and
fcancer . For every instance, only the global and the
one specific corpus feature are set. For example for
a particular feature value xj for the blog dataset we
would have ?xj , xj , 0, 0?. If it would appear in the
cancer dataset we would have ?xj , 0, 0, xj?. Because
the resulting model using L1 regression only selects
a small subset of the features, some features may
only appear either as global features or as corpus-
specific features in the final model.
4.3 Overview different models
Besides experimenting with the joint model, we are
also interested in the performance using only the dis-
covered global features. This can be achieved by ap-
plying the weights for the global features directly as
learned by the joint model, or retraining the model
on the individual datasets using only the global fea-
tures. In summary, we have the following models:
? INDIV: Models trained on the three corpora in-
dividually.
? JOINT: Model trained on all three corpora with
features represented as in Daume? III (2007).
? JOINT-Global: Using the learned JOINT
model but only keeping the global features.
? JOINT-Global-Retrained: Using the discov-
ered global features by the JOINT model, but
retrained on each specific dataset.
4.4 Features
4.4.1 Textual features
We explore the following textual features; all fea-
tures are frequency counts normalized by the length
(number of tokens) of the document.
? Unigrams.
? POS unigrams and bigrams. Text is tagged us-
ing the Stanford POS tagger (Toutanova et al,
2003).
? LIWC (Pennebaker et al, 2001). This is a word
counting program that captures word classes
such as inclusion words (LIWC-incl: ?with,?
?and,? ?include,? etc.), causation words (LIWC-
cause: ?because,? ?hence,? etc.), and stylis-
tic characteristics such as percentage of words
longer than 6 letters (LIWC-Sixltr).
118
Figure 2: Scatterplot of true and predicted age.
-20
-10
0
10
20
30
40
50
60
70
80
90
10 20 30 40 50 60 70 80 90
P
re
di
ct
ed
ag
e
True age
4.4.2 Gender
Because the gender of a person also influences
how age is reflected in a person?s text or speech (e.g.
Argamon et al (2007) ), we add a binary feature for
the gender of the person (Male = 1, Female = 0).
This feature is only known for the blog and Fisher
dataset. For the breast cancer dataset the gender is
not known, but we assume they are all women.
5 Results and discussion
As discussed, we experiment with four different
models. We explore three different feature sets: only
unigrams, only POS, and the full feature set. The re-
sults are presented in Table 2. The most important
features using the JOINT model with the full feature
set (condition 10) are presented in Table 3.
5.1 Quantitative analysis
Overall, similar performance is obtained on the
Fisher and blog datasets. The highest correlations
were achieved on the Fisher dataset, with a best cor-
relation of r = 0.742. This gives an r2 value of
0.551, indicating that 55% of the variance can be
explained by the model. However, a higher mean
absolute error (MAE) was observed compared to
the blog dataset. This may be caused by the larger
spread in distribution of ages in the Fisher dataset.
The lowest correlations were observed on the cancer
dataset. This is probably caused by the small amount
of training instances, the noisy text, and the fact that
the ages lie very close to each other.
Overall, the joint model using all features per-
formed best (condition 10). In Figure 2 a plot is
presented that relates the true and predicted ages for
this condition. We find that for the high ages there
are more instances with high errors, probably caused
by the small amount of training data for the extreme
ages.
We find the correlation metric to be very sensitive
to the amount of data. For example, when comput-
ing the correlation over the aggregated results of all
corpora, we get a much higher correlation (0.830),
but the MAE (5.345) is closer to that computed over
the individual datasets. However, the MAE is de-
pendent on the age distributions in the corpus, which
can be observed by contrasting the MAE on the runs
of the Fisher and cancer dataset. This thus suggests
that these two measures are complementary and both
are useful as evaluation metrics for this task.
For most experiments the joint models show im-
provement over the individual models. Returning
to our question of generality, we can make several
observations. First, performance decreases signif-
icantly when only using the global features (com-
paring JOINT and JOINT-Global-retrained), con-
firming that corpus-specific features are important.
Second, learned weights of global features are rea-
sonably generalizable. When using the full feature
set, retraining the global features on the corpora di-
rectly only gives a slight improvement (e.g. com-
pare conditions 11 and 12). Third, the bias term
(?0) is very corpus-specific and has a big influence
on the MAE. For example, when comparing condi-
tions 11 and 12, the correlations are very similar but
the MAEs are much lower when the model is re-
trained. This is a result of adjusting the bias term
to the specific dataset. For example the bias term of
the model trained on only the blog dataset is 22.45,
compared to the bias of 46.11 when trained on the
cancer dataset.
In addition, we observe better performance in the
cancer dataset when retraining the model using only
the global features compared to the initial feature
set. This suggests that using the global features
might have been an effective method for feature se-
lection to prevent overfitting on this small dataset.
119
Table 2: Results on the test set, reported with Pearson?s correlation (r) and mean absolute error (MAE).
Blogs Fisher Cancer
ID Model #Features r MAE r MAE r MAE
Unigrams
1 INDIV 56,440 0.644 4.236 0.715 7.145 0.426 7.085
2 JOINT 56,440 0.694 4.232 0.723 7.066 0.530 6.537
3 JOINT-Global 656 0.605 5.800 0.628 10.370 0.461 16.632
4 JOINT-Global-retrained 656 0.658 4.409 0.675 7.529 0.498 6.797
POS
5 INDIV 4,656 0.519 5.095 0.553 8.635 0.150 7.699
6 JOINT 4,656 0.563 4.899 0.549 8.657 0.035 8.449
7 JOINT-Global 110 0.495 6.332 0.390 12.232 0.151 19.454
8 JOINT-Global-retrained 110 0.519 5.095 0.475 9.187 0.150 7.699
All features
9 INDIV 61,416 0.699 4.144 0.731 6.926 0.462 6.943
10 JOINT 61,416 0.696 4.227 0.742 6.835 0.535 6.545
11 JOINT-Global 510 0.625 5.295 0.650 11.982 0.459 17.472
12 JOINT-Global-retrained 510 0.629 4.633 0.651 7.862 0.490 6.876
5.2 Feature analysis
The most important features using the JOINT model
with the full feature set (condition 10) are presented
in Table 3. Features associated with a young age
have a negative weight, while features associated
with old age have a positive weight. For almost all
runs and evaluation metrics the full feature set gives
the best performance. However, looking at the per-
formance increase, we observe that the unigram only
baseline gives strong results. Overall, both stylistic
as well as content features are important. For con-
tent features, we see that references to family (e.g.,
?granddaughter? versus ?son?) as well as to daily
life (e.g., ?school? versus ?job?) are very predictive.
Although the performance using only POS tags
is lower, reasonable correlations are obtained using
only POS tags. In Table 3 we see many POS features
associated with old age. This is confirmed when an-
alyzing the whole feature set selected by the JOINT
model (condition 10). In this model 510 features are
nonzero, 161 of which are POS patterns. Of these,
43 have a negative weight, and 118 have a positive
weight. This thus again suggests that old age is char-
acterized more by syntactic effects than young age.
Most important features are consistent with obser-
vations from previous research. For example, in the
Fisher dataset, similar to findings from classification
experiments by Garera and Yarowsky (2009), the
word ?well? is most predictive of older age. ?Like?
has the highest association with younger age. This
agrees with observations by Barbieri (2008). As
was also observed by others, ?just? is highly associ-
ated with young persons. Consistent with literature
that males generally ?sound older? than they truly
are (Argamon et al, 2007, and others), our male
speaker feature has a high negative weight. And, in
agreement with previous observations, younger peo-
ple use more swear words and negative emotions.
The differences between the corpora are reflected
in the features that have the most weight. The effec-
tive features in the Fisher dataset are more typical
of conversational settings and effective features in
the cancer dataset are about being pregnant and hav-
ing kids. Features associated with the blog dataset
are typical of the story telling nature of many blog
posts.
Comparing the extracted corpus-specific features
with the features selected when training on the indi-
vidual corpora, we do see evidence that the JOINT
model separates general versus specific features.
For example, the most important features associ-
ated with young people in the cancer dataset when
only training on the cancer dataset (condition 9)
are: LIWC - Emoticons, LIWC - Pronoun, definitely,
120
Table 3: Most important features in the JOINT model with all features (condition 10).
(a) Features for younger people.
Global Blogs Fisher Cancer
like -1.295 you -0.387 actually -0.457 LIWC-Emotic. -0.188
gender-male -0.539 went -0.310 mean -0.343 young -0.116
LIWC-School -0.442 fun -0.216 everyone -0.273 history -0.092
just -0.354 school -0.192 definitely -0.273 mom -0.087
LIWC-Anger -0.303 but -0.189 mom -0.230 ultrasound -0.083
LIWC-Cause -0.290 LIWC-Comma -0.152 student -0.182 kids -0.071
mom -0.290 go -0.142 pretty -0.137 age -0.069
so -0.271 POS-vbp nn -0.116 POS-lrb cd -0.135 mum -0.069
definitely -0.263 thats -0.115 LIWC-Swear -0.134 POS-sym rrb -0.069
LIWC-Negemo -0.256 well -0.112 huge -0.126 discharge -0.063
(b) Features for older people.
Global Blogs Fisher Cancer
years 0.601 LIWC - Job 0.514 well 1.644 POS - dt 0.713
POS - dt 0.485 son 0.267 LIWC - WC 0.855 POS - md vb 0.450
LIWC - Incl 0.483 kids 0.228 POS - uh prp 0.504 POS - nn 0.369
POS - prp vbp 0.337 years 0.178 retired 0.492 LIWC - Negate 0.327
granddaughter 0.332 work 0.147 POS - prp vbp 0.430 POS - nn vbd 0.321
grandchildren 0.293 wife 0.142 said 0.404 POS - nnp 0.304
had 0.277 husband 0.137 POS - cc fw 0.358 us 0.287
daughter 0.272 meds 0.112 son 0.353 all 0.266
grandson 0.245 dealing 0.096 subject 0.319 good 0.248
ah 0.243 weekend 0.094 POS - cc cc 0.316 POS - cc nn 0.222
mom, mum, really, LIWC - Family, LIWC - Humans,
thank, and she. The difference in age distribution is
reflected in the feature weights. In the JOINT model,
the bias term is 24.866. Because most of the persons
in the cancer dataset are older, the features associ-
ated with young age in the cancer dataset have much
lower weights compared to the other datasets.
Because our goal is to compare features across
the corpora, we have not exploited corpus-specific
features. For example, thread or subforum features
could be used for the breast cancer corpus, and for
the Fisher dataset, one could add features that ex-
ploit the conversational setting of the data.
5.3 Examples
We present examples of text of younger and older
persons and connect them to the learned model.
The examples are manually selected to illustrate
strengths and weaknesses of the model.
5.3.1 Younger people
We first present some examples of text by young
persons. The following is an example of a 17-year
old in the blog dataset, the system predicted this to
be from a 16.48-year-old:
I can?t sleep, but this time I have school
tommorow, so I have to try I guess. My
parents got all pissed at me today because
I forgot how to do the homework [...]. Re-
ally mad, I ended it pissing off my mom
and [...] NOTHING! Damn, when I?m at
my cousin?s I have no urge to use the com-
puter like I do here, [...].
This example matches with important features de-
termined by the system, containing references to
school and parents, and usage of swearing and anger
words.
121
The following are selected turns (T) by a 19-year
old (system prediction: 17.37 years) in a conversa-
tion in the Fisher dataset.
T: yeah it?s too i just just freaked out [...]
T: that kinda sucks for them
T: they were they were like going crazy
[...]
T: it?s like against some law to like
The text has many informal words such as ?kinda?
and well as many occurrences of the word ?like.?
This example is from a 19-year old from the can-
cer dataset. The system?s prediction was far off, es-
timating an age of 35.48.
Im very young and an athlete and I really
do not want to look disfigured, especially
when I work so hard to be fit. I know it
sounds shallow, but Im young and hope
to [...] my husband one day :) [...] My
grandmother died of breast cancer at 51,
and my mother is currently dealing with a
cancerous tumor on her ovaries.
Besides explicit references to being ?very young,?
the text is much more formal than typical texts, mak-
ing it a hard example.
5.3.2 Older people
The following is a snippet from a 47-year-old
(system prediction: 34.42 years) in the blog dataset.
[...]In the weeks leading up to this meet-
ing certain of the managers repeatedly as-
serted strong positions. [...] their previous
(irresponsible yet non-negotiable) opin-
ions[...] Well, today?s my first Father?s
day [...]. Bringing a child into this world
is quite a responsibility especially with all
the fears and challenges we face. [...]
This matches some important features such as ref-
erences to jobs, as well as having kids. The many
references to the word ?father? in the whole text
might have confused the model. The following are
selected turns (T) by a 73-year old (system predic-
tion: 73.26 years) in a conversation in the Fisher
dataset.
T: ah thoughts i?m retired right now
T: i i really can?t ah think of anyth- think
of i would ah ah change considerably ah
i?m i?m very i?ve been very happily mar-
ried and i have ah three children and six
grandchildren
T: yeah that?s right well i i think i would do
things more differently fair- fairly recently
than a long time ago
This example contains references to being retired
and having grandchildren, as well as many usages
of ?ah?. The following is an example of a 70-year
old (system prediction: 71.53 years) in the cancer
dataset.
[...] I was a little bit fearful of having
surgery on both sides at once (reduction
and lift on the right, tissue expander on
the left) [...] On the good side, my son
and family live near the plastic surgeon?s
office and the hospital, [...], at least from
my son and my granddaughter [...]
6 Conclusion
We presented linear regression experiments to pre-
dict the age of a text?s author. As evaluation metrics,
we found correlation as well as mean absolute er-
ror to be complementary and useful measures. We
obtained correlations up to 0.74 and mean absolute
errors between 4.1 and 6.8 years. In three different
corpora, we found both content features and stylis-
tic features to be strong indicators of a person?s age.
Even a unigram only baseline already gives strong
performance and many POS patterns are strong in-
dicators of old age. By learning jointly from all of
the corpora, we were able to separate generally ef-
fective features from corpus-dependent ones.
Acknowledgments
The authors would like to thank the anonymous review-
ers for feedback, Michael Heilman for the regression
code, and other members of the ARK group for help run-
ning the experiments. This work was funded by NSF
grants CAREER IIS-1054319 to N.A.S. and IIS-0968485
to C.P.R.
122
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proc. of
ICML.
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat R. Shimoni. 2003. Gender, genre, and writing
style in formal written texts. Text, 23(3):321?346.
Shlomo Argamon, Moshe Koppel, James Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
age, gender, and the varieties of self-expression.
Federica Barbieri. 2008. Patterns of age-based linguistic
variation in American English. Journal of Sociolin-
guistics, 12(1):58?88.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proc. of LREC, pages
69?71.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proc. of ACL.
Penelope Eckert. 1996. Age as a sociolinguistic variable.
In The Handbook of Sociolinguistics. Oxford: Black-
well.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proc. of ACL.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proc. of ACL-IJCNLP.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers? age and gen-
der. In Proc. of ICWSM.
James W. Pennebaker and Lori D. Stone. 2003. Words
of wisdom: Language use over the lifespan. Journal
of Personality and Social Psychology, 85:291?301.
James W. Pennebaker, Roger J. Booth, and Martha E.
Francis, 2001. Linguistic Inquiry and Word Count
(LIWC): A Computerized Text Analysis Program.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. of SMUC.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James Pennebaker. 2006. Effects of age and gender
on blogging. In Proceedings of the AAAI Spring Sym-
posia on Computational Approaches to Analyzing We-
blogs.
Werner Spiegl, Georg Stemmer, Eva Lasarcyk, Varada
Kolhatkar, Andrew Cassidy, Blaise Potard, Stephen
Shum, Young Chol Song, Puyang Xu, Peter Beyer-
lein, James Harnsberger, and Elmar No?th. 2009. Ana-
lyzing features for automatic age estimation on cross-
sectional data. In Proc. of INTERSPEECH.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society Series B (Methodological), 58(1):267?288.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
NAACL-HLT.
Charl van Heerden, Etienne Barnard, Marelie Davel,
Christiaan van der Walt, Ewald van Dyk, Michael
Feld, and Christian Muller. 2010. Combining re-
gression and classification methods for improving au-
tomatic speaker age recognition. In Proc. of ICASSP.
123
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337?343,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The CMU-ARK German-English Translation System
Chris Dyer Kevin Gimpel Jonathan H. Clark Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{cdyer,kgimpel,jhclark,nasmith}@cs.cmu.edu
Abstract
This paper describes the German-English
translation system developed by the ARK re-
search group at Carnegie Mellon University
for the Sixth Workshop on Machine Trans-
lation (WMT11). We present the results of
several modeling and training improvements
to our core hierarchical phrase-based trans-
lation system, including: feature engineering
to improve modeling of the derivation struc-
ture of translations; better handing of OOVs;
and using development set translations into
other languages to create additional pseudo-
references for training.
1 Introduction
We describe the German-English translation system
submitted to the shared translation task in the Sixth
Workshop on Machine Translation (WMT11) by the
ARK research group at Carnegie Mellon Univer-
sity.1 The core translation system is a hierarchical
phrase-based machine translation system (Chiang,
2007) that has been extended in several ways de-
scribed in this paper.
Some of our innovations focus on modeling.
Since German and English word orders can diverge
considerably, particularly in non-matrix clauses,
we focused on feature engineering to improve the
modeling of long-distance relationships, which are
poorly captured in standard hierarchical phrase-
based translation models. To do so, we devel-
oped features that assess the goodness of the source
1http://www.ark.cs.cmu.edu
language parse tree under the translation grammar
(rather than of a ?linguistic? grammar). To train the
feature weights, we made use of a novel two-phase
training algorithm that incorporates a probabilistic
training objective and standard minimum error train-
ing (Och, 2003). These segmentation features were
supplemented with a 7-gram class-based language
model, which more directly models long-distance
relationships. Together, these features provide a
modest improvement over the baseline and suggest
interesting directions for future work. While our
work on parse modeling was involved and required
substantial changes to the training pipeline, some
other modeling enhancements were quite simple: for
example, improving how out-of-vocabulary words
are handled. We propose a very simple change, and
show that it provides a small, consistent gain.
On the training side, we had two improvements
over our baseline system. First, we were inspired
by the work of Madnani (2010), who showed that
when training to optimize BLEU (Papineni et al,
2002), overfitting is reduced by supplementing a sin-
gle human-generated reference translation with ad-
ditional computer-generated references. We gener-
ated supplementary pseudo-references for our de-
velopment set (which is translated into many lan-
guages, but once) by using MT output from a sec-
ondary Spanish-English translation system. Second,
following Foster and Kuhn (2009), we used a sec-
ondary development set to select from among many
optimization runs, which further improved general-
ization.
We largely sought techniques that did not require
language-specific resources (e.g., treebanks, POS
337
annotations, morphological analyzers). An excep-
tion is a compound segmentation model used for
preprocessing that was trained on a corpus of man-
ually segmented German. Aside from this, no fur-
ther manually annotated data was used, and we sus-
pect many of the improvements described here can
be had in other language pairs. Despite avoiding
language-specific resources and using only the train-
ing data provided by the workshop, an extensive
manual evaluation determined that the outputs pro-
duced were of significantly higher quality than both
statistical and rule-based systems that made use of
language-specific resources (Callison-Burch et al,
2011).
2 Baseline system and data
Our translation system is based on a hierarchical
phrase-based translation model (Chiang, 2007), as
implemented in the cdec decoder (Dyer et al,
2010). Since German is a language that makes
productive use of ?closed? compounds (compound
words written as a single orthographic token), we
use a CRF segmentation model of to evaluate the
probability of all possible segmentations, encoding
the most probable ones compactly in a lattice (Dyer,
2009). For the purposes of grammar induction, the
single most probable segmentation of each word in
the source side of the parallel training data under the
model was inferred.
The parallel data were aligned using the
Giza++ implementation of IBM Model 4 run
in both directions and then symmetrized using
the grow-diag-final-and heuristic (Och and
Ney, 2002; Brown et al, 1993; Koehn et al, 2003).
The aligned corpus was encoded as a suffix array
(Lopez, 2008) and lattice-specific grammars (con-
taining just the rules that are capable of matching
spans in the input lattice) were extracted for each
sentence in the test and development sets, using the
heuristics recommended by Chiang (2007).
A 4-gram modified Kneser-Ney language model
(Chen and Goodman, 1996) was constructed using
the SRI language modeling toolkit (Stolcke, 2002)
from the English side of the parallel text, the mono-
lingual English data, and the English version 4 Giga-
word corpus (Parker et al, 2009). Since there were
many duplicate segments in the training data (much
of which was crawled from the web), duplicate seg-
ments and segments longer than 100 words were re-
moved. Inference was carried out using the language
modeling library described by Heafield (2011).
The newstest-2009 set (with the 500 longest
segments removed) was used for development,2 and
newstest-2010 was used as a development test
set. Results in this paper are reported on the dev-
test set using uncased BLEU4 with a single refer-
ence translation. Minimum error rate training (Och,
2003) was used to optimize the parameters of the
system to maximize BLEU on the development data,
and inference was performed over a pruned hyper-
graph representation of the translation hypothesis
space (Kumar et al, 2009).
For the experiments reported in this paper, Viterbi
(max-derivation) decoding was used. The system
submitted for manual evaluation used segment-level
MBR decoding with 1 ? BLEU as the loss function,
approximated over a 500-best list for each sentence.
This reliably results in a small but consistent im-
provement in translation quality, but is much more
time consuming to compute (Kumar and Byrne,
2004).
3 Source parse structure modeling
Improving phrase-based translation systems is chal-
lenging in part because our intuitions about what
makes a ?good? phrase or translation derivation are
often poor. For example, restricting phrases and
rules to be consistent with syntactic constituents
consistently harms performance (Chiang, 2007; Gal-
ley et al, 2006; Koehn et al, 2003), although our
intuitions might suggest this is a reasonable thing
to do. On the other hand, it has been shown that
incorporating syntactic information in the form of
features can lead to improved performance (Chiang,
2010; Gimpel and Smith, 2009; Marton and Resnik,
2008). Syntactic features that are computed by as-
sessing the overlap of the translation parse with a
linguistic parse can be understood to improve trans-
lation because they lead to a better model of what a
?correct? parse of the source sentence is under the
translation grammar.
Like the ?soft syntactic features? used in pre-
2Removing long segments substantially reduces training
time and does not appear to negatively affect performance.
338
vious work (Marton and Resnik, 2008; Chiang et
al., 2008), we propose features to assess the tree
structure induced during translation. However, un-
like that work, we do not rely on linguistic source
parses, but instead only make use of features that
are directly computable from the source sentence
and the parse structure being considered in the de-
coder. In particular, we take inspiration from the
model of Klein and Manning (2002), which mod-
els constituency in terms of the contexts that rule
productions occur in. Additionally, we make use of
salient aspects of the spans being dominated by a
nonterminal, such as the words at the beginning and
end of the span, and the length of the span. Impor-
tantly, the features do not rely on the target words
being predicted, but only look at the structure of the
translation derivation. As such, they can be under-
stood as monolingual parse features.3
Table 1 lists the feature templates that were used.
Template Description
CTX:fi?1, fj context bigram
CTX:fi?1, fj , x context bigram + NT
CTX:fi?1, fj , x, (j ? i) context bigram + NT + len
LU:fi?1 left unigram
LB:fi?1, fi left bigram (overlapping)
RU:fj right unigram
RB:fj?1, fj right bigram (overlapping)
Table 1: Context feature templates for features extracted
from every translation rule used; i and j indicate hypothe-
sized constituent span, x is its nonterminal category label
(in our grammar, X or S), and fk is the kth word of the
source sentence, with f<1 = ?s? and f>|f| = ?/s?. If a
word fk is not among the 1000 most frequent words in
the training corpus, it is replaced by a special unknown
token. The SMALLCAPS prefixes prevent accidental fea-
ture collisions.
3.1 Two-phase discriminative learning
The parse features just introduced are numerous and
sparse, which means that MERT can not be used
to infer their weights. Instead, we require a learn-
ing algorithm that can cope with millions of fea-
tures and avoid overfitting, perhaps by eliminating
most of the features and keeping only the most valu-
able (which would also keep the model compact).
3Similar features have been proposed for use in discrimina-
tive monolingual parsing models (Taskar et al, 2004).
Furthermore, we would like to be able to still tar-
get the BLEU measure of translation quality during
learning. While large-scale discriminative training
for machine translation is a widely studied problem
(Hopkins and May, 2011; Li and Eisner, 2009; De-
vlin, 2009; Blunsom et al, 2008; Watanabe et al,
2007; Arun and Koehn, 2007; Liang et al, 2006), no
tractable algorithm exists for learning a large num-
ber of feature weights while directly optimizing a
corpus-level metric like BLEU. Rather than resorting
to a decomposable approximation, we have explored
a new two-phase training algorithm in development
of this system.
The two-phase algorithm works as follows. In
phase 1, we use a non-BLEU objective to train a
translation model that includes the large feature set.
Then, we use this model to compute a small num-
ber of coarse ?summary features,? which summa-
rize the ?opinion? of the first model about a trans-
lation hypothesis in a low dimensional space. Then,
in the second training pass, MERT is used to deter-
mine how much weight to give these summary fea-
tures together with the other standard coarse trans-
lation features. At test time, translation becomes a
multi-step process as well. The hypothesis space is
first scored using the phase-1 model, then summary
features are computed, then the hypothesis space is
rescored with the phase-2 model. As long as the fea-
tures used factor with the edges in the translation
space (which ours do), this can be carried out in lin-
ear time in the size of the translation forest.
3.1.1 Phase 1 training
For the first model, which includes the sparse parse
features, we learn weights in order to optimize pe-
nalized conditional log likelihood (Blunsom et al,
2008). We are specifically interested in modeling
an unobserved variable (i.e., the parse tree underly-
ing a translation derivation), this objective is quite
natural, since probabilistic models offer a principled
account of unobserved data. Furthermore, because
our features factor according to edges in the trans-
lation forest (they are ?stateless? in standard MT
terminology), there are efficient dynamic program-
ming algorithms that can be used to exactly compute
the expected values of the features (Lari and Young,
1990), which are necessary for computing the gradi-
ents used in optimization.
339
We are therefore optimizing the following objec-
tive, given a set T of parallel training sentences:
L = ?R(?)?
?
?f,e??T
log
?
d
p?(e,d | f)
where p?(e,d | f) =
exp ?>h(f, e,d)
Z(f)
,
where d is a variable representing the unobserved
synchronous parses giving rise to the pair of sen-
tences ?f, e?, and where R(?) is a penalty that favors
less complex models. Since we not only want to pre-
vent over fitting but also want a small model, we use
R(?) =
?
k |?k|, the `1 norm, which forces many
parameters to be exactly 0.
Although L is not convex in ? (on account of the
latent derivation variable), we make use of an on-
line stochastic gradient descent algorithm that im-
poses an `1 penalty on the objective (Tsuruoka et
al., 2009). Online algorithms are often effective for
non-convex objectives (Liang and Klein, 2009).
We selected 12,500 sentences randomly from the
news-commentary portion of the training data to use
to train the latent variable model. Using the stan-
dard rule extraction heuristics (Chiang, 2007), 9,967
of the sentence pairs could be derived.4 In addition
to the parse features describe above, the standard
phrase features (relative frequency and lexical trans-
lation probabilities), and a rule count feature were
included. Training was run for 48 hours on a sin-
gle machine, which resulted in 8 passes through the
training data, instantiating over 8M unique features.
The regularization strength ? was chosen so that ap-
proximately 10, 000 (of the 8M) features would be
non-zero.5
3.1.2 Summary features
As outlined above, the phase 1 model will be incor-
porated into the final translation model using a low
dimensional ?summary? of its opinion. Because we
are using a probabilistic model, posterior probabili-
ties (given the source sentence f) under the parsing
4When optimizing conditional log likeligood, it is necessary
to be able to exactly derive the training pair. See Blunsom et al
(2008) for more information.
5Ideally, ? would have been tuned to optimize held-out like-
lihood or BLEU; however, the evaluation deadline prevented us
from doing this.
model are easily defined and straightforward to com-
pute with dynamic programming. We made use of
four summary features: the posterior log probability
log p?(e,d|f); for every rule r ? d, the probability of
its span being a constituent under the parse model;
the probabilities that some span starts at the r?s start-
ing index, or that some rule ends at r?s ending index.
Once these summary features have been com-
puted, the sparse features are discarded, and the
summary features are reweighted using coefficients
learned by MERT, together with the standard MT
features (language model, word penalty, etc.). This
provides a small improvement over our already very
strong baseline, as the first two rows in Table 2 show.
Condition BLEU
baseline 25.0
+ parse features 25.2
+ parse features + 7-gram LM 25.4
Table 2: Additional features designed to improve model
of long-range reordering.
3.2 7-gram class-based LM
The parsing features above were intended to im-
prove long range reordering quality. To further sup-
port the modeling of larger spans, we incorporated
a 7-gram class-based language model. Automatic
word clusters are attractive because they can be
learned for any language without supervised data,
and, unlike part-of-speech annotations, each word
is in only a single class, which simplifies inference.
We performed Brown clustering (Brown et al, 1992)
on 900k sentences from our language modeling data
(including the news commentary corpus and a sub-
set of Gigaword). We obtained 1,000 clusters us-
ing an implementation provided by Liang (2005),6
as Turian et al (2010) found that relatively large
numbers clusters gave better performance for infor-
mation extraction tasks. We then replaced words
with their clusters in our language modeling data
and built a 7-gram LM with Witten-Bell smoothing
(Witten and Bell, 1991).7 The last two rows of Ta-
6http://www.cs.berkeley.edu/?pliang/
software
7The distributional assumptions made by the more com-
monly used Kneser-Ney estimator do not hold in the word-
340
ble 2 shows that in conjunction with the source parse
features, a slight improvement comes from includ-
ing the 7-gram LM.
4 Non-translating tokens
When two languages share a common alphabet (as
German and English largely do), it is often appro-
priate to leave some tokens untranslated when trans-
lating. Named entities, numbers, and graphical el-
ements such as emoticons are a few common ex-
amples of such ?non-translating? elements. To en-
sure that such elements are well-modeled, we aug-
ment our translation grammar so that every token
in the input can translate as itself and add a feature
that counts the number of times such self-translation
rules are used in a translation hypothesis. This is in
contrast to the behavior of most other decoders, such
as Moses, which only permit a token to translate as
itself if it is learned from the training data, or if there
is no translation in the phrase table at all.
Since many non-translating tokens are out-of-
vocabulary (OOV) in the target LM, we also add
a feature that fires each time the LM encounters a
word that is OOV.8 This behavior be understood as
discriminatively learning the unknown word penalty
that is part of the LM. Again, this is in contrast to
the behavior of other decoders, which typically add
a fixed (and very large) cost to the LM feature for
every OOV. Our multi-feature parameterization per-
mits the training algorithm to decide that, e.g., some
OOVs are acceptable if they occur in a ?good? con-
text rather than forcing the decoder to avoid them
at all costs. Table 3 shows that always providing
a non-translating translation option together with a
discriminative learned OOV feature improves the
quality of German-English translation.9
Condition BLEU
?OOV (baseline) 24.6
+OOV and non-translating rules 25.0
Table 3: Effect of discriminatively learned penalties for
OOV words.
classified corpus.
8When multiple LMs are used, there is an extra OOV feature
for each LM.
9Both systems were trained using the human+ES-EN refer-
ence set described below (?5).
5 Computer-generated references
Madnani (2010) shows that models learned by op-
timizing BLEU are liable to overfit if only a sin-
gle reference is used, but that this overfitting can
be mitigated by supplementing the single reference
with supplemental computer-generated references
produced by paraphrasing the human reference us-
ing a whole-sentence statistical paraphrase system.
These computer-generated paraphrases are just used
to compute ?better? BLEU scores, but not directly as
examples of target translations.
Although we did not have access to a paraphrase
generator, we took advantage of the fact that our de-
velopment set (newstest-2009) was translated
into several languages other than English. By trans-
lating these back into English, we hypothesized we
would get suitable pseudo-references that could be
used in place of computer-generated paraphrases.
Table 4 shows the results obtained on our held-out
test set simply by altering the reference translations
used to score the development data. These systems
all contain the OOV features described above.
Condition BLEU
1 human 24.7
1 human + ES-EN 25.0
1 human + FR-EN 24.0
1 human + ES-EN + FR-EN 24.2
Table 4: Effect of different sets of reference translations
used during tuning.
While the effect is somewhat smaller than Mad-
nani (2010) reports using a sentential paraphraser,
the extremely simple technique of adding the output
of a Spanish-English (ES-EN) system was found to
consistently improve the quality of the translations
of the held-out data. However, a comparable effect
was not found when using references generated from
a French-English (FR-EN) translation system, indi-
cating that the utility of this technique must be as-
sessed empirically and depends on several factors.
6 Case restoration
Our translation system generates lowercased out-
put, so we must restore case as a post-processing
step. We do so using a probabilistic transducer as
implemented in SRILM?s disambig tool. Each
341
lowercase token in the input can be mapped to a
cased variant that was observed in the target lan-
guage training data. Ambiguities are resolved us-
ing a language model that predicts true-cased sen-
tences.10 We used the same data sources to con-
struct this model as were used above. During devel-
opment, it was observed that many named entities
that did not require translation required some case
change, from simple uppercasing of the first letter,
to more idiosyncratic casings (e.g., iPod). To ensure
that these were properly restored, even when they
did not occur in the target language training data, we
supplement the true-cased LM training data and case
transducer training data with the German source test
set.
Condition BLEU (Cased)
English-only 24.1
English+test-set 24.3
Table 5: Effect of supplementing recasing model training
data with the test set source.
7 Model selection
Minimum error rate training (Och, 2003) is a
stochastic optimization algorithm that typically finds
a different weight vector each time it is run. Foster
and Kuhn (2009) showed that while the variance on
the development set objective may be narrow, the
held-out test set variance is typically much greater,
but that a secondary development set can be used to
select a system that will have better generalization.
We therefore replicated MERT 6 times and selected
the output that performed best on NEWSTEST-2010.
Since we had no additional blind test set, we can-
not measure what the impact is. However, the BLEU
scores we selected on varied from 25.4 to 26.1.
8 Summary
We have presented a summary of the enhancements
made to a hierarchical phrase-based translation sys-
tem for the WMT11 shared translation task. Some
of our results are still preliminary (the source parse
10The model used is p(y | x)p(y). While this model is some-
what unusual (the conditional probability is backwards from a
noisy channel model), it is a standard and effective technique
for case restoration.
model), but a number of changes we made were
quite simple (OOV handling, using MT output to
provide additional references for training) and also
led to improved results.
Acknowledgments
This research was supported in part by the NSF through
grant IIS-0844507, the U. S. Army Research Laboratory
and the U. S. Army Research Office under contract/grant
number W911NF-10-1-0533, and Sandia National Labo-
ratories (fellowship to K. Gimpel). We thank the anony-
mous reviewers for their thorough feedback.
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18:467?479.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of the Sixth Workshop
on Statistical Machine Translation.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL, pages 310?318.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. EMNLP, pages 224?233.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
D. Chiang. 2010. Learning to translate with source and
target syntax. In Proc. of ACL, pages 1443?1452.
J. Devlin. 2009. Lexical features for statistical machine
translation. Master?s thesis, University of Maryland.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
342
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of ACL, pages 961?968.
K. Gimpel and N. A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP, pages 219?228.
K. Heafield. 2011. KenLM: Faster and smaller language
model queries. In Proc. of the Sixth Workshop on Sta-
tistical Machine Translation.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of ACL, pages 128?135.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. of EMNLP,
pages 40?51.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
N. Madnani. 2010. The Circle of Meaning: From Trans-
lation to Paraphrasing and Back. Ph.D. thesis, De-
partment of Computer Science, University of Mary-
land College Park.
Y. Marton and P. Resnik. 2008. Soft syntactic constraints
for hierarchical phrased-based translation. In Proc. of
ACL, pages 1003?1011, Columbus, Ohio.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2009. English gigaword fourth edition.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Intl. Conf. on Spoken Language Pro-
cessing.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proc. of ACL, pages 384?394.
T. Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Trans. Informa-
tion Theory, 37(4).
343
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 512?522,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Generative Models of Monolingual and Bilingual Gappy Patterns
Kevin Gimpel Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{kgimpel,nasmith}@cs.cmu.edu
Abstract
A growing body of machine translation re-
search aims to exploit lexical patterns (e.g., n-
grams and phrase pairs) with gaps (Simard et
al., 2005; Chiang, 2005; Xiong et al, 2011).
Typically, these ?gappy patterns? are discov-
ered using heuristics based on word align-
ments or local statistics such as mutual infor-
mation. In this paper, we develop generative
models of monolingual and parallel text that
build sentences using gappy patterns of arbi-
trary length and with arbitrarily many gaps.
We exploit Bayesian nonparametrics and col-
lapsed Gibbs sampling to discover salient pat-
terns in a corpus. We evaluate the patterns
qualitatively and also add them as features to
an MT system, reporting promising prelimi-
nary results.
1 Introduction
Beginning with the success of phrase-based transla-
tion models (Koehn et al, 2003), a trend arose of
modeling larger and increasingly complex structural
units in translation. One thread of work has focused
on the use of lexical patterns with gaps. Simard et
al. (2005) proposed using phrase pairs with gaps in a
phrase-based translation model, providing a heuris-
tic method to extract gappy phrase pairs from word-
aligned parallel corpora. The widely-used hierarchi-
cal phrase-based translation framework was intro-
duced by Chiang (2005) and also relies on a simple
heuristic for phrase pair extraction. On the mono-
lingual side, researchers have taken inspiration from
trigger-based language modeling for speech recog-
nition (Rosenfeld, 1996). Recently Xiong et al
(2011) used monolingual trigger pairs to improve
handling of long-distance dependencies in machine
translation output.
All of this previous work used heuristics or local
statistical tests to extract patterns from corpora. In
this paper, we present probabilistic models that gen-
erate text using gappy patterns of arbitrary length
and with arbitrarily-many gaps. We exploit non-
parametric priors and use Bayesian inference to dis-
cover the most salient gappy patterns in monolin-
gual and parallel text. We first inspect these pat-
terns manually and discuss the categories of phe-
nomena that they capture. We also add them as
features in a discriminatively-trained phrase-based
MT system, using standard techniques to train their
weights (Arun and Koehn, 2007; Watanabe et al,
2007) and incorporate them during decoding (Chi-
ang, 2007). We present experiments for Spanish-
English and Chinese-English translation, reporting
encouraging preliminary results.
2 Related Work
There is a rich history of trigger-based language
modeling in the speech recognition community, typ-
ically involving the use of statistical tests to discover
useful trigger-word pairs (Rosenfeld, 1996; Jelinek,
1997). Xiong et al (2011) used Rosenfeld?s mutual
information procedure to discover trigger pairs and
added a single feature to a phrase-based MT system
that scores new words based on all potential trig-
gers from previous parts of the derivation. We are
not aware of prior work that uses generative model-
ing and Bayesian nonparametrics to discover these
same types of patterns automatically; doing so al-
lows us to discover larger patterns with more words
and gaps if they are warranted by the data.
In addition to the gappy phrase-based (Simard et
al., 2005) and hierarchical phrase-based (Chiang,
2005) models mentioned earlier, other researchers
have explored the use of bilingual gappy structures
for machine translation. Crego and Yvon (2009) and
512
?(  ) = .
?(  ) = baltic states
it provides either too little or too much .
it 's neither particularly complicated nor novel .
nato must either say " yes " or " no " to the baltic states .
good scientific ideas formulated in bad english either die or get repackaged .
nato must either say " yes " or " no " to the baltic states .
?(  ) = either __ or
???pi????????(  ) = either __ or
?(  ) = to the
?(  ) = " __ " __ " __ "?(  ) = must
?(  ) = yes __ no
?(  ) = say?(  ) = nato
Figure 1: A sentence from the news commentary cor-
pus, along with color assignments for the words and the
pi function for each color.
Galley and Manning (2010) proposed ways of incor-
porating phrase pairs with gaps into standard left-to-
right decoding algorithms familiar to phrase-based
and N -gram-based MT; both used heuristics to ex-
tract phrase pairs. Bansal et al (2011) presented a
model and training procedure for word alignment
that uses phrase pairs with gaps. They use a semi-
Markov model with an enlarged dynamic program-
ming state in order to represent alignment between
gappy phrases. Their model permits up to one gap
per phrase while our models permit an arbitrary
number.
3 Monolingual Pattern Models
We first present a model that generates a sentence as
a set of lexical items that we will refer to as gappy
patterns, or simply patterns. A pattern is defined as
a sequence containing elements of two types: words
and gaps. All patterns must obey the regular expres-
sion w+( w+)*, where w is a word and is a gap.
That is, patterns must begin and end with words and
may not contain consecutive gaps.
We assume that we have an n-word sentence
w1:n.1 We represent patterns in a sentence by as-
sociating each word with a color. To do so, we in-
troduce a vector of color assignment variables c1:n,
with one for each word. We represent a color Cj as
a set in terms of the ci variables: Cj = {i : ci = j}.
Each color corresponds to a pattern that is obtained
by concatenating its words from left to right in the
sentence, inserting gaps when necessary. We denote
the pattern for a color Cj by pi(Cj); Figure 1 shows
examples of the correspondence between colors and
patterns.
The generative story for a single sentence follows:
1We use boldface lowercase letters to denote vectors (e.g.,
f ), denote entry i as fi, and denote the range from i to j as
f i:j .
1. Sample the number of words: n ? Poisson(?)
2. Sample the number of unique colors in the sen-
tence given n: m ? Uniform(1, n)
3. For each word index i = 1 . . . n, sample the color
of word i: ci ? Uniform(1,m). If any of the m
colors has no words, repeat this step.
4. For each color j = 1 . . .m, sample from a
multinomial distribution over patterns: wCj ?
Mult(?). If the words wCj are not consistent
with the color assignments, i.e., wrong number of
words or gaps, gaps not in the correct locations,
repeat this step.
Thus, the probability of generating number of words
n, words w1:n, color assignments c1:n, and number
of colors m is
p(w1:n, c1:n,m | ?, ?)
=
1
Z
(
?n
n!
e??
)(
1
n
)(
1
m
)n m?
j=1
p?(pi(Cj))
(1)
where Z is a normalization constant required by the
potential repetition of sampling in the final two steps
of the generative story. Without Z, the model would
be deficient as we would waste probability mass on
internally inconsistent color assignments.
The core of the model is a single multinomial
distribution p?(?) over patterns. We use a Dirich-
let process (DP) prior for this multinomial so that
we can model an unbounded set of patterns: ? ?
DP(?, P0), where ? is the concentration parameter
and P0 is the base distribution. The base distribution
includes a Poisson(?) over the number of words in
the pattern, a uniform distribution (over word types
in the vocabulary) for each word, a uniform distri-
bution over the number of gaps given the number of
words, and a uniform distribution over the arrange-
ment of gaps given the numbers of gaps and words.2
Inference We use collapsed Gibbs sampling
for inference. Our goal is to obtain samples
from the posterior distribution p({c(i),m(i)}Si=1 |
{w(i)}Si=1, ?, ?), where S is the total number of sen-
tences in the corpus and ? is marginalized out.3
2The number of ways of arranging y gaps among x words is
?(x? 1) choose y?.
3Since we assume the words are given, ? is irrelevant.
513
During each iteration of Gibbs sampling, we pro-
ceed through the corpus and sample a new value for
each ci variable conditioned on the values of all oth-
ers in the corpus. Them variables are determined by
the ci variables and therefore do not need to be sam-
pled directly. When sampling ci, we first remove
ci from the corpus (and its color if the color only
contained i). Where the remaining colors in the sen-
tence are numbered from 1 to m, there are m + 1
possibilities for ci: m for each of the existing colors
and one for choosing a new color.
Since choosing a new color corresponds to creat-
ing a new instance of the pattern pi({i}), the proba-
bility of choosing a new color m+ 1 is proportional
to
#pi({i}) + ?P0(pi({i}))
# + ?
(2)
where #pi is the count of pattern pi in the rest of the
sentence and all other sentences in the corpus, and
# is the total count of all patterns in this same set.
The probability of choosing the existing color j (for
1 ? j ? m) is proportional to
#pi(Cj?{i}) + ?P0(pi(Cj ? {i}))
#pi(Cj) + ?P0(pi(Cj))
(3)
where the denominator encodes the fact that the
move will cause an instance of the pattern for the
color Cj to be removed from the corpus as the new
pattern for Cj ? {i} is added.
We note that, even though these two types of
moves will result in different numbers of colors (m)
in the sentence, we do not have to include a term for
this in the sampler because we use a uniform dis-
tribution for m and therefore all (valid) numbers of
colors have the same probability. The normalization
constant Z in Equation 1 does not affect inference
because our sampler is designed to only consider
valid (i.e., internally consistent) settings for the c(i)
and m(i) variables.
This model makes few assumptions, using uni-
form distributions whenever possible. This simpli-
fies inference and causes the resulting lexicon to be
influenced primarily by the ?rich-get-richer? effect
of the DP prior. Despite its simplicity, we will show
later that this model discovers patterns that capture
a variety of linguistic phenomena.
?(  ) = .
?(  ) = baltic states
it provides either too little or too much .
it 's neither particularly complicated nor novel .
nato must either say " yes " or " no " to the baltic states .
good scientific ideas formulated in bad english either die or get repackaged .
nato must either say " yes " or " no " to the baltic states .
la otan tiene que decir " s? " o " no " a los pa?ses b?lticos .
?(  ) = either      or
???pi????????(  ) = either __ or
?(  ) = to the
?(  ) =
"      "
?(  ) = must
?(  ) = yes __ no
?(  ) = say?(  ) = nato
?(  ) = 
nato
otan ?(  ) =
to the
13-12 15-13 16-15
o " " a
Figure 2: A Spanish-English sentence pair with the in-
tersection of automatic word alignments in each direc-
tion. Some source words accept the colors of target words
aligned to them while others (light gray) do not. Bilingual
patterns for a few colors are shown.
4 Bilingual Pattern Models
We now present a generative model for a sentence
pair that will enable us to discover bilingual pat-
terns. In this section we present one example of ex-
tending th previous model to be bilingu l, but we
note that many other extensions are possible; indeed,
flexibility is one of the key advantages of working
within the framework of probabilistic modeling.
We assume that we are given sentence pairs and
one-to-one word alignments. That is, in addition to
an n-word target sentencew1:n, we assume we have
an n?-word source sentence w?1:n? and word align-
ments a1:n? where ai = j iff w?i is aligned to wj and
ai = 0 if w?i is aligned to null.
To model bilingual patterns, we distinguish
source colors from target colors. A target-language
word can only be colored with a target color, but
a source word can be colored with either a source
color or with the target color of the target word it
is aligned to (if any). We have m target colors as
before and now add m? source colors. We intro-
duce additional random variables in the form of a
binary vector g of length n? that indicates, for each
source word, whether or not it accepts the color of
its aligned target word. We introduce an additional
parameter ? for the probability that a source word
will accept the color of its aligned word. We fix its
value to 0.5 and do not learn it during inference. Fig-
ure 2 shows an example Spanish-English sentence
pair with automatic word alignments and color as-
signments. The bilingual patterns for a few target
colors are shown.
The generative story for a sentence pair follows:
1. Sample the numbers of words in the source and
target sentences: n?, n ? Poisson(?)
514
2. Sample the numbers of source and target col-
ors given n?, n: m? ? Uniform(1, n?),m ?
Uniform(1, n)
3. Sample the alignment vector from any distribu-
tion that ensures links are 1-to-1:4 a1:n? ? p(a)
4. For each target word index i = 1 . . . n, sample
the color of target word i from a uniform distribu-
tion over all target colors: ci ? Uniform(1,m).
While any of the m colors has no words, repeat
this step.
5. For each source word index i = 1 . . . n?:
1. Decide whether to use a source color or to use
the target color of the aligned target word: gi ?
p?(gi | ai)
2. If gi = 1, set c?i = cai ; otherwise, sample a
source color: c?i ? Uniform(1,m
?)
6. If any source color has no words, repeat Step 5.
7. For each source color j = 1 . . .m?:
1. Sample from a multinomial over source pat-
terns: wC?j ? Mult(?
?). While the words wC?j
are not consistent with the color assignments,
repeat this step.
8. For each target color j = 1 . . .m:
1. Sample from a multinomial over bilingual pat-
terns: wCj ? Mult(?). While the words wCj
are not consistent with the color assignments,
repeat this step.
The distribution p?(gi | ai) is defined below:
p?(gi = 1 | ai 6= ?1) = ?
p?(gi = 1 | ai = ?1) = 0
where ? determines how frequently source tokens
will be added to target patterns.
The probability of generating target words w1:n,
source words w?1:n? , alignments a1:n? , target color
assignments c1:n, source color assignments c?1:n? ,
color propagation variables g1:n? , number of target
4Since we assume alignments are provided during inference,
it does not matter what distribution is used, so long as only 1-
to-1 links are permitted.
colors m, and number of source colors m? is
1
Z
p(n)p(n?)p(m | n)p(m? | n?)p(a1:n?)
?
(
n?
i=1
p(ci | m)
)
?
(
n??
i=1
p?(gi | ai)p(c
?
i | m
?)I[gi==0]
)
?
?
?
m??
j=1
p??(pi(C
?
j))
?
?
?
?
m?
j=1
p?(pi(Cj))
?
?
where Z again serves as a normalization constant to
prevent the model from leaking probability mass on
internally inconsistent configurations.
There are now two multinomial distributions over
patterns with parameter vectors ? and ??. They both
use DP priors with identical concentration param-
eters ? and differing base distributions P0 and P ?0.
The base distribution for source patterns, P ?0, takes
the same form as the base distribution for the model
described in ?3.
For target patterns with aligned source words, P0
generates the target part of the pattern like the base
distribution in ?3 and then generates the number
of aligned source words to each target word with
a Poisson(1) distribution; the number of aligned
source words can only be 0 or 1 when all word links
are 1-to-1. If it is 1, the base distribution generates
the aligned source word by sampling uniformly from
among all source types.
While there are connections between this model
and work on performing translation using phrase
pairs with gaps, the patterns we discover are not
guaranteed to be bilingual translation units. Rather,
they typically contain additional target-side words
that have no explicit correlate on the source side.
They can be used to assist an existing translation
model by helping to choose the best phrase trans-
lation for each source phrase. To define a genera-
tive model for phrase pairs with gaps, changes would
have to be made to the bilingual model we presented.
Inference As before, we use collapsed Gibbs sam-
pling for inference. Our goal is to obtain sam-
ples from the posterior p({?c, c?, g,m,m??(i)}Si=1 |
{?w,w?,a?(i)}Si=1).
515
We go through each sentence pair and sample new
color assignment variables for each word. For an
aligned word pair (w?i, wj), we sample a new value
for the tuple (gi, c?i, cj). The possible values for
cj include all target colors, including a new target
color. The possible values for gi are 0, in which case
c?i can be any of the source colors, including a new
source color, and 1, for which c?i must be cj . For an
unaligned target word wj , cj can be any target color,
including a new one, and for an unaligned source
word w?i, c
?
i can be any source color, including a new
one. The full equations for sampling can be easily
derived using the equations from ?3.
5 Evaluation
We conducted evaluation to determine (1) what
types of phenomena are captured by the most prob-
able patterns discovered by our models, and (2)
whether including the patterns as features can im-
prove translation quality.
5.1 Qualitative Evaluation
5.1.1 Monolingual Model
Since inference is computationally expensive,
we used the 126K-sentence English news com-
mentary corpus provided for the WMT shared
tasks (Callison-Burch et al, 2010). We ran Gibbs
sampling for 600 iterations through the data, dis-
carding the first 300 samples for burn-in and com-
puting statistics of the patterns using the remaining
300 samples. Each iteration took approximately 3
minutes on a single 2.2GHz CPU. When looking pri-
marily at the most frequent patterns, we found that
this list did not vary much when only using half of
the data instead. We set ? = 3 and ? = 100; we
found these hyperparameters to have only minor ef-
fects on the results.
Since many frequent patterns include the period
(.), we found it useful to constrain the model to treat
this token differently: we modify the base distribu-
tion so that it assigns zero probability to patterns
that contain a period along with other words and we
force each occurrence of a period to be alone in its
own pattern during initialization. We do not need to
change the inference procedure at all; with the mod-
ified base distribution and with no patterns including
a period with other words, the probability of creat-
" " as as " " " "
? ? the of in why ?
( ) the is , the of
the of not only but from to
, , , it is that the between and
the ( ) of " " such as ,
both and not , but either or
the of and in , in but is
more than the of , " " the
- - what ? has been
, " " between and in , ,
the " " the of ?s an of
Table 1: Top-ranked gappy patterns from samples accord-
ing to p(pi); patterns without gaps are omitted. The spe-
cial string ? ? represents a gap that can be filled by any
nonempty sequence of words.
ing a new illegal pattern during inference is always
zero (Eq. 3).
We also perform inference on a transformed ver-
sion of the corpus in which every word is replaced
with its hard word class obtained from Brown clus-
tering (Brown et al, 1992). One property of Brown
clusters is that each function word effectively re-
ceives its own class, as each ends up in a cluster in
which it occupies ?95% of the token counts of all
types in the cluster. We call clusters that satisfy this
property singleton clusters.
To obtain Brown clusters for the source and tar-
get languages, we used code from Liang (2005).5
We used the data from the news commentary cor-
pus along with the first 500K sentences of the addi-
tional monolingual newswire data also provided for
the WMT shared tasks. We used 300 clusters, ig-
noring words that appeared only once in this corpus.
We did not use the hierarchical information from the
clusters but merely converted each cluster name into
a unique integer, using one additional integer for un-
known words.
We used the same values for ? and ? as above
but ran Gibbs sampling for 1,300 iterations, again
using the last 300 for collecting statistics on pat-
terns. Judging by the number of color assignments
changed on each iteration, the sampler takes longer
to converge when run on word clusters than on
words. As above, we constrain the singleton word
cluster corresponding to the period to be alone dur-
ing both initialization and inference.
5http://www.cs.berkeley.edu/?pliang/
software
516
academy sciences regulators supervisors
beijing shanghai sine non
booms busts stalin mao
council advisers treasury secretary geithner
dominicans haitian sooner later
flemish walloons first foremost
gref program played role
heat droughts down road
humanitarian displaced freedom expression
karnofsky hassenfeld at disposal
kazakhstan kyrgyzstan take granted
portugal greece - -
Table 2: Gappy patterns with highest conditional proba-
bility p(pi|w(pi)).
? ? whether or france germany
( ) around world he his
- - has been allow to
both and how ? for first time
not only but the ( ) china india
" " on basis what do
more than less than we our
either or on other hand over past
why ? at level prevent from
neither nor it is that in way
what ? not , but one another
rule law play role political economic
Table 3: Top-ranked gappy patterns according to
p(pi)p(pi|w(pi)).
Pattern Ranking Statistics Several choices exist
for ranking patterns. The simplest is to take the pat-
tern count from the posterior samples, averaged over
all sampling iterations after burn-in. We refer to this
criterion as the marginal probability:
p(pi) =
#pi
#
where #pi is the average count of the pattern across
the posterior samples and # is the count of all pat-
terns. The top-ranked gappy patterns under this cri-
terion are shown in Table 1. While many of these
patterns match our intuitions, there are also sev-
eral that are highly-ranked simply because their con-
stituent words are frequent.
Alternatively, we can rank patterns by the con-
ditional probability of the pattern given the words
that comprise it:
p(pi|w(pi)) =
#pi
#w(pi)
where w(pi) returns the sequence of words in the
pattern pi and #w(pi) is the number of occurrences
of this sequence of words in the corpus that are com-
patible with pattern pi. The ranking of patterns under
this criterion is shown in Table 2. This method fa-
vors precision but also causes very rare patterns to
be highly ranked.
To address this, we also consider a product-of-
experts model by simply multiplying together the
two probabilities, resulting in the ranking shown in
Table 3. This ranking is similar to that in Table 1
but penalizes patterns that are only ranked highly be-
cause they consist of common words. Table 4 shows
a manual grouping of these highly-ranked patterns
into several categories. We show both lexical and
Brown cluster patterns.6
It is common in both types of patterns to find
long-distance dependencies involving punctuation
near the top of the ranking. Among agreement pat-
terns, the lexical model finds relationships between
pronouns and their associated possessive adjectives
while the cluster model finds more general patterns
involving classes of nouns. Cluster patterns are more
likely to capture topicality within a sentence, while
the finer granularity of the lexical model is required
to identify constructions like those shown (verbs
triggering particular prepositions).
There are also many probable patterns without
gaps, shown at the bottom of Table 4. From these
patterns we can see that our models can also be used
to find collocations, but we note that these are dis-
covered in the context of the gappy patterns. That
is, due to the use of latent variables in our models
(the color assignments), there is a natural trading-off
effect whereby the gappy patterns encourage partic-
ular non-gappy patterns to be used, and vice versa.
5.1.2 Bilingual Model
We use the news commentary corpus for each lan-
guage and take the intersection of GIZA++ (Och
and Ney, 2003) word alignments in each direction,
thereby ensuring that they are 1-to-1 alignments. We
ran Gibbs sampling for 300 iterations, averaging pat-
tern counts from the last 200. We set ? = 100,
? = 3, and ? = 0.5. We ran the model in 3 con-
ditions: source words, target words; source clusters,
target clusters; and source clusters, target words. We
6We filter Brown cluster patterns in which every cluster is
a singleton, since these patterns are typically already accounted
for in the lexical patterns.
517
Rank Gappy Lexical Patterns Rank Gappy Brown Cluster Patterns
P
un
ct
ua
ti
on
1 -- -- 2 {what, why, whom, whatever} {?, !}
2 ( ) 6 {--, -, ?} {--, -, ?}
6 " " 28 {according, compared, subscribe, thanks, referring} to ,
9 why ? 178 {?, -, ?} {even, especially, particularly, mostly, mainly} {?, -, ?}
63 according to , 239 {obama, bush, clinton, mccain, brown} " "
A
gr
ee
m
en
t
26 he his 8 {people, things, americans, journalists, europeans} their
31 we our 12 we {our, my}
46 his his 21 {children, women, others, men, students} their
86 china its 23 {china, europe, america, russia, iran} ?s its
90 his he 43 {obama, bush, clinton, mccain, brown} his
99 you your 46 {our, my} {our, my}
136 leaders their 149 {people, things, americans, journalists, europeans} they
140 we ourselves 172 {president, bill, sen., king, senator} {obama, bush, clinton, mccain, brown} his
165 these are 180 {all, both, either} {countries, companies, banks, groups, issues}
C
on
ne
ct
iv
es
4 both and 5 {more, less} {more, less}
5 not only but 9 if , {will, would, could, should, might}
8 either or 19 {deal, plan, vote, decision, talks} {against, between, involving} and
10 neither nor 40 a {against, between, involving} and
13 whether or 45 {better, different, further, higher, lower} than
19 less than 50 {much, far, slightly, significantly, substantially} than
23 not , but 56 {yet, instead, perhaps, thus, neither} but
54 if then 68 not {only, necessarily} {also, hardly}
109 between and 98 as {much, far, slightly, significantly, substantially} as
192 relationship between and 131 is {more, less} than
To
pi
ca
li
ty
25 france germany 1 ?UNK? ?UNK?
29 china india 15 {china, europe, . . .} ?s {system, crisis, program, recession, situation}
36 political economic 30 {health, security, defense, safety, intelligence} {health, . . .}
43 rich poor 47 {china, europe, . . .} {china, europe, . . .} {china, europe, . . .}
50 oil gas 62 {power, growth, interest, development} {10, 1, 20, 30, 2} {percent, %, p.m., a.m.}
62 billions dollars 72 in {iraq, washington, london, 2008, 2009} {iraq, washington, london, 2008, 2009}
96 economic social 73 the {end, cost, head, rules, average} of {prices, markets, services, problems, costs}
106 the us europe 113 {china, europe, . . .} ?s {economy, election, elections, population, investigation}
181 public private 119 {prices, markets, . . .} {oil, energy, tax, food, investment} {oil, energy, . . .}
P
re
po
si
ti
on
s
14 around world 14 for {first, second, third, final, whole} {time, period, term, class, avenue}
18 on basis 17 in {last, next, 20th} {year, week, month, season, summer}
38 at time 51 at {end, cost, head, rules, average} of
42 in region 71 at {group, rate, leader, level, manager}
80 in manner 112 for {times, points, games, goals, reasons}
85 at expense 126 {over, around, across, behind, above} {country, company, region, nation, virus}
112 during period 190 {one, none} of {best, top, largest, main, biggest}
C
on
st
ru
ct
io
ns 33 prevent from
84 enable to
114 provide for
123 impose on
177 turn into
Non-Gappy Lexical Patterns Non-Gappy Brown Cluster Patterns
as well their own as {well, soon, quickly, seriously, slowly} as {rather, please} than
the united states prime minister the united {states, nations, airlines} {don, didn, doesn, isn, wasn} ?t
have been climate change {president, bill, sen., king, senator} {mr., mr, john, david, michael} {obama, bush, clinton, . . .}
rather than the bush administration {order, plans, needs, efforts, failed} to {make, take, give, keep, provide}
based on developing countries {will, would, could, should, might} not be {can, ?ll} be
Table 4: Gappy patterns manually divided into categories of long-distance dependencies. Patterns were ranked ac-
cording to p(pi)p(pi|w(pi)) and manually selected from the top 300 to exemplify categories. Lower pane shows top
ranked non-gappy patterns. Clusters are shown as enough words to cover 95% of the token counts of the cluster, up to
a maximum of 5.
again ensured that the period and its word class re-
mained isolated in their own patterns for each con-
dition. We note that no source-side word order in-
formation is contained within these bilingual pat-
terns; aligned source words can be in any order in
the source sentence and the pattern will still match.
The most probable patterns included many mono-
lingual source-only and target-only patterns that are
similar to those shown in Table 4. There were also
many phrase pairs with gaps like those that are com-
518
monly extracted by heuristics (Galley and Manning,
2010). Additionally we noted examples of source
words triggering more target-side information than
merely one word. There were several examples of
patterns that encouraged inclusion of the subject in
English when translating from Spanish, as Spanish
often drops the subject when it is clear from context,
e.g., ?we are(estamos)?. Also, one probable pattern
for German-English was ?the of the(des)? (des is
aligned to the final the). The German determiner
des is in the genitive case, so this pattern helps to
encourage its object to also be in the genitive case
when translated.
5.2 Quantitative Evaluation
We consider the Spanish-to-English (ES?EN)
translation task from the ACL-2010 Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2010). We trained a Moses system (Koehn et al,
2007) following the baseline training instructions for
the shared task.7 In particular, we performed word
alignment in each direction using GIZA++ (Och and
Ney, 2003), used the ?grow-diag-final-and? heuristic
for symmetrization, and extracted phrase pairs up to
a maximum length of seven. After filtering sentence
pairs with one sentence longer than 50 words, we
ended up with 1.45M sentence pairs of Europarl data
and 91K sentence pairs of news commentary data.
Language models (N = 5) were estimated using the
SRI language modeling toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Language models were trained on the
target side of the parallel corpus as well as the first 5
million additional sentences from the extra English
monolingual newswire data provided for the shared
tasks. We used news-test2008 for tuning and
news-test2009 for testing.
We also consider Chinese-English (ZH?EN) and
followed a similar training procedure as above. We
used 303K sentence pairs from the FBIS corpus
(LDC2003E14) and segmented the Chinese data
using the Stanford Chinese segmenter in ?CTB?
mode (Chang et al, 2008), giving us 7.9M Chi-
nese words and 9.4M English words. A trigram lan-
guage model was estimated using modified Kneser-
Ney smoothing from the English side of the parallel
7www.statmt.org/wmt10/baseline.html.
corpus concatenated with 200M words of randomly-
selected sentences from the Gigaword v4 corpus (ex-
cluding the NY Times and LA Times). We used
NIST MT03 for tuning and NIST MT05 for test-
ing. For evaluation, we used case-insensitive IBM
BLEU (Papineni et al, 2001).
5.2.1 Training and Decoding
Unlike n-gram language models, our models have
latent structure (the color assignments), making it
difficult to compute the probability of a translation
during decoding. We leave this problem for future
work and instead simply add a feature for each of
the most probable patterns discovered by our mod-
els. Each feature counts the number of occurrences
of its pattern in the translation.
We wish to add thousands of features to our
model, but the standard training algorithm ? mini-
mum error rate training (MERT; Och, 2003) ? can-
not handle large numbers of features. So, we lever-
age recent work on feature-rich training for MT us-
ing online discriminative learning algorithms. Our
training procedure is shown as Algorithm 1. We
find it convenient to notationally distinguish feature
weights for the standard Moses features (?) from
weights for our pattern features (?). We use h(e)
to denote the feature vector for translation e. The
function Bi(t) returns the sentence BLEU score for
translation t given reference ei (i.e., treating the sen-
tence pair as a corpus).8
MERT is run to convergence on the tuning set to
obtain weights for the standard Moses features (line
1). Phrase lattices (Ueffing et al, 2002) are gen-
erated for all source sentences in the tuning set us-
ing the trained weights ?M (line 2). The lattices
are used within a modified version of the margin-
infused relaxed algorithm (MIRA; Crammer et al,
2006) for structured max-margin learning (lines 5-
15). A k-best list is extracted from the current lattice
(line 7), then the translations on the k-best list with
the highest and lowest sentence-level BLEU scores
are found (lines 8 and 9). The step size is then com-
puted using the standard MIRA formula (lines 10-
11) and the update is made (line 12). The returned
weights are averaged over all updates.
This training procedure is inspired by several
8When computing sentence BLEU, we smooth by replacing
precisions of 0.0 with 0.01.
519
Input: input sentences F = {fi}Ni=1, references
E = {ei}Ni=1, initial weights ?0, size of
k-best list k, MIRA max step size C, num.
iterations T
Output: learned weights: ?M , ??
?,???
?M ? MERT (F , E, ?0);1
{`i}Ni=1 ? generateLattices (F , ?M );2
?? ?M ; ? ? 0;3
???, ??? ? ??,??;4
for iter ? 1 to T do5
for i? 1 to N do6
{tj}kj=1 ? Decode(`i, ??,??);7
e+ ? argmax1?j?k Bi(tj);8
e? ? argmin1?j?k Bi(tj);9
?? max(0, ??,??> [h(e?)? h(e+)]10
+Bi(e+)?Bi(e?));
? ? min(C, ?
?h(e+)?h(e?)?2 );11
? ? ? + ? [h(e+)? h(e?)];12
???, ??? ? ???, ???+ ??,??;13
end14
end15
???,??? ? ???, ??? ? 1T?N+1 ;16
return ?M , ??
?,???;17
Algorithm 1: Train
others that have been shown to be effective for
MT (Liang et al, 2006; Arun and Koehn, 2007;
Watanabe et al, 2007; Chiang et al, 2008). Though
not shown in the algorithm, in practice we store the
BLEU-best translation on each k-best list from all
previous iterations and use it as e+ if it has a higher
BLEU score than any on the k-best list on the cur-
rent iteration.
At decoding time, we follow a procedure similar
to training: we generate lattices for each source sen-
tence using Moses with its standard set of features
and using weights ?M . We rescore the lattices us-
ing ?? and use cube pruning (Chiang, 2007; Huang
and Chiang, 2007) to incorporate the gappy pattern
features with weights ??. Cube pruning is necessary
because the pattern features may match anywhere in
the translation; thus they are non-local in the phrase
lattice and require approximate inference.
5.3 Training Algorithm Comparison
Before adding pattern features, we evaluate our
training algorithm by comparing it to MERT us-
ing the same standard Moses features. As the ini-
ES?EN ZH?EN
MERT 25.64 32.47
Alg. 1 25.85 32.33
Table 5: Comparing MERT to our training procedure. All
numbers are %BLEU.
tial weights ?0, we used the default Moses feature
weights. We used k = 100, C = 0.0001, and
T = 15. For the n-best list size used during cube
pruning during both training and decoding, we used
n = 100. There are several Moses parameters that
affect the scope of the search during decoding and
therefore the size of the phrase lattices. We used
default values for these except for the stack size pa-
rameter, for which we used 100. The resulting lat-
tices encode up to 1050 derivations for ES?EN and
1065 derivations for ZH?EN.
Table 5 shows test set %BLEU for each language
pair and training algorithm. Our procedure per-
forms comparably to MERT. Therefore we use it as
our baseline for subsequent experiments since it can
handle a large number of feature weights; this al-
lows us to observe the contribution of the additional
gappy pattern features more clearly.
5.4 Feature Preparation
We chose monolingual and bilingual pattern features
using the posterior samples obtained via the infer-
ence procedures described above. We ranked pat-
terns using the product-of-experts formula, removed
patterns consisting of only a single token, and added
the top 10K patterns from the lexical model and the
top 15K patterns from the Brown cluster model. For
simplicity of implementation, we skipped over pat-
terns with 3 or more gaps and patterns with 2 gaps
and more than 3 total words; this procedure skipped
fewer than 1% of the top patterns. For results with
bilingual pattern features, we added 15K pattern fea-
tures (5K word-word, 5K cluster-cluster, and 5K
cluster-word).
5.5 Results
The first set of results is shown in Table 6. The
first row is the same as in Table 5, the second
row adds monolingual pattern features, the third
adds bilingual pattern features, and the final row in-
cludes both sets. While gains are modest overall,
520
ES?EN ZH?EN
Baseline 25.85 32.33
MONOPATS 25.84 32.81
BIPATS 25.92 32.68
MONOPATS + BIPATS 25.59 32.80
Table 6: Adding gappy pattern features. All numbers are
%BLEU.
Ranking %BLEU
Baseline N/A 32.33
MONOPATS p(pi) 32.65
MONOPATS p(pi|w(pi)) 32.53
MONOPATS p(pi)p(pi|w(pi)) 32.81
BIPATS p(pi) 32.68
MONOPATS + BIPATS p(pi) 32.78
MONOPATS + BIPATS p(pi)p(pi|w(pi)) 32.80
Table 7: Comparing ways of ranking patterns from pos-
terior samples. Scores are on MT05 for ZH?EN transla-
tion.
the pattern features show an encouraging improve-
ment of 0.48 BLEU for ZH?EN. This is similar
to the improvement reported by Xiong et al (2011)
(+0.4 BLEU when adding their trigger pair language
model). While bilingual patterns give an improve-
ment of 0.35 BLEU, using both monolingual and
bilingual features in the same model does not pro-
vide additional improvement over monolingual fea-
tures alone.
For ES?EN, the pattern features have only small
effects on BLEU; we suspect that the decreased
BLEU score for the full feature set is due to over-
fitting. It is unclear why the results differ for the two
language pairs. One possibility is the use of only
a single reference translation when tuning and test-
ing with ES?EN while four references were used
for ZH?EN. Another possibility is that our pattern
features are correcting some of the mid- to long-
range reorderings that are known to be problem-
atic for phrase-based modeling of ZH?EN transla-
tion. ES?EN exhibits less long-range reordering
and therefore may not benefit as much from our pat-
terns.
Table 7 shows additional ZH?EN results when
varying the method of ranking patterns. When us-
ing both sets of features, the ?Ranking? column
contains the criterion for ranking monolingual pat-
terns; bilingual patterns are always ranked using
said that the however , the agence france presse
?s , ?s us iraq reported the
of million , likely said that and
added " - - rate percent
the {media, school, university, election, bank}
{made, established, given, taken, reached}
{said, stressed, stated, indicated, noted} that in
{meeting, report, conference, reports} {1, july, june, march, april}
{news, press, spokesman, reporter} {meeting, . . .} {1, july, . . .}
{news, press, spokesman, reporter} {1, july, june, march, april}
the {enterprises, companies, students, customers, others}
{enterprises, companies, students, customers, others}
{japan, russia, europe, 2003, 2004} {us, japanese, russian, u.s.}
Table 8: Selected features from the 15 most highly-
weighted lexical and cluster pattern features in the best
ZH?EN model.
p(pi). The results show that ranking monolingual
patterns using the product-of-experts method results
in the highest BLEU scores, validating our intu-
itions from observing Tables 1-3. Table 8 shows the
most highly-weighted pattern features for the best
ZH?EN model.
6 Conclusion
We have presented generative models for monolin-
gual and bilingual gappy patterns. A qualitative
analysis shows that the models discover patterns
that match our intuitions in capturing linguistic phe-
nomena. Our experimental results show promise
for the ability of these patterns to improve trans-
lation for certain language pairs. A key advan-
tage of generative models is the ability to rapidly
develop and experiment with variations, especially
when using Gibbs sampling for inference. In order
to encourage modifications and extensions to these
models we have made our source code available at
www.ark.cs.cmu.edu/MT.
Acknowledgments
The authors thank Chris Dyer, Qin Gao, Alon Lavie,
Nathan Schneider, Stephan Vogel, and the anonymous
reviewers for helpful comments. This research was sup-
ported in part by the NSF through grant IIS-0844507, the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-
1-0533, and Sandia National Laboratories (fellowship to
K. Gimpel).
521
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
M. Bansal, C. Quirk, and R. Moore. 2011. Gappy
phrasal alignment by agreement. In Proc. of ACL.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram mod-
els of natural language. Computational Linguistics,
18.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 joint workshop on statistical machine translation
and metrics for machine translation. In Proc. of the
5th Workshop on Statistical Machine Translation.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the Third Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. M. Crego and F. Yvon. 2009. Gappy translation units
under left-to-right SMT decoding. In Proc. of EAMT.
M. Galley and C. D. Manning. 2010. Accurate non-
hierarchical phrase-based translation. In Proc. of
NAACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
F. Jelinek. 1997. Statistical methods for speech recogni-
tion. MIT Press.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10(3).
M. Simard, N. Cancedda, B. Cavestro, M. Dymetman,
E?. Gaussier, C. Goutte, K. Yamada, P. Langlais, and
A. Mauser. 2005. Translating with non-contiguous
phrases. In Proc. of HLT-EMNLP.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP-CoNLL.
D. Xiong, M. Zhang, and H. Li. 2011. Enhancing lan-
guage models in statistical machine translation with
backward N-grams and mutual information triggers.
In Proc. of ACL.
522
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 2?12,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Databases of Named Entities from Bayesian Nonparametrics
Jacob Eisenstein Tae Yano William W. Cohen Noah A. Smith Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jacobeis,taey,wcohen,nasmith,epxing}@cs.cmu.edu
Abstract
We present a nonparametric Bayesian ap-
proach to extract a structured database of enti-
ties from text. Neither the number of entities
nor the fields that characterize each entity are
provided in advance; the only supervision is
a set of five prototype examples. Our method
jointly accomplishes three tasks: (i) identify-
ing a set of canonical entities, (ii) inferring a
schema for the fields that describe each entity,
and (iii) matching entities to their references in
raw text. Empirical evaluation shows that the
approach learns an accurate database of enti-
ties and a sensible model of name structure.
1 Introduction
Consider the task of building a set of structured
records from a collection of text: for example, ex-
tracting the names of people or businesses from
blog posts, where each full name decomposes into
fields corresponding to first-name, last-name, title,
etc. To instruct a person to perform this task, one
might begin with a few examples of the records to
be obtained; assuming that the mapping from text to
records is relatively straightforward, no additional
instruction would be necessary. In this paper, we
present a method for training information extraction
software in the same way: starting from a small table
of partially-complete ?prototype? records (Table 1),
our system learns to add new entries and fields to
the table, while simultaneously aligning the records
to text.
We assume that the dimensionality of the database
is unknown, so that neither the number of entries
John McCain Sen. Mr.
George Bush W. Mr.
Hillary Clinton Rodham Mrs.
Barack Obama Sen.
Sarah Palin
Table 1: A set of partially-complete prototype records,
which constitutes the only supervision for the system.
nor the number of fields is specified in advance. To
accommodate this uncertainty, we apply a Bayesian
model which is nonparametric along three dimen-
sions: the assignment of text mentions to entities
(making popular entries more likely while always al-
lowing new entries); the alignment of individual text
tokens to fields (encouraging the re-use of common
fields, but permitting the creation of new fields); and
the assignment of values to entries in the database
itself (encouraging the reuse of values across entries
in a given field). By adaptively updating the con-
centration parameter of stick-breaking distribution
controlling the assignment of values to entries in the
database, our model can learn domain-specific infor-
mation about each field: for example, that titles are
often repeated, while names are more varied.
Our system?s input consists of a very small proto-
type table and a corpus of text which has been au-
tomatically segmented to identify names. Our de-
sired output is a set of structured records in which
each field contains a single string ? not a distribu-
tion over strings, which would be more difficult to
interpret. This requirement induces a tight proba-
bilistic coupling between the assignment of text to
cells in the table, so special care is required to ob-
2
tain efficient inference. Our procedure alternates
between two phases. In the first phase, we per-
form collapsed Gibbs sampling on the assignments
of string mentions to rows and columns in the table,
while marginalizing the values of the table itself. In
the second phase, we apply Metropolis-Hastings to
swap the values of columns in the table, while simul-
taneously relabeling the affected strings in the text.
Our model performs three tasks: it constructs a
set of entities from raw text, matches mentions in
text with the entities to which they refer, and discov-
ers general categories of tokens that appear in names
(such as titles and first names). We are aware of
no existing system that performs all three of these
tasks jointly. We evaluate on a dataset of political
blogs, measuring our system?s ability to discover
a set of reference entities (recall) while maintain-
ing a compact number of rows and columns (pre-
cision). With as few as five partially-complete pro-
totype examples, our approach gives accurate tables
that match well against a manually-annotated refer-
ence list. Our method outperforms a baseline single-
link clustering approach inspired by one of the most
successful entries (Elmacioglu et al, 2007) in the
SEMEVAL ?Web People Search? shared task (Ar-
tiles et al, 2007).
2 Task Definition
In this work, we assume that a bag of M mentions
in text have been identified. The mth mention wm
is a sequence of contiguous word tokens (its length
is denoted Nm) understood to refer to a real-world
entity. The entities (and the mapping of mentions to
entities) are not known in advance. While our focus
in this paper is names of people, the task is defined
in a more generic way.
Formally, the task is to construct a table x where
rows correspond to entities and columns to func-
tional fields. The number of entities and the num-
ber of fields are not prespecified. x?,j denotes the
jth column of x, and xi,j is a single word type fill-
ing the cell in row i, column j. An example is Ta-
ble 1, where the fields are first-name, last-name, ti-
tle, middle-name, and so on. In addition to the table,
we require that each mention be mapped to an en-
tity (i.e., a row in the table). Success at this task
therefore requires (i) identifying entities, (ii) discov-
ering the internal structure of mentions (effectively
canonicalizing them), and (iii) mapping mentions
to entities (therefore resolving coreference relation-
ships among mentions). Note that this task differs
from previous work on knowledge base population
(e.g., McNamee, 2009) because the schema is not
formally defined in advance; rather, the number of
fields and their meaning must be induced from just
a few prototype examples.
To incorporate partial supervision, a subset of the
table x is specified manually by an annotator. We
denote this subset of ?prototypes? by x?; for entries
that are unspecified by the user, we write x?i,j = ?.
Prototypes are not assumed to provide complete in-
formation for any entity.
3 Model
We now craft a nonparametric generative story that
explains both the latent table and the observed men-
tions. The model incorporates three nonparamet-
ric components, allowing an unbounded number of
rows (entities) and columns (fields), as well as an un-
bounded number of values per column (field values).
A plate diagram for the graphical model is shown in
Figure 1.
A key point is that the column distributions ?
range over possible values at the entity level, not
over mentions in text. For example, ?2 might be
the distribution over possible last names and ?3 the
distribution over elected office titles. Note that ?2
would contain a low value for the last name Obama
? which indicates that few people have this last
name ? even though a very high proportion of men-
tions in our data include the string Obama.
The user-generated entries (x?) can still be treated
as the outcome of the generative process: using ex-
changeability, we treat these entries as the first sam-
ples drawn in each column. In this work, we treat
them as fully observed, but it is possible to treat
them as noisy and incorporate a stochastic depen-
dency between xi,j and x?i,j .
4 Inference
We now develop sampling-based inference for the
model described in the previous section. We be-
gin with a token-based collapsed Gibbs sampler, and
then add larger-scale Metropolis-Hastings moves.
3
? ?2
x ? ?
w r ?r
c ?r
?c ?c
Figure 1: A plate diagram for the
text-and-tables graphical model.
The upper plate is the table x, and
the lower plate is the set of textual
mentions. Notation is defined in the
generative model to the right.
? Generate the table entries. For each column j,
? Draw a concentration parameter ?j from a log-normal distribution,
log?j ? N (?, ?2).
? Draw a distribution over strings from a Dirichlet process ?j ?
DP(?j , G0), where the base distribution G0 is a uniform distribution
over strings in a fixed character alphabet, up to an arbitrary finite length.
? For each row i, draw the entry xi,j ? ?j .
? Generate the text mentions.
? Draw a prior distribution over rows from a stick-breaking distribution,
?r ? Stick(?r).
? Draw a prior distribution over columns from a stick-breaking distribu-
tion, ?c ? Stick(?c).
? For each mention wm,
? Draw a row in the table rm ? ?r.
? For each word token wm,n (n ? {1, . . . , Nm}),
? Draw a column in the table cm,n ? ?c.
? Set the text wm,n = xrm,cm,n .
4.1 Gibbs sampling
A key aspect of the generative process is that the
word token wm,n is completely determined by the
table x and the row and column indicators rm and
cm,n: given that a token was generated by row i
and column j of the table, it must be identical to
the value of xi,j . Using Bayes? rule, we can reverse
this deterministic dependence: given the values for
the row and column indices, the entries in the table
are restricted to exact matches with the text men-
tions that they generate. This allows us to marginal-
ize the unobserved entries in the table. We can also
marginalize the distributions ?r, ?c, and ?j , using
the standard collapsed Gibbs sampling equations for
Dirichlet processes. Thus, sampling the row and col-
umn indices is all that is required to explore the en-
tire space of model configurations.
4.1.1 Conditional probability for word tokens
The conditional sampling distributions for both
rows and columns will marginalize the table (be-
sides the prototypes x?). To do this, we must be
able to compute P (wm,n | rm = i, cm,n =
j, x?,w?(m,n), r?m, c?(m,n), ?j), which represents
the probability of generating word wm,n, given
rm = i and cm,n = j. The notation w?(m,n), r?m,
and c?m,n represent the words, row indices, and col-
umn indices for all mentions besides wm,n. For sim-
plicity, we will elide these variables in much of the
subsequent notation.
We first consider the case where we have a user-
specified entry for the row and column ?i, j?? that
is, if x?ij 6= ?. Then the probability is simply,
P (wm,n | rm = i, cm,n = j, x?, . . .) =
{
1, if x?ij = wm,n
0, if x?ij 6= wm,n.
(1)
Because the table cell xij is observed, we do not
marginalize over it; we have a generative probability
of one if the word matches, and zero otherwise. If
the table cell xij is not specified by the user, then we
marginalize over its possible values. For any given
xij , the probability P (wm,n | xij , rm = i, cm,n =
j) is still a delta function, so we have:
?
P (wm,n | xrm,cm,n)P (xrm,cm,n | . . .) dxrm,cm,n
= P (x = wm,n | w?(m,n), r?m, c?(m,n), x?, . . .)
The integral is equal to the probability of the value
of the cell xrm,cm,n being identical to the string
wm,n, given assignments to all other variables. To
compute this probability, we again must consider
two cases: if the cell xi,j has generated some other
string wm?,n? then its value must be identical to that
4
string; otherwise it is unknown. More formally, for
any cell ?i, j?, if ?wm?,n? : rm? = i ? cm?,n? =
j ? ?m?, n?? 6= ?m,n?, then P (xi,j = wm?,n?) = 1;
all other strings have zero probability. If xi,j has not
generated any other entry, then its probability is con-
ditioned on the other elements of the table x. The
known elements of this table are themselves deter-
mined by either the user entries x? or the observa-
tionsw?(m,n). We can define these known elements
as x?, where x?ij = ? if x?ij = ? ? @?m,n? : rm =
i ? cm,n = j. Then we can apply the standard Chi-
nese restaurant process marginalization to obtain:
P (xij | x??(i,j), ?) =
{ N(x??(i,j)=xij)
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) > 0
?
N(x??(i,j) 6=?)+?
, N(x??(i,j) = xij) = 0
(2)
In our implementation, we maintain the table x?,
updating it as we resample the row and column as-
signments. To construct the conditional distribution
for any given entry, we first consult this table, and
then compute the probability in Equation 2 for en-
tries where x?ij = ?.
4.1.2 Sampling columns
We can now derive sampling equations for the
column indices cm,n. We first apply Bayes? rule
to obtain P (cm,n | wm,n, rm, . . .) ? P (cm,n |
c?(m,n), ?c)?P (wm,n | cm,n, rm, x?, . . .). The like-
lihood term P (wm,n | cm,n, . . .) is defined in the
previous section; we can compute the first factor us-
ing the standard Dirichlet process marginalization
over ?c. Writing N(c?(m,n) = j) for the count of
occurrences of column j in the set c?(m,n), we ob-
tain
P (cm,n = j | c?(m,n), ?c) =
{ N(c?(m,n)=j)
N(c?(m,n))+?c
, if N(c?(m,n) = j) > 0
?c
N(c?(m,n))+?c
, if N(c?(m,n) = j) = 0
(3)
4.1.3 Sampling rows
In principle the row indicators can be sampled
identically to the columns, with the caveat that the
generative probability P (wm | rm, . . .) is a product
across all Nm tokens in wm.1 However, because of
1This relies on the assumption that the values of {cm,n} are
mutually independent given c?m. Future work might apply
the tight probabilistic coupling between the row and
column indicators, straightforward Gibbs sampling
mixes slowly. Instead, we marginalize the column
indicators while sampling r. Only the likelihood
term is affected by this change:
P (wm | rm,w?m, r?m, . . .)
=
?
j
P (c = j | c?m, ?c)P (wm,n | cm,n = j, rm, x?, ?).
(4)
The tokens are conditionally independent given the
row, so we factor and then explicitly marginalize
over each cm,n. The chain rule gives the form in
Equation 4, which contains terms for the prior over
columns and the likelihood of the word; these are
defined in Equations 2 and 3. Note that neither the
inferred table x? nor the heldout column counts c?m
include counts from any of the cells in row m.
4.2 Column swaps
Suppose that during initialization, we encounter the
string Barry Obama before encountering Barack
Obama. We would then put Barry in the first-name
column, and put Barack in some other column for
nicknames. After making these initial decisions,
they would be very difficult to undo using Gibbs
sampling ? we would have to first shift all instances
of Barry to another column, then move an instance
of Barack to the first-name column, and then move
the instances of Barry to the nickname column. To
rectify this issue, we perform sampling on the table
itself, swapping the columns of entries in the table,
while simultaneously updating the relevant column
indices of the mentions.
In the proposal, we select at random a row t and
indices i and j. In the table, we will swap xt,i with
xt,j ; in the text we will swap the values of each cm,n
whenever rm = t and cm,n = i or j. This pro-
posal is symmetric, so no Hastings correction is re-
quired. Because we are simultaneously updating the
table and the column indices, the generative likeli-
hood of the words is unchanged; the only changes
a more structured model of the ways that fields are combined
when mentioning an entity. For example, a first-order Markov
model could learn that family names often follow given names,
but the reverse rarely occurs (in English).
5
in the overall likelihood come from the column in-
dices and the values of the cells in the table. Letting
x?, c? indicate the state of the table and column in-
dices after the proposed move, we will accept with
probability,
Paccept(x? x?) = min
(
1,
P (c?)P (x?)
P (c)P (x)
)
(5)
We first consider the ratio of the table probabili-
ties, P (x
?|?)
P (x|?) . Recall that each column of x is drawn
from a Dirichlet process; appealing to exchangeabil-
ity, we can treat the row t as the last element drawn,
and compute the probabilities P (xt,i | x?(t,i), ?i),
with x?(t,i) indicating the elements of the column i
excluding row t. This probability is given by Equa-
tion 2. For a swap of columns i and j, we compute
the ratio:
P (xt,i | x?(t,j), ?j)P (xt,j | x?(t,i), ?i)
P (xt,i | x?(t,i), ?i)P (xt,j | x?(t,j), ?j)
(6)
Next we consider the ratio of the column proba-
bilities, P (c
?)
P (c) . Again we can apply exchangeabil-
ity, P (c) = P ({cm : rm = t} | {cm? : rm? 6=
t})P ({cm? : rm? 6= t}). The second term P ({cm? :
rm? 6= t}) is unaffected by the move, and so is iden-
tical in both the numerator and denominator of the
likelihood ratio; probabilities from columns other
than i and j also cancel in this way. The remaining
ratio can be simplified to,
(
P (c = j | c?t, ?c)
P (c = i | c?t, ?c)
)N(r=t?c=i)?N(r=t?c=j)
(7)
where the counts N() are from the state of the sam-
pler before executing the proposed move. The prob-
ability P (c = i | c?t, ?c) is defined in Equation 3,
and the overall acceptance ratio for column swaps is
the product of (6) and (7).
4.3 Hyperparameters
The concentration parameters ?r and ?c help to con-
trol the number of rows and columns in the ta-
ble, respectively. These parameters are updated to
their maximum likelihood values using gradient-
based optimization, so our overall inference pro-
cedure is a form of Monte Carlo Expectation-
Maximization (Wei and Tanner, 1990).
The concentration parameters ?j control the di-
versity of each column in the table: if ?j is low then
we expect a high degree of repetition, as with titles;
if ?j is high then we expect a high degree of diver-
sity. When the sampling procedure adds a new col-
umn, there is very little information for how to set
its concentration parameter, as the conditional like-
lihood will be flat. Consequently, greater care must
be taken to handle these priors appropriately.
We place a log-normal hyperprior on the col-
umn concentration parameters, log?j ? N (?, ?2).
The parameters of the log-normal are shared across
columns, which provides additional information to
constrain the concentration parameters of newly-
created columns. We then use Metropolis-Hastings
to sample the values of each ?j , using the joint like-
lihood,
P (?j , x?(j) | ?, ?2) ?
exp(?(log?j ? ?)2)?
kj
j ?(?j)
2?2?(nj + ?j)
,
where x?(j) is column j of the inferred table, nj is
the number of specified entries in column j of the
table x? and kj is the number of unique entries in
the column; see Rasmussen (2000) for a derivation.
After repeatedly sampling several values of ?j for
each column in the table, we update ? and ?2 to their
maximum-likelihood estimates.
5 Temporal Prominence
Andy Warhol predicted, ?in the future, everyone will
be world-famous for fifteen minutes.? A model of
temporal dynamics that accounts for the fleeting and
fickle nature of fame might yield better performance
for transient entities, like Joe the Plumber. Among
several alternatives for modeling temporal dynamics
in latent variable models, we choose a simple non-
parametric approach: the recurrent Chinese restau-
rant process (RCRP; Ahmed and Xing, 2008). The
core idea of the RCRP is that time is partitioned into
epochs, with a unique Chinese restaurant process in
each epoch. Each CRP has a prior which takes the
form of pseudo-counts computed from the counts in
previous epochs. We employ the simplest version of
the RCRP, a first-order Markov model in which the
prior for epoch t is equal to the vector of counts for
epoch t? 1:
6
P (r(t)m = i|r
(t)
1...m?1, r
(t?1), ?r) ?
{
N(r(t)1...m?1 = i) + N(r
(t?1) = i), if > 0;
?r, otherwise.
(8)
The count of row i in epoch t ? 1 is written
N(r(t?1) = i); the count in epoch t for mentions
1 to m ? 1 is written N(r(t)1...m?1 = i). As before,
we can apply exchangeability to treat each mention
as the last in the epoch, so during inference we can
replace this with the count N(r(t)?m). Note that there
is zero probability of drawing an entity that has no
counts in epochs t or t ? 1 but exists in some other
epoch; the probability mass ?r is reserved for draw-
ing a new entity, and the chance of this matching
some existing entity from another epoch is vanish-
ingly small.
During Gibbs sampling, we also need to consider
the effect of r(t)m on the subsequent epoch t + 1.
While space does not permit a derivation, the result-
ing probability is proportional to
P (r(t+1)|r(t)?m, r
(t)
m = i, ?r) ?
?
???
???
1 if N(r(t+1) = i) = 0,
N(r(t+1)=i)
?r
if N(r(t)?m = i) = 0,
1 + N(r
(t+1)=i)
N(r(t)?m=i)
if N(r(t)?m = i) > 0.
(9)
This favors entities which are frequent in epoch
t+ 1 but infrequent in epoch t.
The move to a recurrent Chinese restaurant pro-
cess does not affect the sampling equations for the
columns c, nor the concentration parameters of the
table, ?. The only part of the inference procedure
that needs to be changed is the optimization of the
hyperparameter ?r; the log-likelihood is now the
sum across all epochs, and each epoch makes a con-
tribution to the gradient.
6 Evaluation Setup
Our model jointly performs three tasks: identifying
a set of entities, discovering the set of fields, and
matching mention strings with the entities and fields
to which they refer. We are aware of no prior work
that performs these tasks jointly, nor any dataset that
is annotated for all three tasks.2 Consequently, we
focus our quantitative evaluation on what we take to
be the most important subtask: identifying the enti-
ties which are mentioned in raw text. We annotate
a new dataset of blog text for this purpose, and de-
sign precision and recall metrics to reward systems
that recover as much of the reference set as possi-
ble, while avoiding spurious entities and fields. We
also perform a qualitative analysis, noting the areas
where our method outperforms string matching ap-
proaches, and where there is need for further im-
provement.
Data Evaluation was performed on a corpus
of blogs describing United States politics in
2008 (Eisenstein and Xing, 2010). We ran the Stan-
ford Named Entity Recognition system (Finkel et
al., 2005) to obtain a set of 25,000 candidate men-
tions which the system judged to be names of peo-
ple. We then pruned strings that appeared fewer than
four times and eliminated strings with more than
seven tokens (these were usually errors). The result-
ing dataset has 19,247 mentions comprising 45,466
word tokens, and 813 unique mention strings.
Gold standard We develop a reference set of 100
entities for evaluation. This set was created by sort-
ing the unique name strings in the training set by fre-
quency, and manually merging strings that reference
the same entity. We also manually discarded strings
from the reference set if they resulted from errors in
the preprocessing pipeline (tokenization and named
entity recognition). Each entity is represented by
the set of all word tokens that appear in its refer-
ences; there are a total of 231 tokens for the 100 en-
tities. Most entities only include first and last names,
though the most frequent entities have many more:
for example, the entity Barack Obama has known
names: {Barack, Obama, Sen., Mr.}.
Metrics We evaluate the recall and precision of
a system?s response set by matching against the
reference set. The first step is to create a bipar-
tite matching between response and reference enti-
ties.3 Using a cost function that quantifies the sim-
2Recent work exploiting Wikipedia disambiguation pages
for evaluating cross-document coreference suggests an appeal-
ing alternative for future work (Singh et al, 2011).
3Bipartite matchings are typical in information extraction
evaluation metrics (e.g., Doddington et al, 2004).
7
ilarity of response and reference entities, we opti-
mize the matching using the Kuhn-Munkres algo-
rithm (Kuhn, 1955). For recall, the cost function
counts the number of shared word tokens, divided
by the number of word tokens in the reference enti-
ties; the recall is one minus the average cost of the
best matching (with a cost of one for reference enti-
ties that are not matched, and no cost for unmatched
response entities). Precision is computed identically,
but we normalize by the number of word tokens in
the response entity. Precision assigns a penalty of
one to unmatched response entities and no penalty
for unmatched reference entities.
Note that this metric grossly underrates the preci-
sion of all systems: the reference set is limited to 100
entities, but it is clear that our text mentions many
other people. This is harsh but fair: all systems are
penalized equally for identifying entities that are not
present in the reference set, and the ideal system will
recover the fifty reference entities (thus maximizing
recall) while keeping the table as compact as possi-
ble (thus maximizing precision). However, the raw
precision values have little meaning outside the con-
text of a direct comparison under identical experi-
mental conditions.
Systems The initial seed set for our system con-
sists of a partial annotation of five entities (Table 1)
? larger seed sets did not improve performance. We
run the inference procedure described in the previ-
ous section for 20,000 iterations, and then obtain a
final database by taking the intersection of the in-
ferred tables x? obtained at every 100 iterations, start-
ing with iteration 15,000. To account for variance
across Markov chains, we perform three different
runs. We evaluate a non-temporal version of our
model (as described in Sections 3 and 4), and a tem-
poral version with 5 epochs. For the non-temporal
version, a non-parallel C implementation had a wall
clock sampling time of roughly 16 hours; the tem-
poral version required 24 hours.
We compare against a baseline that incrementally
clusters strings into entities using a string edit dis-
tance metric, based on the work of Elmacioglu et
al. (2007). Starting from a configuration in which
each unique string forms its own cluster, we incre-
mentally merge clusters using the single-link crite-
rion, based on the minimum Jaccard edit distance
0.2 0.3 0.4 0.5 0.6 0.70
0.1
0.2
0.3
recall
pre
cis
ion
 
 
baseline
atemporal model
temporal model
Figure 2: The precision and recall of our models, as com-
pared to the curve defined by the incremental clustering
baseline. Each point indicates a unique sampling run.
Bill Clinton Benazir Bhutto
Nancy Pelosi Speaker
John Kerry Sen. Roberts
Martin King Dr. Jr. Luther
Bill Nelson
Table 2: A subset of the entity database discovered by
our model, hand selected to show highlight interesting
success and failure cases.
between each pair of clusters. This yields a series of
outputs that move along the precision-recall curve,
with precision increasing as the clusters encompass
more strings. There is prior work on heuristics for
selecting a stopping point, but we compare our re-
sults against the entire precision-recall curve (Man-
ning et al, 2008).
7 Results
The results of our evaluation are shown in Figure 2.
All sampling runs from our models lie well beyond
the precision-recall curve defined by the baseline
system, demonstrating the ability to achieve reason-
able recall with a far more compact database. The
baseline system can achieve nearly perfect recall by
creating one entity per unique string, but as it merges
strings to improve precision, its recall suffers sig-
nificantly. As noted above, perfect precision is not
possible on this task, because the reference set cov-
ers only a subset of the entities that appear in the
data. However, the numbers do measure the ability
to recover the reference entities in the most compact
table possible, allowing a quantitative comparison of
our models and the baseline approach.
8
Table 2 shows a database identified by the atem-
poral version of our model. The most densely-
populated columns in the table correspond to well-
defined name parts: columns 1 and 2 are almost
exclusively populated with first and last names re-
spectively, and column 3 is mainly populated by ti-
tles. The remaining columns are more of a grab
bag. Column 4 correctly captures Jr. for Martin
Luther King; column 5 correctly captures Luther,
but mistakenly contains Roberts (thus merging the
John Kerry and John Roberts entities), and Bhutto
(thus helping to merge the Bill Clinton and Benazir
Bhutto entities).
The model successfully distinguishes some, but
not all, of the entities that share tokens. For example,
the model separates Bill Clinton from Bill Nelson;
it also separates John McCain from John Kerry
(whom it mistakenly merges with John Roberts).
The ability to distinguish individuals who share first
names is due in part to the model attributing a low
concentration parameter to first names, meaning that
some repetition in the first name column is expected.
The model correctly identifies several titles and al-
ternative names, including the rare title Speaker for
Nancy Pelosi; however, it misses others, such as the
Senator title for Bill Nelson. This may be due in
part to the sample merging procedure used to gener-
ate this table, which requires that a cell contain the
same value in at least 80% of the samples.
Many errors may be attributed to slow mixing.
After mistakenly merging Bhutto and Clinton at
an early stage, the Gibbs sampler ? which treats
each mention independently ? is unable to sep-
arate them. Given that several other mentions of
Bhutto are already in the row occupied by Clin-
ton, the overall likelihood would benefit little from
creating a new row for a single mention, though
moving all such mentions simultaneously would re-
sult in an improvement. Larger scale Metropolis-
Hastings moves, such as split-merge or type-based
sampling (Liang et al, 2010) may help.
8 Related Work
Information Extraction A tradition of research
in information extraction focuses on processing raw
text to fill in the fields of manually-defined tem-
plates, thus populating databases of events or re-
lations (McNamee and Dang, 2009). While early
approaches focused on surface-level methods such
as wrapper induction (Kushmerick et al, 1997),
more recent work in this area includes Bayesian
nonparametrics to select the number of rows in the
database (Haghighi and Klein, 2010a). However,
even in such nonparametric work, the form of the
template and the number of slots are fixed in ad-
vance. Our approach differs in that the number of
fields and their meaning is learned from data. Recent
work by Chambers and Jurafsky (2011) approaches
a related problem, applying agglomerative cluster-
ing over sentences to detect events, and then clus-
tering syntactic constituents to induce the relevant
fields of each event entity. As described in Section 6,
our method performs well against an agglomerative
clustering baseline, though a more comprehensive
comparison of the two approaches is an important
step for future work.
Name Segmentation and Structure A related
stream of research focuses specifically on names:
identifying them in raw text, discovering their struc-
ture, and matching names that refer to the same en-
tity. We do not undertake the problem of named en-
tity recognition (Tjong Kim Sang, 2002), but rather
apply an existing NER system as a preprocessing
step (Finkel et al, 2005). Typical NER systems
do not attempt to discover the internal structure of
names or a database of canonical names, although
they often use prefabricated ?gazetteers? of names
and name parts as features to improve performance
(Borthwick et al, 1998; Sarawagi and Cohen, 2005).
Charniak (2001) shows that it is possible to learn a
model of name structure, either by using coreference
information as labeled data, or by leveraging a small
set of hand-crafted constraints. Elsner et al (2009)
develop a nonparametric Bayesian model of name
structure using adaptor grammars, which they use to
distinguish types of names (e.g., people, places, and
organizations). Li et al (2004) use a set of manually-
crafted ?transformations? of name parts to build a
model of how a name might be rendered in multi-
ple different ways. While each of these approaches
bears on one or more facets of the problem that we
consider here, none provides a holistic treatment of
name disambiguation and structure.
9
Resolving Mentions to Entities The problem of
resolving mentions to entities has been approach
from a variety of different perspectives. There is
an extensive literature on probabilistic record link-
age, in which database records are compared to de-
termine if they are likely to have the same real-world
referents (e.g., Felligi and Sunter, 1969; Bilenko
et al, 2003). Most approaches focus on pairwise
assessments of whether two records are the same,
whereas our method attempts to infer a single coher-
ent model of the underlying relational data. Some
more recent work in record linkage has explicitly
formulated the task of inferring a latent relational
model of a set of observed datasets (e.g., Cohen
et al, 2000; Pasula et al, 2002; Bhattacharya and
Getoor, 2007); however, to our knowledge, these
prior models have all exploited some predefined
database schema (i.e., set of columns), which our
model does not require. Many of these prior mod-
els have been applied to bibliographic data, where
different conventions and abbreviations lead to im-
perfect matches in different references to the same
publication. In our task, we consider name mentions
in raw text; such mentions are short, and may not
offer as many redundant clues for linkage as biblio-
graphic references.
In natural language processing, coreference res-
olution is the task of grouping entity mentions
(strings), in one or more documents, based on their
common referents in the world. Although much of
coreference resolution has on the single document
setting, there has been some recent work on cross-
document coreference resolution (Li et al, 2004;
Haghighi and Klein, 2007; Poon and Domingos,
2008; Singh et al, 2011). The problem we consider
is related to cross-document coreference, although
we take on the additional challenge of providing
a canonicalized name for each referent (the corre-
sponding table row), and in inferring a structured
representation of entity names (the table columns).
For this reason, our evaluation focuses on the in-
duced table of entities, rather than the clustering of
mention strings. The best coreference systems de-
pend on carefully crafted, problem-specific linguis-
tic features (Bengtson and Roth, 2008) and exter-
nal knowledge (Haghighi and Klein, 2010b). Future
work might consider how to exploit such features for
the more holistic information extraction setting.
9 Conclusion
This paper presents a Bayesian nonparametric ap-
proach to recover structured records from text. Us-
ing only a small set of prototype records, we are able
to recover an accurate table that jointly identifies en-
tities and internal name structure. In our view, the
main advantage of a Bayesian approach compared
to more heuristic alternatives is that it facilitates in-
corporation of additional information sources when
available. In this paper, we have considered one
such additional source, incorporating temporal con-
text using the recurrent Chinese restaurant process.
We envision enhancing the model in several other
respects. One promising direction is the incorpo-
ration of name structure, which could be captured
using a first-order Markov model of the transitions
between name parts. In the nonparametric setting,
a transition matrix is unbounded along both dimen-
sions, and this can be handled by a hierarchical
Dirichlet process (HDP; Teh et al2006).4 We en-
vision other potential applications of the HDP: for
example, learning ?topics? of entities which tend to
appear together (i.e., given a mention of Mahmoud
Abbas in the American press, a mention of Ben-
jamin Netanyahu is likely), and handling document-
specific burstiness (i.e., given that an entity is men-
tioned once in a document, it is much more likely
to be mentioned again). Finally, we would like
to incorporate lexical context from the sentences in
which each entity is mentioned, which might help to
distinguish, say, computer science researchers who
share names with former defense secretaries or pro-
fessional basketball players.
Acknowledgments This research was enabled
by AFOSR FA95501010247, DARPA grant
N10AP20042, ONR N000140910758, NSF DBI-
0546594, IIS-0713379, IIS-0915187, IIS-0811562,
an Alfred P. Sloan Fellowship, and Google?s support
of the Worldly Knowledge project at CMU. We
thank the reviewers for their thoughtful feedback.
4One of the reviewers proposed to draw entire column se-
quences from a Dirichlet process. Given the relatively small
number of columns and canonical name forms, this may be a
straightforward and effective alternative to the HDP.
10
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
restaurant process with applications to evolutionary
clustering. In International Conference on Data Min-
ing.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: establishing a
benchmark for the web people search task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations, SemEval ?07, pages 64?69. Associa-
tion for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 294?303, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Indrajit Bhattacharya and Lise Getoor. 2007. Collec-
tive entity resolution in relational data. ACM Trans.
Knowl. Discov. Data, 1(1), March.
Mikhail Bilenko, William W. Cohen, Stephen Fien-
berg, Raymond J. Mooney, and Pradeep Ravikumar.
2003. Adaptive name-matching in information in-
tegration. IEEE Intelligent Systems, 18(5):16?23,
September/October.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Sixth
Workshop on Very Large Corpora New Brunswick,
New Jersey. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of ACL.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In Proceedings of the
Second Meeting of the North American Chapter of the
Association for Computational Linguistics.
William W. Cohen, Henry Kautz, and David McAllester.
2000. Hardening soft information sources. In Pro-
ceedings of the Sixth International Conference on
Knowledge Discovery and Data Mining, pages 255?
259.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program: Tasks, data, and evaluation. In 4th
international conference on language resources and
evaluation (LREC?04).
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008
political blog corpus. Technical report, Carnegie Mel-
lon University.
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan,
and Dongwon Lee. 2007. Psnus: Web people name
disambiguation by simple clustering with rich features.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 268?
271, Prague, Czech Republic, June. Association for
Computational Linguistics.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 164?172, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
I. P. Felligi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Society,
64:1183?1210.
Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010a. An entity-level
approach to information extraction. In Proceedings
of the ACL 2010 Conference Short Papers, ACLShort
?10, pages 291?295, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010b. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393, Los An-
geles, California, June. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Nicholas Kushmerick, Daniel S. Weld, and Robert
Doorenbos. 1997. Wrapper induction for information
extraction. In Proceedings of IJCAI.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In Proceedings of AAAI, pages
419?424.
11
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-Based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 573?581, Los Angeles, California,
June. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track. In
Proceedings of the Text Analysis Conference (TAC).
Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Rus-
sell, and Ilya Shpitser. 2002. Identity uncertainty and
citation matching. In Advances in Neural Processing
Systems 15, Vancouver, British Columbia. MIT Press.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Carl E. Rasmussen. 2000. The Infinite Gaussian Mixture
Model. In In Advances in Neural Information Process-
ing Systems 12, volume 12, pages 554?560.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information ex-
traction. In Lawrence K. Saul, Yair Weiss, and Le?on
Bottou, editors, Advances in Neural Information Pro-
cessing Systems 17, pages 1185?1192. MIT Press,
Cambridge, MA.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
Yee W. Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581, December.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the Sixth
Conference on Natural Language Learning.
Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo Implementation of the EM Algorithm and the
Poor Man?s Data Augmentation Algorithms. Journal
of the American Statistical Association, 85(411):699?
704.
12
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 64?71,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Bilingual POS Tagging with Markov Random Fields
Desai Chen Chris Dyer Shay B. Cohen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
desaic@andrew.cmu.edu, {cdyer,scohen,nasmith}@cs.cmu.edu
Abstract
In this paper, we give a treatment to the prob-
lem of bilingual part-of-speech induction with
parallel data. We demonstrate that na??ve op-
timization of log-likelihood with joint MRFs
suffers from a severe problem of local max-
ima, and suggest an alternative ? using con-
trastive estimation for estimation of the pa-
rameters. Our experiments show that estimat-
ing the parameters this way, using overlapping
features with joint MRFs performs better than
previous work on the 1984 dataset.
1 Introduction
This paper considers unsupervised learning of lin-
guistic structure?specifically, parts of speech?in
parallel text data. This setting, and more gener-
ally the multilingual learning scenario, has been
found advantageous for a variety of unsupervised
NLP tasks (Snyder et al, 2008; Cohen and Smith,
2010; Berg-Kirkpatrick et al, 2010; Das and Petrov,
2011).
We consider globally normalized Markov random
fields (MRFs) as an alternative to directed models
based on multinomial distributions or locally nor-
malized log-linear distributions. This alternate pa-
rameterization allows us to introduce correlated fea-
tures that, at least in principle, depend on any parts
of the hidden structure. Such models, sometimes
called ?undirected,? are widespread in supervised
NLP; the most notable instances are conditional ran-
dom fields (Lafferty et al, 2001), which have en-
abled rich feature engineering to incorporate knowl-
edge and improve performance. We conjecture that
the ?features view? of NLP problems is also more
appropriate in unsupervised settings than the con-
trived, acyclic causal stories required by directed
models. Indeed, as we will discuss below, previous
work on multilingual POS induction has had to re-
sort to objectionable independence assumptions to
avoid introducing cyclic dependencies in the causal
network.
While undirected models are formally attractive,
they are computationally demanding, particularly
when they are used generatively, i.e., as joint dis-
tributions over input and output spaces. Inference
and learning algorithms for these models are usually
intractable on realistic datasets, so we must resort to
approximations. Our emphasis here is primarily on
the machinery required to support overlapping fea-
tures, not on weakening independence assumptions,
although we weaken them slightly. Specifically, our
parameterization permits us to model the relation-
ship between aligned words in any configuration,
rather than just those that conform to an acyclic gen-
erative process, as previous work in this area has
done (?2). We incorporate word prefix and suffix
features (up to four characters) in an undirected ver-
sion of a model designed by Snyder et al (2008).
Our experiments suggest that feature-based MRFs
offer advantages over the previous approach.
2 Related Work
The task of unsupervised bilingual POS induction
was originally suggested and explored by Snyder et
al. (2008). Their work proposes a joint model over
pairs of tag sequences and words that can be under-
stood as a pair of hidden Markov models (HMMs)
64
in which aligned words share states (a fixed and
observable word alignment is assumed). Figure 1
gives an example for a French-English sentence pair.
Following Goldwater and Griffiths (2007), the tran-
sition, emission and coupling parameters are gov-
erned by Dirichlet priors, and a token-level col-
lapsed Gibbs sampler is used for inference. The hy-
perparameters of the prior distributions are inferred
from data in an empirical Bayesian fashion.
  
Why repeat that catastrophe ?
Pourquoi r?p?ter la m?me ?catastrophe
x1/y1 X2/y2 y3 y4 x5/y6x4/y5
x3
Figure 1: Bilingual Directed POS induction model
When word alignments are monotonic (i.e., there
are no crossing links in the alignment graph), the
model of Snyder et al is straightforward to con-
struct. However, crossing alignment links pose a
problem: they induce cycles in the tag sequence
graph, which corresponds to an ill-defined probabil-
ity model. Their solution is to eliminate such align-
ment pairs (their algorithm for doing so is discussed
below). Unfortunately, this is a potentially a seri-
ous loss of information. Crossing alignments often
correspond to systematic word order differences be-
tween languages (e.g., SVO vs. SOV languages). As
such, leaving them out prevents useful information
about entire subsets of POS types from exploiting of
bilingual context.
In the monolingual setting, Smith and Eisner
(2005) showed similarly that a POS induction model
can be improved with spelling features (prefixes and
suffixes of words), and Haghighi and Klein (2006)
describe an MRF-based monolingual POS induction
model that uses features. An example of such a
monolingual model is shown in Figure 2. Both pa-
pers developed different approximations of the com-
putationally expensive partition function. Haghighi
and Klein (2006) approximated by ignoring all sen-
tences of length greater than some maximum, and
the ?contrastive estimation? of Smith and Eisner
(2005) approximates the partition function with a set
Eco
nom
ic
dis
cre
pan
cie
s
A
N
areV
gro
win
g
V
Figure 2: Monolingual MRF tag model (Haghighi
and Klein, 2006)
of automatically distorted training examples which
are compactly represented in WFSTs.
Das and Petrov (2011) also consider the prob-
lem of unsupervised bilingual POS induction. They
make use of independent conventional HMM mono-
lingual tagging models that are parameterized with
feature-rich log-linear models (Berg-Kirkpatrick et
al., 2010). However, training is constrained with tag
dictionaries inferred using bilingual contexts derived
from aligned parallel data. In this way, the complex
inference and modeling challenges associated with a
bilingual tagging model are avoided.
Finally, multilingual POS induction has also been
considered without using parallel data. Cohen et al
(2011) present a multilingual estimation technique
for part-of-speech tagging (and grammar induction),
where the lack of parallel data is compensated by
the use of labeled data for some languages and unla-
beled data for other languages.
3 Model
Our model is a Markov random field whose ran-
dom variables correspond to words in two parallel
sentences and POS tags for those words. Let s =
?s1, . . . , sNs? and t = ?t1, . . . , tNt? denote the two
word sequences; these correspond to Ns + Nt ob-
served random variables.1 Let x and y denote the se-
quences of POS tags for s and t, respectively. These
are the hidden variables whose values we seek to in-
fer. We assume that a word alignment is provided for
the sentences. Let A ? {1, . . . , Ns} ? {1, . . . Nt}
denote the word correspondences specified by the
alignment. The MRF?s unnormalized probability S
1We use ?source? and ?target? but the two are completely
symmetric in our undirected framework.
65
assigns:
S(s, t,x,y | A,w) =
expw>
(
Ns?
i=1
fs-emit(si, xi) +
Ns?
i=2
fs-tran(xi?1, xi)
+
Nt?
i=1
ft-emit(ti, yi) +
Nt?
i=2
ft-tran(yi?1, yi)
+
?
(i,j)?A
falign-POS(xi, yj)
?
?
where w is a numerical vector of feature weights
that parameterizes the model. Each f? corre-
sponds to features on pairs of random variables;
a source POS tag and word, two adjacent source
POS tags, similarly for the target side, and aligned
source/target POS pairs. For simplicity, we let f de-
note the sum of these five feature vectors. (In most
settings, each feature/coordinate will be specific to
one of the five addends.) In this paper, the features
are indicators for each possible value of the pair of
random variables, plus prefix and suffix features for
words (up to four characters). These features encode
information similar to the Bayesian bilingual HMM
discussed in ?2. Future work might explore exten-
sions to this basic feature set.
The marginal probability of the words is given by:
p(s, t | A,w) =
?
x,y S(x,y, s, t | A,w)
?
s?,t?
?
x,y S(s
?, t?,x,y | A,w)
.
Maximum likelihood estimation would choose
weights w to optimize a product of quantities like
the above, across the training data.
A key advantage of this representation is that any
alignments may be present. In directed models,
crossing links create forbidden cycles in the graph-
ical model. For example, Figure 3 shows a cross-
ing link between ?Economic discrepancies? and ?di-
vergences economiques.? Snyder et al (2008) dealt
with this problem by deleting word correspondences
that created cycles. The authors deleted crossing
links by considering each alignment link in the order
of the source sentence, deleting it if it crossed pre-
vious links. Deleting crossing links removes some
information about word correspondence.
divergences
?conomiques
Eco
nom
ic
disc
rep
anc
ies
N
A
A
N
Les ART
vont areV V
croissant gro
win
g
V V
Figure 3: Bilingual tag model.
4 Inference and Parameter Learning
When using traditional generative models, such as
hidden Markov models, the unsupervised setting
lends itself well to maximizing joint log-likelihood,
leading to a model that performs well (Snyder et
al., 2008). However, as we show in the following
analysis, maximizing joint log-likelihood for a joint
Markov random field with arbitrary features suffers
from serious issues which are related to the com-
plexity of the optimized objective surface.
4.1 MLE with Gradient Descent
For notational simplicity, we assume a single pair of
sentences s and t; generalizing to multiple training
instances is straightforward. The marginalized log-
likelihood of the data given w is
L(w) = log p(s, t | w)
= log
?
x,y S(x,y, s, t | w)
?
s?,t?
?
x,y S(x,y, s
?, t? | w)
.
In general, maximizing marginalized log-
likelihood is a non-concave optimization problem.
Iterative hill-climbing methods (e.g., expectation-
maximization and gradient-based optimization) will
lead only to local maxima, and these may be quite
shallow. Our analysis suggests that the problem
is exacerbated when we move from directed to
undirected models. We next describe a simple
experiment that gives insight into the problem.
We created a small synthetic monolingual data set
for sequence labeling. Our synthetic data consists of
the following five sequences of observations: {(0 1 2
3) , (1 2 3 0) , (2 3 0 1) , (3 0 1 2) , (0 1 2 3)}. We then
66
maximized the marginalized log-likelihood for two
models: a hidden Markov model and an MRF. Both
use the same set features, only the MRF is globally
normalized. The number of hidden states in both
models is 4.
The global maximium in both cases would be
achieved when the emission probabilities (or feature
weights, in the case of MRF) map each observation
symbol to a single state. When we tested whether
this happens in practice, we noticed that it indeed
happens for hidden Markov models. The MRF, how-
ever, tended to use fewer than four tags in the emis-
sion feature weights, i.e., for half of the tags, all
emission feature weights were close to 0. This ef-
fect also appeared in our real data experiments.
The reason for this problem with the MRF, we be-
lieve, is that the parameter space of the MRF is un-
derconstrained. HMMs locally normalize the emis-
sion probabilities, which implies that a tag cannot
?disappear??a total probability mass of 1 must al-
ways be allocated to the observation symbols. With
MRFs, however, there is no such constraint. Fur-
ther, effective deletion of a state y requires zeroing
out transition probabilities from all other states to
y, a large number of parameters that are completely
decoupled within the model.
  Wh Wy rh ry yh yy eh ey ph py ah ayhthrheh
ahchhcthcrh
ceh p e y r W
(a) likelihood
  WhyyWhyr WhyeWhypWhyaWhyt WhycWhysWhyo e p a tWrWpWtW
sWyWWyrWypW
ytWysW a p?c p?ae?a
(b) contrastive objective
Figure 4: Histograms of local optima found by opti-
mizing the length neighborhood objective (a) and the
contrastive objective (b) on a synthetic dataset with
8 sentences of length 7. The weights are initialized
uniformly at random in the interval [?1, 1]. We plot
frequency versus negated log-likelihood (lower hor-
izontal values are better). An HMM always finds a
solution that uses all available tags. The numbers at
the top are numbers of tags used by each local opti-
mum.
Our bilingual model is more complex than the
above example, and we found in preliminary exper-
iments that the effect persists there, as well. In the
following section, we propose a remedy to this prob-
lem based on contrastive estimation (Smith and Eis-
ner, 2005).
4.2 Contrastive Estimation
Contrastive estimation maximizes a modified ver-
sion of the log-likelihood. In the modified version,
it is the normalization constant of the log-likelihood
that changes: it is limited to a sum over possible ele-
ments in a neighborhood of the observed instances.
More specifically, in our bilingual tagging model,
we would define a neighborhood function for sen-
tences, N(s, t) which maps a pair of sentences to
a set of pairs of sentences. Using this neighborhood
function, we maximize the following objective func-
tion:
Lce(w)
= log p(S = s,T = t | S ? N1(s),T ? N2(t),w)
= log
?
x,y S(s, t,x,y | w)
?
s?,t??N(s,t)
?
x,y
S(s?, t?,x,y | w).
(1)
We define the neighborhood function using
a cross-product of monolingual neighborhoods:
N(s, t) = N1(s) ? N1(t). N1 is the ?dynasearch?
neighborhood function (Potts and van de Velde,
1995; Congram et al, 2002), used for contrastive
estimation previously by Smith (2006). This neigh-
borhood defines a subset of permutations of a se-
quence s, based on local transpositions. Specifically,
a permutation of s is in N1(s) if it can be derived
from s through swaps of any adjacent pairs of words,
with the constraint that each word only be moved
once. This neighborhood can be compactly repre-
sented with a finite-state machine of size O(Ns) but
encodes a number of sequences equal to the Nsth
Fibonacci number.
Monolingual Analysis To show that contrastive
estimation indeed gives a remedy to the local max-
imum problem, we return to the monolingual syn-
thetic data example from ?4.1 and apply contrastive
estimation on this problem. The neighborhood we
use is the dynasearch neighborhood. In Figure 4b
67
we compare the maxima identified using MLE with
the monolingual MRF model to the maxima identi-
fied by contrastive estimation. The results are con-
clusive: MLE tends to get stuck much more often in
local maxima than contrastive estimation.
Following an analysis of the feature weights
found by contrastive estimation, we found that con-
trastive estimation puts more weight on the transi-
tion features than emission features, i.e., the tran-
sition features weights have larger absolute values
than emission feature weights. We believe that this
could explain why contrastive estimation finds better
local maximum that plain MLE, but we leave explo-
ration of this effect for future work.
It is interesting to note that even though the con-
trastive objective tends to use more tags available in
the dictionary than the likelihood objective does, the
maximum objective that we were able to find does
not correspond to the tagging that uses all available
tags, unlike with HMM, where the maximum that
achieved highest likelihood also uses all available
tags.
4.3 Optimizing the Contrastive Objective
To optimize the objective in Eq. 1 we use a generic
optimization technique based on the gradient. Using
the chain rule for derivatives, we can derive the par-
tial derivative of the log-likelihood with respect to a
weight wi:
?Lce(w)
?wi
= Ep(X,Y|s,t,w)[fi]
? Ep(S,T,X,Y|S?N1(s),T?N1(t),w)[fi]
The second term corresponds to a computationally
expensive inference problem, because of the loops
in the graphical model. This situation is differ-
ent from previous work on linear chain-structured
MRFs (Smith and Eisner, 2005; Haghighi and Klein,
2006), where exact inference is possible. To over-
come this problem, we use Gibbs sampling to obtain
the two expectations needed by the gradient. This
technique is closely related to methods like stochas-
tic expectation-maximization (Andrieu et al, 2003)
and to contrastive divergence (Hinton, 2000).
The training algorithm iterates between sam-
pling part-of-speech tags and sampling permutations
of words to compute the expected value of fea-
tures. To sample permutations, the sampler iterates
through the sentences and decides, for each sen-
tence, whether to swap a pair of adjacent tags and
words or not. The Markov blanket for computing
the probability of swapping a pair of tags and words
is shown in Figure 5. We run the algorithm for a
fixed number (50) of iterations. By testing on a de-
velopment set, we observed that the accuracy may
increase after 50 iterations, but we chose this small
number of iterations for speed.
  
N A
divergences ?conomiques
A N
Economic discrepancies
V
vont 
  
N A
divergences ?conomiques
A N
Economic discrepancies
AVt
?s 
?
von? 
are
?
Figure 5: Markov blanket of a tag (left) and of a pair
of adjacent tags and words (right).
In preliminary experiments we considered
stochastic gradient descent, with online updating.
We found this led to low-accuracy local optima,
and opted for gradient descent with batch updates
in our implementation. The step size was chosen to
limit the maximum absolute value of the update in
any weight to 0.1. Preliminary experiments showed
only harmful effects from regularization, so we did
not use it. These issues deserve further analysis and
experimentation in future research.
5 Experiments
We next describe experiments using our undirected
model to unsupervisedly learn POS tags.
With unsupervised part-of-speech tagging, it is
common practice to use a full or partial dictionary
that maps words to possible part-of-speech tags. The
goal of the learner is then to discern which tag a
word should take among the tags available for that
word. Indeed, in all of our experiments we make
use of a tag dictionary. We consider both a com-
plete tag dictionary, where all of the POS tags for all
words in the data are known,2 and a smaller tag dic-
tionary that only provides possible tags for the 100
2Of course, additional POS tags may be possible for a given
word that were not in evidence in our finite dataset.
68
most frequent words in each language, leaving the
other words completely ambiguous. The former dic-
tionary makes the problem easier by reducing ambi-
guity; it also speeds up inference.
Our experiments focus on the Orwell novel 1984
dataset for our experiments, the same data used by
Snyder et al (2008). It consists of parallel text of
the 1984 novel in English, Bulgarian, Slovene and
Serbian (Erjavec, 2004), totalling 5,969 sentences in
each language. The 1984 datset uses fourteen part-
of-speech tags, two of which denote punctuation.
The tag sets for English and other languages have
minor differences in determiners and particles.
We use the last 25% of sentences in the dataset
as a test set, following previous work. The dataset
is manually annotated with part-of-speech tags. We
use automatically induced word alignments using
Giza++ (Och and Ney, 2003). The data show very
regular patterns of tags that are aligned together:
words with the same tag in two languages tend to
be aligned with each other.
When a complete tag dictionary derived from the
Slavic language data is available, the level of ambi-
guity is very low. The baseline of choosing random
tags for each word gives an accuracy in the low 80s.
For English, we use an extended tag dictionary built
from the Wall Street Journal and the 1984 data. The
English tag dictionary is much more ambiguous be-
cause it is obtained from a much larger dataset. The
random baseline gives an accuracy of around 56%.
(See Table 1.)
In our first set of experiments (?5.1), we perform
a ?sanity check? with a monolingual version of the
MRF that we described in earlier sections. We com-
pare it against plain HMM to assure that the MRFs
behave well in the unsupervised setting.
In our second set of experiments (?5.2), we com-
pare the bilingual HMM model from Snyder et al
(2008) to the joint MRF model. We show that using
an MRF has an advantage over an HMM model in
the partial tag dictionary setting.
5.1 Monolingual Experiments
We turn now to two monolingual experiments that
verify our model?s suitability for the tagging prob-
lem.
Language Random HMM MRF
Bulgarian 82.7 88.9 93.5
English 56.2 90.7 87.0
Serbian 83.4 85.1 89.3
Slovene 84.7 87.4 94.5
Table 1: Unsupervised monolingual tagging accura-
cies with complete tag dictionary on 1984 data.
Supervised Learning As a very primitive com-
parison, we trained a monolingual supervised MRF
model to compare to the results of supervised
HMMs. The training procedure is based on sam-
pling, just like the unsupervised estimation method
described in ?4.3. The only difference is that there is
no need to sample the words because the tags are the
only random variables to be marginalized over. Our
model and HMM give very close performance with
difference in accuracy less than 0.1%. This shows
that the MRF is capable of representing an equiva-
lent model represented by the HMM. It also shows
that gradient descent with MCMC approximate in-
ference is capable of finding a good model with the
weights initialized to all 0s.
Unsupervised Learning We trained our model
under the monolingual setting as a sanity check for
our approximate training algorithm. Our model un-
der monolingual mode is exactly the same as the
models introduced in ?2. We ran our model on the
1984 data with the complete tag dictionary. A com-
parison between our result and monolingual directed
model is shown in Table 1. ?Random? is obtained by
choosing a random tag for each word according to
the tag dictionary. ?HMM? is a Bayesian HMM im-
plemented by (Snyder et al, 2008). We also imple-
mented a basic (non-Bayesian) HMM. We trained
the HMM with EM and obtained rsults similar to the
Bayesian HMM (not shown).
5.2 Billingual Results
Table 2 gives the full results in the bilingual setting
for the 1984 dataset with a partial tag dictionary. In
general, MRFs do better than their directed counter-
parts, the HMMs. Interestingly enough, removing
crossing links from the data has only a slight adverse
effect. It appears like the prefix and suffix features
are more important than having crossing links. Re-
69
Language pair HMM MRF MRF w/o cross. MRF w/o spell.
English 71.3 73.3? 0.6 73.4? 0.6 67.4? 0.9
Bulgarian 62.6 62.3? 0.3 63.8? 0.4 55.2? 0.5
Serbian 54.1 55.7? 0.2 54.6? 0.3 47.7? 0.5
Slovene 59.7 61.4? 0.3 60.4? 0.3 56.7? 0.4
English 66.5 73.3? 0.3 73.4? 0.2 62.3? 0.5
Slovene 53.8 59.7? 2.5 57.6? 2.0 52.1? 1.3
Bulgarian 54.2 58.1? 0.1 56.3? 1.3 58.0? 0.2
Serbian 56.9 58.6? 0.3 59.0? 1.2 55.1? 0.3
English 68.2 72.8? 0.6 72.7? 0.6 65.7? 0.4
Serbian 54.7 58.5? 0.6 57.7? 0.3 54.2? 0.3
Bulgarian 55.9 59.8? 0.1 60.3? 0.5 55.0? 0.4
Slovene 58.5 61.4? 0.3 61.6? 0.4 58.1? 0.6
Average 59.7 62.9 62.5 56.5
Table 2: Unsupervised bilingual tagging accuracies with tag dictionary only for the top 100 frequent words.
?HMM? is the result reported by (Snyder et al, 2008). ?MRF? is our contrastive model averaged over ten
runs. ?MRF w/o cross.? is our model trained without crossing links, like Snyder et al?s HMM. ?MRF
w/o spell.? is our model without prefix and suffix features. Numbers appearing next to results are standard
deviations over the ten runs.
Language w/ cross. w/o cross.
French 73.8 70.3
English 56.0 59.2
Table 3: Effect of removing crossing links when
learning French and English in a bilingual setting.
moving the prefix and suffix features gives substan-
tially lower results on average, results even below
plain HMMs.
The reason that crossing links do not change the
results much could be related to fact that most of
the sentence pairs in the 1984 dataset do not contain
many crossing links (only 5% of links cross another
link). To see whether crossing links do have an ef-
fect when they come in larger number, we tested our
model on French-English data. We aligned 10,000
sentences from the Europarl corpus (Koehn, 2005),
resulting in 87K crossing links out of a total of 673K
links. Using the Penn treebank (Marcus et al, 1993)
and the French treebank (Abeille? et al, 2003) to
evaluate the model, results are given in Table 3. It is
evident that crossing links have a larger effect here,
but it is mixed: crossing links improve performance
for French while harming it for English.
6 Conclusion
In this paper, we explored the capabilities of joint
MRFs for modeling bilingual part-of-speech mod-
els. Exact inference with dynamic programming is
not applicable, forcing us to experiment with ap-
proximate inference techniques. We demonstrated
that using contrastive estimation together with Gibbs
sampling for the calculation of the gradient of the
objective function leads to better results in unsuper-
vised bilingual POS induction.
Our experiments also show that the advantage of
using MRFs does not necessarily come from the fact
that we can use non-monotonic alignments in our
model, but instead from the ability to use overlap-
ping features such as prefix and suffix features for
the vocabulary in the data.
Acknowledgments
We thank the reviewers and members of the ARK
group for helpful comments on this work. This re-
search was supported in part by the NSF through
grant IIS-0915187 and the U. S. Army Research
Laboratory and the U. S. Army Research Office un-
der contract/grant number W911NF-10-1-0533.
70
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for French. In A. Abeille?, editor, Treebanks.
Kluwer, Dordrecht.
C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.
2003. An introduction to MCMC for machine learn-
ing. Machine Learning, 50:5?43.
T. Berg-Kirkpatrick, A. Bouchard-Cote, J. DeNero, and
D. Klein. 2010. Unsupervised learning with features.
In Proceedings of NAACL.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of EMNLP.
R. K. Congram, C. N. Potts, and S. L. van de Velde.
2002. An iterated Dynasearch algorithm for the
single-machine total weighted tardiness scheduling
problem. Informs Journal On Computing, 14(1):52?
67.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Procedings of ACL.
T. Erjavec. 2004. MULTEXT-East version 3: Multilin-
gual morphosyntactic specifications, lexicons and cor-
pora. In Proceedings of LREC.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
G. E. Hinton. 2000. Training products of experts by
minimizing contrastive divergence. Technical Report
GCNU TR 2000-004, University College London.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit 2005.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
C. N. Potts and S. L. van de Velde. 1995. Dynasearch?
iterative local improvement by dynamic programming.
Part I: The traveling salesman problem. Technical re-
port.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of EMNLP.
71
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 22?32,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Discovering Factions in the Computational Linguistics Community
Yanchuan Sim Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ysim,nasmith}@cs.cmu.edu
David A. Smith
Department of Computer Science
University of Massachusetts
Amherst, MA 01003, USA
dasmith@cs.umass.edu
Abstract
We present a joint probabilistic model of who
cites whom in computational linguistics, and
also of the words they use to do the citing. The
model reveals latent factions, or groups of in-
dividuals whom we expect to collaborate more
closely within their faction, cite within the fac-
tion using language distinct from citation out-
side the faction, and be largely understandable
through the language used when cited from
without. We conduct an exploratory data anal-
ysis on the ACL Anthology. We extend the
model to reveal changes in some authors? fac-
tion memberships over time.
1 Introduction
The ACL Anthology presents an excellent dataset
for studying both the language and the social con-
nections in our evolving research field. Extensive
studies using techniques from the field of biblio-
metrics have been applied to this dataset (Radev et
al., 2009a), quantifying the importance and impact
factor of both authors and articles in the commu-
nity. Moreover, recent work has leveraged the avail-
ability of digitized publications to study trends and
influences within the ACL community (Hall et al,
2008; Gerrish and Blei, 2010; Yogatama et al, 2011)
and to analyze academic collaborations (Johri et al,
2011).
To the best of our knowledge, however, existing
work has mainly pursued ?macroscopic? investiga-
tions of the interaction of authors in collaboration,
citation networks, or the textual content of whole
papers. We seek to complement these results with a
?microscopic? investigation of authors? interactions
by considering the individual sentences authors use
to cite each other.
In this paper, we present a joint model of who
cites whom in computational linguistics, and also of
how they do the citing. Central to this model is the
idea of factions, or groups of individuals whom we
expect to (i) collaborate more closely within their
faction, (ii) cite within the faction using language
distinct from citation outside the faction, (iii) be
largely understandable through the language used
when cited from without, and (iv) evolve over time.
1
Factions can be thought of as ?communities,? which
are loosely defined in the literature on networks
as subgraphs where internal connections are denser
than external ones (Radicchi et al, 2004). The dis-
tinction here is that the strength of connections de-
pends on a latent language model estimated from ci-
tation contexts.
This paper is an exploratory data analysis using a
Bayesian generative model. We aim both to discover
meaningful factions in the ACL community and also
to illustrate the use of a probabilistic model for such
discovery. As such, we do not present any objective
evaluation of the model or make any claims that the
factions optimally explain the research community.
Indeed, we suspect that reaching a broad consensus
among community members about factions (i.e., a
?gold standard?) would be quite difficult, as any so-
cial community?s factions are likely perceived very
1
Our factions are computational abstractions?clusters of
authors?discovered entirely from the corpus. We do not claim
that factions are especially contentious, any more than ?sub-
communities? in social networks are especially collegial.
22
subjectively. It is for this reason that a probabilistic
generative model, in which all assumptions are made
plain, is appropriate for the task. We hope this analy-
sis will prove useful in future empirical research on
social communities (including scientific ones) and
their use of language.
2 Model
In this paper, our approach is a probabilistic model
over (i) coauthorship relations and (ii) the words
in sentences containing citations. The words are
assumed to be generated by a distribution that de-
pends on the (latent) faction memberships of the cit-
ing authors, the cited authors, and whether the au-
thors have coauthored before. To model these dif-
ferent effects on language, we use a sparse additive
generative (SAGE) model (Eisenstein et al, 2011).
In contrast to the popular Dirichlet-multinomial for
topic modeling, which directly models lexical prob-
abilities associated with each (latent) topic, SAGE
models the deviation in log frequencies from a back-
ground lexical distribution. Imposing a sparsity-
inducing prior on the deviation vectors limits the
number of terms whose probabilities diverge from
the background lexical frequencies, thereby increas-
ing robustness to limited training data. SAGE can be
used with or without latent topics; our model does
not include topics. Figure 1 shows the plate diagram
for our model.
We describe the generative process:
? Generate the multinomial distribution over fac-
tion memberships from a Dirichlet distribution:
? ? Dir(?).
? Generate the binomial distribution for whether
two authors coauthor, given that they are in the
same faction, from a Beta distribution: ?
same
?
Beta(?same0 , ?
same
1 ). Generate the analogous bi-
nomial, given that they are in different factions:
?
diff
? Beta(?diff0 , ?
diff
1 ).
? For each author i, draw a faction indicator
ai ? Multinomial(?).
? For all ordered pairs of factions (g, h), draw a
deviation vector ?
(g,h)
? Laplace(0, ?). This
vector, which will be sparse, corresponds to the
?
?
a
(i)
a
(j)
z
(i,j)
?
same
?
same
?
diff
?
diff
w
(i,j)
N
(i,j)
A?A
m
?
(g,h)
?
G?G
Figure 1: Plate diagram for our graphical model. A and
G are the fixed numbers of authors and factions, respec-
tively. m is the background word distribution, ?, ? , ?
are hyperparameters, a are latent author factions, z and
w are the observed coauthorship relations and observed
words in citation sentences between authors, respectively.
Each of the a
(i)
, denoting author i?s faction alignment,
are sampled once every iteration conditioned on all the
other a
(j)
. If i and j are coauthors or i cited j in some
publication, a
(i)
and a
(j)
will not be conditionally inde-
pendent due to the v-structure. ?
same
and ?
diff
are bino-
mial distributions over whether two authors have collab-
orated together before, given that they are assigned to the
same/different factions. Dashed variables are collapsed
out in the Gibbs sampler, while double bordered variables
are optimized in the M-step.
deviations in word log-frequencies when fac-
tion g is citing faction h.
? For each word v in the vocabulary, let the uni-
gram probability that an author in faction g uses
to cite an author in faction h be
?
(g,h)
v =
exp(?(g,h)v +mv)
?
v? exp(?
(g,h)
v? +mv?)
.
? For each ordered pair of authors (i, j),
? For each word that i uses to cite j, draw
w
(i,j)
k ? Multinomial(?
(a(i),a(j))).
? If the authors are from the same faction,
i.e., a
(i) = a(j), draw coauthorship indi-
23
cator z
(i,j)
? Binomial(?same); else, draw
z
(i,j)
? Binomial(?diff).
Thus, our goal is to maximize the conditional like-
lihood of the observed data
p(w, z | ?,?, ?,m,?) =
?
?
?
?
?
a
p(w, z,?,?,a | ?,?, ?,m,?)
with respect to ? and ?. We fix ? and ?, which are
hyperparameters that encode our prior beliefs, and
m, which we assume to be a fixed background word
distribution.
Exact inference in this model is intractable, so we
resort to an approximate inference technique based
on Markov Chain Monte Carlo simulation. We per-
form Bayesian inference over the latent author fac-
tions while using maximum a posteriori estimates
of ? because Bayesian inference of ? is problematic
due to the logistic transformation. We refer the in-
terested reader to Eisenstein et al (2011). We take
an empirical Bayes approach to setting the hyper-
parameter ?. Our overall learning procedure is a
Monte Carlo Expectation Maximization algorithm
(Wei and Tanner, 1990).
3 Learning and Inference
Our learning algorithm is a two-step iterative pro-
cedure. During the E-step, we perform collapsed
Gibbs sampling to obtain distributions over factions
for each author, given the current setting of the hy-
perparameters. In the M-step, we obtain point es-
timates for the hyperparameters ? and ? given the
current posterior distributions for the author fac-
tions.
3.1 E-step
As the Dirichlet and Beta distributions are conjugate
priors to the multinomial and binomial respectively,
we can integrate out the latent variables ?, ?
(same)
and ?
(diff)
. For an author i, we sample his faction
alignment a
(i)
conditioned on faction assignments
to all other authors and citation words between i and
other authors (in both directions). Denoting a
?i
as
the current faction assignments for all the authors
except i,
p(a(i) = g | a(?i),w,?,?,?)
? p(a(i) = g,a(?i),w | ?,?,?)
? (Ng + ?g)
A?
j
?

z +N

z
?

0 + ?

1 +N

0 +N

1
p(w(i) | ?)
where Ng is the number of authors (except i) who
are assigned to faction g, ij = ?same? if g = a(j)
and ij = ?diff? otherwise, and N 1, N

0 denotes
the number of author pairs that have/have not coau-
thored before respectively, given the status of their
factions . We elide the subscripts of  and super-
script of z for notational simplicity and abuse nota-
tion to let w
(i)
refer to all author i?s citation words,
both incoming and outgoing. Using SAGE, the fac-
tor for an author?s words is
p(w(i) | ?) =
?
j
?
v
(
?
(g,a(j))
v
)w(i,j)v (
?
(a(j),g)
v
)w(j,i)v
where w
(i,j)
v is the observed count of the number of
times word v has been used when author i cites j; j
ranges over the A authors.
We sample each author?s faction in turn and do so
several times during the E-step, collecting samples
to estimate our posterior distribution over a.
3.2 M-step
In the M-step, we optimize all ?
(g,h)
and ? given
the posterior distribution over author factions.
Optimizing ?. Eisenstein et al (2011) postu-
lated that the components of ? are drawn from
a compound model
?
N (?;?, ?)E(?; ?)d?, where
E(?; ?) indicates the Exponential distribution. They
fit a variational distribution Q(?) and optimized the
log-likelihood of the data by iteratively fitting the
parameters ? using a Newton optimization step and
maximizing the variational bound.
The compound model described is equivalent to
the Laplace distribution L(?;?, ?) (Lange and Sin-
sheimer, 1993; Figueiredo, 2003). Moreover, a zero
mean Laplace prior has the same effect as placing an
L1 regularizer on ?. Therefore, we can equivalently
24
maximize the regularized likelihood
?c
(g,h)
?
T
?
(g,h)
? ?C
(g,h)
? log
?
v
exp(?(g,h)v +mv)
? ?
?
?
??
(g,h)
?
?
?
1
with respect to ?
(g,h)
. ?c
(g,h)
? is a vector of expected
count of the words that faction g used when citing
faction h, ?c
(g,h)
? =
?
v ?c
(g,h)
v ? and ? is the regu-
larization constant. The regularization constant and
Laplace variance are related by ? = ??1 (Tibshirani,
1996).
We use the gradient-based optimization routine
OWL-QN (Andrew and Gao, 2007) to maximize the
above objective function with respect to ?
(g,h)
for
each pair of factions g and h.
Optimizing ?. As in the empirical Bayes ap-
proach, we learn the hyperparameter setting of ?
from the data by maximizing the log likelihood
with respect to ?. By treating ? as the parame-
ter of a Dirichlet-multinomial compound distribu-
tion, we can directly use the samples of author fac-
tions produced by our Gibbs sampler to estimate
?. Minka (2009) describes in detail several itera-
tive approaches to estimate ?; we use the linear-
time Newton-Raphson iterative update to estimate
the components of ?.
4 Data Analysis
4.1 Dataset
We used the ACL Anthology Network Corpus
(Radev et al, 2009b), which currently contains
18,041 papers written by 12,777 authors. These pa-
pers are published in the field of computational lin-
guistics between 1965 and 2011.
2
Furthermore, the
corpus provides bibliographic data such as authors
of the papers and bibliographic references between
each paper in the corpus. We extracted sentences
containing citations using regular expressions and
linked them between authors with the help of meta-
data provided in the corpus.
We tokenized the extracted sentences and down-
cased them. Words that are numeric, appear less
2
For a list of the journals, conferences and workshops
archived by the ACL anthology, please visit http://
aclweb.org/anthology-new.
than 20 times, or are in a stop word list are dis-
carded. For papers with multiple authors, we divided
the word counts by the number of pairings between
authors in both papers, assigning each word to each
author-pair (i.e., a count of
1
nn? if a paper with n au-
thors cites a paper with n
?
authors).
Due to the large number of authors, we only used
the 500 most cited authors (within the corpus) who
have published at least 5 papers. Papers with no au-
thors left are removed from the dataset. As a re-
sult, we have 8,144 papers containing 80,776 cita-
tion sentences (31,659 citation pairs). After text pro-
cessing, there are 391,711 tokens and 3,037 word
types.
In each iteration of the EM algorithm, we run the
E-step Gibbs sampler for 300 iterations, discarding
the first 100 samples for burn-in and collecting sam-
ples at every 3rd iteration to avoid autocorrelation.
At the M-step, we update our ? and ? using the
samples collected. We run the model for 100 EM
iterations.
We fixed ? = 5, ?same = (0.5, 1) and ?diff =
(1, 0.5). Our setting of ? reflects our prior beliefs
that coauthors tend to be from the same faction.
4.2 Factions in ACL (1965?2011)
We ran the model withG = 30 factions and selected
the most probable faction for each author from the
posterior distribution of the author-faction alignment
obtained in the final E step. Only 26 factions were
selected as most probable for some author.
3
Table 1
presents members of selected factions, along with
citation words that have the largest positive log fre-
quency deviation from the background distribution.
4
Table 2 shows a list of the top three authors associ-
ated with factions not shown in Table 1. Incoming
(outgoing) citation words are found by summing the
log deviation vectors ? across citing (cited) factions.
The author factions are manually labeled.
We see from Table 1, the model has selected key-
words that are arguably significant in certain sub-
fields in computational linguistics. Incoming cita-
tions are generally indicative of the subject areas in
3
In future work, nonparametric priors might be employed to
automate the selection of G.
4
We found it quite difficult to make sense of terms with neg-
ative log frequency deviations. This suggests exploring a model
allowing only positive deviations; we leave that for future work.
25
Formalisms (31) Fernando Pereira, Jason M. Eisner, Stuart M. Shieber, Walter Daelemans, Hitoshi Isa-
hara
Self cites: parsing
In cites: parsing, semiring, grammars, tags, grammar, tag, lexicalized, dependency
Out cites: tagger, regular, dependency, transformationbased, tagging, stochastic, grammars, sense
Evaluation (17) Salim Roukos, Eduard Hovy, Marti A. Hearst, Chin-Yew Lin, Dekang Lin
Self cites: automatic, bleu, linguistics, evaluation, computational, text, proceedings
In cites: automatic, bleu, segmentation, method, proceedings, dependency, parses, text
Out cites: paraphrases, cohesion, agreement, hierarchical, entropy, phrasebased, evaluation, tree-
bank
Semantics (26) Martha Palmer, Daniel Jurafsky, Mihai Surdeanu, David Weir, German Rigau
Self cites: sense, semantic, wordnet
In cites: framenet, sense, semantic, task, wordnet, word, project, question
Out cites: sense, wordnet, moses, preferences, distributional, semantic, focus, supersense
Machine Translation
(MT1) (9)
Kevin Knight, Michel Galley, Jonathan Graehl, Wei Wang, Sanjeev P. Khudanpur
Self cites: inference, scalable, model
In cites: scalable, inference, machine, training, generation, translation, model, syntaxbased
Out cites: phrasebased, hierarchical, inversion, forest, transduction, translation, ibm, discourse
Word Sense Disam-
biguation (WSD) (42)
David Yarowsky, Rada Mihalcea, Eneko Agirre, Ted Pedersen, Yorick Wilks
Self cites: sense, word
In cites: sense, preferences, wordnet, acquired, semcor, word, semantic, calle
Out cites: sense, subcategorization, acquisition, automatic, corpora, lexical, processing, wordnet
Parsing (20) Michael John Collins, Eugene Charniak, Mark Johnson, Stephen Clark, Massimiliano
Ciaramita
Self cites: parser, parsing, model, perceptron, parsers, dependency
In cites: parser, perceptron, supersense, parsing, dependency, results, hmm, models
Out cites: parsing, forest, treebank, model, coreference, stochastic, grammar, task
Discourse (29) Daniel Marcu, Aravind K. Joshi, Barbara J. Grosz, Marilyn A. Walker, Bonnie Lynn
Webber
Self cites: discourse, structure, centering
In cites: discourse, phrasebased, centering, tag, focus, rhetorical, tags, lexicalized
Out cites: discourse, rhetorical, framenet, realizer, tags, resolution, grammars, synonyms
Machine Translation
(MT2) (9)
Franz Josef Och, Hermann Ney, Mitchell P. Marcus, David Chiang, Dekai Wu
Self cites: training, error
In cites: error, giza, rate, alignment, training, minimum, translation, phrasebased
Out cites: forest, subcategorization, arabic, model, translation, machine, models, heuristic
Table 1: Key authors and citation words associated with some factions. For each faction, we show the 5 authors with
highest expected incoming citations (i.e p(faction | author) ? citations). Factions are labeled manually, referring to
key sub-fields in computational linguistics. Faction sizes are in parenthesis following the labels. The citation words
with the strongest positive weights in the deviation vectors are shown.
which the faction holds recognized expertise. For
instance, the faction labeled ?semantics? has cita-
tion terms commonly associated with propositional
semantics: sense, framenet, wordnet. On the other
hand, outgoing citations hint at the related work that
a faction builds on; discourse might require building
on components involving framenet, grammars, syn-
onyms, while word sense disambiguation involves
solving problems like acquisition and modeling sub-
categorization.
4.3 Sensitivity
Given the same initial parameters, we found our
model to be fairly stable across iterations of Monte
26
Adam Lopez, Paul S. Jacobs (2)
Regina Barzilay, Judith L. Klavans, Robert T. Kasper (3)
Lauri Karttunen, Kemal Oflazer, Kimmo Koskenniemi (3)
John Carroll, Ted Briscoe, Scott Miller (7)
Vincent J. Della Pietra, Stephen A. Della Pietra, Robert L.
Mercer (25)
Thorsten Brants, Liang Huang, Anoop Sarkar (9)
Christoph Tillmann, Kenji Yamada, Sharon Goldwater (7)
Alex Waibel, Keh-Jiann Chen, Katrin Kirchhoff (3)
Lynette Hirschman, Claire Cardie, Vincent Ng (26)
Erik F. Tjong Kim Sang, Ido Dagan, Marius Pas?ca (21)
Yuji Matsumoto, Dragomir R. Radev, Chew Lim Tan (18)
Christopher D. Manning, Owen Rambow, Ellen Riloff (19)
Richard Zens, Hieu Hoang, Nicola Bertoldi (9)
Dan Klein, Jun?ichi Tsujii, Yusuke Miyao (6)
Janyce Wiebe, Mirella Lapata, Kathleen R. McKeown (50)
I. Dan Melamed, Ryan McDonald, Joakim Nivre (10)
Philipp Koehn, Lillian Lee, Chris Callison-Burch (80)
Kenneth Ward Church, Eric Brill, Richard M. Schwartz
(19)
Table 2: Top 3 authors of the remaining 18 factions not
displayed in Table 1.
Carlo EM. We found that when G was too small
(e.g., 10), groups were more mixed and the ? vectors
could not capture variation among them well. When
G was larger, the factions were subjectively cleaner,
but fields like translation split into many factions (as
is visible in the G = 30 case illustrated in Tables 1
and 2. Strengthening the L1 penalty made ? more
sparse, of course, but gave less freedom in fitting the
data and therefore more grouping of authors into a
fewer effective factions.
4.4 Inter-Faction Relationships
By using the most probable a posteriori faction for
each author, we can compute the number of cita-
tions between factions. We define the average inter-
faction citations by:
IFC(g, h) =
?(g ? h) + ?(h? g)
Ng +Nh
(1)
where ?(g ? h) is the total number of papers writ-
ten by authors in g that cite papers written by authors
in h.
Figure 2 presents a graph of selected factions
and how these factions talk about each other. As
we would expect, the machine translation faction is
quite strongly connected to formalisms and parsing
factions, reflecting the heavy use of grammars and
Av
era
ge 
ou
t-c
ita
tio
n c
ou
nts
Formalisms
Evaluation
MT 1
Parsing
MT 2
Semantics
WSD
Discourse
Formalisms
Evaluation
MT 1
Parsing
MT 2
Semantics
WSD
Discourse
Figure 3: Heat map showing citation rates across selected
factions. Factions on the horizontal axis are being cited;
factions on the vertical axis are citing. Darker shades de-
note higher average
?(g?h)
Ng
.
parsing algorithms in translation. Moreover, we can
observe that ?deeper? linguistics research, such as
semantics and discourse, are less likely to be cited
by the other factions. This is reflected in Figure 3,
where the statistical MT and parsing factions in the
bottom left exhibit higher citation activity amongst
each other. In addition, we note that factions tend to
self-cite more often than out of their own factions;
this is unsurprising given the prior we selected.
The IFC between discourse and MT2 (as shown
by the edge thickness in figure 2) is higher than ex-
pected, given our prior knowledge of the computa-
tional linguistics community. Further investigation
revealed that, Daniel Marcu, posited by our model
to be a member of the discourse faction, has coau-
thored numerous highly cited papers in MT in re-
cent years (Marcu and Wong, 2002). However, the
model split the translation field, which fragmented
the counts of MT related citation words. Thus,
assigning Daniel Marcu to the discourse faction,
which also has a less diverse citation vocabulary, is
more probable than assigning him to one of the MT
factions. In ?4.6, we consider a model of factions
over time to mitigate this problem.
4.5 Comparison to Graph Clustering
Work in the field of bibliometrics has largely fo-
cused on using the link structure of citation net-
works to study higher level structures. See Osareh
(1996) for a review. Popular methods include bib-
liographic coupling (Kessler, 1963), and co-citation
27
Discourse
Formalisms
MT 2
Parsing
Semantics
Word Sense
Disambiguation
?
p
a
r
s
e
,
p
a
r
s
i
n
g
,
t
r
a
i
n
i
n
g
?
m
o
d
e
l
,
a
l
g
o
r
i
t
h
m
s
,
g
r
a
m
m
a
r
?
a
l
i
g
n
m
e
n
t
,
g
i
z
a
,
u
s
i
n
g
,
m
o
d
e
l
?
p
a
r
s
i
n
g
,
p
a
r
s
e
r
,
p
e
r
c
e
p
-
t
r
o
n
,
h
m
m
,
d
e
p
e
n
d
e
n
c
y
?alignment, giza, training
?phrase, model, joint,
translation, probability
?parsing
?parsing
?
p
r
e
f
e
r
e
n
c
e
s
,
s
e
n
s
e
,
w
o
r
d
-
n
e
t
,
a
c
q
u
i
r
e
d
,
s
e
m
c
o
r
?
s
e
n
s
e
,
s
e
m
a
n
t
i
c
,
l
e
x
i
c
a
l
,
w
o
r
d
n
e
t
,
d
i
s
a
m
b
i
g
u
a
t
i
o
n
?using, alignment,
giza, translation, model
?memory, judges,
voice, allow, sequences
?
t
a
g
s
,
l
e
x
i
c
a
l
i
z
e
d
,
g
r
a
m
-
m
a
r
s
,
a
d
j
o
i
n
i
n
g
,
t
r
e
e
s
?
t
a
g
s
,
g
r
a
m
m
a
r
s
,
l
e
x
i
c
a
l
-
i
z
e
d
,
s
y
n
c
h
r
o
n
o
u
s
,
f
o
r
m
a
l
i
s
m
?
s
u
p
e
r
s
e
n
s
e
,
r
e
s
u
l
t
s
,
w
o
r
d
n
e
t
,
p
a
r
s
i
n
g
,
p
e
r
c
e
p
t
r
o
n
?
t
a
s
k
,
i
n
f
o
r
m
a
t
i
o
n
Figure 2: Citations among some factions. The size of a node is relative to the faction size and edge thickness is relative
to the average number of inter-faction citations (equation 1). The words on the edges are the highest weighted words
from the deviation vectors ?, with the arrow denoting the direction of the citation. Edges with below average IFC
scores are represented as dashed lines, and their citations words are not shown to preserve readability.
analysis (Small, 1973). By using authors as an unit
of analysis in co-citation pairs, author co-citations
have been presented as a technique to analyze their
subject specialties (White and Griffith, 1981). Using
standard graph clustering algorithms on these author
co-citation networks, one can obtain a semblance of
author factions. Hence, we performed graph clus-
tering on both collaboration and citation graphs
5
of
authors in our dataset using Graclus
6
, a graph clus-
tering implementation based on normalized cuts and
ratio associations (Dhillon et al, 2004).
In Table 3, we compare, for selected authors,
how their faction-mates obtained by our model and
graph clustering differ. When clustering on the au-
thor collaboration network, we obtained some clus-
ters easily identified with research labs (e.g., Daniel
Marcu at the Information Sciences Institute). The
co-citation graph leads to groupings dominated by
5
We converted the directed citation graph into a symmetric
graph by performing bibliometric symmetrization described in
Satuluri and Parthasarathy (2011, section 3.3).
6http://www.cs.utexas.edu/users/dml/
Software/graclus.html
heavily co-cited papers in major research areas.
While we do not have an objective measurement
of quality or usefulness, we believe that the fac-
tions identified by our model align somewhat bet-
ter with familiar technical themes around which
sub-communities naturally form than major research
problems or institutions.
4.6 Factions over Time
Faction alignments may be dynamic; we expect that,
over time, individual researchers may move from
one faction to another as their interests evolve. We
consider a slightly modified model whereby authors
are split into different copies of themselves during a
non-overlapping set of discrete time periods. Given
a set of disjoint time periods T , we denote each
author-faction node by {a
(i,t)
| (i, t) ? A? T}. As
we treat each ?incarnation? of an author as a distinct
individual, we can simply use the same inference al-
gorithm described in ?2. (In future work we might
impose an expectation of gradual changes along a
more continuous representation of time.)
28
Our Model Collaboration Network Co-citation Network
Franz Josef Och
Franz Josef Och, Hermann Ney,
Mitchell P. Marcus, David Chiang,
Dekai Wu
Franz Josef Och, Hermann Ney, Richard
Zens, Stephan Vogel, Nicola Ueffing
Franz Josef Och, Hermann Ney, Vincent
J. Della Pietra, Daniel Marcu, Robert L.
Mercer
error, giza, rate, alignment, training giza, mert, popovic, moses, alignments giza, bleu, phrasebased, alignment, mert
Daniel Marcu
Daniel Marcu, Aravind K. Joshi, Bar-
bara J. Grosz, Marilyn A. Walker, Bon-
nie Lynn Webber
Daniel Marcu, Kevin Knight, Daniel
Gildea, David Chiang, Liang Huang
Franz Josef Och, Hermann Ney, Vincent
J. Della Pietra, Daniel Marcu, Robert L.
Mercer
discourse, phrasebased, centering, tag,
focus
phrasebased, forest, cube, spmt, hiero giza, bleu, phrasebased, alignment, mert
Michael John Collins
Eugene Charniak, Michael John Collins,
Mark Johnson, Stephen Clark, Massim-
iliano Ciaramita
Michael John Collins, Joakim Nivre,
Llu??s M?arquez, Xavier Carreras, Jan
Haji?c
Michael John Collins, Christopher D.
Manning, Dan Klein, Eugene Charniak,
Mark Johnson
parser, perceptron, supersense, parsing,
dependency
pseudoprojective, maltparser, percep-
tron, malt, averaged
tnt, prototypedriven, perceptron,
coarsetofine, pcfg
Kathleen R. McKeown
Mirella Lapata, Janyce Wiebe, Kathleen
R. McKeown, Dan Roth, Ralph Grish-
man
Kathleen R. McKeown, Regina Barzi-
lay, Owen Rambow, Marilyn A. Walker,
Srinivas Bangalore
Kenneth Ward Church, David
Yarowsky, Eduard Hovy, Kathleen
R. McKeown, Lillian Lee
semantic, work, learning, corpus, model centering, arabic, pyramid, realpro, cue rouge, minipar, nltk, alignment, mon-
treal
Table 3: Comparing selected factions between our model and graph clustering algorithms. Authors with highest
incoming citations are shown. For our model, we show the largest weighted words in the SAGE vector of incoming
citations for the faction, while for graph clustering, we show words with the highest tf-idf weight.
We split the same data as the earlier sections into
four disjoint time periods, 1965?1989, 1990?1999,
2000?2005 and 2006?2011. The split across time
is unequal due to the number of papers published in
each period: these four periods include 1,917, 3,874,
3,786, and 8,105 papers, respectively. Here we used
G = 20 factions for faster runtime, leading to di-
minished interpretability, though the sparsity of the
deviation vectors mitigates this problem somewhat.
Figure 4 shows graphical plots of selected authors
and their faction membership posteriors over time
(drawn from the final E-step).
With a simple extension of the original model,
we can learn shifts in the subject area the author is
publishing about. Consider Eugene Charniak: the
model observed a major change in faction align-
ment around 2000, when one of the popular Char-
niak parsers (Charniak, 2000) was released; this is
somewhat later than Charniak?s interests shifted, and
the earlier faction?s words are not clearly an ac-
curate description of his work at that time. More
fine-grained modeling of time and also accounting
for the death and birth of factions might ameliorate
these inconsistencies with our background knowl-
edge about Charniak. The model finds that Ar-
avind Joshi was associated with the tagging/parsing
faction in the 1990s and in recent years moved
back towards discourse (Prasad et al, 2008). David
Yarowsky, known for his early work on word sense
disambiguation, has since focused on applying word
sense disambiguation techniques in a multilingual
context (Garera et al, 2009; Bergsma et al, 2011).
As mentioned in the previous section, we observe
that the extended model is able to capture Daniel
Marcu?s shift from discourse-related work to MT
with his work in phrase-based statistical MT (Marcu
and Wong, 2002).
5 Related Work
A number of algorithms use topic modeling to an-
alyze the text in the articles. Topic models such
as latent Dirichlet alocation (Blei et al, 2003) and
its variations have been increasingly used to study
trends in scientific literature (McCallum et al, 2006;
Dietz et al, 2007; Hall et al, 2008; Gerrish and Blei,
2010), predict citation information (McNee et al,
29
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Eugene Charniak
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Aravind K. Joshi
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Daniel Marcu
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
David Y owsky
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Kathleen R. McKeown
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Michael J. Collins
 0
 0.2
 0.4
 0.6
 0.8
 1
1970-1989 1990-1999 2000-2005 2006-2011
year
Martha Palmer
 0
 0.2
 0.4
 0.6
 0.8
 1
1990-1999 2000-2005 2006-2011
year
Daniel Jurafsky
parser, parsing, stylistic, treebank, reduction
sense, npcomplete, inducing, wsd, unsupervised
building, annotated, discourse, treebank, kappa
cotraining, scalable, moses, open, implemen
framenet, roles, variation, semantic, propbank
moses, meteor, open, bbn, discovery
bleu, automatic, method, rouge, eval
pcfg, temporal, logic, linguistic, noun
bengston, shallow, conll, learning, kernel
multitext, linking, alignment, competitive, bilingu
phrasebased, forest, joint, hierarchical, kbest
whats, moses, open, rule, source, syntaxbased
human, metric, spade, evaluation, metrics
distributional, rasp, similarity, clustering, deep
tagger, pos, entropy, partofspeech, mathematics
propbank, labelled, dependency, lfg, correlation
vari, perceptron, ccg, counts, connectives
dependency, parser, proc, parse, parsing
contrastive, minimize, synchron, anneal, logist
giza, lins, minipar, error, alignment
Figure 4: Posterior probability of faction alignment over time periods for eight researchers with significant publication
records in at least three periods. The key for each entry contains the five highest weighted words in the deviation
vectors for the faction?s incoming citations. For each author, we show factions with which he or she is associated with
probability > 0.1 in at least one time period.
2002; Ib?a?nez et al, 2009; Nallapati et al, 2008) and
analyze authorship (Rosen-Zvi et al, 2004; Johri et
al., 2011).
Assigning author factions can be seen as network
classification problem, where the goal is to label
nodes in a network such that there is (i) a corre-
lation between a node?s label and its observed at-
tributes and (ii) a correlation between labels of in-
terconnected nodes (Sen et al, 2008). Such collec-
tive network-based approaches have been used on
scientific literature to classify papers/web pages into
its subject categories (Kubica et al, 2002; Getoor,
2005; Angelova and Weikum, 2006). If we knew
the word distributions between factions beforehand,
learning the author factions in our model would be
equivalent to the network classification task, where
our edge weights are proportional to the probability
of coauthorship multiplied by the probability of ob-
serving the citation words given the author?s faction
labels.
6 Conclusion
In this work, we have defined factions in terms of
how authors talk about each other?s work, going be-
yond co-authorship and citation graph representa-
tions of a research community. We take a first step
toward computationally modeling faction formation
by using a latent author faction model and applied
it to the ACL community, revealing both factions
and how they cite each other. We also extended the
model to capture authors? faction changes over time.
30
Acknowledgments
The authors thank members of the ARK group and the
anonymous reviewers for helpful feedback. We gratefully
acknowledge technical assistance from Matthew Fiorillo.
This research was supported in part by an A
?
STAR fel-
lowship to Y. Sim, NSF grant IIS-0915187 to N. Smith,
and the Center for Intelligent Information Retrieval and
NSF grant IIS-0910884 for D. Smith.
References
G. Andrew and J. Gao. 2007. Scalable training of L
1
-
regularized log-linear models. In Proc. of ICML.
R. Angelova and G. Weikum. 2006. Graph-based text
classification: learn from your neighbors. In Proc. of
SIGIR.
S. Bergsma, D. Yarowsky, and K. Church. 2011. Using
large monolingual and bilingual corpora to improve
coordination disambiguation. In Proc. of ACL.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
I. S. Dhillon, Y. Guan, and B. Kulis. 2004. Kernel k-
means: spectral clustering and normalized cuts. In
Proc. of KDD.
L. Dietz, S. Bickel, and T. Scheffer. 2007. Unsupervised
prediction of citation influences. In Proc. of ICML.
J. Eisenstein, A. Ahmed, and E. P. Xing. 2011. Sparse
additive generative models of text. In Proc. of ICML.
M. A. T. Figueiredo. 2003. Adaptive sparseness for
supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 25(9):1150?1159.
N. Garera, C. Callison-Burch, and D. Yarowsky. 2009.
Improving translation lexicon induction from mono-
lingual corpora via dependency contexts and part-of-
speech equivalences. In Proc. of CoNLL.
S. Gerrish and D. M. Blei. 2010. A language-based
approach to measuring scholarly impact. In Proc. of
ICML.
L. Getoor. 2005. Link-based classification. In Ad-
vanced Methods for Knowledge Discovery from Com-
plex Data, pages 189?207. Springer.
D. Hall, D. Jurafsky, and C. D. Manning. 2008. Studying
the history of ideas using topic models. In Proc. of
EMNLP.
A. Ib?a?nez, P. Larra?naga, and C. Bielza. 2009. Predict-
ing citation count of bioinformatics papers within four
years of publication. Bioinformatics, 25(24):3303?
3309.
N. Johri, D. Ramage, D. A. McFarland, and D. Juraf-
sky. 2011. A study of academic collaborations in
computational linguistics using a latent mixture of au-
thors model. In Proc. of the ACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities.
M. M. Kessler. 1963. Bibliographic coupling between
scientific papers. American documentation, 14(1):10?
25.
J. Kubica, A. Moore, J. Schneider, and Y. Yang. 2002.
Stochastic link and group detection. In Proc. of AAAI.
K. Lange and J. S. Sinsheimer. 1993. Nor-
mal/independent distributions and their applications
in robust regression. Journal of Computational and
Graphical Statistics, 2(2):175?198.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP.
A. McCallum, G. S. Mann, and D. Mimno. 2006. Biblio-
metric impact measures leveraging topic analysis. In
Proc. of JCDL.
S. M. McNee, I. Albert, D. Cosley, P. Gopalkrishnan,
S. K. Lam, A. M. Rashid, J. A. Konstan, and J. Riedl.
2002. On the recommending of citations for research
papers. In Proc. of CSCW.
T. P. Minka. 2009. Estimating a Dirich-
let distribution. Available online at http:
//research.microsoft.com/en-us/
um/people/minka/papers/dirichlet/
minka-dirichlet.pdf.
R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. Cohen.
2008. Joint latent topic models for text and citations.
In Proc. of KDD.
F. Osareh. 1996. Bibliometrics, citation analysis and
co-citation analysis: A review of literature I. Libri,
46(3):149?158.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The Penn discourse
treebank 2.0. In Proc. of LREC.
D. R. Radev, M. T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009a. A bibliometric and network analysis of
the field of computational linguistics. Journal of the
American Society for Information Science and Tech-
nology.
D. R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009b.
The ACL Anthology Network corpus. In Proceed-
ings of the Workshop on Text and Citation Analysis for
Scholarly Digital Libraries.
F. Radicchi, C. Castellano, F. Cecconi, V. Loreto,
D. Parisi, and G. Parisi. 2004. Defining and iden-
tifying communities in networks. Proceedings of the
National Academy of Sciences of the United States of
America, 101(9):2658?2663.
31
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and docu-
ments. In Proc. of UAI.
V. Satuluri and S. Parthasarathy. 2011. Symmetrizations
for clustering directed graphs. In Proc. of Interna-
tional Conference on Extending Database Technology.
P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher,
and T. Eliassi-Rad. 2008. Collective classification in
network data. AI magazine, 29(3):93.
H. Small. 1973. Co-citation in the scientific literature:
A new measure of the relationship between two docu-
ments. Journal of the American Society for informa-
tion Science, 24(4):265?269.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), 58(1):267?288.
G. C. G. Wei and M. A. Tanner. 1990. A Monte Carlo
implementation of the EM algorithm and the poor
man?s data augmentation algorithms. Journal of the
American Statistical Association, 85(411):699?704.
H. D. White and B. C. Griffith. 1981. Author cocitation:
A literature measure of intellectual structure. Jour-
nal of the American Society for Information Science,
32(3):163?171.
D. Yogatama, M. Heilman, B. O?Connor, C.Dyer, B. R.
Routledge, and N. A. Smith. 2011. Predicting a sci-
entific community?s response to an article. In Proc. of
EMNLP.
32
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 66?70,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transliteration by Sequence Labeling with Lattice Encodings and Reranking
Waleed Ammar Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{wammar,cdyer,nasmith}@cs.cmu.edu
Abstract
We consider the task of generating transliter-
ated word forms. To allow for a wide range of
interacting features, we use a conditional ran-
dom field (CRF) sequence labeling model. We
then present two innovations: a training objec-
tive that optimizes toward any of a set of possi-
ble correct labels (since more than one translit-
eration is often possible for a particular in-
put), and a k-best reranking stage to incorpo-
rate nonlocal features. This paper presents re-
sults on the Arabic-English transliteration task
of the NEWS 2012 workshop.
1 Introduction
Transliteration is the transformation of a piece of
text from one language?s writing system into an-
other. Since the transformation is mostly explained
as local substitutions, deletions, and insertions, we
treat word transliteration as a sequence labeling
problem (Ganesh et al, 2008; Reddy and Waxmon-
sky, 2009), using linear-chain conditional random
fields as our model (Lafferty et al, 2001; Sha and
Pereira, 2003). We tailor this model to the transliter-
ation task in several ways.
First, for the Arabic-English task, each Arabic in-
put is paired with multiple valid English transliter-
ation outputs, any of which is judged to be correct.
To effectively exploit these multiple references dur-
ing learning, we use a training objective in which
the model may favor some correct transliterations
over the others. Computationally efficient inference
is achieved by encoding the references in a lattice.
Second, inference for our first-order sequence la-
beling model requires a runtime that is quadratic in
the number of labels. Since our labels are character
n-grams in the target language, we must cope with
thousands of labels. To make the most of each in-
ference call during training, we apply a mini-batch
training algorithm which converges quickly.
Finally, we wish to consider some global features
that would render exact inference intractable. We
therefore use a reranking model (Collins, 2000).
We demonstrate the performance benefits of these
modifications on the Arabic-English transliteration
task, using the open-source library cdec (Dyer et
al., 2010)1 for learning and prediction.
2 Problem Description
In the NEWS 2012 workshop, the task is to gener-
ate a list of ten transliterations in a specified target
language for each named entity (in a known source
language) in the test set. A training set is provided
for each language pair. An entry in the training set
comprises a named entity in the source language and
one or more transliterations in the target language.
Zhang et al (2012) provides a detailed description
of the shared task.
3 Approach
3.1 Character Alignment
In order to extract source-target character map-
pings, we use m2m-aligner (Jiampojamarn et al,
2007),2 which implements a forward-backward al-
gorithm to sum over probabilities of possible charac-
ter sequence mappings, and uses Expectation Max-
imization to learn mapping probabilities. We allow
source characters to be deleted, but not target char-
acters. Parameters -maxX and -maxY are tuned on
a devevelopment set.
Our running example is the Arabic name EAdl
(in Buckwalter?s ASCII-based encoding of Arabic)
with two English transliterations: ADEL and ?ADIL.
The character alignment for the two pairs is shown
in Fig. 1.
1http://www.cdec-decoder.org
2http://code.google.com/p/m2m-aligner
66
AD
E
L
E
A
d
l
?
?
?
?
A
D
I
L
E
A
d
l
?
?
?
?
'
Arabic English Arabic English
Figure 1: Character alignment for transliterating EAdl to
ADEL and ?ADIL.
3.2 Sequence Labeling Scheme and Notation
We frame transliteration as a sequence labeling
problem. However, transliteration is not a one-to-
one process, meaning that a na??ve application of
one-label-per-token sequence models would be un-
likely to perform well. Previous work has taken
two different approaches. Reddy and Waxmonsky
(2009) first segment the input character sequence,
then use the segments to construct a transliteration
in the target language. Since segmentation errors
will compound to produce transliteration errors, we
avoid this. Ganesh et al (2008) do not require a seg-
mentation step, but their model does not allow for
many-to-one and many-to-many character mappings
which are often necessary.
Our approach overcomes both these shortcom-
ings: we have neither an explicit segmentation step,
nor do we forbid many-to-many mappings. In our
model, each character xi in the source-language in-
put x = ?x1, x2, . . . , xn? is assigned a label yi.
However, a label yi is a sequence of one or more
target-language characters, a special marker indi-
cating a deletion (), or a special marker indicat-
ing involvement in a many-to-one mapping (?), that
is, yi ? ?+ ? {, ?}, where ? is the target lan-
guage alphabet.3 When an input x has multiple al-
ternative reference transliterations, we denote the set
Y?(x) = {y1,y2, . . . ,yK}.
We map the many-to-many alignments produced
by m2m-aligner to one label for each input char-
acter, using the scheme in Table 1. Note that zero-
to-one alignments are not allowed.
The two reference label sequences for our running
example, which are constructed from the alignments
in Fig. 1 are:
3For an input type x, we only consider labels that were ac-
tually observed in the training data, which means the label set
is finite.
Type Alignment Labels
1:0 xi :  yi = 
1:1 xi : tj yi = tj
1:many xi : tj . . . tk yi = tj . . . tk
many:1 xi . . . xp : tj yp = tj
yi = ? ? ? = yp?1 = ?
many:many xi . . . xp : tj . . . tk yp = tj . . . tk
yi = ? ? ? = yp?1 = ?
Table 1: Transforming alignments to sequence labels.
x y1 y2
E ? ?
A A A
d DE DI
l L L
Of key importance in our model is defining, for
each source character, the set of labels that can be
considered for it. For each source character, we add
all labels consistent with character alignments to the
lexicon.
3.3 Model
Our model for mapping from inputs to outputs is
a conditional random field (Lafferty et al, 2001),
which defines the conditional probability of every
possible sequence labeling y of a sequence x with
the parametric form:
p?(y | x) ? exp
?|x|
i=1 ? ? f(x, yi, yi?1) (1)
where f is a vector of real-valued feature functions.
3.4 Features
The feature functions used are instantiated by apply-
ing templates shown in Table 2 to each position i in
the input string x.
3.5 Parameter Learning
Given a training dataset of pairs {?xj ,yj?}
`
j=1 (note
that each y is derived from the max-scoring char-
acter alignment), a CRF is trained to maximize the
regularized conditional log-likelihood:
max
?
L{1,...,`}(?) ,
?`
j=1 log p?(yj | xj) ? C||?||
2
2
(2)
The regularization strength hyperparameter is tuned
on development data. On account of the large data
sizes and large label sets in several language pairs
67
Feature Template Description
U1:yi-xi,
U2:yi-xi?1-xi,
U3:yi-xi-xi+1, moving window of unigram,
U4:yi-xi?2-xi?1-xi, bigram and trigram context
U5:yi-xi?1-xi-xi+1,
U6:yi-xi-xi+1-xi+2
U7:yi, B1:yi-yi?1 label unigrams and bigrams
U8:|yi| label size (in characters)
Table 2: Feature templates for features extracted from
transliteration hypotheses. The SMALLCAPS prefixes
prevent accidental feature collisions.
(Table 3), batch optimization with L-BFGS is in-
feasible. Therefore, we use a variant of the mini-
batch L-BFGS learning approach proposed by Le
et al (2011). This algorithm uses a series of ran-
domly chosen mini-batches B(1),B(2), . . ., each a
subset of {1, . . . , `}, to produce a series of weights
?(1),?(2), . . . by running N iterations of L-BFGS
on each mini-batch to compute the following:
max?(i) LB(i)(?
(i)) ? T??(i) ? ?(i?1)?22 (3)
The T parameter controls how far from the previ-
ous weights the optimizer can move in any particu-
lar mini-batch4. We use mini-batch sizes of 5, and
start training with a small value of T and increase it
as we process more iterations. This is equivalent to
reducing the step-size with the number of iterations
in conventional stochastic learning algorithms.
Language Pair Unique Labels
Arabic-English 1,240
Chinese-English 2,985
Thai-English 1,771
English-Chinese 1,321
English-Japanese Kanji 4,572
Table 3: Size of the label set in some language pairs.
3.6 Using Multiple Reference Transliterations
In some language pairs, NEWS-2012 provides mul-
tiple reference transliterations in the training set. In
this section, we discuss two possibilities for using
these multiple references to train our transliteration
4When T = 0, our learning algorithm is identical to the L-
BFGS mini-batch algorithm of Le et al (2011); however, we
find that more rapid convergence is possible when T > 0.
'
A
DI
L
DE
A
?
Figure 2: Lattice encoding two transliterations of EAdl:
ADEL and ?ADIL.
model. The first possibility is to create multiple in-
dependent training inputs for each input x, one for
each correct transliteration in Y?(x). Using this ap-
proach, with K different transliterations, the CRF
training objective will attempt to assign probability
1
K to each correct transliteration, and 0 to all others
(modulo regularization).
Alternatively, we can train the model to maximize
the marginal probability assigned by the model to
the set of correct labels Y? = {y1, . . . ,yK}. That
is, we assume a set of training data {(xj ,Y?j )}
`
j=1
and replace the standard CRF objective with the fol-
lowing (Dyer, 2009):5
max?
?`
j=1 log
?
y?Y?j
p?(y | xj) ? C||?||22 (4)
This learning objective has more flexibility. It can
maximize the likelihood of the training data by giv-
ing uniform probability to each reference transliter-
ation for a given x, but it does not have to. In effect,
we do not care how probability mass is distributed
among the correct labels. Our hope is that if some
transliterations are difficult to model?perhaps be-
cause they are incorrect?the model will be able to
disregard them.
To calculate the marginal probability for each xj ,
we represent Y?(x) as a label lattice, which is sup-
ported as label reference format in cdec. A fur-
ther computational advantage is that each x in the
training data is now only a single training instance
meaning that fewer forward-backward evaluations
are necessary. The lattice encoding of both translit-
erations of our running example is shown in Fig. 2.
3.7 Reranking
CRFs require feature functions to be ?local? to
cliques in the underlying graphical model. One way
to incorporate global features is to first decode the
5Unlike the standard CRF objective in eq. 2, the marginal
probability objective is non-convex, meaning that we are only
guaranteed to converge to a local optimum in training.
68
k-best transliterations using the CRF, then rerank
based on global features combined with the CRF?s
conditional probability of each candidate. We ex-
periment with three non-local features:
Character language model: an estimate of
pcharLM (y) according to a trigram character lan-
guage model (LM). While a bigram LM can be fac-
tored into local features in a first order CRF, higher
n-gram orders require a higher-order CRF.
Class language model: an estimate of pclassLM (y),
similar to the character LM, but collapses characters
which have a similar phonetic function into one class
(vowels, consonants, and hyphens/spaces). Due to
the reduced number of types in this model, we can
train a 5-gram LM.
Transliteration length: an estimate of plen(|y| |
|x|) assuming a multinomial distribution with pa-
rameters estimated using transliteration pairs of the
training set.
The probabilistic model for each of the global
features is trained using training data provided for
the shared task. The reranking score is a linear
combination of log pcrf (y | x), log pcharLM (y),
log pclassLM (y) and log plen(|y| | |x|). Linear co-
efficients are optimized using simulated annealing,
optimizing accuracy of the 1-best transliteration in a
development set. k-best lists are extracted from the
CRF trellis using the lazy enumeration algorithm of
Huang and Chiang (2005).
4 Experiments
We tested on the NEWS 2012 Arabic-English
dataset. The train, development, and test sets con-
sist of 27,177, 1,292, and 1,296 source named enti-
ties, respectively, with an average 9.6 references per
name in each case.
Table 4 summarizes our results using the ACC
score (Zhang et al, 2012) (i.e., word accuracy in
top-1). ?Basic CRF? is the model with mini-batch
learning and represents multiple reference translit-
erations as independent training examples. We man-
ually tuned the number of training examples and
LBFGS iterations per mini-batch to five and eight,
respectively. ?CRF w/lattice? compactly represents
the multiple references in a lattice, as detailed in
?3.6. We consider reranking using each of the three
global features along with the CRF, as well as the
Model Ar-En
Basic CRF 23.5
CRF w/lattice 37.0
CRF w/lattice; rerank pcrf , pcharLM 40.7
CRF w/lattice; rerank pcrf , pclassLM 38.4
CRF w/lattice; rerank pcrf , plen 37.3
CRF w/lattice, rerank all four 42.8
Table 4: Model performance, measured in word accuracy
in top-1 (ACC, %).
full set of four features.
Maximizing the marginal conditional likelihood
of the set of alternative transliterations (rather than
maximizing each alternative independently) shows
a dramatic improvement in transliteration accuracy
for Arabic-English. Moreover, in Arabic-English
the basic CRF model converges in 120K mini-batch
iterations, which is, approximately, seven times the
number of iterations needed for convergence with
lattice-encoded labels. A model converges when its
ACC score on the development set ceases to improve
in 800 mini-batch iterations. Results also show that
reranking a k-best list of only five transliterations
with any of the global features improves accuracy.
Using all the features together to rerank the k-best
list gives further improvements.
5 Conclusion
We built a CRF transliteration model that allows
for many-to-many character mappings. We address
limitations of CRFs using mini-batch learning and
reranking techniques. We also show how to relax
the learning objective when the training set contains
multiple references, resulting in faster convergence
and improved transliteration accuracy.
We suspect that including features of higher-order
n-gram labels would help improve transliteration ac-
curacy further, but it makes inference intractable due
to the large set of labels. In future work, coarse
transformations of label n-grams might address this
problem.
Acknowledgments
This research was supported in part by the U.S. Army
Research Laboratory and the U.S. Army Research Office
under contract/grant number W911NF-10-1-0533. We
thank anonymous reviewers for the valuable comments.
69
References
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
S. Ganesh, S. Harsha, P. Pingali, and V. Varma. 2008.
Statistical transliteration for cross language informa-
tion retrieval using HMM alignment and CRF. In
Proc. of the 2nd Workshop On Cross Lingual Infor-
mation Access.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
In Proc. of the 9th International Workshop on Parsing
Technologies.
S. Jiampojamarn, G. Kondrak, and T. Sherif. 2007. Ap-
plying many-to-many alignments and hidden Markov
models to letter-to-phoneme conversion. In Proc. of
NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
Q. V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow,
and A. Y. Ng. 2011. On optimization methods for
deep learning. In Proc. of ICML.
S. Reddy and S. Waxmonsky. 2009. Substring-based
transliteration with conditional random fields. In Proc.
of the Named Entities Workshop.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of NAACL-HLT.
M. Zhang, H. Li, M. Liu, and A. Kumaran. 2012.
Whitepaper of NEWS 2012 shared task on machine
transliteration.
70
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
A Framework for (Under)specifying Dependency Syntax
without Overloading Annotators
Nathan Schneider?? Brendan O?Connor? Naomi Saphra? David Bamman?
Manaal Faruqui? Noah A. Smith? Chris Dyer? Jason Baldridge?
?School of Computer Science, Carnegie Mellon University
?Department of Linguistics, The University of Texas at Austin
Abstract
We introduce a framework for lightweight
dependency syntax annotation. Our for-
malism builds upon the typical represen-
tation for unlabeled dependencies, per-
mitting a simple notation and annotation
workflow. Moreover, the formalism en-
courages annotators to underspecify parts
of the syntax if doing so would streamline
the annotation process. We demonstrate
the efficacy of this annotation on three lan-
guages and develop algorithms to evaluate
and compare underspecified annotations.
1 Introduction
Computational representations for natural lan-
guage syntax are borne of competing design con-
siderations. When designing such representations,
there may be a tradeoff between parsimony and
expressiveness. A range of linguistic theories at-
tract support due to differing purposes and aes-
thetic principles (Chomsky, 1957; Tesni?re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988,
inter alia). Formalisms concerned with tractable
computation may care chiefly about learnabil-
ity or parsing efficiency (Shieber, 1992; Sleator
and Temperly, 1993; Kuhlmann and Nivre, 2006).
Further considerations may include psychologi-
cal and evolutionary plausibility (Croft, 2001;
Tomasello, 2003; Steels et al, 2011; Fossum and
Levy, 2012), integration with other representa-
tions such as semantics (Steedman, 2000; Bergen
and Chang, 2005), or suitability for particular ap-
plications (e.g., translation).
Here we elevate ease of annotation as a pri-
mary design concern for a syntactic annotation
formalism. Currently, a lack of annotated data
is a huge bottleneck for robust NLP, standing in
the way of parsers for social media text (Foster
et al, 2011) and many low-resourced languages
(to name two examples). Traditional syntactic an-
notation projects like the Penn Treebank (Marcus
?Corresponding author: nschneid@cs.cmu.edu
et al, 1993) or Prague Dependency Treebank (Ha-
jic?, 1998) require highly trained annotators and
huge amounts of effort. Lowering the cost of an-
notation, by making it easier and more accessi-
ble, could greatly facilitate robust NLP in new lan-
guages and genres.
To that end, we design and test new, lightweight
methodologies for syntactic annotation. We pro-
pose a formalism, Fragmentary Unlabeled De-
pendency Grammar (FUDG) for unlabeled de-
pendency syntax that addresses some of the most
glaring deficiencies of basic unlabeled dependen-
cies (?2), with little added burden on annotators.
FUDG requires minimal theoretical commitments,
and can be supplemented with a project-specific
style guide (we provide a brief one for English).
We contribute a simple ASCII markup language?
Graph Fragment Language (GFL; ?3)?that al-
lows annotations to be authored using any text ed-
itor, along with tools for validating, normalizing,
and visualizing GFL annotations.1
An important characteristic of our framework is
annotator flexibility. The formalism supports this
by allowing underspecification of structural por-
tions that are unclear or unnecessary for the pur-
poses of a project. Fully leveraging this power re-
quires new algorithms for evaluation, e.g., of inter-
annotator agreement, where annotations are par-
tial; such algorithms are presented in ?4.2
Finally, small-scale case studies (?5) apply our
framework (formalism, notation, and evaluations)
to syntactically annotate web text in English, news
in Malagasy, and dialogues in Kinyarwanda.
2 A Dependency Grammar for
Annotation
Although dependency-based approaches to syntax
play a major role in computational linguistics, the
nature of dependency representations is far from
uniform. Exemplifying one end of the spectrum
is the Prague Dependency Treebank, which articu-
lates an elaborate dependency-based syntactic the-
1https://github.com/brendano/gfl_syntax/
2Parsing algorithms are left for future work.
51
Found the scarriest mystery door in my school . I?M SO CURIOUS D:
Found** < (the scarriest mystery door*)
Found < in < (my > school)
I?M** < (SO > CURIOUS)
D:**
my = I?M
thers still like 1 1/2 hours till Biebs bday here :P
thers** < still
thers < ((1 1/2) > hours < till < (Biebs > bday))
(thers like 1 1/2 hours)
thers < here
:P**
Figure 1: Two tweets with example GFL annotations. (The formalism and notation are described in ?3.)
ory in a rich, multi-tiered formalism (Hajic?, 1998;
B?hmov? et al, 2003). On the opposite end of
the spectrum are the structures used in dependency
parsing research which organize all the tokens of
a sentence into a tree, sometimes with category la-
bels on the edges (K?bler et al, 2009). Insofar as
they reflect a theory of syntax, these vanilla de-
pendency grammars provide a highly reduction-
ist view of structure?indeed, parses used to train
and evaluate dependency parses are often simpli-
fications of Prague-style parses, or else converted
from constituent treebanks.
In addition to the binary dependency links of
vanilla dependency representations, we offer three
devices to capture certain linguistic phenomena
more straightforwardly:3
1. We make explicit the meaningful lexical units
over which syntactic structure is represented. Our
approach (a) allows punctuation and other extrane-
ous tokens to be excluded so as not to distract from
the essential structure; and (b) permits tokens to be
grouped into shallow multiword lexical units.4
2. Coordination is problematic to represent with
unlabeled dependencies due to its non-binary na-
ture. A coordinating conjunction typically joins
multiple expressions (conjuncts) with equal sta-
tus, and other expressions may relate to the com-
pound structure as a unit. There are several differ-
ent conventions for forcing coordinate structures
into a head-modifier straightjacket (Nivre, 2005;
de Marneffe and Manning, 2008; Marec?ek et al,
2013). Conjuncts, coordinators, and shared de-
pendents can be distinguished with edge labels;
we equivalently use a special notation, permitting
the coordinate structure to be automatically trans-
formed with any of the existing conventions.5
3Some of this is inspired by the conventions of Reed-
Kellogg sentence diagramming, a graphical dependency an-
notation system for English pedagogy (Reed and Kellogg,
1877; Kolln and Funk, 1994; Florey, 2006).
4The Stanford representation supports a limited notion of
multiword expressions (de Marneffe and Manning, 2008).
For simplicity, our formalism treats multiwords as unana-
lyzed (syntactically opaque) wholes, though some multiword
expressions may have syntactic descriptions (Baldwin and
Kim, 2010).
5Tesni?re (1959) and Hudson (1984) similarly use
special structures for coordination (Schneider, 1998;
3. Following Tesni?re (1959), our formalism
offers a simple facility to express anaphora-
antecedent relations (a subset of semantic relation-
ships) that are salient in particular syntactic phe-
nomena such as relative clauses, appositives, and
wh-expressions.
Underspecification. Our desire to facilitate
lightweight annotation scenarios requires us to
abandon the expectation that syntactic informants
provide a complete parse for every sentence. On
one hand, an annotator may be uncertain about the
appropriate parse due to lack of expertise, insuf-
ficiently mature annotation conventions, or actual
ambiguity in the sentence. On the other hand, an-
notators may be indifferent to certain phenomena.
This can happen for a variety of reasons:
? Some projects may only need annotations of
specific constructions. For example, building a
semantic resource for events may require anno-
tation of syntactic verb-argument relations, but
not internal noun phrase structure.
? As a project matures, it may be more useful to
annotate only infrequent lexical items.
? Semisupervised learning from partial annota-
tions may be sufficient to learn complete parsers
(Hwa, 1999; Clark and Curran, 2006).
? Beginning annotators may wish to focus on eas-
ily understood syntactic phenomena.
? Different members of a project may wish to spe-
cialize in different syntactic phenomena, reduc-
ing training cost and cognitive load.
Rather than treating annotations as invalid unless
and until they are complete trees, we formally rep-
resent and reason about partial parse structures.
Annotators produce annotations, which encode
constraints on the (inferred) analysis, the parse
structure, of a sentence. We say that a valid anno-
tation supports (is compatible with) one or more
analyses. Both annotations and analyses are rep-
resented as graphs (the graph representation is de-
scribed below in ?3.2). We require that the di-
rected edges in an analysis graph must form a tree
over all the lexical items in the sentence.6 Less
Sangati and Mazza, 2009).
6While some linguistic phenomena (e.g., relative clauses,
control constructions) can be represented using non-tree
52
stringent well-formedness constraints on the an-
notation graph leave room for underspecification.
Briefly, an annotation can be underspecified in
two ways: (a) an expression may not be attached to
any parent, indicating it might depend on any non-
descendant in a full analysis?this is useful for an-
notating sentences piece by piece; and (b) multiple
expressions may be grouped together in a fudge
expression (?3.3), a constraint that the elements
form a connected subgraph in the full analysis
while leaving the precise nature of that subgraph
indeterminate?this is useful for marking relation-
ships between chunks (possibly constituents).
A formalism, not a theory. Our framework for
dependency grammar annotation is a syntactic
formalism, but it is not sufficiently comprehen-
sive to constitute a theory of syntax. Though
it standardizes the basic treatment of a few ba-
sic phenomena, simplicity of the formalism re-
quires us to be conservative about making such
extensions. Therefore, just as with simpler for-
malisms, language- and project-specific conven-
tions will have to be developed for specific linguis-
tic phenomena. By embracing underspecified an-
notation, however, our formalism aims to encour-
age efficient corpus coverage in a nascent anno-
tation project, without forcing annotators to make
premature decisions.
3 Syntactic Formalism and GFL
In our framework, a syntactic annotation of a sen-
tence follows an extended dependency formalism
based on the desiderata enumerated in the previ-
ous section. We call our formalism Fragmentary
Unlabeled Dependency Grammar (FUDG).
To make it simple to create FUDG annotations
with a text editor, we provide a plain-text de-
pendency notation called Graph Fragment Lan-
guage (GFL). Fragments of the FUDG graph?
nodes and dependencies linking them?are en-
coded in this language; taken together, these frag-
ments describe the annotation in its entirety. The
ordering of GFL fragments, and of tokens within
each fragment, is of no formal consequence. Since
the underlying FUDG representation is transpar-
ently related to GFL constructions, GFL notation
will be introduced alongside the discussion of each
kind of FUDG node.7
structures, we find that being able to alert annotators when
they inadvertently violate the tree constraint is more useful
than the expressive flexibility.
7In principle, FUDG annotations could be created with
3.1 Tokens
We expect a tokenized string, such as a sentence
or short message. The provided tokenization is re-
spected in the annotation. For human readability,
GFL fragments refer to tokens as strings (rather
than offsets), so all tokens that participate in an
annotation must be unambiguous in the input.8 A
token may be referenced multiple times in the an-
notation.
3.2 Graph Encoding
Directed arcs. As in other dependency
formalisms, dependency arcs are directed
links indicating the syntactic headedness
relationship between pairs of nodes. In
GFL, directed arcs are indicated with an-
gle brackets pointing from the dependent to
its head, as in black > cat or (equivalently)
cat < black. Multiple arcs can be chained to-
gether: the > cat < black < jet describes three
arcs. Parentheses help group portions of a chain:
(the > cat < black < jet) > likes < fish (the
structure black < jet > likes, in which jet
appears to have two heads, is disallowed). Note
that another encoding for this structure would be
to place the contents of the parentheses and the
chain cat > likes < fish on separate lines. Curly
braces can be used to list multiple dependents of
the same head: {cat fish} > likes.
Anaphoric links. These undirected links join
coreferent anaphora to each other and to their an-
tecedent(s). In English this includes personal pro-
nouns, relative pronouns (who, which, that), and
anaphoric do and so (Leo loves Ulla and so does
Max). This introduces a bit of semantics into our
annotation, though at present we do not attempt to
mark non-anaphoric coreference. It also allows a
more satisfying treatment of appositives and rel-
ative clauses than would be possible from just the
directed tree (the third example in figures 2 and 3).
Lexical nodes. Whereas in vanilla dependency
grammar syntactic links are between pairs of to-
ken nodes, FUDG abstracts away from the indi-
vidual tokens in the input. The lowest level of a
FUDG annotation consists of lexical nodes, i.e.,
an alternative mechanism such as a GUI, as in Hajic? et al
(2001).
8If a word is repeated within the sentence, it must be in-
dexed in the input string in order to be referred to from a
fragment. In our notation, successive instances of the same
word are suffixed with ~1, ~2, ~3, etc. Punctuation and other
tokens omitted from an annotation do not need to be indexed.
53
'll
If
's
I wake_up
restin' it~1
it~2
weapons
Our three
are
$a
fear surprise efficiency
ruthless
and~1 and~2
are
We knights
the
who
say
Ni
Figure 2: FUDG graphs corresponding to the examples in figure 3. The two special kinds of directed edges are for attaching
conjuncts (bolded) and their coordinators (dotted) in a coordinate structure. Anaphoric links are undirected. The root node of
each sentence is omitted.
If it~1 's restin' I 'll wake it~2 up .
If < (it~1 > 's < restin')
I > 'll < [wake up] < it~2
If > 'll**
it~1 = it~2
Our three weapons are fear and~1 surprise and~2
ruthless efficiency ...
{Our three} > weapons > are < $a
$a :: {fear surprise efficiency} :: {and~1 and~2}
ruthless > efficiency
We are the knights who say ... Ni !
We > are < knights < the
knights < (who > say < Ni)
who = knights
Figure 3: GFL for the FUDG graphs in figure 2.
lexical item occurrences. Every token node maps
to 0 or 1 lexical nodes (punctuation, for instance,
can be ignored).
A multiword is a lexical node incorporating
more than one input token and is atomic (does
not contain internal structure). A multiword node
may group any subset of input tokens; this allows
for multiword expressions which are not neces-
sarily contiguous in the sentence (e.g., the verb-
particle construction make up in make the story
up). GFL notates multiwords with square brack-
ets, e.g., [break a leg].
Coordination nodes. Coordinate structures re-
quire at least two kinds of dependents: co-
ordinators (i.e., lexical nodes for coordinat-
ing conjunctions?at least one per coordina-
tion node) and conjuncts (heads of the con-
joined subgraphs?at least one per coordination
node). The GFL annotation has three parts:
a variable representing the node, a set of con-
juncts, and a set of coordinator nodes. For in-
stance, $a :: {[peanut butter] honey} :: {and}
(peanut butter and honey) can be embedded
within a phrase via the coordination node
variable $a; a [fresh [[peanut butter] and
honey] sandwich] snack would be formed with
{fresh $a} > sandwich > snack < a. A graphical
example of coordination can be seen in figure 2?
note the bolded conjunct edges and the dotted co-
ordinator edges. If the conjoined phrase as a whole
takes modifiers, these are attached to the coordina-
tion node with regular directed arcs. For example,
in Sam really adores kittens and abhors puppies.,
the shared subject Sam and adverb really attach to
the entire conjoined phrase. In GFL:
$a :: {adores abhors} :: {and}
Sam > $a < really
adores < kittens abhors < puppies
Root node. This is a special top-level node used
to indicate that a graph fragment constitutes a stan-
dalone utterance or a discourse connective. For an
input with multiple utterances, the head of each
should be designated with ** to indicate that it at-
taches to the root.
3.3 Means of Underspecification
As discussed in ?2, our framework distinguishes
annotations from full syntactic analyses. With re-
spect to dependency structure (directed edges), the
former may underspecify the latter, allowing the
annotator to commit only to a partial analysis.
For an annotationA, we define support(A) to be
the set of full analyses compatible with that anno-
tation. A full analysis is required to be a directed
rooted tree over all lexical nodes in the annotation.
An annotation is valid if its support is non-empty.
The 2 mechanisms for dependency underspeci-
fication are unattached nodes and fudge nodes.
Unattached nodes. For any node in an annota-
tion, the annotator is free to simply leave it not
attached to any head. This is interpreted as al-
lowing its head to be any other node (including
the root node), subject to the tree constraint. We
call a node?s possible heads its supported par-
ents. Formally, for an unattached node v in an-
notation A, suppParentsA(v) = nodes(A) \ ({v} ?
descendants(v)).
Fudge nodes. Sometimes, however, it is desir-
able to represent a sort of skeletal structure with-
out filling in all the details. A fudge expres-
sion (FE) asserts that a group of nodes (the ex-
pression?s members) belong together in a con-
nected subgraph, while leaving the internal struc-
ture of that subgraph unspecified.9 The notation
9This underspecification semantics is, to the best of our
knowledge, novel, though it has been proposed that con-
nected dependency subgraphs (known as catenae) are of the-
oretical importance in syntax (Osborne et al, 2012).
54
FN2
a
b
f
FN1
c d e
f
b
f
b
b c
b
b a
a
d
a
c a
d
a
d
c db e fe c e fe
a
d d
cf
e e f
c
Figure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments
((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?
log 6
log 75
= .816.
for this is a list of two or more nodes within
parentheses: an annotation for Few if any witches
are friends with Maria. might contain the FE
(Few if any) so as to be compatible with the
structures Few < if < any, Few > if > any, etc.?
but not, for instance, Few > witches < any. In
the FUDG graph, this is represented with a fudge
node to which members are attached by special
member arcs. Fudge nodes may be linked to other
nodes: the GFL fragment (Few if any) > witches
is compatible with (Few < if < any) > witches,
(Few < (if > any)) > witches, and so forth.
Properties. Let f be a fudge expression. From
the connected subgraph definition and the tree
constraint on analyses, it follows that:
? Exactly 1 member of f must, in any compatible
analysis, have a parent that is not a member of f.
Call this node the top of the fudge expression,
denoted f ?. f ? dominates all other members of
f; it can be considered f?s ?internal head.?
? f does not necessarily form a full subtree. Any
of its members may have dependents that are
not themselves members of the fudge expres-
sion. (Such dependencies can be specified in
additional GFL fragments.)
Top designation. A single member of a fudge
expression may optionally be designated as its top
(internal head). This is specified with an asterisk:
(Few* if any) > witches indicates that Few must
attach to witches and also dominate both if and
any. In the FUDG graph, this is represented with
a special top arc as depicted in bold in figure 4.
Nesting. One fudge expression may nest
within another, e.g. (Few (if any)) > witches;
the word analyzed as attaching to witches might
be Few or whichever of (if any) heads the other.
A nested fudge expression can be designated as
top: (Vanishingly few (if any)*).
Modifiers. An arc attaching a node to a
fudge expression as a whole asserts that the
external node should modify the top of the fudge
expression (whether or not that top is designated
in the annotation). For instance, two of the
interpretations of British left waffles on Falklands
would be preserved by specifying British > left
and (left waffles) < on < Falklands. Analyses
British > left < waffles < on < Falklands and
(British > left < on < Falklands) > waffles
would be excluded because the preposition does
not attach to the head of (left waffles).10
Multiple membership. A node may be a mem-
ber of multiple fudge expressions, or a member
of an FE while attached to some other node via
an explicit arc. Each connected component of
the FUDG graph is therefore a polytree (not nec-
essarily a tree). The annotation graph minus all
member edges of fudge nodes and all (undirected)
anaphoric links must be a directed tree or forest.
Enumerating supported parents. Fudge ex-
pressions complicate the procedure for listing a
node?s supported parents (see above). Consider an
FE f having some member v. v might be the top
of f (unless some other node is so designated), in
which case anything the fudge node can attach to
is a potential parent of v. If some node other than
v might be the top of f, then v?s head could be any
member of f. Below (?4.1) we develop an algo-
rithm for enumerating supported parents for any
annotation graph node.
4 Annotation Evaluation Measures
For an annotation task which allows for a great
deal of latitude?as in our case, where a syntac-
tic annotation may be full or partial?quantitative
evaluation of data quality becomes a challenge. In
the context of our formalism, we propose mea-
sures that address:
? Annotation efficiency, quantified in terms of
annotator productivity (tokens per hour).
? The amount of information in an underspeci-
fied annotation. Intuitively, an annotation that
flirts with many full analyses conveys less syn-
tactic information than one which supports few
analyses. We define an annotation?s promiscu-
ity to be the number of full analyses it supports,
and develop an algorithm to compute it (?4.1).
10Not all attachment ambiguities can be precisely encoded
in FUDG. For instance, there is no way to forbid an attach-
ment to a word that lies along the path between the pos-
sible heads. The best that can be done given a sentence
like They conspired to defenestrate themselves on Tuesday. is
They > conspired < to < defenestrate < themselves and
(conspired* to defenestrate (on < Tuesday)).
55
? Inter-annotator agreement between two par-
tial annotations. Our measures for dependency
structure agreement (?4.2) incorporate the no-
tion of promiscuity.
We test these evaluations on our pilot annotation
data in the case studies (?5).
4.1 Promiscuity vs. Commitment
Given a FUDG annotation of a sentence, we quan-
tify the extent to which it underspecifies the full
structure by counting the number of analyses that
are compatible with the constraints in the annota-
tion. We call this number the promiscuity of the
annotation. Each analysis tree is rooted with the
root node and must span all lexical nodes.11
A na?ve algorithm for computing promiscuity
would be to enumerate all directed spanning trees
over the lexical nodes, and then check each of
them for compatibility with the annotation. But
this quickly becomes intractable: for n nodes,
one of which is designated as the root, there are
nn?2 spanning trees. However, we can filter out
edges that are known to be incompatible with
the annotation before searching for spanning
trees. Our ?upward-downward? method for
constructing a graph of supported edges first
enumerates a set of candidate top nodes for every
fudge expression, then uses that information
to infer a set of supported parents for every
node.12 The supported edge graph then consists
of vertices lexnodes(A) ? {root} and edges
?
v?lexnodes(A) {(v? v?) ? v? ? suppParentsA(v)}.
From this graph we can count all directed span-
ning trees in cubic time using Kirchhoff?s matrix
tree theorem (Chaiken and Kleitman, 1978; Smith
and Smith, 2007; Margoliash, 2010).13 If some
lexical node has no supported parents, this reflects
conflicting constraints in the annotation, and no
spanning tree will be found.
Promiscuity will tend to be higher for longer
sentences. To control for this, we define a second
quantity, the annotation?s commitment quotient
(commitment being the opposite of promiscuity),
11This measure assumes a fixed lexical analysis (set of lex-
ical nodes) and does not consider anaphoric links. Coordinate
structures are simplified into ordinary dependencies, with co-
ordinate phrases headed by the coordinator?s lexical node. If
a coordination node has multiple coordinators, one is arbi-
trarily chosen as the head and the others as its dependents.
12Python code for these algorithms appears in Schneider
et al (2013) and the accompanying software release.
13Due to a technicality with non-member attachments to
fudge nodes, for some annotations this is only an upper bound
on promiscuity; see Schneider et al (2013).
which normalizes for the number of possible span-
ning trees given the sentence length. The commit-
ment quotient for an annotation of a sentence with
n?1 lexical nodes and one root node is given by:
com(A) = 1 ?
log prom(A)
log nn?2
(the logs are to attenuate the dominance of the ex-
ponential term). This will be 1 if only a single
tree is supported by the annotation, and 0 if the
annotation does not constrain the structure at all.
(If the constraints in the annotation are internally
inconsistent, then promiscuity will be 0 and com-
mitment undefined.) In practice, there is a trade-
off between efficiency and commitment: more de-
tailed annotations require more time. The value of
minimizing promiscuity will therefore depend on
the resources and goals of the annotation project.
4.2 Inter-Annotator Agreement
FUDG can encode flat groupings and coreference
at the lexical level, as well as syntactic structure
over lexical items. Inter-annotator agreement can
be measured separately for each of these facets.
Pilot annotator feedback indicated that our initial
lexical-level guidelines were inadequate, so we fo-
cus here on measuring structural agreement pend-
ing further clarification of the lexical conventions.
Attachment accuracy, a standard measure for
evaluating dependency parsers, cannot be com-
puted between two FUDG annotations if either of
them underspecifies any part of the dependency
structure. One solution is to consider the inter-
section of supported full trees, in the spirit of
our promiscuity measure. For annotations A1 and
A2 of sentence s, one annotation?s supported an-
alyses can be enumerated and then filtered sub-
ject to the constraints of the other annotation.
The tradeoff between inter-annotator compatibil-
ity and commitment can be accounted for by tak-
ing their product, i.e. comPrec(A1 | A2) =
com(A1)
|supp(A1)?supp(A2)|
|supp(A1)|
.
A limitation of this support-intersection ap-
proach is that if the two annotations are not
compatible, the intersection will be empty. A
more fine-grained approach is to decompose
the comparison by lexical node: we general-
ize attachment accuracy with softComPrec(A1 |
A2) = com(A1)
?
`?s
?
i?{1,2} suppParentsAi (`)?
`?s suppParentsA1 (`)
, comput-
ing com(?) and suppParents(?) as in the previous
section. As lexical nodes may differ between the
two annotations, a reconciliation step is required
56
Language Tokens Rate (tokens/hr)
English Tweets (partial) 667 430
English Tweets (full) 388 250
Malagasy 4,184 47
Kinyarwanda 8,036 80
Table 1: Productivity estimates from pilot annotation project.
All annotators were native speakers of English.
to compare the structures: multiwords proposed in
only one of the two annotations are converted to
fudge expressions. Tokens annotated by neither
annotator are ignored. Like with the promiscuity
measure, we simplify coordinate structures to or-
dinary dependencies (see footnote 11).
5 Case Studies
5.1 Annotation Time
To estimate annotation efficiency, we performed
a pilot annotation project consisting of annotating
several hundred English tweets, about 1,000 sen-
tences in Malagasy, and a further 1,000 sentences
in Kinyarwanda.14 Table 1 summarizes the num-
ber of tokens annotated and the effort required. For
the two Twitter cases, the same annotator was first
permitted to do partial annotation of 100 tweets,
and then spend the same amount of time doing a
complete annotation of all tokens. Although this is
a very small study, the results clearly suggest she
was able to make much more rapid progress when
partial annotation was an option.15
This pilot study helped us to identify linguistic
phenomena warranting specific conventions: these
include wh-expressions, comparatives, vocatives,
discourse connectives, null copula constructions,
and many others. We documented these cases in a
20-page style guide for English,16 which informed
the subsequent pilot studies discussed below.
5.2 Underspecification and Agreement
We annotated 2 small English data samples in
order to study annotators? use of underspecifica-
tion. The first is drawn from Owoputi et al?s 2013
Twitter part-of-speech corpus; the second is from
the Reviews portion of the English Web Treebank
14Malagasy is a VOS Austronesian language spoken by 15
million people, mostly in Madagascar. Kinyarwanda is an
SVO Bantu language spoken by 12 million people mostly in
Rwanda. All annotations were done by native speakers of En-
glish. The Kinyarwanda and Malagasy annotators had basic
proficiency in these languages.
15As a point of comparison, during the Penn Treebank
project, annotators corrected the syntactic bracketings pro-
duced by a high-quality hand-written parser (Fidditch) and
achieved a rate of only 375 tokens/hour using a specialized
GUI interface (Marcus et al, 1993).
16Included with the data and software release (footnote 1).
Omit. prom Hist. Mean
1Ws MWs Tkns FEs 1 >1 ?10 ?102 com
Tweets 60 messages, 957 tokens
A 597 56 304 23 43 17 11 5 .96
B 644 47 266 28 37 23 12 6 .95
Reviews 55 sentences, 778 tokens
A 609 33 136 2 53 2 2 1 1.00
C ? D 643 19 116 114 11 44 38 21 .82
T 704 ? 74 ? 55 0 0 0 1
Table 2: Measures of our annotation samples. Note that
annotator ?D? specialized in noun phrase?internal structure,
while annotator ?C? specialized in verb phrase/clausal phe-
nomena; C ? D denotes the combination of their annotation
fragments. ?T? denotes our dependency conversion of the
English Web Treebank parses. (The value 1.00 was rounded
up from .9994.)
(EWTB) (Bies et al, 2012). (Our annotators only
saw the tokenized text.) Both datasets are infor-
mal and conversational in nature, and are dom-
inated by short messages/sentences. In spite of
their brevity, many of the items were deemed to
contain multiple ?utterances,? which we define to
include discourse connectives and emoticons (at
best marginal parts of the syntax); utterance heads
are marked with ** in figure 1.
Table 2 indicates the sizes of the two data sam-
ples, and gives statistics over the output of each
annotator: total counts of single-word and mul-
tiword lexical nodes, tokens not represented by
any lexical node, and fudge nodes; as well as
a histogram of promiscuity counts and the aver-
age of commitment quotients (see ?4.1). For in-
stance, the two sets of annotations obtained for the
Tweets sample used underspecification in 17/60
and 23/60 tweets, respectively, though the promis-
cuity rarely exceeded 100 compatible trees per an-
notation. Examples can be seen in figure 1, where
annotator ?A? marked only the noun phrase head
for the scarriest mystery door, opted not to choose
a head within the quantity 1 1/2, and left ambigu-
ous the attachment of the hedge like. The strong
but not utter commitment to the dependency struc-
ture is reflected in the mean commitment quotients
for this dataset, both of which exceed 0.95.
Inter-annotator agreement (IAA) is quantified in
table 3. The row marked A ? B, for instance,
considers the agreement between annotator ?A?
and annotator ?B?. Measuring IAA on the depen-
dency structure requires a common set of lexical
nodes, so a lexical reconciliation step ensures that
(a) any token used by either annotation is present
in both, and (b) no multiword node is present
in only one annotation?solved by relaxing in-
compatible multiwords to FEs (which increases
promiscuity). For Tweets, lexical reconciliation
57
thus reduces the commitment averages for each
annotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?
marked more multiwords. An analysis fully com-
patible with both annotations exists for only 27/60
sentences; the finer-grained softComPrec measure
(?4.2), however, offers insight into the balance be-
tween commitment and agreement.
Qualitatively, we observe three leading causes
of incompatibilities (disagreements): obvious an-
notator mistakes (such as the marked as a head);
inconsistent handling of verbal auxiliaries; and un-
certainty whether to attach expressions to a verb
or the root node, as with here in figure 1.17 An-
notators noticed occasional ambiguous cases and
attempted to encode the ambiguity with fudge ex-
pressions: again in the tweet maybe put it off un-
til you feel like ~ talking again ? is one example.
More often, fudge expressions proved useful for
syntactically difficult constructions, such as those
shown in figure 1 as well as: 2 shy of breaking it,
asked what tribe I was from, a $ 13 / day charge,
you two, and the most awkward thing ever.
5.3 Annotator Specialization
As an experiment in using underspecification for
labor division, two of the annotators of Reviews
data were assigned specific linguistic phenomena
to focus on. Annotator ?D? was tasked with the in-
ternal structure of base noun phrases, including re-
solving the antecedents of personal pronouns. ?C?
was asked to mark the remaining phenomena?
i.e., utterance/clause/verb phrase structure?but to
mark base noun phrases as fudge expressions,
leaving their internal structure unspecified. Both
annotators provided a full lexical analysis. For
comparison, a third individual, ?A,? annotated the
same data in full. The three annotators worked
completely independently.
Of the results in tables 2 and 3, the most notable
difference between full and specialized annotation
is that the combination of independent specialized
annotations (C ? D) produces somewhat higher
promiscuity/lower commitment. This is unsurpris-
ing because annotators sometimes overlook rela-
tionships that fall under their specialty.18 Still, an-
notators reported that specialization made the task
17Another example: Some uses of conjunctions like and
and so can be interpreted as either phrasal coordinators or dis-
course connectives (cf. The PDTB Research Group, 2007).
18A more practical and less error-prone approach might be
for specialists to work sequentially or collaboratively (rather
than independently) on each sentence.
com softComPrec
IAA 1 2 N|?|>0 1|2 2|1 F1
Tweets (N=60)
A ? B .82 .91 27 .57 .72 .63
Reviews (N=55)
A ? (C ? D) .95 .76 30 .64 .40 .50
A ? T .92 1 26 .48 .91 .63
(C ? D) ? T .73 1.00 28 .33 .93 .49
Table 3: Measures of inter-annotator agreement. Annotator
labels are as in table 2. Per-annotator com (with lexical rec-
onciliation) and inter-annotator softComPrec are aggregated
over sentences by arithmetic mean.
less burdensome, and the specialized annotations
did prove complementary to each other.19
5.4 Treebank Comparison
Though the annotators in our study were native
speakers well acquainted with representations of
English syntax, we sought to quantify their agree-
ment with the expert treebankers who created the
EWTB (the source of the Reviews sentences). We
converted the EWTB?s constituent parses to de-
pendencies via the PennConverter tool (Johansson
and Nugues, 2007),20 then removed punctuation.
Agreement with the converted treebank parses
appears in the bottom two rows of table 3. Be-
cause the EWTB commits to a single analysis,
precision scores are quite lopsided. Most of its
attachments are consistent with our annotations
(softComPrec > 0.9), but these allow many ad-
ditional analyses (hence the scores below 0.5).
6 Conclusion
We have presented a framework for simple depen-
dency annotation that overcomes some of the rep-
resentational limitations of unlabeled dependency
grammar and embraces the practical realities of
resource-building efforts. Pilot studies (in multiple
languages and domains, supported by a human-
readable notation and a suite of open-source tools)
showed this approach lends itself to rapid annota-
tion with minimal training.
The next step will be to develop algorithms ex-
ploiting these representations for learning parsers.
Other future extensions might include additional
expressive mechanisms (e.g., multi-headedness,
labels), crowdsourcing of FUDG annotations
(Snow et al, 2008), or even a semantic counter-
part to the syntactic representation.
19In fact, for only 2 sentences did ?C? and ?D? have in-
compatible annotations, and both were due to simple mis-
takes that were then fixed in the combination.
20We ran PennConverter with options chosen to emulate
our annotation conventions; see Schneider et al (2013).
58
Acknowledgments
We thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-
jay John, Lori Levin, Andr? Martins, and several anony-
mous reviewers for their insights. This research was sup-
ported in part by the U. S. Army Research Laboratory and
the U. S. Army Research Office under contract/grant number
W911NF-10-1-0533 and by NSF grant IIS-1054319.
References
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.
Benjamin K. Bergen and Nancy Chang. 2005. Embod-
ied Construction Grammar in simulation-based lan-
guage understanding. In Jan-Ola ?stman and Mir-
jam Fried, editors, Construction grammars: cog-
nitive grounding and theoretical extensions, pages
147?190. John Benjamins, Amsterdam.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, Barbora
Hladk?, and Anne Abeill?. 2003. The Prague De-
pendency Treebank: a three-level annotation sce-
nario. In Treebanks: building and using parsed cor-
pora, pages 103?127. Springer.
Seth Chaiken and Daniel J. Kleitman. 1978. Matrix
Tree Theorems. Journal of Combinatorial Theory,
Series A, 24(3):377?381.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
La Haye.
Stephen Clark and James Curran. 2006. Partial training
for a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL (HLT-NAACL 2006), pages 144?151. As-
sociation for Computational Linguistics, New York
City, USA.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Oxford
University Press, Oxford.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies man-
ual. http://nlp.stanford.edu/downloads/
dependencies_manual.pdf.
Kitty Burns Florey. 2006. Sister Bernadette?s Barking
Dog: The quirky history and lost art of diagramming
sentences. Melville House, New York.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incre-
mental sentence processing. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL 2012), pages 61?69. As-
sociation for Computational Linguistics, Montr?al,
Canada.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS Tagging and Parsing the Twitter-
verse. In Proceedings of the 2011 AAAI Workshop
on Analyzing Microtext, pages 20?25. AAAI Press,
San Francisco, CA.
The PDTB Research Group. 2007. The Penn Discourse
Treebank 2.0 annotation manual. Technical Report
IRCS-08-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania, Philadelphia, PA.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Hajic?ov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Jan Hajic?, Barbora Vidov? Hladk?, and Petr Pajas.
2001. The Prague Dependency Treebank: anno-
tation structure and support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114. University of Pennsylvania, Philadelphia,
USA.
Richard A. Hudson. 1984. Word Grammar. Blackwell,
Oxford.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-99), pages 73?79. Association for Computa-
tional Linguistics, College Park, Maryland, USA.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of the
16th Nordic Conference of Computational Linguis-
tics (NODALIDA-2007), pages 105?112. Tartu, Es-
tonia.
Martha Kolln and Robert Funk. 1994. Understanding
English Grammar. Macmillan, New York.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool, San Rafael, CA.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 507?514. Association for Computa-
tional Linguistics, Sydney, Australia.
59
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Marec?ek, Martin Popel, Loganathan Ramasamy,
Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,
and Jan Hajic?. 2013. Cross-language study on in-
fluence of coordination style on dependency parsing
performance. Technial Report 49, ?FAL MFF UK.
Jonathan Margoliash. 2010. Matrix-Tree Theorem for
directed graphs. http://www.math.uchicago.
edu/~may/VIGRE/VIGRE2010/REUPapers/
Margoliash.pdf.
Igor Aleksandrovic? Mel?c?uk. 1988. Dependency Syn-
tax: Theory and Practice. SUNY Press, Albany,
NY.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical Report MSI report 05133,
V?xj? University School of Mathematics and Sys-
tems Engineering, V?xj?, Sweden.
Timothy Osborne, Michael Putnam, and Thomas Gro?.
2012. Catenae: introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390. Association for Computational Lin-
guistics, Atlanta, Georgia, USA.
Alonzo Reed and Brainerd Kellogg. 1877. Work on
English grammar & composition. Clark & Maynard.
Federico Sangati and Chiara Mazza. 2009. An English
dependency treebank ? la Tesni?re. In Marco Pas-
sarotti, Adam Przepi?rkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eigth
International Workshop on Treebanks and Linguistic
Theories, pages 173?184. EDUCatt, Milan, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Mas-
ter?s thesis, University of Zurich.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. arXiv:1306.2091
[cs.CL]. arxiv.org/pdf/1306.2091.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel, Dordrecht and
Academia, Prague.
Stuart M. Shieber. 1992. Constraint-Based Grammar
Formalisms. MIT Press, Cambridge, MA.
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the
Third International Workshop on Parsing Technol-
ogy (IWPT?93), pages 277?292. Tilburg, Nether-
lands.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 132?140. Associa-
tion for Computational Linguistics, Prague, Czech
Republic.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2008), pages 254?263. As-
sociation for Computational Linguistics, Honolulu,
Hawaii.
Mark Steedman. 2000. The Syntatic Process. MIT
Press, Cambridge, MA.
Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.
2011. Design patterns in Fluid Construction Gram-
mar. Number 11 in Constructional Approaches to
Language. John Benjamins, Amsterdam.
Lucien Tesni?re. 1959. El?ments de Syntaxe Struc-
turale. Klincksieck, Paris.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
60
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141?150,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Weakly-Supervised Bayesian Learning of a CCG Supertagger
Dan Garrette
?
Chris Dyer
?
Jason Baldridge
?
Noah A. Smith
?
?
Department of Computer Science, The University of Texas at Austin
?
School of Computer Science, Carnegie Mellon University
?
Department of Linguistics, The University of Texas at Austin
?
Corresponding author: dhg@cs.utexas.edu
Abstract
We present a Bayesian formulation for
weakly-supervised learning of a Combina-
tory Categorial Grammar (CCG) supertag-
ger with an HMM. We assume supervi-
sion in the form of a tag dictionary, and
our prior encourages the use of cross-
linguistically common category structures
as well as transitions between tags that
can combine locally according to CCG?s
combinators. Our prior is theoretically ap-
pealing since it is motivated by language-
independent, universal properties of the
CCG formalism. Empirically, we show
that it yields substantial improvements
over previous work that used similar bi-
ases to initialize an EM-based learner. Ad-
ditional gains are obtained by further shap-
ing the prior with corpus-specific informa-
tion that is extracted automatically from
raw text and a tag dictionary.
1 Introduction
Unsupervised part-of-speech (POS) induction is a
classic problem in NLP. Many proposed solutions
are based on Hidden Markov models (HMMs), with
various improvements obtainable through: induc-
tive bias in the form of tag dictionaries (Kupiec,
1992; Merialdo, 1994), sparsity constraints (Lee
et al., 2010), careful initialization of parameters
(Goldberg et al., 2008), feature based represen-
tations (Berg-Kirkpatrick et al., 2010; Smith and
Eisner, 2005), and priors on model parameters
(Johnson, 2007; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011, inter alia).
When tag dictionaries are available, a situa-
tion we will call type-supervision, POS induc-
tion from unlabeled corpora can be relatively suc-
cessful; however, as the number of possible tags
increases, performance drops (Ravi and Knight,
2009). In such cases, there are a large number
of possible labels for each token, so picking the
right one simply by chance is unlikely; the pa-
rameter space tends to be large; and devising good
initial parameters is difficult. Therefore, it is un-
surprising that the unsupervised (or even weakly-
supervised) learning of a Combinatory Categorial
Grammar (CCG) supertagger, which labels each
word with one of a large (possibly unbounded)
number of structured categories called supertags,
is a considerable challenge.
Despite the apparent complexity of the task, su-
pertag sequences have regularities due to univer-
sal properties of the CCG formalism (?2) that can
be used to reduce the complexity of the problem;
previous work showed promising results by using
these regularities to initialize an HMM that is then
refined with EM (Baldridge, 2008). Here, we ex-
ploit CCG?s category structure to motivate a novel
prior over HMM parameters for use in Bayesian
learning (?3). This prior encourages (i) cross-
linguistically common tag types, (ii) tag bigrams
that can combine using CCG?s combinators, and
(iii) sparse transition distributions. We also go be-
yond the use of these universals to show how ad-
ditional, corpus-specific information can be auto-
matically extracted from a combination of the tag
dictionary and raw data, and how that information
can be combined with the universal knowledge for
integration into the model to improve the prior.
We use a blocked sampling algorithm to sam-
ple supertag sequences for the sentences in the
training data, proportional to their posterior prob-
ability (?4). We experimentally verify that
our Bayesian formulation is effective and sub-
stantially outperforms the state-of-the-art base-
line initialization/EM strategy in several languages
(?5). We also evaluate using tag dictionaries that
are unpruned and have only partial word coverage,
finding even greater improvements in these more
realistic scenarios.
141
2 CCG and Supertagging
CCG (Steedman, 2000; Steedman and Baldridge,
2011) is a grammar formalism in which each lex-
ical token is associated with a structured category,
often referred to as a supertag. CCG categories are
defined by the following recursive definition:
C ? {S, N, NP, PP, ...}
C ? {C/C,C\C}
A CCG category can either be an atomic cate-
gory indicating a particular type of basic gram-
matical phrase (S for a sentence, N for a noun,
NP for a noun phrase, etc), or a complex category
formed from the combination of two categories
by one of two slash operators. In CCG, complex
categories indicate a grammatical relationship be-
tween the two operands. For example, the cate-
gory (S\NP)/NP might describe a transitive verb,
looking first to its right (indicated by /) for an ob-
ject, then to its left (\) for a subject, to produce a
sentence. Further, atomic categories may be aug-
mented with features, such as S
dcl
, to restrict the
set of atoms with which they may unify. The task
of assigning a category to each word in a text is
called supertagging (Bangalore and Joshi, 1999).
Because they are recursively defined, there is
an infinite number of potential CCG categories
(though in practice it is limited by the number
of actual grammatical contexts). As a result, the
number of supertags appearing in a corpus far ex-
ceeds the number of POS tags (see Table 1). Since
supertags specify the grammatical context of a to-
ken, and high frequency words appear in many
contexts, CCG grammars tend to have very high
lexical ambiguity, with frequent word types asso-
ciating with a large number of categories. This
ambiguity has made type-supervised supertagger
learning very difficult because the typical ap-
proaches to initializing parameters for EM become
much less effective.
Grammar-informed supertagger learning.
Baldridge (2008) was successful in extending the
standard type-supervised tagger learning to the
task of CCG supertagging by setting the initial
parameters for EM training of an HMM using
two intrinsic properties of the CCG formalism:
the tendency for adjacent tags to combine, and
the tendency to use less complex tags. These
properties are explained in detail in the original
work, but we restate the ideas briefly throughout
this paper for completeness.
X/Y Y ? X (>)
Y X\Y ? X (<)
X/Y Y/Z ? X/Z (>B)
Y \Z X\Y ? X\Z (<B)
Y/Z X\Y ? X/Z (<B
?
)
Figure 1: Combination rules used by CCGBank.
S
NP
NP/N
N
S\NP
(S\NP)/NP
NP
NP/N
N
The
man
walks
a
dog
Figure 2: CCG parse for ?The man walks a dog.?
Tag combinability. A CCG parse of a sentence is
derived by recursively combining the categories of
sub-phrases. Category combination is performed
using only a small set of generic rules (see Fig-
ure 1). In the tree in Figure 2, we can see that
a and dog can combine via Forward Application
(>), with NP/N and N combining to produce NP.
The associativity engendered by CCG?s compo-
sition rules means that most adjacent lexical cate-
gories may be combined. In the Figure 2 tree, we
can see that instead of combining (walks?(a?dog)),
we could have combined ((walks?a)?dog) since
(S\NP)/NP and NP/N can combine using >B.
3 Model
In this section we define the generative process
we use to model a corpus of sentences. We begin
by generating the model parameters: for each
supertag type t in the tag set T , the transition
probabilities to the next state (pi
t
) and the emis-
sion probabilities (?
t
) are generated by draws
from Dirichlet distributions parameterized with
per-tag mean distributions (pi
0
t
and ?
0
t
, respec-
tively) and concentration parameters (?
pi
and
?
?
). By setting ?
pi
close to zero, we can encode
our prior expectation that transition distributions
should be relatively peaked (i.e., that each tag
type should be followed by relatively few tag
types). The prior means, discussed below, encode
both linguistic intuitions about expected tag-tag
transition behavior and automatically-extracted
corpus information. Given these parameters, we
next generate the sentences of the corpus. This
process is summarized as follows:
142
Parameters:
?
t
? Dirichlet(?
?
, ?
0
t
) ?t ? T
pi
t
? Dirichlet(?
pi
, pi
0
t
) ?t ? T
Sentence:
y
1
? Categorical(pi
?S?
)
for i ? {1, 2, . . .}, until y
i
= ?E?
x
i
| y
i
? Categorical(?
y
i
)
y
i+1
| y
i
? Categorical(pi
y
i
)
This model can be understood as a Bayesian
HMM (Goldwater and Griffiths, 2007). We next
discuss how the prior distributions are constructed
to build in additional inductive bias.
3.1 Transition Prior Means (pi
0
t
)
We use the prior mean for each tag?s transition dis-
tribution to build in two kinds of bias. First, we
want to favor linguistically probable tags. Second,
we want to favor transitions that result in a tag
pair that combines according to CCG?s combina-
tors. For simplicity, we will define pi
0
t
as a mixture
of two components, the first, P
pi
(u) is an (uncon-
ditional) distribution over category types u that fa-
vors cross-linguistically probable categories. The
second component, P
pi
(u | t), conditions on the
previous tag type, t, and assigns higher probabil-
ity to pairs of tags that can be combined. That is,
the probability of transitioning from t to u in the
Dirichlet mean distribution is given by
1
pi
0
t
(u) = ? ? P
pi
(u) + (1? ?) ? P
pi
(u | t).
We discuss the two mixture components in turn.
3.1.1 Unigram Category Generator (P
pi
(u))
In this section, we define a CCG category gener-
ator that generates cross-linguistically likely cat-
egory types. Baldridge?s approach estimated the
likelihood of a category using the inverse number
of sub-categories: P
CPLX
(u) ? 1/complexity(u).
We propose an improvement, P
G
, expressed as a
probabilistic grammar:
2
C ? a p
term
?p
atom
(a)
C ? A/A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A/B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
C ? A\A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A\B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
1
Following Baldridge (2008), we fix ? = 0.5 for our ex-
periments.
2
For readability, we use the notation p = (1? p).
where A,B,C are categories and a is an atomic
category (and terminal): a ? {S, N, NP, ...}.
3
We have designed this grammar to capture sev-
eral important CCG characteristics. In particular
we encode four main ideas, each captured through
a different parameter of the grammar and dis-
cussed in greater detail below:
1. Simpler categories are more likely: e.g. N/N is
a priori more likely than (N/N)/(N/N).
2. Some atoms are more likely than others: e.g.
NP is more likely than S, much more than NP
nb
.
3. Modifiers are more likely: e.g. (S\NP)/(S\NP)
is more likely than (S\NP)/(NP\NP).
4. Operators occur with different frequencies.
The first idea subsumes the complexity measure
used by Baldridge, but accomplishes the goal nat-
urally by letting the probabilities decrease as the
category grows. The rate of decay is governed
by the p
term
parameter: the marginal probability
of generating a terminal (atomic) category in each
expansion. A higher p
term
means a stronger em-
phasis on simplicity. The probability distribution
over categories is guaranteed to be proper so long
as p
term
>
1
2
since the probability of the depth of a
tree will decrease geometrically (Chi, 1999).
The second idea is a natural extension of the
complexity concept and is particularly relevant
when features are used. The original complex-
ity measure treated all atoms uniformly, but e.g.
we would expect NP
expl
/N to be less likely than
NP/N since it contains the more specialized, and
thus rarer, atom NP
expl
. We define the distribution
p
atom
(a) as the prior over atomic categories.
Due to our weak, type-only supervision, we
have to estimate p
atom
from just the tag dictionary
and raw corpus, without frequency data. Our goal
is to estimate the number of each atom in the su-
pertags that should appear on the raw corpus to-
kens. Since we don?t know what the correct su-
pertags are, we first estimate counts of supertags,
from which we can extract estimated atom counts.
Our strategy is to uniformly distribute each raw
corpus token?s counts over all of its possible su-
pertags, as specified in the tag dictionary. Word
types not appearing in the tag dictionary are ig-
3
While very similar to standard probabilistic context-free
grammars seen in NLP work, this grammar is not context-free
because modifier categories must have matching operands.
However, this is not a problem for our approach since the
grammar is unambiguous, defines a proper probability distri-
bution, and is only used for modeling the relative likelihoods
of categories (not parsing categories).
143
nored for the purposes of these estimates. Assum-
ing that C(w) is the number of times that word
type w is seen in the raw corpus, atoms(a, t) is the
number of times atom a appears in t, TD(w) is the
set of tags associated with w, and TD(t) is the set
of word types associated with t:
C
supertag
(t) =
?
w?TD(t)
(C(w)+?)/|TD(w)|
C
atom
(a) =
?
t?T
atoms(a, t) ? C
supertag
(t)
p
atom
(a) ? C
atom
(a) + ?
Adding ? smooths the estimates.
Using the raw corpus and tag dictionary data to
set p
atom
allows us to move beyond Baldridge?s
work in another direction: it provides us with a
natural way to combine CCG?s universal assump-
tions with corpus-specific data.
The third and fourth ideas pertain only to com-
plex categories. If the category is complex, then
we consider two additional parameters. The pa-
rameter p
fw
is the marginal probability that the
complex category?s operator specifies a forward
argument. The parameter p
mod
gives the amount
of marginal probability mass that is allocated for
modifier categories. Note that it is not necessary
for p
mod
to be greater than
1
2
to achieve the de-
sired result of making modifier categories more
likely than non-modifier categories: the number
of potential modifiers make up only a tiny fraction
of the space of possible categories, so allocating
more than that mass as p
mod
will result in a cate-
gory grammar that gives disproportionate weight
to modifiers, increasing the likelihood of any par-
ticular modifier from what it would otherwise be.
3.1.2 Bigram Category Generator (P
pi
(u | t))
While the above processes encode important prop-
erties of the distribution over categories, the in-
ternal structure of categories is not the full story:
cross-linguistically, the categories of adjacent to-
kens are much more likely to be combinable via
some CCG rule. This is the second component of
our mixture model.
Baldridge derives this bias by allocating the ma-
jority of the transition probability mass from each
tag t to tags that can follow t according to some
combination rule. Let ?(t,u) be an indicator of
whether t connects to u; for ? ? [0, 1]:
4
P
?
(u | t) =
{
? ? uniform(u) if ?(t,u)
(1? ?) ? uniform(u) otherwise
4
Again, following Baldridge (2008), we fix ? = 0.95 for
our experiments.
There are a few additional considerations that
must be made in defining ?, however. In assum-
ing the special tags ?S? and ?E? for the start and
end of the sentence, respectively, we can define
?(?S?,u) = 1 when u seeks no left-side argu-
ments (since there are no tags to the left with
which to combine) and ?(t, ?E?) = 1 when t seeks
no right-side arguments. So ?(?S?, NP/N) = 1, but
?(?S?, S\NP) = 0. If atoms have features asso-
ciated, then the atoms are allowed to unify if the
features match, or if at least one of them does
not have a feature. So ?(NP
nb
, S\NP) = 1, but
?(NP
nb
, S\NP
conj
) = 0. In defining ?, it is also im-
portant to ignore possible arguments on the wrong
side of the combination since they can be con-
sumed without affecting the connection between
the two. To achieve this for ?(t,u), it is assumed
that it is possible to consume all preceding argu-
ments of t and all following arguments of u. So
?(NP, (S\NP)/NP) = 1. This helps to ensure the
associativity discussed earlier. Finally, the atom
NP is allowed to unify with N if N is the argument.
So ?(N, S\NP) = 1, but ?(NP/N, NP) = 0. This is
due to the fact that CCGBank assumes that N can
be rewritten as NP.
Type-supervised initialization. As above, we
want to improve upon Baldridge?s ideas by en-
coding not just universal CCG knowledge, but
also automatically-induced corpus-specific infor-
mation where possible. To that end, we can de-
fine a conditional distribution P
tr
(u | t) based on
statistics from the raw corpus and tag dictionary.
We use the same approach as we did above for set-
ting p
atom
(and the definition of ?
0
t
below): we esti-
mate by evenly distributing raw corpus counts over
the tag dictionary entries. Assume that C(w
1
, w
2
)
is the (?-smoothed) count of times word type w
1
was directly followed byw
2
in the raw corpus, and
ignoring any words not found in the tag dictionary:
C(t,u) = ?+
?
w
1
?TD(t), w
2
?TD(u)
C(w
1
, w
2
)
|TD(w
1
)| ? |TD(w
2
)|
P
tr
(u | t) = C(t,u)/
?
u
?
C(t,u
?
)
Then the alternative definition of the compatibility
distribution is as follows:
P
tr
?
(u | t) =
{
? ? P
tr
(u | t) if ?(t,u)
(1??) ? P
tr
(u | t) otherwise
144
Our experiments compare performance when
pi
0
t
is set using P
pi
(u)=P
CPLX
(experiment 3) ver-
sus our category grammar P
G
(4?6), and using
P
pi
(u | t) = P
?
as the compatibility distribution
(3?4) versus P
tr
?
(5?6).
3.2 Emission Prior Means (?
0
t
)
For each supertag type t, ?
0
t
is the mean distri-
bution over words it emits. While Baldridge?s
approach used a uniform emission initialization,
treating all words as equally likely, we can,
again, induce token-level corpus-specific informa-
tion:
5
To set ?
0
t
, we use a variant and simplifica-
tion of the procedure introduced by Garrette and
Baldridge (2012) that takes advantage of our prior
over categories P
G
.
Assuming that C(w) is the count of word type
w in the raw corpus, TD(w) is the set of supertags
associated with word type w in the tag dictionary,
and TD(t) is the set of known word types associ-
ated with supertag t, the count of word/tag pairs
for known words (words appearing in the tag dic-
tionary) is estimated by uniformly distributing a
word?s (?-smoothed) raw counts over its tag dic-
tionary entries:
C
known
(t, w) =
{
C(w)+?
|TD(w)|
if t ? TD(w)
0 otherwise
For unknown words, we first use the idea of tag
?openness? to estimate the likelihood of a partic-
ular tag t applying to an unknown word: if a tag
applies to many word types, it is likely to apply to
some new word type.
P (unk | t) ? |known words w s.t. t ? TD(w)|
Then, we apply Bayes? rule to get P (t | unk), and
use that to estimate word/tag counts for unknown
words:
P (t | unk) ? P (unk | t) ? P
G
(t)
C
unk
(t, w) = C(w) ? P (t | unk)
Thus, with the estimated counts for all words:
P
em
(w | t) =
C
known
(t, w) + C
unk
(t, w)
?
w
?
C
known
(t, w
?
) + C
unk
(t, w
?
)
We perform experiments comparing perfor-
mance when ?
0
t
is uniform (3?5) and when
?
0
t
(w) = P
em
(w | t) (6).
5
Again, without gold tag frequencies.
4 Posterior Inference
We wish to find the most likely supertag of each
word, given the model we just described and a cor-
pus of training data. Since there is exact inference
with these models is intractable, we resort to Gibbs
sampling to find an approximate solution. At a
high level, we alternate between resampling model
parameters (?
t
, pi
t
) given the current tag sequence
and resampling tag sequences given the current
model parameters and observed word sequences.
It is possible to sample a new tagging from the
posterior distribution over tag sequences for a sen-
tence, given the sentence and the HMM parameters
using the forward-filter backward-sample (FFBS)
algorithm (Carter and Kohn, 1996). To effi-
ciently sample new HMM parameters, we exploit
Dirichlet-multinomial conjugacy. By repeating
these alternating steps and accumulating the num-
ber of times each supertag is used in each position,
we obtain an approximation of the required poste-
rior quantities.
Our inference procedure takes as input the tran-
sition prior means pi
0
t
, the emission prior means
?
0
t
, and concentration parameters ?
pi
and ?
?
,
along with the raw corpus and tag dictionary. The
set of supertags associated with a word w will be
known as TD(w). We will refer to the set of word
types included in the tag dictionary as ?known?
words and others as ?unknown? words. For sim-
plicity, we will assume that TD(w), for any un-
known word w, is the full set of CCG categories.
During sampling, we always restrict the possible
tag choices for a word w to the categories found in
TD(w). We refer to the sequence of word tokens
as x and tags as y.
We initialize the sampler by setting pi
t
= pi
0
t
and ?
t
= ?
0
t
and then sampling tagging sequences
using FFBS.
To sample a tagging for a sentence x, the strat-
egy is to inductively compute, for each token x
i
starting with i = 0 and going ?forward?, the prob-
ability of generating x
0
, x
1
, . . . , x
i
via any tag se-
quence that ends with y
i
= u:
p(y
i
= u | x
0:i
) =
?
u
(x
i
) ?
?
t?T
pi
t
(u) ? p(y
i?1
= t | x
0:i?1
)
We then pass through the sequence again, this time
?backward? starting at i = |x| ? 1 and sampling
y
i
| y
i+1
? p(y
i
= t | x
0:i
) ? pi
t
(y
i+1
).
145
num. raw TD TD ambiguity dev test
Corpus tags tokens tokens entries type token tokens tokens
English
CCGBank POS 50
158k 735k
45k 3.75 13.11 ? ?
CCGBank 1,171 65k 56.98 296.18 128k 127k
Chinese CTB-CCG 829 99k 439k 60k 96.58 323.37 59k 85k
Italian CCG-TUT 955 6k 27k 9k 178.88 426.13 5k 5k
Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT
is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates
are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English
POS statistics are shown only for comparison; only CCG experiments were run.
The block-sampling approach of choosing new
tags for a sentence all at once is particularly ben-
eficial given the sequential nature of the model of
the HMM. In an HMM, a token?s adjacent tags tend
to hold onto its current tag due to the relation-
ships between the three. Resampling all tags at
once allows for more drastic changes at each it-
eration, providing better opportunities for mixing
during inference. The FFBS approach has the ad-
ditional advantage that, by resampling the distri-
butions only once per iteration, we are able to re-
sample all sentences in parallel. This is not strictly
true of all HMM problems with FFBS, but because
our data is divided by sentence, and each sentence
has a known start and end tag, the tags chosen dur-
ing the sampling of one sentence cannot affect the
sampling of another sentence in the same iteration.
Once we have sampled tags for the entire cor-
pus, we resample pi and ?. The newly-sampled
tags y are used to compute C(w, t), the count of
tokens with word type w and tag t, and C(t,u),
the number of times tag t is directly followed by
tag u. We then sample, for each t ? T where T is
the full set of valid CCG categories:
pi
t
? Dir
(
??
pi
? pi
0
t
(u) + C(t,u)?
u?T
)
?
t
? Dir
(
??
?
? ?
0
t
(w) + C(w, t)?
w?V
)
It is important to note that this method of re-
sampling allows the draws to incorporate both the
data, in the form of counts, and the prior mean,
which includes all of our carefully-constructed bi-
ases derived from both the intrinsic, universal CCG
properties as well as the information we induced
from the raw corpus and tag dictionary.
With the distributions resampled, we can con-
tinue the procedure by resampling tags as above,
and then resampling distributions again, until a
maximum number of iterations is reached.
5 Experiments
6
To evaluate our approach, we used CCGBank
(Hockenmaier and Steedman, 2007), which is
a transformation of the English Penn Treebank
(Marcus et al., 1993); the CTB-CCG (Tse and
Curran, 2010) transformation of the Penn Chinese
Treebank (Xue et al., 2005); and the CCG-TUT
corpus (Bos et al., 2009), built from the TUT cor-
pus of Italian text (Bosco et al., 2000). Statistics
on the size and ambiguity of these datasets are
shown in Table 1.
For CCGBank, sections 00?15 were used for
extracting the tag dictionary, 16?18 for the raw
corpus, 19?21 for development data, and 22?24
for test data. For TUT, the first 150 sentences of
each of the CIVIL LAW and NEWSPAPER sections
were used for raw data, the next sentences 150?
249 of each was used for development, and the
sentences 250?349 were used for test; the remain-
ing data, 457 sentences from CIVIL LAW and 548
from NEWSPAPER, plus the much smaller 132-
sentence JRC ACQUIS data, was used for the tag
dictionary. For CTB-CCG, sections 00?11 were
used for the tag dictionary, 20?24 for raw, 25?27
for dev, and 28?31 for test.
Because we are interested in showing the rel-
ative gains that our ideas provide over Baldridge
(2008), we reimplemented the initialization pro-
cedure from that paper, allowing us to evaluate
all approaches consistently. For each dataset, we
ran a series of experiments in which we made fur-
ther changes from the original work. We first ran
a baseline experiment with uniform transition and
emission initialization of EM (indicated as ?1.? in
Table 2) followed by our reimplementation of the
initialization procedure by Baldridge (2). We then
6
All code and experimental scripts are available
at http://www.github.com/dhgarrette/
2014-ccg-supertagging
146
Corpus English Chinese Italian
TD cutoff 0.1 0.01 0.001 no 0.1 0.01 0.001 no 0.1 0.01 0.001 no
1. uniform EM 77 62 47 38 64 39 30 26 51 32 30 30
2. init (Baldridge) EM 78 67 55 41 66 43 33 28 54 36 33 32
3. init Bayes 74 68 56 42 65 56 47 37 52 46 40 40
4. P
G
Bayes 74 70 59 42 64 57 47 36 52 40 39 40
5. P
G
, P
tr
?
Bayes 75 72 61 50 66 58 49 44 52 44 41 43
6. P
G
, P
tr
?
, P
em
Bayes 80 80 73 51 69 62 56 49 53 47 45 46
Table 2: Experimental results: test-set per-token supertag accuracies. ?TD cutoff? indicates the level of
tag dictionary pruning; see text. (1) is uniform EM initialization. (2) is a reimplementation of (Baldridge,
2008). (3) is Bayesian formulation using only the ideas from Baldridge: P
CPLX
, P
?
, and uniform emis-
sions. (4?6) are our enhancements to the prior: using our category grammar in P
G
instead of P
CPLX
, using
P
tr
?
instead of P
?
, and using P
em
instead of uniform.
experimented with the Bayesian formulation, first
using the same information used by Baldridge, and
then adding our enhancements: using our category
grammar in P
G
, using P
tr
?
as the transition com-
patability distribution, and using P
em
as ?
0
t
(w).
For each dataset, we ran experiments using four
different levels of tag dictionary pruning. Prun-
ing is the process of artificially removing noise
from the tag dictionary by using token-level anno-
tation counts to discard low-probability tags; for
each word, for cutoff x, any tag with probability
less than x is excluded. Tag dictionary pruning
is a standard procedure in type-supervised train-
ing, but because it requires information that does
not truly conform to the type-supervised scenario,
we felt that it was critical to demonstrate the per-
formance of our approach under situations of less
pruning, including no artificial pruning at all.
We emphasize that unlike in most previous
work, we use incomplete tag dictionaries. Most
previous work makes the unrealistic assumption
that the tag dictionary contains an entry for ev-
ery word that appears in either the training or test-
ing data. This is a poor approximation of a real
tagging system, which will never have complete
lexical knowledge about the test data. Even work
that only assumes complete knowledge of the tag-
ging possibilities for the lexical items in the train-
ing corpus is problematic (Baldridge, 2008; Ravi
et al., 2010). This still makes learning unrealisti-
cally easy since it dramatically reduces the ambi-
guity of words that would have been unseen, and,
in the case of CCG, introduces additional tags that
would not have otherwise been known. To ensure
that our experiments are more realistic, we draw
our tag dictionary entries from data that is totally
disjoint from both the raw and test corpora. Dur-
ing learning, any unknown words (words not ap-
pearing in the tag dictionary) are unconstrained so
that they may take any tag, and are, thus, maxi-
mally ambiguous.
We only performed minimal parameter tuning,
choosing instead to stay consistent with Baldridge
(2008) and simply pick reasonable-seeming val-
ues for any additional parameters. Any tuning that
was performed was done with simple hill-climbing
on the development data of English CCGBank.
All parameters were held consistent across exper-
iments, including across languages. For EM, we
used 50 iterations; for FFBS we used 100 burn-
in iterations and 200 sampling iterations.
7
For
all experiments, we used ? = 0.95 for P
(tr)
?
and
? = 0.5 for pi
0
t
to be consistent with previous
work, ?
pi
= 3000, ?
?
= 7000, p
term
= 0.6,
p
fw
= 0.5, p
mod
= 0.8, and ? = 1000 for p
atom
.
Test data was run only once, for the final figures.
The final results reported were achieved by us-
ing the following training sequence: initialize pa-
rameters according to the scenario, train an HMM
using EM or FFBS starting with that set of parame-
ters, tag the raw corpus with the trained HMM, add-
0.1 smooth counts from the now-tagged raw cor-
pus, and train a maximum entropy Markov model
(MEMM) from this ?auto-supervised? data.
8
Results are shown in Table 2. Most notably, the
contributions described in this paper improve re-
sults in nearly every experimental scenario. We
can see immediate, often sizable, gains in most
7
Final counts are averaged across the sampling iterations.
8
Auto-supervised training of an MEMM increases accu-
racy by 1?3% on average (Garrette and Baldridge, 2013). We
use the OpenNLP MEMM implementation with its standard
set of features: http://opennlp.apache.org
147
cases simply by using the Bayesian formulation.
Further gains are seen from adding each of the
other various contributions of this paper. Perhaps
most interestingly, the gains are only minimal with
maximum pruning, but the gains increase as the
pruning becomes less aggressive ? as the scenar-
ios become more realistic. This indicates that our
improvements make the overall procedure more
robust.
Error Analysis Like POS-taggers, the learned
supertagger frequently confuses nouns (N) and
their modifiers (N/N), but the most frequent er-
ror made by the English (6) experiment was
(((S\NP)\(S\NP))/N) instead of (NP
nb
/N). How-
ever, these are both determiner types, indicating an
interesting problem for the supertagger: it often
predicts an object type-raised determiner instead
of the vanilla NP/N, but in many contexts, both cat-
egories are equally valid. (In fact, for parsers that
use type-raising as a rule, this distinction in lexical
categories does not exist.)
6 Related Work
Ravi et al. (2010) also improved upon the work by
Baldridge (2008) by using integer linear program-
ming to find a minimal model of supertag transi-
tions, thereby generating a better starting point for
EM than the grammatical constraints alone could
provide. This approach is complementary to the
work presented here, and because we have shown
that our work yields gains under tag dictionaries
of various levels of cleanliness, it is probable that
employing minimization to set the base distribu-
tion for sampling could lead to still higher gains.
On the Bayesian side, Van Gael et al. (2009)
used a non-parametric, infinite HMM for truly un-
supervised POS-tagger learning (Van Gael et al.,
2008; Beal et al., 2001). While their model is not
restricted to the standard set of POS tags, and may
learn a more fine-grained set of labels, the induced
labels are arbitrary and not grounded in any gram-
matical formalism.
Bisk and Hockenmaier (2013) developed an ap-
proach to CCG grammar induction that does not
use a tag dictionary. Like ours, their procedure
learns from general properties of the CCG formal-
ism. However, while our work is intended to pro-
duce categories that match those used in a partic-
ular training corpus, however complex they might
be, their work produces categories in a simplified
form of CCG in which N and S are the only atoms
and no atoms have features. Additionally, they as-
sume that their training corpus is annotated with
POS tags, whereas we assume truly raw text.
Finally, we find the task of weakly-supervised
supertagger learning to be particularly relevant
given the recent surge in popularity of CCG.
An array of NLP applications have begun using
CCG, including semantic parsing (Zettlemoyer and
Collins, 2005) and machine translation (Weese et
al., 2012). As CCG finds more applications, and
as these applications move to lower-resource do-
mains and languages, there will be increased need
for the ability to learn without full supervision.
7 Conclusion and Future Work
Standard strategies for type-supervised HMM es-
timation are less effective as the number of cat-
egories increases. In contrast to POS tag sets,
CCG supertags, while quite numerous, have struc-
tural clues that can simplify the learning prob-
lem. Baldridge (2008) used this formalism-
specific structure to inform an initialization pro-
cedure for EM. In this work, we have shown that
CCG structure can instead be used to motivate an
effective prior distribution over the parameters of
an HMM supertagging model, allowing our work
to outperform Baldridge?s previously state-of-the-
art approach, and to do so in a principled manner
that lends itself better to future extensions such as
incorporation in more complex models.
This work also improves on Baldridge?s simple
?complexity? measure, developing instead a prob-
abilistic category grammar over supertags that al-
lows our prior to capture a wider variety of inter-
esting and useful properties of the CCG formalism.
Finally, we were able to achieve further gains
by augmenting the universal CCG knowledge with
corpus-specific information that could be automat-
ically extracted from the weak supervision that is
available: the raw corpus and the tag dictionary.
This allows us to combine the cross-linguistic
properties of the CCG formalism with corpus- or
language-specific information in the data into a
single, unified Bayesian prior.
Our model uses a relatively large number of pa-
rameters, e.g., p
term
, p
fw
, p
mod
, p
atom
, in the prior.
Here, we fixed each to a single value (i.e., a ?fully
Bayesian? approach). Future work might explore
sensitivity to these choices, or empirical Bayesian
or maximum a posteriori inference for their values
(Johnson and Goldwater, 2009).
148
In this work, as in most type-supervised work,
the tag dictionary was automatically extracted
from an existing tagged corpus. However, a tag
dictionary could instead be automatically induced
via multi-lingual transfer (Das and Petrov, 2011)
or generalized from human-provided information
(Garrette and Baldridge, 2013; Garrette et al.,
2013). Again, since the approach presented here
has been shown to be somewhat robust to tag dic-
tionary noise, it is likely that the model would
perform well even when using an automatically-
induced tag dictionary.
Acknowledgements
This work was supported by the U.S. Department
of Defense through the U.S. Army Research Of-
fice (grant number W911NF-10-1-0533). Exper-
iments were run on the UTCS Mastodon Cluster,
provided by NSF grant EIA-0303609.
References
Jason Baldridge. 2008. Weakly supervised supertag-
ging with grammar-informed initialization. In Pro-
ceedings of COLING.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2).
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The innite hidden Markov
model. In NIPS.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
model for inducing combinatory categorial gram-
mars. Transactions of the Association for Compu-
tational Linguistics, 1.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of ACL.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cat-
egorial grammar treebank for Italian. In M. Pas-
sarotti, Adam Przepi?orkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8).
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank
for Italian: a data-driven annotation schema. In Pro-
ceedings of LREC.
Christopher K. Carter and Robert Kohn. 1996. On
Gibbs sampling for state space models. Biometrika,
81(3):341?553.
Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1).
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of POS-
taggers for low-resource languages. In Proceedings
of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: Ex-
periments on unsupervised word segmentation with
adaptor grammars. In Proceedings of NAACL.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &
Language, 6(3).
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
149
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010.
Minimized models and grammar-informed initial-
ization for supertagging with highly ambiguous lex-
icons. In Proceedings of ACL, pages 495?503.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Borjars, editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar. Wiley-
Blackwell.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: Extracting CCG derivations from the Penn
Chinese treebank. In Proceedings of COLING.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite hidden Markov model. In Proceedings of
ICML.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of EMNLP.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proceedings of WMT.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
150
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 5?7,
Baltimore, Maryland, USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Overview of the 2014 NLP Unshared Task in PoliInformatics
Noah A. Smith
?
Claire Cardie
?
Anne L. Washington
?
John D. Wilkerson
?
?
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?
Department of Computer Science, Cornell University, Ithaca, NY 14853, USA
?
School of Public Policy, George Mason University, Arlington, VA 22201, USA
?
Department of Political Science, University of Washington, Seattle, WA 98195, USA
?
Corresponding author: nasmith@cs.cmu.edu
Abstract
We describe a research activity carried
out during January?April 2014, seeking to
increase engagement between the natural
language processing research community
and social science scholars. In this activ-
ity, participants were offered a corpus of
text relevant to the 2007?8 financial cri-
sis and an open-ended prompt. Their re-
sponses took the form of a short paper
and an optional demonstration, to which a
panel of judges will respond with the goal
of identifying efforts with the greatest po-
tential for future interdisciplinary collabo-
ration.
1 Introduction
In recent years, numerous interdisciplinary re-
search meetings have sought to bring together
computer scientists with expertise in automated
text data analysis and scholars with substan-
tive interests that might make use of text data.
The latter group has included political scientists,
economists, and communications scholars. An
NSF Research Coordination Network grant to en-
courage research using open government data was
awarded to co-authors Washington and Wilker-
son in 2013. The network for Political Informat-
ics, or PoliInformatics, brought together a steering
committee from diverse research backgrounds that
convened in February 2013. At that meeting, a
substantive focus on the 2007?8 nancial crisis was
selected.
Drawing inspiration from the ?shared task?
model that has been successful in the natural lan-
guage processing community, we designed a re-
search competition for computer scientists. In a
shared task, a gold-standard dataset is created in
advance of the competition, inputs and outputs are
defined by the organizers, typically creating a su-
pervised learning setup with held-out data used for
evaluation. Constraints on the resources that may
be used are typically set in place as well, to fo-
cus the energies of participants on a core prob-
lem, and the official evaluation scores are pub-
lished, usually as open-source software. Final sys-
tems (or system output) is submitted by a dead-
line and judged automatically against the gold-
standard. Participants report on their systems in
short papers, typically presented at a meeting as-
sociated with a conference or workshop.
With neither a clear definition of what the fi-
nal outcome might be, nor the resources to create
the necessary gold-standard data, we developed a
more open-ended competition. A text corpus was
collected and made available, and a prompt was
offered. Participants were given freedom in how
to respond; competition entries took the form of
short research papers and optional demonstrations
of the results of the projects. Rather than an ob-
jective score, a panel of judges organized by the
PoliInformatics steering committee offered public
reviews of the work, with an emphasis on poten-
tial for future interdisciplinary research efforts that
might stem from these preliminary projects.
2 Setup
The prompts offered to participants were:
Who was the financial crisis? We seek to un-
derstand the participants in the lawmaking and
regulatory processes that formed the government?s
response to the crisis: the individuals, indus-
tries, and professionals targeted by those poli-
cies; the agencies and organizations responsi-
ble for implementing them; and the lobbyists,
witnesses, advocates, and politicians who were
actively involved?and the connections among
them.
What was the financial crisis? We seek to un-
derstand the cause(s) of the crisis, proposals for
reform, advocates for those proposals, arguments
5
for and against, policies ultimately adopted by the
government, and the impact of those policies.
The set of datasets made available is listed in
Table 1. Several additional datasets were sug-
gested on the website,
1
but were not part of the
official data.
3 Response
Forty teams initially registered to participate in the
unshared task; ten submitted papers. The teams
came from a variety of institutions spread across
six countries. Half of the teams included links to
online demonstrations or browsable system out-
put. At this writing, the papers are under review
by the panel of judges. We provide a very brief
summary of the contributions of each team.
3.1 Who was the financial crisis?
Bordea et al. (2014) inferred importance and hi-
erarchy of topics along with expertise mining to
find which participants in the discourse might be
experts (e.g., Paul Volcker and ?proprietary trad-
ing?) based on FOMC, FCIC, and Congressional
hearing and report data.
Baerg et al. (2014) considered transcripts of the
FOMC, developing a method for scaling the pref-
erences of its members with respect to inflation
(hawks to doves); the method incorporates auto-
matic dimensionality reduction and expert topic
interpretation.
Zirn et al. (2014) also focused on the transcripts,
distinguishing between position-taking statements
and shorter ?discussion elements? that express
agreement or disagreement rather than substance,
and used this analysis to quantify similarity among
FOMC members and take first steps toward extrac-
tion of sub-dialogues among them.
Bourreau and Poibeau (2014) focused on the
FCIC report and the two Congressional reports,
identifying named entities and then visualizing
correlations among mentions both statically (as
networks) and dynamically. Clark et al. (2014)
considered Congressional hearings, applying a
reasoning model that integrates analysis of social
roles and relationships with analysis of individ-
ual beliefs in hope of detecting opinion shifts and
signs of influence.
With an eye toward substantive hypotheses
about dependencies among banks? access to
1
https://sites.google.com/site/
unsharedtask2014
bailout funds relating to underlying social con-
nections, Morales et al. (2014) automatically ex-
tracted a social network from the corpus alongside
structured data in Freebase.
3.2 What was the financial crisis?
Miller and McCoy (2014) considered FOMC tran-
scripts, applying topic models for dimensionality
reduction and viewing topic proportions as time
series.
In a study of the TARP, Dodd-Frank, and the
health reform bills, Li et al. (2014) explored the
ideas expressed in those bills, applying models of
text reuse from bills introduced in the 110th and
111th Congresses.
Wang et al. (2014) implemented a query-
focused summarization system for FOMC and
FCIC meeting transcripts and Congressional hear-
ings, incorporating topic and expertise measures
into the score, and queried the corpus with candi-
date causes for the crisis, derived from Wikipedia
(e.g., ?subprime lending? and ?growth housing
bubble?).
Kleinnijenhuis et al. (2014) considered Con-
gressional hearings alongside news text from the
United States and the United Kingdom, carrying
out keyword analysis to compare and measure di-
rectional effects between the two, on different di-
mensions.
4 Conclusion
The unshared task was successful in attracting the
interest of forty participants working on ten teams.
A highly diverse range of activities ensued, each
of which is being reviewed at this writing by a
panel of judges. Reviews and final outcomes will
be posted at the https://sites.google.
com/site/unsharedtask2014 as soon as
they are available, and a presentation summariz-
ing the competition will be part of the ACL 2014
Workshop on Language Technologies and Com-
putational Social Science.
Acknowledgments
We thank the participants and judges for their time
and effort. This activity was supported in part by
NSF grants 1243917 and 1054319.
6
? Federal Open Market Committee (FOMC):
? Meeting transcripts are only made available five years after each meeting date. (The 2008 transcripts came available
around the time of the activity and were kindly made available by participant William Li.)
? Meeting minutes are available for all meetings to date.
? Federal Crisis Inquiry Commission (FCIC; an independent commission created by Congress to investigate the causes of
the crisis):
? Report
? Transcript of the first public hearing
? Congressional reports:
? Senate Committee on Homeland Security and Governmental Affairs: ?Wall Street and the financial crisis: anatomy
of a financial collapse?
? House Committee on Financial Services: ?The stock market plunge: what happened and what is next??
? Congressional bills:
? Troubled Assets Relief Program, 2008 (TARP)
? Dodd-Frank Wall Street Reform and Consumer Protection Act (2010)
? American Recovery and Reinvestment Act of 2009 (Stimulus)
? Housing and Economic Recovery Act of 2008
? Public Company Accounting Reform and Investor Protection Act of 2002 (Sarbanes-Oxley)
? Financial Services Modernization Act of 1999 (Gramm-Leach-Bliley)
? In addition to the above financial reform bills, the text of all versions of all Congressional bills introduced in the
110th and 111th Congresses
? Congressional hearings, segmented into turns:
? Monetary policy (26)
? TARP (12)
? Dodd-Frank (61)
? Other selected committee hearings relating to financial reform (15)
Table 1: Text datasets made available to unshared task participants. These can be downloaded at https://sites.google.
com/site/unsharedtask2014.
References
Nicole Rae Baerg, Will Lowe, Simone Ponzetto,
Heiner Stuckenschmidt, and C?acilia Zirn. 2014. Es-
timating central bank preferences.
Georgeta Bordea, Kartik Asooja, Paul Buitelaar, and
Leona O?Brien. 2014. Gaining insights into the
global financial crisis using Saffron.
Pierre Bourreau and Thierry Poibeau. 2014. Map-
ping the economic crisis: Some preliminary inves-
tigations.
Micah Clark, Adam Dalton, Tomas By, Yorick Wilks,
Samira Shaikh, Ching-Sheng Lin, and Tomek Strza-
lkowski. 2014. Influence and belief in Congres-
sional hearings.
Jan Kleinnijenhuis, Wouter van Atteveldt, and Antske
Fokkens. 2014. Chicken or egg? the reciprocal in-
fluence of press and politics.
William P. Li, David Larochelle, and Andrew W. Lo.
2014. Estimating policy trajectories during the fi-
nancial crisis.
John E. Miller and Kathleen F. McCoy. 2014. Chang-
ing focus of the FOMC through the financial crisis.
Michelle Morales, David Brizan, Hussein Ghaly,
Thomas Hauner, Min Ma, and Andrew Rosenberg.
2014. Application of social network analysis in the
estimation of bank financial strength during the fi-
nancial crisis.
Lu Wang, Parvaz Mahdabi, Joonsuk Park, Dinesh Pu-
ranam, Bishan Yang, and Claire Cardie. 2014.
Cornell expert aided query-focused summarization
(CEAQS): A summarization framework to PoliIn-
formatics.
C?acilia Zirn, Michael Sch?afer, Simone Paolo Ponzetto,
and Michael Strube. 2014. Exploring structural fea-
tures for position analysis in political discussions.
7
