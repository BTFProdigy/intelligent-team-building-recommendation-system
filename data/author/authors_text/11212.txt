Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89?92,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Novel Feature-based Approach to Chinese Entity Relation Extraction
 
Wenjie Li1, Peng Zhang1,2, Furu Wei1, Yuexian Hou2 and Qin Lu1
1Department of Computing 2School of Computer Science and Technology
The Hong Kong Polytechnic University, Hong Kong Tianjin University, China 
{cswjli,csfwei,csluqin}@comp.polyu.edu.hk {pzhang,yxhou}@tju.edu.cn 
 
 
Abstract 
Relation extraction is the task of finding 
semantic relations between two entities from 
text. In this paper, we propose a novel 
feature-based Chinese relation extraction 
approach that explicitly defines and explores 
nine positional structures between two entities. 
We also suggest some correction and inference 
mechanisms based on relation hierarchy and 
co-reference information etc. The approach is 
effective when evaluated on the ACE 2005 
Chinese data set. 
1 Introduction 
Relation extraction is promoted by the ACE program. 
It is the task of finding predefined semantic relations 
between two entities from text. For example, the 
sentence ?Bill Gates is the chairman and chief 
software architect of Microsoft Corporation? conveys 
the ACE-style relation ?ORG-AFFILIATION? 
between the two entities ?Bill Gates (PER)? and 
?Microsoft Corporation (ORG)?.  
The task of relation extraction has been extensively 
studied in English over the past years. It is typically 
cast as a classification problem. Existing approaches 
include feature-based and kernel-based classification. 
Feature-based approaches transform the context of 
two entities into a liner vector of carefully selected 
linguistic features, varying from entity semantic 
information to lexical and syntactic features of the 
context. Kernel-based approaches, on the other hand, 
explore structured representation such as parse tree 
and dependency tree and directly compute the 
similarity between trees. Comparably, feature-based 
approaches are easier to implement and achieve much 
success. 
In contrast to the significant achievements 
concerning English and other Western languages, 
research progress in Chinese relation extraction is 
quite limited. This may be attributed to the different 
characteristic of Chinese language, e.g. no word 
boundaries and lack of morphologic variations, etc. In 
this paper, we propose a character-based Chinese 
entity relation extraction approach that complements 
entity context (both internal and external) character 
N-grams with four word lists extracted from a 
published Chinese dictionary. In addition to entity 
semantic information, we define and examine nine 
positional structures between two entities. To cope 
with the data sparseness problem, we also suggest 
some correction and inference mechanisms according 
to the given ACE relation hierarchy and co-reference 
information. Experiments on the ACE 2005 data set 
show that the positional structure feature can provide 
stronger support for Chinese relation extraction. 
Meanwhile, it can be captured with less effort than 
applying deep natural language processing. But 
unfortunately, entity co-reference does not help as 
much as we have expected. The lack of necessary 
co-referenced mentions might be the main reason. 
2 Related Work 
Many approaches have been proposed in the literature 
of relation extraction. Among them, feature-based and 
kernel-based approaches are most popular. 
Kernel-based approaches exploit the structure of 
the tree that connects two entities. Zelenko et al(2003) 
proposed a kernel over two parse trees, which 
recursively matched nodes from roots to leaves in a 
top-down manner. Culotta and Sorensen (2004) 
extended this work to estimate similarity between 
augmented dependency trees. The above two work 
was further advanced by Bunescu and Mooney (2005) 
who argued that the information to extract a relation 
between two entities can be typically captured by the 
shortest path between them in the dependency graph. 
Later, Zhang et al(2006) developed a composite 
kernel that combined parse tree kernel with entity 
kernel and Zhou et al(2007) experimented with a 
context-sensitive kernel by automatically determining 
context-sensitive tree spans.  
In the feature-based framework, Kambhatla (2004) 
employed ME models to combine diverse lexical, 
syntactic and semantic features derived from word, 
entity type, mention level, overlap, dependency and 
parse tree. Based on his work, Zhou et al(2005) 
89
further incorporated the base phrase chunking 
information and semi-automatically collected country 
name list and personal relative trigger word list. Jiang 
and Zhai (2007) then systematically explored a large 
space of features and evaluated the effectiveness of 
different feature subspaces corresponding to sequence, 
syntactic parse tree and dependency parse tree. Their 
experiments showed that using only the basic unit 
features within each feature subspace can already 
achieve state-of-art performance, while over-inclusion 
of complex features might hurt the performance. 
Previous approaches mainly focused on English 
relations. Most of them were evaluated on the ACE 
2004 data set (or a sub set of it) which defined 7 
relation types and 23 subtypes. Although Chinese 
processing is of the same importance as English and 
other Western language processing, unfortunately few 
work has been published on Chinese relation 
extraction. Che et al(2005) defined an improved edit 
distance kernel over the original Chinese string 
representation around particular entities. The only 
relation they studied is PERSON-AFFLIATION. The 
insufficient study in Chinese relation extraction drives 
us to investigate how to find an approach that is 
particularly appropriate for Chinese. 
3 A Chinese Relation Extraction Model 
Due to the aforementioned reasons, entity relation 
extraction in Chinese is more challenging than in 
English. The system segmented words are already not 
error free, saying nothing of the quality of the 
generated parse trees. All these errors will 
undoubtedly propagate to the subsequent processing, 
such as relation extraction. It is therefore reasonable to 
conclude that kernel-based especially tree-kernel 
approaches are not suitable for Chinese, at least at 
current stage. In this paper, we study a feature-based 
approach that basically integrates entity related 
information with context information. 
3.1 Classification Features  
The classification is based on the following four types 
of features. 
z Entity Positional Structure Features  
We define and examine nine finer positional 
structures between two entities (see Appendix). They 
can be merged into three coarser structures. 
z Entity Features 
Entity types and subtypes are concerned.  
z Entity Context Features 
These are character-based features. We consider 
both internal and external context. Internal context 
includes the characters inside two entities and the 
characters inside the heads of two entities. External 
context involves the characters around two entities 
within a given window size (it is set to 4 in this study). 
All the internal and external context characters are 
transformed to Uni-grams and Bi-grams. 
z Word List Features 
Although Uni-grams and Bi-grams should be able 
to cover most of Chinese words given sufficient 
training data, many discriminative words might not be 
discovered by classifiers due to the severe sparseness 
problem of Bi-grams. We complement character- 
based context features with four word lists which are 
extracted from a published Chinese dictionary. The 
word lists include 165 prepositions, 105 orientations, 
20 auxiliaries and 25 conjunctions. 
3.2 Correction with Relation/Argument 
Constraints and Type/Subtype Consistency Check 
An identified relation is said to be correct only when 
its type/subtype (R) is correct and at the same time its 
two arguments (ARG-1 and ARG-2) must be of the 
correct entity types/subtypes and of the correct order. 
One way to improve the previous feature-based 
classification approach is to make use of the prior 
knowledge of the task to find and rectify the incorrect 
results. Table 1 illustrates the examples of possible 
relations between PER and ORG. We regard possible 
relations between two particular types of entity 
arguments as constraints. Some relations are 
symmetrical for two arguments, such as PER_ 
SOCIAL.FAMILY, but others not, such as ORG_AFF. 
EMPLOYMENT. Argument orders are important for 
asymmetrical relations.  
 PER ORG 
PER PER_SOCIAL.BUS, PER_SOCIAL.FAMILY, ? 
ORG_AFF.EMPLOYMENT, 
 ORG_AFF.OWNERSHIP, ? 
ORG  PART_WHOLE.SUBSIDIARY, ORG_AFF.INVESTOR/SHARE, ?
Table 1 Possible Relations between ARG-1 and ARG-2 
Since our classifiers are trained on relations instead 
of arguments, we simply select the first (as in adjacent 
and separate structures) and outer (as in nested 
structures) as the first argument. This setting works at 
most of cases, but still fails sometimes. The correction 
works in this way. Given two entities, if the identified 
type/subtype is an impossible one, it is revised to 
NONE (it means no relation at all). If the identified 
type/subtype is possible, but the order of arguments 
does not consist with the given relation definition, the 
order of arguments is adjusted.  
Another source of incorrect results is the 
inconsistency between the identified types and 
subtypes, since they are typically classified separately. 
90
This type of errors can be checked against the 
provided hierarchy of relations, such as the subtypes 
OWNERSHIP and EMPLOYMENT must belong to 
the ORG_AFF type. There are existing strategies to 
deal with this problem, such as strictly bottom-up (i.e. 
use the identified subtype to choose the type it belongs 
to), guiding top-down (i.e. to classify types first and 
then subtypes under a certain type). However, these 
two strategies lack of interaction between the two 
classification levels. To insure consistency in an 
interactive manner, we rank the first n numbers of the 
most likely classified types and then check them 
against the classified subtype one by one until the 
subtype conforms to a type. The matched type is 
selected as the result. If the last type still fails, both 
type and subtype are revised to NONE. We call this 
strategy type selection. Alternatively, we can choose 
the most likely classified subtypes, and check them 
with the classified type (i.e. subtype selection 
strategy). Currently, n is 2. 
3.2 Inference with Co-reference Information and 
Linguistic Patterns 
Each entity can be mentioned in different places in 
text. Two mentions are said to be co-referenced to one 
entity if they refers to the same entity in the world 
though they may have different surface expressions. 
For example, both ?he? and ?Gates? may refer to ?Bill 
Gates of Microsoft?. If a relation ?ORG- 
AFFILIATION? is held between ?Bill Gates? and 
?Microsoft?, it must be also held between ?he? and 
?Microsoft?. Formally, given two entities E1={EM11, 
EM12, ?, EM1n} and E2={EM21, EM22, ?, EM2m} (Ei 
is an entity, EMij is a mention of Ei), it is true that 
R(EM11, EM21)? R(EM1l, EM2k). This nature allows 
us to infer more relations which may not be identified 
by classifiers.  
Our previous experiments show that the 
performance of the nested and the adjacent relations is 
much better than the performance of other structured 
relations which suffer from unbearable low recall due 
to insufficient training data. Intuitively we can follow 
the path of ?Nested ? Adjacent ? Separated ? 
Others? (Nested, Adjacent and Separated structures 
are majority in the corpus) to perform the inference. 
But soon we have an interesting finding. If two related 
entities are nested, almost all the mentions of them are 
nested. So basically inference works on ?Adjacent ? 
Separated??. 
When considering the co-reference information, we 
may find another type of inconsistency, i.e. the one 
raised from co-referenced entity mentions. It is 
possible that R(EM11, EM21) ? R(EM12, EM22) when R 
is identified based on the context of EM. Co-reference 
not only helps for inference but also provides the 
second chance to check the consistency among entity 
mention pairs so that we can revise accordingly. As the 
classification results of SVM can be transformed to 
probabilities with a sigmoid function, the relations of 
lower probability mention pairs are revised according 
to the relation of highest probability mention pairs. 
The above inference strategy is called coreference- 
based inference. Besides, we find that pattern-based 
inference is also necessary. The relations of adjacent 
structure can infer the relations of separated structure 
if there are certain linguistic indicators in the local 
context. For example, given a local context ?EM1 and 
EM2 located EM3?, if the relation of EM2 and EM3 has 
been identified, EM1 and EM3 will take the relation 
type/subtype that EM2 and EM3 holds. Currently, the 
only indicators under consideration are ?and? and ?or?. 
However, more patterns can be included in the future. 
4 Experimental Results 
The experiments are conducted on the ACE 2005 
Chinese RDC training data (with true entities) where 6 
types and 18 subtypes of relations are annotated. We 
use 75% of it to train SVM classifiers and the 
remaining to evaluate results.  
The aim of the first set of experiments is to examine 
the role of structure features. In these experiments, a 
?NONE? class is added to indicate a null type/subtype. 
With entity features and entity context features and 
word list features, we consider three different 
classification contexts: (1), only three coarser 
structures 1 , i.e. nested, adjacent and separated, are 
used as feature, and a classifier is trained for each 
relation type and subtype; (2) similar to (1) but all nine 
structures are concerned; and (3) similar to (2) but the 
training data is divided into 9 parts according to 
structure, i.e. type and subtype classifiers are trained 
on the data with the same structures. The results 
presented in Table 2 show that 9-structure is much 
more discriminative than 3-structure. Also, the 
performance can be improved significantly by 
dividing training data based on nine structures. 
Type / Subtype Precision Recall F-measure 
3-Structure 0.7918/0.7356 0.3123/0.2923 0.4479/0.4183
9-Structure 0.7533/0.7502 0.4389/0.3773 0.5546/0.5021
9-Structure_Divide 0.7733/0.7485 0.5506/0.5301 0.6432/0.6209
Table 2 Evaluation on Structure Features 
Structure Positive Class Negative Class Ratio 
Nested 6332 4612 1 : 0.7283
Adjacent 2028 27100 1 : 13.3629
                                                     
1 Nine structures are combined to three by merging (b) and (c) to (a), (e) 
and (f) to (d), (h) and (i) to (g). 
91
Separated 939 79989 1 : 85.1853
Total 9299 111701 1 : 12.01 
Table 3 Imbalance Training Class Problem 
In the experiments, we find that the training class 
imbalance problem is quite serious, especially for the 
separated structure (see Table 3 above where 
?Positive? and ?Negative? mean there exists a relation 
between two entities and otherwise). A possible 
solution to alleviate this problem is to detect whether 
the given two entities have some relation first and if 
they do then to classify the relation types and subtypes 
instead of combining detection and classification in 
one process. The second set of experiment is to 
examine the difference between these two 
implementations. Against our expectation, the 
sequence implementation does better than the 
combination implementation, but not significantly, as 
shown in Table 4 below.  
Type / Subtype Precision Recall F-measure 
Combination 0.7733/0.7485 0.5506/0.5301 0.6432/0.6206
Sequence 0.7374/0.7151 0.5860/0.5683 0.6530/0.6333
Table 4 Evaluation of Two Detection and Classification Modes 
Based on the sequence implementation, we set up 
the third set of experiments to examine the correction 
and inference mechanisms. The results are illustrated 
in Table 5. The correction with constraints and 
consistency check is clearly contributing. It improves 
F-measure 7.40% and 6.47% in type and subtype 
classification respectively. We further compare four 
possible consistency check strategies in Table 6 and 
find that the strategies using subtypes to determine or 
select types perform better than top down strategies. 
This can be attributed to the fact that correction with 
relation/argument constraints in subtype is tighter than 
the ones in type.  
Type / Subtype Precision Recall F-measure 
Seq. + Cor. 0.8198/0.7872 0.6127/0.5883 0.7013/0.6734
Seq. + Cor. + Inf. 0.8167/0.7832 0.6170/0.5917 0.7029/0.6741
Table 5 Evaluation of Correction and Inference Mechanisms 
Type / Subtype Precision Recall F-measure 
Guiding Top-Down 0.7644/0.7853 0.6074/0.5783 0.6770/0.6661
Subtype Selection 0.8069/0.7738 0.6065/0.5817 0.6925/0.6641
Strictly Bottom-Up 0.8120/0.7798 0.6146/0.5903 0.6996/0.6719
Type Selection 0.8198/0.7872 0.6127/0.5883 0.7013/0.6734
Table 6 Comparison of Different Consistency Check Strategies 
Finally, we provide our findings from the fourth set 
of experiments which looks at the detailed 
contributions from four feature types. Entity type 
features themselves do not work. We incrementally 
add the structures, the external contexts and internal 
contexts, Uni-grams and Bi-grams, and at last the 
word lists on them. The observations are: Uni-grams 
provide more discriminative information than 
Bi-grams; external context seems more useful than 
internal context; positional structure provides stronger 
support than other individual recognized features such 
as entity type and context; but word list feature can not 
further boost the performance.  
Type / Subtype Precision Recall F-measure 
Entity Type + Structure 0.7288/0.6902 0.4876/0.4618 0.5843/0.5534
+ External (Uni-) 0.7935/0.7492 0.5817/0.5478 0.6713/0.6321
+ Internal (Uni-) 0.8137/0.7769 0.6113/0.5836 0.6981/0.6665
+ Bi- (Internal & External) 0.8144/0.7828 0.6141/0.5902 0.7002/0.6730
+ Wordlist 0.8167/0.7832 0.6170/0.5917 0.7029/0.6741
Table 6 Evaluation of Feature and Their Combinations 
5 Conclusion 
In this paper, we study feature-based Chinese relation 
extraction. The proposed approach is effective on the 
ACE 2005 data set. Unfortunately, there is no result 
reported on the same data so that we can compare. 
6 Appendix: Nine Positional Structures  
 
Acknowledgments 
This work was supported by HK RGC (CERG PolyU5211/05E) 
and China NSF (60603027). 
References 
Razvan Bunescu and Raymond Mooney. 2005. A Shortest Path 
Dependency Tree Kernel for Relation Extraction, In Proceedings of 
HLT/EMNLP, pages 724-731.  
Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for 
Relation Extraction, in Proceedings of ACL, pages 423-429. 
Jing Jiang, Chengxiang Zhai. 2007. A Systematic Exploration of the 
Feature Space for Relation Extraction. In proceedings of 
NAACL/HLT, pages 113-120. 
Nanda Kambhatla. 2004. Combining Lexical, Syntactic, and Semantic 
Features with Maximum Entropy Models for Extracting Relations. 
In Proceedings of ACL, pages 178-181.  
Dmitry Zelenko, Chinatsu Aone and Anthony Richardella. 2003. 
Kernel Methods for Relation Extraction. Journal of Machine 
Learning Research 3:1083-1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite 
Kernel to Extract Relations between Entities with both Flat and 
Structured Features, in Proceedings of COLING/ACL, pages 
825-832. 
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring 
Various Knowledge in Relation Extraction. In Proceedings of ACL, 
pages 427-434. 
GuoDong Zhou, Min Zhang, Donghong Ji and Qiaoming Zhu. 2007. 
Tree Kernel-based Relation Extraction with Context-Sensitive 
Structured Parse Tree Information. In Proceedings of EMNLP, 
pages 728-736. 
Wanxiang Che et al 2005. Improved-Edit-Distance Kernel for Chinese 
Relation Extraction. In Proceedings of IJCNLP, pages 132-137. 
92
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 489?496
Manchester, August 2008
PNR2: Ranking Sentences with Positive and Negative Reinforcement 
for Query-Oriented Update Summarization 
 
 Abstract 
Query-oriented update summarization is 
an emerging summarization task very 
recently. It brings new challenges to the 
sentence ranking algorithms that require 
not only to locate the important and 
query-relevant information, but also to 
capture the new information when 
document collections evolve. In this 
paper, we propose a novel graph based 
sentence ranking algorithm, namely PNR2, 
for update summarization. Inspired by the 
intuition that ?a sentence receives a 
positive influence from the sentences that 
correlate to it in the same collection, 
whereas a sentence receives a negative 
influence from the sentences that 
correlates to it in the different (perhaps 
previously read) collection?, PNR2 
models both the positive and the negative 
mutual reinforcement in the ranking 
process. Automatic evaluation on the 
DUC 2007 data set pilot task 
demonstrates the effectiveness of the 
algorithm.  
 
1 Introduction 
The explosion of the WWW has brought with it a 
vast board of information. It has become virtually 
impossible for anyone to read and understand 
large numbers of individual documents that are 
abundantly available. Automatic document 
summarization provides an effective means to 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
manage such an exponentially increased 
collection of information and to support 
information seeking and condensing goals.  
The main evaluation forum that provides 
benchmarks for researchers working on 
document summarization to exchange their ideas 
and experiences is the Document Understanding 
Conferences (DUC). The goals of the DUC 
evaluations are to enable researchers to 
participate in large-scale experiments upon the 
standard benchmark and to increase the 
availability of appropriate evaluation techniques. 
Over the past years, the DUC evaluations have 
evolved gradually from single-document 
summarization to multi-document summarization 
and from generic summarization to query-
oriented summarization. Query-oriented multi-
document summarization initiated in 2005 aims 
to produce a short and concise summary for a 
collection of topic relevant documents according 
to a given query that describes a user?s particular 
interests. 
Previous summarization tasks are all targeted 
on a single document or a static collection of 
documents on a given topic. However, the 
document collections can change (actually grow) 
dynamically when the topic evolves over time. 
New documents are continuously added into the 
topic during the whole lifecycle of the topic and 
normally they bring the new information into the 
topic. To cater for the need of summarizing a 
dynamic collection of documents, the DUC 
evaluations piloted update summarization in 2007. 
The task of update summarization differs from 
previous summarization tasks in that the latter 
aims to dig out the salient information in a topic 
while the former cares the information not only 
salient but also novel.  
Up to the present, the predominant approaches 
in document summarization regardless of the 
nature and the goals of the tasks have still been 
built upon the sentence extraction framework. 
Li Wenjie1, Wei Furu1,2, Lu Qin1, He Yanxiang2 
1Department of Computing 
The Hong Kong Polytechnic University, HK 
{csfwei, cswjli, csluqin} 
@comp.polyu.edu.hk 
2Department of Computer Science 
and Technology, Wuhan University, China 
{frwei, yxhe@whu.edu.cn} 
489
Under this framework, sentence ranking is the 
issue of most concern. In general, two kinds of 
sentences need to be evaluated in update 
summarization, i.e. the sentences in an early (old) 
document collection A (denoted by SA) and the 
sentences in a late (new) document collection B 
(denoted by SB). Given the changes from SA to SB, 
an update summarization approach may be 
concerned about four ranking issues: (1) rank SA 
independently; (2) re-rank SA after SB comes; (3) 
rank SB independently; and (4) rank SB given that 
SA is provided. Among them, (4) is of most 
concern. It should be noting that both (2) and (4) 
need to consider the influence from the sentences 
in the same and different collections.  
In this study, we made an attempt to capture 
the intuition that  
?A sentence receives a positive influence from 
the sentences that correlate to it in the same 
collection, whereas a sentence receives a 
negative influence from the sentences that 
correlates to it in the different collection.? 
We represent the sentences in A or B as a text 
graph constructed using the same approach as 
was used in Erkan and Radev (2004a, 2004b). 
Different from the existing PageRank-like 
algorithms adopted in document summarization, 
we propose a novel sentence ranking algorithm, 
called PNR2 (Ranking with Positive and Negative 
Reinforcement). While PageRank models the 
positive mutual reinforcement among the 
sentences in the graph, PNR2 is capable of 
modeling both positive and negative 
reinforcement in the ranking process.  
The remainder of this paper is organized as 
follows. Section 2 introduces the background of 
the work presented in this paper, including 
existing graph-based summarization models, 
descriptions of update summarization and time-
based ranking solutions with web graph and text 
graph. Section 3 then proposes PNR2, a sentence 
ranking algorithm based on positive and negative 
reinforcement and presents a query-oriented 
update summarization model. Next, Section 4 
reports experiments and evaluation results. 
Finally, Section 5 concludes the paper. 
2 Background and Related Work 
2.1 Previous Work in Graph-based 
Document Summarization 
Graph-based ranking algorithms such as 
Google?s PageRank (Brin and Page, 1998) and 
Kleinberg?s HITS (Kleinberg, 1999) have been 
successfully used in the analysis of the link 
structure of the WWW. Now they are springing 
up in the community of document summarization. 
The major concerns in graph-based 
summarization researches include how to model 
the documents using text graph and how to 
transform existing web page ranking algorithms 
to their variations that could accommodate 
various summarization requirements. 
Erkan and Radev (2004a and 2004b) 
represented the documents as a weighted 
undirected graph by taking sentences as vertices 
and cosine similarity between sentences as the 
edge weight function. An algorithm called 
LexRank, adapted from PageRank, was applied 
to calculate sentence significance, which was 
then used as the criterion to rank and select 
summary sentences. Meanwhile, Mihalcea and 
Tarau (2004) presented their PageRank variation, 
called TextRank, in the same year. Besides, they 
reported experimental comparison of three 
different graph-based sentence ranking 
algorithms obtained from Positional Power 
Function, HITS and PageRank (Mihalcea and 
Tarau, 2005). Both HITS and PageRank 
performed excellently. 
Likewise, the use of PageRank family was also 
very popular in event-based summarization 
approaches (Leskovec et al, 2004; Vanderwende 
et al, 2004; Yoshioka and Haraguchi, 2004; Li et 
al., 2006). In contrast to conventional sentence-
based approaches, newly emerged event-based 
approaches took event terms, such as verbs and 
action nouns and their associated named entities 
as graph nodes, and connected nodes according 
to their co-occurrence information or semantic 
dependency relations. They were able to provide 
finer text representation and thus could be in 
favor of sentence compression which was 
targeted to include more informative contents in a 
fixed-length summary. Nevertheless, these 
advantages lied on appropriately defining and 
selecting event terms.  
All above-mentioned representative work was 
concerned with generic summarization. Later on, 
graph-based ranking algorithms were introduced 
in query-oriented summarization too when this 
new challenge became a hot research topic 
recently. For example, a topic-sensitive version 
of PageRank was proposed in (OtterBacher et al, 
2005). The same algorithm was followed by Wan 
et al (2006) and Lin et al (2007) who further 
investigated on its application in query-oriented 
update summarization.  
490
2.2 The DUC 2007 Update Summarization 
Task Description 
The DUC 2007 update summarization pilot task 
is to create short (100 words) multi-document 
summaries under the assumption that the reader 
has already read some number of previous 
documents. Each of 10 topics contains 25 
documents. For each topic, the documents are 
sorted in chronological order and then partitioned 
into three collections, ?A?, ?B? and ?C?. The 
participants are then required to generate (1) a 
summary for ?A?; (2) an update summary for 
?B? assuming documents in ?A? have already 
been read; and (3) an update summary for ?C? 
assuming documents in ?A? and ?B? have 
already been read. Growing out of the DUC 2007, 
the Text Analysis Conference (TAC) 2008 
planed to keep only the DUC 2007 task (1) and 
(2). 
Each topic collection in the DUC 2007 (will 
also in the TAC 2008) is accompanied with a 
query that describes a user?s interests and focuses. 
System-generated summaries should include as 
many responses relevant to the given query as 
possible. Here is a query example from the DUC 
2007 document collection ?D0703A?.  
<topic> 
<num> D0703A </num> 
<title> Steps toward introduction of the 
Euro. </title> 
<narr> Describe steps taken and worldwide 
reaction prior to introduction of the Euro on 
January 1, 1999. Include predictions and 
expectations reported in the press. </narr> 
</topic>                                          [D0703A] 
Update summarization is definitely a time-
related task. An appropriate ranking algorithm 
must be the one capable of coping with the 
change or the time issues.  
2.3 Time-based Ranking Solutions with 
Web Graph and Text Graph 
Graph based models in document summarization 
are inspired by the idea behind web graph models 
which have been successfully used by current 
search engines. As a matter of fact, adding time 
dimension into the web graph has been 
extensively studied in recent literature. 
Basically, the evolution in the web graph stems 
from (1) adding new edges between two existing 
nodes; (2) adding new nodes in the existing graph 
(consequently adding new edges between the 
existing nodes and the new nodes or among the 
new nodes); and (3) deleting existing edges or 
nodes. Berberich et al (2004 and 2005) 
developed two link analysis methods, i.e. T-Rank 
Light and T-Rank, by taking into account two 
temporal aspects, i.e. freshness (i.e. timestamp of 
most recent update) and activity (i.e. update rates) 
of the pages and the links. They modeled the web 
as an evolving graph in which each nodes and 
edges (i.e. web pages and hyperlinks) were 
annotated with time information. The time 
information in the graph indicated different kinds 
of events in the lifespan of the nodes and edges, 
such as creation, deletion and modifications. 
Then they derived a subgraph of the evolving 
graph with respect to the user?s temporal interest. 
Finally, the time information of the nodes and the 
edges were used to modify the random walk 
model as was used in PageRank. Specifically, 
they used it to modify the random jump 
probabilities (in both T-Rank Light and T-Rank) 
and the transition probabilities (in T-Rank only).  
Meanwhile, Yu et al (2004 and 2005) 
introduced a time-weighted PageRank, called 
TimedPageRank, for ranking in a network of 
scientific publications. In their approach, 
citations were weighted based on their ages. Then 
a post-processing step decayed the authority of a 
publication based on the publication?s age. Later, 
Yang et al (2007) proposed TemporalRank, 
based on which they computed the page 
importance from two perspectives: the 
importance from the current web graph snapshot 
and the accumulated historical importance from 
previous web graph snapshot. They used a kinetic 
model to interpret TemporalRank and showed it 
could be regarded as a solution to an ordinary 
differential equation.  
In conclusion, Yu et al tried to cope with the 
problem that PageRank favors over old pages 
whose in-degrees are greater than those of new 
pages. They worked on a static single snapshot of 
the web graph, and their algorithm could work 
well on all pages in the web graph. Yang et al, 
on the other hand, worked on a series of web 
graphs at different snapshots. Their algorithm 
was able to provide more robust ranking of the 
web pages, but could not alleviate the problem 
carried by time dimension at each web graph 
snapshot. This is because they directly applied 
the original PageRank to rank the pages. In other 
words, the old pages still obtained higher scores 
while the newly coming pages still got lower 
scores. Berberich et al focused their efforts on 
the evolution of nodes and edges in the web 
graph. However, their algorithms did not work 
491
when the temporal interest of the user (or query) 
was not available.  
As for graph based update summarization, 
Wan (2007) presented the TimedTextRank 
algorithm by following the same idea presented 
in the work of Yu et al Given three collections of 
chronologically ordered documents, Lin et al 
(2007) proposed to construct the TimeStamped 
graph (TSG) graph by incrementally adding the 
sentences to the graph. They modified the 
construction of the text graph, but the ranking 
algorithm was the same as the one proposed by 
OtterBacher et al  
Nevertheless, the text graph is different from 
the web graph. The evolution in the text graph is 
limited to the type (2) in the web graph. The 
nodes and edges can not be deleted or modified 
once they are inserted. In other words, we are 
only interested in the changes caused when new 
sentences are introduced into the existing text 
graph. As a result, the ideas from Berberich et al 
cannot be adopted directly in the text graph. 
Similarly, the problem in web graph as stated in 
the work of Yu et al (i.e. ?new pages, which may 
be of high quality, have few or no in-links and 
are left behind.?) does not exist in the text graph 
at all. More precisely, the new coming sentences 
are equally treated as the existing sentences, and 
the degree (in or out) of the new sentences are 
also equally accumulated as the old sentences. 
Directly applying the ideas from the work of Yu 
et al does not always make sense in the text 
graph. Recall that the main task for sentence 
ranking in update summarization is to rank SB 
given SA. So the idea from Yang et al is also not 
applicable.  
In fact, the key points include not only 
maximizing the importance in the current new 
document collection but also minimizing the 
redundancy to the old document collection when 
ranking the sentences for update summarization. 
Time dimension does contribute here, but it is not 
the only way to consider the changes. Unlike the 
web graph, the easily-captured content 
information in a text graph can provide additional 
means to analyze the influence of the changes.  
To conclude the previous discussions, adding 
temporal information to the text graph is different 
from it in the web graph. Capturing operations 
(such as addition, deletion, modification of web 
pages and hyperlinks) is most concerned in the 
web graph; however, prohibiting redundant 
information from the old documents is the most 
critical issue in the text graph. 
3 Positive and Negative Reinforcement 
Ranking for Update Summarization 
Existing document summarization approaches 
basically follow the same processes: (1) first 
calculate the significance of the sentences with 
reference to the given query with/without using 
some sorts of sentence relations; (2) then rank the 
sentences according to certain criteria and 
measures; (3) finally extract the top-ranked but 
non-redundant sentences from the original 
documents to create a summary. Under this 
extractive framework, undoubtedly the two 
critical processes involved are sentence ranking 
and sentence selection. In the following sections, 
we will first introduce the sentence ranking 
algorithm based on ranking with positive and 
negative reinforcement, and then we present the 
sentence selection strategy. 
3.1 Ranking with Positive and Negative 
Reinforcement (PNR2) 
Previous graph-based sentence ranking 
algorithms is capable to model the fact that a 
sentence is important if it correlates to (many) 
other important sentences. We call this positive 
mutual reinforcement. In this paper, we study two 
kinds of reinforcement, namely positive and 
negative reinforcement, among two document 
collections, as illustrated in Figure 1.  
 
Figure 1 Positive and Negative Reinforcement 
In Figure 1, ?A? and ?B? denote two document 
collections about the same topics (?A? is the old 
document collection, ?B? is the new document 
collection), SA and SB denote the sentences in 
?A? and ?B?. We assume: 
1. SA performs positive reinforcement on its 
own internally; 
2. SA performs negative reinforcement on SB 
externally; 
3. SB performs negative reinforcement on SA 
externally;  
4. SB performs positive reinforcement on its 
own internally. 
Positive reinforcement captures the intuition 
that a sentence is more important if it associates 
to the other important sentences in the same 
collection. Negative reinforcement, on the other 
hand, reflects the fact that a sentence is less 
A B + + 
- 
- 
492
important if it associates to the important 
sentences in the other collection, since such a 
sentence might repeat the same or very similar 
information which is supposed to be included in 
the summary generated for the other collection.  
Let RA and RB denote the ranking of the 
sentences in A and B, the reinforcement can be 
formally described as 
??
??
?
?+??+??=
?+??+??=
+
+
B
k
BBB
k
ABA
k
B
A
k
BAB
k
AAA
k
A
pRMRMR
pRMRMR
r
r
2
)(
2
)(
2
)1(
1
)(
1
)(
1
)1(
???
???
 (1) 
where the four matrices MAA, MBB, MAB and MBA 
are the affinity matrices of the sentences in SA, in 
SB, from SA to SB and from SB to SA. 
??
?
??
?
=
22
11
??
??
W  is a weight matrix to balance the 
reinforcement among different sentences. Notice 
that 0, 21 <??  such that they perform negative 
reinforcement. Ap
r
 and Bp
r
 are two bias vectors, 
with 1,0 21 << ??  as the damping factors. [ ]
1
1
?
=
n
A n
pr , where n is the order of MAA. Bp
r
 is 
defined in the same way. We will further define 
the affinity matrices in section 3.2 later. With the 
above reinforcement ranking equation, it is also 
true that 
1. A sentence in SB correlates to many new 
sentences in SB is supposed to receive a high 
ranking from RB, and 
2. A sentence in SB correlates to many old 
sentences in SA is supposed to receive a low 
ranking from RB. 
Let [ ]TBA RRR =  and [ ]TBA ppp rrr ??= 21 ?? , then 
the above iterative equation (1) corresponds to 
the linear system, 
( ) pRMI r=??                            (2) 
where, ??
?
??
?
=
BBBA
ABAA
MM
MM
M
22
11
??
??
. 
Up to now, the PNR2 is still query-independent. 
That means only the content of the sentences is 
considered. However, for the tasks of query-
oriented summarization, the reinforcement should 
obviously bias to the user?s query. In this work, 
we integrate query information into PNR2 by 
defining the vector pr  as ( )qsrelp ii |=r , where 
( )qsrel i |  denotes the relevance of the sentence si 
to the query q. 
To guarantee the solution of the linear system 
Equation (2), we make the following two 
transformations on M. First M is normalized by 
columns. If all the elements in a column are zero, 
we replace zero elements with n1  (n is the total 
number of the elements in that column). Second, 
M is multiplied by a decay factor ? ( 10 <<? ), 
such that each element in M is scaled down but 
the meaning of M will not be changed.  
Finally, Equation (2) is rewritten as, 
( ) pRMI r=??? ?                        (3) 
The matrix ( )MI ???  is a strictly diagonally 
dominant matrix now, and the solution of the 
linear system Equation (3) exists.  
3.2 Sentence Ranking based on PNR2 
We use the above mentioned PNR2 framework to 
rank the sentences in both SA and SB 
simultaneously. Section 3.2 defines the affinity 
matrices and presents the ranking algorithm. 
The affinity (i.e. similarity) between two 
sentences is measured by the cosine similarity of 
the corresponding two word vectors, i.e.  
[ ] ( )ji sssimjiM ,, =                     (4) 
where ( )
ji
ji
ji
ss
ss
sssim rr
rr
?
?
=,
. However, when 
calculating the affinity matrices MAA and MBB, the 
similarity of a sentence to itself is defined as 0, 
i.e. 
[ ] ( )
??
?
=
?
= ji
jisssimjiM ji
             0
,
,              (5) 
Furthermore, the relevance of a sentence to the 
query q is defined as 
( )
qs
qs
qsrel
i
i
i rr
rr
?
?
=,                     (6) 
Algorithm 1. RankSentence(SA, SB, q) 
Input: The old sentence set SA, the new 
sentence set SB, and the query q. 
Output: The ranking vectors R of SA and SB. 
1: Construct the affinity matrices, and set the 
weight matrix W; 
2: Construct the matrix ( )MIA ??= ? .  
3: Choose (randomly) the initial non-negative 
vectors TR ]11[)0( L= ; 
4: 0?k , 0?? ; 
5: Repeat 
6:     ( )? ?< >++ ??= ij ij kjijkjiji
ij
k
i RaRap
a
R )()1()1( 1 r ; 
7:     ( ))()1(max kk RR ??? + ; 
8:  )1( +kR is normalized such that the maximal 
element in )1( +kR is 1. 
493
9:     1+? kk ; 
10: Until ?<? 1; 
11: )(kRR ? ; 
12: Return. 
Now, we are ready to adopt the Gauss-Seidel 
method to solve the linear system Equation (3), 
and an iterative algorithm is developed to rank 
the sentences in SA and SB. 
After sentence ranking, the sentences in SB 
with higher ranking will be considered to be 
included in the final summary.  
3.3 Sentence Selection by Removing 
Redundancy 
When multiple documents are summarized, the 
problem of information redundancy is more 
severe than it is in single document 
summarization. Redundancy removal is a must. 
Since our focus is designing effective sentence 
ranking approach, we apply the following simple 
sentence selection algorithm. 
Algorithm 2. GenerateSummary(S, length) 
Input: sentence collection S (ranked in 
descending order of significance) and length 
(the given summary length limitation) 
Output: The generated summary ?  
{}?? ; 
?l length; 
For i ?  0 to |S| do 
     threshold ? ( )( )??ssssim i   ,max ; 
     If threshold <= 0.92 do 
          isU??? ; 
          ll ? - ( )islen ;  
          If ( l <= 0) break; 
     End 
End 
Return ? . 
 
4 Experimental Studies 
4.1 Data Set and Evaluation Metrics 
The experiments are set up on the DUC 2007 
update pilot task data set. Each collection of 
documents is accompanied with a query 
description representing a user?s information 
need. We simply focus on generating a summary 
for the document collection ?B? given that the 
                                                 
1
 ?  is a pre-defined small real number as the 
convergence threshold. 
2
 In fact, this is a tunable parameter in the algorithm. 
We use the value of 0.9 by our intuition. 
user has read the document collection ?A?, which 
is a typical update summarization task.  
Table 1 below shows the basic statistics of the 
DUC 2007 update data set. Stop-words in both 
documents and queries are removed 3  and the 
remaining words are stemmed by Porter 
Stemmer 4 . According to the task definition, 
system-generated summaries are strictly limited 
to 100 English words in length. We incrementally 
add into a summary the highest ranked sentence 
of concern if it doesn?t significantly repeat the 
information already included in the summary 
until the word limitation is reached. 
 A B 
Average number of documents 10 10 
Average number of sentences 237.6 177.3 
Table 1. Basic Statistics of DUC2007 Update Data Set 
As for the evaluation metric, it is difficult to 
come up with a universally accepted method that 
can measure the quality of machine-generated 
summaries accurately and effectively. Many 
literatures have addressed different methods for 
automatic evaluations other than human judges. 
Among them, ROUGE5 (Lin and Hovy, 2003) is 
supposed to produce the most reliable scores in 
correspondence with human evaluations. Given 
the fact that judgments by humans are time-
consuming and labor-intensive, and more 
important, ROUGE has been officially adopted 
for the DUC evaluations since 2005, like the 
other researchers, we also choose it as the 
evaluation criteria. 
In the following experiments, the sentences 
and the queries are all represented as the vectors 
of words. The relevance of a sentence to the 
query is calculated by cosine similarity. Notice 
that the word weights are normally measured by 
the document-level TF*IDF scheme in 
conventional vector space models. However, we 
believe that it is more reasonable to use the 
sentence-level inverse sentence frequency (ISF) 
rather than document-level IDF when dealing 
with sentence-level text processing. This has 
been verified in our early study. 
4.2 Comparison of Positive and Negative 
Reinforcement Ranking Strategy 
The aim of the following experiments is to 
investigate the different reinforcement ranking 
strategies. Three algorithms (i.e. PR(B), 
                                                 
3
 A list of 199 words is used to filter stop-words. 
4
 http://www.tartarus.org/~martin/PorterStemmer. 
5
 ROUGE version 1.5.5 is used. 
494
PR(A+B), PR(A+B/A)) are implemented as 
reference. These algorithms are all based on the 
query-sensitive LexRank (OtterBacher et al, 
2005). The differences are two-fold: (1) the 
document collection(s) used to build the text 
graph are different; and (2) after ranking, the 
sentence selection strategies are different. In 
particular, PR(B) only uses the sentences in ?B? 
to build the graph, and the other two consider the 
sentences in both ?A? and in ?B?. Only the 
sentences in ?B? are considered to be selected in 
PR(B) and PR(A+B/A), but all the sentences in 
?A? and ?B? have the same chance to be selected 
in PR(A+B). Only the sentences from B are 
considered to be selected in the final summaries 
in PNR2 as well. In the following experiments, 
the damping factor is set to 0.85 in the first three 
algorithms as the same in PageRank. The weight 
matrix W is set to ??
?
??
?
?
?
15.0
5.01
 in the proposed 
algorithm (i.e. PNR2) and 5.021 == ?? . We have 
obtained reasonable good results with the decay 
factor ?  between 0.3 and 0.8. So we set it to 0.5 
in this paper.  
Notice that the three PageRank-like graph-
based ranking algorithms can be viewed as only 
the positive reinforcement among the sentences is 
considered, while both positive and negative 
reinforcement are considered in PNR2 as 
mentioned before. Table 2 below shows the 
results of recall scores of ROUGE-1, ROUGE-2 
and ROUGE-SU4 along with their 95% 
confidential internals within square brackets.  
 ROUGE 
-1 
ROUGE 
-2 
ROUGE-
SU4 
PR(B) 0.3323 [0.3164,0.3501] 
0.0814 
[0.0670,0.0959] 
0.1165 
0.1053,0.1286] 
PR(A+B) 0.3059 [0.2841,0.3256] 
0.0746 
[0.0613,0.0893] 
0.1064 
[0.0938,0.1186] 
PR(A+B/A) 0.3376 [0.3186,0.3572] 
0.0865 
[0.0724,0.1007] 
0.1222 
[0.1104,0.1304] 
PNR2 0.3616 [0.3464,0.3756] 
0.0895 
[0.0810,0.0987] 
0.1291 
[0.1208,0.1384] 
Table 2. Experiment Results 
We come to the following three conclusions. 
First, it is not surprising that PR(B) and 
PR(A+B/A) outperform PR(A+B), because the 
update task obviously prefers the sentences from 
the new documents (i.e. ?B?). Second, 
PR(A+B/A) outperforms PR(B) because the 
sentences in ?A? can provide useful information 
in ranking the sentences in ?B?, although we do 
not select the sentences ranked high in ?A?. Third, 
PNR2 achieves the best performance. PNR2 is 
above PR(A+B/A) by 7.11% of ROUGE-1, 
3.47% of ROUGE-2, and 5.65% of ROUGE-SU4. 
This result confirms the idea and algorithm 
proposed in this work. 
4.3 Comparison with DUC 2007 Systems 
Twenty-four systems have been submitted to the 
DUC for evaluation in the 2007 update task. 
Table 3 compares our PNR2 with them. For 
reference, we present the following representative 
ROUGE results of (1) the best and worst 
participating system performance, and (2) the 
average ROUGE scores (i.e. AVG). We can then 
easily locate the positions of the proposed models 
among them. 
 PNR2 Mean Best / Worst 
ROUGE-1 0.3616 0.3262 0.3768/0.2621 
ROUGE2 0.0895 0.0745 0.1117/0.0365 
ROUGE-SU4 0.1291 0.1128 0.1430/0.0745 
Table 3. System Comparison 
4.4 Discussion 
In this work, we use the sentences in the same 
sentence set for positive reinforcement and 
sentences in the different set for negative 
reinforcement. Precisely, the old sentences 
perform negative reinforcement over the new 
sentences while the new sentences perform 
positive reinforcement over each other. This is 
reasonable although we may have a more 
comprehensive alternation. Old sentences may 
express old topics, but they may also express 
emerging new topics. Similarly, new sentences 
are supposed to express new topics, but they may 
also express the continuation of old topics. As a 
result, it will be more comprehensive to classify 
the whole sentences (both new sentences and old 
sentences together) into two categories, i.e. old 
topics oriented sentences and new topic oriented 
sentences, and then to apply these two sentence 
sets in the PNR2 framework. This will be further 
studied in our future work. 
Moreover, in the update summarization task, 
the summary length is restricted to about 100 
words. In this situation, we find that sentence 
simplification is even more important in our 
investigations. We will also work on this issue in 
our forthcoming studies. 
5 Conclusion 
In this paper, we propose a novel sentence 
ranking algorithm, namely PNR2, for update 
summarization. As our pilot study, we simply 
assume to receive two chronologically ordered 
document collections and evaluate the summaries 
495
generated for the collection given later. With 
PNR2, sentences from the new (i.e. late) 
document collection perform positive 
reinforcement among each other but they receive 
negative reinforcement from the sentences in the 
old (i.e. early) document collection. Positive and 
negative reinforcement are concerned 
simultaneously in the ranking process. As a result, 
PNR2 favors the sentences biased to the sentences 
that are important in the new collection and 
meanwhile novel to the sentences in the old 
collection. As a matter of fact, this positive and 
negative ranking scheme is general enough and 
can be used in many other situations, such as 
social network analysis etc. 
 
Acknowledgements 
The research work presented in this paper was 
partially supported by the grants from RGC of 
HKSAR (Project No: PolyU5217/07E), NSF of 
China (Project No: 60703008) and the Hong 
Kong Polytechnic University (Project No: A-
PA6L). 
 
References 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2004. G.T-Rank: Time-Aware Authority 
Ranking. In Algorithms and Models for the Web-
Graph: Third International Workshop, WAW, pp 
131-141. 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2005. Time-Aware Authority Ranking. 
Journal of Internet Mathematics, 2(3): 301-332. 
Klaus Lorenz Berberich. 2004. Time-aware and 
Trend-based Authority Ranking. Master Thesis, 
Saarlandes  University, Germany. 
Sergey Brin and Lawrence Page. 1998. The Anatomy 
of a Large-scale Hypertextual Web Search Engine. 
Computer Networks and ISDN Systems, 30(1-
7):107-117. 
Gunes Erkan and Dragomir R. Radev. 2004a. 
LexPageRank: Prestige in Multi-Document Text 
Summarization, in Proceedings of EMNLP, pp365-
371. 
Gunes Erkan and Dragomir R. Radev. 2004b. 
LexRank: Graph-based Centrality as Salience in 
Text Summarization, Journal of Artificial 
Intelligence Research 22:457-479. 
Jon M. Kleinberg. 1999. Authoritative Sources in 
Hyperlinked Environment, Journal of the ACM, 
46(5):604-632. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of 
Document Semantic Graphs for Document 
Summarization, in Proceedings of LinkKDD 
Workshop, pp133-138. 
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu and Chunfa 
Yuan. 2006. Extractive Summarization using Intra- 
and Inter-Event Relevance, in Proceedings of 
ACL/COLING, pp369-376. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics, in Proceedings of HLT-
NAACL, pp71-78. 
Ziheng Lin, Tat-Seng Chua, Min-Yen Kan, Wee Sun 
Lee, Long Qiu, and Shiren Ye. 2007. NUS at DUC 
2007: Using Evolutionary Models for Text. In 
Proceedings of Document Understanding 
Conference (DUC) 2007. 
Rada Mihalcea and Paul Tarau. 2004. TextRank - 
Bringing Order into Text, in Proceedings of 
EMNLP, pp404-411. 
Rada Mihalcea. 2004. Graph-based Ranking 
Algorithms for Sentence Extraction, Applied to 
Text Summarization, in Proceedings of ACL 
(Companion Volume). 
Jahna OtterBacher, Gunes Erkan, Dragomir R. Radev. 
2005. Using Random Walks for Question-focused 
Sentence Retrieval, in Proceedings of 
HLT/EMNLP, pp915-922. 
Lucy Vanderwende, Michele Banko and Arul 
Menezes. 2004. Event-Centric Summary 
Generation, in Working Notes of DUC 2004. 
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2006. 
Using Cross-Document Random Walks for Topic-
Focused Multi-Document Summarization, in 
Proceedings of the 2006 IEEE/WIC/ACM 
International Conference on Web Intelligence, 
pp1012-1018. 
Xiaojun Wan. 2007. TimedTextRank: Adding the 
Temporal Dimension to Multi-document 
Summarization. In Proceedings of 30th ACM 
SIGIR, pp 867-868. 
Lei Yang, Lei Qi, Yan-Ping Zhao, Bin Gao, and Tie-
Yan Liu. 2007. Link Analysis using Time Series of 
Web Graphs. In Proceedings of CIKM?07. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information, in Working Notes of 
NTCIR-4. 
Philip S. Yu, Xin Li, and Bing Liu. 2004. On the 
Temporal Dimension of Search. In Proceedings of 
the 13th International World Wide Web Conference 
on Alternate Track Papers and Posters, pp 448-449. 
Philip S. Yu, Xin Li, and Bing Liu. 2005. Adding the 
Temporal Dimension to Search ? A Case Study in 
Publication Search. In Proceedings of the 2005 
IEEE/WIC/ACM International Conference on Web 
Intelligence. 
496
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 117?120,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
 
Co-Feedback Ranking for Query-Focused Summarization 
Furu Wei1,2,3  Wenjie Li1 and Yanxiang He2 
1 The Hong Kong Polytechnic University, Hong Kong 
{csfwei,cswjli}@comp.polyu.edu.hk 
2 Wuhan University, China 
{frwei,yxhe}@whu.edu.cn
3 IBM China Research Laboratory, Beijing, China 
 
Abstract 
In this paper, we propose a novel ranking 
framework ? Co-Feedback Ranking (Co-
FRank), which allows two base rankers to 
supervise each other during the ranking 
process by providing their own ranking results 
as feedback to the other parties so as to boost 
the ranking performance. The mutual ranking 
refinement process continues until the two 
base rankers cannot learn from each other any 
more. The overall performance is improved by 
the enhancement of the base rankers through 
the mutual learning mechanism. We apply this 
framework to the sentence ranking problem in 
query-focused summarization and evaluate its 
effectiveness on the DUC 2005 data set. The 
results are promising.  
1 Introduction and Background 
Sentence ranking is the issue of most concern in 
extractive summarization. Feature-based 
approaches rank the sentences based on the 
features elaborately designed to characterize the 
different aspects of the sentences. They have 
been extensively investigated in the past due to 
their easy implementation and the ability to 
achieve promising results. The use of feature-
based ranking has led to many successful (e.g. 
top five) systems in DUC 2005-2007 query-
focused summarization (Over et al, 2007). A 
variety of statistical and linguistic features, such 
as term distribution, sentence length, sentence 
position, and named entity, etc., can be found in 
literature. Among them, query relevance, 
centroid (Radev et al, 2004) and signature term 
(Lin and Hovy, 2000) are most remarkable.  
There are two alternative approaches to 
integrate the features. One is to combine features 
into a unified representation first, and then use it 
to rank the sentences. The other is to utilize rank 
fusion or rank aggregation techniques to combine 
the ranking results (orders, ranks or scores) 
produced by the multiple ranking functions into a 
unified rank. The most popular implementation 
of the latter approaches is to linearly combine the 
features to obtain an overall score which is then 
used as the ranking criterion. The weights of the 
features are either experimentally tuned or 
automatically derived by applying learning-based 
mechanisms. However, both of the above-
mentioned ?combine-then-rank? and ?rank-then-
combine? approaches have a common drawback. 
They do not make full use of the information 
provided by the different ranking functions and 
neglect the interaction among them before 
combination. We believe that each individual 
ranking function (we call it base ranker) is able 
to provide valuable information to the other base 
rankers such that they learn from each other by 
means of mutual ranking refinement, which in 
turn results in overall improvement in ranking. 
To the best of our knowledge, this is a research 
area that has not been well addressed in the past.  
The inspiration for the work presented in this 
paper comes from the idea of Co-Training (Blum 
and Mitchell, 1998), which is a very successful 
paradigm in the semi-supervised learning 
framework for classification. In essence, co-
training employs two weak classifiers that help 
augment each other to boost the performance of 
the learning algorithms. Two classifiers mutually 
cooperate with each other by providing their own 
labeling results to enrich the training data for the 
other parties during the supervised learning 
process. Analogously, in the context of ranking, 
although each base ranker cannot decide the 
overall ranking well on itself, its ranking results 
indeed reflect its opinion towards the ranking 
from its point of view. The two base rankers can 
then share their own opinions by providing the 
ranking results to each other as feedback. For 
each ranker, the feedback from the other ranker 
contains additional information to guide the 
refinement of its ranking results if the feedback 
is defined and used appropriately. This process 
continues until the two base rankers can not learn 
from each other any more. We call this ranking 
paradigm Co-Feedback Ranking (Co-FRank). 
The way how to use the feedback information 
117
 varies depending on the nature of a ranking task. 
In this paper, we particularly consider the task of 
query-focused summarization. We design a new 
sentence ranking algorithm which allows a 
query-dependent ranker and a query-independent 
ranker mutually learn from each other under the 
Co-FRank framework. 
2 Co-Feedback Ranking for Query-
Focused Summarization 
2.1 Co-Feedback Ranking Framework 
Given a set of objects O, one can define two base 
ranker f1 and f2:     Ooofof ??o?o ,, 21 . The 
ranking results produced by f1 and f2 individually 
are by no means perfect but the two rankers can 
provide relatively reasonable ranking 
information to supervise each other so as to 
jointly improve themselves. One way to do Co-
Feedback ranking is to take the most confident 
ranking results (e.g. highly ranked instances 
based on orders, ranks or scores) from one base 
ranker as feedback to update the other?s ranking 
results, and vice versa. This process continues 
iteratively until the termination condition is 
reached, as depicted in Procedure 1. While the 
standard Co-Training algorithm requires two 
sufficient and redundant views, we suggest f1 and 
f2 be two independent rankers which emphasize 
two different aspects of the objects in O. 
Procedure 1. Co-FRank(f1, f2, O) 
1:  Rank O with f1 and obtain the ranking results r1; 
2:  Rank O with f2 and obtain the ranking results r2; 
3:  Repeat  
4:  Select the top N ranked objects 1W  from r1 as 
feedback to supervise f2, and re-rank O using f2 
and 1W ; Update r2; 
5:  Select the top N ranked objects 2W  from r2 as 
feedback to supervise f1, and re-rank O using f1 
and 2W ; Update r1; 
5:  Until I(O). 
The termination condition I(O) can be defined 
according to different application scenarios. For 
example, I(O) may require the top K ranked 
objects in r1 and r2 to be identical if one is 
particularly interested in the top ranked objects. 
It is also very likely that r1 and r2 do not change 
any more after several iterations (or the top K 
objects do not change). In this case, the two base 
rankers can not learn from each other any more, 
and the Co-Feedback ranking process should 
terminate either. The final ranking results can be 
easily determined by combining the two base 
rankers without any parameter, because they 
have already learnt from each other and can be 
equally treated.  
2.2 Query-Focused Summarization based 
on Co-FRank  
The task of query-focused summarization is to 
produce a short summary (250 words in length) 
for a set of related documents D with respect to 
the query q that reflects a user?s information 
need. We follow the traditional extractive 
summarization framework in this study, where 
the two critical processes involved are sentence 
ranking and sentence selection, yet we focus 
more on the sentence ranking algorithm based on 
Co-FRank. As for sentence selection, we 
incrementally add into the summary the highest 
ranked sentence if it doesn?t significantly repeat1 
the information already included in the summary 
until the word limitation is reached. 
In the context of query-focused summarization, 
two kinds of features, i.e. query-dependent and 
query-independent features are necessary and 
they are supposed to complement each other. We 
then use these two kinds of features to develop 
the two base rankers. The query-dependent 
feature (i.e. the relevance of the sentence s to the 
query q) is defined as the cosine similarity 
between s and q.  
    qsqsqsqsrelf ?x  ? /,cos,1  (1) 
The words in s and q vectors are weighted by 
tf*isf. Meanwhile, the query-independent feature 
(i.e. the sentence significance based on word 
centroid) is defined as 
    swcscf
sw
/2 ? ? ?   (2) 
where c(w) is the centroid weight of the word w 
in s and     DSDs ws Nisftfwc w? ? ? . DSN  is the total 
number of the sentences in D, s
w
tf  is the 
frequency of w in s, and  wDw sfNisf Slog  is the 
inverse sentence frequency (ISF) of w, where sfw  
is the sentence frequency of w in D. The sentence 
ranking algorithm based on Co-FRank is detailed 
in the following Algorithm 1.  
Algorithm 1. Co-FRank(f1, f2, D, q) 
1:  Extract sentences S={s1, ? sm} from D;  
2:  Rank S with f1 and obtain the ranking results r1; 
3:  Rank S with f2 and obtain the ranking results r2; 
4:  Normalize r1,            11111 minmaxmin rrrsrsr ii  ;
5:  Normalize r2,            22222 minmaxmin rrrsrsr ii  ; 
6:  Repeat  
                                                 
1 A sentence is discarded if the cosine similarity of it to any 
sentence already selected into the summary is greater than 
0.9. 
118
 7:  Select the top N ranked sentences at round n n
1
W  
from r1 as feedback for f2, and re-rank S using f2 
and n
1
W ,                                                
        nssims n
k
k
ii /,
1
2 1? m WS ,     22 222 minmax
min
SS
SSS 
  
            iii ssfsr 222 1 SKK ??m                         (3)
8: Select the top N ranked sentences at round n n2W  
from r2 as feedback for f1, and re-rank S using f1
and n2W ;  
         nssims n
k
k
ii /,
1
1 2? m WS ,     11 111 minmax
min
SS
SSS 
    
            iii ssfsr 111 1 SKK ??m                              (4) 
9: Until the top K sentences in r1 and r2 are the same, 
both r1 and r2 do not change any more, or 
maximum iteration round is achieved. 
10: Calculate the final ranking results, 
            221 iii srsrsr  .                                        (5) 
The update strategies used in Algorithm 1, as 
formulated in Formulas (3) and (4), are designed 
based on the intuition that the new ranking of the 
sentence s from one base ranker (say f1) consists 
of two parts. The first part is the initial ranking 
produced by f1. The second part is the similarity 
between s and the top N feedback provided by 
the other ranker (say f2), and vice versa. The top 
K ranked sentences by f2 are supposed to be 
highly supported by f2. As a result, a sentence 
that is similar to those top ranked sentences 
should deserve a high rank as well.  nissim 2,W  
captures the effect of such feedback at round n 
and the definition of it may vary with regard to 
the application background. For example, it can 
be defined as the maximum, the minimum or the 
average similarity value between si and a set of 
feedback sentences in 2W . Through this mutual 
interaction, the two base rankers supervise each 
other and are expected as a whole to produce 
more reliable ranking results.  
We assume each base ranker is most confident 
with its first ranked sentence and set N to 1. 
Accordingly,  nissim 2,W is defined as the similarity 
between si and the one sentence in n
2
W . K  is a 
balance factor which can be viewed as the 
proportion of the dependence of the new ranking 
results on its initial ranking results. K is set to 10 
as 10 sentences are basically sufficient for the 
summarization task we work on. We carry out at 
most 5 iterations in the current implementation. 
3 Experimental Study   
We take the DUC 2005 data set as the evaluation 
corpus in this preliminary study. ROUGE (Lin 
and Hovy, 2003), which has been officially 
adopted in the DUC for years is used as the 
evaluation criterion. For the purpose of 
comparison, we implement the following two 
basic ranking functions and the linear 
combination of them for reference, i.e. the query 
relevance based ranker (denoted by QRR, same 
as f1) and the word centroid based ranker 
(denoted by WCR, same as f2), and the linear 
combined ranker, LCR= O QRR+(1- O )WCR, 
where O  is a combination parameter. QRR and 
WCR are normalized by    minmaxmin x , 
where x, max and min denote the original ranking 
score, the maximum ranking score and minimum 
ranking score produced by a ranker, respectively. 
Table 1 shows the results of the average recall 
scores of ROUGE-1, ROUGE-2 and ROUGE-
SU4 along with their 95% confidence intervals 
included within square brackets. Among them, 
ROUGE-2 is the primary DUC evaluation 
criterion.  
 ROUGE-1 ROUGE-2 ROUGE-SU4
QRR 0.3597 [0.3540, 0.3654]
0.0664 
[0.0630, 0.0697] 
0.1229 
[0.1196, 0.1261]
WCR 0.3504 [0.3436, 0.3565]
0.0644 
[0.0614, 0.0675] 
0.1171 
[0.1138, 0.1202]
LCR* 0.3513 [0.3449, 0.3572]
0.0645 
[0.0613, 0.0676] 
0.1177 
[0.1145, 0.1209]
Co- 
FRank+
0.3769 
[0.3712, 0.3829]
0.0762 
[0.0724, 0.0799] 
0.1317 
[0.1282, 0.1351]
LCR** 
0.3753 
[0.3692, 0.3813]
0.0757 
[0.0719, 0.0796] 
0.1302 
[0.1265, 0.1340]
Co- 
FRank++
0.3783 
[0.3719, 0.3852]
0.0775 
[0.0733, 0.0810] 
0.1323 
[0.1293, 0.1360]
* The worst results produced by LCR when O  = 0.1 
+ The worst results produced by Co-FRank when K  = 0.6 
** The best results produced by LCR when O  = 0.4 
++ The best results produced by Co-FRank when K  = 0.8 
Table 1 Compare different ranking strategies 
Note that the improvement of LCR over QRR 
and WCR is rather significant if the combination 
parameter O  is selected appropriately. Besides, 
Co-FRank is always superior to LCR regardless 
of the best or the worst ouput, and the 
improvement is visible. The reason is that both 
QRR and WCR are enhanced step by step in Co-
FRank, which in turn results in the increased 
overall performance. The trend of the 
improvement has been clearly observed in the 
experiments. This observation validates our 
motivation and the rationality of the algorithm 
proposed in this paper and motivates our further 
investigation on this topic.  
We continue to examine the parameter settings 
in LCR and Co-FRank. Table 2 shows the results 
of LCR when the value of O  changes from 0.1 to 
119
 1.0, and Table 3 shows the results of Co-FRank 
with K  ranging from 0.5 to 0.9. Notice that K  is 
not a combination parameter. We believe that a 
base ranker should have at least half belief in its 
initial ranking results and thus the value of the K  
should be greater than 0.5. We find that LCR 
heavily depends on O . LCR produces relatively 
good and stable results with O  varying from 0.4 
to 0.6. However, the ROUGE scores drop 
apparently when O  heading towards its two end 
values, i.e. 0.1 and 1.0. 
O  ROUGE-1 ROUGE-2 ROUGE-SU4
0.1 0.3513 [0.3449, 0.3572] 
0.0645 
[0.0613, 0.0676] 
0.1177 
[0.1145, 0.1209] 
0.2 0.3623 [0.3559, 0.3685] 
0.0699 
[0.0662, 0.0736] 
0.1235 
[0.1197, 0.1271] 
0.3 0.3721 [0.3660, 0.3778] 
0.0741 
[0.0706, 0.0778] 
0.1281 
[0.1246, 0.1318] 
0.4 0.3753 [0.3692, 0.3813] 
0.0757 
[0.0719, 0.0796] 
0.1302 
[0.1265, 0.1340] 
0.5 0.3756 [0.3698, 0.3814] 
0.0755 
[0.0717, 0.0793] 
0.1307 
[0.1272, 0.1342] 
0.6 0.3770 [0.3710, 0.3826] 
0.0754 
[0.0716, 0.0791] 
0.1323 
[0.1286, 0.1357] 
0.7 0.3698 [0.3636, 0.3759] 
0.0718 
[0.0680, 0.0756] 
0.1284 
[0.1246, 0.1318] 
0.8 0.3672 [0.3613, 0.3730] 
0.0706 
[0.0669, 0.0743] 
0.1271 
[0.1234, 0.1305] 
0.9 0.3651 [0.3591, 0.3708] 
0.0689 
[0.0652, 0.0726] 
0.1258 
[0.1220, 0.1293] 
Table 2 LCR with different O  values 
As shown in Table 3, the Co-FRank can 
always produce stable and promising results 
regardless of the change of K . More important, 
even the worst result produced by Co-FRank still 
outperforms the best result produced by LCR. 
K  ROUGE-1 ROUGE-2 ROUGE-SU4
0.5 0.3750 [0.3687, 0.3810] 
0.0766 
[0.0727, 0.0804] 
0.1308 
[0.1270, 0.1344] 
0.6 0.3769 [0.3712, 0.3829] 
0.0762 
[0.0724, 0.0799] 
0.1317 
[0.1282, 0.1351]
0.7 0.3775 [0.3713, 0.3835] 
0.0763 
[0.0724, 0.0801] 
0.1319 
[0.1282, 0.1354]
0.8 0.3783 [0.3719, 0.3852] 
0.0775 
[0.0733, 0.0810] 
0.1323 
[0.1293, 0.1360] 
0.9 0.3779 [0.3722, 0.3835] 
0.0765 
[0.0728, 0.0803] 
0.1319 
[0.1285, 0.1354 
Table 3 Co-FRank with different K  values 
We then compare our results to the DUC 
participating systems. We present the following 
representative ROUGE results of (1) the top 
three DUC participating systems according to 
ROUGE-2 scores (S15, S17 and S10); and (2) 
the NIST baseline which simply selects the first 
sentences from the documents. 
 ROUGE-1 ROUGE-2 ROUGE-SU4
Co-FRank 0.3783 0.0775 0.1323 
S15 - 0.0725 0.1316 
S17 - 0.0717 0.1297 
S10 - 0.0698 0.1253 
Baseline   0.0403 0.0872 
Table 4 Compare with DUC participating systems 
It is clearly shown in Table 4 that Co-FRank 
can produce a very competitive result, which 
significantly outperforms the NIST baseline and 
meanwhile it is superior to the best participating 
system in the DUC 2005. 
4 Conclusion and Future Work 
In this paper, we propose a novel ranking 
framework, namely Co-Feedback Ranking (Co-
FRank), and examine its effectiveness in query-
focused summarization. There is still a lot of 
work to be done on this topic. Although we show 
the promising achievements of Co-Frank from 
the perspective of experimental studies, we 
expect a more theoretical analysis on Co-FRank. 
Meanwhile, we would like to investigate more 
appropriate techniques to use feedback, and we 
are interested in applying Co-FRank to the other 
applications, such as opinion summarization 
where the integration of opinion-biased and 
document-biased ranking is necessary. 
Acknowledgments 
The work described in this paper was supported 
by the Hong Kong Polytechnic University 
internal the grants (G-YG80 and G-YH53) and 
the China NSF grant (60703008). 
References  
Avrim Blum and Tom Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-Training. In 
Proceedings of the Eleventh Annual Conference on 
Computational Learning Theory, pp92-100. 
Chin-Yew Lin and Eduard Hovy. 2000. The 
Automated Acquisition of Topic Signature for Text 
Summarization. In Proceedings of COLING, 
pp495-501. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL, pp71-78. 
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based 
Summarization of Multiple Documents. 
Information Processing and Management, 40:919-
938. 
Paul Over, Hoa Dang and Donna Harman. 2007. DUC 
in Context. Information Processing and 
Management, 43(6):1506-1520. 
120
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 172?182, Dublin, Ireland, August 23-29 2014.
Building Large-Scale Twitter-Specific Sentiment Lexicon :
A Representation Learning Approach
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\?
, Ming Zhou
?
, Ting Liu
\
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach. We cast sentiment lexicon learning as a phrase-level sentiment classification
task. The challenges are developing effective feature representation of phrases and obtaining
training data with minor manual annotations for building the sentiment classifier. Specifical-
ly, we develop a dedicated neural architecture and integrate the sentiment information of tex-
t (e.g. sentences or tweets) into its hybrid loss function for learning sentiment-specific phrase
embedding (SSPE). The neural network is trained from massive tweets collected with positive
and negative emoticons, without any manual annotation. Furthermore, we introduce the Urban
Dictionary to expand a small number of sentiment seeds to obtain more training data for building
the phrase-level sentiment classifier. We evaluate our sentiment lexicon (TS-Lex) by applying
it in a supervised learning framework for Twitter sentiment classification. Experiment results
on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than
previously introduced sentiment lexicons.
1 Introduction
A sentiment lexicon is a list of words and phrases, such as ?excellent?, ?awful? and ?not bad?, each
of which is assigned with a positive or negative score reflecting its sentiment polarity and strength.
Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment in-
formation and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012;
Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to
estimate the sentiment score of each phrase. These methods typically employ parsing results, syntac-
tic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between
phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et
al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit
the dependency relations between sentiment words and aspect words. However, parsing information and
the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon
from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and
it is hard to have reliable tweet parsers due to the informal language style.
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level classi-
fication task. Our method contains two part: (1) a representation learning algorithm to effectively learn
the continuous representation of phrases, which are used as features for phrase-level sentiment classifica-
tion, (2) a seed expansion algorithm that enlarge a small list of sentiment seeds to collect training data for
building the phrase-level classifier. Specifically, we learn sentiment-specific phrase embedding (SSPE),
which is a low-dimensional, dense and real-valued vector, by encoding the sentiment information and
?
This work was partly done when the first author was visiting Microsoft Research.
?
Corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
172
Sentiment 
Classifier 
Sentiment 
Lexicon 
Phrase Embedding 
NEG: goon looser 
Sentiment Seeds 
Tweets with Emoticons 
Soooo nice~ :) 
It?s horrible :( 
Seed 
Expansion 
Representation 
Learning 
POS: good :) 
NEG: poor :( 
NEU: when he 
Training Data 
POS: wanted fave 
NEU: again place 
[1.31,0.97] good: 
[0.99,1.17] coool: 
[-0.81,-0.7] bad: 
[-0.8,-0.72] mess: 
Learning 
Algorithm 
Figure 1: The representation learning approach for building Twitter-specific sentiment lexicon.
syntactic contexts into the continuous representation of phrases
1
. As a result, the nearest neighbors in the
embedding space of SSPE are favored to have similar semantic usage as well as the same sentiment po-
larity. To this end, we extend the existing phrase embedding learning algorithm (Mikolov et al., 2013b),
and develop a dedicated neural architecture with hybrid loss function to incorporate the supervision from
sentiment polarity of text (e.g. tweets). We learn SSPE from tweets, leveraging massive tweets con-
taining positive and negative emoticons as training set without any manual annotation. To obtain more
training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban
Dictionary
2
, which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we
utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in
the sentiment lexicon.
We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learn-
ing framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the bench-
mark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced
lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the top-
performed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by
regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu
and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding
learning algorithms. The main contributions of this work are as follows:
? To our best knowledge, this is the first work that leverages the continuous representation of phrases
for building large-scale sentiment lexicon from Twitter;
? We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from
massive tweets selected with positive and negative emoticons;
? We report the results that our lexicon outperforms existing sentiment lexicons by applying them in
a supervised learning framework for Twitter sentiment classification.
2 Related Work
In this section, we give a brief review about building sentiment lexicon and learning continuous repre-
sentation of words and phrases.
2.1 Sentiment Lexicon Learning
Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das
and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Tur-
ney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods
1
Word/unigram is also regarded as phrase in this paper.
2
http://www.urbandictionary.com/
173
and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by re-
garding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation
algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran,
2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score
of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are
mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples
from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between senti-
ment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet.
Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al.
(2010) represent words and phrases with their syntactic contexts within a window size from the web
documents. Unlike the dominated propagation based methods, we explore the classification framework
based on representation learning for building large-scale sentiment lexicon from Twitter.
To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual
information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and
:(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expres-
sions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and
sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase
embedding and propose a representation learning approach to build sentiment lexicon.
2.2 Learning Continuous Representation of Word and Phrase
Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al.,
2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al.,
2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et
al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al.,
2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend
the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the
embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing
embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentiment-
specific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike
previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike
Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we
integrate the sentiment information of text into our method. It is worth noting that we focus on learning
the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013)
that learn the compositionality of sentences.
3 Methodology
In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a
classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases
as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed
expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment
distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode
the sentiment information into the continuous representation of phrases, we extend an existing phrase
embedding learning algorithm (Mikolov et al., 2013b) and develop a tailored neural architecture to learn
sentiment-specific phrase embedding (SSPE), as described in subsection 3.1. To automatically obtain
more training data for building the phrase-level sentiment classifier, we use the similar words from Urban
Dictionary to expand a small list of sentiment seeds, as described in subsection 3.2.
3.1 Sentiment-Specific Phrase Embedding
Mikolov et al. (2013b) introduce Skip-Gram to learn phrase embedding based on the context words of
phrases, as illustrated in Figure 2(a).
Given a phrase w
i
, Skip-Gram maps it into its continuous representation e
i
. Then, Skip-Gram utilizes
174
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
polj 
(a) Skip-Gram (b) Our Model 
sej 
sj 
Figure 2: The traditional Skip-Gram model and our neural architecture for learning sentiment-specific
phrase embedding (SSPE).
e
i
to predict the context words of w
i
, namely w
i?2
, w
i?1
, w
i+1
, w
i+2
, et al. Hierarchical softmax (Morin
and Bengio, 2005) is leveraged to accelerate the training procedure because the vocabulary size of phrase
table is typically huge. The objective of Skip-Gram is to maximize the average log probability:
f
syntactic
=
1
T
T
?
i=1
?
?c?j?c,j 6=0
log p(w
i+j
|e
i
) (1)
where T is the occurrence of each phrase in the corpus, c is the window size, e
i
is the embedding of the
current phrase w
i
, w
i+j
is the context words of w
i
, p(w
i+j
|e
i
) is calculated with hierarchical softmax.
The basic softmax unit is calculated as softmax
i
= exp(z
i
)/
?
k
exp(z
k
). We leave out the details
of hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013b) due to the page limit. It is
worth noting that, Skip-Gram is capable to learn continuous representation of words and phrases with
the identical model (Mikolov et al., 2013b).
To integrate sentiment information into the continuous representation of phrases, we develop a tailored
neural architecture to learn SSPE, as illustrated in Figure 2(b). Given a triple ?w
i
, s
j
, pol
j
? as input,
where w
i
is a phrase contained in the sentence s
j
whose gold sentiment polarity is pol
j
, our training
objective is to (1) utilize the embedding of w
i
to predict its context words, and (2) use the sentence
representation se
j
to predict the gold sentiment polarity of s
j
, namely pol
j
. We simply average the
embedding of phrases contained in a sentence as its continuous representation (Huang et al., 2012). The
objective of the sentiment part is to maximize the average of log sentiment probability:
f
sentiment
=
1
S
S
?
j=1
log p(pol
j
|se
j
) (2)
where S is the occurrence of each sentence in the corpus,
?
k
pol
jk
= 1. For binary classification
between positive and negative, the distribution of [0,1] is for positive and [1,0] is for negative. Our final
training objective is to maximize the linear combination of the syntactic and sentiment parts:
f = ? ? f
syntactic
+ (1? ?) ? f
sentiment
(3)
where ? weights the two parts. Accordingly, the nearest neighbors in the embedding space of SSPE are
favored to have similar semantic usage as well as the same sentiment polarity.
We train our neural model with stochastic gradient descent and use AdaGrad (Duchi et al., 2011) to
update the parameters. We empirically set embedding length as 50, window size as 3 and the learning
rate of AdaGrad as 0.1. Hyper-parameter ? is tuned on the development set. To obtain large-scale
training corpus, we collect tweets from April, 2013 through TwitterAPI. After filtering the tweets that
are too short (< 5 words) and removing @user and URLs, we collect 10M tweets (5M positive and 5M
negative) with positive and negative emoticons
3
, which is are utilized as the training data to train our
neural model. The vocabulary size is 750,000 after filtering the 1?4 grams through frequency.
3
We use the emoticons selected by Hu et al. (2013), namely :) : ) :-) :D =) as positive and :( : ( :-( as negative ones.
175
3.2 Seed Expansion with Urban Dictionary
Urban Dictionary is a web-based dictionary that contains more than seven million definitions until March,
2013
4
. It was intended as a dictionary of slang, cultural words or phrases not typically found in standard
dictionaries, but it is now used to define any word or phrase. For each item in Urban Dictionary, there is
a list of similar words contributed by volunteers. For example, the similar words of ?cooool? are ?cool?,
?awesome?, ?coooool?, et al
5
and the similar words of ?not bad? are ?good?, ?ok? and ?cool?, et al
6
.
These similar words are typically semantically close to and have the same sentiment polarity with the
target word. We conduct preliminary statistic on the items of Urban Dictionary from ?a? to ?z?, and
find that there are total 799,430 items containing similar words and each of them has about 10.27 similar
words on average.
We utilize Urban Dictionary to expand little sentiment seeds for collecting training data for building
the phrase-level sentiment classifier. We manually label the top frequent 500 words from the vocabulary
of SSPE as positive, negative or neutral. After removing the ambiguous ones, we obtain 125 positive, 109
negative and 140 neutral words, which are regarded as the sentiment seeds
7
. Afterwards, we leverage
the similar words from Urban Dictionary to expand the sentiment seeds. We first build a k-nearest
neighbors (KNN) classifier by regarding the sentiment seeds as gold standard. Then, we employ the KNN
classifier on the items of Urban Dictionary containing similar words, and predict a three-dimensional
discrete vector [knn
pos
, knn
neg
, knn
neu
] for each item, reflecting the hits numbers of sentiment seeds
with different sentiment polarity in its similar words. For example, the vector value of ?not bad? is
[10, 0, 0], which means that there are 10 positive seeds, 0 negative seeds and 0 neutral seeds occur in
its similar words. To ensure the quality of the expanded words, we set threshold for each category to
collect the items with high quality as expanded words. Take the positive category as an example, we
keep an item as positive expanded word if it satisfies knn
pos
> knn
neg
+ threshold
pos
and knn
pos
>
knn
neu
+ threshold
pos
simultaneously. We empirically set the thresholds of positive, negative and
neutral as 6,3,2 respectively by balancing the size of expanded words in three categories. After seed
expansion, we collect 1,512 positive, 1,345 negative and 962 neutral words, which are used as the training
data to build the phrase-level sentiment classifier. We also tried the propagation methods to expand the
sentiment seeds, namely iteratively added the similar words of sentiment seeds from Urban Dictionary
into the expanded word collection. However, the quantity of expanded words is less than the KNN-based
results and the quality is relatively poor.
After obtaining the training data and feature representation of phrases, we build the phrase-level clas-
sifier with softmax, whose length is two for the positive vs negative case:
y(w) = softmax(? ? e
i
+ b) (4)
where ? and b are the parameters of classifier, e
i
is the embedding of the current phrase w
i
, y(w) is the
predicted sentiment distribution of item w
i
. We employ the classifier to predict the sentiment distribution
of each phrase in the vocabulary of SSPE, and save the phrases as well as their sentiment probability in
the positive (negative) lexicon if the positive (negative) probability is larger than 0.5.
4 Experiment
In this section, we conduct experiments to evaluate the effectiveness of our sentiment lexicon (TS-Lex)
by applying it in the supervised learning framework for Twitter sentiment classification, as given in
subsection 4.1. We also directly evaluate the quality of SSPE as it forms the fundamental component for
building sentiment lexicon. We use SSPE as the feature for sentiment classification of items in existing
sentiment lexicons, as described in subsection 4.2.
4
http://en.wikipedia.org/wiki/Urban Dictionary
5
http://www.urbandictionary.com/define.php?term=cooool
6
http://www.urbandictionary.com/define.php?term=not+bad
7
We will publish the sentiment seeds later.
176
4.1 Twitter Sentiment Classification
Experiment Setup and Dataset We conduct experiments on the benchmark Twitter sentiment classi-
fication dataset (message-level) from SemEval 2013 (Nakov et al., 2013). The training and development
sets were completely released to task participants. However, we were unable to download all the training
and development sets because some tweets were deleted or not available due to modified authorization
status. The statistic of the positive and negative tweets in our dataset are given in Table 1(b). We train
positive vs negative classifier with LibLinear (Fan et al., 2008) with default settings on the training set,
tune parameters -c on the dev set and evaluate on the test set. The evaluation metric is Macro-F1.
(a) Sentiment Lexicons
Lexicon Positive Negative Total
HL 2,006 4,780 6,786
MPQA 2,301 4,150 6,451
NRC-Emotion 2,231 3,324 5,555
TS-Lex 178,781 168,845 347,626
HashtagLex 216,791 153,869 370,660
Sentiment140Lex 480,008 260,158 740,166
(b) SemEval 2013 Dataset
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 1: Statistic of sentiment lexicons and Twitter sentiment classification datasets.
Results and Analysis We compare TS-Lex with HL
8
(Hu and Liu, 2004), MPQA
9
(Wilson et al.,
2005), NRC-Emotion
10
(Mohammad and Turney, 2012), HashtagLex and Sentiment140Lex
11
(Moham-
mad et al., 2013). The statistics of TS-Lex and other sentiment lexicons are illustrated in Table 1(a). HL,
MPQA and NRC-Emotion are traditional sentiment lexicons with a relative small lexicon size. Hashta-
gLex and Sentiment140Lex are Twitter-specific sentiment lexicons. We can find that, TS-Lex is larger
than the traditional sentiment lexicons.
We evaluate the effectiveness of TS-Lex by applying it as the features for Twitter sentiment classifica-
tion in the supervised learning framework (Pang et al., 2002). We conduct experiments in two settings,
namely only utilizing the lexicon features (Unique) and appending lexicon feature to existing feature
sets (Appended). In the first setting, we design the lexicon features as same as the top-performed Twit-
ter sentiment classification system in SemEval2013
12
(Mohammad et al., 2013). For each sentiment
polarity (positive vs negative), the lexicon features are:
? total count of tokens in the tweet with score greater than 0;
? the sum of the scores for all tokens in the tweet;
? the maximal score;
? the non-zero score of the last token in the tweet;
In the second experiment setting, we append the lexicon features to the existing basic feature. We use
the feature sets of Mohammad et al. (2013) excluding the lexicon feature as the basic feature, including
bag-of-words, pos-tagging, emoticons, hashtags, elongated words, etc. Experiment results of the Unique
features and Appended features from different sentiment lexicons on Twitter sentiment classification are
given in Table 2(a).
From Table 2(a), we can find that TS-Lex yields best performance in both Unique and Appended
feature sets among all sentiment lexicons, including two large-scale Twitter-specific sentiment lexicons.
The reason is that the classifier for building TS-Lex utilize (1) the well developed feature representation
of phrases (SSPE), which captures the semantic and sentiment connections between phrases, and (2) the
enlarged sentiment words through web intelligence as training data. HashtagLex and Sentiment140Lex
8
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html#lexicon
9
http://mpqa.cs.pitt.edu/lexicons/subj lexicon/
10
http://www.saifmohammad.com/WebPages/ResearchInterests.html
11
We utilize the unigram and bigram lexicons from HashtagLex and Sentiment140Lex.
12
http://www.saifmohammad.com/WebPages/Abstracts/NRC-SentimentAnalysis.htm
177
(a)
Lexicon Unique Appended
HL 60.49 79.40
MPQA 59.15 76.54
NRC-Emotion 54.81 76.79
HashtagLex 65.30 76.67
Sentiment140Lex 72.51 80.68
TS-Lex 78.07 82.36
(b)
Lexicon Unique
Seed 57.92
Expand 60.69
Lexicon(seed) 74.64
TS-Lex 78.07
Table 2: Macro-F1 on Twitter sentiment classification with different lexicon features.
only utilize the relations between phrases and hashtag/emoticon seeds, yet do not well capture the con-
nections between phrases. In the Unique setting, the performances of the traditional lexicons (HL, MPQA
and NRC-Emotion) are lower than large-scale Twitter-specific lexicons (HashtagLex, Sentiment140Lex
and our lexicon). The reason is that, tweets have the informal language style and contain slangs and di-
verse multi-word phrases, which are not well covered by the traditional sentiment lexicons with a small
size. After incorporating the lexicon feature of TS-Lex into the top-performs system (Mohammad et al.,
2013), we further improve the macro-F1 from 84.70% to 85.65%.
Effect of Seed Expansion with Urban Dictionary To verify the effectiveness of seed expansion
through Urban Dictionary, we conduct experiments by applying (1) sentiment seeds (Seed), (2) words
after expansion (Expand), (3) sentiment lexicon generated from the classifier only utilizing sentiment
seeds as training data (Lexicon(seed)), (4) the final lexicon (TS-Lex) exploiting the expanded words as
training data to build sentiment classifier, to produce lexicon features, and only use them for Twitter
sentiment classification (Unique). From Table 2(b), we find that the performance of sentiment seeds and
expanded words are relatively poor due to their low coverage. Under this scenario, seed expansion yields
2.77% improvement (from 57.92% to 60.69%) on macro-F1. By utilizing the expanded words as training
data to build the phrase-level sentiment classifier, TS-Lex obtains 3.43% improvements on Twitter senti-
ment classification (from 74.64% to 78.07%), which verifies the effectiveness of seed expansion through
Urban Dictionary. In addition, we find that only using a small number of sentiment seeds as the training
data, we can obtain superior performance (74.64%) than all baseline lexicons. This indicates that the
representation learning approach effectively capture the semantic and sentimental connections between
phrases through SSPE, and leverage them for building the sentiment lexicon.
Effect of ? in SSPE We tune the hyper-parameter ? of SSPE on the development set of SemEval 2013,
and study its influence on the performance of Twitter sentiment classification by applying the generated
lexicon as features. We utilize the expanded words as training data to train softmax and only utilize the
lexicon features (Unique) for Twitter sentiment classification. Experiment results with different ? are
illustrated in Figure 3(a).
From Figure 3(a), we can see that that SSPE performs better when ? is in the range of [0.1, 0.3], which
is dominated by the sentiment information. The model with ? = 1 stands for Skip-Gram model. The
sharp decline at ? = 1 indicates the importance of sentiment information in learning sentiment-specific
phrase embedding for building sentiment lexicon.
Discussion In the experiment, we do not apply TS-Lex into the unsupervised learning framework for
Twitter sentiment classification. The reason is that the lexicon-based unsupervised method typically
require the sentiment lexicon to have high precision, yet our task is to build large-scale lexicon (TS-Lex)
with broad coverage. We leave this as the future work, although we may set higher threshold (e.g. larger
than 0.5) to increase the precision of TS-Lex and loose the recall.
4.2 Evaluation of Different Representation Learning Methods
Experiment Setup and Dataset We conduct sentiment classification of items in two traditional senti-
ment lexicons, HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005), to evaluate the effective of the
178
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.735
0.74
0.745
0.75
0.755
0.76
0.765
0.77
0.775
0.78
0.785
?
Ma
cro
?F1
 
 
TS?Lex
(a) SSPE with different ? on the development set for Twitter
sentiment classification.
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
C&W ReEmbed(C&W) W2V ReEmbed(W2V) MVSA SSPE
MPQAHL
(b) Sentiment classification of lexicons with different embed-
ding learning algrithms.
Figure 3: Experiment results with different settings.
sentiment-specific phrase embedding (SSPE). We train the positive vs negative classifier with LibLin-
ear (Fan et al., 2008). The evaluation metric is the macro-F1 of 5-fold cross validation. The statistics of
HL and MPQA are listed in Table 1(a).
Baseline Embedding Learning Algorithms We compare SSPE with the following embedding learn-
ing algorithms:
(1) C&W. C&W is one of the most representative embedding learning algorithms (Collobert et al.,
2011) for learning word embedding, which has been proven effective in many NLP tasks.
(2) W2V. Mikolov et al. (2013a) introduce Word2Vec for learning the continuous vectors for words
and phrases. We utilize Skip-Gram as it performs better than CBOW in the experiments.
(3) MVSA. Maas et al. (2011) learn word vectors for sentiment analysis with a probabilistic model of
documents utilizing the sentiment polarity of documents.
(4) ReEmbed. Lebret et al. (2013) learn task-specific embedding from existing embedding and task-
specific corpus. We utilize the training set of Twitter sentiment classification as the labeled corpus to
re-embed words. ReEmbed(C&W) and ReEmbed(W2V) stand for the use of different embedding results
as the reference word embedding.
The embedding results of the baseline algorithms and SSPE are trained with the same dataset and
parameter sets.
Results and Analysis Experiment results of the baseline embedding learning algorithms and SSPE are
given in Figure 3(b). We can see that SSPE yields best performance on both lexicons. The reason is that
SSPE effectively encode the sentiment information of tweets as well as the syntactic contexts of phrases
from massive data into the continuous representation of phrases. The performances of C&W and W2V
are relatively low because they only utilize the syntactic contexts of items, yet ignore the sentiment in-
formation of text, which is crucial for sentiment analysis. ReEmbed(C&W) and ReEmbed(W2V) achieve
better performance than C&W and W2V because the sentiment information of sentences are incorporated
into the continuous representation of phrases. There is a gap between ReEmbed and SSPE because SSPE
leverages more sentiment supervision from massive tweets collected by positive and negative emoticons.
5 Conclusion
In this paper, we propose building large-scale Twitter-specific sentiment lexicon with a representation
learning approach. Our method contains two parts: (1) a representation learning algorithm to effectively
learn the embedding of phrases, which are used as features for classification, (2) a seed expansion al-
gorithm that enlarge a small list of sentiment seeds to obtain training data for building the phrase-level
sentiment classifier. We introduce a tailored neural architecture and integrate the sentiment information
of tweets into its hybrid loss function for learning sentiment-specific phrase embedding (SSPE). We
learn SSPE from the tweets collected by positive and negative emoticons, without any manual annota-
179
tion. To collect more training data for building the phrase-level classifier, we utilize the similar words
from Urban Dictionary to expand a small list of sentiment seeds. The effectiveness of our sentiment
lexicon (TS-Lex) has been verified through applied in the supervised learning framework for Twitter
sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-
Lex outperforms previously introduced sentiment lexicons and further improves the top-perform system
in SemEval 2013 with feature combination. In future work, we plan to apply TS-Lex into the unsuper-
vised learning framework for Twitter sentiment classification.
Acknowledgements
We thank Nan Yang, Yajuan Duan and Yaming Sun for their great help. This research was partly sup-
ported by National Natural Science Foundation of China (No.61133012, No.61273321, No.61300113).
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200?2204.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. Journal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspec-
tives. IEEE Trans. Pattern Analysis and Machine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity from twitter. In ICWSM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537.
George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word
observations. ICML.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for amazon: Sentiment extraction from small talk on the web.
Management Science, 53(9):1375?1388.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochas-
tic optimization. The Journal of Machine Learning Research, pages 2121?2159.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classifi-
cation. In Proceedings of the 14th ACM international conference on Information and knowledge management,
pages 617?624. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In
ACL, volume 7, pages 442?431.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. The Journal of Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and applications for sentiment analysis. Communications of the ACM,
56(4):82?89.
Ming Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. 2013. Unsupervised sentiment analysis with emotional signals.
In Proceedings of the International World Wide Web Conference, pages 607?618.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In ACL, pages 873?882. ACL.
180
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th
international conference on Computational Linguistics, page 1367. Association for Computational Linguistics.
Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Annual Meeting of the Association for Computa-
tional Linguistics.
R?emi Lebret, Jo?el Legrand, and Ronan Collobert. 2013. Is deep learning really necessary for word embeddings?
NIPS workshop.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the 50th ACL, pages 410?419. ACL, July.
Dekang Lin. 1994. Principar: an efficient, broad-coverage, principle-based parser. In Proceedings of the 15th
conference on COLING, pages 482?488. Association for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies,
5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of
words and phrases and their compositionality. The Conference on Neural Information Processing Systems.
Saif M Mohammad and Peter D Turney. 2012. Crowdsourcing a word?emotion association lexicon. Computa-
tional Intelligence.
Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-the-art in
sentiment analysis of tweets. Proceedings of the International Workshop on Semantic Evaluation.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceed-
ings of the international workshop on artificial intelligence and statistics, pages 246?252.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic
Evaluation, volume 13.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information
retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine
learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 79?86.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double
propagation. In IJCAI, volume 9, pages 1199?1204.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational linguistics, 37(1):9?27.
Delip Rao and Deepak Ravichandran. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675?682.
Association for Computational Linguistics.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng, and C.D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions. In Conference on Empirical Methods in Natural Language
Processing, pages 151?161.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher
Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference
on Empirical Methods in Natural Language Processing, pages 1631?1642.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2014. Radical-enhanced chinese
character embedding. arXiv preprint arXiv:1404.4714.
181
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word
embedding for twitter sentiment classification. In Procedding of the 52th Annual Meeting of Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. Annual Meeting of the Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of
reviews. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 417?424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, pages 777?785. Association for Computational
Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In AAAI/IAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
347?354.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets in a
two-stage framework. In Proceedings of the 51st ACL, pages 1764?1773. ACL.
182
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Joint Segmentation and Classification Framework
for Sentiment Analysis
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\
, Li Dong
]?
, Ting Liu
\
, Ming Zhou
?
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
]
Beihang University, Beijing, China
\
{dytang, qinb, tliu}@ir.hit.edu.cn
?
{fuwei, mingzhou}@microsoft.com
]
donglixp@gmail.com
Abstract
In this paper, we propose a joint segmenta-
tion and classification framework for sen-
timent analysis. Existing sentiment clas-
sification algorithms typically split a sen-
tence as a word sequence, which does not
effectively handle the inconsistent senti-
ment polarity between a phrase and the
words it contains, such as ?not bad? and
?a great deal of ?. We address this issue
by developing a joint segmentation and
classification framework (JSC), which si-
multaneously conducts sentence segmen-
tation and sentence-level sentiment classi-
fication. Specifically, we use a log-linear
model to score each segmentation candi-
date, and exploit the phrasal information
of top-ranked segmentations as features to
build the sentiment classifier. A marginal
log-likelihood objective function is de-
vised for the segmentation model, which
is optimized for enhancing the sentiment
classification performance. The joint mod-
el is trained only based on the annotat-
ed sentiment polarity of sentences, with-
out any segmentation annotations. Experi-
ments on a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that, our joint model performs com-
parably with the state-of-the-art methods.
1 Introduction
Sentiment classification, which classifies the senti-
ment polarity of a sentence (or document) as posi-
tive or negative, is a major research direction in the
field of sentiment analysis (Pang and Lee, 2008;
Liu, 2012; Feldman, 2013). Majority of existing
approaches follow Pang et al. (2002) and treat sen-
?
This work was partly done when the first and fourth
authors were visiting Microsoft Research.
timent classification as a special case of text cate-
gorization task. Under this perspective, previous
studies typically use pipelined methods with two
steps. They first produce sentence segmentation-
s with separate text analyzers (Choi and Cardie,
2008; Nakagawa et al., 2010; Socher et al., 2013b)
or bag-of-words (Paltoglou and Thelwall, 2010;
Maas et al., 2011). Then, feature learning and sen-
timent classification algorithms take the segmenta-
tion results as inputs to build the sentiment classi-
fier (Socher et al., 2011; Kalchbrenner et al., 2014;
Dong et al., 2014).
The major disadvantage of a pipelined method
is the problem of error propagation, since sen-
tence segmentation errors cannot be corrected by
the sentiment classification model. A typical kind
of error is caused by the polarity inconsistency be-
tween a phrase and the words it contains, such
as ?not bad, bad? and ?a great deal of, great?.
The segmentations based on bag-of-words or syn-
tactic chunkers are not effective enough to han-
dle the polarity inconsistency phenomenons. The
reason lies in that bag-of-words segmentations re-
gard each word as a separate unit, which losses
the word order and does not capture the phrasal
information. The segmentations based on syntac-
tic chunkers typically aim to identify noun group-
s, verb groups or named entities from a sentence.
However, many sentiment indicators are phrases
constituted of adjectives, negations, adverbs or id-
ioms (Liu, 2012; Mohammad et al., 2013a), which
are splitted by syntactic chunkers. Besides, a bet-
ter approach would be to utilize the sentiment in-
formation to improve the segmentor. Accordingly,
the sentiment-specific segmentor will enhance the
performance of sentiment classification in turn.
In this paper, we propose a joint segmentation
and classification framework (JSC) for sentimen-
t analysis, which simultaneous conducts sentence
segmentation and sentence-level sentiment clas-
sification. The framework is illustrated in Fig-
477
Segmentations Input 
that is not bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
Polarity: +1 
-1 
-1 
+1 
+1 
<+1,-1>   NO 
Polarity Update 
<+1,-1>   NO 
<+1,+1>  YES 
<+1,+1>  YES 
SC SEG CG 
Update 
SC 
2.3  
1.6 
0.6 
0.4 
0.6 
0.4 
2.3 
1.6 
SEG 
Rank 
Top K  
Figure 1: The joint segmentation and classification framework (JSC) for sentiment classification. CG
represents the candidate generation model, SC means the sentiment classification model and SEG stands
for the segmentation ranking model. Down Arrow means the use of a specified model, and Up Arrow
indicates the update of a model.
ure 1. We develop (1) a candidate generation mod-
el to generate the segmentation candidates of a
sentence, (2) a segmentation ranking model to s-
core each segmentation candidate of a given sen-
tence, and (3) a classification model to predic-
t the sentiment polarity of each segmentation. The
phrasal information of top-ranked candidates from
the segmentation model are utilized as features to
build the sentiment classifier. In turn, the predict-
ed sentiment polarity of segmentation candidates
from classification model are leveraged to update
the segmentor. We score each segmentation can-
didate with a log-linear model, and optimize the
segmentor with a marginal log-likelihood objec-
tive. We train the joint model from sentences an-
notated only with sentiment polarity, without any
segmentation annotations.
We evaluate the effectiveness of our joint mod-
el on a benchmark Twitter sentiment classifica-
tion dataset in SemEval 2013. Results show that
the joint model performs comparably with state-
of-the-art methods, and consistently outperforms
pipeline methods in various experiment settings.
The main contributions of the work presented in
this paper are as follows.
? To our knowledge, this is the first work that
automatically produces sentence segmenta-
tion for sentiment classification within a joint
framework.
? We show that the joint model yields com-
parable performance with the state-of-the-art
methods on the benchmark Twitter sentiment
classification datasets in SemEval 2013.
2 Related Work
Existing approaches for sentiment classification
are dominated by two mainstream directions.
Lexicon-based approaches (Turney, 2002; Ding
et al., 2008; Taboada et al., 2011; Thelwall et
al., 2012) typically utilize a lexicon of sentiment
words, each of which is annotated with the sen-
timent polarity or sentiment strength. Linguis-
tic rules such as intensifications and negations are
usually incorporated to aggregate the sentimen-
t polarity of sentences (or documents). Corpus-
based methods treat sentiment classification as a
special case of text categorization task (Pang et al.,
2002). They mostly build the sentiment classifier
from sentences (or documents) with manually an-
notated sentiment polarity or distantly-supervised
corpora collected by sentiment signals like emoti-
cons (Go et al., 2009; Pak and Paroubek, 2010;
Kouloumpis et al., 2011; Zhao et al., 2012).
Majority of existing approaches follow Pang et
al. (2002) and employ corpus-based method for
sentiment classification. Pang et al. (2002) pi-
oneer to treat the sentiment classification of re-
views as a special case of text categorization prob-
lem and first investigate machine learning meth-
ods. They employ Naive Bayes, Maximum En-
tropy and Support Vector Machines (SVM) with a
diverse set of features. In their experiments, the
best performance is achieved by SVM with bag-
of-words feature. Under this perspective, many s-
tudies focus on designing or learning effective fea-
tures to obtain better classification performance.
On movie or product reviews, Wang and Man-
ning (2012) present NBSVM, which trades-off
478
between Naive Bayes and NB-feature enhanced
SVM. Kim and Zhai (2009) and Paltoglou and
Thelwall (2010) learn the feature weights by in-
vestigating variants weighting functions from In-
formation Retrieval. Nakagawa et al. (2010) uti-
lize dependency trees, polarity-shifting rules and
conditional random fields (Lafferty et al., 2001)
with hidden variables to compute the documen-
t feature. On Twitter, Mohammad et al. (2013b)
develop a state-of-the-art Twitter sentiment classi-
fier in SemEval 2013, using a variety of sentiment
lexicons and hand-crafted features.
With the revival of deep learning (representa-
tion learning (Hinton and Salakhutdinov, 2006;
Bengio et al., 2013; Jones, 2014)), more recen-
t studies focus on learning the low-dimensional,
dense and real-valued vector as text features for
sentiment classification. Glorot et al. (2011) inves-
tigate Stacked Denoising Autoencoders to learn
document vector for domain adaptation in sen-
timent classification. Yessenalina and Cardie
(2011) represent each word as a matrix and
compose words using iterated matrix multipli-
cation. Socher et al. propose Recursive Au-
toencoder (RAE) (2011), Matrix-Vector Recursive
Neural Network (MV-RNN) (2012) and Recur-
sive Neural Tensor Network (RNTN) (2013b) to
learn the composition of variable-length phrases
based on the representation of its children. To
learn the sentence representation, Kalchbrenner et
al. (2014) exploit Dynamic Convolutional Neu-
ral Network and Le and Mikolov (2014) inves-
tigate Paragraph Vector. To learn word vectors
for sentiment analysis, Maas et al. (2011) propose
a probabilistic document model following Blei et
al. (2003), Labutov and Lipson (2013) re-embed
words from existing word embeddings and Tang
et al. (2014b) develop three neural networks to
learn word vectors from tweets containing posi-
tive/negative emoticons.
Unlike most previous corpus-based algorithms
that build sentiment classifier based on splitting a
sentence as a word sequence, we produce sentence
segmentations automatically within a joint frame-
work, and conduct sentiment classification based
on the segmentation results.
3 The Proposed Approach
In this section, we first give the task definition
of two tasks, namely sentiment classification and
sentence segmentation. Then, we present the
overview of the proposed joint segmentation and
classification model (JSC) for sentiment analysis.
The segmentation candidate generation model and
the segmentation ranking model are described in
Section 4. The details of the sentiment classifica-
tion model are presented in Section 5.
3.1 Task Definition
The task of sentiment classification has been well
formalized in previous studies (Pang and Lee,
2008; Liu, 2012). The objective is to identify the
sentiment polarity of a sentence (or document) as
positive or negative
1
.
The task of sentence segmentation aims to s-
plit a sentence into a sequence of exclusive part-
s, each of which is a basic computational unit of
the sentence. An example is illustrated in Table 1.
The original text ?that is not bad? is segmented
as ?[that] [is] [not bad]?. The segmentation re-
sult is composed of three basic computational u-
nits, namely [that], [is] and [not bad].
Type Sample
Sentence that is not bad
Segmentation [that] [is] [not bad]
Basic units [that], [is], [not bad]
Table 1: Example for sentence segmentation.
3.2 Joint Model (JSC)
The overview of the proposed joint segmentation
and classification model (JSC) for sentiment anal-
ysis is illustrated in Figure 1. The intuitions of the
joint model are two-folds:
? The segmentation results have a strong influ-
ence on the sentiment classification perfor-
mance, since they are the inputs of the sen-
timent classification model.
? The usefulness of a segmentation can be
judged by whether the sentiment classifier
can use it to predict the correct sentence po-
larity.
Based on the mutual influence observation, we
formalize the joint model in Algorithm 1. The in-
puts contain two parts, training data and feature
extractors. Each sentence s
i
in the training data
1
In this paper, the sentiment polarity of a sentence is not
relevant to the target (or aspect) it contains (Hu and Liu, 2004;
Jiang et al., 2011; Mitchell et al., 2013).
479
Algorithm 1 The joint segmentation and classifi-
cation framework (JSC) for sentiment analysis
Input:
training data: T = [s
i
, pol
g
i
], 1 ? i ? |T |
segmentation feature extractor: sfe(?)
classification feature extractor: cfe(?)
Output:
sentiment classifier: SC
segmentation ranking model: SEG
1: Generate segmentation candidates ?
i
for each
sentence s
i
in T , 1 ? i ? |T |
2: Initialize sentiment classifier SC
(0)
based on
cfe(?
ij
), randomize j ? [1, |?
i
|], 1 ? i ?
|T |
3: Randomly initialize the segmentation ranking
model SEG
(0)
4: for r ? 1 ... R do
5: Predict the sentiment polarity pol
i
for ?
i
based on SC
(r?1)
and cfe(?
i?
)
6: Update the segmentation model SEG
(r)
with SEG
(r?1)
and [?
i
, sfe(?
i?
),
pol
i?
, pol
g
i
], 1 ? i ? |T |
7: for i? 1 ... |T | do
8: Calculate the segmentation score for ?
i?
based on SEG
(r)
and sfe(?
i?
)
9: Select the top-ranked K segmentation
candidates ?
i?
from ?
i
10: end for
11: Train the sentiment classifier SC
(r)
with
cfe(?
i?
), 1 ? i ? |T |
12: end for
13: SC? SC
(R)
14: SEG? SEG
(R)
T is annotated only with its gold sentiment po-
larity pol
g
i
, without any segmentation annotation-
s. There are two feature extractors for the task
of sentence segmentation (sfe(?)) and sentiment
classification (cfe(?)), respectively. The output-
s of the joint model are the segmentation ranking
model SEG and the sentiment classifier SC.
In Algorithm 1, we first generate segmentation
candidates ?
i
for each sentence s
i
in the training
set (line 1). Each ?
i
contains no less than one
segmentation candidates. We randomly select one
segmentation result from each ?
i
and utilize their
classification features to initialize the sentimen-
t classifier SC
(0)
(line 2). We randomly initialize
the segmentation model SEG
(0)
(line 3). Subse-
quently, we iteratively train the segmentation mod-
el SEG
(r)
and sentiment classifier SC
(r)
in a join-
t manner (line 4-12). At each iteration, we pre-
dict the sentiment polarity of each segmentation
candidate ?
i?
with the current sentiment classifi-
er SC
(r?1)
(line 5), and then leverage them to up-
date the segmentation model SEG
(r)
(line 6). Af-
terwards, we utilize the recently updated segmen-
tation ranking model SEG
(r)
to update the senti-
ment classifier SC
(r)
(line 7-11). We extract the
segmentation features for each segmentation can-
didate ?
i?
, and employ them to calculate the seg-
mentation score (line 8). The top-ranked K seg-
mentation results ?
i?
of each sentence s
i
is select-
ed (line 9), and further used to train the sentimen-
t classifier SC
(r)
(line 11). Finally, after training
R iterations, we dump the segmentation ranking
model SEG
(R)
and sentiment classifier SC
(R)
in
the last iteration as outputs (line 13-14).
At training time, we train the segmentation
model and classification model from sentences
with manually annotated sentiment polarity. At
prediction time, given a test sentence, we gener-
ate its segmentation candidates, and then calculate
segmentation score for each candidate. Afterward-
s, we select the top-ranked K candidates and vote
their predicted sentiment polarity from sentiment
classifier as the final result.
4 Segmentation Model
In this section, we present details of the segmenta-
tion candidate generation model (Section 4.1), the
segmentation ranking model (Section 4.2) and the
feature description for segmentation ranking mod-
el (Section 4.3).
4.1 Segmentation Candidate Generation
In this subsection, we describe the strategy to gen-
erate segmentation candidates for each sentence.
Since the segmentation results have an exponen-
tial search space in the number of words in a
sentence, we approximate the computation using
beam search with constrains on a phrase table,
which is induced from massive corpora.
Many studies have been previously proposed to
recognize phrases in the text. However, it is out
of scope of this work to compare them. We ex-
ploit a data-driven approach given by Mikolov et
al. (2013), which identifies phrases based on the
occurrence frequency of unigrams and bigrams,
freq(w
i
, w
j
) =
freq(w
i
, w
j
)? ?
freq(w
i
)? freq(w
j
)
(1)
480
where ? is a discounting coefficient that prevents
too many phrases consisting of very infrequen-
t words. We run 2-4 times over the corpora to get
longer phrases containing more words. We em-
pirically set ? as 10 in our experiment. We use
the default frequency threshold (value=5) in the
word2vec toolkit
2
to select bi-terms.
Given a sentence, we initialize the beam of each
index with the current word, and sequentially add
phrases into the beam if the new phrase is con-
tained in the phrase table. At each index of a sen-
tence, we rank the segmentation candidates by the
inverted number of items within a segmentation,
and save the top-ranked N segmentation candi-
dates into the beam. An example of the generated
segmentation candidates is given in Table 2.
Type Sample
Sentence that is not bad
Phrase Table [is not], [not bad], [is not bad]
Segmentations
[that] [is not bad]
[that] [is not] [bad]
[that] [is] [not bad]
[that] [is] [not] [bad]
Table 2: Example for segmentation candidate gen-
eration.
4.2 Segmentation Ranking Model
The objective of the segmentation ranking model
is to assign a scalar to each segmentation candi-
date, which indicates the usefulness of the seg-
mentation result for sentiment classification. In
this subsection, we describe a log-linear model to
calculate the segmentation score. To effectively
train the segmentation ranking model, we devise a
marginal log-likelihood as the optimization objec-
tive.
Given a segmentation candidate ?
ij
of the sen-
tence s
i
, we calculate the segmentation score
for ?
ij
with a log-linear model, as given in Equa-
tion 2.
?
ij
= exp(b+
?
k
sfe
ijk
? w
k
) (2)
where ?
ij
is the segmentation score of ?
ij
; sfe
ijk
is the k-th segmentation feature of ?
ij
; w and b are
the parameters of the segmentation ranking model.
During training, given a sentence s
i
and its gold
sentiment polarity pol
g
i
, the optimization objec-
2
Available at https://code.google.com/p/word2vec/
tive of the segmentation ranking model is to max-
imize the segmentation scores of the hit candi-
dates, whose predicted sentiment polarity equal-
s to the gold polarity of sentence pol
p
i
. The loss
function of the segmentation model is given in E-
quation 3.
loss = ?
|T |
?
i=1
log(
?
j?H
i
?
ij
?
j
?
?A
i
?
ij
?
) + ?||w||
2
2
(3)
where T is the training data; A
i
represents all the
segmentation candidates of sentence s
i
; H
i
mean-
s the hit candidates of s
i
; ? is the weight of the
L2-norm regularization factor. We train the seg-
mentation model with L-BFGS (Liu and Nocedal,
1989), running over the complete training data.
4.3 Feature
We design two kinds of features for sentence seg-
mentation, namely the phrase-embedding feature
and the segmentation-specific feature. The final
feature representation of each segmentation is the
concatenation of these two features. It is worth
noting that, the phrase-embedding feature is used
in both sentence segmentation and sentiment clas-
sification.
Segmentation-Specific Feature We empirically
design four segmentation-specific features to re-
flect the information of each segmentation, as list-
ed in Table 3.
Phrase-Embedding Feature We leverage
phrase embedding to generate the features of
segmentation candidates for both sentence seg-
mentation and sentiment classification. The
reason is that, in both tasks, the basic compu-
tational units of each segmentation candidate
might be words or phrases of variable length.
Under this scenario, phrase embedding is highly
suitable as it is capable to represent phrases with
different length into a consistent distributed vector
space (Mikolov et al., 2013). For each phrase,
phrase embedding is a dense, real-valued and
continuous vector. After the phrase embedding is
trained, the nearest neighbors in the embedding
space are favored to have similar grammatical us-
ages and semantic meanings. The effectiveness of
phrase embedding has been verified for building
large-scale sentiment lexicon (Tang et al., 2014a)
and machine translation (Zhang et al., 2014).
We learn phrase embedding with Skip-Gram
model (Mikolov et al., 2013), which is the state-of-
481
Feature Feature Description
#unit the number of basic computation units in the segmentation candidate
#unit / #word the ratio of units? number in a candidate to the length of original sentence
#word ? #unit the difference between sentence length and the number of basic computational units
#unit > 2 the number of basic component units composed of more than two words
Table 3: Segmentation-specific features for segmentation ranking.
Feature Feature Description
All-Caps the number of words with all characters in upper case
Emoticon the presence of positive (or negative) emoticons, whether the last unit is emoticon
Hashtag the number of hashtag
Elongated units the number of basic computational containing elongated words (with one character
repeated more than two times), such as gooood
Sentiment lexicon the number of sentiment words, the score of last sentiment words, the total sentiment
score and the maximal sentiment score for each lexicon
Negation the number of negations as individual units in a segmentation
Bag-of-Units an extension of bag-of-word for a segmentation
Punctuation the number of contiguous sequences of dot, question mark and exclamation mark.
Cluster the presence of units from each of the 1,000 clusters from Twitter NLP tool (Gimpel
et al., 2011)
Table 4: Classification-specific features for sentiment classification.
the-art phrase embedding learning algorithm. We
compose the representation (or feature) of a seg-
mentation candidate from the embedding of the
basic computational units (words or phrases) it
contains. In this paper, we explore min, max and
average convolution functions, which have been
used as simple and effective methods for composi-
tion learning in vector-based semantics (Mitchell
and Lapata, 2010; Collobert et al., 2011; Socher et
al., 2013a; Shen et al., 2014; Tang et al., 2014b),
to calculate the representation of a segmentation
candidate. The final phrase-embedding feature is
the concatenation of vectors derived from different
convolutional functions, as given in Equation 4,
pf(seg) = [pf
max
(seg), pf
min
(seg), pf
avg
(seg)]
(4)
where pf(seg) is the representation of the given
segmentation; pf
x
(seg) is the result of the con-
volutional function x ? {min,max, avg}. Each
convolutional function pf
x
(?) conducts the matrix-
vector operation of x on the sequence represented
by columns in the lookup table of phrase embed-
ding. The output of pf
x
(?) is calculated as
pf
x
(seg) = ?
x
?L
ph
?
seg
(5)
where ?
x
is the convolutional function of pf
x
;
?L
ph
?
seg
is the concatenated column vectors of
the basic computational units in the segmentation;
L
ph
is the lookup table of phrase embedding.
5 Classification Model
For sentiment classification, we follow the su-
pervised learning framework (Pang et al., 2002)
and build the classifier from sentences with man-
ually labelled sentiment polarity. We extend the
state-of-the-art hand-crafted features in SemEval
2013 (Mohammad et al., 2013b), and design the
classification-specific features for each segmenta-
tion. The detailed feature description is given in
Table 4.
6 Experiment
In this section, we conduct experiments to evaluate
the effectiveness of the joint model. We describe
the experiment settings and the result analysis.
6.1 Dataset and Experiment Settings
We conduct sentiment classification of tweets on a
benchmark Twitter sentiment classification dataset
in SemEval 2013. We run 2-class (positive vs neg-
ative) classification as sentence segmentation has a
great influence on the positive/negative polarity of
tweets due to the polarity inconsistency between a
phrase and its constitutes, such as ?not bad, bad?.
482
We leave 3-class classification (positive, negative,
neutral) and fine-grained classification (very neg-
ative, negative, neutral, positive, very positive) in
the future work.
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 5: Statistics of the SemEval 2013 Twitter
sentiment classification dataset (positive vs nega-
tive).
The statistics of our dataset crawled from Se-
mEval 2013 are given in Table 5. The evalua-
tion metric is the macro-F1 of sentiment classifi-
cation. We train the joint model on the training
set, tune parameters on the dev set and evaluate
on the test set. We train the sentiment classifier
with LibLinear (Fan et al., 2008) and utilize exist-
ing sentiment lexicons
3
to extract classification-
specific features. We randomly crawl 100M tweets
from February 1st, 2013 to April 30th, 2013 with
Twitter API, and use them to learn the phrase em-
bedding with Skip-Gram
4
. The vocabulary size
of the phrase embedding is 926K, from unigram
to 5-gram. The parameter -c in SVM is tuned on
the dev-set in both baseline and our method. We
run the L-BFGS for 50 iterations, and set the reg-
ularization factor ? as 0.003. The beam size N of
the candidate generation model and the top-ranked
segmentation number K are tuned on the dev-set.
6.2 Baseline Methods
We compare the proposed joint model with the fol-
lowing sentiment classification algorithms:
? DistSuper: We collect 10M balanced tweets
selected by positive and negative emoticons
5
as
training data, and build classifier using the Lib-
Linear and ngram features (Go et al., 2009; Zhao
et al., 2012).
? SVM: The n-gram features and Support Vec-
tor Machine are widely-used baseline methods to
build sentiment classifiers (Pang et al., 2002). We
use LibLinear to train the SVM classifier.
3
In this work, we use HL (Hu and Liu, 2004), M-
PQA (Wilson et al., 2005), NRC Emotion Lexicon (Moham-
mad and Turney, 2012), NRC Hashtag Lexicon and Senti-
ment140Lexicon (Mohammad et al., 2013b).
4
https://code.google.com/p/word2vec/
5
We use the emoticons selected by Hu et al. (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
? NBSVM: NBSVM (Wang and Manning,
2012) trades-off between Naive Bayes and NB-
features enhanced SVM. We use NBSVM-bi be-
cause it performs best on sentiment classification
of reviews.
? RAE: Recursive Autoencoder (Socher et al.,
2011) has been proven effective for sentiment clas-
sification by learning sentence representation. We
train the RAE using the pre-trained phrase embed-
ding learned from 100M tweets.
? SentiStrength: Thelwall et al. (2012) build a
lexicon-based classifier which uses linguistic rules
to detect the sentiment strength of tweets.
? SSWE
u
: Tang et al. (2014b) propose to learn
sentiment-specific word embedding (SSWE) from
10M tweets collected by emoticons. They apply
SSWE as features for Twitter sentiment classifica-
tion.
? NRC: NRC builds the state-of-the-art system
in SemEval 2013 Twitter Sentiment Classifica-
tion Track, incorporating diverse sentiment lexi-
cons and hand-crafted features (Mohammad et al.,
2013b). We re-implement this system because the
codes are not publicly available. We do not di-
rectly report their results in the evaluation task,
as our training and development sets are smaller
than their dataset. In NRC + PF, We concatenate
the NRC features and the phrase embeddings fea-
ture (PF), and build the sentiment classifier with
LibLinear.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al., 2013b) be-
cause the tweets in our dataset do not have accu-
rately parsed results. Another reason is that, due to
the differences between domains, the performance
of RNTN trained on movie reviews might be de-
creased if directly applied on the tweets (Xiao et
al., 2013).
6.3 Results and Analysis
Table 6 shows the macro-F1 of the baseline sys-
tems as well as our joint model (JSC) on senti-
ment classification of tweets (positive vs negative).
As is shown in Table 6, distant supervision is
relatively weak because the noisy-labeled tweets
are treated as the gold standard, which decreases
the performance of sentiment classifier. The result
of bag-of-unigram feature (74.50%) is not satisfied
as it losses the word order and does not well cap-
483
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + 5-gram 63.92
SVM + unigram 74.50
SVM + 5-gram 74.97
Recursive Autoencoder 75.42
NBSVM 75.28
SentiStrength 73.23
SSWE
u
84.98
NRC (Top System in SemEval 2013) 84.73
NRC + PF 84.75
JSC 85.51
Table 6: Macro-F1 for positive vs negative classi-
fication of tweets.
ture the semantic meaning of phrases. The integra-
tion of high-order n-ngram (up to 5-gram) does not
achieve significant improvement (+0.47%). The
reason is that, if a sentence contains a bigram ?not
bad?, they will use ?bad? and ?not bad? as par-
allel features, which confuses the sentiment clas-
sification model. NBSVM and Recursive Autoen-
coder perform comparatively and have a big gap
in comparison with JSC. In RAE, the representa-
tion of a sentence is composed from the represen-
tation of words it contains. Accordingly, ?great?
in ?a great deal of ? also contributes to the final
sentence representation via composition function.
JSC automatically conducts sentence segmenta-
tion by considering the sentiment polarity of sen-
tence, and utilize the phrasal information from the
segmentations. Ideally, JSC regards phrases like
?not bad? and ?a great deal of ? as basic compu-
tational units, and yields better classification per-
formance. JSC (85.51%) performs slightly better
than the state-of-the-art systems (SSWE
u
, 84.98%;
NRC+PF, 84.75%), which verifies its effective-
ness.
6.4 Comparing Joint and Pipelined Models
We compare the proposed joint model with
pipelined methods on Twitter sentiment classifi-
cation with different feature sets. Figure 2 gives
the experiment results. The tick [A, B] on x-
axis means the use of A as segmentation feature
and the use of B as classification feature. PF
represents the phrase-embedding feature; SF and
CF stand for the segmentation-specific feature and
classification-specific feature, respectively. We
use the bag-of-word segmentation result to build
sentiment classier in Pipeline 1, and use the seg-
mentation candidate with maximum phrase num-
ber in Pipeline 2.
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
[PF, PF] [PF+SF, PF] [PF, PF+CF] [PF+SF, PF+CF]
Pipeline 1Pipeline 2Joint
Figure 2: Macro-F1 for positive vs negative classi-
fication of tweets with joint and pipelined models.
From Figure 2, we find that the joint model
consistently outperforms pipelined baseline meth-
ods in all feature settings. The reason is that
the pipelined methods suffer from error propaga-
tion, since the errors from linguistic-driven and
bag-of-word segmentations cannot be corrected by
the sentiment classification model. Besides, tra-
ditional segmentors do not update the segmenta-
tion model with the sentiment information of tex-
t. Unlike pipelined methods, the joint model is
capable to address these problems by optimizing
the segmentation model with the classification re-
sults in a joint framework, which yields better
performance on sentiment classification. We also
find that Pipeline 2 always outperforms Pipeline
1, which indicates the usefulness of phrase-based
segmentation for sentiment classification.
6.5 Effect of the beam size N
We investigate the influence of beam size N ,
which is the maximum number of segmentation
candidates of a sentence. In this part, we clamp the
feature set as [PF+SF, PF+CF], and vary the beam
size N in [1,2,4,8,16,32,64]. The experiment re-
sults of macro-F1 on the development set are il-
lustrated in Figure 3 (a). The time cost of each
training iteration is given in Figure 3 (b).
From Figure 3 (a), we can see that when larg-
er beam size is considered, the classification per-
formance is improved. When beam size is 1, the
model stands for the greedy search with the bag-
of-words segmentation. When the beam size is s-
mall, such as 2, beam search losses many phrasal
information of sentences and thus the improve-
ment is not significant. The performance remains
steady when beam size is larger than 16. From
484
1 2 4 8 16 32 640.81
0.82
0.83
0.84
0.85
0.86
Beam Size
Mac
ro?F
1
(a) Macro-F1 score for senti-
ment classification.
1 2 4 8 16 32 640
20
40
60
80
100
120
Beam Size
Run
time
 (Sec
ond)
 
 
(b) Time cost (seconds) of
each training iteration.
Figure 3: Sentiment classification of tweets with
different beam size N .
Figure 3 (b), we can find that the runtime of each
training iteration increases with larger beam size.
It is intuitive as the joint model with larger beam
considers more segmentation results, which in-
creases the training time of the segmentation mod-
el. We set beam size as 16 after parameter learn-
ing.
6.6 Effect of the top-ranked segmentation
number K
We investigate how the top-ranked segmentation
number K affects the performance of sentimen-
t classification. In this part, we set the feature as
[PF+SF, PF+CF], and the beam size as 16. The
results of macro-F1 on the development set are il-
lustrated in Figure 4.
1 3 5 7 9 11 13 150.82
0.83
0.84
0.85
0.86
Top?ranked candidate number
Ma
cro
?F1
Figure 4: Sentiment classification of tweets with
different top-ranked segmentation number K.
From Figure 4, we find that the classification
performance increases with K being larger. The
reason is that when a larger K is used, (1) at train-
ing time, the sentiment classifier is built by using
more phrasal information from multiple segmen-
tations, which benefits from the ensembles; (2) at
test time, the joint model considers several top-
ranked segmentations and get the final sentiment
polarity through voting. The performance remain-
s stable when K is larger than 7, as the phrasal
information has been mostly covered.
7 Conclusion
In this paper, we develop a joint segmentation
and classification framework (JSC) for sentiment
analysis. Unlike existing sentiment classification
algorithms that build sentiment classifier based
on the segmentation results from bag-of-words or
separate segmentors, the proposed joint model si-
multaneously conducts sentence segmentation and
sentiment classification. We introduce a marginal
log-likelihood function to optimize the segmenta-
tion model, and effectively train the joint mod-
el from sentences annotated only with sentiment
polarity, without segmentation annotations of sen-
tences. The effectiveness of the joint model has
been verified by applying it on the benchmark
dataset of Twitter sentiment classification in Se-
mEval 2013. Results show that, the joint model
performs comparably with state-of-the-art meth-
ods, and outperforms pipelined methods in various
settings. In the future, we plan to apply the join-
t model on other domains, such as movie/product
reviews.
Acknowledgements
We thank Nan Yang, Yajuan Duan, Yaming
Sun and Meishan Zhang for their helpful dis-
cussions. We thank the anonymous reviewers
for their insightful comments and feedbacks on
this work. This research was partly supported
by National Natural Science Foundation of Chi-
na (No.61133012, No.61273321, No.61300113).
The contact author of this paper, according to the
meaning given to this role by Harbin Institute of
Technology, is Bing Qin.
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 793?801.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
485
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics.
Nicola Jones. 2014. Computer science: The learning
machines. Nature, 505(7482):146.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A sentence model based on convolu-
tional neural networks. In Procedding of the 52th
Annual Meeting of Association for Computational
Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of CIKM 2009. ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of international con-
ference on Machine learning. ACM.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. Proceedings
of International Conference on Machine Learning.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. Conference on Neural Information Processing
Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,
and Benjamin Van Durme. 2013. Open domain tar-
geted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654.
Saif M Mohammad and Peter D Turney. 2012. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence.
Saif M Mohammad, Bonnie J Dorr, Graeme Hirst, and
Peter D Turney. 2013a. Computing lexical contrast.
Computational Linguistics, 39(3):555?590.
486
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013b. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 786?794.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Georgios Paltoglou and Mike Thelwall. 2010. A s-
tudy of information retrieval weighting schemes for
sentiment analysis. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1386?1395.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014. Learning semantic rep-
resentations using convolutional neural networks for
web search. In Proceedings of the companion publi-
cation of the 23rd international conference on World
wide web companion, pages 373?374.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Y Ng. 2013a. Reasoning with neu-
ral tensor networks for knowledge base completion.
The Conference on Neural Information Processing
Systems.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and
Ting Liu. 2014a. Building large-scale twitter-
specific sentiment lexicon : A representation learn-
ing approach. In Proceedings of COLING 2014,
the 25th International Conference on Computation-
al Linguistics, pages 172?182.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014b. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 152?
162, October.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 111?121.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
487
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 359?367,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Recognizing Named Entities in Tweets
Xiaohua Liu ? ?, Shaodian Zhang? ?, Furu Wei ?, Ming Zhou ?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, 200240, China
?Microsoft Research Asia
Beijing, 100190, China
?{xiaoliu, fuwei, mingzhou}@microsoft.com
? zhangsd.sjtu@gmail.com
Abstract
The challenges of Named Entities Recogni-
tion (NER) for tweets lie in the insufficient
information in a tweet and the unavailabil-
ity of training data. We propose to com-
bine a K-Nearest Neighbors (KNN) classi-
fier with a linear Conditional Random Fields
(CRF) model under a semi-supervised learn-
ing framework to tackle these challenges. The
KNN based classifier conducts pre-labeling to
collect global coarse evidence across tweets
while the CRF model conducts sequential la-
beling to capture fine-grained information en-
coded in a tweet. The semi-supervised learn-
ing plus the gazetteers alleviate the lack of
training data. Extensive experiments show the
advantages of our method over the baselines
as well as the effectiveness of KNN and semi-
supervised learning.
1 Introduction
Named Entities Recognition (NER) is generally un-
derstood as the task of identifying mentions of rigid
designators from text belonging to named-entity
types such as persons, organizations and locations
(Nadeau and Sekine, 2007). Proposed solutions to
NER fall into three categories: 1) The rule-based
(Krupka and Hausman, 1998); 2) the machine learn-
ing based (Finkel and Manning, 2009; Singh et al,
2010) ; and 3) hybrid methods (Jansche and Abney,
2002). With the availability of annotated corpora,
such as ACE05, Enron (Minkov et al, 2005) and
? This work has been done while the author was visiting
Microsoft Research Asia.
CoNLL03 (Tjong Kim Sang and DeMeulder, 2003),
the data driven methods now become the dominating
methods.
However, current NER mainly focuses on for-
mal text such as news articles (Mccallum and Li,
2003; Etzioni et al, 2005). Exceptions include stud-
ies on informal text such as emails, blogs, clini-
cal notes (Wang, 2009). Because of the domain
mismatch, current systems trained on non-tweets
perform poorly on tweets, a new genre of text,
which are short, informal, ungrammatical and noise
prone. For example, the average F1 of the Stan-
ford NER (Finkel et al, 2005) , which is trained
on the CoNLL03 shared task data set and achieves
state-of-the-art performance on that task, drops from
90.8% (Ratinov and Roth, 2009) to 45.8% on tweets.
Thus, building a domain specific NER for tweets
is necessary, which requires a lot of annotated tweets
or rules. However, manually creating them is tedious
and prohibitively unaffordable. Proposed solutions
to alleviate this issue include: 1) Domain adaption,
which aims to reuse the knowledge of the source do-
main in a target domain. Two recent examples are
Wu et al (2009), which uses data that is informa-
tive about the target domain and also easy to be la-
beled to bridge the two domains, and Chiticariu et
al. (2010), which introduces a high-level rule lan-
guage, called NERL, to build the general and do-
main specific NER systems; and 2) semi-supervised
learning, which aims to use the abundant unlabeled
data to compensate for the lack of annotated data.
Suzuki and Isozaki (2008) is one such example.
Another challenge is the limited information in
tweet. Two factors contribute to this difficulty. One
359
is the tweet?s informal nature, making conventional
features such as part-of-speech (POS) and capital-
ization not reliable. The performance of current
NLP tools drops sharply on tweets. For example,
OpenNLP 1, the state-of-the-art POS tagger, gets
only an accuracy of 74.0% on our test data set. The
other is the tweet?s short nature, leading to the ex-
cessive abbreviations or shorthand in tweets, and
the availability of very limited context information.
Tackling this challenge, ideally, requires adapting
related NLP tools to fit tweets, or normalizing tweets
to accommodate existing tools, both of which are
hard tasks.
We propose a novel NER system to address these
challenges. Firstly, a K-Nearest Neighbors (KNN)
based classifier is adopted to conduct word level
classification, leveraging the similar and recently
labeled tweets. Following the two-stage predic-
tion aggregation methods (Krishnan and Manning,
2006), such pre-labeled results, together with other
conventional features used by the state-of-the-art
NER systems, are fed into a linear Conditional Ran-
dom Fields (CRF) (Lafferty et al, 2001) model,
which conducts fine-grained tweet level NER. Fur-
thermore, the KNN and CRF model are repeat-
edly retrained with an incrementally augmented
training set, into which high confidently labeled
tweets are added. Indeed, it is the combination of
KNN and CRF under a semi-supervised learning
framework that differentiates ours from the exist-
ing. Finally, following Lev Ratinov and Dan Roth
(2009), 30 gazetteers are used, which cover com-
mon names, countries, locations, temporal expres-
sions, etc. These gazetteers represent general knowl-
edge across domains. The underlying idea of our
method is to combine global evidence from KNN
and the gazetteers with local contextual information,
and to use common knowledge and unlabeled tweets
to make up for the lack of training data.
12,245 tweets are manually annotated as the test
data set. Experimental results show that our method
outperforms the baselines. It is also demonstrated
that integrating KNN classified results into the CRF
model and semi-supervised learning considerably
boost the performance.
Our contributions are summarized as follows.
1http://sourceforge.net/projects/opennlp/
1. We propose to a novel method that combines
a KNN classifier with a conventional CRF
based labeler under a semi-supervised learning
framework to combat the lack of information in
tweet and the unavailability of training data.
2. We evaluate our method on a human anno-
tated data set, and show that our method outper-
forms the baselines and that both the combina-
tion with KNN and the semi-supervised learn-
ing strategy are effective.
The rest of our paper is organized as follows. In
the next section, we introduce related work. In Sec-
tion 3, we formally define the task and present the
challenges. In Section 4, we detail our method. In
Section 5, we evaluate our method. Finally, Section
6 concludes our work.
2 Related Work
Related work can be roughly divided into three cat-
egories: NER on tweets, NER on non-tweets (e.g.,
news, bio-logical medicine, and clinical notes), and
semi-supervised learning for NER.
2.1 NER on Tweets
Finin et al (2010) use Amazons Mechanical Turk
service 2 and CrowdFlower 3 to annotate named en-
tities in tweets and train a CRF model to evaluate
the effectiveness of human labeling. In contrast, our
work aims to build a system that can automatically
identify named entities in tweets. To achieve this,
a KNN classifier with a CRF model is combined
to leverage cross tweets information, and the semi-
supervised learning is adopted to leverage unlabeled
tweets.
2.2 NER on Non-Tweets
NER has been extensively studied on formal text,
such as news, and various approaches have been pro-
posed. For example, Krupka and Hausman (1998)
use manual rules to extract entities of predefined
types; Zhou and Ju (2002) adopt Hidden Markov
Models (HMM) while Finkel et al (2005) use CRF
to train a sequential NE labeler, in which the BIO
(meaning Beginning, the Inside and the Outside of
2https://www.mturk.com/mturk/
3http://crowdflower.com/
360
an entity, respectively) schema is applied. Other
methods, such as classification based on Maximum
Entropy models and sequential application of Per-
ceptron or Winnow (Collins, 2002), are also prac-
ticed. The state-of-the-art system, e.g., the Stanford
NER, can achieve an F1 score of over 92.0% on its
test set.
Biomedical NER represents another line of active
research. Machine learning based systems are com-
monly used and outperform the rule based systems.
A state-of-the-art biomedical NER system (Yoshida
and Tsujii, 2007) uses lexical features, orthographic
features, semantic features and syntactic features,
such as part-of-speech (POS) and shallow parsing.
A handful of work on other domains exists. For
example, Wang (2009) introduces NER on clinical
notes. A data set is manually annotated and a linear
CRF model is trained, which achieves an F-score of
81.48% on their test data set; Downey et al (2007)
employ capitalization cues and n-gram statistics to
locate names of a variety of classes in web text;
most recently, Chiticariu et al (2010) design and im-
plement a high-level language NERL that is tuned
to simplify the process of building, understanding,
and customizing complex rule-based named-entity
annotators for different domains.
Ratinov and Roth (2009) systematically study
the challenges in NER, compare several solutions
and report some interesting findings. For exam-
ple, they show that a conditional model that does
not consider interactions at the output level per-
forms comparably to beam search or Viterbi, and
that the BILOU (Beginning, the Inside and the Last
tokens of multi-token chunks as well as Unit-length
chunks) encoding scheme significantly outperforms
the BIO schema (Beginning, the Inside and Outside
of a chunk).
In contrast to the above work, our study focuses
on NER for tweets, a new genre of texts, which are
short, noise prone and ungrammatical.
2.3 Semi-supervised Learning for NER
Semi-supervised learning exploits both labeled and
un-labeled data. It proves useful when labeled data
is scarce and hard to construct while unlabeled data
is abundant and easy to access.
Bootstrapping is a typical semi-supervised learn-
ing method. It iteratively adds data that has been
confidently labeled but is also informative to its
training set, which is used to re-train its model. Jiang
and Zhai (2007) propose a balanced bootstrapping
algorithm and successfully apply it to NER. Their
method is based on instance re-weighting, which
allows the small amount of the bootstrapped train-
ing sets to have an equal weight to the large source
domain training set. Wu et al (2009) propose an-
other bootstrapping algorithm that selects bridging
instances from an unlabeled target domain, which
are informative about the target domain and are also
easy to be correctly labeled. We adopt bootstrapping
as well, but use human labeled tweets as seeds.
Another representative of semi-supervised learn-
ing is learning a robust representation of the input
from unlabeled data. Miller et al (2004) use word
clusters (Brown et al, 1992) learned from unla-
beled text, resulting in a performance improvement
of NER. Guo et al (2009) introduce Latent Seman-
tic Association (LSA) for NER. In our pilot study of
NER for tweets, we adopt bag-of-words models to
represent a word in tweet, to concentrate our efforts
on combining global evidence with local informa-
tion and semi-supervised learning. We leave it to
our future work to explore which is the best input
representation for our task.
3 Task Definition
We first introduce some background about tweets,
then give a formal definition of the task.
3.1 The Tweets
A tweet is a short text message containing no
more than 140 characters in Twitter, the biggest
micro-blog service. Here is an example of
tweets: ?mycraftingworld: #Win Microsoft Of-
fice 2010 Home and Student *2Winners* #Con-
test from @office and @momtobedby8 #Giveaway
http://bit.ly/bCsLOr ends 11/14?, where ?mycraft-
ingworld? is the name of the user who published
this tweet. Words beginning with the ?#? char-
acter, like ??#Win?, ?#Contest? and ?#Giveaway?,
are hash tags, usually indicating the topics of the
tweet; words starting with ?@?, like ?@office?
and ?@momtobedby8?, represent user names, and
?http://bit.ly/bCsLOr? is a shortened link.
Twitter users are interested in named entities, such
361
Figure 1: Portion of different types of named entities in
tweets. This is based on an investigation of 12,245 ran-
domly sampled tweets, which are manually labeled.
as person names, organization names and product
names, as evidenced by the abundant named entities
in tweets. According to our investigation on 12,245
randomly sampled tweets that are manually labeled,
about 46.8% have at least one named entity. Figure
1 shows the portion of named entities of different
types.
3.2 The Task
Given a tweet as input, our task is to identify both the
boundary and the class of each mention of entities of
predefined types. We focus on four types of entities
in our study, i.e., persons, organizations, products,
and locations, which, according to our investigation
as shown in Figure 1, account for 89.0% of all the
named entities.
Here is an example illustrating our task.
The input is ?...Me without you is like an
iphone without apps, Justin Bieber without
his hair, Lady gaga without her telephone, it
just wouldn...? The expected output is as fol-
lows:?...Me without you is like an <PRODUCT
>iphone</PRODUCT>without apps,
<PERSON>Justin Bieber</PERSON>without his
hair,<PERSON>Lady gaga</PERSON> without
her telephone, it just wouldn...?, meaning that
?iphone? is a product, while ?Justin Bieber? and
?Lady gaga? are persons.
4 Our Method
Now we present our solution to the challenging task
of NER for tweets. An overview of our method
is first given, followed by detailed discussion of its
core components.
4.1 Method Overview
NER task can be naturally divided into two sub-
tasks, i.e., boundary detection and type classifica-
tion. Following the common practice , we adopt
a sequential labeling approach to jointly resolve
these sub-tasks, i.e., for each word in the input
tweet, a label is assigned to it, indicating both the
boundary and entity type. Inspired by Ratinov and
Roth (2009), we use the BILOU schema.
Algorithm 1 outlines our method, where: trains
and traink denote two machine learning processes
to get the CRF labeler and the KNN classifier, re-
spectively; reprw converts a word in a tweet into a
bag-of-words vector; the reprt function transforms
a tweet into a feature matrix that is later fed into the
CRF model; the knn function predicts the class of
a word; the update function applies the predicted
class by KNN to the inputted tweet; the crf function
conducts word level NE labeling;? and ? represent
the minimum labeling confidence of KNN and CRF,
respectively, which are experimentally set to 0.1 and
0.001; N (1,000 in our work) denotes the maximum
number of new accumulated training data.
Our method, as illustrated in Algorithm 1, repeat-
edly adds the new confidently labeled tweets to the
training set 4 and retrains itself once the number
of new accumulated training data goes above the
threshold N . Algorithm 1 also demonstrates one
striking characteristic of our method: A KNN clas-
sifier is applied to determine the label of the current
word before the CRF model. The labels of the words
that confidently assigned by the KNN classifier are
treated as visible variables for the CRF model.
4.2 Model
Our model is hybrid in the sense that a KNN clas-
sifier and a CRF model are sequentially applied to
the target tweet, with the goal that the KNN classi-
fier captures global coarse evidence while the CRF
model fine-grained information encoded in a single
tweet and in the gazetteers. Algorithm 2 outlines the
training process of KNN, which records the labeled
word vector for every type of label.
Algorithm 3 describes how the KNN classifier
4The training set ts has a maximum allowable number of
items, which is 10,000 in our work. Adding an item into it will
cause the oldest one being removed if it is full.
362
Algorithm 1 NER for Tweets.
Require: Tweet stream i; output stream o.
Require: Training tweets ts; gazetteers ga.
1: Initialize ls, the CRF labeler: ls = trains(ts).
2: Initialize lk, the KNN classifier: lk = traink(ts).
3: Initialize n, the # of new training tweets: n = 0.
4: while Pop a tweet t from i and t ?= null do
5: for Each word w ? t do
6: Get the feature vector w?: w? =
reprw(w, t).
7: Classify w? with knn: (c, cf) =
knn(lk, w?).
8: if cf > ? then
9: Pre-label: t = update(t, w, c).
10: end if
11: end for
12: Get the feature vector t?: t? = reprt(t, ga).
13: Label t? with crf : (t, cf) = crf(ls, t?).
14: Put labeled result (t, cf) into o.
15: if cf > ? then
16: Add labeled result t to ts , n = n + 1.
17: end if
18: if n > N then
19: Retrain ls: ls = trains(ts).
20: Retrain lk: lk = traink(ts).
21: n = 0.
22: end if
23: end while
24: return o.
Algorithm 2 KNN Training.
Require: Training tweets ts.
1: Initialize the classifier lk:lk = ?.
2: for Each tweet t ? ts do
3: for Each word,label pair (w, c) ? t do
4: Get the feature vector w?: w? =
reprw(w, t).
5: Add the w? and c pair to the classifier: lk =
lk ? {(w?, c)}.
6: end for
7: end for
8: return KNN classifier lk.
predicts the label of the word. In our work, K is
experimentally set to 20, which yields the best per-
formance.
Two desirable properties of KNN make it stand
out from its alternatives: 1) It can straightforwardly
incorporate evidence from new labeled tweets and
retraining is fast; and 2) combining with a CRF
Algorithm 3 KNN predication.
Require: KNN classifier lk ;word vector w?.
1: Initialize nb, the neighbors of w?: nb =
neigbors(lk, w?).
2: Calculate the predicted class c?: c? =
argmaxc
?
(w?? ,c? )?nb ?(c, c
?) ? cos(w?, w??).
3: Calculate the labeling confidence cf : cf =
?
(w?? ,c? )?nb ?(c,c
?
)?cos(w?,w?
?
)
?
(w?? ,c? )?nb cos(w?,w?
? ) .
4: return The predicted label c? and its confidence cf .
model, which is good at encoding the subtle interac-
tions between words and their labels, compensates
for KNN?s incapability to capture fine-grained evi-
dence involving multiple decision points.
The Linear CRF model is used as the fine model,
with the following considerations: 1) It is well-
studied and has been successfully used in state-of-
the-art NER systems (Finkel et al, 2005; Wang,
2009); 2) it can output the probability of a label
sequence, which can be used as the labeling con-
fidence that is necessary for the semi-supervised
learning framework.
In our experiments, the CRF++ 5 toolkit is used to
train a linear CRF model. We have written a Viterbi
decoder that can incorporate partially observed la-
bels to implement the crf function in Algorithm 1.
4.3 Features
Given a word in a tweet, the KNN classifier consid-
ers a text window of size 5 with the word in the mid-
dle (Zhang and Johnson, 2003), and extracts bag-of-
word features from the window as features. For each
word, our CRF model extracts similar features as
Wang (2009) and Ratinov and Roth (2009), namely,
orthographic features, lexical features and gazetteers
related features. In our work, we use the gazetteers
provided by Ratinov and Roth (2009).
Two points are worth noting here. One is that
before feature extraction for either the KNN or the
CRF, stop words are removed. The stop words
used here are mainly from a set of frequently-used
words 6. The other is that tweet meta data is normal-
ized, that is, every link becomes *LINK* and every
5http://crfpp.sourceforge.net/
6http://www.textfixer.com/resources/common-english-
words.txt
363
account name becomes *ACCOUNT*. Hash tags
are treated as common words.
4.4 Discussion
We now discuss several design considerations re-
lated to the performance of our method, i.e., addi-
tional features, gazetteers and alternative models.
Additional Features. Features related to chunking
and parsing are not adopted in our final system, be-
cause they give only a slight performance improve-
ment while a lot of computing resources are required
to extract such features. The ineffectiveness of these
features is linked to the noisy and informal nature of
tweets. Word class (Brown et al, 1992) features are
not used either, which prove to be unhelpful for our
system. We are interested in exploring other tweet
representations, which may fit our NER task, for ex-
ample the LSA models (Guo et al, 2009).
Gazetteers. In our work, gazetteers prove to be sub-
stantially useful, which is consistent with the obser-
vation of Ratinov and Roth (2009). However, the
gazetteers used in our work contain noise, which
hurts the performance. Moreover, they are static,
directly from Ratinov and Roth (2009), thus with
a relatively lower coverage, especially for person
names and product names in tweets. We are devel-
oping tools to clean the gazetteers. In future, we plan
to feed the fresh entities correctly identified from
tweets back into the gazetteers. The correctness of
an entity can rely on its frequency or other evidence.
Alternative Models. We have replaced KNN by
other classifiers, such as those based on Maximum
Entropy and Support Vector Machines, respectively.
KNN consistently yields comparable performance,
while enjoying a faster retraining speed. Similarly,
to study the effectiveness of the CRF model, it is re-
placed by its alternations, such as the HMM labeler
and a beam search plus a maximum entropy based
classifier. In contrast to what is reported by Ratinov
and Roth (2009), it turns out that the CRF model
gives remarkably better results than its competitors.
Note that all these evaluations are on the same train-
ing and testing data sets as described in Section 5.1.
5 Experiments
In this section, we evaluate our method on a man-
ually annotated data set and show that our system
outperforms the baselines. The contributions of the
combination of KNN and CRF as well as the semi-
supervised learning are studied, respectively.
5.1 Data Preparation
We use the Twigg SDK 7 to crawl all tweets
from April 20th 2010 to April 25th 2010, then drop
non-English tweets and get about 11,371,389, from
which 15,800 tweets are randomly sampled, and are
then labeled by two independent annotators, so that
the beginning and the end of each named entity are
marked with<TYPE> and</TYPE>, respectively.
Here TYPE is PERSON, PRODUCT, ORGANIZA-
TION or LOCATION. 3555 tweets are dropped be-
cause of inconsistent annotation. Finally we get
12,245 tweets, forming the gold-standard data set.
Figure 1 shows the portion of named entities of dif-
ferent types. On average, a named entity has 1.2
words. The gold-standard data set is evenly split into
two parts: One for training and the other for testing.
5.2 Evaluation Metrics
For every type of named entity, Precision (Pre.), re-
call (Rec.) and F1 are used as the evaluation met-
rics. Precision is a measure of what percentage the
output labels are correct, and recall tells us to what
percentage the labels in the gold-standard data set
are correctly labeled, while F1 is the harmonic mean
of precision and recall. For the overall performance,
we use the average Precision, Recall and F1, where
the weight of each name entity type is proportional
to the number of entities of that type. These metrics
are widely used by existing NER systems to evaluate
their performance.
5.3 Baselines
Two systems are used as baselines: One is the
dictionary look-up system based on the gazetteers;
the other is the modified version of our system
without KNN and semi-supervised learning. Here-
after these two baselines are called NERDIC and
NERBA, respectively. The OpenNLP and the Stan-
ford parser (Klein and Manning, 2003) are used to
extract linguistic features for the baselines and our
method.
7It is developed by the Bing social search team, and cur-
rently is only internally available.
364
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERBA 83.6 68.6 75.4
NERDIC 32.6 25.4 28.6
Table 1: Overall experimental results.
System Pre.(%) Rec.(%) F1(%)
NERCB 78.4 74.5 76.4
NERBA 83.6 68.4 75.2
NERDIC 37.1 29.7 33.0
Table 2: Experimental results on PERSON.
5.4 Basic Results
Table 1 shows the overall results for the baselines
and ours with the name NERCB . Here our sys-
tem is trained as described in Algorithm 1, combin-
ing a KNN classifier and a CRF labeler, with semi-
supervised learning enabled. As can be seen from
Table 1, on the whole, our method significantly out-
performs (with p < 0.001) the baselines. Tables 2-5
report the results on each entity type, indicating that
our method consistently yields better results on all
entity types.
5.5 Effects of KNN Classifier
Table 6 shows the performance of our method
without combining the KNN classifier, denoted by
NERCB?KNN . A drop in performance is observed
then. We further check the confidently predicted la-
bels of the KNN classifier, which account for about
22.2% of all predications, and find that its F1 is as
high as 80.2% while the baseline system based on
the CRF model achieves only an F1 of 75.4%. This
largely explains why the KNN classifier helps the
CRF labeler. The KNN classifier is replaced with
its competitors, and only a slight difference in per-
formance is observed. We do observe that retraining
KNN is obviously faster.
System Pre.(%) Rec.(%) F1(%)
NERCB 81.3 65.4 72.5
NERBA 82.5 58.4 68.4
NERDIC 8.2 6.1 7.0
Table 3: Experimental results on PRODUCT.
System Pre.(%) Rec.(%) F1(%)
NERCB 80.3 77.5 78.9
NERBA 81.6 69.7 75.2
NERDIC 30.2 30.0 30.1
Table 4: Experimental results on LOCATION.
System Pre.(%) Rec.(%) F1(%)
NERCB 83.2 60.4 70.0
NERBA 87.6 52.5 65.7
NERDIC 54.5 11.8 19.4
Table 5: Experimental results on ORGANIZATION.
5.6 Effects of the CRF Labeler
Similarly, the CRF model is replaced by its alterna-
tives. As is opposite to the finding of Ratinov and
Roth (2009), the CRF model gives remarkably bet-
ter results, i.e., 2.1% higher in F1 than its best fol-
lowers (with p < 0.001). Table 7 shows the overall
performance of the CRF labeler with various feature
set combinations, where Fo, Fl and Fg denote the
orthographic features, the lexical features and the
gazetteers related features, respectively. It can be
seen from Table 7 that the lexical and gazetteer re-
lated features are helpful. Other advanced features
such as chunking are also explored but with no sig-
nificant improvement.
5.7 Effects of Semi-supervised Learning
Table 8 compares our method with its modified ver-
sion without semi-supervised learning, suggesting
that semi-supervised learning considerably boosts
the performance. To get more details about self-
training, we evenly divide the test data into 10 parts
and feed them into our method sequentially; we
record the average F1 score on each part, as shown
in Figure 2.
5.8 Error Analysis
Errors made by our system on the test set fall into
three categories. The first kind of error, accounting
for 35.5% of all errors, is largely related to slang ex-
pressions and informal abbreviations. For example,
our method identifies ?Cali?, which actually means
?California?, as a PERSON in the tweet ?i love Cali
so much?. In future, we can design a normalization
365
System Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NERCB?KNN 82.6 74.8 78.5
Table 6: Overall performance of our system with and
without the KNN classifier, respectively.
Features Pre.(%) Rec.(%) F1(%)
Fo 71.3 42.8 53.5
Fo + Fl 76.2 44.2 55.9
Fo + Fg 80.5 66.2 72.7
Fo + Fl + Fg 82.6 74.8 78.5
Table 7: Overview performance of the CRF labeler (com-
bined with KNN) with different feature sets.
component to handle such slang expressions and in-
formal abbreviations.
The second kind of error, accounting for 37.2%
of all errors, is mainly attributed to the data sparse-
ness. For example, for this tweet ?come to see jaxon
someday?, our method mistakenly labels ?jaxon?
as a LOCATION, which actually denotes a PER-
SON. This error is understandable somehow, since
this tweet is one of the earliest tweets that mention
?jaxon?, and at that time there was no strong evi-
dence supporting that it represents a person. Possi-
ble solutions to these errors include continually en-
riching the gazetteers and aggregating additional ex-
ternal knowledge from other channels such as tradi-
tional news.
The last kind of error, which represents 27.3%
of all errors, somehow links to the noise prone na-
ture of tweets. Consider this tweet ?wesley snipes
ws cought 4 nt payin tax coz ths celebz dnt take it
cirus.?, in which ?wesley snipes? is not identified
as a PERSON but simply ignored by our method,
because this tweet is too noisy to provide effective
features. Tweet normalization technology seems a
possible solution to alleviate this kind of error.
Features Pre.(%) Rec.(%) F1(%)
NERCB 81.6 78.8 80.2
NER?CB 82.1 71.9 76.7
Table 8: Performance of our system with and without
semi-supervised learning, respectively.
Figure 2: F1 score on 10 test data sets sequentially fed
into the system, each with 600 instances. Horizontal and
vertical axes represent the sequential number of the test
data set and the averaged F1 score (%), respectively.
6 Conclusions and Future work
We propose a novel NER system for tweets, which
combines a KNN classifier with a CRF labeler under
a semi-supervised learning framework. The KNN
classifier collects global information across recently
labeled tweets while the CRF labeler exploits infor-
mation from a single tweet and from the gazetteers.
A serials of experiments show the effectiveness of
our method, and particularly, show the positive ef-
fects of KNN and semi-supervised learning.
In future, we plan to further improve the per-
formance of our method through two directions.
Firstly, we hope to develop tweet normalization
technology to make tweets friendlier to the NER
task. Secondly, we are interested in integrating
new entities from tweets or other channels into the
gazetteers.
Acknowledgments
We thank Long Jiang, Changning Huang, Yunbo
Cao, Dongdong Zhang, Zaiqing Nie for helpful dis-
cussions, and the anonymous reviewers for their
valuable comments. We also thank Matt Callcut for
his careful proofreading of an early draft of this pa-
per.
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18:467?479.
366
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP, pages
1002?1012.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating Complex Named Entities in Web Text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In CSLDAMT, pages 80?88.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141?150.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL, pages 363?370.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL, pages 281?289.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320?327.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL, pages 264?
271.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In ACL, pages
1121?1128.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
John D. Lafferty, AndrewMcCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Andrew Mccallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In HLT-NAACL, pages 188?191.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In HLT-NAACL, pages 337?342.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In HLT,
pages 443?450.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, 30:3?26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147?155.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73?81.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In ACL, pages 665?673.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL, pages 142?147.
Yefeng Wang. 2009. Annotating and recognising named
entities in clinical notes. In ACL-IJCNLP, pages 18?
26.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In EMNLP, pages 1523?1532.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In BioNLP,
pages 209?216.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In HLT-NAACL, pages 204?207.
GuoDong Zhou and Jian Su. 2002. Named entity recog-
nition using an hmm-based chunk tagger. In ACL,
pages 473?480.
367
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526?535,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Inference of Named Entity Recognition and Normalization for Tweets
Xiaohua Liu ? ?, Ming Zhou ?, Furu Wei ?, Zhongyang Fu ?, Xiangyang Zhou ?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?Department of Computer Science and Engineering
Shanghai Jiao Tong University, Shanghai, 200240, China
?School of Computer Science and Technology
Shandong University, Jinan, 250100, China
?Microsoft Research Asia
Beijing, 100190, China
?{xiaoliu, fuwei, mingzhou}@microsoft.com
? zhongyang.fu@gmail.com ? v-xzho@microsoft.com
Abstract
Tweets represent a critical source of fresh in-
formation, in which named entities occur fre-
quently with rich variations. We study the
problem of named entity normalization (NEN)
for tweets. Two main challenges are the er-
rors propagated from named entity recogni-
tion (NER) and the dearth of information in
a single tweet. We propose a novel graphi-
cal model to simultaneously conduct NER and
NEN on multiple tweets to address these chal-
lenges. Particularly, our model introduces a
binary random variable for each pair of words
with the same lemma across similar tweets,
whose value indicates whether the two related
words are mentions of the same entity. We
evaluate our method on a manually annotated
data set, and show that our method outper-
forms the baseline that handles these two tasks
separately, boosting the F1 from 80.2% to
83.6% for NER, and the Accuracy from 79.4%
to 82.6% for NEN, respectively.
1 Introduction
Tweets, short messages of less than 140 characters
shared through the Twitter service 1, have become
an important source of fresh information. As a re-
sult, the task of named entity recognition (NER)
for tweets, which aims to identify mentions of rigid
designators from tweets belonging to named-entity
types such as persons, organizations and locations
(2007), has attracted increasing research interest.
For example, Ritter et al (2011) develop a sys-
tem that exploits a CRF model to segment named
1http://www.twitter.com
entities and then uses a distantly supervised ap-
proach based on LabeledLDA to classify named en-
tities. Liu et al (2011) combine a classifier based
on the k-nearest neighbors algorithm with a CRF-
based model to leverage cross tweets information,
and adopt the semi-supervised learning to leverage
unlabeled tweets.
However, named entity normalization (NEN) for
tweets, which transforms named entities mentioned
in tweets to their unambiguous canonical forms, has
not been well studied. Owing to the informal nature
of tweets, there are rich variations of named enti-
ties in them. According to our investigation on the
data set provided by Liu et al (2011), every named
entity in tweets has an average of 3.3 variations 2.
As an illustrative example, we show ?Anneke Gron-
loh?, which may occur as ?Mw.,Gronloh?, ?Anneke
Kronloh? or ?Mevrouw G?. We thus propose NEN
for tweets, which plays an important role in entity
retrieval, trend detection, and event and entity track-
ing. For example, Khalid et al (2008) show that
even a simple normalization method leads to im-
provements of early precision, for both document
and passage retrieval, and better normalization re-
sults in better retrieval performance.
Traditionally, NEN is regarded as a septated task,
which takes the output of NER as its input (Li et al,
2002; Cohen, 2005; Jijkoun et al, 2008; Dai et al,
2011). One limitation of this cascaded approach is
that errors propagate from NER to NEN and there is
no feedback from NEN to NER. As demonstrated by
Khalid et al (2008), most NEN errors are caused
2This data set consists of 12,245 randomly sampled tweets
within five days.
526
by recognition errors. Another challenge of NEN
is the dearth of information in a single tweet, due
to the short and noise-prone nature of tweets. Re-
portedly, the accuracy of a baseline NEN system
based on Wikipedia drops considerably from 94%
on edited news to 77% on news comments, a kind of
user generated content (UGC) with similar style to
tweets (Jijkoun et al, 2008).
We propose jointly conducting NER and NEN
on multiple tweets using a graphical model, to
address these challenges. Intuitively, improving
the performance of NER boosts the performance
of NEN. For example, consider the following two
tweets: ?? ? ?Alex?s jokes. Justin?s smartness. Max?s
randomnes? ? ? ? and ?? ? ?Alex Russo was like the
best character on Disney Channel? ? ? ?. Identify-
ing ?Alex? and ?Alex Russo? as PERSON will en-
courage NEN systems to normalize ?Alex? into
?Alex Russo?. On the other hand, NEN can guide
NER. For instance, consider the following two
tweets: ?? ? ? she knew Burger King when he was a
Prince!? ? ? ? and ?? ? ? I?m craving all sorts of food:
mcdonalds, burger king, pizza, chinese? ? ? ?. Sup-
pose the NEN system believes that ?burger king?
cannot be mapped to ?Burger King? since these two
tweets are not similar in content. This will help NER
to assign them different types of labels. Our method
optimizes these two tasks simultaneously by en-
abling them to interact with each other. This largely
differentiates our method from existing work.
Furthermore, considering multiple tweets simul-
taneously allows us to exploit the redundancy in
tweets, as suggested by Liu et al (2011). For exam-
ple, consider the following two tweets: ?? ? ?Bobby
Shaw you don?t invite the wind? ? ? ? and ?? ? ? I own
yah ! Loool bobby shaw? ? ? ?. Recognizing ?Bobby
Shaw? in the first tweet as a PERSON is easy owing
to its capitalization and the following word ?you?,
which in turn helps to identify ?bobby shaw? in the
second tweet as a PERSON.
We adopt a factor graph as our graphical model,
which is constructed in the following manner. We
first introduce a random variable for each word in
every tweet, which represents the BILOU (Begin-
ning, the Inside and the Last tokens of multi-token
entities as well as Unit-length entities) label of the
corresponding word. Then we add a factor to con-
nect two neighboring variables, forming a conven-
tional linear chain CRFs. Hereafter, we use tm to
denote the mth tweet ,tim and yim to denote the ith
word of of tm and itsBILOU label, respectively, and
f im to denote the factor related to yi?1m and yim. Next,
for each word pair with the same lemma, denoted by
tim and t
j
n, we introduce a binary random variable,
denoted by zijmn, whose value indicates whether tim
and tjn belong to two mentions of the same entity. Fi-
nally, for any zijmn we add a factor, denoted by f ijmn,
to connect yim, y
j
n and zijmn. Factors in the same
group ({f ijmn} or {f im}) share the same set of fea-
ture templates. Figure 1 illustrates an example of
our factor graph for two tweets.
Figure 1: A factor graph that jointly conducts NER and
NEN on multiple tweets. Blue and green circles rep-
resent NE type (y-serials) and normalization variables
(z-serials), respectively; filled circles indicate observed
random variables; blue rectangles represent the factors
connecting neighboring y-serial variables while red rect-
angles stand for the factors connecting distant y-serial
and z-serial variables.
It is worth noting that our factor graph is differ-
ent from the skip-chain CRFs (Galley, 2006) in the
sense that any skip-chain factor of our model con-
sists not only of two NE type variables (yim and y
j
n),
which is the case for skip-chain CRFs, but also a nor-
malization variable (zijmn). It is these normalization
variables that enable us to conduct NER and NEN
jointly.
We manually add normalization information to
the data set shared by Liu et al (2011), to eval-
uate our method. Experimental results show that
our method achieves 83.6% F1 for NER and 82.6%
Accuracy for NEN, outperforming the baseline with
80.2%F1 for NER and 79.4% Accuracy for NEN.
We summarize our contributions as follows.
1. We introduce the task of NEN for tweets, and
propose jointly conducting NER and NEN for
527
multiple tweets using a factor graph, which
leverages redundancy in tweets to make up for
the dearth of information in a single tweet and
allows these two tasks to inform each other.
2. We evaluate our method on a human annotated
data set, and show that our method compares
favorably with the baseline, achieving better
performance in both tasks.
Our paper is organized as follows. In the next sec-
tion, we introduce related work. In Section 3 and 4,
we formally define the task and present our method.
In Section 5, we evaluate our method. And finally
we conclude our work in Section 6.
2 Related Work
Related work can be divided into two categories:
NER and NEN.
2.1 NER
NER has been well studied and its solutions can be
divided into three categories: 1) Rule-based (Krupka
and Hausman, 1998); 2) machine learning based
(Finkel and Manning, 2009; Singh et al, 2010); and
3) hybrid methods (Jansche and Abney, 2002). Ow-
ing to the availability of annotated corpora, such as
ACE05, Enron (Minkov et al, 2005) and CoNLL03
(Tjong Kim Sang and De Meulder, 2003), data
driven methods are now dominant.
Current studies of NER mainly focus on formal
text such as news articles (Mccallum and Li, 2003;
Etzioni et al, 2005). A representative work is that
of Ratinov and Roth (2009), in which they system-
atically study the challenges of NER, compare sev-
eral solutions, and show some interesting findings.
For example, they show that the BILOU encoding
scheme significantly outperforms the BIO schema
(Beginning, the Inside and Outside of a chunk).
A handful of work on other genres of texts exists.
For example, Yoshida and Tsujii build a biomedi-
cal NER system (2007) using lexical features, or-
thographic features, semantic features and syntactic
features, such as part-of-speech (POS) and shallow
parsing; Downey et al (2007) employ capitaliza-
tion cues and n-gram statistics to locate names of a
variety of classes in web text; Wang (2009) intro-
duces NER to clinical notes. A linear CRF model
is trained on a manually annotated data set, which
achieves an F1 of 81.48% on the test data set; Chiti-
cariu et al (2010) design and implement a high-
level language NERL which simplifies the process
of building, understanding, and customizing com-
plex rule-based named-entity annotators for differ-
ent domains.
Recently, NER for Tweets attracts growing inter-
est. Finin et al (2010) use Amazons Mechani-
cal Turk service 3 and CrowdFlower 4 to annotate
named entities in tweets and train a CRF model to
evaluate the effectiveness of human labeling. Rit-
ter et al (2011) re-build the NLP pipeline for
tweets beginning with POS tagging, through chunk-
ing, to NER, which first exploits a CRF model to
segment named entities and then uses a distantly su-
pervised approach based on LabeledLDA to clas-
sify named entities. Unlike this work, our work de-
tects the boundary and type of a named entity si-
multaneously using sequential labeling techniques.
Liu et al (2011) combine a classifier based on
the k-nearest neighbors algorithm with a CRF-based
model to leverage cross tweets information, and
adopt the semi-supervised learning to leverage un-
labeled tweets. Our method leverages redundance
in similar tweets, using a factor graph rather than a
two-stage labeling strategy. One advantage of our
method is that local and global information can in-
teract with each other.
2.2 NEN
There is a large body of studies into normalizing
various types of entities for formally written texts.
For instance, Cohen (2005) normalizes gene/protein
names using dictionaries automatically extracted
from gene databases; Magdy et al (2007) address
cross-document Arabic name normalization using a
machine learning approach, a dictionary of person
names and frequency information for names in a
collection; Cucerzan (2007) demostrates a large-
scale system for the recognition and semantic dis-
ambiguation of named entities based on informa-
tion extracted from a large encyclopedic collection
and Web search results; Dai et al (2011) employ
a Markov logic network to model interweaved con-
3https://www.mturk.com/mturk/
4http://crowdflower.com/
528
straints in a setting of gene mention normalization.
Jijkoun et al (2008) study NEN for UGC. They
report that the accuracy of a baseline NEN system
based on Wikipedia drops considerably from 94%
on edited news to 77% on UGC. They identify three
main error sources, i.e., entity recognition errors,
multiple ways of referring to the same entity and am-
biguous references, and exploit hand-crafted rules to
improve the baseline NEN system.
We introduce the task of NEN for tweets, a new
genre of texts with rich entity variations. In contrast
to existing NEN systems, which take the output of
NER systems as their input, our method conducts
NER and NEN at the same time, allowing them to
reinforce each other, as demonstrated by the experi-
mental results.
3 Task Definition
A tweet is a short text message with no more than
140 characters. Here is an example of a tweet: ?my-
craftingworld: #Win Microsoft Office 2010 Home
and Student #Contest from @office http://bit.ly/ ? ? ?
?, where ?mycraftingworld? is the name of the user
who published this tweet. Words beginning with
?#? like ??#Win? are hash tags; words starting
with ?@? like ?@office? represent user names; and
?http://bit.ly/? is a shortened link.
Given a set of tweets, e.g., tweets within some pe-
riod or related to some query, our task is: 1) To rec-
ognize each mention of entities of predefined types
for each tweet; and 2) to restore each entity mention
into its unambiguous canonical form. Following Liu
et al (2011), we focus on four types of entities, i.e.,
PERSON, ORGANIZATION, PRODUCT, and LO-
CATION, and constrain our scope to English tweets.
Note that the NEN sub-task can be transformed as
follows. Given each pair of entity mentions, decide
whether they denote the same entity. Once this is
achieved, we can link all the mentions of the same
entity, and choose a representative mention, e.g., the
longest mention, as their canonical form.
As an illustrative example, consider the following
three tweets: ?? ? ?Gaga?s Christmas dinner with her
family. Awwwwn? ? ? ?, ?? ? ?Lady Gaaaaga with her
family on Christmas? ? ? ? and ?? ? ?Buying a maga-
zine just because Lady Gaga?s on the cover? ? ? ?. It
is expected that ?Gaga?, ?Lady Gaaaaga? and ?Lady
Gaga? are all labeled as PERSON, and can be re-
stored as ?Lady Gaga?.
4 Our Method
In contrast to existing work, our method jointly
conducts NER and NEN for multiple tweets. We
first give an overview of our method, then detail its
model and features.
4.1 Overview
Given a set of tweets as input, our method recog-
nizes predefined types of named entities and for each
entity outputs its unambiguous canonical form.
To resolve NER, we assign a label to each
word in a tweet, indicating both the boundary
and entity type. Following Ratinov and Roth
(2009), we use the BILOU schema. For ex-
ample, consider the tweet ?? ? ?without you is
like an iphone without apps; Lady gaga with-
out her telephone? ? ? ?, the labeled sequence us-
ing the BILOU schema is: ?? ? ?withoutO youO
isO likeO anO iphoneU?PRODUCT withoutO appsO;
LadyB?PERSON gagaL?PERSON withoutO herO
telephoneO? ? ? ? , where ?iphoneU?PRODUCT ? indi-
cates that ?iphone? is a product name of unit length;
?LadyB?PERSON? means ?Lady? is the beginning
of a person name while ?gagaL?PERSON? suggests
that ?gaga? is the last token of a person name.
To resolve NEN, we assign a binary value label
zijmn to each pair of words tim and t
j
n which share the
same lemma. zijmn = 1 or -1, indicating whether tim
and tjn belong to two mentions of the same entity 5.
For example, consider the three tweets presented in
Section 3. ?Gaga11? 6 and ?Gaga13? will be assigned
a ?1? label, since they are part of two mentions of the
same entity ?Lady Gaga?; similarly, ?Lady12? and
?Lady13? are connected with a ?1? label. Note that
there are no NEN labels for pairs like ?her11? and
?her12? or ?with11 and ?with12?, since words like ?her?
and ?with? are stop words.
With NE type and normalization labels obtained,
we judge two mentions, denoted by ti1???ikm and
5Stop words have no normalization labels. The stop words
are mainly from http://www.textfixer.com/resources/common-
english-words.txt.
6We use wim to denote word w?s ith appearance in the mth
tweet. For example, ?Gaga11? denotes the first occurance of
?Gaga? in the first tweet.
529
tj1???jln , respectively, refer to the same entity if and
only if: 1) The two mentions share the same entity
type; 2) ti1???ikm is a sub-string of t
j1???jln or vise versa;
and 3) zijmn = 1, i = i1, ? ? ? , ik and j = j1, ? ? ? , jl,
if zijmn exists. Still take the three tweets presented
in Section 3 for example. Suppose ?Gaga11? and
?Lady Gaga13? are labeled as PERSON, and there
is only one related NE normalization label, which
is associated with ??Gaga11? and ?Gaga13? and has 1
as its value. We then consider that these two men-
tions can be normalized into the same entity; in a
similar way, we can align ?Lady12 Gaaaaga? with
?Lady13 Gaga?. Combining these pieces informa-
tion together, we can infer that ??Gaga11?, ?Lady12
Gaaaaga? and ?Lady13 Gaga? are three mentions of
the same entity. Finally, we can select ?Lady13 Gaga?
as the representative, and output ?Lady Gaga? as
their canonical form. We choose the mention with
the maximum number of words as the representa-
tive. In case of a tie, we prefer the mention with an
Wikipedia entry 7.
The central problem with our method is infer-
ring all the NE type (y-serial) and normalization
(z-serial) variables. To achieve this, we construct
a factor graph according to the input tweets, which
can evaluate the probability of every possible assign-
ment of y-serials and z-serials, by checking the
characteristics of the assignment. Each character-
istic is called a feature. In this way, we can select
the assignment with the highest probability. Next
we will introduce our model in detail, including its
training and inference procedure and features.
4.2 Model
We adopt a factor graph as our model. One advan-
tage of our model is that it allows y-serials and
z-serials variables to interact with each other to
jointly optimize NER and NEN.
Given a set of tweets T = {tm}Nm=1, we can build
a factor graph G = (Y,Z, F,E), where: Y and Z
denote y-serials and z-serials variables, respec-
tively; F represents factor vertices, consisting of
{f im} and {f
ij
mn}, f im = f im(yi?1m , yim) and f
ij
mn =
f ijmn(yim, y
j
n, zijmn); E stands for edges, which de-
pends on F , and consists of edges between yi?1m and
yim, and those between yim,y
j
n and f ijmn.
7If it still ends up as a draw, we will randomly choose one
from the best.
G = (Y, Z, F,E) defines a probability distribu-
tion according to Formula 1.
lnP (Y, Z|G, T ) ?
?
m,i
ln f im(yi?1m , yim)+
?
m,n,i,j
?ijmn ? ln f ijmn(yim, yjn, zijmn)
(1)
where ?ijmn = 1 if and only if tim and t
j
n have the
same lemma and are not stop words, otherwise zero.
A factor factorizes according to a set of features, so
that:
ln f im(yi?1m , yim) =
?
k
?(1)k ?
(1)
k (y
i?1
m , yim)
ln f ijmn(yim, yjn, zijmn) =
?
k
?(2)k ?
(2)
k (y
i
m, yjn, zijmn)
(2)
{?(1)k }
K1
k=1 and {?
(2)
k }
K2
k=1 are two feature sets. ? =
{?(1)k }
K1
k=1
?
{?(2)k }
K2
k=1 is called the feature weight
set or parameter set of G. Each feature has a real
value as its weight.
Training ? is learnt from annotated tweets T , by
maximizing the data likelihood, i.e.,
?? = argmax
?
lnP (Y,Z|?, T ) (3)
To solve this optimization problem, we first calcu-
late its gradient:
? lnP (Y, Z|T ; ?)
??1k
=
?
m,i
?(1)k (y
i?1
m , yim)
?
?
m,i
?
yi?1m ,yim
p(yi?1m , yim|T ; ?)?
(1)
k (y
i?1
m , yim)
(4)
? lnP (Y, Z|T ; ?)
??2k
=
?
m,n,i,j
?ijmn ? ?
(2)
k (y
i
m, yjn, zijmn)
?
?
m,n,i,j
?ijmn
?
yim,y
j
n,zijmn
p(yim, yjn, zijmn|T ; ?)
??(2)k (y
i
m, yjn, zijmn)
(5)
Here, the two marginal probabilities
p(yi?1m , yim|T ; ?) and p(yim, y
j
n, zijmn|T ; ?) are
computed using loopy belief propagation (Murphy
et al, 1999). Once we have computed the gradient,
?? can be worked out by standard techniques such
as steepest descent, conjugate gradient and the
530
limited-memory BFGS algorithm (L-BFGS). We
choose L-BFGS because it is particularly well suited
for optimization problems with a large number of
variables.
Inference Supposing the parameters ? have been
set to ??, the inference problem is: Given a set
of testing tweets T , output the most probable
assignment of Y and Z, i.e.,
(Y, Z)? = argmax
(Y,Z)
lnP (Y,Z|??, T ) (6)
We adopt the max-product algorithm to solve this
inference problem. The max-product algorithm is
nearly identical to the loopy belief propagation al-
gorithm, with the sums replaced by maxima in the
definitions. Note that in both the training and test-
ing stage, the factor graph is constructed in the same
way as described in Section 1.
Efficiency We take several actions to improve our
model?s efficiency. Firstly, we manually compile a
comprehensive named entity dictionary from vari-
ous sources including Wikipedia, Freebase 8, news
articles and the gazetteers shared by Ratinov and
Roth (2009). In total this dictionary contains 350
million entries 9. By looking up this dictionary 10,
we generate the possible BILOU labels, denoted by
Y im hereafter, for each word tim. For instance, con-
sider ?? ? ?Good Morning new11 york11? ? ? ?. Suppose
?New York City? and ?New York Times? are in
our dictionary, then ?new11 york11? is the matched
string with two corresponding entities. As a re-
sult, ?B-LOCATION? and ?B-ORGANIZATION?
will be added to Ynew11 , and ?I-LOCATION? and
?I-ORGANIZATION? will be added to Yyork11 . If
Y im ?= ?, we enforce the constraint for training and
testing that yim ? Y im , to reduce the search space.
Secondly, in the testing phase, we introduce three
rules related to zijmn: 1) zijmm = 1, which says two
words sharing the same lemma in the same tweet
denote the same entity; 2) set zijmn to 1, if the sim-
ilarity between tm and tn is above a threshold (0.8
in our work), or tm and tn share one hash tag; and
3)zmnij = ?1, if the similarity between tm and
tn is below a threshold (0.3 in work). To compute
8http://freebase.com/view/military
9One phrase refereing to L entities has L entries.
10We use case-insensitive leftmost longest match.
the similarity, each tweet is represented as a bag-of-
words vector with the stop words removed, and the
cosine similarity is adopted, as defined in Formula
7. These rules pre-label a significant part of z-serial
variables (accounting for 22.5%), with an accuracy
of 93.5%.
sim(tm, tn) =
t?m ? t?n
|?tm||?tn|
(7)
Note that in our experiments, these measures reduce
the training and testing time by 36.2% and 62.8%,
respectively, while no obvious performance drop is
observed.
4.3 Features
A feature in {?(1)k }
K1
k=1 involves a pair of neighbor-
ing NE-type labels, i.e., yi?1m and yim, while a fea-
ture in {?(2)k }
K2
k=1 concerns a pair of distant NE-type
labels and its associated normalization label, i.e.,
yim,y
j
n and zijmn. Details are given below.
4.3.1 Feature Set One: {?(1)k }
K1
k=1
We adopts features similar to Wang (2009), and
Ratinov and Roth (2009), i.e., orthographic features,
lexical features and gazetteer-related features. These
features are defined on the observation. Combining
them with yi?1m and yim constitutes {?
(1)
k }
K1
k=1.
Orthographic features: Whether tim is capitalized
or upper case; whether it is alphanumeric or contains
any slashes; wether it is a stop word; word prefixes
and suffixes.
Lexical features: Lemma of tim, ti?1m and ti+1m ,
respectively; whether tim is an out-of-vocabulary
(OOV) word 11; POS of tim, ti?1m and ti+1m , respec-
tively; whether tim is a hash tag, a link, or a user
account.
Gazetteer-related features: Whether Y im is empty;
the dominating label/entity type in Y im. Which one
is dominant is decided by majority voting of the en-
tities in our dictionary. In case of a tie, we randomly
choose one from the best.
4.3.2 Feature Set Two: {?(2)k }
K2
k=1
Similarly, we define orthographic, lexical features
and gazetteer-related features on the observation, yim
11We first conduct a simple dictionary-lookup based normal-
ization with the incorrect/correct word pair list provided by Han
et al (2011) to correct common ill-formed words. Then we call
an online dictionary service to judge whether a word is OOV.
531
and yjn; and then we combine these features with
zijmn, forming {?(2)k }
K2
k=1.
Orthographic features: Whether tim / t
j
n is capital-
ized or upper case; whether tim / t
j
n is alphanumeric
or contains any slashes; prefixes and suffixes of tim.
Lexical features: Lemma of tim; whether tim is
OOV; whether tim / ti+1m / ti?1m and t
j
n / tj+1n / tj?1n
have the same POS; whether yim and y
j
n have the
same label/entity type.
Gazetteer-related features: Whether Y im
?
Y jn /
Y i+1m
?
Y j+1n / Y i?1m
?
Y j?1n is empty; whether the
dominating label/entity type in Y im is the same as
that in Y jn .
5 Experiments
We manually annotate a data set to evaluate our
method. We show that our method outperforms the
baseline, a cascaded system that conducts NER and
NEN individually.
5.1 Data Preparation
We use the data set provided by Liu et al (2011),
which consists of 12,245 tweets with four types of
entities annotated: PERSON, LOCATION, ORGA-
NIZATION and PRODUCT. We enrich this data set
by adding entity normalization information. Two
annotators 12 are involved. For any entity mention,
two annotators independently annotate its canonical
form. The inter-rater agreement measured by kappa
is 0.72. Any inconsistent case is discussed by the
two annotators till a consensus is reached. 2, 245
tweets are used for development, and the remainder
are used for 5-fold cross validation.
5.2 Evaluation Metrics
We adopt the widely-used Precision, Recall and F1
to measure the performance of NER for a partic-
ular type of entity, and the average Precision, Re-
call and F1 to measure the overall performance of
NER (Liu et al, 2011; Ritter et al, 2011). As for
NEN, we adopt the widely-used Accuracy, i.e., to
what percentage the outputted canonical forms are
correct (Jijkoun et al, 2008; Cucerzan, 2007; Li et
al., 2002).
12Two native English speakers.
5.3 Baseline
We develop a cascaded system as the baseline,
which conducts NER and NEN sequentially. Its
NER module, denoted by SBR, is based on the state-
of-the-art method introduced by Liu et al (2011);
and its NEN model , denoted by SBN , follows
the NEN system for user-generated news comments
proposed by Jijkoun et al (2008), which uses
handcrafted rules to improve a typical NEN system
that normalizes surface forms to Wikipedia page ti-
tles. We use the POS tagger developed by Ritter et
al. (2011) to extract POS related features, and the
OpenNLP toolkit to get lemma related features.
5.4 Results
Tables 1- 2 show the overall performance of the
baseline and ours (denoted by SRN ). It can be
seen that, our method yields a significantly higher
F1 (with p < 0.01) than SBR, and a moderate im-
provement of accuracy as compared with SBN (with
p < 0.05). As a case study, we show that our system
successfully identified ?jaxon11? as a PERSON in the
tweet ?? ? ? come to see jaxon11 someday? ? ? ?, which
is mistakenly labeled as a LOCATION by SBR.
This is largely owing to the fact that our system
aligns ?jaxon11? with ?Jaxson12? in the tweet ?? ? ? I
love Jaxson12,Hes like my little brother? ? ? ?, in which
?Jaxson12? is identified as a PERSON. As a result,
this encourages our system to consider ?jaxon11? as
a PERSON. We also find cases where our system
works but SBN fails. For example, ?Goldman11?
in the tweet ?? ? ?Goldman sees massive upside risk
in oil prices? ? ? ? is normalized into ?Albert Gold-
man? by SBR, because it is mistakenly identified as
a PERSON by SBS ; in contrast, our system recog-
nizes ?Goldman12 Sachs? as an ORGANIZATION,
and successfully links ?Goldman12? to ?Goldman11?,
resulting that ?Goldman11? is identified as an ORGA-
NIZATION and normalized into ?Goldman Sachs?.
Table 3 reports the NER performance of our
method for each entity type, from which we see that
our system consistently yields better F1 on all entity
types than SBR. We also see that our system boosts
the F1 for ORGANIZATION most significantly, re-
flecting the fact that a large number of organizations
that are incorrectly labeled as PERSON by SBR, are
now correctly recognized by our method.
532
System Pre Rec F1
SRN 84.7 82.5 83.6
SBR 81.6 78.8 80.2
Table 1: Overall performance (%) of NER.
System Accuracy
SRN 82.6
SBN 79.4
Table 2: Overall Accuracy (%) of NEN .
System PER PRO LOC ORG
SRN 84.2 80.5 82.1 85.2
SBR 83.9 78.7 81.3 79.8
Table 3: F1 (%) of NER on different entity types.
Features NER (F1) NEN (Accuracy)
Fo 59.2 61.3
Fo + Fl 65.8 68.7
Fo + Fg 80.1 77.2
Fo + Fl + Fg 83.6 82.6
Table 4: Overall F1 (%) of NER and Accuracy (%) of
NEN with different feature sets.
Table 4 shows the overall performance of our
method with various feature set combinations,
where Fo, Fl and Fg denote the orthographic fea-
tures, the lexical features, and the gazetteer-related
features, respectively. From Table 4 we see that
gazetteer-related features significantly boost the F1
for NER and Accuracy for NEN, suggesting the im-
portance of external knowledge for this task.
5.5 Discussion
One main error source for NER and NEN, which
accounts for more than half of all the errors, is
slang expressions and informal abbreviations. For
instance, our method recognizes ?California11? in
the tweet ?? ? ?And Now, He Lives All The Way In
California11? ? ? ? as a LOCATION, however, it mis-
takenly identifies ?Cali12? in the tweet ?? ? ? i love
Cali so much? ? ? ? as a PERSON. One reason is our
system does not generate any z-serial variable for
?California11? and ?Cali12? since they have differ-
ent lemmas. A more complicated case is ?BS11? in
the tweet ?? ? ? I, bobby shaw, am gonna put BS11 on
everything? ? ? ?, in which ?BS11? is the abbreviation
of ?bobby shaw?. Our method fails to recognize
?BS11? as an entity. There are two possible ways to
fix these errors: 1) Extending the scope of z-serial
variables to each word pairs with a common prefix;
and 2) developing advanced normalization compo-
nents to restore such slang expressions and informal
abbreviations into their canonical forms.
Our method does not directly exploit Wikipedia
for NEN. This explains the cases where our system
correctly links multiple entity mentions but fails to
generate canonical forms. Take the following two
tweets for example: ?? ? ? nitip link win711 sp1? ? ? ?
and ?? ? ?Hit the 3TB wall on SRT installing fresh
Win712? ? ? ?. Our system recognizes ?win711? and
?Win712? as two mentions of the same product, but
cannot output their canonical forms ?Windows 7?.
One possible solution is to exploit Wikipedia to
compile a dictionary consisting of entities and their
variations.
6 Conclusions and Future work
We study the task of NEN for tweets, a new genre
of texts that are short and prone to noise. Two chal-
lenges of this task are the dearth of information in
a single tweet and errors propagated from the NER
component. We propose jointly conducting NER
and NEN for multiple tweets using a factor graph, to
address these challenges. One unique characteristic
of our model is that a NE normalization variable is
introduced to indicate whether a word pair belongs
to the mentions of the same entity. We evaluate our
method on a manually annotated data set. Experi-
mental results show our method yields better F1 for
NER and Accuracy for NEN than the state-of-the-art
baseline that conducts two tasks sequentially.
In the future, we plan to explore two directions to
improve our method. First, we are going to develop
advanced tweet normalization technologies to re-
solve slang expressions and informal abbreviations.
Second, we are interested in incorporating knowl-
edge mined from Wikipedia into our factor graph.
Acknowledgments
We thank Yunbo Cao, Dongdong Zhang, and Mu Li
for helpful discussions, and the anonymous review-
ers for their valuable comments.
533
References
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP, pages
1002?1012.
Aaron Cohen. 2005. Unsupervised gene/protein named
entity normalization using automatically extracted dic-
tionaries. In Proceedings of the ACL-ISMB Work-
shop on Linking Biological Literature, Ontologies and
Databases: Mining Biological Semantics, pages 17?
24, Detroit, June. Association for Computational Lin-
guistics.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In In Proc. 2007
Joint Conference on EMNLP and CNLL, pages 708?
716.
Hong-Jie Dai, Richard Tzong-Han Tsai, and Wen-Lian
Hsu. 2011. Entity disambiguation using a markov-
logic network. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 846?855, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating Complex Named Entities in Web Text.
In IJCAI.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In CSLDAMT, pages 80?88.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141?150.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Association for Computational Linguistics, pages
364?372.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
ACL HLT.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320?327.
Valentin Jijkoun, Mahboob Alam Khalid, Maarten Marx,
and Maarten de Rijke. 2008. Named entity normal-
ization in user generated content. In Proceedings of
the second workshop on Analytics for noisy unstruc-
tured text data, AND ?08, pages 23?30, New York,
NY, USA. ACM.
Mahboob Khalid, Valentin Jijkoun, and Maarten de Ri-
jke. 2008. The impact of named entity normaliza-
tion on information retrieval for question answering.
In Craig Macdonald, Iadh Ounis, Vassilis Plachouras,
Ian Ruthven, and Ryen White, editors, Advances in In-
formation Retrieval, volume 4956 of Lecture Notes in
Computer Science, pages 705?710. Springer Berlin /
Heidelberg.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li.
2002. Location normalization for information extrac-
tion. In COLING.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In ACL.
Walid Magdy, Kareem Darwish, Ossama Emam, and
Hany Hassan. 2007. Arabic cross-document person
name normalization. In In CASL Workshop 07, pages
25?32.
Andrew Mccallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In HLT-NAACL, pages 188?191.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In HLT,
pages 443?450.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In In Proceedings of Un-
certainty in AI, pages 467?475.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, 30:3?26.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147?155.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1524?1534, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73?81.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL, pages 142?147.
534
Yefeng Wang. 2009. Annotating and recognising named
entities in clinical notes. In ACL-IJCNLP, pages 18?
26.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In BioNLP,
pages 209?216.
535
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572?581,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-Lingual Mixture Model for Sentiment Classification
Xinfan Meng? ?Furu Wei? Xiaohua Liu? Ming Zhou? Ge Xu? Houfeng Wang?
?MOE Key Lab of Computational Linguistics, Peking University
?Microsoft Research Asia
?{mxf, xuge, wanghf}@pku.edu.cn
?{fuwei,xiaoliu,mingzhou}@microsoft.com
Abstract
The amount of labeled sentiment data in En-
glish is much larger than that in other lan-
guages. Such a disproportion arouse interest
in cross-lingual sentiment classification, which
aims to conduct sentiment classification in the
target language (e.g. Chinese) using labeled
data in the source language (e.g. English).
Most existing work relies on machine trans-
lation engines to directly adapt labeled data
from the source language to the target lan-
guage. This approach suffers from the limited
coverage of vocabulary in the machine transla-
tion results. In this paper, we propose a gen-
erative cross-lingual mixture model (CLMM)
to leverage unlabeled bilingual parallel data.
By fitting parameters to maximize the likeli-
hood of the bilingual parallel data, the pro-
posed model learns previously unseen senti-
ment words from the large bilingual parallel
data and improves vocabulary coverage signifi-
cantly. Experiments on multiple data sets show
that CLMM is consistently effective in two set-
tings: (1) labeled data in the target language are
unavailable; and (2) labeled data in the target
language are also available.
1 Introduction
Sentiment Analysis (also known as opinion min-
ing), which aims to extract the sentiment informa-
tion from text, has attracted extensive attention in
recent years. Sentiment classification, the task of
determining the sentiment orientation (positive, neg-
ative or neutral) of text, has been the most exten-
sively studied task in sentiment analysis. There is
?Contribution during internship atMicrosoft ResearchAsia.
already a large amount of work on sentiment classi-
fication of text in various genres and in many lan-
guages. For example, Pang et al (2002) focus on
sentiment classification of movie reviews in English,
and Zagibalov and Carroll (2008) study the problem
of classifying product reviews in Chinese. During
the past few years, NTCIR1 organized several pi-
lot tasks for sentiment classification of news articles
written in English, Chinese and Japanese (Seki et
al., 2007; Seki et al, 2008).
For English sentiment classification, there are sev-
eral labeled corpora available (Hu and Liu, 2004;
Pang et al, 2002; Wiebe et al, 2005). However, la-
beled resources in other languages are often insuf-
ficient or even unavailable. Therefore, it is desir-
able to use the English labeled data to improve senti-
ment classification of documents in other languages.
One direct approach to leveraging the labeled data
in English is to use machine translation engines as a
black box to translate the labeled data from English
to the target language (e.g. Chinese), and then us-
ing the translated training data directly for the devel-
opment of the sentiment classifier in the target lan-
guage (Wan, 2009; Pan et al, 2011).
Although the machine-translation-based methods
are intuitive, they have certain limitations. First,
the vocabulary covered by the translated labeled
data is limited, hence many sentiment indicative
words can not be learned from the translated labeled
data. Duh et al (2011) report low overlapping
between vocabulary of natural English documents
and the vocabulary of documents translated to En-
glish from Japanese, and the experiments of Duh
1http://research.nii.ac.jp/ntcir/index-en.html
572
et al (2011) show that vocabulary coverage has a
strong correlation with sentiment classification ac-
curacy. Second, machine translation may change the
sentiment polarity of the original text. For exam-
ple, the negative English sentence ?It is too good to
be true? is translated to a positive sentence in Chi-
nese ?????????? by Google Translate
(http://translate.google.com/), which literally means
?It is good and true?.
In this paper we propose a cross-lingual mixture
model (CLMM) for cross-lingual sentiment classifi-
cation. Instead of relying on the unreliable machine
translated labeled data, CLMM leverages bilingual
parallel data to bridge the language gap between the
source language and the target language. CLMM is
a generative model that treats the source language
and target language words in parallel data as gener-
ated simultaneously by a set of mixture components.
By ?synchronizing? the generation of words in the
source language and the target language in a parallel
corpus, the proposed model can (1) improve vocabu-
lary coverage by learning sentiment words from the
unlabeled parallel corpus; (2) transfer polarity label
information between the source language and target
language using a parallel corpus. Besides, CLMM
can improve the accuracy of cross-lingual sentiment
classification consistently regardless of whether la-
beled data in the target language are present or not.
We evaluate the model on sentiment classification
of Chinese using English labeled data. The exper-
iment results show that CLMM yields 71% in accu-
racy when no Chinese labeled data are used, which
significantly improves Chinese sentiment classifica-
tion and is superior to the SVMand co-training based
methods. When Chinese labeled data are employed,
CLMMyields 83% in accuracy, which is remarkably
better than the SVM and achieve state-of-the-art per-
formance.
This paper makes two contributions: (1) we pro-
pose a model to effectively leverage large bilin-
gual parallel data for improving vocabulary cover-
age; and (2) the proposed model is applicable in both
settings of cross-lingual sentiment classification, ir-
respective of the availability of labeled data in the
target language.
The paper is organized as follows. We review re-
lated work in Section 2, and present the cross-lingual
mixture model in Section 3. Then we present the ex-
perimental studies in Section 4, and finally conclude
the paper and outline the future plan in Section 5.
2 Related Work
In this section, we present a brief review of the re-
lated work on monolingual sentiment classification
and cross-lingual sentiment classification.
2.1 Sentiment Classification
Early work of sentiment classification focuses on
English product reviews or movie reviews (Pang et
al., 2002; Turney, 2002; Hu and Liu, 2004). Since
then, sentiment classification has been investigated
in various domains and different languages (Zag-
ibalov and Carroll, 2008; Seki et al, 2007; Seki et
al., 2008; Davidov et al, 2010). There exist two
main approaches to extracting sentiment orientation
automatically. The Dictionary-based approach (Tur-
ney, 2002; Taboada et al, 2011) aims to aggregate
the sentiment orientation of a sentence (or docu-
ment) from the sentiment orientations of words or
phrases found in the sentence (or document), while
the corpus-based approach (Pang et al, 2002) treats
the sentiment orientation detection as a conventional
classification task and focuses on building classifier
from a set of sentences (or documents) labeled with
sentiment orientations.
Dictionary-based methods involve in creating or
using sentiment lexicons. Turney (2002) derives
sentiment scores for phrases by measuring the mu-
tual information between the given phrase and the
words ?excellent? and ?poor?, and then uses the av-
erage scores of the phrases in a document as the
sentiment of the document. Corpus-based meth-
ods are often built upon machine learning mod-
els. Pang et al (2002) compare the performance
of three commonly used machine learning models
(Naive Bayes, Maximum Entropy and SVM). Ga-
mon (2004) shows that introducing deeper linguistic
features into SVM can help to improve the perfor-
mance. The interested readers are referred to (Pang
and Lee, 2008) for a comprehensive review of senti-
ment classification.
2.2 Cross-Lingual Sentiment Classification
Cross-lingual sentiment classification, which aims
to conduct sentiment classification in the target lan-
guage (e.g. Chinese) with labeled data in the source
573
language (e.g. English), has been extensively stud-
ied in the very recent years. The basic idea is to ex-
plore the abundant labeled sentiment data in source
language to alleviate the shortage of labeled data in
the target language.
Most existing work relies on machine translation
engines to directly adapt labeled data from the source
language to target language. Wan (2009) proposes
to use ensemble method to train better Chinese sen-
timent classification model on English labeled data
and their Chinese translation. English Labeled data
are first translated to Chinese, and then two SVM
classifiers are trained on English andChinese labeled
data respectively. After that, co-training (Blum and
Mitchell, 1998) approach is adopted to leverage Chi-
nese unlabeled data and their English translation to
improve the SVM classifier for Chinese sentiment
classification. The same idea is used in (Wan, 2008),
but the ensemble techniques used are various vot-
ing methods and the individual classifiers used are
dictionary-based classifiers.
Instead of ensemblemethods, Pan et al (2011) use
matrix factorization formulation. They extend Non-
negative Matrix Tri-Factorization model (Li et al,
2009) to bilingual view setting. Their bilingual view
is also constructed by using machine translation en-
gines to translate original documents. Prettenhofer
and Stein (2011) use machine translation engines in
a different way. They generalize Structural Corre-
spondence Learning (Blitzer et al, 2006) to multi-
lingual setting. Instead of using machine translation
engines to translate labeled text, the authors use it to
construct the word translation oracle for pivot words
translation.
Lu et al (2011) focus on the task of jointly im-
proving the performance of sentiment classification
on two languages (e.g. English and Chinese) . the
authors use an unlabeled parallel corpus instead of
machine translation engines. They assume paral-
lel sentences in the corpus should have the same
sentiment polarity. Besides, they assume labeled
data in both language are available. They propose
a method of training two classifiers based on maxi-
mum entropy formulation to maximize their predic-
tion agreement on the parallel corpus. However, this
method requires labeled data in both the source lan-
guage and the target language, which are not always
readily available.
3 Cross-Lingual Mixture Model for
Sentiment Classification
In this section we present the cross-lingual mix-
ture model (CLMM) for sentiment classification.
We first formalize the task of cross-lingual sentiment
classification. Then we describe the CLMM model
and present the parameter estimation algorithm for
CLMM.
3.1 Cross-lingual Sentiment Classification
Formally, the task we are concerned about is to de-
velop a sentiment classifier for the target language T
(e.g. Chinese), given labeled sentiment data DS in
the source language S (e.g. English), unlabeled par-
allel corpus U of the source language and the target
language, and optional labeled dataDT in target lan-
guage T . Aligning with previous work (Wan, 2008;
Wan, 2009), we only consider binary sentiment clas-
sification scheme (positive or negative) in this paper,
but the proposed method can be used in other classi-
fication schemes with minor modifications.
3.2 The Cross-Lingual Mixture Model
The basic idea underlying CLMM is to enlarge
the vocabulary by learning sentiment words from the
parallel corpus. CLMM defines an intuitive genera-
tion process as follows. Suppose we are going to
generate a positive or negative Chinese sentence, we
have two ways of generating words. The first way
is to directly generate a Chinese word according to
the polarity of the sentence. The other way is to first
generate an English word with the same polarity and
meaning, and then translate it to a Chinese word.
More formally, CLMM defines a generative mix-
ture model for generating a parallel corpus. The un-
observed polarities of the unlabeled parallel corpus
are modeled as hidden variables, and the observed
words in parallel corpus are modeled as generated by
a set of words generation distributions conditioned
on the hidden variables. Given a parallel corpus, we
fit CLMM model by maximizing the likelihood of
generating this parallel corpus. By maximizing the
likelihood, CLMM can estimate words generation
probabilities for words unseen in the labeled data but
present in the parallel corpus, hence expand the vo-
cabulary. In addition, CLMM can utilize words in
both the source language and target language for de-
574
termining polarity classes of the parallel sentences.
POS 
NEG 
POS 
NEG 
...? 
Source 
Target 
U 
u wt 
ws 
Figure 1: The generation process of the
cross-lingual mixture model
Figure 1 illustrates the detailed process of gener-
ating words in the source language and target lan-
guage respectively for the parallel corpus U , from
the four mixture components in CLMM. Particu-
larly, for each pair of parallel sentences ui ? U , we
generate the words as follows.
1. Document class generation: Generating the
polarity class.
(a) Generating a polarity class cs from a
Bernoulli distribution Ps(C).
(b) Generating a polarity class ct from a
Bernoulli distribution Pt(C)
2. Words generation: Generating the words
(a) Generating source language wordsws from
a Multinomial distribution P (ws|cs)
(b) Generating target language words wt from
a Multinomial distribution P (wt|ct)
3. Words projection: Projecting the words onto
the other language
(a) Projecting the source language wordsws to
target language words wt by word projec-
tion probability P (wt|ws)
(b) Projecting the target language words wt to
source language words ws by word projec-
tion probability P (ws|wt)
CLMM finds parameters by using MLE (Maxi-
mum Likelihood Estimation). The parameters to be
estimated include conditional probabilities of word
to class, P (ws|c) and P (wt|c), and word projection
probabilities, P (ws|wt) and P (wt|ws). We will de-
scribe the log-likelihood function and then show how
to estimate the parameters in subsection 3.3. The
obtained word-class conditional probability P (wt|c)
can then be used to classify text in the target lan-
guages using Bayes Theorem and the Naive Bayes
independence assumption.
Formally, we have the following log-likelihood
function for a parallel corpus U2.
L(?|U) =
|Us|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
[
Nsi log
(
P (ws|cj) + P (ws|wt)P (wt|cj)
)]
+
|Ut|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
[
Nti log
(
P (wt|cj) + P (wt|ws)P (ws|cj)
)]
(1)
where ? is the model parameters;Nsi (Nti) is the oc-
currences of thewordws (wt) in document di; |Ds| is
the number of documents; |C| is the number of class
labels; Vs and Vt are the vocabulary in the source lan-
guage and the vocabulary in the target language.|Us|
and |Ut| are the number of unlabeled sentences in the
source language and target language.
Meanwhile, we have the following log-likelihood
function for labeled data in the source language Ds.
L(?|Ds) =
|Ds|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
Nsi logP (ws|cj)?ij (2)
where ?ij = 1 if the label of di is cj , and 0 otherwise.
In addition, when labeled data in the target lan-
guage is available, we have the following log-
likelihood function.
L(?|Dt) =
|Dt|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
Nti logP (wt|cj)?ij (3)
Combining the above three likelihood functions
together, we have the following likelihood function.
L(?|Dt, Ds, U) = L(?|U) + L(?|Ds) + L(?|Dt)
(4)
Note that the third term on the right hand side
(L(?|Dt)) is optional.
2For simplicity, we assume the prior distribution P (C) is
uniform and drop it from the formulas.
575
3.3 Parameter Estimation
Instead of estimating word projection probability
(P (ws|wt) and P (wt|ws)) and conditional proba-
bility of word to class (P (wt|c) and P (ws|c)) si-
multaneously in the training procedure, we estimate
them separately since the word projection probabil-
ity stays invariant when estimating other parame-
ters. We estimate word projection probability using
word alignment probability generated by the Berke-
ley aligner (Liang et al, 2006). The word align-
ment probabilities serves two purposes. First, they
connect the corresponding words between the source
language and the target language. Second, they ad-
just the strength of influences between the corre-
sponding words. Figure 2 gives an example of word
alignment probability. As is shown, the three words
?tour de force? altogether express a positive mean-
ing, while in Chinese the same meaning is expressed
with only one word ???? (masterpiece). CLMM
use word alignment probability to decrease the in-
fluences from ???? (masterpiece) to ?tour?, ?de?
and ?force? individually, using the word projection
probability (i.e. word alignment probability), which
is 0.3 in this case.
Herman Melville's Moby Dick was a tour de force.  
 
??? ???? ? ?????? ?? ??? 
1  1  .5  .5  1  1  .3  . 3  . 3  
Figure 2: Word Alignment Probability
We use Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977) to estimate the con-
ditional probability of word ws and wt given class
c, P (ws|c) and P (wt|c) respectively. We derive the
equations for EM algorithm, using notations similar
to (Nigam et al, 2000).
In the E-step, the distribution of hidden variables
(i.e. class label for unlabeled parallel sentences) is
computed according to the following equations.
P (cj |usi) = Z(cusi = cj) =
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
?
cj
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
(5)
P (cj |uti) = Z(cuti = cj) =
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
?
cj
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
(6)
whereZ(cusi = cj)
(
Z(cuti) = cj
)
is the probability
of the source (target) language sentence usi (uti) in
the i-th pair of sentences ui having class label cj .
In the M-step, the parameters are computed by the
following equations.
P (ws|cj) =
1 +
?|Ds|
i=1 ?s(i)NsiP (cj |di)
|V | +
?|Vs|
s=1 ?(i)NsiP (cj |di)
(7)
P (wt|cj) =
1 +
?|Dt|
i=1 ?t(i)NtiP (cj |di)
|V | +
?|Vt|
t=1 ?(i)NtiP (cj |di)
(8)
where ?s(i) and ?t(i) are weighting factor to con-
trol the influence of the unlabeled data. We set ?s(i)
(
?t(i)
)
to ?s
(
?t
)
when di belongs to unlabeled
data, 1 otherwise. When di belongs to labeled data,
P (cj |di) is 1 when its label is cj and 0 otherwise.
When di belongs to unlabeled data, P (cj |di) is com-
puted according to Equation 5 or 6.
4 Experiment
4.1 Experiment Setup and Data Sets
Experiment setup: We conduct experiments on
two common cross-lingual sentiment classification
settings. In the first setting, no labeled data in the
target language are available. This setting has real-
istic significance, since in some situations we need to
quickly develop a sentiment classifier for languages
that we do not have labeled data in hand. In this
case, we classify text in the target language using
only labeled data in the source language. In the sec-
ond setting, labeled data in the target language are
also available. In this case, a more reasonable strat-
egy is to make full use of both labeled data in the
source language and target language to develop the
sentiment classifier for the target language. In our
experiments, we consider English as the source lan-
guage and Chinese as the target language.
Data sets: For Chinese sentiment classification,
we use the same data set described in (Lu et al,
2011). The labeled data sets consist of two English
data sets and one Chinese data set. The English data
set is from the Multi-Perspective Question Answer-
ing (MPQA) corpus (Wiebe et al, 2005) and the NT-
CIR Opinion Analysis Pilot Task data set (Seki et
al., 2008; Seki et al, 2007). The Chinese data set
also comes from the NTCIR Opinion Analysis Pi-
lot Task data set. The unlabeled parallel sentences
576
are selected from ISI Chinese-English parallel cor-
pus (Munteanu and Marcu, 2005). Following the
description in (Lu et al, 2011), we remove neutral
sentences and keep only high confident positive and
negative sentences as predicted by a maximum en-
tropy classifier trained on the labeled data. Table 1
shows the statistics for the data sets used in the ex-
periments. We conduct experiments on two data set-
tings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN
+ NTCIR-CH.
MPQA NTCIR-EN NTCIR-CH
Positive 1,471(30%) 528 (30%) 2,378 (55%)
Negative 3,487(70%) 1,209(70%) 1,916(44%)
Total 4,958 1,737 4,294
Table 1: Statistics about the Data
CLMM includes two hyper-parameters (?s and
?t) controlling the contribution of unlabeled parallel
data. Larger weights indicate larger influence from
the unlabeled data. We set the hyper-parameters
by conducting cross validations on the labeled data.
WhenChinese labeled data are unavailable, we set?t
to 1 and ?s to 0.1, since no Chinese labeled data are
used and the contribution of target language to the
source language is limited. When Chinese labeled
data are available, we set ?s and ?t to 0.2.
To prevent long sentences from dominating the pa-
rameter estimation, we preprocess the data set by
normalizing the length of all sentences to the same
constant (Nigam et al, 2000), the average length of
the sentences.
4.2 Baseline Methods
For the purpose of comparison, we implement the
following baseline methods.
MT-SVM:We translate the English labeled data to
Chinese using Google Translate and use the transla-
tion results to train the SVM classifier for Chinese.
SVM: We train a SVM classifier on the Chinese
labeled data.
MT-Cotrain: This is the co-training based ap-
proach described in (Wan, 2009). We summarize
the main steps as follows. First, two monolingual
SVM classifiers are trained on English labeled data
and Chinese data translated from English labeled
data. Second, the two classifiers make prediction on
Chinese unlabeled data and their English translation,
respectively. Third, the 100 most confidently pre-
dicted English and Chinese sentences are added to
the training set and the twomonolingual SVMclassi-
fiers are re-trained on the expanded training set. The
second and the third steps are repeated for 100 times
to obtain the final classifiers.
Para-Cotrain: The training process is the same as
MT-Cotrain. However, we use a different set of En-
glish unlabeled sentences. Instead of using the corre-
sponding machine translation of Chinese unlabeled
sentences, we use the parallel English sentences of
the Chinese unlabeled sentences.
Joint-Train: This is the state-of-the-art method de-
scribed in (Lu et al, 2011). This model use En-
glish labeled data and Chinese labeled data to obtain
initial parameters for two maximum entropy clas-
sifiers (for English documents and Chinese docu-
ments), and then conduct EM-iterations to update
the parameters to gradually improve the agreement
of the two monolingual classifiers on the unlabeled
parallel sentences.
4.3 Classification Using Only English Labeled
Data
The first set of experiments are conducted on us-
ing only English labeled data to create the sentiment
classifier for Chinese. This is a challenging task,
since we do not use any Chinese labeled data. And
MPQA and NTCIR data sets are compiled by differ-
ent groups using different annotation guidelines.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM N/A N/A
MT-Cotrain 65.13 59.11
Para-Cotrain 67.21 60.71
Joint-Train N/A N/A
CLMM 70.96 71.52
Table 2: Classification Accuracy Using Only
English Labeled Data
Table 2 shows the accuracy of the baseline sys-
tems as well as the proposed model (CLMM). As
is shown, sentiment classification does not bene-
fit much from the direct machine translation. For
NTCIR-EN+NTCIR-CH, the accuracy of MT-SVM
577
is only 62.34%. For MPQA-EN+NTCIR-CH, the
accuracy is 54.33%, even lower than a trivial
method, which achieves 55.4% by predicting all sen-
tences to be positive. The underlying reason is that
the vocabulary coverage in machine translated data
is low, therefore the classifier learned from the la-
beled data is unable to generalize well on the test
data. Meanwhile, the accuracy of MT-SVM on
NTCIR-EN+NTCIR-CH data set is much better than
that on MPQA+NTCIR-CH data set. That is be-
cause NTCIR-EN and NTCIR-CH cover similar top-
ics. The other two methods using machine translated
data, MT-Cotrain and Para-Cotrain also do not per-
form verywell. This result is reasonable, because the
initial Chinese classifier trained on machine trans-
lated data (MT-SVM) is relatively weak. We also
observe that using a parallel corpus instead of ma-
chine translations can improve classification accu-
racy. It should be noted that we do not have the result
for Joint-Train model in this setting, since it requires
both English labeled data and Chinese labeled data.
4.4 Classification Using English and Chinese
Labeled Data
The second set of experiments are conducted on
using both English labeled data and Chinese labeled
data to develop the Chinese sentiment classifier. We
conduct 5-fold cross validations on Chinese labeled
data. We use the same baseline methods as described
in Section 4.2, but we use natural Chinese sentences
instead of translated Chinese sentences as labeled
data in MT-Cotrain and Para-Cotrain. Table 3 shows
the accuracy of baseline systems as well as CLMM.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM 80.58 80.58
MT-Cotrain 82.28 80.93
Para-Cotrain 82.35 82.18
Joint-Train 83.11 83.42
CLMM 82.73 83.02
Table 3: Classification Accuracy Using English and
Chinese Labeled Data
As is seen, SVMperforms significantly better than
MT-SVM. One reason is that we use natural Chi-
nese labeled data instead of translated Chinese la-
beled data. Another reason is that we use 5-fold
cross validations in this setting, while the previous
setting is an open test setting. In this setting, SVM
is a strong baseline with 80.6% accuracy. Never-
theless, all three methods which leverage an unla-
beled parallel corpus, namely Para-Cotrain, Joint-
Train and CLMM, still show big improvements over
the SVM baseline. Their results are comparable and
all achieve state-of-the-art accuracy of about 83%,
but in terms of training speed, CLMM is the fastest
method (Table 4). Similar to the previous setting,We
also have the same observation that using a parallel
corpus is better than using translations.
Method Iterations Total Time
Para-Cotrain 100 6 hours
Joint-Train 10 55 seconds
CLMM 10 30 seconds
Table 4: Training Speed Comparison
4.5 The Influence of Unlabeled Parallel Data
We investigate how the size of the unlabeled par-
allel data affects the sentiment classification in this
subsection. We vary the number of sentences in the
unlabeled parallel from 2,000 to 20,000. We use
only English labeled data in this experiment, since
this more directly reflects the effectiveness of each
model in utilizing unlabeled parallel data. From Fig-
ure 3 and Figure 4, we can see that when more unla-
beled parallel data are added, the accuracy of CLMM
consistently improves. The performance of CLMM
is remarkably superior than Para-Cotrain and MT-
Cotrain. When we have 10,000 parallel sentences,
the accuracy of CLMM on the two data sets quickly
increases to 68.77% and 68.91%, respectively. By
contrast, we observe that the performance of Para-
Cotrain and MT-Cotrain is able to obtain accuracy
improvement only after about 10,000 sentences are
added. The reason is that the two methods use ma-
chine translated labeled data to create initial Chinese
classifiers. As is depicted in Table 2, these classifiers
are relatively weak. As a result, in the initial itera-
tions of co-training based methods, the predictions
made by the Chinese classifiers are inaccurate, and
co-training based methods need to see more parallel
578
Number of Sentences
Accur
acy
62
64
66
68
70
l
l l l
l l
l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 3: Accuracy with different size of
unlabeled data for NTICR-EN+NTCIR-CH
Number of Sentences
Accur
acy
55
60
65
70
l
l
l
l l
l l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 4: Accuracy with different size of
unlabeled data for MPQA+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 5: Accuracy with different size of
labeled data for NTCIR-EN+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 6: Accuracy with different size of
labeled data for MPQA+NTCIR-CH
sentences to refine the initial classifiers.
4.6 The Influence of Chinese Labeled Data
In this subsection, we investigate how the size of
the Chinese labeled data affects the sentiment classi-
fication. As is shown in Figure 5 and Figure 6, when
only 500 labeled sentences are used, CLMM is capa-
ble of achieving 72.52% and 74.48% in accuracy on
the two data sets, obtaining 10% and 8% improve-
ments over the SVM baseline, respectively. This
indicates that our method leverages the unlabeled
data effectively. When more sentences are used,
CLMM consistently shows further improvement in
accuracy. Para-Cotrain and Joint-Train show simi-
lar trends. When 3500 labeled sentences are used,
SVM achieves 80.58%, a relatively high accuracy
for sentiment classification. However, CLMM and
the other two models can still gain improvements.
This further demonstrates the advantages of expand-
ing vocabulary using bilingual parallel data.
5 Conclusion and Future Work
In this paper, we propose a cross-lingual mix-
ture model (CLMM) to tackle the problem of cross-
lingual sentiment classification. This method has
two advantages over the existing methods. First, the
proposed model can learn previously unseen senti-
ment words from large unlabeled data, which are not
covered by the limited vocabulary in machine trans-
lation of the labeled data. Second, CLMM can ef-
fectively utilize unlabeled parallel data regardless of
whether labeled data in the target language are used
or not. Extensive experiments suggest that CLMM
consistently improve classification accuracy in both
settings. In the future, we will work on leverag-
ing parallel sentences and word alignments for other
tasks in sentiment analysis, such as building multi-
lingual sentiment lexicons.
Acknowledgment We thank Bin Lu and Lei Wang for
their help. This research was partly supported by National High
Technology Research and Development Program of China (863
Program) (No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009, No.60973053)
579
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, page 120?128.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, page 92?100.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
page 241?249.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), page 1?38.
Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for Cross-Lingual sentiment
classification? In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, page 429?433,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. InProceedings of the
20th international conference on Computational Lin-
guistics, page 841.
Mingqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge
discovery and data mining, page 168?177.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to sentiment
classification with lexical prior knowledge. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, page 244?252, Suntec, Singapore, August.
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, page 104?111.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies-
Volume 1, page 320?330.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine learning, 39(2):103?134.
Junfeng Pan, Gui-Rong Xue, Yong Yu, and Yang Wang.
2011. Cross-lingual sentiment classification via bi-
view non-negative matrix tri-factorization. Advances
in Knowledge Discovery and Data Mining, page
289?300.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, page 79?86.
Peter Prettenhofer and Benno Stein. 2011. Cross-lingual
adaptation using structural correspondence learning.
ACM Transactions on Intelligent Systems and Technol-
ogy (TIST), 3(1):13.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of opinion analysis pilot task at NTCIR-6.
In Proceedings of NTCIR-6 Workshop Meeting, page
265?278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin.
2008. Overview of multilingual opinion analysis task
at NTCIR-7. In Proc. of the Seventh NTCIR Workshop.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based meth-
ods for sentiment analysis. Comput. Linguist., page to
appear.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics,
page 417?424.
Xiaojun Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised chinese sentiment
analysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
?08, page 553?561, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
580
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume 1,
page 235?243.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2):165?210.
Taras Zagibalov and John Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifica-
tion of chinese text. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, page 1073?1080.
581
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 13?18,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
QuickView: NLP-based Tweet Search
Xiaohua Liu ? ?, Furu Wei ?, Ming Zhou ?, Microsoft QuickView Team ?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?Microsoft Research Asia
Beijing, 100190, China
?{xiaoliu, fuwei, mingzhou,qv}@microsoft.com
Abstract
Tweets have become a comprehensive repos-
itory for real-time information. However, it
is often hard for users to quickly get informa-
tion they are interested in from tweets, ow-
ing to the sheer volume of tweets as well as
their noisy and informal nature. We present
QuickView, an NLP-based tweet search plat-
form to tackle this issue. Specifically, it ex-
ploits a series of natural language process-
ing technologies, such as tweet normalization,
named entity recognition, semantic role label-
ing, sentiment analysis, tweet classification, to
extract useful information, i.e., named entities,
events, opinions, etc., from a large volume
of tweets. Then, non-noisy tweets, together
with the mined information, are indexed, on
top of which two brand new scenarios are en-
abled, i.e., categorized browsing and advanced
search, allowing users to effectively access
either the tweets or fine-grained information
they are interested in.
1 Introduction
Tweets represent a comprehensive fresh informa-
tion repository. However, users often have diffi-
culty finding information they are interested in from
tweets, because of the huge number of tweets as well
as their noisy and informal nature. Tweet search,
e.g., Twitter 1, is a kind of service aiming to tackle
this issue. Nevertheless, existing tweet search ser-
vices provide limited functionality. For example, in
Twitter, only a simple keyword-based search is sup-
1http://twitter.com/
ported, and the returned list often contains meaning-
less results.
This demonstration introduces QuickView, which
employs a series of NLP technologies to extract
useful information from a large volume of tweets.
Specifically, for each tweet, it first conducts nor-
malization, followed by named entity recognition
(NER). Then it conducts semantic role labeling
(SRL) to get predicate-argument structures, which
are further converted into events, i.e., triples of who
did what. After that, it performs sentiment analysis
(SA), i.e., extracting positive or negative comments
about something/somebody. Next, tweets are clas-
sified into predefined categories. Finally, non-noisy
tweets together with the mined information are in-
dexed.
On top of the index, QuickView enables two brand
new scenarios, allowing users to effectively access
the tweets or fine-grained information mined from
tweets.
Categorized Browsing. As illustrated in Figure
1(a), QuickView shows recent popular tweets, enti-
ties, events, opinions and so on, which are organized
by categories. It also extracts and classifies URL
links in tweets and allows users to check out popular
links in a categorized way.
Advanced Search. As shown in Figure 1(b), Quick-
View provides four advanced search functions: 1)
search results are clustered so that tweets about the
same/similar topic are grouped together, and for
each cluster only the informative tweets are kept;
2) when the query refers to a person or a company,
two bars are presented followed by the words that
strongly suggest opinion polarity. The bar?s width
13
is proportional to the number of associated opin-
ions; 3) similarly, the top six most frequent words
that most clearly express event occurrences are pre-
sented; 4) users can search tweets with opinions
or events, e.g., search tweets containing any posi-
tive/negative opinion about ?Obama? or any event
involving ?Obama?.
The implementation of QuickView requires adapt-
ing existing NLP components trained on formal
texts, which often performs poorly on tweets. For
example, the average F1 of the Stanford NER
(Finkel et al, 2005) drops from 90.8% (Ratinov
and Roth, 2009) to 45.8% on tweets, while Liu et
al. (2010) report that the F1 score of a state-of-
the-art SRL system (Meza-Ruiz and Riedel, 2009)
falls to 42.5% on tweets as apposed to 75.5% on
news. However, the adaptation of those components
is challenging, owing to the lack of annotated tweets
and the inadequate signals provided by a noisy and
short tweet. Our general strategy is to leverage ex-
isting resources as well as unsupervised or semi-
supervised learning methods to reduce the labeling
efforts, and to aggregate as much evidence as pos-
sible from a broader context to compensate for the
lack of information in a tweet.
This strategy is embodied by various components
we have developed. For example, our NER com-
ponent combines a k-nearest neighbors (KNN) clas-
sifier, which collects global information across re-
cently labeled tweets with a Conditional Random
Fields (CRF) labeler, which exploits information
from a single tweet and the gazetteers. Both the
KNN classifier and the CRF labeler are repeatedly
retrained using the results that they have confidently
labeled. The SRL component caches and clusters
recent labeled tweets, and aggregates information
from the cluster containing the tweet. Similarly, the
classifier considers not only the current tweet but
also its neighbors in a tweet graph, where two tweets
are connected if they are similar in content or have a
tweet/retweet relationship.
QuickView has been internally deployed, and re-
ceived extremely positive feedback. Experimental
results on a human annotated dataset alo indicate
the effectiveness of our adaptation strategy.
Our contributions are summarized as follows.
1. We demonstrate QuickView, an NLP-based
tweet search. Different from existing methods,
it exploits a series of NLP technologies to ex-
tract useful information from a large volume
of tweets, and enables categorized browsing
and advanced search scenarios, allowing users
to efficiently access information they are inter-
ested in from tweets.
2. We present core components of QuickView, fo-
cusing on how to leverage existing resources
and technologies as well as how to make up
for the limited information in a short and often
noisy tweet by aggregating information from a
broader context.
The rest of this paper is organized as follows. In
the next section, we introduce related work. In Sec-
tion 3, we describe our system. In Section 4, we
evaluate our system. Finally, Section 5 concludes
and presents future work.
2 Related Work
Information Extraction Systems. Essentially,
QuickView is an information extraction (IE) system.
However, unlike existing IE systems, such as Evita
(Saur?? et al, 2005), a robust event recognizer for QA
system, and SRES (Rozenfeld and Feldman, 2008),
a self-supervised relation extractor for the web, it
targets tweets, a new genre of text, which are short
and informal, and its focus is on adapting existing IE
components to tweets.
Tweet Search Services. A couple of tweet search
services exist, including Twitter, Bing social search
2 and Google social search 3. Most of them provide
only keyword-based search interfaces, i.e., return-
ing a list of tweets related to a given word/phrase.
In contrast, our system extracts fine-grained in-
formation from tweets and allows a new end-to-
end search experience beyond keyword search, such
as clustering of search results, and search with
events/opinions.
NLP Components. The NLP technologies adopted
in our system , e.g., NER, SRL and classification,
have been extensively studied on formal text but
rarely on tweets. At the heart of our system is
the re-use of existing resources, methodologies as
2http://www.bing.com/social
3http://www.google.com/realtime
14
(a) A screenshot of the categorized browsing scenario.
(b) A screenshot of the advanced search scenario.
Figure 1: Two scenarios of QuickView.
well as components, and the the adaptation of them
to tweets. The adaptation process, though varying
across components, consists of three common steps:
1) annotating tweets; 2) defining the decision con-
text that usually involves more than one tweet, such
as a cluster of similar tweets; and 3) re-training mod-
els (often incrementally) with both conventional fea-
tures and features derived from the context defined
in step 2.
3 System Description
We first give an overview of our system, then present
more details about NER and SRL, as two represen-
tative core components, to illustrate the adaptation
process.
3.1 Overview
Architecture. QuickView can be divided into four
parts, as illustrated in Figure 2. The first part in-
cludes a crawler and a buffer of raw tweets. The
crawler repeatedly downloads tweets using the Twit-
ter APIs, and then pre-filters noisy tweets using
some heuristic rules, e.g., removing a tweet if it is
too short, say, less than 3 words, or if it contains
any predefined banned word. At the moment, we
focus on English tweets, so non-English tweets are
filtered as well. Finally, the un-filtered are put into
the buffer.
The second part consists of several tweet extrac-
tion pipelines. Each pipeline has the same configura-
tion, constantly fetching a tweet from the raw tweet
buffer, and conducting the following processes se-
15
Figure 2: System architecture of QuickView.
quentially: 1) normalization; 2) parsing including
part-of-speech (POS), chunking, and dependency
parsing; 3) NER; 4) SRL; 5) SA and 6) classifica-
tion. The normalization model identifies and cor-
rects ill-formed words. For example, after normal-
ization, ?loooove? in ?? ? ? I loooove my icon? ? ? ?
will be transformed to ?love?. A phrase-based trans-
lation system without re-ordering is used to imple-
ment this model. The translation table includes man-
ually compiled ill/good form pairs, and the language
model is a trigram trained on LDC data 4 using
SRILM (Stolcke, 2002). The OpenNLP 5 toolkit
is directly used to implement the parsing model.
In future, the parsing model will be re-trained us-
ing annotated tweets. The SA component is imple-
mented according to Jiang et al (2011), which incor-
porates target-dependent features and considers re-
lated tweets by utilizing a graph-based optimization.
The classification model is a KNN-based classifier
that caches confidently labeled results to re-train it-
self, which also recognizes and drops noisy tweets.
4http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-
logId=LDC2005T12
5http://sourceforge.net/projects/opennlp/
Each processed tweet, if not identified as noise, is
put into a shared buffer for indexing.
The third part is responsible for indexing and
querying. It constantly takes from the indexing
buffer a processed tweet, which is then indexed with
various entries including words, phrases, metadata
(e.g., source, publish time, and account), named en-
tities, events, and opinions. On top of this, it answers
any search request, and returns a list of matched re-
sults, each of which contains both the original tweet
and the extracted information from that tweet. We
implement an indexing/querying engine similar to
Lucene 6 in C#. This part also maintains a cache of
recent processed tweets, from which the following
information is extracted and indexed: 1) top tweets;
2) top entities/events/opinions in tweets; and 3)
top accounts. Whether a tweet/entity/event/opinion
ranks top depends on their re-tweeted/mentioned
times as well as its publisher, while whether an ac-
count is top relies on the number of his/her followers
and tweets.
The fourth part is a web application that returns
related information to end users according to their
browsing or search request. The implementation of
the web application is organized with the model-
view-control pattern so that other kinds of user in-
terfaces, e.g., a mobile application, can be easily im-
plemented.
Deployment. QuickView is deployed into 5 work-
stations 7 including 2 processing pipelines, as illus-
trated in Table 1. The communication between com-
ponents is through TCP/IP. On average, it takes 0.01
seconds to process each tweet, and in total about
10 million tweets are indexed every day. Note that
QuickView?s processing capability can be enhanced
in a straightforward manner by deploying additional
pipelines.
3.2 Core Components
Because of limited space, we only discuss two core
components of QuickView: NER and SRL.
NER. NER is the task of identifying mentions of
rigid designators from text belonging to named-
entity types such as persons, organizations and loca-
tions. Existing solutions fall into three categories: 1)
6http://lucene.apache.org/java/docs/index.html
7Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM,
OS of Windows Server 2003 EnterpriseX64 version
16
Table 1: Current deployment of QuickView.
Workstation Hosted components
#1 Crawler,Raw tweet buffer
#2, 3 Process pipeline
#4 Indexing Buffer, Indexer/Querier
#5 Web application
the rule-based (Krupka and Hausman, 1998); 2) the
machine learning based (Finkel and Manning, 2009;
Singh et al, 2010); and 3) hybrid methods (Jansche
and Abney, 2002). With the availability of annotated
corpora, such as ACE05, Enron and CoNLL03, the
data-driven methods become the dominating meth-
ods. However, because of domain mismatch, cur-
rent systems trained on non-tweets perform poorly
on tweets.
Our NER system takes three steps to address
this problem. Firstly, it defines those recently la-
beled tweets that are similar to the current tweet
as its recognition context, under which a KNN-
based classifier is used to conduct word level clas-
sification. Following the two-stage prediction ag-
gregation methods (Krishnan and Manning, 2006),
such pre-labeled results, together with other con-
ventional features used by the state-of-the-art NER
systems, are fed into a linear CRF models, which
conducts fine-grained tweet level NER. Secondly,
the KNN and CRF model are repeatedly retrained
with an incrementally augmented training set, into
which highly confidently labeled tweets are added.
Finally, following Lev Ratinov and Dan Roth
(2009), 30 gazetteers are used, which cover common
names, countries, locations, temporal expressions,
etc. These gazetteers represent general knowledge
across domains, and help to make up for the lack of
training data.
SRL. Given a sentence, the SRL component identi-
fies every predicate, and for each predicate further
identifies its arguments. This task has been exten-
sively studied on well-written corpora like news, and
a couple of solutions exist. Examples include: 1)
the pipelined approach, i.e., dividing the task into
several successive components such as argument
identification, argument classification, global infer-
ence, etc., and conquering them individually (Xue,
2004; Koomen et al, 2005); 2) sequentially labeling
based approach (Ma`rquez et al, 2005), i.e., label-
ing the words according to their positions relative
to an argument (i.e., inside, outside, or at the be-
ginning); and 3) Markov Logic Networks (MLN)
based approach (Meza-Ruiz and Riedel, 2009),
i.e., simultaneously resolving all the sub-tasks using
learnt weighted formulas. Unsurprisingly, the per-
formance of the state-of-the-art SRL system (Meza-
Ruiz and Riedel, 2009) drops sharply when applied
to tweets.
The SRL component of QuickView is based on
CRF, and uses the recently labeled tweets that are
similar to the current tweet as the broader context.
Algorithm 1 outlines its implementation, where:
train denotes a machine learning process to get a
labeler l, which in our work is a linear CRF model;
the cluster function puts the new tweet into a clus-
ter; the label function generates predicate-argument
structures for the input tweet with the help of the
trained model and the cluster; p, s and cf denote a
predicate, a set of argument and role pairs related to
the predicate and the predicted confidence, respec-
tively. To prepare the initial clusters required by the
SRL component as its input, we adopt the predicate-
argument mapping method (Liu et al, 2010) to
get some automatically labeled tweets, which (plus
the manually labeled tweets) are then organized into
groups using a bottom-up clustering procedure.
It is worth noting that: 1) our SRL component
uses the general role schema defined by PropBank,
which includes core roles such as A0, A1 (usually
indicating the agent and patient of the predicate, re-
spectively), and auxiliary roles such as AM-TMP
and AM-LOC (representing the temporal and loca-
tion information of the predicate, respectively); 2)
only verbal predicates are considered, which is con-
sistent with most existing SRL systems; and 3) fol-
lowing Ma`rquez et al (2005), it conducts word level
labeling.
4 Evaluation
Overall Performance. We provide a textbox in the
home page of QuickView to collect feedback. We
have got 165 feedbacks, of which 85.5% are posi-
tive. The main complaint is related to the quality of
the extracted information.
Core Components. We manually labeled the POS,
17
Algorithm 1 SRL of QuickView.
Require: Tweet stream i;clusters cl;output stream o.
1: Initialize l, the CRF labeler: l = train(cl).
2: while Pop a tweet t from i and t ?= null do
3: Put t to a cluster c: c = cluster(cl, t).
4: Label t with l:(t, {(p, s, cf)}) = label(l, c, t).
5: Update cluster c with labeled results
(t, {(p, s, cf)}).
6: Output labeled results (t, {(p, s, cf)}) to o.
7: end while
8: return o.
NER, SRL and SA information for about 10,000
tweets, based on which the NER and SRL com-
ponents are evaluated. Experimental results show
that: 1) our NER component achieves an average
F1 of 80.2%, as opposed to 75.4% of the baseline,
which is a CRF-based system similar to Ratinov and
Roth?s (2009) but re-trained on annotated tweets;
and 2) our SRL component gets an F1 of 59.7%, out-
performing both the state-of-the-art system (Meza-
Ruiz and Riedel, 2009) (42.5%) and the system of
Liu et al (2010) (42.3%), which is trained on au-
tomatically annotated news tweets (tweets reporting
news).
5 Conclusions and Future work
We have described the motivation, scenarios, archi-
tecture, deployment and implementation of Quick-
View, an NLP-based tweet search. At the heart of
QuickView is the adaptation of existing NLP tech-
nologies, e.g., NER, SRL and SA, to tweets, a new
genre of text, which are short and informal. We
have illustrated our strategy to tackle this challeng-
ing task, i.e., leveraging existing resources and ag-
gregating as much information as possible from a
broader context, using NER and SRL as case stud-
ies. Preliminary positive feedback suggests the use-
fulness of QuickView and its advantages over exist-
ing tweet search services. Experimental results on
a human annotated dataset indicate the effectiveness
of our adaptation strategy.
We are improving the quality of the core compo-
nents of QuickView by labeling more tweets and ex-
ploring alternative models. We are also customizing
QuickView for non-English tweets. As it progresses,
we will release QuickView to the public.
References
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In EMNLP, pages
141?150.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL, pages 363?370.
Martin Jansche and Steven P. Abney. 2002. Informa-
tion extraction from voicemail transcripts. In EMNLP,
pages 320?327.
Long Jiang, Mo Yu, Ming Zhou, and Xiaohua Liu. 2011.
Target-dependent twitter sentiment classification. In
ACL.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In CONLL, pages
181?184.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In ACL, pages
1121?1128.
George R. Krupka and Kevin Hausman. 1998. Isoquest:
Description of the netowlTM extractor system as used
in muc-7. In MUC-7.
Xiaohua Liu, Kuan Li, Bo Han, Ming Zhou, Long Jiang,
Zhongyang Xiong, and Changning Huang. 2010. Se-
mantic role labeling for news tweets. In Coling, pages
698?706.
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and Neus
Catala`. 2005. Semantic role labeling as sequential
tagging. In CONLL, pages 193?196.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In NAACL, pages 155?163.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147?155.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-
supervised relation extraction from the web. Knowl.
Inf. Syst., 17:17?33, October.
Roser Saur??, Robert Knippen, Marc Verhagen, and James
Pustejovsky. 2005. Evita: A robust event recognizer
for qa systems. In EMNLP, pages 700?707.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In HLT-NAACL, pages 73?81.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In ICSLP, volume 2, pages 901?904.
Nianwen Xue. 2004. Calibrating features for seman-
tic role labeling. In In Proceedings of EMNLP 2004,
pages 88?94.
18
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1304?1311,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Entity Linking for Tweets
Xiaohua Liu?, Yitong Li?, Haocheng Wu?, Ming Zhou?, Furu Wei?, Yi Lu?
?Microsoft Research Asia, Beijing, 100190, China
?School of Electronic and Information Engineering
Beihang University, Beijing, 100191, China
?University of Science and Technology of China
No. 96, Jinzhai Road, Hefei, Anhui, China
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
?{xiaoliu, mingzhou, fuwei}@microsoft.com
?tong91222@126.com ?v-haowu@microsoft.com ?v-y@microsoft.com
Abstract
We study the task of entity linking for
tweets, which tries to associate each
mention in a tweet with a knowledge base
entry. Two main challenges of this task are
the dearth of information in a single tweet
and the rich entity mention variations.
To address these challenges, we propose
a collective inference method that
simultaneously resolves a set of mentions.
Particularly, our model integrates three
kinds of similarities, i.e., mention-entry
similarity, entry-entry similarity, and
mention-mention similarity, to enrich
the context for entity linking, and to
address irregular mentions that are not
covered by the entity-variation dictionary.
We evaluate our method on a publicly
available data set and demonstrate the
effectiveness of our method.
1 Introduction
Twitter is a widely used social networking service.
With millions of active users and hundreds of
millions of new published tweets every day1,
it has become a popular platform to capture
and transmit the human experiences of the
moment. Many tweet related researches are
inspired, from named entity recognition (Liu et al,
2012), topic detection (Mathioudakis and Koudas,
2010), clustering (Rosa et al, 2010), to event
extraction (Grinev et al, 2009).
In this work, we study the entity linking task
for tweets, which maps each entity mention in
a tweet to a unique entity, i.e., an entry ID
of a knowledge base like Wikipedia. Entity
1http://siteanalytics.compete.com/twitter.com/
linking task is generally considered as a bridge
between unstructured text and structured machine-
readable knowledge base, and represents a critical
role in machine reading program (Singh et al,
2011). Entity linking for tweets is particularly
meaningful, considering that tweets are often hard
to read owing to its informal written style and
length limitation of 140 characters.
Current entity linking methods are built on top
of a large scale knowledge base such asWikipedia.
A knowledge base consists of a set of entities,
and each entity can have a variation list2. To
decide which entity should be mapped, they may
compute: 1) the similarity between the context of
a mention, e.g., a text window around the mention,
and the content of an entity, e.g., the entity page of
Wikipedia (Mihalcea and Csomai, 2007; Han and
Zhao, 2009); 2) the coherence among the mapped
entities for a set of related mentions, e.g, multiple
mentions in a document (Milne and Witten, 2008;
Kulkarni et al, 2009; Han and Zhao, 2010; Han et
al., 2011).
Tweets pose special challenges to entity linking.
First, a tweet is often too concise and too
noisy to provide enough information for similarity
computing, owing to its short and grass root
nature. Second, tweets have rich variations of
named entities3, and many of them fall out of
the scope of the existing dictionaries mined from
Wikipedia (called OOV mentions hereafter). On
2Entity variation lists can be extracted from the
entity resolution pages of Wikipedia. For example, the
link ?http://en.wikipedia.org/wiki/Svm? will lead us to a
resolution page, where ?Svm? are linked to entities like
?Space vector modulation? and ?Support vector machine?.
As a result, ?Svm? will be added into the variation lists of
?Space vector modulation? and ?Support vector machine? ,
respectively.
3According to Liu et al (2012), on average a named entity
has 3.3 different surface forms in tweets.
1304
the other hand, the huge redundancy in tweets
offers opportunities. That means, an entity
mention often occurs in many tweets, which
allows us to aggregate all related tweets to
compute mention-mention similarity and mention-
entity similarity.
We propose a collective inference method
that leverages tweet redundancy to address those
two challenges. Given a set of mentions, our
model tries to ensure that similar mentions are
linked to similar entities while pursuing the
high total similarity between matched mention-
entity pairs. More specifically, we define
local features, including context similarity and
edit distance, to model the similarity between
a mention and an entity. We adopt in-link
based similarity (Milne and Witten, 2008), to
measure the similarity between entities. Finally,
we introduce a set of features to compute
the similarity between mentions, including how
similar the tweets containing the mentions are,
whether they come from the tweets of the same
account, and their edit distance. Notably, our
model can resolve OOV mentions with the help
of their similar mentions. For example, for the
OOVmention ?LukeBryanOnline?, our model can
find similar mentions like ?TheLukeBryan? and
?LukeBryan?. Considering that most of its similar
mentions are mapped to the American country
singer ?Luke Bryan?, our model tends to link
?LukeBryanOnline? to the same entity.
We evaluate our method on the public available
data set shared by Meij et al (2012)4.
Experimental results show that our method
outperforms two baselines, i.e., Wikify! (Mihalcea
and Csomai, 2007) and system proposed by Meij
et al (2012). We also study the effectiveness
of features related to each kind of similarity, and
demonstrate the advantage of our method for OOV
mention linkage.
We summarize our contributions as follows.
1. We introduce a novel collective inference
method that integrates three kinds of
similarities, i.e., mention-entity similarity,
entity-entity similarity, and mention-mention
similarity, to simultaneously map a set of
tweet mentions to their proper entities.
2. We propose modeling the mention-mention
similarity and demonstrate its effectiveness
4http://ilps.science.uva.nl/resources/wsdm2012-adding-
semantics-to-microblog-posts/
in entity linking for tweets, particularly for
OOV mentions.
3. We evaluate our method on a public data
set, and show our method compares favorably
with the baselines.
Our paper is organized as follows. In the next
section, we introduce related work. In Section
3, we give the formal definition of the task. In
Section 4, we present our solution, including
the framework, features related to different kinds
of similarities, and the training and decoding
procedures. We evaluate our method in Section 5.
Finally in Section 6, we conclude with suggestions
of future work.
2 Related Work
Existing entity linking work can roughly be
divided into two categories. Methods of the
first category resolve one mention at each time,
and mainly consider the similarity between
a mention-entity pair. In contrast, methods
of the second category take a set of related
mentions (e.g., mentions in the same document)
as input, and figure out their corresponding entities
simultaneously.
Examples of the first category include the first
Web-scale entity linking system SemTag (Dill
et al, 2003), Wikify! (Mihalcea and Csomai,
2007), and the recent work of Milne and Witten
(2008). SemTag uses the TAP knowledge
base5, and employs the cosine similarity with
TF-IDF weighting scheme to compute the
match degree between a mention and an entity,
achieving an accuracy of around 82%. Wikify!
identifies the important concepts in the text
and automatically links these concepts to the
corresponding Wikipedia pages. It introduces two
approaches to define mention-entity similarity,
i.e., the contextual overlap between the paragraph
where the mention occurs and the corresponding
Wikipedia pages, and a Naive Bayes classifier
that predicts whether a mention should be linked
to an entity. It achieves 80.69% F1 when two
approaches are combined. Milne and Witten
work on the same task of Wikify!, and also
train a classifier. However, they cleverly use the
5TAB (http://www.w3.org/2002/05/tap/) is a shallow
knowledge base that contains a broad range of lexical and
taxonomic information about popular objects like music,
movies, authors, sports, autos, health, etc.
1305
links found within Wikipedia articles for training,
exploiting the fact that for every link, aWikipedian
has manually selected the correct destination to
represent the intended sense of the anchor. Their
method achieves an F1 score of 75.0%.
Representative studies of the second category
include the work of Kulkarni et al (2009),
Han et al (2011), and Shen et al (2012).
One common feature of these studies is that
they leverage the global coherence between
entities. Kulkarni et al (2009) propose
a graphical model that explicitly models the
combination of evidence from local mention-
entity compatibility and global document-level
topical coherence of the entities, and show that
considering global coherence between entities
significantly improves the performance. Han et
al. (2011) introduce a graph-based representation,
called Referent Graph, to model the global
interdependence between different entity linking
decisions, and jointly infer the referent entities of
all name mentions in a document by exploiting
the interdependence captured in Referent Graph.
Shen et al (2012) propose LIEGE, a framework
to link the entities in web lists with the knowledge
base, with the assumption that entities mentioned
in a Web list tend to be a collection of entities of
the same conceptual type.
Most work of entity linking focuses on web
pages. Recently, Meij et al (2012) study
this task for tweets. They propose a machine
learning based approach using n-gram features,
concept features, and tweet features, to identify
concepts semantically related to a tweet, and
for every entity mention to generate links to its
corresponding Wikipedia article. Their method
belongs to the first category, in the sense that
they only consider the similarity between mention
(tweet) and entity (Wikipedia article).
Our method belongs to the second category.
However, in contrast with existing collective
approaches, our method works on tweets which
are short and often noisy. Furthermore, our
method is based on the ?similar mention with
similar entity? assumption, and explicitly models
and integrates the mention similarity into the
optimization framework. Compared with Meij et
al. (2012), our method is collective, and integrates
more features.
3 Task Definition
Given a sequence of mentions, denoted by
M? = (m1,m2, ? ? ? ,mn), our task is to
output a sequence of entities, denoted by
E? = (e1, e2, ? ? ? , en), where ei is the entity
corresponding to mi. Here, an entity refers
to an item of a knowledge base. Following
most existing work, we use Wikipedia as the
knowledge base, and an entity is a definition page
in Wikipedia; a mention denotes a sequence of
tokens in a tweet that can be potentially linked to
an entity.
Several notes should be made. First, we
assume that mentions are given, e.g., identified by
some named entity recognition system. Second,
mentions may come from multiple tweets. Third,
mentions with the same token sequence may
refer to different entities, depending on mention
context. Finally, we assume each entity e has
a variation list6, and a unique ID through which
all related information about that entity can be
accessed.
Here is an example to illustrate the task. Given
mentions ?nbcbightlynews?, ?Santiago?, ?WH?
and ?Libya? from the following tweet ?Chuck
Todd: Prepping for @nbcnightlynews here in
Santiago, reporting on WH handling of Libya
situation.?, the expected output is ?NBC Nightly
News(194735)?, ?Santiago Chile(51572)?,
?White House(33057)? and ?Libya(17633)?,
where the numbers in the parentheses are the IDs
of the corresponding entities.
4 Our Method
In this section, we first present the framework of
our entity linking method. Then we introduce
features related to different kinds of similarities,
followed by a detailed discussion of the training
and decoding procedures.
4.1 Framework
Given the input mention sequence M? =
(m1,m2, ? ? ? ,mn), our method outputs the entity
sequence E?? = (e?1, e?2, ? ? ? , e?n) according to
Formula 1:
6For example, the variation list of the entity ?Obama? may
contain ?Barack Obama?, ?Barack Hussein Obama II?, etc.
1306
E?? = argmax?E??C(M?)?
n?
i=1
w? ? f?(ei,mi)
+(1 ? ?)
?
i ?=j
r(ei, ej)s(mi,mj)
(1)
Where:
? C(M?) is the set of all possible entity
sequences for the mention sequence M? ;
? E? denotes an entity sequence instance,
consisting of e1, e2, ? ? ? , en;
? f?(ei,mi) is the feature vector that models the
similarity between mention mi and its linked
entity ei;
? w? is the feature weight vector related to f? ,
which is trained on the training data set; w? ?
f?(ei,mi) is the similarity between mention
mi and entity ei;
? r(ei, ej) is the function that returns the
similarity between two entities ei and ej ;
? s(mi,mj) is the function that returns the
similarity between two mentions mi and mj ;
? ? ? (0, 1) is a systematic parameter, which
is determined on the development data set; it
is used to adjust the tradeoff between local
compatibility and global consistence. It is
experimentally set to 0.8 in our work.
From Formula 1, we can see that: 1) our
method considers the mention-entity similarly,
entity-entity similarity and mention-mention
similarity. Mention-entity similarly is used to
model local compatibility, while entity-entity
similarity and mention-mention similarity
combined are to model global consistence; and 2)
our method prefers configurations where similar
mentions have similar entities and with high local
compatibility.
C(M?) is worth of more discussion here.
It represents the search space, which can be
generated using the entity variation list. To
achieve this, we first build an inverted index
of all entity variation lists, with each unique
variation as an entry pointing to a list of entities.
Then for any mention m, we look up the index,
and get al possible entities, denoted by C(m).
In this way, given a mention sequence M? =
(m1,m2, ? ? ? ,mn), we can enumerate all possible
entity sequence E? = (e1, e2, ? ? ? , en), where ei ?
C(m). This means |C(M?)| = ?m?M |C(m)| ,
which is often large. There is one special case:
if m is an OOV mention, i.e., |C(m)| = 0, then
|C(M?)| = 0, and we get no solution. To address
this problem, we can generate a list of candidates
for an OOV mention using its similar mentions.
Let S(m) denote OOV mention m?s similar
mentions, we define C(m) = ?m??S(m) C(m
?).
If still C(m) = 0, we remove m from M? , and
report we cannot map it to any entity.
Here is an example to illustrate our framework.
Suppose we have the following tweets:
? UserA: Yeaaahhgg #habemusfut..
I love monday night futbol =)
#EnglishPremierLeague ManU vs
Liverpool1
? UserA: Manchester United 3 - Liverpool2
2 #EnglishPremierLeague GLORY, GLORY,
MAN.UNITED!
? ? ? ?
Figure 1: An illustrative example to show our
framework. Ovals in orange and in blue represent
mentions and entities, respectively. Each mention
pair, entity pair, and mention entity pair have
a similarity score represented by s, r and f ,
respectively.
We need find out the best entity sequence
E?? for mentions M? = { ?Liverpool1?,
?Manchester United?, ?ManU?, ?Liverpool2?},
from the entity sequences C(M?) = { (Liverpool
(film), Manchester United F.C., Manchester
United F.C., Liverpool (film)), ? ? ? , (Liverpool,
F.C.,Manchester United, F.C., Manchester United
F.C., Liverpool (film) }. Figure 1 illustrate
our solution, where ?Liverpool1? (on the left)
and ?Liverpool2? (on the right) are linked
1307
to ?Liverpool F.C.? (the football club), and
?Manchester United? and ?ManU? are linked to
?Manchester United F.C.?. Notably, ?ManU?
is an OOV mention, but has a similar mention
?Manchester United?, with which ?ManU? is
successfully mapped.
4.2 Features
We group features into three categories: local
features related to mention-entity similarity
(f?(e,m)), features related to entity-entity
similarity (r(ei, ej)) , and features related to
mention-mention similarity (s(mi,mj)).
4.2.1 Local Features
? Prior Probability:
f1(mi, ei) =
count(ei)?
?ek?C(mi) count(ek)
(2)
where count(e) denotes the frequency of
entity e in Wikipedia?s anchor texts.
? Context Similarity:
f2(mi, ei) =
coocurence number
tweet length (3)
where: coccurence number is the the
number of the words that occur in both the
tweet containing mi and the Wikipedia page
of ei; tweet length denotes the number of
tokens of the tweet containing mention mi.
? Edit Distance Similarity:
IfLength(mi)+ED(mi, ei) = Length(ei),
f3(mi, ei) = 1, otherwise 0. ED(?, ?)
computes the character level edit distance.
This feature helps to detect whether
a mention is an abbreviation of its
corresponding entity7.
? Mention Contains Title: If the mention
contains the entity title, namely the title of
the Wikipedia page introducing the entity ei,
f4(mi, ei) = 1, else 0.
? Title Contains Mention: If the entry title
contains the mention, f5(mi, ei) = 1,
otherwise 0.
7Take ?ms? and ?Microsoft? for example. The length of
?ms? is 2, and the edit distance between them is 7. 2 plus 7
equals to 9, which is the length of ?Microsoft?.
4.2.2 Features Related to Entity Similarity
There are two representative definitions of entity
similarity: in-link based similarity (Milne and
Witten, 2008) and category based similarity (Shen
et al, 2012). Considering that the Wikipedia
categories are often noisy (Milne and Witten,
2008), we adopt in-link based similarity, as
defined in Formula 4:
r(ei, ej) =
log|g(ei) ? g(ej)| ? log max(|g(ei)|, |g(ej)|)
log(Total)? log min(|g(ei)|, |g(ej)|)
(4)
Where:
? Total is the total number of knowledge base
entities;
? g(e) is the number of Wikipedia definition
pages that have a link to entity e.
4.2.3 Features Related to Mention Similarity
We define 5 features to model the similarity
between two mentions mi and mj , as listed
below, where t(m) denotes the tweet that contains
mention m:
? s1(mi,mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as TF-
IDF vectors;
? s2(mi,mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as
topic distribution vectors;
? s3(mi,mj): Whether t(mi) and t(mj) are
published by the same account;
? s4(mi,mj): Whether t(mi) and t(mj)
contain any common hash tag;
? s5(mi,mj): Edit distance related similarity
between mi and mj , as defined in Formula 5.
s5(mi,mj) = 1, if min{Length(mi), Length(mj)}
+ED(mi,mj) = max{Length(mi), Length(mj)},
else s5(mi,mj) = 1 ? ED(mi,mj)max{Length(mi), Length(mj)}
(5)
Note that: 1) before computing TF-IDF vectors,
stop words are removed; 2) we use the Stanford
Topic Modeling Toolbox8 to compute the topic
model, and experimentally set the number of
topics to 50.
8http://nlp.stanford.edu/software/tmt/tmt-0.4/
1308
Finally, Formula 6 is used to integrate all the
features. a? = (a1, a2, a3, a4, a5) is the feature
weight vector for mention similarity, where ak ?
(0, 1), k = 1, 2, 3, 4, 5, and?5k=1 ak = 1.
s(mi,mj) =
5?
k=1
aksk(mi,mj) (6)
4.3 Training and Decoding
Given n mentions m1,m2, ? ? ? ,mn and their
corresponding entities e1, e2, ? ? ? , en, the goal of
training is to determine: w??, the weights of local
features, and a??, the weights of the features related
to mention similarity, according to Formula 7 9.
(w??, a??) = arg minw?,?a{
1
n
n?
i=1
L1(ei,mi)
+?1||w?||2 +
?2
2
n?
i,j=1
s(mi,mj)L2(?a, ei, ej)}
(7)
Where:
? L1 is the loss function related to local
compatibility, which is defined as
1
w??f?(ei,mi)+1
;
? L2(?a, ei, ej) is the loss function related
to global coherence, which is defined as
1
r(ei,ej)
?5
k=1 aksk(mi,mj)+1
;
? ?1 is the weight of regularization, which is
experimentally set to 1.0;
? ?2 is the weight of L2 loss, which is
experimentally set to 0.2.
Since the decoding problem defined by
Formula 1 is NP hard (Kulkarni et al, 2009), we
develop a greedy hill-climbing approach to tackle
this challenge, as demonstrated in Algorithm 1.
In Algorithm 1, it is the number of iterations;
Score(E?, M?) = ??ni=1 w? ? f?(ei,mi) + (1 ?
?)
?
i?=j r(ei, ej)s(mi,mj); E?ij is the vector after
replacing ei with ej ? C(mi) for current E?;
scij is the score of E?ij , i.e., Score(E?ij , M?). In
each iteration, this rounding solution iteratively
substitute entry ei in E? to increase the total score
cur. If the score cannot be further improved, it
stops and returns current E?.
9This optimization problem is non-convex. We use
coordinate descent to get a local optimal solution.
Algorithm 1 Decoding Algorithm.
Input: Mention Set M? = (m1,m2, ? ? ? ,mn)
Output: Entity Set E? = (e1, e2, ? ? ? , en)
1: for i = 1 to n do
2: Initialize e(0)i as the entity with the largest prior
probability given mention mi.
3: end for
4: cur = Score(E?(0), M?)
5: it = 1
6: while true do
7: for i = 1 to n do
8: for ej ? C(mi) do
9: if ej ?= e(it?1)i then
10: E?(it)ij = E?(it?1) ? {e(it?1)i } + {ej}.
11: end if
12: scij = Score(E?(it)ij , M?).
13: end for
14: end for
15: (l,m) = argmax(i,j)scij .
16: sc? = sclm
17: if sc? > cur then
18: cur = sc?.
19: E?(it) = E?(it?1) ? {e(it?1)l } + {em}.
20: it = it + 1.
21: else
22: break
23: end if
24: end while
25: return E?(it).
5 Experiments
In this section, we introduce the data set and
experimental settings, and present results.
5.1 Data Preparation
Following most existing studies, we choose
Wikipedia as our knowledge base10. We index
the Wikipedia definition pages, and prepare all
required prior knowledge, such as count(e), g(e),
and entity variation lists. We also build an inverted
index with about 60 million entries for the entity
variation lists.
For tweets, we use the data set shared by Meij et
al. (2012)11. This data set is annotated manually
by two volunteers. We get 502 annotated tweets
from this data set. We keep 55 of them for
10We download the December 2012 version of Wikipedia,
which contains about four million articles.
11http://ilps.science.uva.nl/resources/wsdm2012-adding-
semantics-to-microblog-posts/.
1309
development, and the remaining for 5 fold cross-
validation.
5.2 Settings
We consider following settings to evaluate our
method.
? Comparing our method with two baselines,
i.e., Wikify! (Mihalcea and Csomai, 2007)
and the system proposed byMeij et al (2012)
12;
? Using only local features;
? Using various mention similarity features;
? Experiments on OOV mentions.
5.3 Results
Table 1 reports the comparison results. Our
method outperforms both systems in terms of
all metrics. Since the main difference between
our method and the baselines is that our method
considers not only local features, but also global
features related to entity similarity and mention
similarity, these results indicate the effectiveness
of collective inference and global features. For
example, we find two baselines incorrectly link
?Nickelodeon? in the tweet ?BOH will make a
special appearance on Nickelodeon?s ?Yo Gabba
Gabba? tomorrow? to the theater instead of a TV
channel. In contrast, our method notices that ?Yo
Gabba Gabba? in the same tweet can be linked
to ?Yo Gabba Gabba (TV show)?, and thus it
correctly maps ?Nickelodeon? to ?Nickelodeon
(TV channel)?.
System Pre. Rec. F1
Wikify! 0.375 0.421 0.396
Meij?s Method 0.734 0.632 0.679
Our Method 0.752 0.675 0.711
Table 1: Comparison with Baselines.
Table 2 shows the results when local features
are incrementally added. It can be seen that:
1) using only Prior Probability feature already
yields a reasonable F1; and 2) Context Similarity
and Edit Distance Similarity feature have little
contribution to the F1, while Mention and Entity
Title Similarity feature greatly boosts the F1.
12We re-implement Wikify! since we use a new evaluation
data set.
Local Feature Pre. Rec. F1
P.P. 0.700 0.599 0.646
+C.S. 0.694 0.597 0.642
+E.D.S. 0.696 0.598 0.643
+M.E.T.S. 0.735 0.632 0.680
Table 2: Local Feature Analysis. P.P.,C.S., E.D.S.,
and M.E.T.S. denote Prior Probability, Context
Similarity, Edit Distance Similarity, and Mention
and Entity Title Similarity, respectively.
The performance of our method with various
mention similarity features is reported in Table 3.
First, we can see that with this kind of features,
the F1 can be significantly improved from 0.680
to 0.704. Second, we notice that TF-IDF (s1) and
Topic Model (s2) features perform equally well,
and combining all mention similarity features
yields the best performance.
Global Feature Pre. Rec. F1
s3+s4+s5 0.744 0.653 0.700
s3+s4+s5 +s1 0.759 0.652 0.702
s3+s4+s5+s2 0.760 0.653 0.703
s3+s4+s5+s1+s2 0.764 0.653 0.704
Table 3: Mention Similarity Feature Analysis.
For any OOV mention, we use the strategy
of guessing its possible entity candidates using
similar mentions, as discussed in Section 4.1.
Table 4 shows the performance of our system for
OOV mentions. It can be seen that with our
OOV strategy, the recall is improved from 0.653
to 0.675 (with p < 0.05) while the Precision is
slightly dropped and the overall F1 still gets better.
A further study reveals that among all the 125
OOV mentions, there are 48 for which our method
cannot find any entity; and nearly half of these
48 OOV mentions do have corresponding entities
13. This suggests that we may need enlarge the
size of variation lists or develop some mention
normalization techniques.
OOV Method Precision Recall F1
Ignore OOV Mention 0.764 0.653 0.704
+ OOV Method 0.752 0.675 0.711
Table 4: Performance for OOV Mentions.
13?NATO-ukraine cooperations? is such an example. It
is mapped to NULL but actually has a corresponding entity
?Ukraine-NATO relations?
1310
6 Conclusions and Future work
We have presented a collective inference method
that jointly links a set of tweet mentions to
their corresponding entities. One distinguished
characteristic of our method is that it integrates
mention-entity similarity, entity-entity similarity,
and mention-mention similarity, to address the
information lack in a tweet and rich OOV
mentions. We evaluate our method on a
public data set. Experimental results show our
method outperforms two baselines, and suggests
the effectiveness of modeling mention-mention
similarity, particularly for OOV mention linking.
In the future, we plan to explore two directions.
First, we are going to enlarge the size of entity
variation lists. Second, we want to integrate
the entity mention normalization techniques as
introduced by Liu et al (2012).
Acknowledgments
We thank the anonymous reviewers for their
valuable comments. We also thank all the
QuickView team members for the helpful
discussions.
References
S. Dill, N. Eiron, D. Gibson, D. Gruhl, and R. Guha.
2003. Semtag and seeker: bootstrapping the
semantic web via automated semantic annotation. In
Proceedings of the 12th international conference on
World Wide Web, WWW ?03, pages 178?186, New
York, NY, USA. ACM.
Maxim Grinev, Maria Grineva, Alexander Boldakov,
Leonid Novak, Andrey Syssoev, and Dmitry
Lizorkin. 2009. Sifting micro-blogging stream for
events of user interest. In Proceedings of the 32nd
international ACM SIGIR conference on Research
and development in information retrieval, SIGIR
?09, pages 837?837, New York, NY, USA. ACM.
Xianpei Han and Jun Zhao. 2009. Nlpr-kbp in tac 2009
kbp track: A two-stage method to entity linking. In
Proceedings of Test Analysis Conference.
Xianpei Han and Jun Zhao. 2010. Structural
semantic relatedness: a knowledge-based method
to named entity disambiguation. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: A graph-based method.
In SIGIR?11.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective
annotation of wikipedia entities in web text.
In Proceedings of the 15th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 457?465.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012. Joint inference
of named entity recognition and normalization for
tweets. In ACL (1), pages 526?535.
Michael Mathioudakis and Nick Koudas. 2010.
Twittermonitor: trend detection over the twitter
stream. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
SIGMOD ?10, pages 1155?1158, New York, NY,
USA. ACM.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts.
In Proceedings of the fifth ACM international
conference on Web search and data mining.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge.
In Proceedings of the sixteenth ACM conference
on Conference on information and knowledge
management, CIKM ?07, pages 233?242, NewYork,
NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning
to link with wikipedia. In Proceeding of the 17th
ACM conference on Information and knowledge
management.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole
Gershman, and Robert Frederking. 2010. Topical
clustering of tweets. In SWSM?10.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Liege: Link entities in web lists with
knowledge base. In KDD?12.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-
scale cross-document coreference using distributed
inference and hierarchical models. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 793?
803, Stroudsburg, PA, USA. Association for
Computational Linguistics.
1311
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555?1565,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Nan Yang
\
, Ming Zhou
?
, Ting Liu
?
, Bing Qin
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
\
University of Science and Technology of China, Hefei, China
{dytang, tliu, qinb}@ir.hit.edu.cn
{fuwei, v-nayang, mingzhou}@microsoft.com
Abstract
We present a method that learns word em-
bedding for Twitter sentiment classifica-
tion in this paper. Most existing algorithm-
s for learning continuous word represen-
tations typically only model the syntactic
context of words but ignore the sentimen-
t of text. This is problematic for senti-
ment analysis as they usually map word-
s with similar syntactic context but oppo-
site sentiment polarity, such as good and
bad, to neighboring word vectors. We
address this issue by learning sentiment-
specific word embedding (SSWE), which
encodes sentiment information in the con-
tinuous representation of words. Specif-
ically, we develop three neural networks
to effectively incorporate the supervision
from sentiment polarity of text (e.g. sen-
tences or tweets) in their loss function-
s. To obtain large scale training corpora,
we learn the sentiment-specific word em-
bedding from massive distant-supervised
tweets collected by positive and negative
emoticons. Experiments on applying SS-
WE to a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that (1) the SSWE feature performs
comparably with hand-crafted features in
the top-performed system; (2) the perfor-
mance is further improved by concatenat-
ing SSWE with existing feature set.
1 Introduction
Twitter sentiment classification has attracted in-
creasing research interest in recent years (Jiang et
al., 2011; Hu et al, 2013). The objective is to clas-
sify the sentiment polarity of a tweet as positive,
?
This work was done when the first and third authors
were visiting Microsoft Research Asia.
negative or neutral. The majority of existing ap-
proaches follow Pang et al (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
designing effective features to obtain better clas-
sification performance. For example, Mohammad
et al (2013) build the top-performed system in the
Twitter sentiment classification track of SemEval
2013 (Nakov et al, 2013), using diverse sentiment
lexicons and a variety of hand-crafted features.
Feature engineering is important but labor-
intensive. It is therefore desirable to discover ex-
planatory factors from the data and make the learn-
ing algorithms less dependent on extensive fea-
ture engineering (Bengio, 2013). For the task of
sentiment classification, an effective feature learn-
ing method is to compose the representation of a
sentence (or document) from the representation-
s of the words or phrases it contains (Socher et
al., 2013b; Yessenalina and Cardie, 2011). Ac-
cordingly, it is a crucial step to learn the word
representation (or word embedding), which is a
dense, low-dimensional and real-valued vector for
a word. Although existing word embedding learn-
ing algorithms (Collobert et al, 2011; Mikolov et
al., 2013) are intuitive choices, they are not effec-
tive enough if directly used for sentiment classi-
fication. The most serious problem is that tradi-
tional methods typically model the syntactic con-
text of words but ignore the sentiment information
of text. As a result, words with opposite polari-
ty, such as good and bad, are mapped into close
vectors. It is meaningful for some tasks such as
pos-tagging (Zheng et al, 2013) as the two words
have similar usages and grammatical roles, but it
becomes a disaster for sentiment analysis as they
have the opposite sentiment polarity.
In this paper, we propose learning sentiment-
specific word embedding (SSWE) for sentiment
analysis. We encode the sentiment information in-
1555
to the continuous representation of words, so that
it is able to separate good and bad to opposite ends
of the spectrum. To this end, we extend the ex-
isting word embedding learning algorithm (Col-
lobert et al, 2011) and develop three neural net-
works to effectively incorporate the supervision
from sentiment polarity of text (e.g. sentences
or tweets) in their loss functions. We learn the
sentiment-specific word embedding from tweet-
s, leveraging massive tweets with emoticons as
distant-supervised corpora without any manual an-
notations. These automatically collected tweet-
s contain noises so they cannot be directly used
as gold training data to build sentiment classifier-
s, but they are effective enough to provide weak-
ly supervised signals for training the sentiment-
specific word embedding.
We apply SSWE as features in a supervised
learning framework for Twitter sentiment classi-
fication, and evaluate it on the benchmark dataset
in SemEval 2013. In the task of predicting posi-
tive/negative polarity of tweets, our method yields
84.89% in macro-F1 by only using SSWE as fea-
ture, which is comparable to the top-performed
system based on hand-crafted features (84.70%).
After concatenating the SSWE feature with ex-
isting feature set, we push the state-of-the-art to
86.58% in macro-F1. The quality of SSWE is al-
so directly evaluated by measuring the word sim-
ilarity in the embedding space for sentiment lexi-
cons. In the accuracy of polarity consistency be-
tween each sentiment word and its top N closest
words, SSWE outperforms existing word embed-
ding learning algorithms.
The major contributions of the work presented
in this paper are as follows.
? We develop three neural networks to learn
sentiment-specific word embedding (SSWE)
from massive distant-supervised tweets with-
out any manual annotations;
? To our knowledge, this is the first work that
exploits word embedding for Twitter senti-
ment classification. We report the results that
the SSWE feature performs comparably with
hand-crafted features in the top-performed
system in SemEval 2013;
? We release the sentiment-specific word em-
bedding learned from 10 million tweets,
which can be adopted off-the-shell in other
sentiment analysis tasks.
2 Related Work
In this section, we present a brief review of the
related work from two perspectives, Twitter senti-
ment classification and learning continuous repre-
sentations for sentiment classification.
2.1 Twitter Sentiment Classification
Twitter sentiment classification, which identifies
the sentiment polarity of short, informal tweets,
has attracted increasing research interest (Jiang et
al., 2011; Hu et al, 2013) in recent years. Gen-
erally, the methods employed in Twitter sentiment
classification follow traditional sentiment classifi-
cation approaches. The lexicon-based approaches
(Turney, 2002; Ding et al, 2008; Taboada et al,
2011; Thelwall et al, 2012) mostly use a dictio-
nary of sentiment words with their associated sen-
timent polarity, and incorporate negation and in-
tensification to compute the sentiment polarity for
each sentence (or document).
The learning based methods for Twitter sen-
timent classification follow Pang et al (2002)?s
work, which treat sentiment classification of texts
as a special case of text categorization issue. Many
studies on Twitter sentiment classification (Pak
and Paroubek, 2010; Davidov et al, 2010; Bar-
bosa and Feng, 2010; Kouloumpis et al, 2011;
Zhao et al, 2012) leverage massive noisy-labeled
tweets selected by positive and negative emoticon-
s as training set and build sentiment classifiers di-
rectly, which is called distant supervision (Go et
al., 2009). Instead of directly using the distant-
supervised data as training set, Liu et al (2012)
adopt the tweets with emoticons to smooth the lan-
guage model and Hu et al (2013) incorporate the
emotional signals into an unsupervised learning
framework for Twitter sentiment classification.
Many existing learning based methods on Twit-
ter sentiment classification focus on feature engi-
neering. The reason is that the performance of sen-
timent classifier being heavily dependent on the
choice of feature representation of tweets. The
most representative system is introduced by Mo-
hammad et al (2013), which is the state-of-the-
art system (the top-performed system in SemEval
2013 Twitter Sentiment Classification Track) by
implementing a number of hand-crafted features.
Unlike the previous studies, we focus on learning
discriminative features automatically from mas-
sive distant-supervised tweets.
1556
2.2 Learning Continuous Representations for
Sentiment Classification
Pang et al (2002) pioneer this field by using bag-
of-word representation, representing each word as
a one-hot vector. It has the same length as the size
of the vocabulary, and only one dimension is 1,
with all others being 0. Under this assumption,
many feature learning algorithms are proposed to
obtain better classification performance (Pang and
Lee, 2008; Liu, 2012; Feldman, 2013). However,
the one-hot word representation cannot sufficient-
ly capture the complex linguistic characteristics of
words.
With the revival of interest in deep learn-
ing (Bengio et al, 2013), incorporating the con-
tinuous representation of a word as features has
been proving effective in a variety of NLP tasks,
such as parsing (Socher et al, 2013a), language
modeling (Bengio et al, 2003; Mnih and Hin-
ton, 2009) and NER (Turian et al, 2010). In the
field of sentiment analysis, Bespalov et al (2011;
2012) initialize the word embedding by Laten-
t Semantic Analysis and further represent each
document as the linear weighted of ngram vec-
tors for sentiment classification. Yessenalina and
Cardie (2011) model each word as a matrix and
combine words using iterated matrix multiplica-
tion. Glorot et al (2011) explore Stacked Denois-
ing Autoencoders for domain adaptation in sen-
timent classification. Socher et al propose Re-
cursive Neural Network (RNN) (2011b), matrix-
vector RNN (2012) and Recursive Neural Tensor
Network (RNTN) (2013b) to learn the composi-
tionality of phrases of any length based on the
representation of each pair of children recursively.
Hermann et al (2013) present Combinatory Cate-
gorial Autoencoders to learn the compositionality
of sentence, which marries the Combinatory Cat-
egorial Grammar with Recursive Autoencoder.
The representation of words heavily relies on
the applications or tasks in which it is used (Lab-
utov and Lipson, 2013). This paper focuses
on learning sentiment-specific word embedding,
which is tailored for sentiment analysis. Un-
like Maas et al (2011) that follow the proba-
bilistic document model (Blei et al, 2003) and
give an sentiment predictor function to each word,
we develop neural networks and map each n-
gram to the sentiment polarity of sentence. Un-
like Socher et al (2011c) that utilize manually
labeled texts to learn the meaning of phrase (or
sentence) through compositionality, we focus on
learning the meaning of word, namely word em-
bedding, from massive distant-supervised tweets.
Unlike Labutov and Lipson (2013) that produce
task-specific embedding from an existing word
embedding, we learn sentiment-specific word em-
bedding from scratch.
3 Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
In this section, we present the details of learn-
ing sentiment-specific word embedding (SSWE)
for Twitter sentiment classification. We pro-
pose incorporating the sentiment information of
sentences to learn continuous representations for
words and phrases. We extend the existing word
embedding learning algorithm (Collobert et al,
2011) and develop three neural networks to learn
SSWE. In the following sections, we introduce the
traditional method before presenting the details of
SSWE learning algorithms. We then describe the
use of SSWE in a supervised learning framework
for Twitter sentiment classification.
3.1 C&W Model
Collobert et al (2011) introduce C&W model to
learn word embedding based on the syntactic con-
texts of words. Given an ngram ?cat chills on a
mat?, C&W replaces the center word with a ran-
dom wordw
r
and derives a corrupted ngram ?cat
chills w
r
a mat?. The training objective is that the
original ngram is expected to obtain a higher lan-
guage model score than the corrupted ngram by a
margin of 1. The ranking objective function can
be optimized by a hinge loss,
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(1)
where t is the original ngram, t
r
is the corrupted
ngram, f
cw
(?) is a one-dimensional scalar repre-
senting the language model score of the input n-
gram. Figure 1(a) illustrates the neural architec-
ture of C&W, which consists of four layers, name-
ly lookup ? linear ? hTanh ? linear (from
bottom to top). The original and corrupted ngram-
s are treated as inputs of the feed-forward neural
network, respectively. The output f
cw
is the lan-
guage model score of the input, which is calculat-
ed as given in Equation 2, where L is the lookup
table of word embedding,w
1
, w
2
, b
1
, b
2
are the pa-
rameters of linear layers.
f
cw
(t) = w
2
(a) + b
2
(2)
1557
so cooool :D 
lookup 
linear 
hTanh 
linear 
softmax 
(a) C&W 
so cooool :D 
(b) SSWEh 
so cooool :D 
(c) SSWEu 
syntactic 
sentiment 
positive 
negative 
Figure 1: The traditional C&W model and our neural networks (SSWE
h
and SSWE
u
) for learning
sentiment-specific word embedding.
a = hTanh(w
1
L
t
+ b
1
) (3)
hTanh(x) =
?
?
?
?
?
?1 if x < ?1
x if ? 1 ? x ? 1
1 if x > 1
(4)
3.2 Sentiment-Specific Word Embedding
Following the traditional C&W model (Collobert
et al, 2011), we incorporate the sentiment infor-
mation into the neural network to learn sentiment-
specific word embedding. We develop three neural
networks with different strategies to integrate the
sentiment information of tweets.
Basic Model 1 (SSWE
h
). As an unsupervised
approach, C&W model does not explicitly capture
the sentiment information of texts. An intuitive
solution to integrate the sentiment information is
predicting the sentiment distribution of text based
on input ngram. We do not utilize the entire sen-
tence as input because the length of different sen-
tences might be variant. We therefore slide the
window of ngram across a sentence, and then pre-
dict the sentiment polarity based on each ngram
with a shared neural network. In the neural net-
work, the distributed representation of higher lay-
er are interpreted as features describing the input.
Thus, we utilize the continuous vector of top layer
to predict the sentiment distribution of text.
Assuming there are K labels, we modify the di-
mension of top layer in C&W model as K and
add a softmax layer upon the top layer. The
neural network (SSWEh) is given in Figure 1(b).
Softmax layer is suitable for this scenario be-
cause its outputs are interpreted as conditional
probabilities. Unlike C&W, SSWE
h
does not gen-
erate any corrupted ngram. Let f
g
(t), where K
denotes the number of sentiment polarity label-
s, be the gold K-dimensional multinomial distri-
bution of input t and
?
k
f
g
k
(t) = 1. For pos-
itive/negative classification, the distribution is of
the form [1,0] for positive and [0,1] for negative.
The cross-entropy error of the softmax layer is :
loss
h
(t) = ?
?
k={0,1}
f
g
k
(t) ? log(f
h
k
(t)) (5)
where f
g
(t) is the gold sentiment distribution and
f
h
(t) is the predicted sentiment distribution.
Basic Model 2 (SSWE
r
). SSWE
h
is trained by
predicting the positive ngram as [1,0] and the neg-
ative ngram as [0,1]. However, the constraint of
SSWE
h
is too strict. The distribution of [0.7,0.3]
can also be interpreted as a positive label because
the positive score is larger than the negative s-
core. Similarly, the distribution of [0.2,0.8] indi-
cates negative polarity. Based on the above obser-
vation, the hard constraints in SSWE
h
should be
relaxed. If the sentiment polarity of a tweet is pos-
itive, the predicted positive score is expected to be
larger than the predicted negative score, and the
exact reverse if the tweet has negative polarity.
We model the relaxed constraint with a rank-
ing objective function and borrow the bottom four
layers from SSWE
h
, namely lookup? linear ?
hTanh ? linear in Figure 1(b), to build the re-
laxed neural network (SSWEr). Compared with
SSWE
h
, the softmax layer is removed because
SSWE
r
does not require probabilistic interpreta-
tion. The hinge loss of SSWE
r
is modeled as de-
1558
scribed below.
loss
r
(t) = max(0, 1? ?
s
(t)f
r
0
(t)
+ ?
s
(t)f
r
1
(t) )
(6)
where f
r
0
is the predicted positive score, f
r
1
is
the predicted negative score, ?
s
(t) is an indicator
function reflecting the sentiment polarity of a sen-
tence,
?
s
(t) =
{
1 if f
g
(t) = [1, 0]
?1 if f
g
(t) = [0, 1]
(7)
Similar with SSWE
h
, SSWE
r
also does not gen-
erate the corrupted ngram.
Unified Model (SSWE
u
). The C&W model
learns word embedding by modeling syntactic
contexts of words but ignoring sentiment infor-
mation. By contrast, SSWE
h
and SSWE
r
learn
sentiment-specific word embedding by integrating
the sentiment polarity of sentences but leaving out
the syntactic contexts of words. We develop a uni-
fied model (SSWEu) in this part, which captures
the sentiment information of sentences as well as
the syntactic contexts of words. SSWE
u
is illus-
trated in Figure 1(c).
Given an original (or corrupted) ngram and
the sentiment polarity of a sentence as the in-
put, SSWE
u
predicts a two-dimensional vector for
each input ngram. The two scalars (f
u
0
, f
u
1
) s-
tand for language model score and sentiment s-
core of the input ngram, respectively. The training
objectives of SSWE
u
are that (1) the original n-
gram should obtain a higher language model score
f
u
0
(t) than the corrupted ngram f
u
0
(t
r
), and (2) the
sentiment score of original ngram f
u
1
(t) should be
more consistent with the gold polarity annotation
of sentence than corrupted ngram f
u
1
(t
r
). The loss
function of SSWE
u
is the linear combination of t-
wo hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(8)
where loss
cw
(t, t
r
) is the syntactic loss as given
in Equation 1, loss
us
(t, t
r
) is the sentiment loss
as described in Equation 9. The hyper-parameter
? weighs the two parts.
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(9)
Model Training. We train sentiment-specific
word embedding from massive distant-supervised
tweets collected with positive and negative emoti-
cons
1
. We crawl tweets from April 1st, 2013 to
April 30th, 2013 with TwitterAPI. We tokenize
each tweet with TwitterNLP (Gimpel et al, 2011),
remove the @user and URLs of each tweet, and fil-
ter the tweets that are too short (< 7 words). Final-
ly, we collect 10M tweets, selected by 5M tweets
with positive emoticons and 5M tweets with nega-
tive emoticons.
We train SSWE
h
, SSWE
r
and SSWE
u
by
taking the derivative of the loss through back-
propagation with respect to the whole set of pa-
rameters (Collobert et al, 2011), and use Ada-
Grad (Duchi et al, 2011) to update the parame-
ters. We empirically set the window size as 3, the
embedding length as 50, the length of hidden lay-
er as 20 and the learning rate of AdaGrad as 0.1
for all baseline and our models. We learn embed-
ding for unigrams, bigrams and trigrams separate-
ly with same neural network and same parameter
setting. The contexts of unigram (bigram/trigram)
are the surrounding unigrams (bigrams/trigrams),
respectively.
3.3 Twitter Sentiment Classification
We apply sentiment-specific word embedding for
Twitter sentiment classification under a supervised
learning framework as in previous work (Pang et
al., 2002). Instead of hand-crafting features, we
incorporate the continuous representation of word-
s and phrases as the feature of a tweet. The senti-
ment classifier is built from tweets with manually
annotated sentiment polarity.
We explore min, average and max convolu-
tional layers (Collobert et al, 2011; Socher et
al., 2011a), which have been used as simple and
effective methods for compositionality learning
in vector-based semantics (Mitchell and Lapata,
2010), to obtain the tweet representation. The re-
sult is the concatenation of vectors derived from
different convolutional layers.
z(tw) = [z
max
(tw), z
min
(tw), z
average
(tw)]
where z(tw) is the representation of tweet tw and
z
x
(tw) is the results of the convolutional layer x ?
{min,max, average}. Each convolutional layer
1
We use the emoticons selected by Hu et al (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
1559
zx
employs the embedding of unigrams, bigrams
and trigrams separately and conducts the matrix-
vector operation of x on the sequence represented
by columns in each lookup table. The output of
z
x
is the concatenation of results obtained from
different lookup tables.
z
x
(tw) = [w
x
?L
uni
?
tw
, w
x
?L
bi
?
tw
, w
x
?L
tri
?
tw
]
where w
x
is the convolutional function of z
x
,
?L?
tw
is the concatenated column vectors of the
words in the tweet. L
uni
, L
bi
and L
tri
are the
lookup tables of the unigram, bigram and trigram
embedding, respectively.
4 Experiment
We conduct experiments to evaluate SSWE by in-
corporating it into a supervised learning frame-
work for Twitter sentiment classification. We also
directly evaluate the effectiveness of the SSWE by
measuring the word similarity in the embedding
space for sentiment lexicons.
4.1 Twitter Sentiment Classification
Experiment Setup and Datasets. We conduct
experiments on the latest Twitter sentiment clas-
sification benchmark dataset in SemEval 2013
(Nakov et al, 2013). The training and develop-
ment sets were completely in full to task partici-
pants. However, we were unable to download all
the training and development sets because some
tweets were deleted or not available due to mod-
ified authorization status. The test set is directly
provided to the participants. The distribution of
our dataset is given in Table 1. We train sentiment
classifier with LibLinear (Fan et al, 2008) on the
training set, tune parameter ?c on the dev set and
evaluate on the test set. Evaluation metric is the
Macro-F1 of positive and negative categories
2
.
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of the SemEval 2013 Twitter
sentiment classification dataset.
2
We investigate 2-class Twitter sentiment classifica-
tion (positive/negative) instead of 3-class Twitter sentiment
classification (positive/negative/neutral) in SemEval2013.
Baseline Methods. We compare our method
with the following sentiment classification algo-
rithms:
(1) DistSuper: We use the 10 million tweets se-
lected by positive and negative emoticons as train-
ing data, and build sentiment classifier with Lib-
Linear and ngram features (Go et al, 2009).
(2) SVM: The ngram features and Support Vec-
tor Machine are widely used baseline methods to
build sentiment classifiers (Pang et al, 2002). Li-
bLinear is used to train the SVM classifier.
(3) NBSVM: NBSVM (Wang and Manning,
2012) is a state-of-the-art performer on many sen-
timent classification datasets, which trades-off be-
tween Naive Bayes and NB-enhanced SVM.
(4) RAE: Recursive Autoencoder (Socher et al,
2011c) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. We run RAE with randomly initial-
ized word embedding.
(5) NRC: NRC builds the top-performed system
in SemEval 2013 Twitter sentiment classification
track which incorporates diverse sentiment lexi-
cons and many manually designed features. We
re-implement this system because the codes are
not publicly available
3
. NRC-ngram refers to the
feature set of NRC leaving out ngram features.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al, 2013b) be-
cause we cannot efficiently train the RNTN model.
The reason lies in that the tweets in our dataset do
not have accurately parsed results or fine grained
sentiment labels for phrases. Another reason is
that the RNTN model trained on movie reviews
cannot be directly applied on tweets due to the d-
ifferences between domains (Blitzer et al, 2007).
Results and Analysis. Table 2 shows the macro-
F1 of the baseline systems as well as the SSWE-
based methods on positive/negative sentimen-
t classification of tweets. Distant supervision is
relatively weak because the noisy-labeled tweet-
s are treated as the gold standard, which affects
the performance of classifier. The results of bag-
of-ngram (uni/bi/tri-gram) features are not satis-
fied because the one-hot word representation can-
not capture the latent connections between words.
NBSVM and RAE perform comparably and have
3
For 3-class sentiment classification in SemEval 2013,
our re-implementation of NRC achieved 68.3%, 0.7% low-
er than NRC (69%) due to less training data.
1560
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + uni/bi/tri-gram 63.84
SVM + unigram 74.50
SVM + uni/bi/tri-gram 75.06
NBSVM 75.28
RAE 75.12
NRC (Top System in SemEval) 84.73
NRC - ngram 84.17
SSWE
u
84.98
SSWE
u
+NRC 86.58
SSWE
u
+NRC-ngram 86.48
Table 2: Macro-F1 on positive/negative classifica-
tion of tweets.
a big gap in comparison with the NRC and SSWE-
based methods. The reason is that RAE and NB-
SVM learn the representation of tweets from the
small-scale manually annotated training set, which
cannot well capture the comprehensive linguistic
phenomenons of words.
NRC implements a variety of features and
reaches 84.73% in macro-F1, verifying the impor-
tance of a better feature representation for Twit-
ter sentiment classification. We achieve 84.98%
by using only SSWE
u
as features without borrow-
ing any sentiment lexicons or hand-crafted rules.
The results indicate that SSWE
u
automatically
learns discriminative features from massive tweets
and performs comparable with the state-of-the-art
manually designed features. After concatenating
SSWE
u
with the feature set of NRC, the perfor-
mance is further improved to 86.58%. We also
compare SSWE
u
with the ngram feature by inte-
grating SSWE into NRC-ngram. The concatenated
features SSWE
u
+NRC-ngram (86.48%) outperfor-
m the original feature set of NRC (84.73%).
As a reference, we apply SSWE
u
on subjec-
tive classification of tweets, and obtain 72.17% in
macro-F1 by using only SSWE
u
as feature. Af-
ter combining SSWE
u
with the feature set of NR-
C, we improve NRC from 74.86% to 75.39% for
subjective classification.
Comparision between Different Word Embed-
ding. We compare sentiment-specific word em-
bedding (SSWE
h
, SSWE
r
, SSWE
u
) with base-
line embedding learning algorithms by only us-
ing word embedding as features for Twitter sen-
timent classification. We use the embedding of u-
nigrams, bigrams and trigrams in the experimen-
t. The embeddings of C&W (Collobert et al,
2011), word2vec
4
, WVSA (Maas et al, 2011) and
our models are trained with the same dataset and
same parameter setting. We compare with C&W
and word2vec as they have been proved effective
in many NLP tasks. The trade-off parameter of
ReEmb (Labutov and Lipson, 2013) is tuned on
the development set of SemEval 2013.
Table 3 shows the performance on the pos-
itive/negative classification of tweets
5
. ReEm-
b(C&W) and ReEmb(w2v) stand for the use
of embeddings learned from 10 million distant-
supervised tweets with C&W and word2vec, re-
spectively. Each row of Table 3 represents a word
embedding learning algorithm. Each column s-
tands for a type of embedding used to compose
features of tweets. The column uni+bi denotes the
use of unigram and bigram embedding, and the
column uni+bi+tri indicates the use of unigram,
bigram and trigram embedding.
Embedding unigram uni+bi uni+bi+tri
C&W 74.89 75.24 75.89
Word2vec 73.21 75.07 76.31
ReEmb(C&W) 75.87 ? ?
ReEmb(w2v) 75.21 ? ?
WVSA 77.04 ? ?
SSWE
h
81.33 83.16 83.37
SSWE
r
80.45 81.52 82.60
SSWE
u
83.70 84.70 84.98
Table 3: Macro-F1 on positive/negative classifica-
tion of tweets with different word embeddings.
From the first column of Table 3, we can see that
the performance of C&W and word2vec are obvi-
ously lower than sentiment-specific word embed-
dings by only using unigram embedding as fea-
tures. The reason is that C&W and word2vec do
not explicitly exploit the sentiment information of
the text, resulting in that the words with oppo-
site polarity such as good and bad are mapped
to close word vectors. When such word embed-
dings are fed as features to a Twitter sentimen-
t classifier, the discriminative ability of sentiment
words are weakened thus the classification perfor-
mance is affected. Sentiment-specific word em-
4
Available at https://code.google.com/p/word2vec/. We
utilize the Skip-gram model because it performs better than
CBOW in our experiments.
5
MVSA and ReEmb are not suitable for learning bigram
and trigram embedding because their sentiment predictor
functions only utilize the unigram embedding.
1561
beddings (SSWE
h
, SSWE
r
, SSWE
u
) effectively
distinguish words with opposite sentiment polarity
and perform best in three settings. SSWE outper-
forms MVSA by exploiting more contextual infor-
mation in the sentiment predictor function. SSWE
outperforms ReEmb by leveraging more senti-
ment information from massive distant-supervised
tweets. Among three sentiment-specific word em-
beddings, SSWE
u
captures more context informa-
tion and yields best performance. SSWE
h
and
SSWE
r
obtain comparative results.
From each row of Table 3, we can see that the
bigram and trigram embeddings consistently im-
prove the performance of Twitter sentiment classi-
fication. The underlying reason is that a phrase,
which cannot be accurately represented by uni-
gram embedding, is directly encoded into the n-
gram embedding as an idiomatic unit. A typical
case in sentiment analysis is that the composed
phrase and multiword expression may have a dif-
ferent sentiment polarity than the individual word-
s it contains, such as not [bad] and [great] deal
of (the word in the bracket has different sentiment
polarity with the ngram). A very recent study by
Mikolov et al (2013) also verified the effective-
ness of phrase embedding for analogically reason-
ing phrases.
Effect of ? in SSWE
u
We tune the hyper-
parameter ? of SSWE
u
on the development set by
using unigram embedding as features. As given
in Equation 8, ? is the weighting score of syntac-
tic loss of SSWE
u
and trades-off the syntactic and
sentiment losses. SSWE
u
is trained from 10 mil-
lion distant-supervised tweets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
?
Ma
cro
?F1
 
 
SSWEu
Figure 2: Macro-F1 of SSWE
u
on the develop-
ment set of SemEval 2013 with different ?.
Figure 2 shows the macro-F1 of SSWE
u
on pos-
itive/negative classification of tweets with differ-
ent ? on our development set. We can see that
SSWE
u
performs better when ? is in the range
of [0.5, 0.6], which balances the syntactic context
and sentiment information. The model with ?=1
stands for C&W model, which only encodes the
syntactic contexts of words. The sharp decline at
?=1 reflects the importance of sentiment informa-
tion in learning word embedding for Twitter senti-
ment classification.
Effect of Distant-supervised Data in SSWE
u
We investigate how the size of the distant-
supervised data affects the performance of SSWE
u
feature for Twitter sentiment classification. We
vary the number of distant-supervised tweets from
1 million to 12 million, increased by 1 million.
We set the ? of SSWE
u
as 0.5, according to the
experiments shown in Figure 2. Results of posi-
tive/negative classification of tweets on our devel-
opment set are given in Figure 3.
1 2 3 4 5 6 7 8 9 10 11 12
x 106
0.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
# of distant?supervised tweets
Mac
ro?F
1
 
 
SSWEu
Figure 3: Macro-F1 of SSWE
u
with different size
of distant-supervised data on our development set.
We can see that when more distant-supervised
tweets are added, the accuracy of SSWE
u
con-
sistently improves. The underlying reason is that
when more tweets are incorporated, the word em-
bedding is better estimated as the vocabulary size
is larger and the context and sentiment informa-
tion are richer. When we have 10 million distant-
supervised tweets, the SSWE
u
feature increases
the macro-F1 of positive/negative classification of
tweets to 82.94% on our development set. When
we have more than 10 million tweets, the per-
formance remains stable as the contexts of words
have been mostly covered.
4.2 Word Similarity of Sentiment Lexicons
The quality of SSWE has been implicitly evaluat-
ed when applied in Twitter sentiment classification
in the previous subsection. We explicitly evaluate
it in this section through word similarity in the em-
1562
bedding space for sentiment lexicons. The evalua-
tion metric is the accuracy of polarity consistency
between each sentiment word and its topN closest
words in the sentiment lexicon,
Accuracy =
?
#Lex
i=1
?
N
j=1
?(w
i
, c
ij
)
#Lex?N
(10)
where #Lex is the number of words in the senti-
ment lexicon, w
i
is the i-th word in the lexicon, c
ij
is the j-th closest word tow
i
in the lexicon with co-
sine similarity, ?(w
i
, c
ij
) is an indicator function
that is equal to 1 if w
i
and c
ij
have the same sen-
timent polarity and 0 for the opposite case. The
higher accuracy refers to a better polarity consis-
tency of words in the sentiment lexicon. We set N
as 100 in our experiment.
Experiment Setup and Datasets We utilize
the widely-used sentiment lexicons, namely M-
PQA (Wilson et al, 2005) and HL (Hu and Liu,
2004), to evaluate the quality of word embedding.
For each lexicon, we remove the words that do
not appear in the lookup table of word embedding.
We only use unigram embedding in this section
because these sentiment lexicons do not contain
phrases. The distribution of the lexicons used in
this paper is listed in Table 4.
Lexicon Positive Negative Total
HL 1,331 2,647 3,978
MPQA 1,932 2,817 4,749
Joint 1,051 2,024 3,075
Table 4: Statistics of the sentiment lexicons. Join-
t stands for the words that occur in both HL and
MPQA with the same sentiment polarity.
Results. Table 5 shows our results com-
pared to other word embedding learning al-
gorithms. The accuracy of random result is
50% as positive and negative words are ran-
domly occurred in the nearest neighbors of
each word. Sentiment-specific word embed-
dings (SSWE
h
, SSWE
r
, SSWE
u
) outperform ex-
isting neural models (C&W, word2vec) by large
margins. SSWE
u
performs best in three lexicon-
s. SSWE
h
and SSWE
r
have comparable perfor-
mances. Experimental results further demonstrate
that sentiment-specific word embeddings are able
to capture the sentiment information of texts and
distinguish words with opposite sentiment polari-
ty, which are not well solved in traditional neural
Embedding HL MPQA Joint
Random 50.00 50.00 50.00
C&W 63.10 58.13 62.58
Word2vec 66.22 60.72 65.59
ReEmb(C&W) 64.81 59.76 64.09
ReEmb(w2v) 67.16 61.81 66.39
WVSA 68.14 64.07 67.12
SSWE
h
74.17 68.36 74.03
SSWE
r
73.65 68.02 73.14
SSWE
u
77.30 71.74 77.33
Table 5: Accuracy of the polarity consistency of
words in different sentiment lexicons.
models like C&W and word2vec. SSWE outper-
forms MVSA and ReEmb by exploiting more con-
text information of words and sentiment informa-
tion of sentences, respectively.
5 Conclusion
In this paper, we propose learning continuous
word representations as features for Twitter sen-
timent classification under a supervised learning
framework. We show that the word embedding
learned by traditional neural networks are not ef-
fective enough for Twitter sentiment classification.
These methods typically only model the contex-
t information of words so that they cannot dis-
tinguish words with similar context but opposite
sentiment polarity (e.g. good and bad). We learn
sentiment-specific word embedding (SSWE) by
integrating the sentiment information into the loss
functions of three neural networks. We train SS-
WE with massive distant-supervised tweets select-
ed by positive and negative emoticons. The ef-
fectiveness of SSWE has been implicitly evaluat-
ed by using it as features in sentiment classifica-
tion on the benchmark dataset in SemEval 2013,
and explicitly verified by measuring word similar-
ity in the embedding space for sentiment lexicon-
s. Our unified model combining syntactic context
of words and sentiment information of sentences
yields the best performance in both experiments.
Acknowledgments
We thank Yajuan Duan, Shujie Liu, Zhenghua Li,
Li Dong, Hong Sun and Lanjun Zhou for their
great help. This research was partly supported
by National Natural Science Foundation of China
(No.61133012, No.61273321, No.61300113).
1563
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy da-
ta. In Proceedings of International Conference on
Computational Linguistics, pages 36?44.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
Yoshua Bengio. 2013. Deep learning of represen-
tations: Looking forward. arXiv preprint arX-
iv:1305.0445.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings
of the Conference on Information and Knowledge
Management, pages 375?382.
Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shok-
oufandeh. 2012. Sentiment classification with su-
pervised sequence embedding. In Machine Learn-
ing and Knowledge Discovery in Databases, pages
159?174. Springer.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Annual Meeting of the Association for
Computational Linguistics, volume 7.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of International Con-
ference on Computational Linguistics, pages 241?
249.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894?904.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. E-
moticon smoothed language models for twitter sen-
timent analysis. In The Association for the Advance-
ment of Artificial Intelligence.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
1564
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-
do, and Jeffrey Dean. 2013. Distributed representa-
tions of words and phrases and their compositionali-
ty. The Conference on Neural Information Process-
ing Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Andriy Mnih and Geoffrey E Hinton. 2009. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the International Conference on Ma-
chine Learning, pages 129?136.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011c. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with composi-
tional vector grammars. In Annual Meeting of the
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Annual Meeting of the
Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmenta-
tion and pos tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 647?657.
1565
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49?54,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Adaptive Recursive Neural Network
for Target-dependent Twitter Sentiment Classification
Li Dong
??
Furu Wei
?
Chuanqi Tan
??
Duyu Tang
??
Ming Zhou
?
Ke Xu
?
?
Beihang University, Beijing, China
?
Microsoft Research, Beijing, China
?
Harbin Institute of Technology, Harbin, China
donglixp@gmail.com fuwei@microsoft.com {ysjtcq,tangduyu}@gmail.com
mingzhou@microsoft.com kexu@nlsde.buaa.edu.cn
Abstract
We propose Adaptive Recursive Neural
Network (AdaRNN) for target-dependent
Twitter sentiment classification. AdaRNN
adaptively propagates the sentiments of
words to target depending on the context
and syntactic relationships between them.
It consists of more than one composition
functions, and we model the adaptive sen-
timent propagations as distributions over
these composition functions. The experi-
mental studies illustrate that AdaRNN im-
proves the baseline methods. Further-
more, we introduce a manually annotated
dataset for target-dependent Twitter senti-
ment analysis.
1 Introduction
Twitter becomes one of the most popular social
networking sites, which allows the users to read
and post messages (i.e. tweets) up to 140 charac-
ters. Among the great varieties of topics, people
in Twitter tend to express their opinions for the
brands, celebrities, products and public events. As
a result, it attracts much attention to estimate the
crowd?s sentiments in Twitter.
For the tweets, our task is to classify their senti-
ments for a given target as positive, negative, and
neutral. People may mention several entities (or
targets) in one tweet, which affects the availabil-
ities for most of existing methods. For example,
the tweet ?@ballmer: windows phone is better
than ios!? has three targets (@ballmer, windows
phone, and ios). The user expresses neutral, pos-
itive, and negative sentiments for them, respec-
tively. If target information is ignored, it is diffi-
cult to obtain the correct sentiment for a specified
target. For target-dependent sentiment classifica-
tion, the manual evaluation of Jiang et al (2011)
?
Contribution during internship at Microsoft Research.
show that about 40% of errors are caused by not
considering the targets in classification.
The features used in traditional learning-based
methods (Pang et al, 2002; Nakagawa et al, 2010)
are independent to the targets, hence the results
are computed despite what the targets are. Hu and
Liu (2004) regard the features of products as tar-
gets, and sentiments for them are heuristically de-
termined by the dominant opinion words. Jiang
et al (2011) combine the target-independent fea-
tures (content and lexicon) and target-dependent
features (rules based on the dependency parsing
results) together in subjectivity classification and
polarity classification for tweets.
In this paper, we mainly focus on integrating
target information with Recursive Neural Network
(RNN) to leverage the ability of deep learning
models. The neural models use distributed repre-
sentation (Hinton, 1986; Rumelhart et al, 1986;
Bengio et al, 2003) to automatically learn fea-
tures for target-dependent sentiment classification.
RNN utilizes the recursive structure of text, and it
has achieved state-of-the-art sentiment analysis re-
sults for movie review dataset (Socher et al, 2012;
Socher et al, 2013). The recursive neural mod-
els employ the semantic composition functions,
which enables them to handle the complex com-
positionalities in sentiment analysis.
Specifically, we propose a framework which
learns to propagate the sentiments of words to-
wards the target depending on context and syn-
tactic structure. We employ a novel adaptive
multi-compositionality layer in recursive neural
network, which is named as AdaRNN (Dong et
al., 2014). It consists of more than one compo-
sition functions, and we model the adaptive sen-
timent propagations as learning distributions over
these composition functions. We automatically
learn the composition functions and how to select
them from supervisions, instead of choosing them
heuristically or by hand-crafted rules. AdaRNN
49
determines how to propagate the sentiments to-
wards the target and handles the negation or in-
tensification phenomena (Taboada et al, 2011) in
sentiment analysis. In addition, we introduce a
manually annotated dataset, and conduct extensive
experiments on it. The experimental results sug-
gest that our approach yields better performances
than the baseline methods.
2 RNN: Recursive Neural Network
RNN (Socher et al, 2011) represents the phrases
and words as D-dimensional vectors. It performs
compositions based on the binary trees, and obtain
the vector representations in a bottom-up way.
not very good
Negative
Softmax
very good
not very good
Figure 1: The composition process for ?not very
good? in Recursive Neural Network.
As illustrated in Figure 1, we obtain the repre-
sentation of ?very good? by the composition of
?very? and ?good?, and the representation of tri-
gram ?not very good? is recursively obtained by
the vectors of ?not? and ?very good?. The di-
mensions of parent node are calculated by linear
combination of the child vectors? dimensions. The
vector representation v is obtained via:
v = f (g (v
l
,v
r
)) = f
(
W
[
v
l
v
r
]
+ b
)
(1)
where v
l
,v
r
are the vectors of its left and right
child, g is the composition function, f is the non-
linearity function (such as tanh, sigmoid, softsign,
etc.), W ? R
D?2D
is the composition matrix, and
b is the bias vector. The dimension of v is the
same as its child vectors, and it is recursively used
in the next step. Notably, the word vectors in the
leaf nodes are regarded as the parameters, and will
be updated according to the supervisions.
The vector representation of root node is then
fed into a softmax classifier to predict the label.
The k-th element of softmax(x) is
exp{x
k
}?
j
exp{x
j
}
. For
a vector, the softmax obtains the distribution over
K classes. Specifically, the predicted distribution
is y = softmax (Uv), where y is the predicted
distribution, U ? R
K?D
is the classification ma-
trix, and v is the vector representation of node.
3 Our Approach
We use the dependency parsing results to find the
words syntactically connected with the interested
target. Adaptive Recursive Neural Network is pro-
posed to propagate the sentiments of words to the
target node. We model the adaptive sentiment
propagations as semantic compositions. The com-
putation process is conducted in a bottom-up man-
ner, and the vector representations are computed
recursively. After we obtain the representation of
target node, a classifier is used to predict the sen-
timent label according to the vector.
In Section 3.1, we show how to build recur-
sive structure for target using the dependency pars-
ing results. In Section 3.2, we propose Adaptive
Recursive Neural Network and use it for target-
dependent sentiment analysis.
3.1 Build Recursive Structure
The dependency tree indicates the dependency re-
lations between words. As described above, we
propagate the sentiments of words to the target.
Hence the target is placed at the root node to com-
bine with its connected words recursively. The de-
pendency relation types are remained to guide the
sentiment propagations in our model.
Algorithm 1 Convert Dependency Tree
Input: Target node, Dependency tree
Output: Converted tree
1: function CONV(r)
2: E
r
? SORT(dep edges connected with r)
3: v? r
4: for (r
t
?? u/u
t
?? r) in E
r
do
5: if r is head of u then
6: w? node with CONV(u), v as children
7: else
8: w? node with v, CONV(u) as children
9: v? w
10: return v
11: Call CONV(target node) to get converted tree
As illustrated in the Algorithm 1, we recursively
convert the dependency tree starting from the tar-
get node. We find all the words connected to the
target, and these words are combined with target
node by certain order. Every combination is con-
sidered as once propagation of sentiments. If the
target is head of the connected words, the target
vector is combined as the right node; if otherwise,
it is combined as the left node. This ensures the
50
child nodes in a certain order. We use two rules
to determine the order of combinations: (1) the
words whose head is the target in dependency tree
are first combined, and then the rest of connected
words are combined; (2) if the first rule cannot de-
termine the order, the connected words are sorted
by their positions in sentence from right to left.
Notably, the conversion is performed recursively
for the connected words and the dependency rela-
tion types are remained. Figure 2 shows the con-
verted results for different targets in one sentence.
3.2 AdaRNN: Adaptive Recursive Neural
Network
RNN employs one global matrix to linearly com-
bine the elements of vectors. Sometimes it is
challenging to obtain a single powerful function
to model the semantic composition, which moti-
vates us to propose AdaRNN. The basic idea of
AdaRNN is to use more than one composition
functions and adaptively select them depending on
the linguistic tags and the combined vectors. The
model learns to propagate the sentiments of words
by using the different composition functions.
Figure 2 shows the computation process for the
example sentence ?windows is better than ios?,
where the user expresses positive sentiment to-
wards windows and negative sentiment to ios. For
the targets, the order of compositions and the de-
pendency types are different. AdaRNN adap-
tively selects the composition functions g
1
. . . g
C
depending on the child vectors and the linguistic
types. Thus it is able to determine how to propa-
gate the sentiments of words towards the target.
Based on RNN described in Section 2, we de-
fine the composition result v in AdaRNN as:
v = f
(
C
?
h=1
P (g
h
|v
l
,v
r
, e) g
h
(v
l
,v
r
)
)
(2)
where g
1
, . . . , g
C
are the composition functions,
P (g
h
|v
l
,v
r
, e) is the probability of employing g
h
given the child vectors v
l
,v
r
and external feature
vector e, and f is the nonlinearity function. For
the composition functions, we use the same forms
as in Equation (1), i.e., we have C composition
matrices W
1
. . .W
C
. We define the distribution
over these composition functions as:
?
?
?
P (g
1
|v
l
,v
r
, e)
.
.
.
P (g
C
|v
l
,v
r
, e)
?
?
?
= softmax
?
?
?S
?
?
v
l
v
r
e
?
?
?
?
(3)
where ? is the hyper-parameter, S ? R
C?(2D+|e|)
is the matrix used to determine which composition
function we use, v
l
,v
r
are the left and right child
vectors, and e are external feature vector. In this
work, e is a one-hot binary feature vector which
indicates what the dependency type is. If relation
is the k-th type, we set e
k
to 1 and the others to 0.
Adding ? in softmax function is a widely used
parametrization method in statistical mechanics,
which is known as Boltzmann distribution and
Gibbs measure (Georgii, 2011). When ? = 0, this
function produces a uniform distribution; when
? = 1, it is the same as softmax function; when
? ??, it only activates the dimension with max-
imum weight, and sets its probability to 1.
3.3 Model Training
We use the representation of root node as the fea-
tures, and feed them into the softmax classifier to
predict the distribution over classes. We define the
ground truth vector t as a binary vector. If the k-th
class is the label, only t
k
is 1 and the others are
0. Our goal is to minimize the cross-entropy error
between the predicted distribution y and ground
truth distribution t. For each training instance, we
define the objective function as:
min
?
?
?
j
t
j
logy
j
+
?
???
?
?
???
2
2
(4)
where ? represents the parameters, and the L
2
-
regularization penalty is used.
Based on the converted tree, we employ back-
propagation algorithm (Rumelhart et al, 1986) to
propagate the errors from root node to the leaf
nodes. We calculate the derivatives to update the
parameters. The AdaGrad (Duchi et al, 2011) is
employed to solve this optimization problem.
4 Experiments
As people tend to post comments for the celebri-
ties, products, and companies, we use these key-
words (such as ?bill gates?, ?taylor swift?, ?xbox?,
?windows 7?, ?google?) to query the Twitter API.
After obtaining the tweets, we manually anno-
tate the sentiment labels (negative, neutral, posi-
tive) for these targets. In order to eliminate the
effects of data imbalance problem, we randomly
sample the tweets and make the data balanced.
The negative, neutral, positive classes account for
25%, 50%, 25%, respectively. Training data con-
sists of 6,248 tweets, and testing data has 692
51
w indows
is
better
io s than
g1 gC...
g 1 gC...
g1 gC...
g 1 gC...
n s u b j
cop
p rep
pobj
Positve
Softmax
io s
than
w indows
is better
g1 g C...
g1 gC...
g1 gC...
g 1 g C...
pobj
prep
ns u b j
cop
Negative
Softmax
w indows is better than io s
RO OT
cop
ns ub j
p rep pobj
( target ) ( target )
Dependency tree :
windows is target : ios is target :
Figure 2: For the sentence ?windows is better than ios?, we convert its dependency tree for the different
targets (windows and ios). AdaRNN performs semantic compositions in bottom-up manner and forward
propagates sentiment information to the target node. The g
1
, . . . , g
C
are different composition functions,
and the combined vectors and dependency types are used to select them adaptively. These composition
functions decide how to propagate the sentiments to the target.
tweets. We randomly sample some tweets, and
they are assigned with sentiment labels by two an-
notators. About 82.5% of them have the same la-
bels. The agreement percentage of polarity clas-
sification is higher than subjectivity classification.
To the best of our knowledge, this is the largest
target-dependent Twitter sentiment classification
dataset which is annotated manually. We make the
dataset publicly available
1
for research purposes.
We preprocess the tweets by replacing the tar-
gets with $T$ and setting their POS tags to NN.
Liblinear (Fan et al, 2008) is used for baselines.
A tweet-specific tokenizer (Gimpel et al, 2011)
is employed, and the dependency parsing results
are computed by Stanford Parser (Klein and Man-
ning, 2003). The hyper-parameters are chosen by
cross-validation on the training split, and the test
accuracy and macro-average F1-score score are re-
ported. For recursive neural models, the dimen-
sion of word vector is set to 25, and f = tanh
is used as the nonlinearity function. We employ
10 composition matrices in AdaRNN. The param-
eters are randomly initialized. Notably, the word
vectors will also be updated.
SVM-indep: It uses the uni-gram, bi-gram,
punctuations, emoticons, and #hashtags as the
content features, and the numbers of positive or
negative words in General Inquirer as lexicon fea-
tures. These features are all target-independent.
SVM-dep: We re-implement the method pro-
posed by Jiang et al (2011). It combines both
1
http://goo.gl/5Enpu7
the target-independent (SVM-indep) and target-
dependent features and uses SVM as the classifier.
There are seven rules to extract target-sensitive
features. We do not implement the social graph
optimization and target expansion tricks in it.
SVM-conn: The words, punctuations, emoti-
cons, and #hashtags included in the converted de-
pendency tree are used as the features for SVM.
RNN: It is performed on the converted depen-
dency tree without adaptive composition selection.
AdaRNN-w/oE: Our approach without using
the dependency types as features in adaptive se-
lection for the composition functions.
AdaRNN-w/E: Our approach with employing
the dependency types as features in adaptive se-
lection for the composition functions.
AdaRNN-comb: We combine the root vectors
obtained by AdaRNN-w/E with the uni/bi-gram
features, and they are fed into a SVM classifier.
Method Accuracy Macro-F1
SVM-indep 62.7 60.2
SVM-dep 63.4 63.3
SVM-conn 60.0 59.6
RNN 63.0 62.8
AdaRNN-w/oE 64.9 64.4
AdaRNN-w/E 65.8 65.5
AdaRNN-comb 66.3 65.9
Table 1: Evaluation results on target-dependent
Twitter sentiment classification dataset. Our ap-
proach outperforms the baseline methods.
52
As shown in the Table 1, AdaRNN achieves bet-
ter results than the baselines. Specifically, we find
that the performances of SVM-dep increase than
SVM-indep. It indicates that target-dependent fea-
tures help improve the results. However, the accu-
racy and F1-score do not gain significantly. This
is caused by mismatch of the rules (Jiang et al,
2011) used to extract the target-dependent fea-
tures. The POS tagging and dependency parsing
results are not precise enough for the Twitter data,
so these hand-crafted rules are rarely matched.
Further, the results of SVM-conn illustrate that us-
ing the words which have paths to target as bag-of-
words features does not perform well.
RNN is also based on the converted depen-
dency tree. It outperforms SVM-indep, and is
comparable with SVM-dep. The performances
of AdaRNN-w/oE are better than the above base-
lines. It shows that multiple composition functions
and adaptive selection help improve the results.
AdaRNN provides more powerful composition
ability, so that it achieves better semantic compo-
sition for recursive neural models. AdaRNN-w/E
obtains best performances among the above meth-
ods. Its macro-average F1-score rises by 5.3%
than the target-independent method SVM-indep.
It employs dependency types as binary features to
select the composition functions adaptively. The
results illustrate that the syntactic tags are helpful
to guide the model propagate sentiments of words
towards target. Although the dependency results
are also not precise enough, the composition se-
lection is automatically learned from data. Hence
AdaRNN is more robust for the imprecision of
parsing results than the hand-crafted rules. The
performances become better after adding the uni-
gram and bi-gram features (target-independent).
4.1 Effects of ?
We compare different ? for AdaRNN defined in
Equation (3) in this section. Different parameter ?
leads to different composition selection schemes.
As illustrated in Figure 3, the AdaRNN-w/oE
and AdaRNN-w/E achieve the best accuracies at
? = 2, and they have a similar trend. Specifi-
cally, ? = 0 obtains a uniform distribution over
the composition functions which does not help im-
prove performances. ? ? ? results in a max-
imum probability selection algorithm, i.e., only
the composition function which has the maximum
probability is used. This selection scheme makes
0 20 21 22 23 24 25 26?61
62
63
64
65
66
Accur
acy
RNNAdaRNN-w/oEAdaRNN-w/E
Figure 3: The curve shows the accuracy as the
hyper-parameter ? = 0, 2
0
, 2
1
, . . . , 2
6
increases.
AdaRNN achieves the best results at ? = 2
1
.
the optimization instable. The performances of
? = 1, 2 are similar and they are better than
other settings. It indicates that adaptive selection
method is useful to model the compositions. The
hyper-parameter ? makes trade-offs between uni-
form selection and maximum selection. It adjusts
the effects of these two perspectives.
5 Conclusion
We propose Adaptive Recursive Neural Network
(AdaRNN) for the target-dependent Twitter senti-
ment classification. AdaRNN employs more than
one composition functions and adaptively chooses
them depending on the context and linguistic tags.
For a given tweet, we first convert its dependency
tree for the interested target. Next, the AdaRNN
learns how to adaptively propagate the sentiments
of words to the target node. AdaRNN enables
the sentiment propagations to be sensitive to both
linguistic and semantic categories by using differ-
ent compositions. The experimental results illus-
trate that AdaRNN improves the baselines without
hand-crafted rules.
Acknowledgments
This research was partly supported by the National
863 Program of China (No. 2012AA011005), the
fund of SKLSDE (Grant No. SKLSDE-2013ZX-
06), and Research Fund for the Doctoral Pro-
gram of Higher Education of China (Grant No.
20111102110019).
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155, March.
53
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In Twenty-Eighth AAAI Conference on Artificial In-
telligence (AAAI). AAAI.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 12:2121?2159,
July.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
H.O. Georgii. 2011. Gibbs Measures and Phase
Transitions. De Gruyter studies in mathematics. De
Gruyter.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
HLT ?11, pages 42?47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Geoffrey E. Hinton. 1986. Learning distributed repre-
sentations of concepts. In Proceedings of the Eighth
Annual Conference of the Cognitive Science Society,
pages 1?12. Hillsdale, NJ: Erlbaum.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 786?794.
Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. As-
sociation for Computational Linguistics.
D.E. Rumelhart, G.E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating er-
rors. Nature, 323(6088):533?536.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In ICML.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
EMNLP-CoNLL, pages 1201?1211.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In EMNLP, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267?307, June.
54
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 208?212,
Dublin, Ireland, August 23-24, 2014.
Coooolll: A Deep Learning System for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Bing Qin
?
, Ting Liu
?
, Ming Zhou
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we develop a deep learn-
ing system for message-level Twitter sen-
timent classification. Among the 45 sub-
mitted systems including the SemEval
2013 participants, our system (Coooolll)
is ranked 2nd on the Twitter2014 test set
of SemEval 2014 Task 9. Coooolll is
built in a supervised learning framework
by concatenating the sentiment-specific
word embedding (SSWE) features with
the state-of-the-art hand-crafted features.
We develop a neural network with hybrid
loss function
1
to learn SSWE, which en-
codes the sentiment information of tweets
in the continuous representation of words.
To obtain large-scale training corpora, we
train SSWE from 10M tweets collected by
positive and negative emoticons, without
any manual annotation. Our system can
be easily re-implemented with the publicly
available sentiment-specific word embed-
ding.
1 Introduction
Twitter sentiment classification aims to classify
the sentiment polarity of a tweet as positive, nega-
tive or neutral (Jiang et al., 2011; Hu et al., 2013;
Dong et al., 2014). The majority of existing ap-
proaches follow Pang et al. (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
?
This work was partly done when the first author was
visiting Microsoft Research.
1
This is one of the three sentiment-specific word embed-
ding learning algorithms proposed in Tang et al. (2014).
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
designing effective features to obtain better clas-
sification performance (Pang and Lee, 2008; Liu,
2012; Feldman, 2013). For example, Mohammad
et al. (2013) implement diverse sentiment lexicons
and a variety of hand-crafted features. To leverage
massive tweets containing positive and negative e-
moticons for automatically feature learning, Tang
et al. (2014) propose to learn sentiment-specific
word embedding and Kalchbrenner et al. (2014)
model sentence representation with Dynamic Con-
volutional Neural Network.
In this paper, we develop a deep learning sys-
tem for Twitter sentiment classification. First-
ly, we learn sentiment-specific word embedding
(SSWE) (Tang et al., 2014), which encodes the
sentiment information of text into the continuous
representation of words (Mikolov et al., 2013; Sun
et al., 2014). Afterwards, we concatenate the SS-
WE features with the state-of-the-art hand-crafted
features (Mohammad et al., 2013), and build the
sentiment classifier with the benchmark dataset
from SemEval 2013 (Nakov et al., 2013). To
learn SSWE, we develop a tailored neural net-
work, which incorporates the supervision from
sentiment polarity of tweets in the hybrid loss
function. We learn SSWE from tweets, lever-
aging massive tweets with emoticons as distant-
supervised corpora without any manual annota-
tions.
We evaluate the deep learning system on the
test set of Twitter Sentiment Analysis Track in Se-
mEval 2014
2
. Our system (Coooolll) is ranked
2nd on the Twitter2014 test set, along with the
SemEval 2013 participants owning larger train-
ing data than us. The performance of only us-
ing SSWE as features is comparable to the state-
of-the-art hand-crafted features (detailed in Ta-
ble 3), which verifies the effectiveness of the
sentiment-specific word embedding. We release
the sentiment-specific word embedding learned
2
http://alt.qcri.org/semeval2014/task9/
208
Training 
Data 
Learning 
Algorithm 
Feature 
Representation 
Sentiment 
Classifier 
1 
2 
?. 
 
N 
 
N+1 
N+2 
? 
N+K 
STATE 
Feature 
SSWE 
Feature 
all-cap 
emoticon 
? 
?. 
dimension 1 
dimension 2 
dimension N 
elongated 
Massive 
Tweets 
Embedding 
Learning 
Figure 1: Our deep learning system (Coooolll) for
Twitter sentiment classification.
from 10 million tweets, which can be easily used
to re-implement our system and adopted off-the-
shell in other sentiment analysis tasks.
2 A Deep Learning System
In this section, we present the details of our deep
learning system for Twitter sentiment classifica-
tion. As illustrated in Figure 1, Coooolll is a su-
pervised learning method that builds the sentimen-
t classifier from tweets with manually annotated
sentiment polarity. In our system, the feature rep-
resentation of tweet is composed of two parts, the
sentiment-specific word embedding features (SS-
WE features) and the state-of-the-art hand-crafted
features (STATE features). In the following parts,
we introduce the SSWE features and STATE fea-
tures, respectively.
2.1 SSWE Features
In this part, we first describe the neural network
for learning sentiment-specific word embedding.
Then, we generate the SSWE features of a tweet
from the embedding of words it contains.
Our neural network is an extension of the tra-
ditional C&W model (Collobert et al., 2011), as
illustrated in Figure 2. Unlike C&W model that
learns word embedding by only modeling syntac-
tic contexts of words, we develop SSWEu, which
captures the sentiment information of sentences as
well as the syntactic contexts of words. Given an
original (or corrupted) ngram and the sentiment
polarity of a sentence as the input, SSWE
u
predict-
s a two-dimensional vector for each input ngram.
The two scalars (f
u
0
, f
u
1
) stand for language model
score and sentiment score of the input ngram, re-
so cooool :D 
syntactic 
sentiment 
Figure 2: Our neural network (SSWE
u
) for learn-
ing sentiment-specific word embedding.
spectively. The training objectives of SSWE
u
are
that (1) the original ngram should obtain a high-
er language model score f
u
0
(t) than the corrupted
ngram f
u
0
(t
r
), and (2) the sentiment score of orig-
inal ngram f
u
1
(t) should be more consistent with
the gold polarity annotation of sentence than cor-
rupted ngram f
u
1
(t
r
). The loss function of SSWE
u
is the linear combination of two hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(1)
where where t is the original ngram, t
r
is the cor-
rupted ngram which is generated from t with mid-
dle word replaced by a randomly selected one,
loss
cw
(t, t
r
) is the syntactic loss as given in E-
quation 2, loss
us
(t, t
r
) is the sentiment loss as
described in Equation 3. The hyper-parameter ?
weighs the two parts.
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(2)
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(3)
where ?
s
(t) is an indicator function reflecting the
sentiment polarity of a sentence, whose value is 1
if the sentiment polarity of tweet t is positive and
-1 if t?s polarity is negative. We train sentiment-
specific word embedding from 10M tweets col-
lected with positive and negative emoticons (Hu
et al., 2013). The details of training phase are de-
scribed in Tang et al. (2014).
After finish learning SSWE, we explore min,
average and max convolutional layers (Collobert
et al., 2011; Socher et al., 2011; Mitchell and Lap-
ata, 2010), to obtain the tweet representation. The
result is the concatenation of vectors derived from
different convolutional layers.
209
2.2 STATE Features
We re-implement the state-of-the-art hand-crafted
features (Mohammad et al., 2013) for Twitter sen-
timent classification. The STATE features are de-
scribed below.
? All-Caps. The number of words with all char-
acters in upper case.
? Emoticons. We use the presence of positive
(or negative) emoticons and whether the last
unit of a segmentation is emoticon
3
.
? Elongated Units. The number of elongated
words (with one character repeated more than
two times), such as gooood.
? Sentiment Lexicon. We utilize several senti-
ment lexicons
4
to generate features. We ex-
plore the number of sentiment words, the s-
core of last sentiment words, the total senti-
ment score and the maximal sentiment score
for each lexicon.
? Negation. The number of individual nega-
tions
5
within a tweet.
? Punctuation. The number of contiguous se-
quences of dot, question mark and exclama-
tion mark.
? Cluster. The presence of words from each
of the 1,000 clusters from the Twitter NLP
tool (Gimpel et al., 2011).
? Ngrams. The presence of word ngrams (1-4)
and character ngrams (3-5).
3 Experiments
We evaluate our deep learning system by applying
it for Twitter sentiment classification within a su-
pervised learning framework. We conduct exper-
iments on both positive/negative/neutral and posi-
tive/negative classification of tweets.
3
We use the positive and negative emoticons from Sen-
tiStrength, available at http://sentistrength.wlv.ac.uk/.
4
HL (Hu and Liu, 2004), MPQA (Wilson et al., 2005), N-
RC Emotion (Mohammad and Turney, 2013), NRC Hashtag
and Sentiment140Lexicon (Mohammad et al., 2013).
5
http://sentiment.christopherpotts.net/lingstruc.html
3.1 Dataset and Setting
We train the Twitter sentiment classifier on the
benchmark dataset in SemEval 2013 (Nakov et
al., 2013). The training and development sets were
completely in full to task participants of SemEval
2013. However, we were unable to download al-
l the training and development sets because some
tweets were deleted or not available due to modi-
fied authorization status. The distribution of our
dataset is given in Table 1. We train sentimen-
t classifiers with LibLinear (Fan et al., 2008) on
the training set and dev set, and tune parameter
?c,?wi of SVM on the test set of SemEval 2013.
In both experiment settings, the evaluation met-
ric is the macro-F1 of positive and negative class-
es (Nakov et al., 2013).
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of our SemEval 2013 Twitter
sentiment classification dataset.
The test sets of SemEval 2014 is directly pro-
vided to the participants, which is composed of
five parts. The statistic of test sets in SemEval
2014 is given in Table 2.
Positive Negative Neutral Total
T1 427 304 411 1,142
T2 492 394 1,207 2,093
T3 1,572 601 1,640 3,813
T4 982 202 669 1,939
T5 33 40 13 86
Table 2: Statistics of SemEval 2014 Twitter senti-
ment classification test set. T1 is LiveJournal2014,
T2 is SMS2013, T3 is Twitter2013, T4 is Twit-
ter2014, T5 is Twitter2014Sarcasm.
3.2 Results and Analysis
The experiment results of different methods
on positive/negative/neutral and positive/negative
Twitter sentiment classification are listed in Ta-
ble 3. The meanings of T1?T5 in each column are
described in Table 2. SSWE means the approach
that only utilizes the sentiment-specific word em-
bedding as features for Twitter sentiment classi-
fication. In STATE, we only utilize the existing
features (Mohammad et al., 2013) for building the
210
Method
Positive/Negative/Neutral Positive/Negative
T1 T2 T3 T4 T5 T1 T2 T3 T4 T5
SSWE 70.49 64.29 68.69 66.86 50.00 84.51 85.19 85.06 86.14 62.02
Coooolll 72.90 67.68 70.40 70.14 46.66 86.46 85.32 86.01 87.61 56.55
STATE 71.48 65.43 66.18 67.07 44.89 83.96 82.82 84.39 86.16 58.27
W2V 55.19 52.98 52.33 50.58 49.63 68.87 71.89 74.50 71.52 61.60
Top 74.84 70.28 72.12 70.96 58.16 - - - - - - - - - -
Average 63.52 55.63 59.78 60.41 45.44 - - - - - - - - - -
Table 3: Macro-F1 of positive and negative classes in positive/negative/neutral and positive/negative
Twitter sentiment classification on the test sets (T1-T5, detailed in Table 2) of SemEval 2014. The
performances of Coooolll on the Twitter-relevant test sets are bold.
sentiment classifier. In Coooolll, we use the con-
catenation of SSWE features and STATE features.
In W2V, we only use the word embedding learned
from word2vec
6
as features. Top and Average are
the top and average performance of the 45 team-
s of SemEval 2014, including the SemEval 2013
participants who owns larger training data.
On positive/negative/neutral classification of
tweets as listed in Table 3 (left table), we find
that the learned sentiment-specific word embed-
ding features (SSWE) performs comparable with
the state-of-the-art hand-crafted features (STATE),
especially on the Twitter-relevant test sets (T3
and T4)
7
. After feature combination, Coooolll
yields 4.22% and 3.07% improvement by macro-
F1 on T3 and T4,which verifies the effective-
ness of SSWE by learning discriminate features
from massive data for Twitter sentiment classifi-
cation. From the 45 teams, Coooolll gets the Rank
5/2/3/2 on T1-T4 respectively, along with the Se-
mEval 2013 participants owning larger training
data. We also comparing SSWE with the context-
based word embedding (W2V), which don?t cap-
ture the sentiment supervision of tweets. We find
that W2V is not effective enough for Twitter sen-
timent classification as there is a big gap between
W2V and SSWE on T1-T4. The reason is that W2V
does not capture the sentiment information of text,
which is crucial for sentiment analysis tasks and
effectively leveraged for learning the sentiment-
specific word embedding.
We also conduct experiments on the posi-
6
We utilize the Skip-gram model. The embedding is
trained from the 10M tweets collected by positive and neg-
ative emoticons, as same as the training data of SSWE.
7
The result of STATE on T3 is different from the results
reported in Mohammad et al. (2013) and Tang et al. (2014)
because we have different training data with the former and
different -wi of SVM with the latter.
tive/negative classification of tweets. The reason
is that the sentiment-specific word embedding is
learned from the positive/negative supervision of
tweets through emoticons, which is tailored for
positive/negative classification of tweets. From
Table 3 (right table), we find that the performance
of positive/negative Twitter classification is con-
sistent with the performance of 3-class classifica-
tion. SSWE performs comparable to STATE on T3
and T4, and yields better performance (1.62% and
1.45% improvements on T3 and T4, respectively)
through feature combination. SSWE outperform-
s W2V by large margins (more than 10%) on T3
and T4, which further verifies the effectiveness of
sentiment-specific word embedding.
4 Conclusion
We develop a deep learning system (Coooolll) for
message-level Twitter sentiment classification in
this paper. The feature representation of Cooool-
ll is composed of two parts, a state-of-the-art
hand-crafted features and the sentiment-specific
word embedding (SSWE) features. The SSWE
is learned from 10M tweets collected by posi-
tive and negative emoticons, without any manu-
al annotation. The effectiveness of Coooolll has
been verified in both positive/negative/neutral and
positive/negative classification of tweets. Among
45 systems of SemEval 2014 Task 9 subtask(b),
Coooolll yields Rank 2 on the Twitter2014 test set,
along with the SemEval 2013 participants owning
larger training data.
Acknowledgments
We thank Li Dong for helpful discussions. This
work was partly supported by National Natu-
ral Science Foundation of China (No.61133012,
No.61273321, No.61300113).
211
References
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computation-
al Linguistics, pages 655?665.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion, pages 321?327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13, pages
312?320.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2014. Radical-enhanced
chinese character embedding. arXiv preprint arX-
iv:1404.4714.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
212
