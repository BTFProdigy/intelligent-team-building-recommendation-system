Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 195?203,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation
Cem Akkaya
University of Pittsburgh
cem@cs.pitt.edu
Alexander Conrad
University of Pittsburgh
conrada@cs.pitt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
Amazon Mechanical Turk (MTurk) is a mar-
ketplace for so-called ?human intelligence
tasks? (HITs), or tasks that are easy for hu-
mans but currently difficult for automated pro-
cesses. Providers upload tasks to MTurk
which workers then complete. Natural lan-
guage annotation is one such human intelli-
gence task. In this paper, we investigate us-
ing MTurk to collect annotations for Subjec-
tivity Word Sense Disambiguation (SWSD),
a coarse-grained word sense disambiguation
task. We investigate whether we can use
MTurk to acquire good annotations with re-
spect to gold-standard data, whether we can
filter out low-quality workers (spammers), and
whether there is a learning effect associated
with repeatedly completing the same kind of
task. While our results with respect to spam-
mers are inconclusive, we are able to ob-
tain high-quality annotations for the SWSD
task. These results suggest a greater role for
MTurk with respect to constructing a large
scale SWSD system in the future, promising
substantial improvement in subjectivity and
sentiment analysis.
1 Introduction
Many Natural Language Processing (NLP) systems
rely on large amounts of manually annotated data
that is collected from domain experts. The anno-
tation process to obtain this data is very laborious
and expensive. This makes supervised NLP systems
subject to a so-called knowledge acquisition bottle-
neck. For example, (Ng, 1997) estimates an effort of
16 person years to construct training data for a high-
accuracy domain independent Word Sense Disam-
biguation (WSD) system.
Recently researchers have been investigating
Amazon Mechanical Turk (MTurk) as a source of
non-expert natural language annotation, which is a
cheap and quick alternative to expert annotations
(Kaisser and Lowe, 2008; Mrozinski et al, 2008).
In this paper, we utilize MTurk to obtain training
data for Subjectivity Word Sense Disambiguation
(SWSD) as described in (Akkaya et al, 2009). The
goal of SWSD is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with ob-
jective senses. SWSD is a new task which suffers
from the absence of a substantial amount of anno-
tated data and thus can only be applied on a small
scale. SWSD has strong connections to WSD. Like
supervised WSD, it requires training data where tar-
get word instances ? words which need to be dis-
ambiguated by the system ? are labeled as having
an objective sense or a subjective sense. (Akkaya
et al, 2009) show that SWSD may bring substantial
improvement in subjectivity and sentiment analysis,
if it could be applied on a larger scale. The good
news is that training data for 80 selected keywords is
enough to make a substantial difference (Akkaya et
al., 2009). Thus, large scale SWSD is feasible. We
hypothesize that annotations for SWSD can be pro-
vided by non-experts reliably if the annotation task
is presented in a simple way.
The annotations obtained from MTurk workers
are noisy by nature, because MTurk workers are
not trained for the underlying annotation task. That
is why previous work explored methods to assess
annotation quality and to aggregate multiple noisy
annotations for high reliability (Snow et al, 2008;
Callison-Burch, 2009). It is understandable that not
every worker will provide high-quality annotations,
195
depending on their background and interest. Un-
fortunately, some MTurk workers do not follow the
annotation guidelines and carelessly submit annota-
tions in order to gain economic benefits with only
minimal effort. We define this group of workers
as spammers. We believe it is essential to distin-
guish between workers as well-meaning annotators
and workers as spammers who should be filtered out
as a first step when utilizing MTurk. In this work,
we investigate how well the built-in qualifications in
MTurk function as such a filter.
Another important question about MTurk workers
is whether they learn to provide better annotations
over time in the absence of any interaction and feed-
back. The presence of a learning effect may support
working with the same workers over a long time and
creating private groups of workers. In this work, we
also examine if there is a learning effect associated
with MTurk workers.
To summarize, in this work we investigate the fol-
lowing questions:
? Can MTurk be utilized to collect reliable train-
ing data for SWSD ?
? Are the built-in methods provided by MTurk
enough to avoid spammers ?
? Is there a learning effect associated with MTurk
workers ?
The remainder of the paper is organized as fol-
lows. In Section 2, we give general background in-
formation on the Amazon Mechanical Turk service.
In Section 3, we discuss sense subjectivity. In Sec-
tion 4, we describe the subjectivity word sense dis-
ambiguation task. In Section 5, we discuss the de-
sign of our experiment and our filtering mechanisms
for workers. In Section 6, we evaluate MTurk anno-
tations and relate results to our questions. In Section
7, we review related work. In Section 8, we draw
conclusions and discuss future work.
2 Amazon Mechanical Turk
Amazon Mechanical Turk (MTurk)1 is a market-
place for so-called ?human intelligence tasks,? or
HITs. MTurk has two kinds of users: providers and
1http://mturk.amazon.com
workers. Providers create HITs using the Mechan-
ical Turk API and, for a small fee, upload them to
the HIT database. Workers search through the HIT
database, choosing which to complete in exchange
for monetary compensation. Anyone can sign up as
a provider and/or worker. Each HIT has an associ-
ated monetary value, and after reviewing a worker?s
submission, a provider may choose whether to ac-
cept the submission and pay the worker the promised
sum or to reject it and pay the worker nothing. HITs
typically consist of tasks that are easy for humans
but difficult or impossible for computers to complete
quickly or effectively, such as annotating images,
transcribing speech audio, or writing a summary of
a video.
One challenge for requesters using MTurk is that
of filtering out spammers and other workers who
consistently produce low-quality annotations. In or-
der to allow requesters to restrict the range of work-
ers who can complete their tasks, MTurk provides
several types of built-in statistics, known as quali-
fications. One such qualification is approval rating,
a statistic that records a worker?s ratio of accepted
HITs compared to the total number of HITs sub-
mitted by that worker. Providers can require that a
worker?s approval rating be above a certain threshold
before allowing that worker to submit one of his/her
HITs. Country of residence and lifetime approved
number of HITs completed also serve as built-in
qualifications that providers may check before al-
lowing workers to access their HITs.2 Amazon also
allows providers to define their own qualifications.
Typically, provider-defined qualifications are used to
ensure that HITs which require particular skills are
only completed by qualified workers. In most cases,
workers acquire provider-defined qualifications by
completing an online test.
Amazon also provides a mechanism by which
multiple unique workers can complete the same HIT.
The number of times a HIT is to be completed is
known as the number of assignments for the HIT.
By having multiple workers complete the same HIT,
2According to the terms of use, workers are prohibited from
having more than one account, but to the writer?s knowledge
there is no method in place to enforce this restriction. Thus,
a worker with a poor approval rating could simply create a
new account, since all accounts start with an approval rating
of 100%.
196
Subjective senses:
His alarm grew.
alarm, dismay, consternation ? (fear resulting from the aware-
ness of danger)
=> fear, fearfulness, fright ? (an emotion experienced in an-
ticipation of some specific pain or danger (usually accompa-
nied by a desire to flee or fight))
What?s the catch?
catch ? (a hidden drawback; ?it sounds good but what?s the
catch??)
=> drawback ? (the quality of being a hindrance; ?he
pointed out all the drawbacks to my plan?)
Objective senses:
The alarm went off.
alarm, warning device, alarm system ? (a device that signals the
occurrence of some undesirable event)
=> device ? (an instrumentality invented for a particular pur-
pose; ?the device is small enough to wear on your wrist?; ?a
device intended to conserve water?)
He sold his catch at the market.
catch, haul ? (the quantity that was caught; ?the catch was only
10 fish?)
=> indefinite quantity ? (an estimated quantity)
Figure 1: Subjective and objective word sense examples.
techniques such as majority voting among the sub-
missions can be used to aggregate the results for
some types of HITs, resulting in a higher-quality
final answer. Previous work (Snow et al, 2008)
demonstrates that aggregating worker submissions
often leads to an increase in quality.
3 Word Sense Subjectivity
(Wiebe and Mihalcea, 2006) define subjective ex-
pressions as words and phrases being used to ex-
press mental and emotional states, such as specula-
tions, evaluations, sentiments, and beliefs. Many ap-
proaches to sentiment and subjectivity analysis rely
on lexicons of such words (subjectivity clues). How-
ever, such clues often have both subjective and ob-
jective senses, as illustrated by (Wiebe and Mihal-
cea, 2006). Figure 1 provides subjective and objec-
tive examples of senses.
(Akkaya et al, 2009) points out that most sub-
jectivity lexicons are compiled as lists of keywords,
rather than word meanings (senses). Thus, subjec-
tivity clues used with objective senses ? false hits ?
are a significant source of error in subjectivity and
sentiment analysis. SWSD specifically deals with
this source of errors. (Akkaya et al, 2009) shows
that SWSD helps with various subjectivity and sen-
timent analysis systems by ignoring false hits.
4 Annotation Task
4.1 Subjectivity Word Sense Disambiguation
Our target task is Subjectivity Word Sense Disam-
biguation (SWSD). SWSD aims to determine which
word instances in a corpus are being used with sub-
jective senses and which are being used with ob-
jective senses. It can be considered to be a coarse-
grained application-specific WSD that distinguishes
between only two senses: (1) the subjective sense
and (2) the objective sense.
Subjectivity word sense annotation is done in the
following way. We try to keep the annotation task
for the worker as simple as possible. Thus, we do
not directly ask them if the instance of a target word
has a subjective or an objective sense (without any
sense inventory), because the concept of subjectivity
is fairly difficult to explain to someone who does not
have any linguistics background. Instead we show
MTurk workers two sets of senses ? one subjective
set and one objective set ? for a specific target word
and a text passage in which the target word appears.
Their job is to select the set that best reflects the
meaning of the target word in the text passage. The
specific sense set automatically gives us the subjec-
tivity label of the instance. This makes the annota-
tion task easier for them as (Snow et al, 2008) shows
that WSD can be done reliably by MTurk workers.
This approach presupposes a set of word senses that
have been annotated as subjective or objective. The
annotation of senses in a dictionary for subjectivity
is not difficult for an expert annotator. Moreover,
it needs to be done only once per target word, al-
lowing us to collect hundreds of subjectivity labeled
instances for each target word through MTurk.
In this annotation task, we do not inform the
MTurk workers about the nature of the sets. This
means the MTurk workers have no idea that they are
annotating subjectivity of senses; they are just se-
lecting the set which contains a sense matching the
usage in the sentence or being as similar to it as pos-
sible. This ensures that MTurk workers are not bi-
ased by the contextual subjectivity of the sentence
while tagging the target word instance.
197
Sense Set1 (Subjective)
{ look, appear, seem } ? give a certain impression or have a
certain outward aspect; ?She seems to be sleeping?; ?This ap-
pears to be a very difficult problem?; ?This project looks fishy?;
?They appeared like people who had not eaten or slept for a
long time?
{ appear, seem } ? seem to be true, probable, or apparent; ?It
seems that he is very gifted?; ?It appears that the weather in
California is very bad?
Sense Set2 (Objective)
{ appear } ? come into sight or view; ?He suddenly appeared
at the wedding?; ?A new star appeared on the horizon?
{ appear, come out } ? be issued or published, as of news in a
paper, a book, or a movie; ?Did your latest book appear yet??;
?The new Woody Allen film hasn?t come out yet?
{ appear, come along } ? come into being or existence, or ap-
pear on the scene; ?Then the computer came along and changed
our lives?; ?Homo sapiens appeared millions of years ago?
{ appear } ? appear as a character on stage or appear in a play,
etc.; ?Gielgud appears briefly in this movie?; ?She appeared in
?Hamlet? on the London
{ appear } ? present oneself formally, as before a (judicial) au-
thority; ?He had to appear in court last month?; ?She appeared
on several charges of theft?
Figure 2: Sense sets for target word ?appear?.
Below, we describe a sample annotation problem.
An MTurk worker has access to the following two
sense sets of the target word ?appear?, as seen in
Figure 2. The information that the first sense set is
subjective and second sense set is objective is not
available to the worker. The worker is presented
with the following text passage holding the target
word ?appear?.
It?s got so bad that I don?t even know what
to say. Charles |target| appeared |target|
somewhat embarrassed by his own behav-
ior. The hidden speech was coming, I
could tell.
In this passage, the MTurk worker should be able
to understand that ?appeared? refers to the outward
impression given by ?Charles?. This use of appear is
most similar to the first entry in sense set one; thus,
the correct answer for this problem is Sense Set-1.
4.2 Gold Standard
The gold standard dataset, on which we evaluate
MTurk worker annotations, is provided by (Akkaya
et al, 2009). This dataset (called subjSENSEVAL)
consists of target word instances in a corpus labeled
as S or O, indicating whether they are used with
a subjective or objective sense. It is based on the
lexical sample corpora from SENSEVAL1 (Kilgar-
riff and Palmer, 2000), SENSEVAL2 (Preiss and
Yarowsky, 2001), and SENSEVAL3 (Mihalcea and
Edmonds, 2004). SubjSENSEVAL consists of in-
stances for 39 ambiguous (having both subjective
and objective meanings) target words.
(Akkaya et al, 2009) also provided us with sub-
jectivity labels for word senses which are used in the
creation of subjSENSEVAL. Sense labels of the tar-
get word senses are defined on the sense inventory
of the underlying corpus (Hector for SENSEVAL1;
WordNet1.7 for SENSEVAL2; and WordNet1.7.1
for SENSEVAL3). This means the target words
from SENSEVAL1 have their senses annotated in
the Hector dictionary, while the target words from
SENSEVAL2 and SENSEVAL3 have their senses
annotated in WordNet1.7. We make use of these la-
beled sense inventories to build our subjective and
objective sets of senses, which we present to the
MTurk worker as Sense Set1 and Sense Set2 re-
spectively. We want to have a uniform sense rep-
resentation for the words we ask subjectivity sense
labels for. Thus, we consider only SENSEVAL2 and
SENSEVAL3 subsets of subjSENSEVAL, because
SENSEVAL1 relies on a sense inventory other than
WordNet.
5 Experimental Design
We chose randomly 8 target words that have a distri-
bution of subjective and objective instances in sub-
jSENSEVAL with less skew than 75%. That is, no
more than 75% of a word?s senses are subjective or
objective. Our concern is that using skewed data
might bias the workers to choose from the more fre-
quent label without thinking much about the prob-
lem. Another important fact is that these words with
low skew are more ambiguous and responsible for
more false hits. Thus, these target words are the ones
for which we really need subjectivity word sense
disambiguation. For each of these 8 target words, we
select 40 passages from subjSENSEVAL in which
the target word appears, to include in our experi-
ments. Table 1 summarizes the selected target words
198
Word FLP Word FLP
appear 55% fine 72.5%
judgment 65% solid 55%
strike 62.5% difference 67.5%
restraint 70% miss 50%
Average 62.2%
Table 1: Frequent label percentages for target words.
and their label distribution. In this table, frequent la-
bel percentage (FLP) represents the skew for each
word. A word?s FLP is equal to the percent of the
senses that are of the most frequently occurring type
of sense (subjective or objective) for that word.
We believe this annotation task is a good candi-
date for attracting spammers. This task requires only
binary annotations, where the worker just chooses
from one of the two given sets, which is not a dif-
ficult task. Since it is easy to provide labels, we
believe that there will be a distinct line, with re-
spect to quality of annotations, between spammers
and mediocre annotators.
For our experiments, we created three different
HIT groups each having different qualification re-
quirements but sharing the same data. To be con-
crete, each HIT group consists of the same 320 in-
stances: 40 instances for each target word listed in
Table 1. Each HIT presents an MTurk worker with
four instances of the same word in a text passage
? this makes 80 HITs for each HIT group ? and
asks him to choose the set to which the activated
sense belongs. We know for each HIT the mapping
between sense set numbers and subjectivity. Thus,
we can evaluate each HIT response on our gold-
standard data, as discussed in Section 4.2. We pay
seven cents per HIT. We consider this to be generous
compensation for such a simple task.
There are many builtin qualifications in MTurk.
We concentrated only on three of them: location,
HIT approval rate, and approved HITs, as discussed
in Section 2. In our experience, these qualifications
are widely used for quality assurance. As mentioned
before, we created three different HIT groups in or-
der to see how well different built-in qualification
combinations do with respect to filtering spammers.
These groups ? starting from the least constrained to
the most constrained ? are listed in Table 2.
Group1 Location: USA
Group2 Location: USAHIT Approval Rate > 96%
Group3
Location: USA
HIT Approval Rate > 96%
Approved HITs > 500
Table 2: Constraints for each HIT group.
Group1 required only that the MTurk workers are
located in the US. This group is the least constrained
one. Group2 additionally required an approval rate
greater than 96%. Group3 is the most constrained
one, requiring a lifetime approved HIT number to
be greater than 500, in addition to the qualifications
in Group1 and Group2.
We believe that neither location nor approval rate
and location together is enough to avoid spammers.
While being a US resident does to some extent guar-
antee English proficiency, it does not guarantee well-
thought answers. Since there is no mechanism in
place preventing users from creating new MTurk
worker accounts at will and since all worker ac-
counts are initialized with a 100% approval rate, we
do not think that approval rate is sufficient to avoid
serial spammers and other poor annotators. We hy-
pothesize that the workers with high approval rate
and a large number of approved HITs have a reputa-
tion to maintain, and thus will probably be careful in
their answers. We think it is unlikely that spammers
will have both a high approval rate and a large num-
ber of completed HITs. Thus, we anticipated that
Group3?s annotations will be of higher quality than
those of the other groups.
Note that an MTurk worker who has access to the
HITs in one of the HIT groups also has access to
HITs in less constrained groups. For example, an
MTurk worker who has access to HITs in Group3
also has access to HITs in Group2 and Group1. We
did not prevent MTurk workers from working in
multiple HIT groups because we did not want to
influence worker behavior, but instead simulate the
most realistic annotation scenario.
In addition to the qualifications described above,
we also required each worker to take a qualification
test in order to prove their competence in the anno-
tation task. The qualification test consists of 10 sim-
199
Figure 3: Venn diagram illustrating worker distribution.
ple annotation questions identical in form to those
present in the HITs. These questions are split evenly
between two target words, ?appear? and ?restraint?.
There are a total of five subjective and five objective
usages in the test. We required an accuracy of 90%
in the qualification test, corresponding to a Kappa
score of .80, before a worker was allowed to submit
any of our HITs. If a worker failed to achieve a score
of 90% on an attempt, that worker could try the test
again after a delay of 4 hours.
We collected three sets of assignments within
each HIT group. In other words, each HIT was com-
pleted three times by three different workers in each
group. This gives us a total of 960 assignments in
each HIT group. A total of 26 unique workers par-
ticipated in the experiment: 17 in Group1, 17 in
Group2 and 8 in Group3. As mentioned before, a
worker is able to participate in all the groups for
which he is qualified. Thus the unique worker num-
bers in each group does not sum up to the total num-
ber of workers in the experiment, since some work-
ers participated in the HITs for more than one group.
Figure 3 summarizes how workers are distributed
between groups.
6 Evaluation
We are interested in how accurate the MTurk annota-
tions are with respect to gold-standard data. We are
also interested in how the accuracy of each group
differs from the others. We evaluate each group it-
self separately on the gold-standard data. Addition-
ally, we evaluate each worker?s performance on the
gold-standard data and inspect their distribution in
various groups.
6.1 Group Evaluation
As mentioned in the previous section, we collect
three annotations for each HIT. They are assigned to
respective trials in the order submitted by the work-
ers. The results are summarized in Table 3. Trials
are labeled as TX and MV is the majority vote an-
notation among the three trials. The final column
contains the baseline agreement where a worker la-
bels each instance of a word with the most frequent
label of that word in the gold-standard data. It is
clear from this table that, since worker accuracy
always exceeds the baseline agreement, subjectiv-
ity word sense annotation can be done reliably by
MTurk workers. This is very promising. Consid-
ering the low cost and low time required to obtain
MTurk annotations, a large scale SWSD is realis-
tic. For example, (Akkaya et al, 2009) shows that
the most frequent 80 lexicon keywords are respon-
sible for almost half of the false hits in the MPQA
Corpus3 (Wiebe et al, 2005; Wilson, 2008), a cor-
pus annotated for subjective expressions. Utilizing
MTurk to collect training data for these 80 lexicon
keywords will be quick and cheap and most impor-
tantly reliable.
When we compare groups with each other, we
see that the best trial result is achieved in Group3.
However, according to McNemar?s test (Dietterich,
1998), there is no statistically significant difference
between any trial of any group. On the other hand,
the best majority vote annotation is achieved in
Group2, but again there is no statistically significant
difference between any majority vote annotation of
any group. These results are surprising to us, since
we do not see any significant difference in the qual-
ity of the data throughout different groups.
6.2 Worker Evaluation
In this section, we evaluate all 26 workers and group
them as either spammers or well-meaning workers.
All workers who deviate from the gold-standard by a
3http://www.cs.pitt.edu/mpqa/
200
Group3 Group2 Group1 baseline
T1 T2 T3 MV T1 T2 T3 MV T1 T2 T3 MV
Accuracy 89.7 86.9 86.6 88.4 87.2 86.3 88.1 90.3 84.4 87.5 87.5 88.4 62.2
Kappa .79 .74 .73 .77 .74 .73 .76 .81 .69 .75 .75 .77
Table 3: Accuracy and kappa scores for each group of workers.
Threshold 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
Spammer Count
G1 2 2 2 2 2 4 7 9
G2 1 2 2 2 2 3 5 8
G3 0 0 0 0 0 0 2 2
Spammer Percentage
G1 12% 12% 12% 12% 12% 24% 41% 53%
G2 6% 12% 12% 12% 12% 12% 29% 42%
G3 0% 0% 0% 0% 0% 0% 25% 25%
Table 4: Spammer representation in groups.
large margin beyond a certain threshold will be con-
sidered to be spammers. As discussed in Section 5,
we require all participating workers to pass a quali-
fication test before answering HITs. Thus, we know
that they are competent to do subjectivity sense an-
notations, and providing consistently erroneous an-
notations means that they are probably spammers.
We think a kappa score of 0.6 is a good threshold
to distinguish spammers from well-meaning work-
ers. For this threshold, we had 2 spammers par-
ticipating in Group1, 2 spammers in Group2 and
0 spammers in Group3. Table 4 presents spammer
count and spammer percentage in each group for
various threshold values. We see that Group3 has
consistently fewer spammers and a smaller spammer
percentage. The lowest kappa scores for Group1,
Group2, and Group3 are .35, .40, and .69, respec-
tively. The mean kappa scores for Group1, Group2,
and Group3 are .73, .75, and .77, respectively.
These results indicate that Group3 is less prone
to spammers, apparently contradicting Section 6.1.
We see the reason when we inspect the data more
closely. It turns out that spammers contributed in
Group1 and Group2 only minimally. On the other
hand there are two mediocre workers (Kappa of
0.69) who submit around 1/3 of the HITs in Group3.
This behavior might be a coincidence. In the face of
contradicting results, we think that we need a more
extensive study to derive conclusions about the rela-
tion between spammer distribution and built-in qual-
ification.
6.3 Learning Effect
Expert annotators can learn to provide more accu-
rate annotations over time. (Passonneau et al, 2006)
reports a learning effect early in the annotation pro-
cess. This might be due to the formal and informal
interaction between annotators. Another possibility
is that the annotators might get used to the annota-
tion task over time. This is to be expected if there is
not an extensive training process before the annota-
tion takes place.
On the other hand, the MTurk workers have no
interaction among themselves. They do not receive
any formal training and do not have access to true
annotations except a few examples if provided by
the requester. These properties make MTurk work-
ers a unique annotation workforce. We are interested
if the learning effect common to expert annotators
holds in this unique workforce in the absence of any
interaction and feedback. That may justify working
with the same set of workers over a long time by
creating private groups of workers.
We sort annotations of a worker after the submis-
sion date. This way, we get for each worker an or-
dered list of annotations. We split the list into bins
of size 40 and we test for an increasing trend in
the proportion of successes over time. We use the
Chi-squared Test for binomial proportions (Rosner,
2006). Using this test, we find that all of the p-values
201
are substantially larger than 0.05. Thus, there is no
increasing trend in the proportion of successes and
no learning effect. This is true for both mediocre
workers and very reliable workers. We think that the
results may differ for harder annotation tasks where
the input is more complex and requires some adjust-
ment.
7 Related Work
There has been recently an increasing interest in
Amazon Mechanical Turk. Many researchers have
utilized MTurk as a source of non-expert natural
language annotation to create labeled datasets. In
(Mrozinski et al, 2008), MTurk workers are used to
create a corpus of why-questions and corresponding
answers on which QA systems may be developed.
(Kaisser and Lowe, 2008) work on a similar task.
They make use of MTurk workers to identify sen-
tences in documents as answers and create a corpus
of question-answer sentence pairs. MTurk is also
considered in other fields than natural language pro-
cessing. For example, (Sorokin and Forsyth, 2008)
utilizes MTurk for image labeling. Our ultimate goal
is similar; namely, to build training data (in our case
for SWSD).
Several studies have concentrated specifically on
the quality aspect of the MTurk annotations. They
investigated methods to assess annotation quality
and to aggregate multiple noisy annotations for high
reliability. (Snow et al, 2008) report MTurk an-
notation quality on various NLP tasks (e.g. WSD,
Textual Entailment, Word Similarity) and define
a bias correction method for non-expert annota-
tors. (Callison-Burch, 2009) uses MTurk workers
for manual evaluation of automatic translation qual-
ity and experiments with weighed voting to com-
bine multiple annotations. (Hsueh et al, 2009) de-
fine various annotation quality measures and show
that they are useful for selecting annotations leading
to more accurate classifiers. Our work investigates
the effect of built-in qualifications on the quality of
MTurk annotations.
(Hsueh et al, 2009) applies MTurk to get senti-
ment annotations on political blog snippets. (Snow
et al, 2008) utilizes MTurk for affective text annota-
tion task. In both works, MTurk workers annotated
larger entities but on a more detailed scale than we
do. (Snow et al, 2008) also provides a WSD anno-
tation task which is similar to our annotation task.
The difference is the MTurk workers are choosing
an exact sense not a sense set.
8 Conclusion and Future Work
In this paper, we address the question of whether
built-in qualifications are enough to avoid spam-
mers. The investigation of worker performances
indicates that the lesser constrained a group is the
more spammers it attracts. On the other hand, we did
not find any significant difference between the qual-
ity of the annotations for each group. It turns out that
workers considered as spammers contributed only
minimally. We do not know if it is just a coincidence
or if it is correlated to the task definition. We did not
get conclusive results. We need to do more extensive
experiments before arriving at conclusions.
Another aspect we investigated is the learning ef-
fect. Our results show that there is no improvement
in annotator reliability over time. We should not ex-
pect MTurk workers to provide more consistent an-
notations over time. This will probably be the case
in similar annotation tasks. For harder annotation
tasks (e.g. parse tree annotation) things may be dif-
ferent. An interesting follow-up would be whether
showing the answers of other workers on the same
HIT will promote learning.
We presented our subjectivity sense annotation
task to the worker in a very simple way. The an-
notation results prove that subjectivity word sense
annotation can be done reliably by MTurk workers.
This is very promising since the MTurk annotations
can be collected for low costs in a short time pe-
riod. This implies that a large scale general SWSD
component, which can help with various subjectivity
and sentiment analysis tasks, is feasible. We plan to
work with selected workers to collect new annotated
data for SWSD and use this data to train a SWSD
system.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation awards IIS-
0916046 and IIS-0917170 and by Department of
Homeland Security award N000140710152. The au-
thors are grateful to the three paper reviewers for
their helpful suggestions.
202
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 286?295, Morristown, NJ,
USA. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10:1895?1923.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In HLT ?09: Proceedings
of the NAACL HLT 2009 Workshop on Active Learning
for Natural Language Processing, pages 27?35, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michael Kaisser and John Lowe. 2008. Creat-
ing a research collection of question answer sen-
tence pairs with amazons mechanical turk. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08). http://www.lrec-
conf.org/proceedings/lrec2008/.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a why-question corpus for develop-
ment and evaluation of an automatic QA-system. In
Proceedings of ACL-08: HLT, pages 443?451, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hwee Tou Ng. 1997. Getting serious about word sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why,What, and How?
Rebecca Passonneau, Nizar Habash, and Owen Rambow.
2006. Inter-annotator agreement on a multilingual se-
mantic annotation task. In Proceedings of the Fifth
International Conference on Language Resources and
Evaluation (LREC).
Bernard Rosner. 2006. Fundamentals of Biostatistics.
Thompson Brooks/Cole.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP ?08: Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 254?263, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
A. Sorokin and D. Forsyth. 2008. Utility data annotation
with amazon mechanical turk. pages 1 ?8, june.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In (ACL-06), Sydney, Australia.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2/3):164?
210.
Theresa Wilson. 2008. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
203
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 87?96,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Improving the Impact of Subjectivity Word Sense Disambiguation on
Contextual Opinion Analysis
Cem Akkaya, Janyce Wiebe, Alexander Conrad
University of Pittsburgh
Pittsburgh PA, 15260, USA
{cem,wiebe,conrada}@cs.pitt.edu
Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu
Abstract
Subjectivity word sense disambiguation
(SWSD) is automatically determining which
word instances in a corpus are being used with
subjective senses, and which are being used
with objective senses. SWSD has been shown
to improve the performance of contextual
opinion analysis, but only on a small scale and
using manually developed integration rules.
In this paper, we scale up the integration of
SWSD into contextual opinion analysis and
still obtain improvements in performance,
by successfully gathering data annotated by
non-expert annotators. Further, by improving
the method for integrating SWSD into con-
textual opinion analysis, even greater benefits
from SWSD are achieved than in previous
work. We thus more firmly demonstrate the
potential of SWSD to improve contextual
opinion analysis.
1 Introduction
Often, methods for opinion, sentiment, and sub-
jectivity analysis rely on lexicons of subjective
(opinion-carrying) words (e.g., (Turney, 2002;
Whitelaw et al, 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al, 2007; Andreevskaia and Bergler, 2008;
Agarwal et al, 2009)). Examples of such words are
the following (in bold):
(1) He is a disease to every team he has gone to.
Converting to SMF is a headache.
The concert left me cold.
That guy is such a pain.
However, even manually developed subjectiv-
ity lexicons have significant degrees of subjectivity
sense ambiguity (Su and Markert, 2008; Gyamfi et
al., 2009). That is, many clues in these lexicons have
both subjective and objective senses. This ambiguity
leads to errors in opinion and sentiment analysis, be-
cause objective instances represent false hits of sub-
jectivity clues. For example, the following sentence
contains the keywords from (1) used with objective
senses:
(2) Early symptoms of the disease include severe
headaches, red eyes, fevers and cold chills, body
pain, and vomiting.
Recently, in (Akkaya et al, 2009), we introduced
the task of subjectivity word sense disambiguation
(SWSD), which is to automatically determine which
word instances in a corpus are being used with sub-
jective senses, and which are being used with objec-
tive senses. We developed a supervised system for
SWSD, and exploited the SWSD output to improve
the performance of multiple contextual opinion anal-
ysis tasks.
Although the reported results are promising, there
are three obvious shortcomings. First, we were able
to apply SWSD to contextual opinion analysis only
on a very small scale, due to a shortage of anno-
tated data. While the experiments show that SWSD
improves contextual opinion analysis, this was only
on the small amount of opinion-annotated data that
was in the coverage of our system. Two questions
arise: is it feasible to obtain greater amounts of
the needed data, and do SWSD performance im-
provements on contextual opinion analysis hold on a
87
larger scale. Second, the annotations in (Akkaya et
al., 2009) are piggy-backed on SENSEVAL sense-
tagged data, which are fine-grained word sense an-
notations created by trained annotators. A concern
is that SWSD performance improvements on con-
textual opinion analysis can only be achieved using
such fine-grained expert annotations, the availability
of which is limited. Third, (Akkaya et al, 2009) uses
manual rules to apply SWSD to contextual opinion
analysis. Although these rules have the advantage
that they transparently show the effects of SWSD,
they are somewhat ad hoc. Likely, they are not opti-
mal and are holding back the potential of SWSD to
improve contextual opinion analysis.
To address these shortcomings, in this paper, we
investigate (1) the feasibility of obtaining a substan-
tial amount of annotated data, (2) whether perfor-
mance improvements on contextual opinion analy-
sis can be realized on a larger scale, and (3) whether
those improvements can be realized with subjectiv-
ity sense tagged data that is not built on expert full-
inventory sense annotations. In addition, we explore
better methods for applying SWSD to contextual
opinion analysis.
2 Subjectivity Word Sense Disambiguation
2.1 Annotation Tasks
We adopt the definitions of subjective (S) and ob-
jective (O) from (Wiebe et al, 2005; Wiebe and Mi-
halcea, 2006; Wilson, 2007). Subjective expressions
are words and phrases being used to express mental
and emotional states, such as speculations, evalua-
tions, sentiments, and beliefs. A general covering
term for such states is private state (Quirk et al,
1985), an internal state that cannot be directly ob-
served or verified by others. Objective expressions
instead are words and phrases that lack subjectivity.
The contextual opinion analysis experiments de-
scribed in Section 3 include both S/O and polar-
ity (positive,negative, neutral) classifications. The
opinion-annotated data used in those experiments is
from the MPQA Corpus (Wiebe et al, 2005; Wilson,
2007),1 which consists of news articles annotated for
subjective expressions, including polarity.
1Available at http://www.cs.pitt.edu/mpqa
2.1.1 Subjectivity Sense Labeling
For SWSD, we need the notions of subjective
and objective senses of words in a dictionary. We
adopt the definitions from (Wiebe and Mihalcea,
2006), who describe the annotation scheme as fol-
lows. Classifying a sense as S means that, when
the sense is used in a text or conversation, one ex-
pects it to express subjectivity, and also that the
phrase or sentence containing it expresses subjectiv-
ity. As noted in (Wiebe and Mihalcea, 2006), sen-
tences containing objective senses may not be objec-
tive. Thus, objective senses are defined as follows:
Classifying a sense as O means that, when the sense
is used in a text or conversation, one does not expect
it to express subjectivity and, if the phrase or sen-
tence containing it is subjective, the subjectivity is
due to something else.
Both (Wiebe and Mihalcea, 2006) and (Su and
Markert, 2008) performed agreement studies of the
scheme and report that good agreement can be
achieved between human annotators labeling the
subjectivity of senses (? values of 0.74 and 0.79, re-
spectively).
(Akkaya et al, 2009) followed the same annota-
tion scheme to annotate the senses of the words used
in the experiments. For this paper, we again use
the same scheme and annotate WordNet senses of
90 new words (the process of selecting the words is
described in Section 2.4).
2.1.2 Subjectivity Sense Tagging
The training and test data for SWSD consists of
word instances in a corpus labeled as S or O, in-
dicating whether they are used with a subjective or
objective sense.
Because there was no such tagged data at the time,
(Akkaya et al, 2009) created a data set by com-
bining two types of sense annotations: (1) labels of
senses within a dictionary as S or O (i.e., the subjec-
tivity sense labels of the previous section), and (2)
sense tags of word instances in a corpus (i.e., SEN-
SEVAL sense-tagged data).2 The subjectivity sense
labels were used to collapse the sense labels in the
sense-tagged data into the two new senses, S and O.
The target words (Akkaya et al, 2009) chose are the
words tagged in SENSEVAL that are also members
2Please see the paper for details on the SENSEVAL data
used in the experiments.
88
Sense Set1 (Subjective)
{ attack, round, assail, lash out, snipe, assault } ? attack in
speech or writing; ?The editors attacked the House Speaker?
{ assail, assault, set on, attack } ? attack someone emotionally;
?Nightmares assailed him regularly?
Sense Set2 (Objective)
{ attack } ? begin to injure; ?The cancer cells are attacking his
liver?; ?Rust is attacking the metal?
{ attack, aggress } ? take the initiative and go on the offensive;
?The visiting team started to attack?
Figure 1: Sense sets for target word ?attack? (abridged).
of the subjectivity lexicon of (Wilson et al, 2005;
Wilson, 2007).3 There are 39 such words. (Akkaya
et al, 2009) chose words from a subjectivity lexicon
because such words are known to have subjective
usages.
For this paper, subjectivity sense-tagged data was
obtained from the MTurk workers using the anno-
tation scheme of (Akkaya et al, 2010). A goal is to
keep the annotation task as simple as possible. Thus,
the workers are not directly asked if the instance of
a target word has a subjective or an objective sense,
because the concept of subjectivity would be diffi-
cult to explain in this setting. Instead the workers
are shown two sets of senses ? one subjective set and
one objective set ? for a specific target word and a
text passage in which the target word appears. Their
job is to select the set that best reflects the meaning
of the target word in the text passage. The set they
choose gives us the subjectivity label of the instance.
A sample annotation task is shown below. An
MTurk worker has access to two sense sets of the
target word ?attack? as seen in Figure 1. The S and
O labels appear here only for the purpose of this pa-
per; the workers do not see them. The worker is pre-
sented with the following text passage holding the
target word ?attack?:
Ivkovic had been a target of intra-party
feuding that has shaken the party. He was
attacked by Milosevic for attempting to
carve out a new party from the Socialists.
In this passage, the use of ?attack? is most similar
to the first entry in sense set one; thus, the correct
answer for this problem is Sense Set-1.
3Available at http://www.cs.pitt.edu/mpqa
(Akkaya et al, 2010) carried out a pilot study
where a subjectivity sense-tagged dataset was cre-
ated for eight SENSEVAL words through MTurk.
(Akkaya et al, 2010) evaluated the non-expert la-
bel quality against gold-standard expert labels which
were obtained from (Akkaya et al, 2009) relying
on SENSEVAL. The non-expert annotations are reli-
able, achieving ? scores around 0.74 with the expert
annotations.
For some words, there may not be a clean split be-
tween the subjective and objective senses. For these,
we opted for another strategy for obtaining MTurk
annotations. Rather than presenting the workers
with WordNet senses, we show them a set of objec-
tive usages, a set of subjective usages, and a text pas-
sage in which the target word appears. The workers?
job is to judge which set of usages the target instance
is most similar to.
2.2 SWSD System
We follow the same approach as in (Akkaya et al,
2009) to build our SWSD system. We train a differ-
ent supervised SWSD classifier for each target word
separately. This means the overall SWSD system
consists of as many SWSD classifiers as there are
target words. We utilize the same machine learning
features as in (Akkaya et al, 2009), which are com-
monly used in Word Sense Disambiguation (WSD).
2.3 Expert SWSD vs. Non-expert SWSD
Before creating a large subjectivity sense-tagged
corpus via MTurk, we want to make sure that non-
expert annotations are good enough to train reliable
SWSD classifiers. Thus, we decided to compare
the performance of a SWSD system trained on non-
expert annotations and on expert annotations. For
this purpose, we need a subjectivity sense-tagged
corpus where word instances are tagged both by ex-
pert and non-expert annotations. Fortunately, we
have such a corpus. As discussed in Section 3,
(Akkaya et al, 2009) created a subjecvitivity sense-
tagged corpus piggybacked on SENSEVAL. This
gives us a gold-standard corpus tagged by experts.
There is also a small subjectivity sense-tagged cor-
pus consisting of eight target words obtained from
non-expert annotators in (Akkaya et al, 2010). This
corpus is a subset of the gold-standard corpus from
(Akkaya et al, 2009) and it consists of 60 tagged
89
Acc p-value
SWSDGOLD 79.2 -
SWSDMJL 78.4 0.542
SWSDMJC 78.8 0.754
Table 1: Comparison of SWSD systems
instances for each target word.
Actually, (Akkaya et al, 2010) gathered three la-
bels for each instance. This gives us two options
to train the non-expert SWSD system: (1) training
the system on the majority vote labels (SWSDMJL)
(2) training three systems on the three separate la-
bel sets and taking the majority vote prediction
(SWSDMJC). Additionally, we train an expert SWSD
system (SWSDGOLD) ? a system trained on gold
standard expert annotations. All these systems are
trained on 60 instances of the eight target words for
which we have both non-expert and expert annota-
tions and are evaluated on the remaining instances
of the gold-standard corpus. This makes a total of
923 test instances for the eight target words with a
majority class baseline of 61.8.
Table 1 reports micro-average accuracy of each
system and the two-tailed p-value between the ex-
pert SWSD system and the two non-expert SWSD
systems. The p-value is calculated with McNemar?s
test. It shows that there is no statistically signif-
icant difference between classifiers trained on ex-
pert gold-standard annotations and non-expert anno-
tations. We adopt SWSDMJL in all our following ex-
periments, because it is more efficient.
2.4 Corpus Creation
For our experiments, we have multiple goals, which
effect our decisions on how to create the subjectiv-
ity sense-tagged corpus via MTurk. First, we want
to be able to disambiguate more target words than
(Akkaya et al, 2009). This way, SWSD will be able
to disambiguate a larger portion of the MPQA Cor-
pus allowing us to evaluate the effect of SWSD on
contextual opinion analysis on a larger scale. This
will also allow us to investigate additional integra-
tion methods of SWSD into contextual opinion anal-
ysis rather than simple ad hoc manual rules utilized
in (Akkaya et al, 2009). Second, we want to show
that we can rely on non-expert annotations instead of
expert annotations, which will make an annotation
effort on a larger-scale both practical and feasible,
timewise and costwise. Optimally, we could have
annotated via MTurk the same subjectivity sense-
tagged corpus from (Akkaya et al, 2009) in order to
compare the effect of a non-expert SWSD system on
contextual opinion analysis directly with the results
reported for an expert SWSD system in (Akkaya et
al., 2009). But, this would have diverted our re-
sources to reproduce the same corpus and contradict
our goal to extend the subjectivity sense-tagged cor-
pus to new target words. Moreover, we have already
shown in Section 2.3 that non-expert annotations can
be utilized to train reliable SWSD classifiers. It is
reasonable to believe that similar performance on
the SWSD task will reflect to similar improvements
on contextual opinion analysis. Thus, we decided
to prioritize creating a subjectivity sense-tagged cor-
pus for a totally new set of words. We aim to show
that the favourable results reported in (Akkaya et al,
2009) will still hold on new target words relying on
non-expert annotations.
We chose our target words from the subjectivity
lexicon of (Wilson et al, 2005), because we know
they have subjective usages. The contextual opin-
ion systems we want to improve rely on this lexicon.
We call the words in the lexicon subjectivity clues.
At this stage, we want to concentrate on the fre-
quent and ambiguous subjectivity clues. We chose
frequent ones, because they will have larger cov-
erage in the MPQA Corpus. We chose ambiguous
ones, because these clues are the ones that are most
important for SWSD. Choosing most frequent and
ambiguous subjectivity clues guarantees that we uti-
lize our limited resources in the most efficient way.
We judge a clue to be ambiguous if it appears more
than 25% and less than 75% of the times in a sub-
jective expression. We get these statistics by simply
counting occurrences in the MPQA Corpus inside
and outside of subjective expressions.
There are 680 subjectivity clues that appear in the
MPQA Corpus and are ambiguous. Out of those, we
selected the 90 most frequent that have to some ex-
tent distinct objective and subjective senses in Word-
Net, as judged by the co-authors. The co-authors an-
notated the WordNet senses of those 90 target words.
For each target word, we selected approximately 120
instances randomly from the GIGAWORD Corpus.
In a first phase, we collected three sets of MTurk an-
90
notations for the selected instances. In this phase,
MTurk workers base their judgements on two sense
sets they observe. This way, we get training data to
build SWSD classifiers for these 90 target words.
The quality of these classifiers is important, be-
cause we will exploit them for contextual opinion
analysis. Thus, we evaluate them by 10-fold cross-
validation. We split the target words into three
groups. If the majority class baseline of a word is
higher than 90%, it is considered as skewed (skewed
words have a performance at least as good as the ma-
jority class baseline). If a target word improves over
its majority class baseline by 25% in accuracy, it is
considered as good. Otherwise, it is considered as
mediocre. This way, we end up with 24 skewed, 35
good, and 31 mediocre words. There are many pos-
sible reasons for the less reliable performance for
the mediocre group. We hypothesize that a major
problem is the similarity between the objective and
subjective sense sets of a word, thus leading to poor
annotation quality. To check this, we calculate the
agreement between three annotation sets and report
averages. The agreement in the mediocre group is
78.68%, with a ? value of 0.57, whereas the aver-
age agreement in the good group is 87.51%, with
a ? value of 0.75. These findings support our hy-
pothesis. Thus, the co-authors created usage inven-
tories for the words in the mediocre group as de-
scribed in Section 2.1.1. We initiated a second phase
of MTurk annotations. We collect for the mediocre
group another three sets of MTurk annotations for
120 instances, this time utilizing usage inventories.
The 10-fold cross-validation experiments show that
nine of the 31 words in the mediocre group shift to
the good group. Only for these nine words, we ac-
cept the annotations collected via usage inventories.
For all other words, we use the annotations collected
via sense inventories. From now on, we will refer
to this non-expert subjectivity sense-tagged corpus
consisting of the tagged data for all 90 target words
as the MTurkSWSD Corpus (agreement on the entire
MTurkSWSD corpus is 85.54%, ?:0.71).
3 SWSD Integration
Now that we have the MTurkSWSD Corpus, we
are ready to evaluate the effect of SWSD on con-
textual opinion analysis. In this section, we ap-
ply our SWSD system trained on MTurkSWSD to
both expression-level classifiers from (Akkaya et al,
2009): (1) the subjective/objective (S/O) classifier
and (2) the contextual polarity classifier. Both clas-
sifiers are introduced in Section 3.1
Our SWSD system can disambiguate 90 target
words, which have 3737 instances in the MPQA
Corpus. We refer to this subset of the MPQA Corpus
as MTurkMPQA. This subset makes up the cover-
age of our SWSD system. Note that MTurkMPQA
is 5.2 times larger than the covered MPQA subset
in (Akkaya et al, 2009) referred as senMPQA. We
try different strategies to integrate SWSD into the
contextual classifiers. In Section 3.2, we follow the
same rule-based strategy as in (Akkaya et al, 2009)
for completeness. In Section 3.3, we introduce two
new learning strategies for SWSD integration out-
performing existing rule-based strategy. We evalu-
ate the improvement gained by SWSD on MTurkM-
PQA.
3.1 Contextual Classifiers
The original contextual polarity classifier is intro-
duced in (Wilson et al, 2005). We use the same im-
plementation as in (Akkaya et al, 2009). This classi-
fier labels clue instances in text as contextually neg-
ative/positive/neutral. The gold standard is defined
on the MPQA Corpus as follows. If a clue instance
appears in a positive expression, it is contextually
positive (Ps). If it appears in a negative expression,
it is contextually negative (Ng). If it is in an objec-
tive expression or in a neutral subjective expression,
it is contextually neutral (N). The contextual polar-
ity classifier consists of two separate steps. The first
step is an expression-level neutral/polar (N/P) clas-
sifier. The second step classifies only polar instances
further into positive and negative classes. This way,
the overall system performs a three-way classifica-
tion (Ng/Ps/N).
The subjective/objective classifier is introduced in
(Akkaya et al, 2009). It relies on the same machine
learning features as the N/P classifier (i.e. the first
step of the contextual polarity classifier). The only
difference is that the classes are S/O instead of N/P.
The gold standard is defined on the MPQA Corpus
in the following way. If a clue instance appears in
a subjective expression, it is contextually S. If it ap-
pears in an objective expression, it is contextually O.
Both contextual classifiers are supervised.
91
Baseline Acc OF SF
MTurkMPQA 52.4% (O)
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
senMPQA 63.1% (O)
OS/O 75.4 65.4 80.9
R1R2 81.3 75.9 84.8
Table 2: S/O classifier with and without SWSD.
3.2 Rule-Based SWSD Integration
(Akkaya et al, 2009) integrates SWSD into a con-
textual classifier by simple rules. The rules flip the
output of the contextual classifier if some conditions
hold. They make use of following information: (1)
SWSD output, (2) the contextual classifier?s confi-
dence and (3) the presence of another subjectivity
clue ? any clue from the subjectivity lexicon ? in the
same expression.
For the contextual S/O classifier, (Akkaya et al,
2009) defines two rules: one flipping the S/O classi-
fier?s output from O to S (R1) and one flipping from
S to O (R2). R1 is defined as follows : if the contex-
tual classifier decides a target word instance is con-
textually O and SWSD decides that it is used in a S
sense, then SWSD overrules the contextual S/O clas-
sifier?s output and flips it from O to S, because an
instance in a S sense will make the surrounding ex-
pression subjective. R2 is a little bit more complex.
It is defined as follows: If the contextual classifier la-
bels a clue instance as S but (1) SWSD decides that
it is used in an O sense, (2) the contextual classifier?s
confidence is low, and (3) there is no other subjec-
tivity clue in the same expression, then R2 flips the
contextual classifier?s output from S to O. The ra-
tionale behind R2 is that even if the target word in-
stance has an O sense, there might be another reason
(e.g. the presence of another subjectivity clue in the
same expression) for the expression enclosing it to
be subjective.
We use the exact same rules and adopt the same
confidence threshold. Table 2 holds the comparison
of the original contextual classifier and the classi-
fier with SWSD support on senMPQA as reported in
(Akkaya et al, 2009) and on MTurkMPQA. OS/O is
the original S/O classifier; R1R2 is the system with
SWSD support utilizing both rules. We report only
R1R2, since (Akkaya et al, 2009) gets highest im-
provement utilizing both rules.
Baseline Acc NF PF
MTurkMPQA 70.6% (P)
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
senMPQA 73.9% (P)
ON/P 79.0 86.7 50.3
R4 81.6 88.6 52.3
Table 3: N/P classifier with and without SWSD
In Table 2 we see that R1R2 achieves 4% percent-
age points improvement in accuracy over OS/O on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test. It
is accompanied with improvements both in subjec-
tive F-measure (SF) and objective F-measure (OF).
It is not possible to directly compare improvements
on senMPQA and MTurkMPQA since they are dif-
ferent subsets of the MPQA Corpus. SWSD support
brings 24% error reduction on senMPQA over the
original S/O classifier. In comparison, on MTurkM-
PQA, the error reduction is 12%. We see that the im-
provements on the large MTurkMPQA set still hold,
but not as strong as in (Akkaya et al, 2009).
(Akkaya et al, 2009) uses a similar rule to
make the contextual polarity classifier sense-aware.
Specifically, the rule is applied to the output of the
first step (N/P classifier). The rule, R4, flips P to N
and is analogous to R2. If the contextual classifier
labels a clue instance as P but (1) SWSD decides
that it is used in an O sense, (2) the contextual clas-
sifier?s confidence is low, and (3) there is no other
clue instance in the same expression, then R4 flips
the contextual classifier?s output from P to N.
Table 3 holds the comparison of the original N/P
classifier with and without SWSD support on sen-
MPQA as reported in (Akkaya et al, 2009) and on
MTurkMPQA. ON/P is the original N/P classifier; R4
is the system with SWSD support utilizing rule R4.
Since our main focus is not rule-based integration,
we did not run the second step of the polarity classi-
fier. We report the second step result below for the
learning-based SWSD integration in section 3.4.
In Table 3, we see that R4 achieves 2.2 percent-
age points improvement in accuracy over ON/P on
MTurkMPQA. The improvement is statistically sig-
nificant at the p < .01 level with McNemar?s test.
It is accompanied with improvement only in objec-
tive F-measure (OF). SWSD support brings 12.4%
error reduction on senMPQA (Akkaya et al, 2009).
92
On MTurkMPQA, the error reduction is 8%. We see
that the rule-based SWSD integration still improves
both contextual classifiers on MTurkMPQA, but the
gain is not as large as on senMPQA. This might be
due to the brittleness of the rule-based integration.
3.3 Learning SWSD Integration
Now that we can disambiguate a larger portion of
the MPQA Corpus than in (Akkaya et al, 2009),
we can investigate machine learning methods for
SWSD integration to deal with the brittleness of the
rule-based integration. In this section, we introduce
two learning methods to apply SWSD to the contex-
tual classifiers. For the learning methods, we rely on
exactly the same information as the rule-based inte-
gration: (1) SWSD output, (2) the contextual clas-
sifier?s output, (3) the contextual classifier?s confi-
dence, and (4) the presence of another clue instance
in the same expression. The rationale is the same as
for the rule-based integration, namely to relate sense
subjectivity and contextual subjectivity.
3.3.1 Method1
In the first method, we extend the machine learn-
ing features of the underlying contextual classifiers
by adding (1) and (4) from above. We evaluate the
extended contextual classifiers on MTurkMPQA via
10-fold cross-validation. Tables 4 and 5 hold the
comparison of Method1 (EXTS/O, EXTN/P) to the
original contextual classifiers (OS/O, ON/P) and to the
rule-based SWSD integration (R1R2, R4). We see
substantial improvement for Method1. It achieves
39% error reduction over OS/O and 25% error reduc-
tion over ON/P. For both classifiers, the improvement
in accuracy over the rule-based integration is statisti-
cally significant at the p< .01 level with McNemar?s
test.
3.3.2 Method2
This method defines a third classifier that accepts
as input the contextual classifier?s output and the
SWSD output and predicts what the contextual clas-
sifier?s output should have been. We can think of
this third classifier as the learning counterpart of
the manual rules from Section 3.2, since it actu-
ally learns when to flip the contextual classifier?s
output considering SWSD evidence. Specifically,
this merger classifier relies on four machine learn-
ing features (1), (2), (3), (4) from above (the ex-
Acc OF SF
OS/O 67.1 68.9 65.0
R1R2 71.1 72.7 69.2
EXTS/O 80.0 81.4 78.3
MERGERS/O 78.2 80.3 75.5
Table 4: S/O classifier with learned SWSD integration
Acc NF PF
ON/P 72.3 82.0 39.8
R4 74.5 84.0 37.8
EXTN/P 79.1 85.7 61.1
MERGERN/P 80.4 86.7 62.8
Table 5: N/P classifier with learned SWSD integration
act same information used in rule-based integration).
Because it is a supervised classifier, we need train-
ing data where we have clue instances with cor-
responding contextual classifier and SWSD predic-
tions. Fortunately, we can use senMPQA for this
purpose. We train our merger classifier on senM-
PQA (we get contextual classifier predictions via 10-
fold cross-validation on the MPQA Corpus) and ap-
ply it to MTurkMPQA. We use SVM classifier from
the Weka package (Witten and Frank., 2005) with
its default settings. Tables 4 and 5 hold the com-
parison of Method2 (MERGERS/O, MERGERN/P) to
the original contextual classifiers (Oo/s, ON/P) and
the rule-based SWSD integration (R1R2, R4). It
achieves 29% error reduction over OS/O and 29% er-
ror reduction over ON/P. The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. Method2
performs better (statistically significant at the p <
.05 level) than Method1 for the N/P classifier but
worse (statistically significant at the p < .01 level)
for the S/O classifier.
3.4 Improving Contextual Polarity
Classification
We have seen that Method2 is the best method to
improve the N/P classifier, which is the first step
of the contextual polarity classifier. To assess the
overall improvement in polarity classification, we
run the second step of the contextual polarity clas-
sifier after correcting the first step with Method2.
Table 6 summarizes the improvement propagated to
93
Acc NF NgF PsF
MTurkMPQA
OPs/Ng/N 72.1 83.0 34.2 15.0
MERGERN/P 77.8 87.4 53.0 27.7
senMPQA
OPs/Ng/N 77.6 87.2 39.5 40.0
R4 80.6 89.1 43.2 44.0
Table 6: Polarity classifier with and without SWSD.
Ps/Ng/N classification. For comparison, we also
include results from (Akkaya et al, 2009) on sen-
MPQA. Method2 results in 20% error reduction in
accuracy over OPs/Ng/N (R4 achieves 13.4% error
reduction on senMPQA). The improvement on the
rule-based integration is statistically significant at
the p < .01 level with McNemar?s test. More im-
portantly, the F-measure for all the labels improves.
This indicates that non-expert MTurk annotations
can replace expert annotations for our end-goal ? im-
proving contextual opinion analysis ? while reduc-
ing time and cost requirements by a large margin.
Moreover, we see that the improvements in (Akkaya
et al, 2009) scale up to new subjectivity clues.
4 Related Work
One related line of research is to automatically
assign subjectivity and/or polarity labels to word
senses in a dictionary (Valitutti et al, 2004; An-
dreevskaia and Bergler, 2006; Wiebe and Mihalcea,
2006; Esuli and Sebastiani, 2007; Su and Markert,
2009). In contrast, the task in our paper is to auto-
matically assign labels to word instances in a corpus.
Recently, some researchers have exploited full
word sense disambiguation in methods for opinion-
related tasks. For example, (Mart??n-Wanton et al,
2010) exploit WSD for recognizing quotation polar-
ities, and (Rentoumi et al, 2009; Mart??n-Wanton et
al., 2010) exploit WSD for recognizing headline po-
larities. None of this previous work investigates per-
forming a coarse-grained variation of WSD such as
SWSD to improve their application results, as we do
in this work.
A notable exception is (Su and Markert, 2010),
who exploit SWSD to improve the performance on
a contextual NLP task, as we do. While the task
in our paper is subjectivity and sentiment analy-
sis, their task is English-Chinese lexical substitu-
tion. As (Akkaya et al, 2009) did, they anno-
tated word senses, and exploited SENSEVAL data
as training data for SWSD. They did not directly an-
notate words in context with S/O labels, as we do in
our work. Further, they did not separately evaluate a
SWSD system component.
Many researchers work on reducing the granular-
ity of sense inventories for WSD (e.g., (Palmer et al,
2004; Navigli, 2006; Snow et al, 2007; Hovy et al,
2006)). Their criteria for grouping senses are syn-
tactic and semantic similarities, while the groupings
in work on SWSD are driven by the goals to improve
contextual subjectivity and sentiment analysis.
5 Conclusions and Future Work
In this paper, we utilized a large pool of non-expert
annotators (MTurk) to collect subjectivity sense-
tagged data for SWSD. We showed that non-expert
annotations are as good as expert annotations for
training SWSD classifiers. Moreover, we demon-
strated that SWSD classifiers trained on non-expert
annotations can be exploited to improve contextual
opinion analysis.
The additional subjectivity sense-tagged data en-
abled us to evaluate the benefits of SWSD on con-
textual opinion analysis on a corpus of opinion-
annotated data that is five times larger. Using the
same rule-based integration strategies as in (Akkaya
et al, 2009), we found that contextual opinion anal-
ysis is improved by SWSD on the larger datasets.
We also experimented with new learning strategies
for integrating SWSD into contextual opinion analy-
sis. With the learning strategies, we achieved greater
benefits from SWSD than the rule-based integration
strategies on all of the contextual opinion analysis
tasks.
Overall, we more firmly demonstrated the poten-
tial of SWSD to improve contextual opinion analy-
sis. We will continue to gather subjectivity sense-
tagged data, using sense inventories for words that
are well represented in WordNet for our purposes,
and with usage inventories for those that are not.
6 Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.
94
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 24?32. Asso-
ciation for Computational Linguistics.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 190?199, Singa-
pore, August. Association for Computational Linguis-
tics.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 195?203, Los Angeles, June. Associa-
tion for Computational Linguistics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings of the 11rd
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006).
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Overcom-
ing domain dependence in sentiment tagging. In Pro-
ceedings of ACL-08: HLT, pages 290?298, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308?315, Rochester, NY.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
424?431, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2009), pages
10?18, Boulder, Colorado, June. Association for Com-
putational Linguistics.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of the Human Language Technology
Conference of the NAACL, Companion Volume: Short
Papers, New York City.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the Twen-
tieth International Conference on Computational Lin-
guistics (COLING 2004), pages 1267?1373, Geneva,
Switzerland.
Tamara Mart??n-Wanton, Aurora Pons-Porrata, Andre?s
Montoyo-Guijarro, and Alexandra Balahur. 2010.
Opinion polarity detection - using word sense disam-
biguation to determine the polarity of opinions. In
ICAART 2010 - Proceedings of the International Con-
ference on Agents and Artificial Intelligence, Volume
1, pages 483?486.
R. Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia.
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004.
Different sense granularities for different applications.
In HLT-NAACL 2004 Workshop: 2nd Workshop on
Scalable Natural Language Understanding, Boston,
Massachusetts.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sentiment
analysis of figurative language using a word sense
disambiguation approach. In Proceedings of the In-
ternational Conference RANLP-2009, pages 370?375,
Borovets, Bulgaria, September. Association for Com-
putational Linguistics.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-2003), pages 105?
112, Sapporo, Japan.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007.
Learning to merge word senses. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic.
Fangzhong Su and Katja Markert. 2008. From word
to sense: a case study of subjectivity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-2008), Manch-
ester.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 1?9, Boulder, Colorado, June. Associ-
ation for Computational Linguistics.
95
Fangzhong Su and Katja Markert. 2010. Word sense
subjectivity for cross-lingual lexical substitution. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 357?
360, Los Angeles, California, June. Association for
Computational Linguistics.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-02), pages 417?424, Philadelphia, Pennsyl-
vania.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2004. Developing affective lexical resources.
PsychNology, 2(1):61?83.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal taxonomies for sentiment anal-
ysis. In Proceedings of CIKM-05, the ACM SIGIR
Conference on Information and Knowledge Manage-
ment, Bremen, DE.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1065?1072, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes of private states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
I. Witten and E. Frank. 2005. Data Mining: Practical
Machine Learning Tools and Techniques, Second Edi-
tion. Morgan Kaufmann, June.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2003), pages 129?136, Sapporo, Japan.
96
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 80?88, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Recognizing Arguing Subjectivity and Argument Tags
Alexander Conrad, Janyce Wiebe, and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
Pittsburgh PA, 15260, USA
{conrada,wiebe,hwa}@cs.pitt.edu
Abstract
In this paper we investigate two distinct
tasks. The first task involves detecting ar-
guing subjectivity, a type of linguistic sub-
jectivity on which relatively little work has
yet to be done. The second task involves
labeling instances of arguing subjectivity
with argument tags reflecting the concep-
tual argument being made. We refer to
these two tasks collectively as ?recogniz-
ing arguments?. We develop a new anno-
tation scheme and assemble a new anno-
tated corpus to support our learning ef-
forts. Through our machine learning ex-
periments, we investigate the utility of a
sentiment lexicon, discourse parser, and
semantic similarity measures with respect
to recognizing arguments. By incorpo-
rating information gained from these re-
sources, we outperform a unigram baseline
by a significant margin. In addition, we ex-
plore a two-phase approach to recognizing
arguments, with promising results.
1 Introduction
Subjectivity analysis is a thriving field within
natural language processing. However, most
research into subjectivity has focused on sen-
timent with respect to concrete things such
as product debates (e.g., (Somasundaran and
Wiebe, 2009), (Yu et al, 2011)) and movie re-
views (e.g., (He et al, 2011), (Maas et al, 2011),
(Pang and Lee, 2004)). Analysis often follows
the opinion-target paradigm, in which expres-
sions of sentiment are assessed with respect to
the aspects of the object(s) under consideration
towards which they are targeted. For example,
in the domain of smartphone reviews, aspects
could include product features such as the key-
board, screen quality, and battery life.
Although sentiment analysis is interesting
and important in its own right, this paradigm
does not seem to be the best match for fine-
grained analysis of ideological domains. While
sentiment is also present in documents from
this domain, previous work (Somasundaran and
Wiebe, 2010) has found that arguing subjec-
tivity, a less-studied form of subjectivity, is
more frequently employed and more relevant
for a robust assessment of ideological positions.
Whereas sentiment conveys the polarity of a
writer?s affect towards a topic, arguing subjec-
tivity is a type of linguistic subjectivity in which
a person expresses a controversial belief about
what is true or what action ought to be taken
regarding a central contentious issue (Somasun-
daran, 2010). For example, consider this sen-
tence about health care reform:
(1) Almost everyone knows that we
must start holding insurance compa-
nies accountable and give Americans a
greater sense of stability and security
when it comes to their health care.
In a traditional opinion-target or sentiment-
topic paradigm, perhaps this sentence could be
labeled as containing a negative sentiment to-
wards a topic representing ?insurance compa-
nies?, or a positive sentiment towards a topic
representing ?stability? or ?security?. However,
a reader of a political editorial or blog may be
more interested in why the author is negative to-
80
wards insurers, and how the author proposes to
improve stability of the healthcare system. By
focusing on the arguments conveyed through ar-
guing subjectivity, we aim to capture these kind
of conceptual reasons an author provides when
arguing for his or her position.
However, identifying when someone is arguing
is only part of the challenge. Since arguing sub-
jectivity is used to express arguments, the next
natural step is to identify the argument being
expressed through each instance of arguing sub-
jectivity. To illustrate this distinction, consider
the following three example spans:
(2) the bill is a job destroyer
(3) President Obamas signature do-
mestic policy will throw 100,000 peo-
ple out of work come January
(4) he can?t expand his business be-
cause he can?t afford the burden of
Obamacare
Each of these examples contains arguing
subjectivity, but more importantly, each ex-
presses roughly the same idea, namely, that the
recently-passed health care reform bill will cause
economic harm. This latent, shared idea giving
rise to each of the three spans is what we mean
by ?argument tag?.
However, although all three are related, exam-
ple spans (2) and (3) are more similar than (4)
in terms of the notions they convey: while the
first two explicitly are concerned with the loss
of jobs, the last focuses on business expansion
and the economy as a whole. If we were to tag
these three spans with respect to the argument
that each is making, should they all receive the
same tag, or should (4)?s tag be different?
To address these challenges, we propose in this
work a new annotation scheme for identifying
arguing subjectivity and a hierarchical model for
organizing ?argument tags?. In our hierarchical
model, (4) would receive a different tag from (2)
and (3), but because of the tags? relatedness all
would share the same parent tag.
In addition to presenting this new scheme for
labeling arguing subjectivity, we also explore
sentiment, discourse, and distributional similar-
ity as tools to enhance identification and classi-
fication of arguing subjectivity. Finally, we also
investigate splitting the arguing subjectivity de-
tection task up into two distinct phases: iden-
tifying expressions of arguing subjectivity, and
labelling each such expression with an appropri-
ate argument tag.
Since no corpora annotated for arguing sub-
jectivity yet exist, we gather and annotate a cor-
pus of blog posts and op-eds about a contro-
versial topic, namely, the recently-passed ?Oba-
maCare? health care reform bill.
2 Annotation Scheme
We designed our annotation scheme with two
goals in mind: identifying all spans of text which
express arguing subjectivity, and labelling each
such span with an argument tag. To address
the first goal, our annotators manually identified
and annotated spans of text containing arguing
subjectivity using the GATE environment1. An-
notators were instructed to identify spans of 1
sentence or less in which a writer ?conveys a
controversial private state concerning what she
believes to be true or what action she believes
should be taken? concerning the health care re-
form debate. To train our annotators to recog-
nize arguing subjectivity, we performed several
rounds of practice on a separate dataset. Be-
tween each round, our annotators met to discuss
their annotations and resolve disagreements.
As a heuristic to help distinguish between bor-
derline sentences, we advised our annotators to
imagine disputants from each side writing the
sentence in isolation. If a disputant from either
side could conceivably write the sentence, then
the sentence is likely objective. For example,
statements of accepted facts and statistics gen-
erally fall into this category. However, if only
one side could conceivably be the author of the
sentence, it is highly likely that the sentence ex-
presses a controversial belief relevant to the de-
bate and thus should be labeled as subjective.
Next, the annotators labeled each arguing
span with an argument tag. As illustrated in
earlier examples, an argument tag represents a
1http://gate.ac.uk/
81
controversial abstract belief expressed through
arguing subjectivity. Since the meanings of
many tags may be related, we organize these
tags in a hierarchical ?stance structure?. A
stance structure is a tree-based data structure
containing all of the argument tags associated
with a particular debate, organizing those tags
using ?is-a? relationships. Our stance structure
contains two levels of argument tags: upper-
level ?primary? argument tags and lower-level
?secondary? tags. Each primary tag has one of
the stances (either ?pro? or ?anti? in our case)
as its parent, while each secondary tag has a
primary tag as its parent2.
Political science ?arguing dimension? ap-
proaches to debate framing analysis served, in
part, as an inspiration for our stance structure
(Baumgartner et al, 2008). Also, as illustrated
in Section 1, this approach permits us additional
flexibility, supporting classification at different
levels of specificity depending on the task at
hand and the amount of data available. We en-
vision a future scenario in which a community of
users collaboratively builds a stance structure to
represent a new topic or debate, or in which an-
alysts build a stance structure to categorize the
issues expressed towards a proposed law, such
as in the context of e-rulemaking (Cardie et al,
2008).
Because each stance contains a large number
of argument tags, we back-off from each sec-
ondary argument tag to its primary argument
parent for the classification experiments. We
chose to do this in order to ensure that we have
a sufficient amount of data with which to train
the classifier.
3 Dataset
For this study, we chose to focus on online ed-
itorials and blog posts concerning the ongoing
debate over health insurance reform legislation
in the United States. Our intuition is that blogs
and editorials represent a genre rich in both
2Our stance structure contains an additional ?aspect?
level consisting of a-priori categories adopted from politi-
cal science research. However, we do not utilize this level
of the stance structure in this work.
?pro? documents 37
?pro? sentences 1,222
?anti? documents 47
?anti? sentences 1,456
total documents 84
total sentences 2,678
Table 1: Dataset summary statistics.
arguing subjectivity
objective 683
subjective 588
argument labels
no label 683
improves healthcare access 130
improves healthcare affordability 104
people dont know truth
about bill
75
controls healthcare costs 54
improves quality of healthcare 52
helps economy 51
bill should be passed 43
other argument 79
Table 2: Arguing and argument label statistics for
the ?pro? stance.
subjectivity and arguments. We collected docu-
ments written both before and after the passage
of the final ?Patient Protection and Affordable
Care Act? bill using the ?Google Blog Search?3
and ?Daily Op Ed?4 search portals. By choosing
a relatively broad time window, from early 2009
to late 2011, we aimed to capture a wide range
of arguments expressed throughout the debate.
The focus of this paper is on sentence-level
argument detection rather than document-level
stance classification (e.g., (Anand et al, 2011),
(Park et al, 2011), (Somasundaran and Wiebe,
2010), (Burfoot et al, 2011)). We treat stance
classification as a separate step preceding argu-
ing subjectivity detection, and thus provide or-
acle stance labels for our data.
We treat documents written from the ?pro?
3http://www.google.com/blogsearch
4http://www.dailyoped.com/
82
arguing subjectivity
objective 913
subjective 575
argument labels
no label 913
diminishes quality of care 122
too expensive 67
unpopular 60
hurts economy 55
expands govt 52
bill is politically motivated 44
other reforms more appropriate 35
other argument 140
Table 3: Arguing and argument label statistics for
the ?anti? stance.
stance and documents written from the ?anti?
stance as separate datasets. Being written from
different positions, the two stances will have dif-
ferent argument labels and may employ different
styles of arguing subjectivity. Table 1 provides
an overview of the size of this dataset. Summary
statistics concerning the density of arguing and
argument labels in the two sides of the dataset
is presented in Tables 2 and 3. However, since
it can be difficult to summarize a complex ar-
gument in a short phrase, many of these labels
by themselves do not clearly convey the meaning
they are meant to represent. To better illustrate
the meanings of some of the more ambiguous la-
bels, Table 4 presents several annotated example
spans for some of the more unclear ambiguous
argument labels.
4 Agreement Study
One of our authors performed annotation of our
corpus, the broad outlines of which are sketched
in the previous section. However, to assess inter-
annotator agreement for this annotation scheme,
we recruited a non-author to independently an-
notate a subset of our corpus consisting of 384
sentences across 10 documents. This non-author
both identified spans of arguing subjectivity and
assigned argument tags. She was given a stance
structure from which to select argument tags.
improves healthcare access
?Our reform will prohibit insurance compa-
nies from denying coverage because of your
medical history.?
?Let?s also not overlook the news from last
week about the millions of younger Americans
who are getting coverage thanks to consumer
protections that are now in place.?
improves healthcare affordability
? new health insurance exchanges will offer
competitive, consumer-centered health insur-
ance marketplaces...?
?Millions of seniors can now afford medication
they would otherwise struggle to pay for.?
people dont know truth about bill
?...the cynics and the naysayers will continue
to exploit fear and concerns for political gain.?
?Republican leaders, who see opportunities
to gain seats in the elections, have made
clear that they will continue to peddle fictions
about a government takeover of the health
care system and about costs too high to bear.?
unpopular
?The 1,000-page monstrosity that emerged in
various editions from Congress was done in by
widespread national revulsion...?
?Support for ObamaCare?s repeal is broad,
and includes one group too often overlooked
during the health care debate: America?s doc-
tors.?
expands govt
?...the real goal of the health care overhaul
was to enact the largest entitlement program
in history...?
?the new bureaucracy the health care legisla-
tion creates is so complex and indiscriminate
that its size and cost is ?currently unknow-
able.? ?
bill is politically motivated
?...tawdry backroom politics were used to sell
off favors in exchange for votes.?
?From the wildly improper gifts to senators
like Nebraska?s Ben Nelson to this week?s
backroom deals for unions...?
Table 4: Example annotated spans for several argu-
ment labels.
83
metric recall precision f-measure
agr 0.677 0.690 0.683
kappa for overlapping annotations 0.689
Table 5: Inter-annotator span agr (top) and argu-
ment label kappa on overlapping spans (bottom).
In assessing inter-annotator agreement on this
subset of the corpus, we must address two levels
of agreement, arguing spans and argument tags.
At first glance, how to assess agreement
of annotated arguing spans is not obvious.
Because our annotation scheme did not enforce
strict boundaries, we hypothesized that both
annotators would both frequently see an in-
stance of arguing subjectivity within a local
region of text, but would disagree with respect
to where the arguing begins and ends. Thus, we
adopt from (Wilson and Wiebe, 2003) the agr
directional agreement metric to measure the
degree of annotation overlap. Given two sets
of spans A and B annotated by two different
annotators, this metric measures the fraction
of spans in A which at least partially overlap
with any spans in B. Specifically, agreement is
computed as:
agr(A B) = A matching BA
When A is the gold standard set of annota-
tions, agr is equivalent to recall. Similarly, when
B is the gold standard, agr is equivalent to pre-
cision. For this evaluation, we treat the dataset
annotated by our primary annotator as the gold
standard. Table 5 presents these agr scores and
f-measures for the arguing spans.
Second, we measure agreement with respect
to the argument tags assigned by the two an-
notators. Continuing to follow the methodol-
ogy of (Wilson and Wiebe, 2003), we look at
each pair of annotations, one from each anno-
tator, which share at least a partial overlap.
For each such pair, we assess whether the two
spans share the same primary argument tag.
Scores for primary argument label agreement in
terms of Cohen?s kappa are also presented in Ta-
ble 5. Since this kappa score falls within the
range of 0.67 ? K ? 0.8, according to Krippen-
dorf?s scale (Krippendorff, 2004) this allows us
to draw tentative conclusions concerning a sig-
nificant level of tag agreement.
5 Methods
As discussed earlier, recognizing arguments can
be thought of in terms of two related but dif-
ferent tasks: recognizing a type of subjectivity,
and labeling instances of that subjectivity with
tags. We refer to the binary arguing subjectiv-
ity detection task as ?arg?, and to the multi-
class argument labeling task as ?tag?. For the
?tag? task, we create eight classes: one for each
of the seven most-frequent labels, and an eighth
into which we agglomerate the remaining less-
frequent labels. We only consider the sentences
known to be subjective (via oracle information)
for the ?tag? task.
We also perform a ?combined? task. This
third task is conceptually similar to the ?tag?
task, except that all sentences are considered
rather than only the subjective sentences. In ad-
dition to the eight classes used by ?tag?, ?com-
bined? adds an additional class for non-arguing
sentences. Finally, we also perform a two-stage
?arg+tag? task. In this two-stage task, the in-
stances labeled as subjective by the ?arg? clas-
sifier are passed as input to the ?tag? classifier.
The intuition behind this two-phase approach is
that the features most useful for identifying ar-
guing subjectivity may not be the most useful
for discriminating between argument tags, and
vice versa. For all of our classification tasks,
we treat both the ?pro? and ?anti? stances
separately, building separate classifiers for each
stance for each of the above tasks.
In general, we perform single-label classifi-
cation at the sentence level. However, sen-
tences containing multiple labels pose a chal-
lenge. Since this was an early exploratory work
on a very difficult task, we decided to handle
this situation by splitting sentences containing
multiple labels into separate instances for the
purpose of learning, assigning a single label to
each instance. However, only about 3% of the
sentences in our corpus contained multiple la-
84
bels. Thus, replacing this splitting step in the
future with another method that does not re-
quire oracle information, such as choosing the
label which covers the most words in the sen-
tence, is a reasonable simplification of the task.
Since discourse actions, such as contrasting,
restating, and identifying causation, play a sub-
stantial role in arguing, we hypothesize that in-
formation about the discourse roles played by
a span of text will help improve classification.
Although discourse parsers historically haven?t
been found to be effective for subjectivity anal-
ysis, a new parser (Lin et al, 2010) trained on
the Penn Discourse TreeBank (PDTB) tagset
(Prasad et al, 2008) has recently been released.
Previous work has demonstrated that this parser
can reliably detect discourse relationships be-
tween adjacent sentences (Lin et al, 2011), and
the PDTB tagset, being relatively flat, is con-
ducive to feature engineering for our task.
To give a feeling for the kind of discourse re-
lations identified by this parser, the following
example illustrates a concession relation identi-
fied in the corpus by the parser. The italicized
text represents the concession, while the bolded
text indicates the overall point that the author
is making. The underlined word was identified
by the parser as an explicit concessionary clue.
(7) the health care reform legisla-
tion that President Obama now seems
likely to sign into law , while an
unlovely mess , will be remembered
as a landmark accomplishment .
Using this automatic information, we define
features indicating the discourse relationships by
which the instance is connected to surrounding
text. Specifically, the class of discourse rela-
tionship connecting the target instance to the
previous instance, the relationship connecting it
to the following instance, and any internal dis-
course relationships by which the parts of the
instance are connected to each other are each
added as features. Since PDTB contains many
fine-grained discourse relations, we replace each
discourse relationship type inferred by the dis-
course parser with the parent top-level PDTB
discourse relationship class. We arrive at a total
of 15 binary discourse relationship features: (4
top-level classes + ?other?) x (connects to pre-
vious + connects to following + internal connec-
tion) = 15. We refer to these features as ?rels?.
As illustrated in our earlier examples, while
arguing subjectivity is different from sentiment,
the two types of subjectivity are often related.
Thus, we investigate incorporating sentiment
information based on the presence of unigram
clues from a publically-available sentiment lexi-
con5 (Wilson, 2005). Each clue in the lexicon is
marked as being either ?strong? or ?weak?.
We found that this lexicon was producing
many false hits for positive sentiment. Thus, a
span containing a minimum of two positive clues
of which at least one is marked as ?strong?, or
three positive ?weak? clues, is augmented with a
feature indicating positive sentiment. For nega-
tive sentiment the threshold is slightly lower, at
one ?strong? clue or two ?weak? clues. These
features are referred to as ?senti?.
A challenge to argument tag assignment is the
broad diversity of language through which in-
dividual entities or specific actions may be ref-
erenced, as illustrated in Examples (2-4) from
Section 1. To address this problem, we in-
vestigate expanding each instance with terms
that are most similar, according to a distribu-
tional model generated from Wikipedia articles,
to the nouns and verbs present within the in-
stance (Pantel et al, 2009). We refer to these
features as ?expn?, where n is the number of
most-similar terms with which to expand the in-
stance for each noun or verb. We experiment
with values of n = 5 and n = 10.
Subjectivity classification of small units of
text, such as individual microblog posts (Jiang
et al, 2011) and sentences (Riloff et al, 2003),
has been shown to benefit from additional con-
text. Thus, we augment the feature representa-
tion of each target sentence with features from
the two preceding and two following sentences.
These additional features are modified so that
they do not fall within the same feature space
5downloaded from http://www.cs.pitt.edu/mpqa/
subj_lexicon.html
85
feat.
abbrev.
elaboration
unigram
senti 2 binary features indicating posi-
tive or negative sentiment based on
presence of lexicon clues
rels 15 binary features indicating kinds
of discourse relationships and how
they connect instance to surround-
ing text
exp5 for each noun and verb in instance,
expand instance with top 5 most
distributionally similar words
exp10 for each noun and verb in instance,
expand instance with top 10 most
distributionally similar words
Table 6: Overview of features used in the arguing
and argument experiments.
as the features representing the target sentence.
Using the Naive Bayes classifier within the
WEKA machine learning toolkit (Hall et al,
2009), we explore the impact of the features de-
scribed above on our four experiment configu-
rations. We perform our experiments using k-
fold cross-validation, where k equals the num-
ber of documents within the stance. The test
set for each fold consists of a single document?s
instances. For the ?pro? dataset k = 37, while
for the ?anti? dataset k = 47.
6 Results
Table 7 presents the accuracy scores from each of
our stand-alone classifiers across combinations
of feature sets. Each feature set consists of
unigrams augmented with the designated addi-
tional features, as described in Section 5. To
evaluate the ?tag? classifier in isolation, we use
oracle information to provide this classifier with
only the subjective instances. To assess signif-
icance of the performance differences between
feature sets, we used the Pearson Chi-squared
test with Yates continuity correction.
Expansion of nouns and verbs with
distributionally-similar terms (?exp5?, ?exp10?)
plays the largest role in improving classifier
features arg tag comb.
unigram baseline 0.610 0.425 0.458
senti 0.614 0.426 0.459
rels 0.614 0.422 0.462
senti, rels 0.618 0.424 0.465
exp5 0.635 0.522 0.482
exp5, senti 0.638 0.515 0.486
exp5, rels 0.640 0.522 0.484
exp5, senti, rels 0.643 0.516 0.484
exp10 0.645 0.517 0.488
exp10, senti 0.647 0.515 0.489
exp10, rels 0.642 0.512 0.490
exp10, senti, rels 0.644 0.513 0.490
Table 7: Classifier accuracy for differing feature sets.
Significant improvement (p < 0.05) over baseline is
boldfaced (0.05 < p < 0.1 italicized). Underline in-
dicates best performance per column.
performance. While differences between con-
figurations using ?exp5? versus ?exp10? were
generally not significant, all of the configu-
rations incorporating some version of term
expansion outperformed the unigram baseline
by either a statistically significant margin
(p < 0.05) or by a margin that approached
significance (0.05 < p < 0.1).
Sentiment features consistently produce im-
provements in accuracy for the ?arg? and ?com-
bined? tasks. While these improvements are
promising, the lack of a significant margin of im-
provement when incorporating sentiment is sur-
prising. Since sentiment lexicons are known to
be highly domain-dependent (Pan et al, 2010),
it may be the case that, having been learned
from a general news corpus, the sentiment lexi-
con employed in this work is not the best match
for the domain of ?ObamaCare? blogs and edito-
rials. Similarly, the discourse features also fail to
produce significant improvements in accuracy.
Finally, we aim to test our hypothesis that
separating the ?arg? and ?tag? phases results in
improvement beyond treating the two in a single
?combined? phase. The first step of our hierar-
chy involves normal classification of all sentences
using the ?arg? classifier. Next, all sentences
judged to contain arguing subjectivity by ?arg?
86
arg features tag features acc.
exp5, senti, rels
exp5 0.506
exp5, rels 0.506
exp10 0.501
exp10
exp5 0.514
exp5, rels 0.513
exp10 0.512
exp10, senti
exp5 0.514
exp5, rels 0.513
exp10 0.512
Table 8: Accuracies of two-stage classifiers across dif-
ferent combinations of feature sets for the ?arg? and
?tag? phases. Italics indicate improvement over the
top ?combined? configuration which approaches sig-
nificance (0.05 < p < 0.1). Underline indicates best
overall performance.
are passed to the ?tag? classifier to have an ar-
gument tag assigned. We choose three promis-
ing feature sets for the ?arg? and ?tag? phases,
based on best performance in isolation.
Results of this hierarchical experiment are
presented in Table 8. We evaluate the hi-
erarchical system against the best-performing
?combined? single-phase systems from Table 7.
While all of the hierarchical configurations beat
the best ?combined? classifier, none beats the
top combined classifier by a significant margin,
although the best configurations approach sig-
nificance (0.05 < p < 0.1).
7 Related Work
Much recent work in ideological subjectivity
detection has focused on detecting a writer?s
stance in domains of varying formality, such as
online forums, debating websites, and op-eds.
(Anand et al, 2011) demonstrates the usefulness
of dependency relations, LIWC counts (Pen-
nebaker et al, 2001), and information about re-
lated posts for this task. (Lin et al, 2006) ex-
plores relationships between sentence-level and
document-level classification for a stance-like
prediction task.
Among the literature on ideological subjectiv-
ity, perhaps most similar to our work is (Soma-
sundaran and Wiebe, 2010). This paper investi-
gates the impact of incorporating arguing-based
and sentiment-based features into binary stance
prediction for debate posts. Also closely related
to our work is (Somasundaran et al, 2007). To
support answering of opinion-based questions,
this work investigates the use of high-precision
sentiment and arguing clues for sentence-level
sentiment and arguing prediction.
Another active area of related research focuses
on identifying important aspects towards which
sentiment is expressed within a domain. (He
et al, 2011) approaches this problem through
topic modeling, extending the joint sentiment-
topic (JST) model which aims to simultaneously
learn sentiment and aspect probabilities for a
unit of text. (Yu et al, 2011) takes a different
approach, investigating thesaurus methods for
learning aspects based on groups of synonymous
nouns within product reviews.
8 Conclusion
In this paper, we explored recognizing argu-
ments in terms of arguing subjectivity and ar-
gument tags. We presented and evaluated a
new annotation scheme to capture arguing sub-
jectivity and argument tags, and annotated a
new dataset. Utilizing existing sentiment, dis-
course, and distributional similarity resources,
we explored ways in which these three forms
of knowledge could be used to enhance argu-
ment recognition. In particular, our empirical
results highlight the important role played by
distributional similarity in all phases of detect-
ing arguing subjectivity and argument tags. We
have also provided tentative evidence suggesting
that addressing the problem of recognizing argu-
ments in two separate phases may be beneficial
to overall classification accuracy.
9 Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation award
#0916046. We would like to thank Patrick Pan-
tel for sharing his thesaurus of distributionally
similar words from Wikipedia with us, Amber
Boydstun for insightful conversations about de-
bate frame categorization, and the anonymous
reviewers for their useful feedback.
87
References
Pranav Anand, Marilyn Walker, Rob Abbott,
Jean E. Fox Tree, Robeson Bowmani, and Michael
Minor. 2011. Cats rule and dogs drool!: Classi-
fying stance in online debate. In WASSA, pages
1?9, Portland, Oregon, June.
F.R. Baumgartner, S.D. Boef, and A.E. Boydstun.
2008. The decline of the death penalty and the dis-
covery of innocence. Cambridge University Press.
Clinton Burfoot, Steven Bird, and Timothy Bald-
win. 2011. Collective classification of congres-
sional floor-debate transcripts. In ACL, pages
1506?1515, Portland, Oregon, USA, June.
Claire Cardie, Cynthia Farina, Adil Aijaz, Matt
Rawding, and Stephen Purpura. 2008. A study in
rule-specific issue categorization for e-rulemaking.
In DG.O, pages 244?253.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics
for cross-domain sentiment classification. In ACL,
pages 123?131, Portland, Oregon, USA, June.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter
sentiment classification. In ACL, pages 151?160,
Portland, Oregon, USA, June.
K. Krippendorff. 2004. Content analysis: an intro-
duction to its methodology. Sage.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In CoNLL, pages 109?116.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. CoRR.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In ACL, pages 997?1006, Port-
land, Oregon, USA, June.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher
Potts. 2011. Learning word vectors for sentiment
analysis. In ACL, pages 142?150, Portland, Ore-
gon, USA, June.
Sinno Jialin Pan, Xiaochuan Ni, Jian tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment.
In WWW.
Bo Pang and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In ACL,
pages 271?278, Barcelona, Spain, July.
Patrick Pantel, Eric Crestan, Arkady Borkovsky,
Ana-Maria Popescu, and Vishnu Vyas. 2009.
Web-scale distributional similarity and entity set
expansion. In EMNLP, pages 938?947, Morris-
town, NJ, USA.
Souneil Park, Kyung Soon Lee, and Junehwa Song.
2011. Contrasting opposing views of news arti-
cles on contentious issues. In ACL, pages 340?349,
Portland, Oregon, USA, June.
James W Pennebaker, Roger J Booth, and Martha E
Francis. 2001. Linguistic inquiry and word count
(liwc): Liwc2001. Linguistic Inquiry, (Mahwah,
NJ):0.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0.
In LREC, May.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In CoNLL, pages 25?32.
Swapna Somasundaran and Janyce Wiebe. 2009.
Recognizing stances in online debates. In ACL-
AFNLP, pages 226?234.
Swapna Somasundaran and Janyce Wiebe. 2010.
Recognizing stances in ideological on-line debates.
In CAAGET, pages 116?124.
Swapna Somasundaran, Theresa Wilson, Janyce
Wiebe, and Veselin Stoyanov. 2007. Qa with atti-
tude: Exploiting opinion type analysis for improv-
ing question answering in on-line discussions and
the news. In ICWSM.
Swampa Somasundaran. 2010. Discourse-Level Re-
lations for Opinion Analysis. Ph.D. thesis, Uni-
versity of Pittsburgh, USA.
Theresa Wilson and Janyce Wiebe. 2003. Annotat-
ing opinions in the world press. In SIGdial, pages
13?22.
Theresa Wilson. 2005. Recognizing contextual
polarity in phrase-level sentiment analysis. In
EMNLP, pages 347?354.
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect ranking: Identifying
important product aspects from online consumer
reviews. In ACL, pages 1496?1505, Portland, Ore-
gon, USA, June.
88
