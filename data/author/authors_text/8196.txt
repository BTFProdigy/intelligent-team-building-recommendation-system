Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 80?87,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An annotation scheme for citation function
Simone Teufel Advaith Siddharthan Dan Tidhar
Natural Language and Information Processing Group
Computer Laboratory
Cambridge University, CB3 0FD, UK
{Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk
Abstract
We study the interplay of the discourse struc-
ture of a scientific argument with formal ci-
tations. One subproblem of this is to clas-
sify academic citations in scientific articles ac-
cording to their rhetorical function, e.g., as a
rival approach, as a part of the solution, or
as a flawed approach that justifies the cur-
rent research. Here, we introduce our anno-
tation scheme with 12 categories, and present
an agreement study.
1 Scientific writing, discourse structure
and citations
In recent years, there has been increasing interest in
applying natural language processing technologies to
scientific literature. The overwhelmingly large num-
ber of papers published in fields like biology, genetics
and chemistry each year means that researchers need
tools for information access (extraction, retrieval, sum-
marization, question answering etc). There is also in-
creased interest in automatic citation indexing, e.g.,
the highly successful search tools Google Scholar and
CiteSeer (Giles et al, 1998).1 This general interest in
improving access to scientific articles fits well with re-
search on discourse structure, as knowledge about the
overall structure and goal of papers can guide better in-
formation access.
Shum (1998) argues that experienced researchers are
often interested in relations between articles. They
need to know if a certain article criticises another and
what the criticism is, or if the current work is based
on that prior work. This type of information is hard
to come by with current search technology. Neither
the author?s abstract, nor raw citation counts help users
in assessing the relation between articles. And even
though CiteSeer shows a text snippet around the phys-
ical location for searchers to peruse, there is no guar-
antee that the text snippet provides enough information
for the searcher to infer the relation. In fact, studies
from our annotated corpus (Teufel, 1999), show that
69% of the 600 sentences stating contrast with other
work and 21% of the 246 sentences stating research
continuation with other work do not contain the cor-
responding citation; the citation is found in preceding
1CiteSeer automatically citation-indexes all scientific ar-
ticles reached by a web-crawler, making them available to
searchers via authors or keywords in the title.
Nitta and Niwa 94
Resnik 95
Brown et al 90a
Rose et al 90
Church and Gale 91
Dagan et al 94
Li and Abe 96
Hindle 93
Hindle 90
Dagan et al93
Pereira et al 93
Following Pereira et al we measure
word similarity by the relative entropy
or Kulbach?Leibler (KL) distance, bet?
ween the corresponding conditional
distributions.
His notion of similarityseems to agree with ourintuitions in many cases,but it is not clear how itcan  be used directly toconstruct word classesand corresponding models of association.
Figure 1: A rhetorical citation map
sentences (i.e., the sentence expressing the contrast or
continuation would be outside the CiteSeer snippet).
We present here an approach which uses the classifica-
tion of citations to help provide relational information
across papers.
Citations play a central role in the process of writing
a paper. Swales (1990) argues that scientific writing
follows a general rhetorical argumentation structure:
researchers must justify that their paper makes a con-
tribution to the knowledge in their discipline. Several
argumentation steps are required to make this justifica-
tion work, e.g., the statement of their specific goal in
the paper (Myers, 1992). Importantly, the authors also
must relate their current work to previous research, and
acknowledge previous knowledge claims; this is done
with a formal citation, and with language connecting
the citation to the argument, e.g., statements of usage of
other people?s approaches (often near textual segments
in the paper where these approaches are described), and
statements of contrast with them (particularly in the
discussion or related work sections). We argue that the
automatic recognition of citation function is interest-
ing for two reasons: a) it serves to build better citation
indexers and b) in the long run, it will help constrain
interpretations of the overall argumentative structure of
a scientific paper.
Being able to interpret the rhetorical status of a ci-
tation at a glance would add considerable value to ci-
tation indexes, as shown in Fig. 1. Here differences
and similarities are shown between the example paper
(Pereira et al, 1993) and the papers it cites, as well as
80
the papers that cite it. Contrastive links are shown in
grey ? links to rival papers and papers the current pa-
per contrasts itself to. Continuative links are shown in
black ? links to papers that are taken as starting point of
the current research, or as part of the methodology of
the current paper. The most important textual sentence
about each citation could be extracted and displayed.
For instance, we see which aspect of Hindle (1990) the
Pereira et al paper criticises, and in which way Pereira
et al?s work was used by Dagan et al (1994).
We present an annotation scheme for citations, based
on empirical work in content citation analysis, which
fits into this general framework of scientific argument
structure. It consists of 12 categories, which allow us
to mark the relationships of the current paper with the
cited work. Each citation is labelled with exactly one
category. The following top-level four-way distinction
applies:
? Weakness: Authors point out a weakness in cited
work
? Contrast: Authors make contrast/comparison with
cited work (4 categories)
? Positive: Authors agree with/make use of/show
compatibility or similarity with cited work (6 cat-
egories), and
? Neutral: Function of citation is either neutral, or
weakly signalled, or different from the three func-
tions stated above.
We first turn to the point of how to classify citation
function in a robust way. Later in this paper, we will
report results for a human annotation experiment with
three annotators.
2 Annotation schemes for citations
In the field of library sciences (more specifically, the
field of Content Citation Analysis), the use of informa-
tion from citations above and beyond simple citation
counting has received considerable attention. Biblio-
metric measures assesses the quality of a researcher?s
output, in a purely quantitative manner, by counting
how many papers cite a given paper (White, 2004;
Luukkonen, 1992) or by more sophisticated measures
like the h-index (Hirsch, 2005). But not all citations
are alike. Researchers in content citation analysis have
long stated that the classification of motivations is a
central element in understanding the relevance of the
paper in the field. Bonzi (1982), for example, points out
that negational citations, while pointing to the fact that
a given work has been noticed in a field, do not mean
that that work is received well, and Ziman (1968) states
that many citations are done out of ?politeness? (to-
wards powerful rival approaches), ?policy? (by name-
dropping and argument by authority) or ?piety? (to-
wards one?s friends, collaborators and superiors). Re-
searchers also often follow the custom of citing some
1. Cited source is mentioned in the introduction or
discussion as part of the history and state of the
art of the research question under investigation.
2. Cited source is the specific point of departure for
the research question investigated.
3. Cited source contains the concepts, definitions,
interpretations used (and pertaining to the disci-
pline of the citing article).
4. Cited source contains the data (pertaining to the
discipline of the citing article) which are used
sporadically in the article.
5. Cited source contains the data (pertaining to the
discipline of the citing particle) which are used
for comparative purposes, in tables and statistics.
6. Cited source contains data and material (from
other disciplines than citing article) which is
used sporadically in the citing text, in tables or
statistics.
7. Cited source contains the method used.
8. Cited source substantiated a statement or assump-
tion, or points to further information.
9. Cited source is positively evaluated.
10. Cited source is negatively evaluated.
11. Results of citing article prove, verify, substantiate
the data or interpretation of cited source.
12. Results of citing article disprove, put into ques-
tion the data as interpretation of cited source.
13. Results of citing article furnish a new interpreta-
tion/explanation to the data of the cited source.
Figure 2: Spiegel-Ru?sing?s (1977) Categories for Cita-
tion Motivations
particular early, basic paper, which gives the founda-
tion of their current subject (?paying homage to pio-
neers?). Many classification schemes for citation func-
tions have been developed (Weinstock, 1971; Swales,
1990; Oppenheim and Renn, 1978; Frost, 1979; Chu-
bin and Moitra, 1975), inter alia. Based on such an-
notation schemes and hand-analyzed data, different in-
fluences on citation behaviour can be determined, but
annotation in this field is usually done manually on
small samples of text by the author, and not confirmed
by reliability studies. As one of the earliest such stud-
ies, Moravcsik and Murugesan (1975) divide citations
in running text into four dimensions: conceptual or
operational use (i.e., use of theory vs. use of techni-
cal method); evolutionary or juxtapositional (i.e., own
work is based on the cited work vs. own work is an al-
ternative to it); organic or perfunctory (i.e., work is cru-
cially needed for understanding of citing article or just
a general acknowledgement); and finally confirmative
vs. negational (i.e., is the correctness of the findings
disputed?). They found, for example, that 40% of the
citations were perfunctory, which casts further doubt
on the citation-counting approach.
Other content citation analysis research which is rel-
81
evant to our work concentrates on relating textual spans
to authors? descriptions of other work. For example, in
O?Connor?s (1982) experiment, citing statements (one
or more sentences referring to other researchers? work)
were identified manually. The main problem encoun-
tered in that work is the fact that many instances of cita-
tion context are linguistically unmarked. Our data con-
firms this: articles often contain large segments, par-
ticularly in the central parts, which describe other peo-
ple?s research in a fairly neutral way. We would thus
expect many citations to be neutral (i.e., not to carry
any function relating to the argumentation per se).
Many of the distinctions typically made in content
citation analysis are immaterial to the task considered
here as they are too sociologically orientated, and can
thus be difficult to operationalise without deep knowl-
edge of the field and its participants (Swales, 1986). In
particular, citations for general reference (background
material, homage to pioneers) are not part of our an-
alytic interest here, and so are citations ?in passing?,
which are only marginally related to the argumentation
of the overall paper (Ziman, 1968).
Spiegel-Ru?sing?s (1977) scheme (Fig. 2) is an exam-
ple of a scheme which is easier to operationalise than
most. In her scheme, more than one category can apply
to a citation; for instance positive and negative evalu-
ation (category 9 and 10) can be cross-classified with
other categories. Out of 2309 citations examined, 80%
substantiated statements (category 8), 6% discussed
history or state of the art of the research area (cate-
gory 1) and 5% cited comparative data (category 5).
Category Description
Weak Weakness of cited approach
CoCoGM Contrast/Comparison in Goals or Meth-
ods (neutral)
CoCoR0 Contrast/Comparison in Results (neutral)
CoCo- Unfavourable Contrast/Comparison (cur-
rent work is better than cited work)
CoCoXY Contrast between 2 cited methods
PBas author uses cited work as starting point
PUse author uses tools/algorithms/data
PModi author adapts or modifies
tools/algorithms/data
PMot this citation is positive about approach or
problem addressed (used to motivate work
in current paper)
PSim author?s work and cited work are similar
PSup author?s work and cited work are compat-
ible/provide support for each other
Neut Neutral description of cited work, or not
enough textual evidence for above cate-
gories or unlisted citation function
Figure 3: Our annotation scheme for citation function
Our scheme (given in Fig. 3) is an adaptation of the
scheme in Fig. 2, which we arrived at after an analysis
of a corpus of scientific articles in computational lin-
guistics. We tried to redefine the categories such that
they should be reasonably reliably annotatable; at the
same time, they should be informative for the appli-
cation we have in mind. A third criterion is that they
should have some (theoretical) relation to the particu-
lar discourse structure we work with (Teufel, 1999).
Our categories are as follows: One category (Weak)
is reserved for weakness of previous research, if it is ad-
dressed by the authors (cf. Spiegel-Ru?sing?s categories
10, 12, possibly 13). The next three categories describe
comparisons or contrasts between own and other work
(cf. Spiegel-Ru?sing?s category 5). The difference be-
tween them concerns whether the comparison is be-
tween methods/goals (CoCoGM) or results (CoCoR0).
These two categories are for comparisons without ex-
plicit value judgements. We use a different category
(CoCo-) when the authors claim their approach is bet-
ter than the cited work.
Our interest in differences and similarities between
approaches stems from one possible application we
have in mind (the rhetorical citation search tool). We
do not only consider differences stated between the cur-
rent work and other work, but we also mark citations if
they are explicitly compared and contrasted with other
work (not the current paper). This is expressed in cat-
egory CoCoXY. It is a category not typically consid-
ered in the literature, but it is related to the other con-
trastive categories, and useful to us because we think
it can be exploited for search of differences and rival
approaches.
The next set of categories we propose concerns pos-
itive sentiment expressed towards a citation, or a state-
ment that the other work is actively used in the cur-
rent work (which is the ultimate praise). Like Spiegel-
Ru?sing, we are interested in use of data and methods
(her categories 4, 5, 6, 7), but we cluster different us-
ages together and instead differentiate unchanged use
(PUse) from use with adaptations (PModi). Work
which is stated as the explicit starting point or intellec-
tual ancestry is marked with our category PBas (her
category 2). If a claim in the literature is used to
strengthen the authors? argument, this is expressed in
her category 8, and vice versa, category 11. We col-
lapse these two in our category PSup. We use two
categories she does not have definitions for, namely
similarity of (aspect of) approach to other approach
(PSim), and motivation of approach used or problem
addressed (PMot). We found evidence for prototypi-
cal use of these citation functions in our texts. How-
ever, we found little evidence for her categories 12 or
13 (disproval or new interpretation of claims in cited
literature), and we decided against a ?state-of-the-art?
category (her category 1), which would have been in
conflict with our PMot definition in many cases.
Our fourteenth category, Neut, bundles truly neutral
descriptions of other researchers? approaches with all
those cases where the textual evidence for a citation
function was not enough to warrant annotation of that
category, and all other functions for which our scheme
did not provide a specific category. As stated above, we
do in fact expect many of our citations to be neutral.
82
Citation function is hard to annotate because it in
principle requires interpretation of author intentions
(what could the author?s intention have been in choos-
ing a certain citation?). Typical results of earlier cita-
tion function studies are that the sociological aspect of
citing is not to be underestimated. One of our most fun-
damental ideas for annotation is to only mark explicitly
signalled citation functions. Our guidelines explicitly
state that a general linguistic phrase such as ?better?
or ?used by us? must be present, in order to increase
objectivity in finding citation function. Annotators are
encouraged to point to textual evidence they have for
assigning a particular function (and are asked to type
the source of this evidence into the annotation tool for
each citation). Categories are defined in terms of cer-
tain objective types of statements (e.g., there are 7 cases
for PMot). Annotators can use general text interpreta-
tion principles when assigning the categories, but are
not allowed to use in-depth knowledge of the field or
of the authors.
There are other problematic aspects of the annota-
tion. Some concern the fact that authors do not al-
ways state their purpose clearly. For instance, several
earlier studies found that negational citations are rare
(Moravcsik and Murugesan, 1975; Spiegel-Ru?sing,
1977); MacRoberts and MacRoberts (1984) argue that
the reason for this is that they are potentially politically
dangerous, and that the authors go through lengths to
diffuse the impact of negative references, hiding a neg-
ative point behind insincere praise, or diffusing the
thrust of criticism with perfunctory remarks. In our
data we found ample evidence of this effect, illustrated
by the following example:
Hidden Markov Models (HMMs) (Huang
et al 1990) offer a powerful statistical ap-
proach to this problem, though it is unclear
how they could be used to recognise the units
of interest to phonologists. (9410022, S-24)2
It is also sometimes extremely hard to distinguish
usage of a method from statements of similarity be-
tween a method and the own method. This happens
in cases where authors do not want to admit they are
using somebody else?s method:
The same test was used in Abney and Light
(1999). (0008020, S-151)
Unication of indices proceeds in the same
manner as unication of all other typed
feature structures (Carpenter 1992).
(0008023, S-87)
In this case, our annotators had to choose between
categories PSim and PUse.
It can also be hard to distinguish between continu-
ation of somebody?s research (i.e., taking somebody?s
2In all corpus examples, numbers in brackets correspond
to the official Cmp lg archive number, ?S-? numbers to sen-
tence numbers according to our preprocessing.
research as starting point, as intellectual ancestry, i.e.
PBas) and simply using it (PUse). In principle, one
would hope that annotation of all usage/positive cate-
gories (starting with P), if clustered together, should re-
sult in higher agreement (as they are similar, and as the
resulting scheme has fewer distinctions). We would ex-
pect this to be the case in general, but as always, cases
exist where a conflict between a contrast (CoCo) and a
change to a method (PModi) occur:
In contrast to McCarthy, Kay and Kiraz,
we combine the three components into a sin-
gle projection. (0006044, S-182)
The markable units in our scheme are a) all full cita-
tions (as recognized by our automatic citation proces-
sor on our corpus), and b) all names of authors of cited
papers anywhere in running text outside of a formal
citation context (i.e., without date). Our citation pro-
cessor recognizes these latter names after parsing the
citation list an marks them up. This is unusual in com-
parison to other citation indexers, but we believe these
names function as important referents comparable in
importance to formal citations. In principle, one could
go even further as there are many other linguistic ex-
pressions by which the authors could refer to other peo-
ple?s work: pronouns, abbreviations such as ?Mueller
and Sag (1990), henceforth M & S?, and names of ap-
proaches or theories which are associated with partic-
ular authors. If we could mark all of these up auto-
matically (which is not technically possible), annota-
tion would become less difficult to decide, but techni-
cal difficulty prevent us from recognizing these other
cases automatically. As a result, in these contexts it is
impossible to annotate citation function directly on the
referent, which sometimes causes problems. Because
this means that annotators have to consider non-local
context, one markable may have different competing
contexts with different potential citation functions, and
problems about which context is ?stronger? may oc-
cur. We have rules that context is to be constrained to
the paragraph boundary, but for some categories paper-
wide information is required (e.g., for PMot, we need
to know that a praised approach is used by the authors,
information which may not be local in the paragraph).
Appendix A gives unambiguous example cases
where the citation function can be decided on the ba-
sis of the sentence alone, but Fig. 4 shows a more typ-
ical example where more context is required to inter-
pret the function. The evaluation of the citation Hin-
dle (1990) is contrastive; the evaluative statement is
found 4 sentences after the sentence containing the ci-
tation3. It consists of a positive statement (agreement
with authors? view), followed by a weakness, under-
lined, which is the chosen category. This is marked on
the nearest markable (Hindle, 3 sentences after the ci-
tation).
3In Fig. 4, markables are shown in boxes, evaluative state-
ments underlined, and referents in bold face.
83
S-5 Hindle (1990)/Neut proposed dealing with the
sparseness problem by estimating the likelihood of un-seen events from that of ?similar? events that have beenseen.S-6 For instance, one may estimate the likelihood of aparticular direct object for a verb from the likelihoodsof that direct object for similar verbs.S-7 This requires a reasonable definition of verb simi-larity and a similarity estimation method.
S-8 In Hindle/Weak ?s proposal, words are similarif we have strong statistical evidence that they tend toparticipate in the same events.S-9 His notion of similarity seems to agree with our in-tuitions in many cases, but it is not clear how it can beused directly to construct word classes and correspond-
ing models of association. (9408011)
Figure 4: Annotation example: influence of context
A naive view on this annotation scheme could con-
sider the first two sets of categories in our scheme as
?negative? and the third set of categories ?positive?.
There is indeed a sentiment aspect to the interpretation
of citations, due to the fact that authors need to make
a point in their paper and thus have a stance towards
their citations. But this is not the whole story: many
of our ?positive? categories are more concerned with
different ways in which the cited work is useful to the
current work (which aspect of it is used, e.g., just a
definition or the entire solution?), and many of the con-
trastive statements have no negative connotation at all
and simply state a (value-free) difference between ap-
proaches. However, if one looks at the distribution of
positive and negative adjectives around citations, one
notices a (non-trivial) connection between our task and
sentiment classification.
There are written guidelines of 25 pages, which in-
struct the annotators to only assign one category per
citation, and to skim-read the paper before annotation.
The guidelines provide a decision tree and give deci-
sion aids in systematically ambiguous cases, but sub-
jective judgement of the annotators is nevertheless nec-
essary to assign a single tag in an unseen context. We
implemented an annotation tool based on XML/XSLT
technology, which allows us to use any web browser to
interactively assign one of the 12 tags (presented as a
pull-down list) to each citation.
3 Data
The data we used came from the CmpLg (Computation
and Language archive; 320 conference articles in com-
putational linguistics). The articles are in XML format.
Headlines, titles, authors and reference list items are
automatically marked up with the corresponding tags.
Reference lists are parsed, and cited authors? names
are identified. Our citation parser then applies regu-
lar patterns and finds citations and other occurrences of
the names of cited authors (without a date) in running
text and marks them up. Self-citations are detected by
overlap of citing and cited authors. The citation pro-
cessor developped in our group (Ritchie et al, 2006)
achieves high accuracy for this task (96% of citations
recognized, provided the reference list was error-free).
On average, our papers contain 26.8 citation instances
in running text4.
4 Human Annotation: results
In order to machine learn citation function, we are
in the process of creating a corpus of scientific arti-
cles with human annotated citations, according to the
scheme discussed before. Here we report preliminary
results with that scheme, with three annotators who are
developers of the scheme.
In our experiment, the annotators independently an-
notated 26 conference articles with this scheme, on the
basis of guidelines which were frozen once annotation
started5. The data used for the experiment contained a
total of 120,000 running words and 548 citations.
The relative frequency of each category observed in
the annotation is listed in Fig. 5. As expected, the dis-
tribution is very skewed, with more than 60% of the
citations of category Neut.6 What is interesting is the
relatively high frequency of usage categories (PUse,
PModi, PBas) with a total of 18.9%. There is
a relatively low frequency of clearly negative cita-
tions (Weak, CoCoR-, total of 4.1%), whereas the
neutral?contrastive categories (CoCoGM, CoCoR0,
CoCoXY) are slightly more frequent at 7.6%. This
is in concordance with earlier annotation experiments
(Moravcsik and Murugesan, 1975; Spiegel-Ru?sing,
1977).
We reached an inter-annotator agreement of K=.72
(n=12;N=548;k=3)7. This is comparable to aggreement
on other discourse annotation tasks such as dialogue
act parsing and Argumentative Zoning (Teufel et al,
1999). We consider the agreement quite good, consid-
ering the number of categories and the difficulties (e.g.,
non-local dependencies) of the task.
The annotators are obviously still disagreeing on
some categories. We were wondering to what de-
gree the fine granularity of the scheme is a prob-
lem. When we collapsed the obvious similar cat-
egories (all P categories into one category, and
all CoCo categories into another) to give four top
level categories (Weak, Positive, Contrast,
Neutral), this only raised kappa to 0.76. This
4As opposed to reference list items, which are fewer.
5The development of the scheme was done with 40+ dif-
ferent articles.
6Spiegel-Ru?sing found that out of 2309 citations she ex-
amined, 80% substantiated statements.
7Following Carletta (1996), we measure agreement in
Kappa, which follows the formula K = P (A)?P (E)1?P (E) whereP(A) is observed, and P(E) expected agreement. Kappa
ranges between -1 and 1. K=0 means agreement is only as
expected by chance. Generally, Kappas of 0.8 are considered
stable, and Kappas of .69 as marginally stable, according to
the strictest scheme applied in the field.
84
Neut PUse CoCoGM PSim Weak CoCoXY PMot PModi PBas PSup CoCo- CoCoR0
62.7% 15.8% 3.9% 3.8% 3.1% 2.9% 2.2% 1.6% 1.5% 1.1% 1.0% 0.8%
Figure 5: Distribution of the categories
Weak CoCo- CoCoGM CoCoR0 CoCoXY PUse PBas PModi PMot PSim PSup Neut
Weak 5 3
CoCo- 1 3
CoCoGM 23 3
CoCoR0 4
CoCoXY 1
PUse 86 6 2 1 12
PBas 3 2
PModi 3
PMot 13 4
PSim 3 20 5
PSup 1 2 1
Neut 6 10 6 4 17 1 6 4 287
Figure 6: Confusion matrix between two annotators
points to the fact that most of our annotators disagreed
about whether to assign a more informative category
or Neut, the neutral fall-back category. Unfortunately,
Kappa is only partially sensitive to such specialised dis-
agreements. While it will reward agreement with in-
frequent categories more than agreement with frequent
categories, it nevertheless does not allow us to weight
disagreements we care less about (Neut vs more in-
formative category) less than disagreements we do care
a lot about (informative categories which are mutually
exclusive, such as Weak and PSim).
Fig. 6 shows a confusion matrix between the two an-
notators who agreed most with each other. This again
points to the fact that a large proportion of the confu-
sion involves an informative category and Neut. The
issue with Neut and Weak is a point at hand: au-
thors seem to often (deliberately or not) mask their in-
tended citation function with seemingly neutral state-
ments. Many statements of weakness of other ap-
proaches were stated in such caged terms that our anno-
tators disagreed about whether the signals given were
?explicit? enough.
While our focus is not sentiment analysis, it is pos-
sible to conflate our 12 categories into three: positive,
weakness and neutral by the following mapping:
Old Categories New Category
Weak, CoCo- Negative
PMot, PUse, PBas, PModi, PSim, PSup Positive
CoCoGM, CoCoR0, CoCoXY, Neut Neutral
Thus negative contrasts and weaknesses are grouped
into Negative, while neutral contrasts are grouped
into Neutral. All the positive classes are conflated
into Positive. This resulted in kappa=0.75 for three
annotators.
Fig. 7 shows the confusion matrix between two an-
notators for this sentiment classification. Fig. 7 is par-
ticularly instructive, because it shows that annotators
Weakness Positive Neutral
Weakness 9 1 12
Positive 140 13
Neutral 4 30 339
Figure 7: Confusion matrix between two annotators;
categories collapsed to reflect sentiment
have only one case of confusion between positive and
negative references to cited work. The vast majority of
disagreements reflects genuine ambiguity as to whether
the authors were trying to stay neutral or express a sen-
timent.
Distinction Kappa
PMot v. all others .790
CoCoGM v. all others .765
PUse v. all others .761
CoCoR0 v. all others .746
Neut v. all others .742
PSim v. all others .649
PModi v. all others .553
CoCoXY v. all others .553
Weak v. all others .522
CoCo- v. all others .462
PBas v. all others .414
PSup v. all others .268
Figure 8: Distinctiveness of categories
In an attempt to determine how well each cate-
gory was defined, we created artificial splits of the
data into binary distinctions: each category versus a
super-category consisting of all the other collapsed cat-
egories. The kappas measured on these datasets are
given in Fig. 8. The higher they are, the better the anno-
tators could distinguish the given category from all the
other categories. We can see that out of the informa-
85
tive categories, four are defined at least as well as the
overall distinction (i.e. above the line in Fig. 8: PMot,
PUse, CoCoGM and CoCoR0. This is encouraging,
as the application of citation maps is almost entirely
centered around usage and contrast. However, the se-
mantics of some categories are less well-understood by
our annotators: in particular PSup (where the difficulty
lies in what an annotator understands as ?mutual sup-
port? of two theories), and (unfortunately) PBas. The
problem with PBas is that its distinction from PUse is
based on subjective judgement of whether the authors
use a part of somebody?s previous work, or base them-
selves entirely on this previous work (i.e., see them-
selves as following in the same intellectual framework).
Another problem concerns the low distinctivity for the
clearly negative categories CoCo- and Weak. This is
in line with MacRoberts and MacRoberts? hypothesis
that criticism is often hedged and not clearly lexically
signalled, which makes it more difficult to reliably an-
notate such citations.
5 Conclusion
We have described a new task: human annotation of
citation function, a phenomenon which we believe to
be closely related to the overall discourse structure of
scientific articles. Our annotation scheme concentrates
on contrast, weaknesses of other work, similarities be-
tween work and usage of other work. One of its prin-
ciples is the fact that relations are only to be marked if
they are explicitly signalled. Here, we report positive
results in terms of interannotator agreement.
Future work on the annotation scheme will concen-
trate on improving guidelines for currently suboptimal
categories, and on measuring intra-annotator agree-
ment and inter-annotator agreement with naive annota-
tors. We are also currently investigating how well our
scheme will work on text from a different discipline,
namely chemistry. Work on applying machine learning
techniques for automatic citation classification is cur-
rently underway (Teufel et al, 2006); the agreement
of one annotator and the system is currently K=.57,
leaving plenty of room for improvement in comparison
with the human annotation results presented here.
6 Acknowledgements
This work was funded by the EPSRC projects
CITRAZ (GR/S27832/01, ?Rhetorical Citation Maps
and Domain-independent Argumentative Zoning?) and
SCIBORG (EP/C010035/1, ?Extracting the Science
from Scientific Publications?).
References
Susan Bonzi. 1982. Characteristics of a literature as predic-
tors of relatedness between cited and citing works. JASIS,
33(4):208?216.
Jean Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Daryl E. Chubin and S. D. Moitra. 1975. Content analysis
of references: Adjunct or alternative to citation counting?Social Studies of Science, 5(4):423?441.
Carolyn O. Frost. 1979. The use of citations in literary re-
search: A preliminary classification of citation functions.Library Quarterly, 49:405.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998.
Citeseer: An automatic citation indexing system. In Pro-ceedings of the Third ACM Conference on Digital Li-braries, pages 89?98.
Jorge E. Hirsch. 2005. An index to quantify an individ-
ual?s scientific research output. Proceedings of the Na-tional Academy of Sciences of the United Stated of Amer-ica (PNAS), 102(46).
Terttu Luukkonen. 1992. Is scientists? publishing behaviour
reward-seeking? Scientometrics, 24:297?319.
Michael H. MacRoberts and Barbara R. MacRoberts. 1984.
The negational reference: Or the art of dissembling. So-cial Studies of Science, 14:91?94.
Michael J. Moravcsik and Poovanalingan Murugesan. 1975.
Some results on the function and quality of citations. So-cial Studies of Science, 5:88?91.
Greg Myers. 1992. In this paper we report...?speech acts
and scientific facts. Journal of Pragmatics, 17(4):295?
313.
John O?Connor. 1982. Citing statements: Computer recogni-
tion and use to improve retrieval. Information Processingand Management, 18(3):125?131.
Charles Oppenheim and Susan P. Renn. 1978. Highly cited
old papers and the reasons why they continue to be cited.JASIS, 29:226?230.
Anna Ritchie, Simone Teufel, and Steven Robertson. 2006.
Creating a test collection for citation-based IR experi-
ments. In Proceedings of HLT-06.
Simon Buckingham Shum. 1998. Evolving the web for sci-
entific knowledge: First steps towards an ?HCI knowledge
web?. Interfaces, British HCI Group Magazine, 39:16?21.
Ina Spiegel-Ru?sing. 1977. Bibliometric and content analy-
sis. Social Studies of Science, 7:97?113.
John Swales. 1986. Citation analysis and discourse analysis.Applied Linguistics, 7(1):39?56.
John Swales, 1990. Genre Analysis: English in Academicand Research Settings. Chapter 7: Research articles in En-glish, pages 110?176. Cambridge University Press, Cam-
bridge, UK.
Simone Teufel, Jean Carletta, and Marc Moens. 1999. An
annotation scheme for discourse-level argumentation in re-
search articles. In Proceedings of the Ninth Meeting of theEuropean Chapter of the Association for ComputationalLinguistics (EACL-99), pages 110?117.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.
Automatic classification of citation function. In Proceed-ings of EMNLP-06.
Simone Teufel. 1999. Argumentative Zoning: InformationExtraction from Scientific Text. Ph.D. thesis, School of
Cognitive Science, University of Edinburgh, UK.
Melvin Weinstock. 1971. Citation indexes. In Encyclopediaof Library and Information Science, volume 5, pages 16?
40. Dekker, New York, NY.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
John M. Ziman. 1968. Public Knowledge: An Essay Con-cerning the Social Dimensions of Science. Cambridge
University Press, Cambridge, UK.
86
A Annotation examples
Weak However, Koskenniemi himself understood that his initial implementation had signif-
icant limitations in handling non-concatenative morphotactic processes.
(0006044, S-4)
CoCoGM The goals of the two papers are slightly different: Moore ?s approach is designed to
reduce the total grammar size (i.e., the sum of the lengths of the productions), while
our approach minimizes the number of productions.
(0008021, S-22)
CoCoR0 This is similar to results in the literature (Ramshaw and Marcus 1995).
(0008022, S-147)
CoCo- For the Penn Treebank, Ratnaparkhi (1996) reports an accuracy of 96.6% using the
Maximum Entropy approach, our much simpler and therefore faster HMM approach
delivers 96.7%. (0003055, S-156)
CoCoXY Unlike previous approaches (Ellison 1994, Walther 1996), Karttunen ?s approach
is encoded entirely in the nite state calculus, with no extra-logical procedures for
counting constraint violations. (0006038, S-5)
PBas Our starting point is the work described in Ferro et al (1999) , which used a fairly
small training set. (0008004, S-11)
PUse In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et
al. 1996). (0003060, S-105)
PModi In our experiments, we have used a conjugate-gradient optimization program adapted
from the one presented in Press et al (0008028, S-72)
PMot It has also been shown that the combined accuracy of an ensemble of multiple clas-
siers is often signicantly greater than that of any of the individual classiers that
make up the ensemble (e.g., Dietterich (1997)). (0005006, S-9)
PSim Our system is closely related to those proposed in Resnik (1997) and Abney and
Light (1999). (0008020, S-24)
PSup In all experiments the SVM Light system outperformed other learning algorithms,
which conrms Yang and Liu ?s (1999) results for SVMs fed with Reuters data.
(0003060, S-141)
Neut The cosine metric and Jaccard?s coefcient are commonly used in information re-
trieval as measures of association (Salton and McGill 1983). (0001012, S-29)
87
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 103?110,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic classication of citation function
Simone Teufel Advaith Siddharthan Dan Tidhar
Natural Language and Information Processing Group
Computer Laboratory
Cambridge University, CB3 0FD, UK
{Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk
Abstract
Citation function is defined as the author?s
reason for citing a given paper (e.g. ac-
knowledgement of the use of the cited
method). The automatic recognition of the
rhetorical function of citations in scientific
text has many applications, from improve-
ment of impact factor calculations to text
summarisation and more informative ci-
tation indexers. We show that our anno-
tation scheme for citation function is re-
liable, and present a supervised machine
learning framework to automatically clas-
sify citation function, using both shallow
and linguistically-inspired features. We
find, amongst other things, a strong re-
lationship between citation function and
sentiment classification.
1 Introduction
Why do researchers cite a particular paper? This
is a question that has interested researchers in
discourse analysis, sociology of science, and in-
formation sciences (library sciences) for decades
(Garfield, 1979; Small, 1982; White, 2004). Many
annotation schemes for citation motivation have
been created over the years, and the question has
been studied in detail, even to the level of in-depth
interviews with writers about each individual cita-
tion (Hodges, 1972).
Part of this sustained interest in citations can
be explained by the fact that bibliometric met-
rics are commonly used to measure the impact of
a researcher?s work by how often they are cited
(Borgman, 1990; Luukkonen, 1992). However, re-
searchers from the field of discourse studies have
long criticised purely quantitative citation analy-
sis, pointing out that many citations are done out
of ?politeness, policy or piety? (Ziman, 1968),
and that criticising citations or citations in pass-
ing should not ?count? as much as central cita-
tions in a paper, or as those citations where a re-
searcher?s work is used as the starting point of
somebody else?s work (Bonzi, 1982). A plethora
of manual annotation schemes for citation motiva-
tion have been invented over the years (Garfield,
1979; Hodges, 1972; Chubin and Moitra, 1975).
Other schemes concentrate on citation function
(Spiegel-Ru?sing, 1977; O?Connor, 1982; Wein-
stock, 1971; Swales, 1990; Small, 1982)). One
of the best-known of these studies (Moravcsik
and Murugesan, 1975) divides citations in running
text into four dimensions: conceptual or opera-
tional use (i.e., use of theory vs. use of technical
method); evolutionary or juxtapositional (i.e., own
work is based on the cited work vs. own work is an
alternative to it); organic or perfunctory (i.e., work
is crucially needed for understanding of citing ar-
ticle or just a general acknowledgement); and fi-
nally confirmative vs. negational (i.e., is the cor-
rectness of the findings disputed?). They found,
for example, that 40% of the citations were per-
functory, which casts further doubt on the citation-
counting approach.
Based on such annotation schemes and hand-
analyzed data, different influences on citation be-
haviour can be determined. Nevertheless, re-
searchers in the field of citation content analysis
do not normally cross-validate their schemes with
independent annotation studies with other human
annotators, and usually only annotate a small num-
ber of citations (in the range of hundreds or thou-
sands). Also, automated application of the annota-
tion is not something that is generally considered
in the field, though White (2004) sees the future of
discourse-analytic citation analysis in automation.
Apart from raw material for bibliometric stud-
ies, citations can also be used for search purposes
in document retrieval applications. In the library
world, printed or electronic citation indexes such
as ISI (Garfield, 1979) serve as an orthogonal
103
Nitta and Niwa 94
Resnik 95
Brown et al 90a
Rose et al 90
Church and Gale 91
Dagan et al 94
Li and Abe 96
Hindle 93
Hindle 90
Dagan et al93
Pereira et al 93
Following Pereira et al we measure
word similarity by the relative entropy
or Kulbach?Leibler (KL) distance, bet?
ween the corresponding conditional
distributions.
His notion of similarityseems to agree with ourintuitions in many cases,but it is not clear how itcan  be used directly toconstruct word classesand corresponding models of association.
Figure 1: A rhetorical citation map
search tool to find relevant papers, starting from a
source paper of interest. With the increased avail-
ability of documents in electronic form in recent
years, citation-based search and automatic citation
indexing have become highly popular, cf. the suc-
cessful search tools Google Scholar and CiteSeer
(Giles et al, 1998).1
But not all search needs are fulfilled by current
citation indexers. Experienced researchers are of-
ten interested in relations between articles (Shum,
1998). They want to know if a certain article crit-
icises another and what the criticism is, or if the
current work is based on that prior work. This
type of information is hard to come by with current
search technology. Neither the author?s abstract,
nor raw citation counts help users in assessing the
relation between articles.
Fig. 1 shows a hypothetical search tool which
displays differences and similarities between a tar-
get paper (here: Pereira et al, 1993) and the pa-
pers that it cites and that cite it. Contrastive links
are shown in grey ? links to rival papers and pa-
pers the current paper contrasts itself to. Continu-
ative links are shown in black ? links to papers that
use the methodology of the current paper. Fig. 1
also displays the most characteristic textual sen-
tence about each citation. For instance, we can see
which aspect of Hindle (1990) our example paper
criticises, and in which way the example paper?s
work was used by Dagan et al (1994).
Note that not even the CiteSeer text snippet
1These tools automatically citation-index all scientific ar-
ticles reached by a web-crawler, making them available to
searchers via authors or keywords in the title, and displaying
the citation in context of a text snippet.
can fulfil the relation search need: it is always
centered around the physical location of the ci-
tations, but the context is often not informative
enough for the searcher to infer the relation. In
fact, studies from our annotated corpus (Teufel,
1999) show that 69% of the 600 sentences stat-
ing contrast with other work and 21% of the
246 sentences stating research continuation with
other work do not contain the corresponding cita-
tion; the citation is found in preceding sentences
(which means that the sentence expressing the
contrast or continuation is outside the CiteSeer
snippet). A more sophisticated, discourse-aware
citation indexer which finds these sentences and
associates them with the citation would add con-
siderable value to the researcher?s bibliographic
search (Ritchie et al, 2006b).
Our annotation scheme for citations is based
on empirical work in content citation analysis. It
is designed for information retrieval applications
such as improved citation indexing and better bib-
liometric measures (Teufel et al, 2006). Its 12 cat-
egories mark relationships with other works. Each
citation is labelled with exactly one category. The
following top-level four-way distinction applies:
? Explicit statement of weakness
? Contrast or comparison with other work (4
categories)
? Agreement/usage/compatibility with other
work (6 categories), and
? A neutral category.
In this paper, we show that the scheme can be
reliably annotated by independent coders. We also
report results of a supervised machine learning ex-
periment which replicates the human annotation.
2 An annotation scheme for citations
Our scheme (given in Fig. 2) is adapted from that
of Spiegel-Ru?sing (1977) after an analysis of a
corpus of scientific articles in computational lin-
guistics. We avoid sociologically orientated dis-
tinctions (?paying homage to pioneers?), as they
can be difficult to operationalise without deep
knowledge of the field and its participants (Swales,
1986). Our redefinition of the categories aims at
reliably annotation; at the same time, the cate-
gories should be informative enough for the docu-
ment management application sketched in the in-
troduction.
104
Category Description
Weak Weakness of cited approach
CoCoGM Contrast/Comparison in Goals or Meth-
ods(neutral)
CoCo- Author?s work is stated to be superior to
cited work
CoCoR0 Contrast/Comparison in Results (neutral)
CoCoXY Contrast between 2 cited methods
PBas Author uses cited work as basis or starting
point
PUse Author uses
tools/algorithms/data/definitions
PModi Author adapts or modifies
tools/algorithms/data
PMot This citation is positive about approach
used or problem addressed (used to mo-
tivate work in current paper)
PSim Author?s work and cited work are similar
PSup Author?s work and cited work are compat-
ible/provide support for each other
Neut Neutral description of cited work, or not
enough textual evidence for above cate-
gories, or unlisted citation function
Figure 2: Annotation scheme for citation function.
Our categories are as follows: One category
(Weak) is reserved for weakness of previous re-
search, if it is addressed by the authors. The next
four categories describe comparisons or contrasts
between own and other work. The difference be-
tween them concerns whether the contrast is be-
tween methods employed or goals (CoCoGM), or
results, and in the case of results, a difference is
made between the cited results being worse than
the current work (CoCo-), or comparable or bet-
ter results (CoCoR0). As well as considering dif-
ferences between the current work and other work,
we also mark citations if they are explicitly com-
pared and contrasted with other work (i.e. not
the work in the current paper). This is expressed
in category CoCoXY. While this is not typically
annotated in the literature, we expect a potential
practical benefit of this category for our applica-
tion, particularly in searches for differences and
rival approaches.
The next set of categories we propose concerns
positive sentiment expressed towards a citation, or
a statement that the other work is actively used
in the current work (which we consider the ulti-
mate praise). We mark statements of use of data
and methods of the cited work, differentiating un-
changed use (PUse) from use with adaptations
(PModi). Work which is stated as the explicit
starting point or intellectual ancestry is marked
with our category PBas. If a claim in the liter-
ature is used to strengthen the authors? argument,
or vice versa, we assign the category PSup. We
also mark similarity of (an aspect of) the approach
to the cited work (PSim), and motivation of ap-
proach used or problem addressed (PMot).
Our twelfth category, Neut, bundles truly neu-
tral descriptions of cited work with those cases
where the textual evidence for a citation function
was not enough to warrant annotation of that cate-
gory, and all other functions for which our scheme
did not provide a specific category.
Citation function is hard to annotate because it
in principle requires interpretation of author inten-
tions (what could the author?s intention have been
in choosing a certain citation?). One of our most
fundamental principles is thus to only mark explic-
itly signalled citation functions. Our guidelines
explicitly state that a general linguistic phrase such
as ?better? or ?used by us? must be present; this
increases the objectivity of defining citation func-
tion. Annotators must be able to point to textual
evidence for assigning a particular function (and
are asked to type the source of this evidence into
the annotation tool for each citation). Categories
are defined in terms of certain objective types of
statements (e.g., there are 7 cases for PMot, e.g.
?Citation claims that or gives reasons for why
problem Y is hard?). Annotators can use general
text interpretation principles when assigning the
categories (such as anaphora resolution and par-
allel constructions), but are not allowed to use in-
depth knowledge of the field or of the authors.
Guidelines (25 pages, ? 150 rules) describe the
categories with examples, provide a decision tree
and give decision aids in systematically ambigu-
ous cases. Nevertheless, subjective judgement of
the annotators is still necessary to assign a single
tag in an unseen context, because of the many dif-
ficult cases for annotation. Some of these concern
the fact that authors do not always state their pur-
pose clearly. For instance, several earlier studies
found that negational citations are rare (Moravc-
sik and Murugesan, 1975; Spiegel-Ru?sing, 1977);
MacRoberts and MacRoberts (1984) argue that the
reason for this is that they are potentially politi-
cally dangerous. In our data we found ample evi-
dence of the ?meekness? effect. Other difficulties
concern the distinction of the usage of a method
from statements of similarity between a method
and the own method (i.e., the choice between cat-
egories PSim and PUse). This happens in cases
where authors do not want to admit (or stress)
105
that they are using somebody else?s method. An-
other difficult distinction concerns the judgement
of whether the authors continue somebody?s re-
search (i.e., consider their research as intellectual
ancestry, i.e. PBas), or whether they simply use
the work (PUse).
The unit of annotation is a) the full citation (as
recognised by our automatic citation processor on
our corpus), and b) names of authors of cited pa-
pers anywhere in running text outside of a for-
mal citation context (i.e., without date). These
latter are marked up, slightly unusually in com-
parison to other citation indexers, because we be-
lieve they function as important referents compa-
rable in importance to formal citations.2 In prin-
ciple, there are many other linguistic expressions
by which the authors could refer to other people?s
work: pronouns, abbreviations such as ?Mueller
and Sag (1990), henceforth M & S?, and names of
approaches or theories which are associated with
particular authors. The fact that in these contexts
citation function cannot be annotated (because it
is not technically feasible to recognise them well
enough) sometimes causes problems with context
dependencies.
While there are unambiguous example cases
where the citation function can be decided on the
basis of the sentence alone, this is not always the
case. Most approaches are not criticised in the
same sentence where they are also cited: it is more
likely that there are several descriptive sentences
about a cited approach between its formal cita-
tion and the evaluative statement, which is often at
the end of the textual segment about this citation.
Nevertheless, the annotator must mark the func-
tion on the nearest appropriate annotation unit (ci-
tation or author name). Our rules decree that con-
text is in most cases constrained to the paragraph
boundary. In rare cases, paper-wide information
is required (e.g., for PMot, we need to know that
a praised approach is used by the authors, infor-
mation which may not be local in the paragraph).
Annotators are thus asked to skim-read the paper
before annotation.
One possible view on this annotation scheme
could consider the first two sets of categories as
?negative? and the third set of categories ?posi-
tive?, in the sense of Pang et al (2002) and Turney
(2002). Authors need to make a point (namely,
2Our citation processor can recognise these after parsing
the citation list.
that they have contributed something which is bet-
ter or at least new (Myers, 1992)), and they thus
have a stance towards their citations. But although
there is a sentiment aspect to the interpretation of
citations, this is not the whole story. Many of our
?positive? categories are more concerned with dif-
ferent ways in which the cited work is useful to the
current work (which aspect of it is used, e.g., just a
definition or the entire solution?), and many of the
contrastive statements have no negative connota-
tion at all and simply state a (value-free) differ-
ence between approaches. However, if one looks
at the distribution of positive and negative adjec-
tives around citations, it is clear that there is a non-
trivial connection between our task and sentiment
classification.
The data we use comes from our corpus of
360 conference articles in computational linguis-
tics, drawn from the Computation and Language
E-Print Archive (http://xxx.lanl.gov/cmp-lg). The
articles are transformed into XML format; head-
lines, titles, authors and reference list items are au-
tomatically marked up. Reference lists are parsed
using regular patterns, and cited authors? names
are identified. Our citation parser then finds cita-
tions and author names in running text and marks
them up. Ritchie et al (2006a) report high ac-
curacy for this task (94% of citations recognised,
provided the reference list was error-free). On av-
erage, our papers contain 26.8 citation instances in
running text3. For human annotation, we use our
own annotation tool based on XML/XSLT tech-
nology, which allows us to use a web browser to
interactively assign one of the 12 tags (presented
as a pull-down list) to each citation.
We measure inter-annotator agreement between
three annotators (the three authors), who indepen-
dently annotated 26 articles with the scheme (con-
taining a total of 120,000 running words and 548
citations), using the written guidelines. The guide-
lines were developed on a different set of articles
from the ones used for annotation.
Inter-annotator agreement was Kappa=.72
(n=12;N=548;k=3)4 . This is quite high, consider-
ing the number of categories and the difficulties
3As opposed to reference list items, which are fewer.
4Following Carletta (1996), we measure agreement in
Kappa, which follows the formula K = P (A)?P (E)1?P (E) whereP(A) is observed, and P(E) expected agreement. Kappa
ranges between -1 and 1. K=0 means agreement is only as
expected by chance. Generally, Kappas of 0.8 are considered
stable, and Kappas of .69 as marginally stable, according to
the strictest scheme applied in the field.
106
(e.g., non-local dependencies) of the task. The
relative frequency of each category observed in
the annotation is listed in Fig. 3. As expected,
the distribution is very skewed, with more than
60% of the citations of category Neut.5 What
is interesting is the relatively high frequency
of usage categories (PUse, PModi, PBas)
with a total of 18.9%. There is a relatively low
frequency of clearly negative citations (Weak,
CoCo-, total of 4.1%), whereas the neutral?
contrastive categories (CoCoR0, CoCoXY,
CoCoGM) are slightly more frequent at 7.6%.
This is in concordance with earlier annotation
experiments (Moravcsik and Murugesan, 1975;
Spiegel-Ru?sing, 1977).
3 Features for automatic recognition of
citation function
This section summarises the features we use for
machine learning citation function. Some of these
features were previously found useful for a dif-
ferent application, namely Argumentative Zoning
(Teufel, 1999; Teufel and Moens, 2002), some are
specific to citation classification.
3.1 Cue phrases
Myers (1992) calls meta-discourse the set of ex-
pressions that talk about the act of presenting re-
search in a paper, rather than the research itself
(which is called object-level discourse). For in-
stance, Swales (1990) names phrases such as ?to
our knowledge, no. . . ? or ?As far as we aware? as
meta-discourse associated with a gap in the cur-
rent literature. Strings such as these have been
used in extractive summarisation successfully ever
since Paice?s (1981) work.
We model meta-discourse (cue phrases) and
treat it differently from object-level discourse.
There are two different mechanisms: A finite
grammar over strings with a placeholder mecha-
nism for POS and for sets of similar words which
can be substituted into a string-based cue phrase
(Teufel, 1999). The grammar corresponds to 1762
cue phrases. It was developed on 80 papers which
are different to the papers used for our experiments
here.
The other mechanism is a POS-based recog-
niser of agents and a recogniser for specific actions
these agents perform. Two main agent types (the
5Spiegel-Ru?sing found that out of 2309 citations she ex-
amined, 80% substantiated statements.
authors of the paper, and everybody else) are mod-
elled by 185 patterns. For instance, in a paragraph
describing related work, we expect to find refer-
ences to other people in subject position more of-
ten than in the section detailing the authors? own
methods, whereas in the background section, we
often find general subjects such as ?researchers in
computational linguistics? or ?in the literature?.
For each sentence to be classified, its grammatical
subject is determined by POS patterns and, if pos-
sible, classified as one of these agent types. We
also use the observation that in sentences without
meta-discourse, one can assume that agenthood
has not changed.
20 different action types model the main verbs
involved in meta-discourse. For instance, there is
a set of verbs that is often used when the over-
all scientific goal of a paper is defined. These
are the verbs of presentation, such as ?propose,
present, report? and ?suggest?; in the corpus we
found other verbs in this function, but with a lower
frequency, namely ?describe, discuss, give, intro-
duce, put forward, show, sketch, state? and ?talk
about?. There are also specialised verb clusters
which co-occur with PBas sentences, e.g., the
cluster of continuation of ideas (eg. ?adopt, agree
with, base, be based on, be derived from, be orig-
inated in, be inspired by, borrow, build on,. . . ?).
On the other hand, the semantics of verbs in Weak
sentences is often concerned with failing (of other
researchers? approaches), and often contain verbs
such as ?abound, aggravate, arise, be cursed, be
incapable of, be forced to, be limited to, . . . ?.
We use 20 manually acquired verb clusters.
Negation is recognised, but too rare to define its
own clusters: out of the 20 ? 2 = 40 theoretically
possible verb clusters, only 27 were observed in
our development corpus. We have recently auto-
mated the process of verb?object pair acquisition
from corpora for two types of cue phrases (Abdalla
and Teufel, 2006) and are planning on expanding
this work to other cue phrases.
3.2 Cues Identified by annotators
During the annotator training phase, the anno-
tators were encouraged to type in the meta-
description cue phrases that justify their choice of
category. We went through this list by hand and
extracted 892 cue phrases (around 75 per cate-
gory). The files these cues came from were not
part of the test corpus. We included 12 features
107
Neut PUse CoCoGM PSim Weak PMot CoCoR0 PBas CoCoXY CoCo- PModi PSup
62.7% 15.8% 3.9% 3.8% 3.1% 2.2% 0.8% 1.5% 2.9% 1.0% 1.6% 1.1%
Figure 3: Distribution of citation categories
Weak CoCoGM CoCoR0 CoCo- CoCoXY PBas PUse PModi PMot PSim PSup Neut
P .78 .81 .77 .56 .72 .76 .66 .60 .75 .68 .83 .80
R .49 .52 .46 .19 .54 .46 .61 .27 .64 .38 .32 .92
F .60 .64 .57 .28 .62 .58 .63 .37 .69 .48 .47 .86
Percentage Accuracy 0.77
Kappa (n=12; N=2829; k=2) 0.57
Macro-F 0.57
Figure 4: Summary of Citation Analysis results (10-fold cross-validation; IBk algorithm; k=3).
that recorded the presence of cues that our annota-
tors associated with a particular class.
3.3 Other features
There are other features which we use for this
task. We know from Teufel and Moens (2002) that
verb tense and voice should be useful for recogniz-
ing statements of previous work, future work and
work performed in the paper. We also recognise
modality (whether or not a main verb is modified
by an auxiliary, and which auxiliary it is).
The overall location of a sentence containing
a reference should be relevant. We observe that
more PMot categories appear towards the begin-
ning of the paper, as do Weak citations, whereas
comparative results (CoCoR0, CoCoR-) appear
towards the end of articles. More fine-grained lo-
cation features, such as the location within the
paragraph and the section, have also been imple-
mented.
The fact that a citation points to own previous
work can be recognised, as we know who the pa-
per authors are. As we have access to the infor-
mation in the reference list, we also know the last
names of all cited authors (even in the case where
an et al statement in running text obscures the
later-occurring authors). With self-citations, one
might assume that the probability of re-use of ma-
terial from previous own work should be higher,
and the tendency to criticise lower.
4 Results
Our evaluation corpus for citation analysis con-
sists of 116 articles (randomly drawn from the part
of our corpus which was not used for guideline
development or cue phrase acquisition). The 116
articles contain 2829 citation instances. Each
citation instance was manually tagged as one
Weakness Positive Contrast Neutral
P .80 .75 .77 .81
R .49 .65 .52 .90
F .61 .70 .62 .86
Percentage Accuracy 0.79
Kappa (n=12; N=2829; k=2) 0.59
Macro-F 0.68
Figure 5: Summary of results (10-fold cross-
validation; IBk algorithm; k=3): Top level classes.
Weakness Positive Neutral
P .77 .75 .85
R .42 .65 .92
F .54 .70 .89
Percentage Accuracy 0.83
Kappa (n=12; N=2829; k=2) 0.58
Macro-F 0.71
Figure 6: Summary of results (10-fold cross-
validation; IBk algorithm; k=3): Sentiment Anal-
ysis.
of {Weak, CoCoGM, CoCo-, CoCoR0, CoCoXY,
PBas, PUse, PModi, PMot, PSim, PSup, Neut}.
The papers are then further processed (e.g. to-
kenised and POS-tagged). All other features are
automatically determined (e.g. self-citations are
detected by overlap of citing and cited authors);
then, machine learning is applied to the feature
vectors.
The 10-fold cross-validation results for citation
classification are given in Figure 4, comparing the
system to one of the annotators. Results are given
in three overall measures: Kappa, percentage ac-
curacy, and Macro-F (following Lewis (1991)).
Macro-F is the mean of the F-measures of all
twelve categories. We use Macro-F and Kappa be-
cause we want to measure success particularly on
the rare categories, and because Micro-averaging
techniques like percentage accuracy tend to over-
estimate the contribution of frequent categories in
108
heavily skewed distributions like ours6.
In the case of Macro-F, each category is treated
as one unit, independent of the number of items
contained in it. Therefore, the classification suc-
cess of the individual items in rare categories
is given more importance than classification suc-
cess of frequent category items. However, one
should keep in mind that numerical values in
macro-averaging are generally lower (Yang and
Liu, 1999), due to fewer training cases for the rare
categories. Kappa has the additional advantage
over Macro-F that it filters out random agreement
(random use, but following the observed distribu-
tion of categories).
For our task, memory-based learning outper-
formed other models. The reported results use the
IBk algorithm with k = 3 (we used the Weka ma-
chine learning toolkit (Witten and Frank, 2005)
for our experiments). Fig. 7 provides a few ex-
amples from one file in the corpus, along with the
gold standard citation class, the machine predic-
tion, and a comment.
Kappa is even higher for the top level distinc-
tion. We collapsed the obvious similar categories
(all P categories into one category, and all CoCo
categories into another) to give four top level
categories (Weak, Positive, Contrast,
Neutral; results in Fig. 5). Precision for all the
categories is above 0.75, and K=0.59. For con-
trast, the human agreement for this situation was
K=0.76 (n=3,N=548,k=3).
In a different experiment, we grouped the cate-
gories as follows, in an attempt to perform senti-
ment analysis over the classifications:
Old Categories New Category
Weak, CoCo- Negative
PMot, PUse, PBas, PModi, PSim, PSup Positive
CoCoGM, CoCoR0, CoCoXY, Neut Neutral
Thus negative contrasts and weaknesses are
grouped into Negative, while neutral contrasts
are grouped into Neutral. All positive classes
are conflated into Positive.
Results show that this grouping raises results
to a smaller degree than the top-level distinction
did (to K=.58). For contrast, the human agree-
ment for these collapsed categories was K=.75
(n=3,N=548,k=3).
6This situation has parallels in information retrieval,
where precision and recall are used because accuracy over-
estimates the performance on irrelevant items.
5 Conclusion
We have presented a new task: annotation of ci-
tation function in scientific text, a phenomenon
which we believe to be closely related to the over-
all discourse structure of scientific articles. Our
annotation scheme concentrates on weaknesses of
other work, and on similarities and contrast be-
tween work and usage of other work. In this
paper, we present machine learning experiments
for replicating the human annotation (which is re-
liable at K=.72). The automatic result reached
K=.57 (acc=.77) for the full annotation scheme;
rising to Kappa=.58 (acc=.83) for a three-way
classification (Weak, Positive, Neutral).
We are currently performing an experiment to
see if citation processing can increase perfor-
mance in a large-scale, real-world information
retrieval task, by creating a test collection of
researchers? queries and relevant documents for
these (Ritchie et al, 2006a).
6 Acknowledgements
This work was funded by the EPSRC projects CIT-
RAZ (GR/S27832/01, ?Rhetorical Citation Maps
and Domain-independent Argumentative Zon-
ing?) and SCIBORG (EP/C010035/1, ?Extracting
the Science from Scientific Publications?).
References
Rashid M. Abdalla and Simone Teufel. 2006. A bootstrap-
ping approach to unsupervised detection of cue phrase
variants. In Proc. of ACL/COLING-06.
Susan Bonzi. 1982. Characteristics of a literature as predic-
tors of relatedness between cited and citing works. JASIS,
33(4):208?216.
Christine L. Borgman, editor. 1990. Scholarly Communica-tion and Bibliometrics. Sage Publications, CA.
Jean Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
Daryl E. Chubin and S. D. Moitra. 1975. Content analysis
of references: Adjunct or alternative to citation counting?Social Studies of Science, 5(4):423?441.
Eugene Garfield. 1979. Citation Indexing: Its Theory andApplication in Science, Technology and Humanities. J.
Wiley, New York, NY.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998.
Citeseer: An automatic citation indexing system. In Proc.of the Third ACM Conference on Digital Libraries, pages
89?98.
T.L. Hodges. 1972. Citation Indexing: Its Potential for Bibli-ographical Control. Ph.D. thesis, University of California
at Berkeley.
David D. Lewis. 1991. Evaluating text categorisation. InSpeech and Natural Language: Proceedings of the ARPAWorkshop of Human Language Technology.
109
Context Human Machine Comment
We have compared four complete and three partial data rep-
resentation formats for the baseNP recognition task pre-
sented in Ramshaw and Marcus (1995).
PUse PUse Cues can be weak: ?for... task...presented in?
In the version of the algorithm that we have used, IB1-IG,
the distances between feature representations are computed
as the weighted sum of distances between individual features(Bosch 1998).
Neut PUse Human decided citation was for
detail in used package, not di-
rectly used by paper.
We have used the baseNP data presented in Ramshaw andMarcus (1995). PUse PUse Straightforward case
We will follow Argamon et al (1998) and use
a combination of the precision and recall rates:
F=(2*precision*recall)/(precision+recall).
PSim PUse Human decided F-measure was
not attributable to citation. Hence
similarity rather than usage.
This algorithm standardly uses the single training item clos-
est to the test i.e. However Daelemans et al (1999) report
that for baseNP recognition better results can be obtained by
making the algorithm consider the classification values of the
three closest training items.
Neut PUse Shallow processing by Machine
means that it is mislead by the
strong cue in preceding sentence.
They are better than the results for section 15 because more
training data was used in these experiments. Again the
best result was obtained with IOB1 (F=92.37) which is an
improvement of the best reported F-rate for this data setRamshaw and Marcus 1995 (F=92.03).
CoCo- PUse Machine attached citation to the
data set being used. Human at-
tached citation to the result being
compared.
Figure 7: Examples of classifications by the machine learner.
Terttu Luukkonen. 1992. Is scientists? publishing behaviour
reward-seeking? Scientometrics, 24:297?319.
Michael H. MacRoberts and Barbara R. MacRoberts. 1984.
The negational reference: Or the art of dissembling. So-cial Studies of Science, 14:91?94.
Michael J. Moravcsik and Poovanalingan Murugesan. 1975.
Some results on the function and quality of citations. So-cial Studies of Science, 5:88?91.
Greg Myers. 1992. In this paper we report...?speech acts
and scientific facts. Journal of Pragmatics, 17(4).
John O?Connor. 1982. Citing statements: Computer recogni-
tion and use to improve retrieval. Information Processingand Management, 18(3):125?131.
Chris D. Paice. 1981. The automatic generation of literary
abstracts: an approach based on the identification of self-
indicating phrases. In R. Oddy, S. Robertson, C. van Rijs-
bergen, and P. W. Williams, editors, Information RetrievalResearch. Butterworth, London, UK.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine
learning techniques. In Proc. of EMNLP-02.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006a. Creating a test collection for citation-based IR ex-
periments. In Proc. of HLT/NAACL 2006, New York, US.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006b. How to find better index terms through citations.
In Proc. of ACL/COLING workshop ?Can ComputationalLinguistics improve IR?.
Simon Buckingham Shum. 1998. Evolving the web for sci-
entific knowledge: First steps towards an ?HCI knowledge
web?. Interfaces, British HCI Group Magazine, 39.
Henry Small. 1982. Citation context analysis. In P. Dervin
and M. J. Voigt, editors, Progress in Communication Sci-ences 3, pages 287?310. Ablex, Norwood, N.J.
Ina Spiegel-Ru?sing. 1977. Bibliometric and content analy-
sis. Social Studies of Science, 7:97?113.
John Swales. 1986. Citation analysis and discourse analysis.Applied Linguistics, 7(1):39?56.
John Swales, 1990. Genre Analysis: English in Academicand Research Settings. Chapter 7: Research articles inEnglish, pages 110?176. Cambridge University Press,
Cambridge, UK.
Simone Teufel and Marc Moens. 2002. Summarising scien-
tific articles ? experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?446.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.
An annotation scheme for citation function. In Proc. ofSIGDial-06.
Simone Teufel. 1999. Argumentative Zoning: InformationExtraction from Scientific Text. Ph.D. thesis, School of
Cognitive Science, University of Edinburgh, UK.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classification
of reviews. In Proc. of ACL-02.
Melvin Weinstock. 1971. Citation indexes. In Encyclope-dia of Library and Information Science, volume 5. Dekker,
New York, NY.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practi-cal machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In Proc. of SIGIR-99.
John M. Ziman. 1968. Public Knowledge: An Essay Con-cerning the Social Dimensions of Science. Cambridge
University Press, Cambridge, UK.
110
Learn ing  to Se lect  a Good Trans la t ion  
Dan T idhar  and Uwe Ki i ssner  
Technische UniversitAt 13erlin 
Fachbereich Informatik 
Franklinstr.  28/29 
D-10587 Berlin 
Germany 
{ukl dan}~cs,  tu -ber l in ,  de 
Abst ract  
Within tile machine translation system Verb- 
mobil, translation is 1)ertbrmed simultaneously 
1)y four indel)endent translation lnodules. The 
\['our competing l;ranslatiol~s are coati)|ned 1)y a 
se,\[e('tion module so as to forln a single opti- 
mal outlmt for each intmt utterance. The se- 
lection module relies on confidence values that 
are delivered together with each of the alter- 
native translations. Sin(:e the (:onfidence val- 
ues are computed t)y four independent mod- 
ules that are flmdanlentally difl'erent from (me 
another, they are not dire(:tly (:oml)arat)le and 
ne, ed to l)e rescaled in order to gain (:onq)arative 
signiticance. In this pat)er we describe a ma- 
chine lecturing method tailored to overcome this 
difficulty l)y using offl ine hmnan thedback to 
determine an at)prol)riate confidence res(:aling 
scheme. Additionally, we des(:rit)e some other 
sour(:es of information that are used tbr select- 
ing 1)el;ween the comt)eting translations, and de- 
scribe the way in which the seh',ction t)rocess 
relates to quality of service specifi('ations. 
1 In t roduct ion  
Verbmobil (Wahlster, 2000) is a speech to 
speech machine translation system, aimed at 
handling a wide range of spontaneous pee('h 
phenomena within the restricted domain of 
travel t)lanning and at)pointment sche(hfling 
dialogues. For the language 1)airs English- 
German an(1 Gerinan-English, tbur different 
translation methods are applied in parallel, thus 
increasing the system's robustness and versa- 
tility. Since, exa(:tly one translation should t)(; 
t)roduce(1 for each int)ut utterance, a selection 
1)rocedure is necessary. In order to benelit more 
from this diversity, the alternative translations 
are furthermore, combined within the t)omld- 
aries of single utterances, o as to form \]low corn- 
pound translations. Combining translations 
t'ronl different sources within a multi-thread MT 
system has already proved beneficial in the past 
(Frederking and Nire, nlmrg, 1994). Our i)resent 
work diflhrs fl:om the work reported in there ill 
several ways (at)art from the trivial fact that 
we use 'four heads' rather than three). Firstly, 
we attempt o investigate a systematic solution 
to |;11(; incolnparability of the various confidence 
values. Secondly, as we deal with speech to 
speech rather than text to text translation, dit'- 
ti;renl; segmentations for each given input string 
are allowed, lnaking the segment combination 
process signitic~mtly ulore COml)licated. 
1.1 I ncomparab i l i ty  
Eat;h translation lnodule calculates a confidence 
value tbr each translation that it; produces. 
However, since the w~rious translation methods 
are flmdalnentally difl'erent from one another, 
the resulting contidenee values cannot t)e di- 
re(:tly (:omt)ared across modules. Whereas we 
do assmne a gener~fl (:orrest)ondence between 
confidence wflues and translation quality within 
each one of the mo(hfles, there is no guaranty 
whatsoever that a high value delivered t)y a cer- 
tain module wouM indeed signify a l)etter trans- 
lation when comt)ared with another value, even 
a much lower one, which was delivered l)y an- 
other module. An additional step needs to 1)e 
taken in order to make the confidence wflues 
comparable with one another. 
1.2 Working Hypotheses  
It should be noted that one of our working 
hypotheses, namely, that coniidence values do 
generally reflect translation quality, also (:om- 
t)ensates to ~t certain extent tbr tile lack of a 
wide range theory of translation, according to 
which translations of difli;rent sorts could be 
mlanimously ewduated. The task of evaluating 
843 
translation quality is non-trivial also for human 
annotators, ince the applicable criteria are di- 
verse, and at the absence of a comprehensive 
translation theory, very often lead to contra- 
dicting conclusions. This difficulty is partially 
dealt with in section 4.1 below, but tbr practical 
reasons we tend to accept the need to rely on 
human judgment, partially theory assisted and 
partially intuitive, as inevitable. Another pre- 
supposition that underlies the current work is 
that the desirable rescaling Call be well approx- 
imated by means of linear polynomials. This 
assumption allows us to remain within the rel- 
atively friendly realm of linear equations (al- 
beit inconsistent), and reflects two basic guid- 
ing principles: firstly, that tile rescaling is mo- 
tivated by pragnlatical needs, rather than by 
descriptive aspirations, and secondly, that it 
should not contradict he presupposed correla- 
tion between confidence and quality within each 
module, which implies that the rescaling func- 
tions should be monotonous. 
2 The Var ious Trans lat ion  Paths  
The Vcrbmobil system includes four indepen- 
dent translations paths that operate ill paral- 
lel. The input shared by all paths consists 
of sequences of annotated Word Itypothcscs 
Graphs (WHG), produced by the speech rec- 
ognizer. Each translation inodule chooses in- 
dependently a path through the WItG, and a 
possible segmentation according to its gram- 
mar and to the prosody information (Buckow 
et al, 1998). This implies that even though 
all translation modules share the same input 
data structure, both the chosen input string and 
its chosen segmentation may well differ across 
modules. This section provides tile reader with 
very brief descriptions of the different trans- 
lation subsystems, along with their respective 
confidence value calculation methods. 
? The ali subsystem implenlents an exam- 
pie based translation approach. Confi- 
dence values are calculated according to the 
matching-level of the input string with its 
counterparts in the database. 
? The s ta t t rans  (Ochet  al., 1999) sub- 
system is a statistical translation system. 
Confidence values are calculated according 
to a statistical language model of the target 
language, in conjunction with a statistical 
translation model. 
? The syndia log (K ippct  al., 1999) subsys- 
tem is a dialogue act based translation sys- 
tem. Here the translation invariant con- 
sists of a recognized ialogue act, together 
with its extracted propositional content. 
Tile confidence value reflects the probabil- 
ity that tile dialogue act was recognized 
correctly, together with the extent o which 
the propositional content was successflflly 
extracted. 
? The deep translation path in itself con- 
sists of nmltiple pipelined nlodules: lin- 
guistic analysis, senlantic onstruction, di- 
alogue and discourse semantics, and trans- 
fer (Emele and Dorna, 1996) and gener- 
ation (Kilger and Finklcr, 1.995) compo- 
nents. The transfer module receives dis- 
anlbiguation information from the conte.xt 
(Koch et al, 2000) and dialogue modules. 
The linguistic analysis part consists of sev- 
eral parsers which, in turn, also operate ill 
parallel (Ruland et al, 1998). They include 
all HPSG parser, a Clmnk Parser and a 
statistical parser, all producing data struc- 
tm:es of tile same kind, namely, the Verb- 
mobil Inter:face Terms (VITa) (Schiehlen ct 
al., 2000). Thus, within the deep process- 
ing path, a selection problem arises, simi- 
lar to the larger scale problem of selecting 
the best translation. This internal selec- 
tion process within the deep path is based 
on a probabilistic VIT model. Confidence 
values within the deep path are computed 
according to the amount of coverage of the 
input string by the selected parse, and are 
subject to modifications as a byproduct of 
combining and repairing rules that oper- 
ate within the semantics mechanism. An- 
other source of intbrmation which is used 
for calculating the 'deetf confidence val- 
ues is the generation module, which es- 
timates the percentage of each transfered 
VIT which can be successfiflly realized ill 
the target language. 
Although all confidence values are finally scaled 
to the interval \[0, 100\] by their respective gen- 
crating modules, there seems to be hardly any 
reason to believe that such fimdamentally dif- 
844 
ferent calculation methods would yield magni- 
tudes that aye directly comi)aral)le with one an- 
other. As expected, our experience has shown 
ttsat when confidence values are taken as such, 
without any further modification, their compar- 
at|w; significance is indeed very linfited. 
3 The Se lec t ion  Procedure  
In order to improve their coml)arative signifi- 
cance, the delivered confidence wflues c(s), tbr 
each given segment s, arc rescaled by linear 
flmctions of the tbrm: 
a-c(,s) + b . (1) 
Note that each inI)ut utterance is decomt)osed 
is|to several segments indet)endently , and hence 
t)otentially differently, by each of tim translation 
I)aths. The different segments arc then cora- 
l)|ned to tbrm a data structure which, by anal- 
ogy to Word Itypotheses Graph, (:ass be called 
~l~'av, slation Alternatives Graph, (TAG). The size 
of tiffs graph is bound t)y 4 '~, whi(:h is reached if 
all translation paths hat)pen to (:hoose an idea> 
tical partition into exactly n segments. The 
following vectorial not~tion was adot)ted in or- 
deY to simpli(y the simultaneous reference to all 
tYanslation t)aths. The linear coefficients are 
represented by the following tbur-disnensional 
vectors: 
~, ~ O"syndial?g i; = l)'s'Y'ndial?9 
(1,start,tans ~)~tattra'n.s 
adccp bdeep 
(2) 
Single vector comt)onents cms then be referred 
to by sinq)le t)Yojections, if we ret)Yenent the d i f  
ferent translatiols paths as orthogonal refit vec- 
tors, so that .~ denotes the vector torrent)ending 
to the module by which s had been generated. 
The normalized confidence in then represented 
by: 
(a. + (3) 
In order to express the desirable fav<)ring of 
translations with higher input string coverage, 
the COml)ared magnitudes are actually the 
(rescaled) confidence wflues integrated with 
respect to the time axis, rather than the 
(rencaled) confides, co values as n.ch. Le|; I1 11 
be tim \]ength of a segment .s of the input 
stream, in milliseconds. Let SEQ be the set of 
all possible segment sequences wil;lfin the TAG, 
and Seq E SEQ any particular sequence. 
We define tile normalized confidence of 
Seq as tbllows: 
s~Seq 
Tlfis induces the following order relation: 
Based on this relation, we define the set B of 
best sequences as tbllown: 
B(SEQ)  = {scq E SEQI  seq is a maxinmm 
element in (SEQ; _<c)} ? (4) 
The selection procedure consists isl generating 
the various possible sequences, comlmting their 
respective normalized confidence values, and ar- 
bitrarily choosing a member of the set of best 
sequences. It should be noted that not all 
sequences need to be actually generated and 
tested, due to the incorporation of Dijkstra's 
well known "Shortest Path" algorithm (e.g. in 
(Cormen ctal., 1989)). 
4 The Learn ing  Cyc le  
Learning the Yes(:aling coefficients in l)erformed 
off-line, and shouht normally take place only 
once, ulsless new training data is asseml)led, or 
new criteria tbr the desirable nystelll l)eh~tvior 
have been tbrmulate(l. Tim learning <;y<:le con- 
sisl, n of incorporating human feedback (training 
set alotation) and finding a set of rescaling 
(:oe\[ticients so as to yield a selection t)ro(:edure 
with optimal or close to optimal accord with the 
human ewfluation. A training set, consisting of 
test dialogues that cover the desirable systens 
functionality, is fed through the system, while 
separately storing the outt)uts produced 1)y the 
various translation modules. These are then 
subject to two phases of mmotation (see sec- 
tion 4.1), resulting in a set of 'best' sequences 
of translated segments tbr each input utterance. 
The next tank is to determine the atsl)ropri - 
ate linear rescaling, that would maximize the 
accord 1)etween the rescaled confidence wflues 
and the 1)references xpressed by those 'best' se- 
quences. In order to do that, we first generate a 
large set of ilmqualities as (tescYibed in section 
4.2 below, and then ai)proximate their optimal 
solution, as described in section 4.a. 
845 
4.1 Training Set Annotat ion 
As mentioned above, evaluating alternative 
translations is a complex task, which some- 
times appears to be difficult even for specially 
trained people. When one alternative seems 
highly appropriate and all the others are clearly 
wrong, a vigilant annotator would normally en- 
counter very little difficulty. But when all op- 
tions fall within the reasonable reahn and differ 
only slightly fl'om one another, or even more 
so, when all options are far from perfect, each 
having its mfiquely combined weaknesses and 
advantages what criterion should be used by 
the annotator to decide which weaknesses are 
more crucial tlmn the others? Our human feed- 
back cycle is twotbld: first, the outputs of the al- 
ternative translations paths are annotated sep- 
arately, so as to enable the calculation of the 
'off-line confidence values' as described below. 
For each dialogue turn, all possible combina- 
tions of translated segments that cover the in- 
put are then generated. For each of those possi- 
ble combinations, an overall off-line confidence 
value is calculated, in a similar way to which 
the 'on-line' confidence is calculated (see sec- 
tion 3), leaving out the rescaling coefficients, 
but keeping the time axis integration. These 
segment combinations are then t)resented to the 
annotators tbr a second round, sorted accord- 
ing to their respective otfl ine confidence values. 
Tlle annotator is requested at this stage merely 
to select the best segment combination, which 
would normally be one of the first; to appear 
on the list. The first mmotation stage may be 
described as 'theory assisted annotation', and 
the second is its more intuitive complement. %) 
assist the first mmotation rotund we have com- 
piled a set of mmotation criteria, and designed a
specialized annotation tool for their application. 
These criteria direct the annotator's attention 
to 'essential information items', and rethr to the 
number of such items that have t)een deleted, 
inserted or maintained during the translation. 
Other criteria are the semantic and syntactic 
correctness of the translated utterance as well 
as those of the source utterance. The separate 
annotation of these criteria allows us to express 
the 'off-line confidence' as their weighted linear 
combination. The different weights can be seen 
as implicitly establishing a method of quantify- 
ing translation qnality. One can determine, for 
instance, which is of higher importance - -  syn- 
tactical correctness, or the transmission of all 
essential intbrmation items. Using the vague no- 
tion of 'translation qnality' as a single criterion 
would have definitely caused a great divergence 
in personal annotation style and preferences, as 
can be very well exemplified by the case of the 
dialogue act based translation: some people find 
word by word correctness of a translation mch 
more important than the dialogue act; invari- 
ante, while others argue exactly the opposite 
(Schmitz, 1997),(Schmitz and Quantz, 1995). 
4.2 Generat ing Inequalit ies 
Once the best segment sequences for each ut- 
terance have been determined by the completed 
am~otation procedure, a set of inequalities is 
created using the linear rescaling coetficients as 
variables. This is done simply by stating the re- 
quirement hat the normalized confidence value 
of the best segment sequence should be better 
than the normalized confidence values of each 
one of the other possible sequences. For each 
utterance with n possible segment sequences, 
this requirement is expressed by (n -1 )  inequal- 
ities. It is worth mentioning at this point that 
it sometimes occurs during the second annota- 
tion phase, that numerous equences relating to 
the same utterance are considered 'equally best' 
by the annotator, in such cases, when not all 
sequences are concerned but only a sul)set of 
all possible sequences, we have allowed the an- 
notator to select nnfltiple seqnences as q)est', 
correspondingly multiplying the number of in- 
equalities that are introduced by the utterance 
in question. These multiple sets are known in 
adwmce to be inconsistent, as they in fact for- 
mulate contradictory requirements. Since the 
optinfization procedure attempts to satisfy the 
largest possible subset of inequalities, the logi- 
cal relation between such contradicting sets can 
be seen as disjunction rather than conjunction, 
and they do seem to contribute to the learn- 
ing process, because the different 'equally best' 
sequences are still favored in comparison to all 
other sequences relating to the same utterance. 
The overall resulting set; of inequalities is nor- 
mally very large, and can be expected to be con- 
sistent only in a very idealized world, even in 
the absence of 'equally best' mmotations. The 
inconsistencies reflect many imperfections that 
characterize both the problenl at hand and the 
846 
long way to its solution, inost outstanding of 
which is the fact that the original confidence 
values, as useflfl as they may l)e, are neverthe- 
less far from reilecting the human annotation 
and evaluation results, which are, furthermore, 
not always consistent anlong themselves. The 
rest of the learning process consists in trying to 
satisf~y as many inequalities as possible without 
reaching a contradiction. 
4.3 Opt imizat ion  Heur i s t i cs  
The l)rol)lem of finding the l)est rescaling co- 
eiliciellts reduces itself, under the al)ove inen- 
tioncd presut)t)ositions, to that of fin(ling the 
maxilnal COllsistent sul)set of inequalities within 
a larger, most likely inconsistent, set; of linear in- 
equalities, and solving it. In (Amaldi and Mat- 
tavel\]i, 1997), the problem of extracting close- 
to-lnaxilllUlll consistent subsystelns fi'om an in- 
consistent linear system (MAX CS) is treated 
as part of a strategy for solving the prol)lenl 
of partitioning an inconsistent linear system 
into a lain|real nuIntmr of consistxmt sul)systems 
(MIN PCS). Both t)rol)h',nls are NP-hard, |)uI; 
through a thernlal variation of previous work 
by (Agmon, 1954) and (Motzkin and Schoen- 
berg, 1954), a greedy algorithm is tbrmulated 
t)y (Amaldi and Mattavclli, 1997), which can 
serve as an effective heuristic tbr ol)taining op- 
timal or near to optimal solutions lbr MAX CS. 
hnplementing this algorithm ill the C lmlguagc, 
ellal)led us to comple, te the learning cych', t)y 
tindiug a sol; of coetlicients that maximizes, or 
al. least nearly maximizes, the accord of t;h(; 
rescaled (:onfidence wflues with the judgment 
1)rovided by human aunot;ators. 
5 Add i t iona l  I n fo rmat ion  Sources  
llndel)endently of the confidence rescaling pro- 
cess, we have made several attempts to incorpo- 
rate additional latin'mat|on ill order to refine the 
selection procedure. Some of these attempts, 
such as using probabilistic language model in- 
fi)rmation~ or inferring fi'om the logical relation 
between the approximated propositional con- 
te, nts of neighboring utterances (e.g. trying to 
eliminate contra(liction), have not been fruit- 
ful enough to be worth full descril)tion ill the 
present work. The following two sections de- 
scribe two attempts that do seem to be worth 
mentioning in fltrther detail. 
5.1 D ia logue  Act  I n fo rmat ion  
Our experience shows that the translation qual- 
ity that is accomplished by the different mod- 
ules w~ries, among the rest;, according to the 
dialogue act at hand. This seelns to be par- 
ticularly true for syndia log,  the dialogue act 
based translation path. Those dialogue acts 
that normally transmit very little propositional 
content, or those that transmit no propositional 
content at all, are normally handled better 
t)y synd ia log  compared to dialogue acts that 
transmit more information (such as INFORM, 
wlfieh can in principle transmit any proposi- 
tion). The dialogue act recognition algorithm 
used by synd ia log  does not comt)ute the sin- 
gle most likely dialog act, but rather a probabil- 
ity distril)ution of all possible dialogue acts 1 We 
represent the dialogue act probability distribu- 
tion for a giv(m segment .s by the vector d~t(,~), 
where each component denotes the conditional 
i)rol)al)ility of a certain dialogue act, given the 
segment .s: 
( P I ) &(.*) = 
The. vectors g and b fl:om section 3 above are re- 
placed by the matrices A and 13 which are sim- 
ply a coneatem~tion f the rest)ective (tbdogue 
a('t v(x'tors? 
lJ,,t X = and 0 = 
The normalized confidence wflue, with. incorpo- 
rated dialogue act information can then be ex- 
pressed as: 
= + ? .7 ) .  I t , l l  ? 
.sd?'cq 
5.2 Disambiguat ion  In format ion  
Within the deep translation path, several types 
of underspecitication are used for representing 
ambiguities (Kiissner, 1997), (Kiissner, 1998), 
(Emele and Dorna, 1.998). Whenever an ambi- 
guity has to be resolved in order for the trans- 
lation to succeed, resolution is triggered on de- 
mand (Buschbeck-Wolf, 1997)? Several types 
1For more ilfformation about dialogue, acts in Vca'b- 
mobil, see (Alexandersson ctal., 1997) 
847 
of disambiguation are perfornmd by the context 
module (Koch et al, 2000), which uses various 
knowledge sources in conjunction fbr resolving 
anaphorical and lexical ambiguities. Examples 
for such knowledge sources are world knowl- 
edge, knowledge about the dialogue state, as 
well as various sorts of morphological, syntac- 
tic and semantic information. Since the deep 
translation path is the only one that includes 
contextual disambiguation, its confidence value 
is incremented by the selection module when- 
ever such ambiguities occur. 
6 Qual i ty of Service Parameters  
Translation quality is t)erhaps the most signifi- 
cant Quality of Service (QoS) parameter as far 
as MT systems are concerned. The selection 
module and the learning procedure as described 
above, are indeed ainmd at optimizing this pa- 
rameter. Additionally, we have further exper- 
imented with our selection module in order to 
accommodate for other QoS parameters as well. 
Analogously to QoS in Open Distributed Pro- 
gramming (ODP), we can distinguish t)etween 
the tbllowing main categories: timeliness, vol- 
ume, and reliability. In the timeliness category, 
we refer to the delay from the beginning of the 
acoustic intmt till the begilnfing of the acoustic 
output, which is highly dependent on the sys- 
tem's incrementality. The algorithm described 
so far requires the presence of all translated seg- 
ments within a given dialogue turn, betbre the 
selection itself cast take place. This implies a 
relatively long delay, because the biggest pos- 
sible increment unit, i.e. the whole turn, is 
being used. The maximal increlnentality, and 
therefore the minimal delay, are achieved when 
the first ready segment is being chosen at each 
point. This implies, however, a possible deterio- 
ration in translation quality, and increasing the 
risk that due to segmentation differences across 
modules, no appropriate continuation would be 
found tbr the frst  segment hat had been cho- 
sen. The latter is referred to as 'loss rate', and 
belongs to the reliability category of QoS di- 
mensions. The trade-off between loss rate and 
incrementality is parameterized by the selec- 
tion module, by selecting a segment as soon as 
n translation modules have delivered segments 
with similar segmentations (1 < n < 4). Within 
the vohnne category, we define the real time fac- 
tor (RTF) as the relation between the overall 
processing time (from the beginning of acous- 
tic input till the end of acoustic output) and 
the overall speaking time (lmginning of acous- 
tic input till the end of acoustic input). In 
order to SUl)port conformance to RTF specifi- 
cation tbr the translation service, the selection 
module supports a QoS signal interface. A QoS 
managenmnt module monitors the runtime be- 
havior of the translation modules, and signals 
the selection process if the estimated R3~F is 
expected to exceed the specification. Upon re- 
ceiving such a signal, the selection module at- 
tempts to complete its output without waiting 
\['or fllrther translated segments. 
7 Conclus ion 
We have described certain difficulties that arise 
fl'om the attempt o integrate multiple alterna- 
tive translation paths and to choose their op- 
timal coml)ination into one 'l)est' translatiou. 
Using confidence values that originate fl'om dif- 
ferent translation modules as our basic selec- 
tion criterion, we have introduced a learning 
method which enables us to select in maximal 
accord with decisions taken by human annota- 
tors. Along the way, we have also tackled some 
problematic aspects of translation evaluation as 
such, described some additional sources of in- 
formation that are used by our selection mod- 
ule, and briefly sketched the way in which it; 
supports quality of service specifications. The 
extent to which this module succeeds in creat- 
ing higher quality compound translations is of 
course highly dependent on the appropriate as- 
signment of confidence values, pertbrmed by the 
translation modules themselves. As a rough cri- 
terion tbr evaluating our success, we compared 
the selection module's output to the best re- 
suits achieved by a single translation path. Re- 
cent Verbmobil evaluation results demonstrate 
an improvement of 27.8% achieved by the selec- 
tion module, measured by the number of dia- 
logue turns that were marked 'good' by amm- 
tators who were presented with live alternative 
translations tbr each turn, namely, those deliv- 
ered by the four single paths, and the coml)ound 
translation delivered by the selection module. 
848 
References  
S.Agmon. Th, c rclazatiov, mcth, od for linear in- 
cq'aalitics C;madi:m ,\]ournM of M~tthcmati(:s, 
6:382-392, 19M. 
J.Ale.xmMersson, B.liuschb(~(:k-Wolf, 
T.lhtjin~tmi, M.Kipp, S.Koch, l,kMaier, 
N.1l.cithingcr, B.Schmil;z, M.Siegel. Dialoquc 
Acts in VERBMOBIL-2 Sc<:oud Editio~l, 
\])FKI Saarbriickcn, Universitiil; Sl;ul;l;garl;, 
Technis('he Univ(,rsitSt \]h'rlin, Univ('rsiti~t 
des Sa~MmMcs, Verbmol)il Rot)oft 226, Mai 
\ ] .997.  
E.AmMdi, M.Mattavelli. A combinatorical op- 
timization approach to extract pieccwise tin- 
car structure .\[:rom nonlinear data and a'n ap- 
plication to optical Jto'w scgmantatio'n , TR 
\[)7-12, Corncll Comlml;ational ()t)timiz~tion 
Project, Corn(;ll University, Itlm(:a NY, USA. 
J.Bu(:kow, A.Batlincr, F.Gallwitz, R.Ihfl)cr, 
E.NSth, V.Wm:nke, mM H.Nicmmm. Dove- 
tailing of Acov, stics and Prosody i'n ,5'pov, ta- 
rico'as Speech R, ccognitio',, In Pro(:. int. Conf. 
on St)okcn Languagc Processing, vohmw, 3, 
pages 571-574, Sydlmy, Australia, De,(:cmbcr 
1998. 
B.Buschbc(:k-Wolf. lb:sol,atio',, on Demar~,d. 
1Jnivcrsil;i~t Stuttgarl;. Verl)mol)il l{,et)ort 196. 
May \]997. 
T.(~orlllCll, (Ll~eiscrson, \].\]/,ivcl;. I)~.t'rod'ltction 
to Algorithms MIT \])r(;ss, (Smfl)ri(tgc, Mas- 
sachusetts, \] 98!t. 
M.Emcle, M.Dorna. EJficicnt hnph:mcu, tation 
of a Semantic-based T'lnn.~/~r Approach In 
Proceedings ofthe 12th Eurot)can Conference 
on Artificial intelligence (ECAI-9(i). August 
1996. 
MZEmclc, M.Dorna. Ambiguity Pre, s+:r,ving Ma- 
chine 7;innslation 'asing l)ackcd Rcprcs'c'uta- 
tions. In Proceedings of tim \]7th Int(;rna- 
l;iomfl Confcrcncc on Comtml;~tionM Linguis- 
tics (COLING-ACL '98), MontreM, Canada. 
August 1998. 
FLlKcdcrking, S.Nirenburg. Thr(',c ltc,,ds arc 
Better than One, ANLP94P, p 95-100, 1994. 
A.Kilger, W.Finklcr. \]ncrcmcntal Generation 
.for Real-Time Applications, DFKI Report; 
RR-95-11, German tl,('sc~rch Center for Ar- 
titiciM Intelligence - DFKI GmbH, 1995. 
M.Kipp, J.Alcxandersson, N.Rcithinger. Un- 
derstanding Spou, tancous Negotiation Dia- 
h).q'uc Proceedings of the, IJCAI Worksho I)
Knowledge and Reasoning in Practical Dia- 
h)guc Systcllts, Stockhohn, Sweden, August 
1999. 
S.Koch, U.Kfissncr, M.Stcdc. Contc:ctnaI Dis- 
ambig'aation, hi W.Wahlstcr, Ed. Vcrbmobih 
l;b'andations of Spccch, to Speech, 2~'anslation 
Springer Verlag, 2000. 
U.Kiissncr. Applying DL in Automatic Dialogv, e 
Interpreting, Proceedings of the InternaI;ional 
Workshop on Descril)tion Logics - DL-97, pp 
54-58, Gif sur Yvette, France, 1997. 
U.Kiissncr. Description Logic Unplugged Pro- 
ceedings of the, InternationM Workshop on 
Description Logics - DL-98, pp 142-146,  
~lYcnto, Italy, 1998. 
T.S.Motzkin, I.J.Schocnberg. Th, c rclazation 
method for linear inequalities Cmm(tian Jour- 
nal of Mathem~l;ics, 6:393-4()4, 1954. 
F..l.()ch, C.Tillmnnn, lt.Ncy. Improvcd Align- 
mcnt models for Statistical A/htchinc ~J)'au, sla- 
tion, In Proc. of the Joint SIGI)AT Conf. on 
Empirical Methods in Natural Language Pro- 
tossing and Very Large Corpora, Univcrsity 
of Maryland, 1999. 
T.Ruland, C.J.Rut)p, J.Spilker, H.Weber, 
C.Worm. Mak, in 9 th, c Most of Mult@licity: A 
M'ulti-Parscr Multi-Strategy Ar(:h, itcct'ar~: for 
th, c \]?,ob'ast Proccssiu, g of ,5'poke.v, La'n,g'aagc. 
Proceedings of ICSLP 1998. 
M.Schich;n, ,\].Bos, M.l)orna Verbmobil nl, c,r- 
fat(', ~lhrms (VITs), In W.Wahlsl;er, Ed. 
Ve'rbmobil: I,b',,ndatiou,~" of Spccch, to Spccch, 
'l'ranslation Springer Vcrlag, 2000. 
B.Schmitz. Pragmatikbasicrtcs Masch, inellcs 
Dolmctschcn. \])isscrtation, FB Informatik, 
TU Berlin, 1997. 
B.S(:hmitz, J.J.Quantz. Diah)guc Acts in Auto- 
matic Dialoquc Interpreting in Proceedings of
the Sixth inl;crll~l;ion~l Conference OlI rlThco- 
rcticM and McthodologicM Issues in Machine 
'lYanslation (TMI-95), Lcuven, 1995. 
W.Wahlster, Ed. Vcrbmobih ~bundations of 
Speech, to Speech, ~}'anslation Springer Vcrlag, 
2000. 
C.Worm, C.J.l:l,ut)t). Towards Robust Uudcr- 
standing of Spccch by Combination of Partial 
Analyses Proceedings of ECAI 1998 
849 
