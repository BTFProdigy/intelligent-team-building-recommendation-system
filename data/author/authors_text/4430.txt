Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 161?168, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Local Phrase Reordering Models for Statistical Machine Translation
Shankar Kumar, William Byrne?
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD 21218, U.S.A.
Machine Intelligence Lab, Cambridge University Engineering Department,
Trumpington Street, Cambridge CB2 1PZ, U.K.
skumar@jhu.edu , wjb31@cam.ac.uk
Abstract
We describe stochastic models of local
phrase movement that can be incorpo-
rated into a Statistical Machine Transla-
tion (SMT) system. These models pro-
vide properly formulated, non-deficient,
probability distributions over reordered
phrase sequences. They are imple-
mented by Weighted Finite State Trans-
ducers. We describe EM-style parameter
re-estimation procedures based on phrase
alignment under the complete translation
model incorporating reordering. Our ex-
periments show that the reordering model
yields substantial improvements in trans-
lation performance on Arabic-to-English
and Chinese-to-English MT tasks. We
also show that the procedure scales as the
bitext size is increased.
1 Introduction
Word and Phrase Reordering is a crucial component
of Statistical Machine Translation (SMT) systems.
However allowing reordering in translation is com-
putationally expensive and in some cases even prov-
ably NP-complete (Knight, 1999). Therefore any
translation scheme that incorporates reordering must
necessarily balance model complexity against the
ability to realize the model without approximation.
In this paper our goal is to formulate models of lo-
cal phrase reordering in such a way that they can be
embedded inside a generative phrase-based model
? This work was supported by an ONR MURI Grant
N00014-01-1-0685.
of translation (Kumar et al, 2005). Although this
model of reordering is somewhat limited and can-
not capture all possible phrase movement, it forms
a proper parameterized probability distribution over
reorderings of phrase sequences. We show that with
this model it is possible to perform Maximum A
Posteriori (MAP) decoding (with pruning) and Ex-
pectation Maximization (EM) style re-estimation of
model parameters over large bitext collections.
We now discuss prior work on word and phrase
reordering in translation. We focus on SMT systems
that do not require phrases to form syntactic con-
stituents.
The IBM translation models (Brown et al, 1993)
describe word reordering via a distortion model de-
fined over word positions within sentence pairs. The
Alignment Template Model (Och et al, 1999) uses
phrases rather than words as the basis for transla-
tion, and defines movement at the level of phrases.
Phrase reordering is modeled as a first order Markov
process with a single parameter that controls the de-
gree of movement.
Our current work is inspired by the block
(phrase-pair) orientation model introduced by Till-
mann (2004) in which reordering allows neighbor-
ing blocks to swap. This is described as a sequence
of orientations (left, right, neutral) relative to the
monotone block order. Model parameters are block-
specific and estimated over word aligned trained bi-
text using simple heuristics.
Other researchers (Vogel, 2003; Zens and Ney,
2003; Zens et al, 2004) have reported performance
gains in translation by allowing deviations from
monotone word and phrase order. In these cases,
161
0c 4c 5c
0d 1d
1v 2v 3v 4v 5v 6v 7v
1f 2f 3f 4f 5f 6f 7f 8f 9f
2d 3d 4d 5d
2c 3c1c
x 1 x 2 x 3 x 4 x 5
1e 5e 7e2e 3e 4e 6e 9e8e
u 1 u 2 u 3 u 4 u 5
y 1 y 5y 4y 3y 2
doivent de_25_%exportationsgrains fl?chir
exportations grains de_25_%doivent fl?chir
1 
les exportations de 
les  exportations  de  grains  doivent  fl?chir de  25  %
grains doivent fl?chir de_25_%
1.exportations doiventgrains fl?chir de_25_%
grain exports are_projected_to by_25_%
grain  exports  are  projected  to  fall  by  25  %Sentence
fall
Source Language
Target Language Sentence
Figure 1: TTM generative translation process; here,
I = 9,K = 5, R = 7, J = 9.
reordering is not governed by an explicit probabilis-
tic model over reordered phrases; a language model
is employed to select the translation hypothesis. We
also note the prior work of Wu (1996), closely re-
lated to Tillmann?s model.
2 The WFST Reordering Model
The Translation Template Model (TTM) is a genera-
tive model of phrase-based translation (Brown et al,
1993). Bitext is described via a stochastic process
that generates source (English) sentences and trans-
forms them into target (French) sentences (Fig 1 and
Eqn 1).
P (fJ1 , v
R
1 , d
K
0 , c
K
0 , y
K
1 , x
K
1 , u
K
1 ,K, e
I
1) =
P (eI1)?
Source Language Model G
P (uK1 ,K|e
I
1)?
Source Phrase Segmentation W
P (xK1 |u
K
1 ,K, e
I
1)?
Phrase Translation and Reordering R
P (vR1 , d
K
0 , c
K
0 , y
K
1 |x
K
1 , u
K
1 ,K, e
I
1)?
Target Phrase Insertion ?
P (fJ1 |v
R
1 , d
K
0 , c
K
0 , y
K
1 , x
K
1 , u
K
1 ,K, e
I
1)
Target Phrase Segmentation ?
(1)
The TTM relies on a Phrase-Pair Inventory (PPI)
consisting of target language phrases and their
source language translations. Translation is mod-
eled via component distributions realized as WFSTs
(Fig 1 and Eqn 1) : Source Language Model (G),
Source Phrase Segmentation (W ), Phrase Transla-
tion and Reordering (R), Target Phrase Insertion
(?), and Target Phrase Segmentation (?) (Kumar et
al., 2005).
TTM Reordering Previously, the TTM was for-
mulated with reordering prior to translation; here,
we perform reordering of phrase sequences follow-
ing translation. Reordering prior to translation was
found to be memory intensive and unwieldy (Kumar
et al, 2005). In contrast, we will show that the cur-
rent model can be used for both phrase alignment
and translation.
2.1 The Phrase Reordering Model
We now describe two WFSTs that allow local re-
ordering within phrase sequences. The simplest al-
lows swapping of adjacent phrases. The second al-
lows phrase movement within a three phrase win-
dow. Our formulation ensures that the overall model
provides a proper parameterized probability distri-
bution over reordered phrase sequences; we empha-
size that the resulting distribution is not degenerate.
Phrase reordering (Fig 2) takes as its input a
French phrase sequence in English phrase order
x1, x2, ..., xK . This is then reordered into French
phrase order y1, y2, ..., yK . Note that words within
phrases are not affected.
We make the following conditional independence
assumption:
P (yK1 |x
K
1 , u
K
1 ,K, e
I
1) = P (y
K
1 |x
K
1 , u
K
1 ). (2)
Given an input phrase sequence xK1 we now as-
sociate a unique jump sequence bK1 with each per-
missible output phrase sequence yK1 . The jump bk
measures the displacement of the kth phrase xk, i.e.
xk ? yk+bk , k ? {1, 2, ...,K}. (3)
The jump sequence bK1 is constructed such that yK1
is a permutation of xK1 . This is enforced by con-
structing all models so that
?K
k=1 bk = 0.
We now redefine the model in terms of the jump
sequence
P (yK1 |x
K
1 , u
K
1 ) (4)
=
{
P (bK1 |x
K
1 , u
K
1 ) yk+bk = xk ?k
0 otherwise,
162
x 2 x 3 x 4 x 5x 1
y 2 y 3 y 4 y 5y 1
3b = 01b = +12b = ?1 4b = 0 5b = 0
doivent de_25_%exportations fl?chir
exportations grains de_25_%doivent fl?chir
grains
Figure 2: Phrase reordering and jump sequence.
-
where yK1 is determined by xK1 and bK1 .
Each jump bk depends on the phrase-pair (xk, uk)
and preceding jumps bk?11
P (bK1 |x
K
1 , u
K
1 ) =
K?
k=1
P (bk|xk, uk, ?k?1), (5)
where ?k?1 is an equivalence classification (state)
of the jump sequence bk?11 .
The jump sequence bK1 can be described by a
deterministic finite state machine. ?(bk?11 ) is the
state arrived at by bk?11 ; we will use ?k?1 to denote
?(bk?11 ).
We will investigate phrase reordering by restrict-
ing the maximum allowable jump to 1 phrase and
to 2 phrases; we will refer to these reordering
models as MJ-1 and MJ-2. In the first case,
bk ? {0,+1,?1} while in the second case, bk ?
{0,+1,?1,+2,?2}.
2.2 Reordering WFST for MJ-1
We first present the Finite State Machine of the
phrase reordering process (Fig 3) which has two
equivalence classes (FSM states) for any given his-
tory bk?11 ; ?(b
k?1
1 ) ? {1, 2}. A jump of +1 has to
be followed by a jump of ?1, and 1 is the start and
end state; this ensures
?K
k=1 bk = 0.
1 b=+1
b=?1
b=0 
2
Figure 3: Phrase reordering process for MJ-1.
Under this restriction, the probability of the jump
bk (Eqn 5) can be simplified as
P (bk|xk, uk, ?(b
k?1
1 )) = (6)?
??
??
?1(xk, uk) bk = +1, ?k?1 = 1
1 ? ?1(xk, uk) bk = 0, ?k?1 = 1
1 bk = ?1, ?k?1 = 2.
There is a single parameter jump probability
?1(x, u) = P (b = +1|x, u) associated with each
phrase-pair (x, u) in the phrase-pair inventory. This
is the probability that the phrase-pair (x, u) appears
out of order in the transformed phrase sequence.
We now describe the MJ-1 WFST. In the presen-
tation, we use upper-case letters to denote the En-
glish phrases (uk) and lower-case letters to denote
the French phrases (xk and yk).
The PPI for this example is given in Table 1.
English French Parameters
u x P (x|u) ?1(x, u)
A a 0.5 0.2
A d 0.5 0.2
B b 1.0 0.4
C c 1.0 0.3
D d 1.0 0.8
Table 1: Example phrase-pair inventory with trans-
lation and reordering probabilities.
The input to the WFST (Fig 4) is a lattice of
French phrase sequences derived from the French
sentence to be translated. The outputs are the cor-
responding English phrase sequences. Note that the
reordering is performed on the English side.
The WFST is constructed by adding a self-loop
for each French phrase in the input lattice, and
a 2-arc path for every pair of adjacent French
phrases in the lattice. The WFST incorporates the
translation model P (x|u) and the reordering model
P (b|x, u). The score on a self-loop with labels
(u, x) is P (x|u) ? (1 ? ?1(x, u)); on a 2-arc path
with labels (u1, x1) and (u2, x2), the score on the
1st arc is P (x2|u1) ? ?1(x2, u1) and on the 2nd arc
is P (x1|u2).
In this example, the input to this transducer is a
single French phrase sequence V : a, b, c. We per-
form the WFST composition R?V , project the result
on the input labels, and remove the epsilons to form
the acceptor (R?V )1 which contains the six English
phrase sequences (Fig 4).
Translation Given a French sentence, a lattice of
translations is obtained using the weighted finite
state composition: T = G ? W ? R ? ? ? ? ? T .
The most-likely translation is obtained as the path
with the highest probability in T .
Alignment Given a sentence-pair (E,F ), a lattice
of phrase alignments is obtained by the finite state
composition: B = S ? W ? R ? ? ? ? ? T , where
163
A : b / 0.1
A B D 0.4 x 0.6 x 0.2 = 0.480
B A D 0.4 x 0.5 x 0.2 = 0.040
A D B 0.4 x 0.8 x 0.4 = 0.128
A A B 0.4 x 0.1 x 0.4 = 0.016
A B A 0.4 x 0.6 x 0.4 = 0.096
B A A 0.4 x 0.5 x 0.4 = 0.080
VR 1( )
A : b / 0.5
R
V a b d
B : b / 0.6D : d / 0.2
A : d / 0.4A : a / 0.4
B : a / 0.4
B : d / 0.4
D : b / 0.8
Figure 4: WFST for the MJ-1 model.
S is an acceptor for the English sentence E, and
T is an acceptor for the French sentence F . The
Viterbi alignment is found as the path with the high-
est probability in B. The WFST composition gives
the word-to-word alignments between the sentences.
However, to obtain the phrase alignments, we need
to construct additional FSTs not described here.
2.3 Reordering WFST for MJ-2
MJ-2 reordering restricts the maximum allowable
jump to 2 phrases and also insists that the reorder-
ing take place within a window of 3 phrases. This
latter condition implies that for an input sequence
{a, b, c, d}, we disallow the three output sequences:
{b, d, a, c; c, a, d, b; c, d, a, b; }. In the MJ-2 finite
state machine, a given history bk?11 can lead to one
of the six states in Fig 5.
b=0
1
23
45
6
b=?1
b=+1b=?1
b=+2
b=0
b=?2
b=?1
b=+1 b=?2
Figure 5: Phrase reordering process for MJ-2.
The jump probability of Eqn 5 becomes
P (bk|xk, uk, ?k?1) =
?
????
????
?1(xk, uk) bk = 1, ?k?1 = 1
?2(xk, uk) bk = 2, ?k?1 = 1{
1 ? ?1(xk, uk)
??2(xk, uk)
bk = 0, ?k?1 = 1
(7)
{
?1(xk, uk) bk = 1, ?k?1 = 2
1 ? ?1(xk, uk) bk = ?1, ?k?1 = 2
(8)
{
0.5 bk = 0, ?k?1 = 3
0.5 bk = ?1, ?k?1 = 3.
(9)
{
1 bk = ?2, ?k?1 = 4 (10)
{
1 bk = ?2, ?k?1 = 5 (11)
{
1 bk = ?1, ?k?1 = 6 (12)
We note that the distributions (Eqns 7 and 8) are
based on two parameters ?1(x, u) and ?2(x, u) for
each phrase-pair (x, u).
Suppose the input is a phrase sequence a, b, c, the
MJ-2 model (Fig 5) allows 6 possible reorderings:
a, b, c; a, c, b; b, a, c; b, c, a; c, a, b; c, b, a. The distri-
bution Eqn 9 ensures that the sequences b, c, a and
c, b, a are assigned equal probability. The distribu-
tions in Eqns 10-12 ensure that the maximum jump
is 2 phrases and the reordering happens within a
window of 3 phrases. By insisting that the pro-
cess start and end at state 1 (Fig 5), we ensure that
the model is not deficient. A WFST implementing
the MJ-2 model can be easily constructed for both
phrase alignment and translation, following the con-
struction described for the MJ-1 model.
3 Estimation of the Reordering Models
The Translation Template Model relies on an in-
ventory of target language phrases and their source
language translations. Our goal is to estimate the
reordering model parameters P (b|x, u) for each
phrase-pair (x, u) in this inventory. However, when
translating a given test set, only a subset of the
phrase-pairs is needed. Although there may be an
advantage in estimating the model parameters under
an inventory that covers all the training bitext, we fix
the phrase-pair inventory to cover only the phrases
on the test set. Estimation of the reordering model
parameters over the training bitext is then performed
under this test-set specific inventory.
164
We employ the EM algorithm to obtain Maximum
Likelihood (ML) estimates of the reordering model
parameters. Applying EM to the MJ-1 reordering
model gives the following ML parameter estimates
for each phrase-pair (u, x).
??1(x, u) =
Cx,u(0,+1)
Cx,u(0,+1) + Cx,u(0, 0)
. (13)
Cx,u(?, b) is defined for ? = 1, 2 and b =
?1, 0,+1. Any permissible phrase alignment of a
sentence pair corresponds to a bK1 sequence, which
in turn specifies a ?K1 sequence. Cx,u(?, b) is the
expected number of times the phrase-pair x, u is
aligned with a jump of b phrases when the jump his-
tory is ?. We do not use full EM but a Viterbi train-
ing procedure that obtains the counts for the best
(Viterbi) alignments. If a phrase-pair (x, u) is never
seen in the Viterbi alignments, we back-off to a flat
parameter ?1(x, u) = 0.05.
The ML parameter estimates for the MJ-2 model
are given in Table 2, with Cx,u(?, b) defined sim-
ilarly. In our training scenario, we use WFST op-
erations to obtain Viterbi phrase alignments of the
training bitext where the initial reordering model
parameters (?0(x, u)) are set to a uniform value of
0.05. The counts Cx,u(s, b) are then obtained over
the phrase alignments. Finally the ML estimates of
the parameters are computed using Eqn 13 (MJ-1) or
Eqn 14 (MJ-2). We will refer to the Viterbi trained
models as MJ-1 VT and MJ-2 VT. Table 3 shows the
MJ-1 VT parameters for some example phrase-pairs
in the Arabic-English (A-E) task.
u x ?1(x, u)
which is the closest Aqrb 1.0
international trade tjArp EAlmyp 0.8
the foreign ministry wzArp xArjyp 0.6
arab league jAmEp dwl Erbyp 0.4
Table 3: MJ-1 parameters for A-E phrase-pairs.
To validate alignment under a PPI, we mea-
sure performance of the TTM word alignments
on French-English (500 sent-pairs) and Chinese-
English (124 sent-pairs) (Table 4). As desired, the
Alignment Recall (AR) and Alignment Error Rate
(AER) improve modestly while Alignment Preci-
sion (AP) remains constant. This suggests that the
models allow more words to be aligned and thus im-
prove the recall; MJ-2 gives a further improvement
in AR and AER relative to MJ-1. Alignment preci-
Reordering Metrics (%)
Frn-Eng Chn-Eng
AP AR AER AP AR AER
None 94.2 84.8 10.0 85.1 47.1 39.3
MJ-1 VT 94.1 86.8 9.1 85.3 49.4 37.5
MJ-2 VT 93.9 87.4 8.9 85.3 50.9 36.3
Table 4: Alignment Performance with Reordering.
sion depends on the quality of the word alignments
within the phrase-pairs and does not change much
by allowing phrase reordering. This experiment val-
idates the estimation procedure based on the phrase
alignments; however, we do not advocate the use of
TTM as an alternate word alignment technique.
4 Translation Experiments
We perform our translation experiments on the large
data track of the NIST Arabic-to-English (A-E) and
Chinese-to-English (C-E) MT tasks; we report re-
sults on the NIST 2002, 2003, and 2004 evaluation
test sets 1.
4.1 Exploratory Experiments
In these experiments the training data is restricted to
FBIS bitext in C-E and the news bitexts in A-E. The
bitext consists of chunk pairs aligned at sentence
and sub-sentence level (Deng et al, 2004). In A-E,
the training bitext consists of 3.8M English words,
3.2M Arabic words and 137K chunk pairs. In C-E,
the training bitext consists of 11.7M English words,
8.9M Chinese words and 674K chunk pairs.
Our Chinese text processing consists of word seg-
mentation (using the LDC segmenter) followed by
grouping of numbers. For Arabic our text pro-
cessing consisted of a modified Buckwalter analysis
(LDC2002L49) followed by post processing to sep-
arate conjunctions, prepostions and pronouns, and
Al-/w- deletion. The English text is processed us-
ing a simple tokenizer based on the text processing
utility available in the the NIST MT-eval toolkit.
The Language Model (LM) training data consists
of approximately 400M words of English text de-
rived from Xinhua and AFP (English Gigaword), the
English side of FBIS, the UN and A-E News texts,
and the online archives of The People?s Daily.
Table 5 gives the performance of the MJ-1 and
MJ-2 reordering models when translation is per-
formed using a 4-gram LM. We report performance
on the 02, 03, 04 test sets and the combined test set
1http://www.nist.gov/speech/tests/mt/
165
??1(x, u) =
Cx,u(1,+1) + Cx,u(2,+1)
Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1)
??2(x, u) =
(Cx,u(1, 0) + Cx,u(2,?1) + Cx,u(1,+2))Cx,u(1,+2)
(Cx,u(1,+1) + Cx,u(1, 0) + Cx,u(1,+2) + Cx,u(2,+1) + Cx,u(2,?1))(Cx,u(1,+2) + Cx,u(1, 0))
Table 2: ML parameter estimates for MJ-2 model.
Reordering BLEU (%)
Arabic-English Chinese-English
02 03 04 ALL 02 03 04 ALL
None 37.5 40.3 36.8 37.8 ? 0.6 24.2 23.7 26.0 25.0 ? 0.5
MJ-1 flat 40.4 43.9 39.4 40.7 ? 0.6 25.7 24.5 27.4 26.2 ? 0.5
MJ-1 VT 41.3 44.8 40.3 41.6 ? 0.6 25.8 24.5 27.8 26.5 ? 0.5
MJ-2 flat 41.0 44.4 39.7 41.1 ? 0.6 26.4 24.9 27.7 26.7 ? 0.5
MJ-2 VT 41.7 45.3 40.6 42.0 ? 0.6 26.5 24.9 27.9 26.8 ? 0.5
Table 5: Performance of MJ-1 and MJ-2 reordering models with a 4-gram LM.
(ALL=02+03+04). For the combined set (ALL), we
also show the 95% BLEU confidence interval com-
puted using bootstrap resampling (Och, 2003).
Row 1 gives the performance when no reorder-
ing model is used. The next two rows show the in-
fluence of the MJ-1 reordering model; in row 2, a
flat probability of ?1(x, u) = 0.05 is used for all
phrase-pairs; in row 3, a reordering probability is
estimated for each phrase-pair using Viterbi Train-
ing (Eqn 13). The last two rows show the effect of
the MJ-2 reordering model; row 4 uses flat proba-
bilities (?1(x, u) = 0.05, ?2(x, u) = 0.01) for all
phrase-pairs; row 5 applies reordering probabilities
estimating with Viterbi Training for each phrase-pair
(Table 2).
On both language-pairs, we observe that reorder-
ing yields significant improvements. The gains from
phrase reordering are much higher on A-E relative
to C-E; this could be related to the fact that the word
order differences between English and Arabic are
much higher than the differences between English
and Chinese. MJ-1 VT outperforms flat MJ-1 show-
ing that there is value in estimating the reordering
parameters from bitext. Finally, the MJ-2 VT model
performs better than the flat MJ-2 model, but only
marginally better than the MJ-1 VT model. There-
fore estimation does improve the MJ-2 model but
allowing reordering beyond a window of 1 phrase is
not useful when translating either Arabic or Chinese
into English in this framework.
The flat MJ-1 model outperforms the no-
reordering case and the flat MJ-2 model is better
than the flat MJ-1 model; we hypothesize that phrase
reordering increases search space of translations that
allows the language model to select a higher qual-
ity hypothesis. This suggests that these models of
phrase reordering actually require strong language
models to be effective. We now investigate the inter-
action between language models and reordering.
Our goal here is to measure translation perfor-
mance of reordering models over variable span n-
gram LMs (Table 6). We observe that both MJ-1
and MJ-2 models yield higher improvements under
higher order LMs: e.g. on A-E, gains under 3g
(3.6 BLEU points on MJ-1, 0.2 points on MJ-2) are
higher than the gains with 2g (2.4 BLEU points on
MJ-1, 0.1 points on MJ-2).
Reordering BLEU (%)
A-E C-E
2g 3g 4g 2g 3g 4g
None 21.0 36.8 37.8 16.1 24.8 25.0
MJ-1 VT 23.4 40.4 41.6 16.2 25.9 26.5
MJ-2 VT 23.5 40.6 42.0 16.0 26.1 26.8
Table 6: Reordering with variable span n-gram LMs
on Eval02+03+04 set.
We now measure performance of the reorder-
ing models across the three test set genres used in
the NIST 2004 evaluation: news, editorials, and
speeches. On A-E, MJ-1 and MJ-2 yield larger im-
provements on News relative to the other genres;
on C-E, the gains are larger on Speeches and Ed-
itorials relative to News. We hypothesize that the
Phrase-Pair Inventory, reordering models and lan-
guage models could all have been biased away from
the test set due to the training data. There may also
be less movement across these other genres.
166
Reordering BLEU (%)
A-E C-E
News Eds Sphs News Eds Sphs
None 41.1 30.8 33.3 23.6 25.9 30.8
MJ-1 VT 45.6 32.6 35.7 24.8 27.8 33.3
MJ-2 VT 46.2 32.7 35.5 24.8 27.8 33.7
Table 7: Performance across Eval 04 test genres.
BLEU (%)
Arabic-English Chinese-English
Reordering 02 03 04n 02 03 04n
None 40.2 42.3 43.3 28.9 27.4 27.3
MJ-1 VT 43.1 45.0 45.6 30.2 28.2 28.9
MET-Basic 44.8 47.2 48.2 31.3 30.3 30.3
MET-IBM1 45.2 48.2 49.7 31.8 30.7 31.0
Table 8: Translation Performance on Large Bitexts.
4.2 Scaling to Large Bitext Training Sets
We here describe the integration of the phrase re-
ordering model in an MT system trained on large
bitexts. The text processing and language mod-
els have been described in ? 4.1. Alignment Mod-
els are trained on all available bitext (7.6M chunk
pairs/207.4M English words/175.7M Chinese words
on C-E and 5.1M chunk pairs/132.6M English
words/123.0M Arabic words on A-E), and word
alignments are obtained over the bitext. Phrase-pairs
are then extracted from the word alignments (Koehn
et al, 2003). MJ-1 model parameters are estimated
over all bitext on A-E and over the non-UN bitext
on C-E. Finally we use Minimum Error Training
(MET) (Och, 2003) to train log-linear scaling fac-
tors that are applied to the WFSTs in Equation 1.
04news (04n) is used as the MET training set.
Table 8 reports the performance of the system.
Row 1 gives the performance without phrase re-
ordering and Row 2 shows the effect of the MJ-1
VT model. The MJ-1 VT model is used in an initial
decoding pass with the four-gram LM to generate
translation lattices. These lattices are then rescored
under parameters obtained using MET (MET-basic),
and 1000-best lists are generated. The 1000-best
lists are augmented with IBM Model-1 (Brown et
al., 1993) scores and then rescored with a second set
of MET parameters. Rows 3 and 4 show the perfor-
mance of the MET-basic and MET-IBM1 models.
We observe that the maximum likelihood phrase
reordering model (MJ-1 VT) yields significantly im-
proved translation performance relative to the mono-
tone phrase order translation baseline. This confirms
the translation performance improvements found
over smaller training bitexts.
We also find additional gains by applying MET to
optimize the scaling parameters that are applied to
the WFST component distributions within the TTM
(Equation 1). In this procedure, the scale factor ap-
plied to the MJ-1 VT Phrase Translation and Re-
ordering component is estimated along with scale
factors applied to the other model components; in
other words, the ML-estimated phrase reordering
model itself is not affected by MET, but the likeli-
hood that it assigns to a phrase sequence is scaled
by a single, discriminatively optimized weight. The
improvements from MET (see rows MET-Basic and
MET- IBM1) demonstrate that the MJ-1 VT reorder-
ing models can be incorporated within a discrimi-
native optimized translation system incorporating a
variety of models and estimation procedures.
5 Discussion
In this paper we have described local phrase reorder-
ing models developed for use in statistical machine
translation. The models are carefully formulated
so that they can be implemented as WFSTs, and
we show how the models can be incorporated into
the Translation Template Model to perform phrase
alignment and translation using standard WFST op-
erations. Previous approaches to WFST-based re-
ordering (Knight and Al-Onaizan, 1998; Kumar
and Byrne, 2003; Tsukada and Nagata, 2004) con-
structed permutation acceptors whose state spaces
grow exponentially with the length of the sentence to
be translated. As a result, these acceptors have to be
pruned heavily for use in translation. In contrast, our
models of local phrase movement do not grow ex-
plosively and do not require any pruning or approx-
imation in their construction. In other related work,
Bangalore and Ricardi (2001) have trained WF-
STs for modeling reordering within translation; their
WFST parses word sequences into trees containing
reordering information, which are then checked for
well-formed brackets. Unlike this approach, our
model formulation does not use a tree representation
and also ensures that the output sequences are valid
permutations of input phrase sequences; we empha-
size again that the probability distribution induced
over reordered phrase sequences is not degenerate.
Our reordering models do resemble those of (Till-
mann, 2004; Tillmann and Zhang, 2005) in that we
167
treat the reordering as a sequence of jumps relative
to the original phrase sequence, and that the likeli-
hood of the reordering is assigned through phrase-
pair specific parameterized models. We note that
our implementation allows phrase reordering be-
yond simply a 1-phrase window, as was done by Till-
mann. More importantly, our model implements a
generative model of phrase reordering which can be
incorporated directly into a generative model of the
overall translation process. This allows us to per-
form ?embedded? EM-style parameter estimation,
in which the parameters of the phrase reordering
model are estimated using statistics gathered under
the complete model that will actually be used in
translation. We believe that this estimation of model
parameters directly from phrase alignments obtained
under the phrase translation model is a novel contri-
bution; prior approaches derived the parameters of
the reordering models from word aligned bitext, e.g.
within the phrase pair extraction procedure.
We have shown that these models yield improve-
ments in alignment and translation performance on
Arabic-English and Chinese-English tasks, and that
the reordering model can be integrated into large
evaluation systems. Our experiments show that dis-
criminative training procedures such Minimum Er-
ror Training also yield additive improvements by
tuning TTM systems which incorporate ML-trained
reordering models. This is essential for integrating
our reordering model inside an evaluation system,
where a variety of techniques are applied simultane-
ously.
The MJ-1 and MJ-2 models are extremely sim-
ple models of phrase reordering. Despite their sim-
plicity, these models provide large improvements
in BLEU score when incorporated into a monotone
phrase order translation system. Moreover, they
can be used to produced translation lattices for use
by more sophisticated reordering models that allow
longer phrase order movement. Future work will
build on these simple structures to produce more
powerful models of word and phrase movement in
translation.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Y. Deng, S. Kumar, and W. Byrne. 2004. Bitext chunk
alignment for statistical machine translation. In Re-
search Note, Center for Language and Speech Pro-
cessing, Johns Hopkins University.
K. Knight and Y. Al-Onaizan. 1998. Translation
with finite-state devices. In AMTA, pages 421?437,
Langhorne, PA, USA.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs & Discussion, 25(4).
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 127?133,
Edmonton, Canada.
S. Kumar and W. Byrne. 2003. A weighted finite state
transducer implementation of the alignment template
model for statistical machine translation. In HLT-
NAACL, pages 142?149, Edmonton, Canada.
S. Kumar, Y. Deng, and W. Byrne. 2005. A weighted fi-
nite state transducer translation template model for sta-
tistical machine translation. Journal of Natural Lan-
guage Engineering, 11(4).
F. Och, C. Tillmann, and H. Ney. 1999. Improved align-
ment models for statistical machine translation. In
EMNLP-VLC, pages 20?28, College Park, MD, USA.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, Sapporo, Japan.
C. Tillmann and T. Zhang. 2005. A localized prediction
model for statistical machine translation. In ACL, Ann
Arbor, Michigan, USA.
C. Tillmann. 2004. A block orientation model for sta-
tistical machine translation. In HLT-NAACL, Boston,
MA, USA.
H. Tsukada and M. Nagata. 2004. Efficient decoding for
statistical machine translation with a fully expanded
WFST model. In EMNLP, Barcelona, Spain.
S. Vogel. 2003. SMT Decoder Dissected: Word Reorder-
ing. In NLPKE, Beijing, China.
D. Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In ACL, pages 152?158,
Santa Cruz, CA, USA.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In ACL, pages 144?151, Sapporo, Japan.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering constraints for phrase-based statistical ma-
chine translation. In COLING, pages 205?211,
Boston, MA, USA.
168
A Weighted Finite State Transducer Implementation of the Alignment
Template Model for Statistical Machine Translation
Shankar Kumar and William Byrne
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA
 
skumar,byrne  @jhu.edu
Abstract
We present a derivation of the alignment tem-
plate model for statistical machine translation
and an implementation of the model using
weighted finite state transducers. The approach
we describe allows us to implement each con-
stituent distribution of the model as a weighted
finite state transducer or acceptor. We show
that bitext word alignment and translation un-
der the model can be performed with standard
FSM operations involving these transducers.
One of the benefits of using this framework
is that it obviates the need to develop special-
ized search procedures, even for the generation
of lattices or N-Best lists of bitext word align-
ments and translation hypotheses. We evaluate
the implementation of the model on the French-
to-English Hansards task and report alignment
and translation performance.
1 Introduction
The Alignment Template Translation Model
(ATTM) (Och et al, 1999) has emerged as a promising
modeling framework for statistical machine translation.
The ATTM attempts to overcome the deficiencies of
word-to-word translation models (Brown et al, 1993)
through the use of phrasal translations. The overall
model is based on a two-level alignment between the
source and the target sentence: a phrase-level alignment
between source and target phrases and a word-level
alignment between words in these phrase pairs.
The goal of this paper is to reformulate the ATTM
so that the operations we intend to perform under a sta-
tistical translation model, namely bitext word alignment
and translation, can be implementation using standard
weighted finite state transducer (WFST) operations. Our
main motivation for a WFST modeling framework lies
in the resulting simplicity of alignment and translation
processes compared to dynamic programming or  de-
coders. The WFST implementation allows us to use stan-
dard optimized algorithms available from an off-the-shelf
FSM toolkit (Mohri et al, 1997). This avoids the need to
develop specialized search procedures, even for the gen-
TEMPLATE
SEQUENCE
MODEL
PERMUTATION
MODEL
PHRASE
PHRASAL
TRANSLATION
MODEL
TARGET
LANGUAGE MODEL
v 2 v 31v
SOURCE
SEGMENTATION
MODEL
u
z
 
TARGET LANGUAGE SENTENCE 
SENTENCESOURCE LANGUAGE
source language phrases
alignment templates
target language phrases
f f f ff f2 3 4 5 6 f7
v 2 1v v 3
z z1 2 3
u u1 2
e1 e2
1
e4 e5 e6 eee3e
3
7 8 9
a aa 2 31
Figure 1: ATTM Architecture.
eration of lattices or N-best lists of bitext word alignment
or translation hypotheses.
Weighted Finite State Transducers for Statistical Ma-
chine Translation (SMT) have been proposed in the
literature to implement word-to-word translation mod-
els (Knight and Al-Onaizan, 1998) or to perform trans-
lation in an application domain such as the call routing
task (Bangalore and Ricardi, 2001). One of the objec-
tives of these approaches has been to provide an imple-
mentation for SMT that uses standard FSM algorithms
to perform model computations and therefore make SMT
techniques accessible to a wider community. Our WFST
implementation of the ATTM has been developed with
similar objectives.
We start off by presenting a derivation of the ATTM
that identifies the conditional independence assumptions
that underly the model. The derivation allows us to spec-
ify each component distribution of the model and imple-
ment it as a weighted finite state transducer. We then
show that bitext word alignment and translation can be
performed with standard FSM operations involving these
transducers. Finally we report bitext word alignment
and translation performance of the implementation on the
Canadian French-to-English Hansards task.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 63-70
                                                         Proceedings of HLT-NAACL 2003
2 Alignment Template Translation Models
We present here a derivation of the alignment template
translation model (ATTM) (Och et al, 1999; Och, 2002)
and give an implementation of the model using weighted
finite state transducers (WFSTs). The finite state model-
ing is performed using the AT&T FSM Toolkit (Mohri et
al., 1997).
In this model, the translation of a source language sen-
tence to a target language sentence is described by a joint
probability distribution over all possible segmentations
and alignments. This distribution is presented in Figure 1
and Equations 1-7. The components of the overall trans-
lation model are the source language model (Term 2),
the source segmentation model (Term 3), the phrase per-
mutation model (Term 4), the template sequence model
(Term 5), the phrasal translation model (Term 6) and the
target language model (Term 7). Each of these condi-
tional distributions is modeled independently and we now
define each in turn and present its implementation as a
weighted finite state acceptor or transducer.
 
	
A Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
Minimum Bayes-Risk Decoding for Statistical Machine Translation
Shankar Kumar and William Byrne  
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA

skumar,byrne  @jhu.edu
Abstract
We present Minimum Bayes-Risk (MBR) de-
coding for statistical machine translation. This
statistical approach aims to minimize expected
loss of translation errors under loss functions
that measure translation performance. We de-
scribe a hierarchy of loss functions that incor-
porate different levels of linguistic information
from word strings, word-to-word alignments
from an MT system, and syntactic structure
from parse-trees of source and target language
sentences. We report the performance of the
MBR decoders on a Chinese-to-English trans-
lation task. Our results show that MBR decod-
ing can be used to tune statistical MT perfor-
mance for specific loss functions.
1 Introduction
Statistical Machine Translation systems have achieved
considerable progress in recent years as seen from their
performance on international competitions in standard
evaluation tasks (NIST, 2003). This rapid progress has
been greatly facilitated by the development of automatic
translation evaluation metrics such as BLEU score (Pa-
pineni et al, 2001), NIST score (Doddington, 2002)
and Position Independent Word Error Rate (PER) (Och,
2002). However, given the many factors that influence
translation quality, it is unlikely that we will find a single
translation metric that will be able to judge all these fac-
tors. For example, the BLEU, NIST and the PER metrics,

This work was supported by the National Science Foun-
dation under Grant No. 0121285 and an ONR MURI Grant
N00014-01-1-0685. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of the National
Science Foundation or the Office of Naval Research.
though effective, do not take into account explicit syntac-
tic information when measuring translation quality.
Given that different Machine Translation (MT) eval-
uation metrics are useful for capturing different aspects
of translation quality, it becomes desirable to create MT
systems tuned with respect to each individual criterion. In
contrast, the maximum likelihood techniques that under-
lie the decision processes of most current MT systems do
not take into account these application specific goals. We
apply the Minimum Bayes-Risk (MBR) techniques devel-
oped for automatic speech recognition (Goel and Byrne,
2000) and bitext word alignment for statistical MT (Ku-
mar and Byrne, 2002), to the problem of building au-
tomatic MT systems tuned for specific metrics. This is
a framework that can be used with statistical models of
speech and language to develop decision processes opti-
mized for specific loss functions.
We will show that MBR decoding can be applied to
machine translation in two scenarios. Given an automatic
MT metric, we design a loss function based on the met-
ric and use MBR decoding to tune MT performance un-
der the metric. We also show how MBR decoding can
be used to incorporate syntactic structure into a statistical
MT system by building specialized loss functions. These
loss functions can use information from word strings,
word-to-word alignments and parse-trees of the source
sentence and its translation. In particular we describe
the design of a Bilingual Tree Loss Function that can ex-
plicitly use syntactic structure for measuring translation
quality. MBR decoding under this loss function allows
us to integrate syntactic knowledge into a statistical MT
system without building detailed models of linguistic fea-
tures, and retraining the system from scratch.
We first present a hierarchy of loss functions for trans-
lation based on different levels of lexical and syntactic
information from source and target language sentences.
This hierarchy includes the loss functions useful in both
situations where we intend to apply MBR decoding. We
then present the MBR framework for statistical machine
translation under the various translation loss functions.
We finally report the performance of MBR decoders op-
timized for each loss function.
2 Translation Loss Functions
We now introduce translation loss functions to measure
the quality of automatically generated translations. Sup-
pose we have a sentence   in a source language for
which we have generated an automatic translation 
with word-to-word alignment  relative to   . The word-
to-word alignment  specifies the words in the source
sentence   that are aligned to each word in the transla-
tion  . We wish to compare this automatic translation
with a reference translation  with word-to-word align-
ment  relative to   .
We will now present a three-tier hierarchy of trans-
lation loss functions of the form 	


 
that measure 
 against 
 . These loss func-
tions will make use of different levels of information from
word strings, MT alignments and syntactic structure from
parse-trees of both the source and target strings as illus-
trated in the following table.
Loss Function Functional Form
Lexical 
Target Language Parse-Tree Minimum Bayes-Risk Word Alignments of Bilingual Texts
Shankar Kumar and William Byrne
Center for Language and Speech Processing, Johns Hopkins University,
3400 North Charles Street, Baltimore, MD, 21218, USA
skumar,byrne @jhu.edu
Abstract
We present Minimum Bayes-Risk word
alignment for machine translation. This
statistical, model-based approach attempts
to minimize the expected risk of align-
ment errors under loss functions that mea-
sure alignment quality. We describe var-
ious loss functions, including some that
incorporate linguistic analysis as can be
obtained from parse trees, and show that
these approaches can improve alignments
of the English-French Hansards.
1 Introduction
The automatic determination of word alignments in
bilingual corpora would be useful for Natural Lan-
guage Processing tasks such as statistical machine
translation, automatic dictionary construction, and
multilingual document retrieval. The development
of techniques in all these areas would be facili-
tated by automatic performance metrics, and align-
ment and translation quality metrics have been pro-
posed (Och and Ney, 2000b; Papineni et al, 2002).
However, given the difficulty of judging translation
quality, it is unlikely that a single, global metric will
be found for any of these tasks. It is more likely
that specialized metrics will be developed to mea-
sure specific aspects of system performance. This is
even desirable, as these specialized metrics could be
used in tuning systems for particular applications.
We have applied Minimum Bayes-Risk (MBR)
procedures developed for automatic speech recog-
nition (Goel and Byrne, 2000) to word alignment of
bitexts. This is a modeling approach that can be used
with statistical models of speech and language to de-
velop algorithms that are optimized for specific loss
functions. We will discuss loss functions that can
be used for word alignment and show how the over-
all alignment process can be improved by the use
of loss functions that incorporate linguistic features,
such as parses and part-of-speech tags.
2 Word-to-Word Bitext Alignment
We will study the problem of aligning an English
sentence to a French sentence and we will use the
word alignment of the IBM statistical translation
models (Brown et al, 1993).
Let and denote a pair of translated
English and French sentences. An English word is
defined as an ordered pair
, where the index refers to the posi-
tion of the word in the English sentence; is the
vocabulary of English; and the word at position is
the NULL word to which ?spurious? French words
may be aligned. Similarly, a French word is written
as .
An alignment between and is defined to be
a sequence where
. Under the alignment
, the French word is connected to the English
word . For every alignment , we define a link
set defined as whose ele-
ments are given by the alignment links
.
3 Alignment Loss Functions
In this section we introduce loss functions to mea-
sure the quality of automatically produced align-
ments. Suppose we wish to compare an automat-
ically produced alignment to a reference align-
ment , which we assume was produced by a com-
petent translator. We will define various loss func-
tions that measure the quality of relative
to through their link sets and .
The desirable qualities in translation are fluency
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 140-147.
                         Proceedings of the Conference on Empirical Methods in Natural
and adequacy. We assume here that both word se-
quences are fluent and adequate translations but that
the word and phrase correspondences are unknown.
It is these correspondences that we wish to deter-
mine and evaluate automatically.
We now present two general classes of loss func-
tions that measure alignment quality. In subsequent
sections, we will give specific examples of these and
show how to construct decoders that are optimized
for each loss function.
3.1 Alignment Error
The Alignment Error Rate (AER) introduced by
Och and Ney (2000b) measures the fraction of links
by which the automatic alignment differs from the
reference alignment. Links to the NULL word are
ignored. This is done by defining modified link sets
for the reference alignment
and the automatic alignment
.
The reference annotation procedure allowed the
human transcribers to identify which links in they
judged to be unambiguous. In addition to the ref-
erence alignment, this gives a set of sure links (S)
which is a subset of .
AER is defined as (Och and Ney, 2000b)
(1)
Since our modeling techniques require loss func-
tions rather than error rates, we introduce the Align-
ment Error loss function
(2)
We consider error rates to be ?normalized? loss
functions. We also note that, unlike AER, does
not distinguish between ambiguous and unambigu-
ous links. However, if a decoder generates an align-
ment for which is zero, the AER is
also zero. Therefore if AER is the metric of inter-
est, we will design alignment procedures to mini-
mize .
3.2 Generalized Alignment Error
We are interested in extending the Alignment Er-
ror loss function to incorporate various linguistic
features into the measurement of alignment quality.
The Generalized Alignment Error loss is defined as
(3)
where and
(4)
Here we have introduced the word-to-word distance
measure which compares the
links and as a function of the words in
the translation. refers to all loss functions that
have the form of Equation 3. Specific loss functions
are determined through the choice of . To see the
value in this, suppose is a verb in the French sen-
tence and that it is aligned in the reference alignment
to , the verb in the English sentence. If our goal is
to ensure verb alignment, then can be constructed
to penalize any link in the automatic align-
ment in which is not a verb. We will later give ex-
amples of distances in which is based on Part-
of-Speech (POS) tags, parse tree distances, and au-
tomatically determined word clusters. We note that
the can almost be reduced to , except for
the treatment of NULL in the English sentence.
4 Minimum Bayes-Risk Decoding For
Automatic Word Alignment
We present the Minimum Bayes-Risk alignment for-
mulation and derive MBR alignment procedures un-
der the loss functions of Section 3.
Given a translated pair of English-French sen-
tences , the decoder produces an align-
ment . Relative to a reference align-
ment , the decoder performance is measured as
. Our goal is to find the decoder that
has the best performance over all translated sen-
tences. This is measured through the Bayes Risk
. The ex-
pectation is taken with respect to the true distribu-
tion that describes ?human quality? align-
ments of translations as they are found in bitext.
Given a loss function and a probability distribu-
tion, it is well known that the decision rule which
minimizes the Bayes Risk is given by the follow-
ing expression (Bickel and Doksum, 1977; Goel and
Byrne, 2000).
(5)
Several modeling assumptions have been made to
obtain this form of the decoder. We do not have ac-
cess to the true distribution over translations. We
therefore use statistical MT models to approximate
. We furthermore assume that the space of
alignment alternatives can be restricted to an align-
ment lattice , which is a compact representation of
the most likely word alignments of the sentence pair
under the baseline models.
It is clear from Equation 5 that the MBR de-
coder is determined by the loss function. The Sen-
tence Alignment Error refers to the loss function
that gives a penalty of 1 for any errorful alignment:
, where is the indi-
cator function of the set . The MBR decoder un-
der this loss can easily be seen to be the Maximum
Likelihood (ML) alignment under the MT models:
. This illustrates why we
are interested in MBR decoders based on other loss
functions: the ML decoder is optimal with respect to
a loss function that is overly harsh. It does not dis-
tinguish between different types of alignment errors
and good alignments receive the same penalty as
poor alignments. Moreover, such a harsh penalty is
particularly inappropriate when unambiguous word-
to-word alignments cannot be provided in all cases
even by human translators who produce the refer-
ence alignments. The AER makes an explicit dis-
tinction between ambiguous and unambiguous word
alignments. Ideally, the decoder should be able to do
so as well. Motivated by this, the MBR hypothesis
can be thought of as the consensus hypothesis un-
der a particular loss function: Equation 5 selects the
hypothesis that is, in an average sense, close to the
other likely hypotheses. In this way, ambiguity can
be reduced by selecting the hypothesis that is ?most
similar? to the collection of most likely competing
hypotheses.
We now describe the alignment lattice (Sec-
tion 4.1) and introduce the lattice based probabilities
required for the MBR alignment (Section 4.2). The
derivation of the MBR alignment under the AE and
GAE loss functions is presented in Sections 4.3 and
4.4.
4.1 Alignment Lattice
The lattice is represented as a Weighted Finite
State Transducer (WFST) (Mohri et al, 2000)
with a finite set of states , a set of
transition labels , an initial state , the set of fi-
nal states , and a finite set of transitions . A
transition in this WFST is given by
where is the starting state, is the ending state,
is the alignment link and is the weight. For
an English sentence of length and a French sen-
tence of length , we define as
.
A complete path through the WFST is a sequence
of transitions given by
such that and . Each complete
path defines an alignment link set .
When we write , we mean that is derived
from a complete path through . This allows us to
use alignment models in which the probability of an
alignment can be written as a sum over alignment
link weights, i.e. .
4.2 Alignment Link Posterior Probability
We first introduce the lattice transition posterior
probability of each transition in the
lattice
(6)
where is if and otherwise. The
lattice transition posterior probability is the sum of
the posterior probabilities of all lattice paths pass-
ing through the transition . This can be com-
puted very efficiently with a forward-backward al-
gorithm on the alignment lattice (Wessel et al,
1998). is the posterior probability of an
alignment link set which can be written as
(7)
We now define the alignment link posterior prob-
ability for a link
(8)
where . This is the probability
that any two words are aligned given all the
alignments in the lattice .
4.3 MBR Alignment Under
In this section we derive MBR alignment under the
Alignment Error loss function (Equation 2). The op-
timal decoder has the form (Equation 5)
(9)
The summation is equal to
If is the subset of transitions ( )
that do not contain links with the NULL word, we
can simplify the bracketed term as
For an alignment link we note that
. Therefore, the
MBR alignment (Equation 9) can be found in terms
of the modified link weight for each alignment link
(10)
We can rewrite the above equation as
(11)
4.4 MBR Alignment Under
We now derive MBR alignment under the Gener-
alized Alignment Error loss function (Equation 3).
The optimal decoder has the form (Equation 5)
(12)
The summation can be rewritten as
where and .
We can simplify the bracketed term as
where and .
The MBR alignment (Equation 12) can be found
in terms of the modified link weight for each align-
ment link
(13)
4.5 MBR Alignment Using WFST Techniques
The MBR alignment procedures under the and
loss functions begin with a WFST that con-
tains the alignment probabilities as de-
scribed in Section 4.1. To build the MBR decoder
for each loss function the weights on the transitions
( ) of the WFST are modified ac-
cording to either Equation 11 ( ) or Equa-
tion 13 ( ). Once the weights are modified,
the search procedure for the MBR alignment is the
same in each case. The search is carried out using a
shortest-path algorithm (Mohri et al, 2000).
5 Word Alignment Experiments
We present here examples of Generalized Align-
ment Error loss functions based on three types of
linguistic features and show how they can be incor-
porated into a statistical MT system to obtain auto-
matic alignments.
5.1 Syntactic Distances From Parse-Trees
Suppose a parser is available that generates a parse-
tree for the English sentence. Our goal is to con-
struct an alignment loss function that incorporates
features from the parse. One way to do this is to
define a graph distance
(14)
Here and are the parse-tree leaf nodes cor-
responding to the English words and . This
quantity is computed as the sum of the distances
from each node to their closest common ancestor.
It gives a syntactic distance between any pair of
English words based on the parse-tree. This dis-
tance has been used to measure word association for
information retrieval (Mittendorfer and Winiwarter,
2001). It reflects how strongly the words and
are bound together by the syntactic structure of the
English sentence as determined by the parser. Fig-
ure 1 shows the parse tree for an English sentence
in the test data with the pairwise syntactic distances
between the English words corresponding to the leaf
nodes.
TOP
S
NP
PRP i
VP
VBP think SBAR
S
NP
DT that
VP
VBZ is ADJP
JJ good . .
Pairwise Distances
g("i","think") = 4
g("i", "that") = 7
g("i","is") = 7
g("i" , "good") = 8
g("i" , ".") = 8
Figure 1: Parse tree for a English sentence with the
pairwise syntactic distances between words.
To obtain these distances, Ratnaparkhi?s part-
of-speech (POS) tagger (Ratnaparkhi, 1996) and
Collins? parser (Collins, 1999) were used to obtain
parse trees for the English side of the test corpus.
With defined as in Equation 14, the Generalized
Alignment Error loss function (Equation 3) is called
the Parse-Tree Syntactic Distance ( ).
5.2 Distances Derived From Part-of-Speech
Labels
Suppose a Part-of-Speech(POS) tagger is available
to tag each word in the English sentence. If POS
denotes the POS of the English word , we can de-
fine the word-to-word distance measure (Equa-
tion 4) as
POS POS (15)
Ratnaparkhi?s POS tagger (Ratnaparkhi, 1996)
was used to obtain POS tags for each word in
the English sentence. With specified by Equa-
tion 15, the Generalized Alignment Error loss func-
tion (Equation 3) is called the Part-Of-Speech Dis-
tance ( ).
5.3 Automatic Word Cluster Distances
Suppose we are working in a language for which
parsers and POS taggers are not available. In this
situation we might wish to construct the loss func-
tions based on word classes determined by auto-
matic clustering procedures. If specifies the
word cluster for the English word , then we define
the distance
(16)
In our experiments we obtained word clusters
for English words using a statistical learning proce-
dure (Kneser and Ney, 1991) where the total number
of word classes is restricted to be 100. With as
defined in Equation 16, the Generalized Alignment
Error loss function (Equation 3) is called the Auto-
matic Word Class Distance ( ).
5.4 IBM-3 Word Alignment Models
Since the true distribution over alignments is not
known, we used the IBM-3 statistical transla-
tion model (Brown et al, 1993) to approximate
. This model is specified through four
components: Fertility probabilities for words; Fer-
tility probabilities for NULL; Word Translation
probabilities; and Distortion probabilities. We
used a modified version of the IBM-3 distortion
model (Knight and Al-Onaizan, 1998) in which
each of the possible permutations of the French
sentence is equally likely. The IBM-3 models
were trained on a subset of the Canadian Hansards
French-English data which consisted of 50,000 par-
allel sentences (Och and Ney, 2000b). The vocab-
ulary size was 18,499 for English and 24,198 for
French. The GIZA++ toolkit (Och and Ney, 2000a)
was used for training the IBM-3 models (as in (Och
and Ney, 2000b)).
5.5 Word Alignment Lattice Generation
We obtained word alignments under the
modified IBM-3 models using the finite
state translation framework introduced by
Knight and Al-Onaizan (1998). The finite state
operations were carried out using the AT&T Finite
State Machine Toolkit (Mohri et al, 2001; Mohri et
al., 2000).
The WFST framework involves building a trans-
ducer for each constituent of the IBM-3 Alignment
Models: the word fertility model ; the NULL fer-
tility model ; and the word translation model
(Section 5.4). For each sentence pair we also built a
finite state acceptor that accepts the English sen-
tence and another acceptor which accepts all legal
permutations of the French sentence. The alignment
lattice for the sentence pair was then obtained
by the following weighted finite state composition
. In practice, the WFST ob-
tained by the composition was pruned to a maximum
of 10,000 states using a likelihood based pruning op-
eration. In terms of AT&T Finite State Toolkit shell
commands, these operations are given as:
fsmcompose E M fsmcompose - N
fsmcompose - T fsmcompose - F
fsmprune -n 10000
The finite state composition and pruning were per-
formed using lazy implementations of algorithms
provided in AT&T Finite State libraries (Mohri et
al., 2000). This made the computation efficient be-
cause even though five WFSTs are composed into
a potentially huge transducer, only a small portion
of it is actually searched during the pruning used to
generate the final lattice.
A heavily pruned alignment lattice for a
sentence-pair from the test data is shown in Fig-
ure 2. For clarity of presentation, each alignment
link in the lattice is shown as an ordered
pair where and are
the English and French words on the link. For each
sentence, we also computed the lattice path with the
highest probability . This gives the ML
alignment under the statistical MT models that will
give our baseline performance under the various loss
functions.
5.6 Performance Under The Alignment Error
Rates
Our unseen test data consisted of 207 French-
English sentence pairs from the Hansards cor-
pus (Och and Ney, 2000b). These sentence pairs had
at most 16 words in the French sentence; this restric-
tion on the sentence length was necessary to control
the memory requirements of the composition.
5.6.1 MBR Consensus Alignments
In the previous sections we introduced a total
of four loss functions: , , and
. Using either Equation 11 or 13, an MBR
decoder can be constructed for each. These decoders
are called MBR-AE, MBR-PTSD, MBR-POSD, and
MBR-AWCD, respectively.
5.6.2 Evaluation Metrics
The performance of the four decoders was mea-
sured with respect to the alignments provided by hu-
man experts (Och and Ney, 2000b). The first eval-
uation metric used was the Alignment Error Rate
(Equation 1). We also evaluated each decoder un-
der the Generalized Alignment Error Rates (GAER).
These are defined as:
(17)
There are six variants of GAER. These arise
when is specified by ,
or . There are two versions of each
of these: one version is sensitive only to sure
(S) links. The other version considers all (A)
links in the reference alignment. We there-
fore have the following six Generalized Alignment
Error Rates: PTSD-S, POSD-S, AWCD-S, and
PTSD-A, POSD-A, AWCD-A. We say we have a
matched condition when the same loss function is
used in both the error rate and the decoder design.
 0
1NULL_0:a_4/5.348
3
it_1:ce_1/2.344
2it_1:ce_1/1.927
4NULL_0:a_4/5.348
6
is_2:est_2/1.349
5
is_2:est_2/1.349
9
quite_3:tout_3/4.132
8
quite_3:fait_5/4.405is_2:est_2/0.933
7NULL_0:a_4/5.348
10
quite_3:fait_5/2.195
quite_3:tout_3/1.921
quite_3:tout_3/3.715
quite_3:fait_5/3.989
11understandable_4:comprehensible_6/2.161 12/0._5:._7/0.432
Figure 2: A heavily pruned alignment lattice for the English-French sentence pair
e=?it is quite understandable .? f=?ce est tout a fait comprehensible .?.
5.6.3 Decoder Performance
The performance of the decoders under various
loss functions is given in Table 1. We observe that
in none of these experiments was the ML decoder
found to be optimal. In all instances, the MBR
decoder tuned for each loss function was the best
performing decoder under the corresponding error
rate. In particular, we note that alignment perfor-
mance as measured under the AER metric can be
improved by using MBR instead of ML alignment.
This demonstrates the value of finding decoding pro-
cedures matched to the performance criterion of in-
terest.
We observe some affinity among the loss func-
tions. In particular, the ML decoder performs better
under the AER than any of the MBR-GAE decoders.
This is because the loss, for which the ML de-
coder is optimal, is closer to the loss than any
of the loss functions. The NULL symbol is
treated quite differently under and , and
this leads to a large mismatch between the MBR-
GAE decoders and the AER metric. Similarly, the
performance of the MBR-POS decoder degrades
significantly under the AWCD-S and AWCD-A met-
rics. Since there are more word clusters (100) than
POS tags (55), the MBR-POS decoder is therefore
incapable of producing hypotheses that can match
the word clusters used in the AWCD metrics.
6 Discussion And Conclusions
We have presented a Minimum Bayes-Risk decod-
ing strategy for obtaining word alignments of bilin-
gual texts. MBR decoding is a general formulation
that allows the construction of specialized decoders
from general purpose models. The strategy aims at
direct minimization of the expected risk of align-
ment errors under a given alignment loss function.
We have introduced several alignment loss func-
tions to measure the alignment quality. These in-
corporate information from varied analyses, such
as parse trees, POS tags, and automatically derived
word clusters. We have derived and implemented
lattice based MBR consensus decoders under these
loss functions. These decoders rescore the lattices
produced by maximum likelihood decoding to pro-
duce the optimal MBR alignments.
We have chosen to present MBR decoding using
the IBM-3 statistical MT models implemented via
WFSTs. However MBR decoding is not restricted
to this framework. It can be applied more broadly
using other MT model architectures that might be
selected for reasons of modeling fidelity or compu-
tational efficiency.
We have presented these alignment loss functions
to explore how linguistic knowledge might be in-
corporated into machine translation systems without
building detailed statistical models of these linguis-
tic features. However we stress that the MBR decod-
ing procedures described here do not preclude the
construction of complex MT models that incorporate
linguistic features. The application of such mod-
els, which could be trained using conventional max-
imum likelihood estimation techniques, should still
benefit by the application of MBR decoding tech-
niques.
In future work we will investigate loss functions
that incorporate French and English parse-tree infor-
mation into the alignment decoding process. Our ul-
timate goal, towards which this work is the first step,
is to construct loss functions that take advantage of
linguistic structures such as syntactic dependencies
found through monolingual analysis of the sentences
to be aligned. Recent work (Hwa et al, 2002) sug-
gests that translational corresponence of linguistic
structures can indeed be useful in projecting parses
across languages. Our ideal would be to construct
MBR decoders based on loss functions that are sen-
sitive both to word alignment as well as to agreement
in higher level structures such as parse trees. In this
way ambiguity present in word-to-word alignments
will be resolved by the alignment of linguistic struc-
tures.
Generalized Alignment Error Rates
Decoder AER PTSD-S POSD-S AWCD-S PTSD-A POSD-A AWCD-A
ML 18.13 3.13 4.35 4.69 29.39 51.36 54.58
MBR-AE 14.87 1.34 1.89 1.94 19.81 36.42 38.58
MBR-PTSD 23.26 0.62 0.69 0.82 14.45 26.76 28.42
MBR-POSD 28.60 2.43 0.69 3.23 15.70 26.28 29.48
MBR-AWCD 24.71 1.00 0.95 0.86 14.92 26.83 28.39
Table 1: Performance (%) of the MBR decoders under the Alignment Error and Generalized Alignment
Error Rates. For each metric the error rate of the matched decoder is in bold.
MBR alignment is a promising modeling frame-
work for the detailed linguistic annotation of bilin-
gual texts. It is a simple model rescoring formalism
that improves well trained statistical models by tun-
ing them for particular performance criteria. Ideally,
it will be used to produce decoders optimized for
the loss functions that actually measure the qualities
that we wish to see in newly developed automatic
systems.
Acknowledgments
We would like to thank F. J. Och of RWTH, Aachen
for providing us the GIZA++ SMT toolkit, the mk-
cls toolkit to train word classes, the Hansards 50K
training and test data, and the reference word align-
ments and AER metric software. We would also like
to thank P. Resnik, R. Hwa and O. Kolak of the Univ.
of Maryland for useful discussions and help with the
GIZA++ setup. We thank AT&T Labs - Research for
use of the FSM Toolkit. This work was supported by
an ONR MURI grant N00014-01-1-0685.
References
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia, PA, USA.
V. Goel and W. Byrne. 2000. Minimum Bayes-risk auto-
matic speech recognition. Computer Speech and Lan-
guage, 14(2):115?135.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using annota-
tion projection. In Proceedings of ACL-2002. To ap-
pear.
R. Kneser and H. Ney. 1991. Forming word classes by
statistical clustering for statistical language modelling.
In The 1st Quantititative Linguistics Conference, Trier,
Germany.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In Proceedings of the AMTA Con-
ference, pages 421?437, Langhorne, PA, USA.
M. Mittendorfer and W. Winiwarter. 2001. Experiments
with the use of syntactic analysis in information re-
trieval. In Proceedings of the 6th International Work-
shop on Applications of Natural Language and Infor-
mation Systems, Bonn, Germany.
M. Mohri, F. C. N. Pereira, and M. Riley. 2000. The
design principles of a weighted finite-state transducer
library. Theoretical Computer Science, 231(1):17?32.
M. Mohri, F. Pereira, and M. Riley, 2001. ATT
General-purpose finite-state machine software tools.
http://www.research.att.com/sw/tools/fsm/.
F. Och and H. Ney. 2000a. A comparison of alignment
models for statistical machine translation. In Proceed-
ings Of 18th Conference On Computational Linguis-
tics, pages 1086?1090, Saarbrucken, Germany.
F. Och and H. Ney. 2000b. Improved statistical align-
ment models. In Proceedings of ACL-2000, pages
440?447, Hong Kong, China.
K. Papineni, S. Roukos, T. Ward, J. Henderson, and
F. Reeder. 2002. Corpus-based comprehensive and di-
agnostic mt evaluation: Initial arabic, chinese, french,
and spanish results. In Proceedings of HLT 2002.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 133?142, Philadelphia, PA, USA.
F. Wessel, K. Macherey, and R. Schlueter. 1998. Us-
ing word probabilities as confidence measures. In Pro-
ceedings of ICASSP-98, pages 225?228, Seattle, WA,
USA.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 42?50, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Word Alignment with Bridge Languages
Shankar Kumar and Franz Och and Wolfgang Macherey
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, U.S.A.
{shankarkumar,och,wmach}@google.com
Abstract
We describe an approach to improve
Statistical Machine Translation (SMT)
performance using multi-lingual, parallel,
sentence-aligned corpora in several bridge
languages. Our approach consists of a sim-
ple method for utilizing a bridge language to
create a word alignment system and a proce-
dure for combining word alignment systems
from multiple bridge languages. The final
translation is obtained by consensus de-
coding that combines hypotheses obtained
using all bridge language word alignments.
We present experiments showing that mul-
tilingual, parallel text in Spanish, French,
Russian, and Chinese can be utilized in
this framework to improve translation
performance on an Arabic-to-English task.
1 Introduction
Word Alignment of parallel texts forms a cru-
cial component of phrase-based statistical machine
translation systems. High quality word alignments
can yield more accurate phrase-pairs which improve
quality of a phrase-based SMT system (Och and
Ney, 2003; Fraser and Marcu, 2006b).
Much of the recent work in word alignment has
focussed on improving the word alignment quality
through better modeling (Och and Ney, 2003; Deng
and Byrne, 2005; Martin et al, 2005) or alternative
approaches to training (Fraser and Marcu, 2006b;
Moore, 2005; Ittycheriah and Roukos, 2005). In
this paper we explore a complementary approach to
improve word alignments using multi-lingual, par-
allel (or multi-parallel) corpora. Two works in the
literature are very relevant to our approach. Borin
(2000) describes a non-statistical approach where a
pivot alignment is used to combine direct translation
and indirect translation via a third language. Filali
and Bilmes (2005) present a multi-lingual extension
to the IBM/HMMmodels. Our current approach dif-
fers from this latter work in that we propose a sim-
ple framework to combine word alignments from
any underlying statistical alignment model without
the need for changing the structure of the model.
While both of the above papers focus on improv-
ing word alignment quality, we demonstrate that
our approach can yield improvements in transla-
tion performance. In particular, we aim to improve
an Arabic-to-English (Ar-En) system using multi-
parallel data from Spanish (Es), French (Fr), Rus-
sian (Ru) and Chinese (Zh). The parallel data in
these languages X ? {Es, Fr,Ru, Zh} is used to
generate word alignments between Arabic-X and
X-English. These alignments are then combined to
obtain multiple word alignments for Arabic-English
and the final translation systems.
The motivation for this approach is two-fold.
First, we believe that parallel corpora available
in several languages provide a better training ma-
terial for SMT systems relative to bilingual cor-
pora. Such multi-lingual parallel corpora are be-
coming widely available; examples include proceed-
ings of the United Nations in six languages (UN,
2006), European Parliament (EU, 2005; Koehn,
2003), JRC Acquis corpus (EU, 2007) and religious
texts (Resnik et al, 1997). Word alignment systems
42
trained on different language-pairs (e.g. French-
English versus Russian-English) make errors which
are somewhat orthogonal. In such cases, incorrect
alignment links between a sentence-pair can be cor-
rected when a translation in a third language is avail-
able. Thus it can help resolve errors in word align-
ment. We combine word alignments using several
bridge languages with the aim of correcting some
of the alignment errors. The second advantage of
this approach is that the word alignment from each
bridge language can be utilized to build a phrase-
based SMT system. This provides a diverse collec-
tion of translation hypotheses for MT system com-
bination (Bangalore et al, 2002; Sim et al, 2007;
Matusov et al, 2006; Macherey and Och, 2007). Fi-
nally, a side benefit of this paper is that it provides a
study that compares alignment qualities and BLEU
scores for models in different languages trained on
parallel text which is held identical across all lan-
guages.
We show that parallel corpora in multiple lan-
guages can be exploited to improve the translation
performance of a phrase-based translation system.
This paper gives specific recipes for using a bridge
language to construct a word alignment and for com-
bining word alignments produced by multiple statis-
tical alignment models.
The rest of this paper is organized as follows: Sec-
tion 2 gives an overview of our framework for gen-
erating word alignments in a single language-pair.
In Section 3, we describe how a bridge language
may be used for producing word alignments. In Sec-
tion 4, we describe a scheme to combine word align-
ments from several bridge languages. Section 5 de-
scribes our experimental setup and reports the align-
ment and translation performance. A final discus-
sion is presented in Section 6.
2 Word Alignment Framework
A statistical translation model (Brown et al, 1993;
Och and Ney, 2003) describes the relationship be-
tween a pair of sentences in the source and target
languages (f = fJ1 , e = e
I
1) using a translation
probability P (f |e). Alignment models introduce a
hidden alignment variable a = aJ1 to specify a map-
ping between source and target words; aj = i in-
dicates that the jth source word is linked to the ith
target word. Alignment models assign a probabil-
ity P (f ,a|e) to the source sentence and alignment
conditioned on the target sentence. The transla-
tion probability is related to the alignment model as:
P (f |e) =
?
a P?(f ,a|e), where ? is a set of param-
eters.
Given a sentence-pair (f , e), the most likely
(Viterbi) word alignment is found as (Brown et al,
1993): a? = argmaxa P (f ,a|e). An alternate cri-
terion is the Maximum A-Posteriori (MAP) frame-
work (Ge, 2004; Matusov et al, 2004). We use a
refinement of this technique.
Given any word alignment model, posterior prob-
abilities can be computed as (Brown et al, 1993)
P (aj = i|e, f) =
?
a
P (a|f , e)?(i, aj), (1)
where i ? {0, 1, ..., I}. The assignment aj = 0
corresponds to the NULL (empty) alignment. These
posterior probabilities form a matrix of size (I+1)?
J , where entries along each column sum to one.
The MAP alignment for each source position j ?
{1, 2, ..., J} is then computed as
aMAP (j) = argmax
i
P (aj = i|e, f). (2)
We note that these posterior probabilities can be
computed efficiently for some alignment models
such as the HMM (Vogel et al, 1996; Och and Ney,
2003), Models 1 and 2 (Brown et al, 1993).
In the next two sections, we describe how poste-
rior probabilities can be used to a) construct align-
ment systems from a bridge language, and b) merge
several alignment systems.
3 Constructing Word Alignment Using a
Bridge Language
We assume here that we have triples of sentences
that are translations of each other in languages F, E,
and the bridge language G: f = fJ1 , e = e
I
1,g =
gK1 . Our goal is to obtain posterior probability es-
timates for the sentence-pair in FE: (f , e) using the
posterior probability estimates for the sentence pairs
in FG: (f ,g) and GE: (g, e). The word alignments
between the above sentence-pairs are referred to as
aFE , aFG, and aGE respectively; the notation aFE
indicates that the alignment maps a position in F to
a position in E.
43
We first express the posterior probability as a sum
over all possible translations g in G and hidden
alignments aFG.
P (aFEj = i|e, f)
=
?
g
P (aFEj = i,g|e, f)
=
?
g,k
P (aFEj = i,g, a
FG
j = k|e, f)
=
?
g,k
{
P (g|e, f)P (aFGj = k|g, e, f)
?P (aFEj = i|a
FG
j = k,g, e, f)
}
(3)
We now make some assumptions to simplify the
above expression. First, there is exactly one trans-
lation g in bridge language G corresponding to the
sentence-pair f , e. Since aGE
aFGj
= i = aFEj , we can
express
P (aFEj = i|a
FG
j = k,g, f , e) = P (a
GE
k = i|g, e).
Finally, alignments in FG do not depend on E.
Under these assumptions, we arrive at the final ex-
pression for the posterior probability FE in terms of
posterior probabilities for GF and EG
P (aFEj = i|e, f) = (4)
K?
k=0
P (aFGj = k|g, f)P (a
GE
k = i|g, e)
The above expression states that the posterior prob-
ability matrix for FE can be obtained using a simple
matrix multiplication of posterior probability ma-
trices for GE and FG. In this multiplication, we
prepend a column to the GE matrix corresponding
to k = 0. This probability P (aGEk = i) when k = 0
is not assigned by the alignment model; we set it as
follows
P (aGEk = i|k = 0) =
{
 i = 0
1?
I i ? {1, 2, ..., I}
The parameter  controls the number of empty align-
ments; a higher value favors more empty alignments
and vice versa. In our experiments, we set  = 0.5.
4 Word Alignment Combination Using
Posterior Probabilities
We next show how Word Alignment Posterior Prob-
abilities can be used for combining multiple word
alignment systems. In our context, we use this pro-
cedure to combine word alignments produced using
multiple bridge languages.
Suppose we have translations in bridge languages
G1, G2, ..., GN , we can generate a posterior prob-
ability matrix for FE using each of the bridge lan-
guages. In addition, we can always generate a poste-
rior probability matrix for FE with the FE alignment
model directly without using any bridge language.
These N + 1 posterior matrices can be combined as
follows. Here, the variable B indicates the bridge
language. B ? {G0, G1, ..., GN}; G0 indicates the
case when no bridge language is used.
P (aFEj = i|e, f) (5)
=
N?
l=0
P (B = Gl, a
FE
j = i|e, f)
=
N?
l=0
P (B = Gl)P (a
FE
j = i|Gl, e, f),
where P (aFEj = i|Gl, j, e, f) is the posterior proba-
bility when bridge language B = Gl. The probabili-
ties P (B = Gl) sum to one over l ? {0, 1, 2, ..., N}
and represent the prior probability of bridge lan-
guage l. In our experiments, we use a uniform prior
P (B = Gl) = 1N+1 . Equation 5 provides us a way
to combine word alignment posterior probabilites
from multiple bridge languages. In our alignment
framework (Section 2), we first interpolate the pos-
terior probability matrices (Equation 5) and then ex-
tract the MAP word alignment (Equation 2) from the
resulting matrix.
5 Experiments
We now present experiments to demonstrate the ad-
vantages of using bridge languages. Our experi-
ments are performed in the open data track of the
NIST Arabic-to-English (A-E) machine translation
task 1.
5.1 Training and Test Data
Our approach to word alignment (Section 3) requires
aligned sentences in multiple languages. For train-
ing alignment models, we use the ODS United Na-
1http://www.nist.gov/speech/tests/mt/
44
Set # of Ar words (K) # of sentences
dev1 48.6 2007
dev2 11.4 498
test 37.8 1610
blind 36.5 1797
Table 1: Statistics for the test data.
tions parallel data (UN, 2006) which contains par-
liamentary documents from 1993 onwards in all six
official languages of the UN: Arabic (Ar), Chinese
(Zh), English (En), French (Fr), Russian (Ru), and
Spanish (Es).
We merge the NIST 2001-2005 Arabic-English
evaluation sets into a pool and randomly sam-
ple this collection to create two development sets
(dev1,dev2) and a test set (test) with 2007, 498, and
1610 sentences respectively. Our blind test (blind)
set is the NIST part of the NIST 06 evaluation set
consisting of 1797 sentences. The GALE portion of
the 06 evaluation set is not used in this paper. We re-
port results on the test and blind sets. Some statistics
computed on the test data are shown in Table 1.
5.2 Alignment Model Training
For training Arabic-English alignment models, we
use Chinese, French, Russian and Spanish as bridge
languages. We train a model for Ar-En and 4 mod-
els each for Ar-X and X-En, where X is the bridge
language. To obtain aligned sentences in these lan-
guage pairs, we train 9 sentence aligners. We then
train alignment models for all 9 language-pairs us-
ing a recipe consisting of 6 Model-1 iterations and
6 HMM iterations. Finally, Word Alignment Poste-
rior Probabilities are generated over the bitext. In
Table 2, we report the perplexities of the alignment
models for the translation directions where either
Arabic or English is predicted. There are 55M Ara-
bic tokens and 58M English tokens. We observe
that the alignment model using Spanish achieves the
lowest perplexity; this value is even lower than the
perplexity of the direct Arabic-English model. Per-
plexity is related to the hardness of the word align-
ment; the results suggest that bridge languages such
as Spanish make alignment task easier while others
do not. We stress that perplexity is not related to the
alignment or the translation performance.
Bridge Perplexity
Lang ? Ar ?En
None 113.8 26.1
Es 99.0 22.9
Fr 138.6 30.2
Ru 128.3 27.5
Zh 126.1 34.6
Table 2: Perplexities of the alignment models.
5.3 Bridge Language Word Alignments
Each of the 4 bridge languages is utilized for con-
structing a word alignment for Arabic-English. Us-
ing each bridge language X, we obtain Arabic-
English word alignments in both translation direc-
tions (AE and EA). The posterior matrix for AE is
obtained using AX and XE matrices while the EA
matrix is obtained from EX and XA matrices (Equa-
tion 4). The AE (EA) matrices from the bridge
languages are then interpolated with the AE (EA)
matrix obtained from the alignment model trained
directly on Arabic-English (Section 4). The MAP
word alignment for AE (EA) direction is computed
from the AE (EA) matrix. We next outline how these
word alignments are utilized in building a phrase-
based SMT system.
5.4 Phrase-based SMT system
Our phrase-based SMT system is similar to the
alignment template system described in Och and
Ney (2004). We first extract an inventory of phrase-
pairs up to length 7 from the union of AE and EA
word alignments. Various feature functions (Och
and Ney, 2004) are then computed over the entries
in the phrase table. 5-gram word language models
in English are trained on a variety of monolingual
corpora (Brants et al, 2007). Minimum Error Rate
Training (MERT) (Och, 2003) under BLEU crite-
rion is used to estimate 20 feature function weights
over the larger development set (dev1).
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004). Decoding is done in two passes. An ini-
tial list of 1000-best hypotheses is generated by the
decoder. This list is then rescored using Minimum
Bayes-Risk (MBR) decoding (Kumar and Byrne,
2004). The MBR scaling parameter is tuned on the
smaller development set (dev2).
45
Bridge Metrics(%)
Language AE EA
Prec Rec AER Prec Rec AER
None 74.1 73.9 26.0 67.3 57.7 37.9
Es 61.7 56.3 41.1 50.0 40.2 55.4
Fr 52.9 48.0 49.7 42.3 33.6 62.5
Ru 57.4 50.8 46.1 40.2 31.6 64.6
Zh 44.3 39.3 58.3 39.7 29.9 65.9
AC1 70.0 65.0 32.6 56.8 46.4 48.9
Table 3: Alignment Performance with Bridge Lan-
guages
5.5 Alignment Results
We first report alignment performance (Table 3) of
the alignment models obtained using the bridge lan-
guages. Alignment results are reported in terms
of Precision (Prec), Recall (Rec) and Alignment
Error Rate (AER). We report these numbers on
a 94-sentence test set with translations in all six
languages and human word alignments in Arabic-
English. Our human word alignments do not dis-
tinguish between Sure and Probable links (Och and
Ney, 2003).
In these experiments, we first identify the com-
mon subset of sentences which have translations in
all six languages. Each of the 9 alignment models
is then trained on this subset. We report Alignment
performance in both translation directions: Arabic-
to-English (AE) and English-to-Arabic (EA). The
first row (None) gives the results when no bridge
language is used.
Among the bridge languages, Spanish gives the
best alignment for Arabic-English while Chinese re-
sults in the worst. This might be related to how dif-
ferent the bridge language is relative to either En-
glish or Arabic. The last row (AC1) shows the per-
formance of the alignment obtained by combining
None/Es/Fr/Ru/Zh alignments. This alignment out-
performs all bridge alignments but is weaker than
the alignment without any bridge language. Our
hypothesis is that a good choice of interpolation
weights (Equation 5) would reduce AER of the AC1
combination. However, we did not investigate these
choices in this paper. We report alignment error rates
here to give the readers an idea of the vastly differ-
ent alignment performance using each of the bridge
languages.
5.6 Translation Results
We now report translation performance of our tech-
niques. We measure performance using the NIST
implementation of case sensitive BLEU-4 on true-
cased translations. We observed in experiments
not reported here that results are almost identical
with/without Minimum Error Rate Training ; we
therefore report the results without the training. We
note that the blind set is the NIST subset of the 2006
NIST evaluation set. The systems reported here are
for the Unlimited Data Track in Arabic-to-English
and obtain competitive performance relative to the
results reported on the NIST official results page 2
We present three sets of experiments. In Table 4,
we describe the first set where all 9 alignment mod-
els are trained on nearly the same set of sentences
(1.9M sentences, 57.5M words in English). This
makes the alignment models in all bridge languages
comparable. In the first rowmarked None, we do not
use a bridge language. Instead, an Ar-En alignment
model is trained directly on the set of sentence pairs.
The next four rows give the performance of align-
ment models trained using the bridge languages Es,
Fr, Ru and Zh respectively. For each language, we
use the procedure (Section 3) to obtain the posterior
probability matrix for Arabic-English from Arabic-
X and X-English matrices. The row AC1 refers to
alignment combination using interpolation of poste-
rior probabilities described in Section 4. We com-
bine posterior probability matrices from the systems
in the first four rows: None, Es, Ru and Zh. We
exclude the Zh system from the AC1 combination
because it is found to degrade the translation perfor-
mance by 0.2 points on the test set.
In the final six rows of Table 4, we show the per-
formance of a consensus decoding technique that
produces a single output hypothesis by combin-
ing translation hypotheses from multiple systems;
this is an MBR-like candidate selection procedure
based on BLEU correlation matrices and is de-
scribed in Macherey and Och (2007). We first report
performance of the consensus output by combining
None systems with/without MERT. Each of the fol-
lowing rows provides the results from consensus de-
coding for adding an extra system both with/without
MERT. Thus, the final row (TC1) combines transla-
2
http://www.nist.gov/speech/tests/mt/mt06eval official results.html
46
tions from 12 systems: None, Es, Fr, Ru, Zh, AC1
with/without MERT. All entries marked with an as-
terisk are better than the None baseline with 95%
statistical significance computed using paired boot-
strap resampling (Koehn, 2004).
35 40 45 50 55 60 65 7037
37.5
38
38.5
39
39.5
40
40.5
None
Es
Fr
Ru
Zh
AC1
100?AER(%)
BLE
U(%
)
Figure 1: 100-AER (%) vs. BLEU(%) on the blind
set for 6 systems from Table 3.
Figure 1 shows the plot between 100-AER% (av-
erage of EA/AE directions) and BLEU for the six
systems in Table 3. We observe that AER is loosely
correlated to BLEU (? = 0.81) though the re-
lation is weak, as observed earlier by Fraser and
Marcu (2006a). Among the bridge languages, Span-
ish gives the lowest AER/highest BLEU while Chi-
nese results in highest AER/lowest BLEU. We can
conclude that Spanish is closest to Arabic/English
while Chinese is the farthest. All the bridge lan-
guages yield lower BLEU/higher AER relative to the
No-Bridge baseline. Therefore, our estimate of the
posterior probability (Equation 4) is always worse
than the posterior probability obtained using a di-
rect model. The alignment combination (AC1) be-
haves differently from other bridge systems in that it
gives a higher AER and a higher BLEU relative to
None baseline. We hypothesize that AC1 is differ-
ent from the bridge language systems since it arises
from a different process: interpolation with the di-
rect model (None).
Both system combination techniques give im-
provements relative to None baseline: alignment
combination AC1 gives a small gain (0.2 points)
while the consensus translation TC1 results in a
larger improvement (0.8 points). The last 4 rows
of the table show that the performance of the hy-
pothesis consensus steadily increases as systems get
added to the None baseline. This shows that while
bridge language systems are weaker than the di-
rect model, they can provide complementary sources
of evidence. To further validate this hypothesis,
we compute inter-system BLEU scores between
None/es and all the systems in Table 5. We observe
that the baseline (None) is very dissimilar from the
rest of the systems. We hypothesize that the baseline
system has an alignment derived from a real align-
ment model while the rest of the bridge systems are
derived using matrix multiplication. The low inter-
system BLEU scores show that the bridge systems
provide diverse hypotheses relative to the baseline
and therefore contribute to gains in consensus de-
coding.
Bridge Lang # Msents BLEU (%)
test blind
None 1.9 52.1 40.1
Es 1.9 51.7 39.8
Fr 1.9 51.2 39.5
Ru 1.9 50.4 38.7
Zh 1.9 48.4 37.1
AC1 1.9 52.1 40.3
Hypothesis Consensus
None 1.9 51.9 39.8
+Es 1.9 52.2 40.0
+Fr 1.9 52.4? 40.5?
+Ru 1.9 52.8? 40.7?
+Zh 1.9 52.6? 40.6?
+AC1 = TC1 1.9 53.0? 40.9?
Table 4: Translation Experiments for Set 1; Results
are reported on the test and blind set: (NIST portion
of 2006 NIST eval set).
Ref None es fr ru zh AC1
None 100.0 60.0 59.8 59.7 59.5 58.7
es 59.6 100.0 79.9 69.3 67.4 70.5
Table 5: Inter-system BLEU scores (%) between
None/es and all systems in Table 3.
To gain some insight about how the bridge sys-
tems help in Table 4, we present an example in Ta-
ble 6. The example shows the consensus Transla-
tions and the 12 input translations for the consensus
decoding. The example suggests that the inputs to
the consensus decoding exhibit diversity.
Table 7 reports the second and third sets of ex-
periments. For both sets, we first train each bridge
language system X using all aligned sentences avail-
47
System MERT Hypothesis
None N The President of the National Conference Visit Iraqi Kurdistan Iraqi
None Y President of the Iraqi National Conference of Iraqi Kurdistan Visit
Es N President of the Iraqi National Congress to Visit Iraqi Kurdistan
Es Y President of the Iraqi National Congress to Visit Iraqi Kurdistan
Fr N President of the Iraqi National Conference Visits Iraqi Kurdistan
Fr Y Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ru N The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ru Y Chairman of the Iraqi National Conference Visit the Iraqi Kurdistan
Zh N The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Zh Y The Chairman of the Iraqi National Conference Visit Iraqi Kurdistan
AC1 N President of the Iraqi National Congress to Visit Iraqi Kurdistan
AC1 Y Chairman of the Iraqi National Congress to Visit Iraqi Kurdistan
TC1 - The Chairman of the Iraqi National Conference Visits Iraqi Kurdistan
Ref - Head of Iraqi National Congress Visits Iraqi Kurdistan
Table 6: An example showing the Consensus Translation (TC1) and the 12 inputs for consensus decoding.
The final row shows the reference translation.
able in Ar, En and X. In Set 2, the first row (Union)
is an alignment model trained on all sentence-pairs
in Ar-En which are available in at least one bridge
language X. AC2 refers to alignment combination
using bridge languages Es/Fr/Ru and Union. TC2
refers to the translation combination from 12 sys-
tems: Es/Fr/Ru/Zh/Union/AC2 with/without Mini-
mum Error Rate training. Finally, the goal in Set 3
(last 3 rows) is to improve the best Arabic-English
system that can be built using all available sen-
tence pairs from the UN corpus. The first row
(Direct) gives the performance of this Ar-En sys-
tem; AC3 refers to alignment combination using
Es/Fr/Ru and Direct. TC3 merges translations from
Es/Fr/Ru/Zh/Direct/AC3. All entries marked with
an asterisk (plus) are better than the Union (Direct)
baseline with 95% statistical significance computed
using paired bootstrap resampling (Koehn, 2004).
The motivation behind Sets 2 and 3 is to train all
bridge language systems on as much bitext as possi-
ble. As a consequence, these systems give better re-
sults than the corresponding systems in Table 4. The
Union system outperforms None by 1.7/1.4 BLEU
points and provides a better baseline. We show un-
der this scenario that system combination techniques
AC2 and TC2 can still give smaller improvements
(0.3/0.5 and 1.0/0.7 points) relative to this baseline.
As mentioned earlier, our approach requires
sentence-aligned corpora. In our experiments, we
use a single sentence aligner for each language pair
(total of 9 aligners). Since these aligners make inde-
pendent decisions on sentence boundaries, we end
up with a smaller pool of sentences (1.9M) that is
common across all language pairs. In contrast, a
sentence aligner that makes simultaneous decisions
in multiple languages would result in a larger set of
common sentence pairs (close to 7M sentence pairs).
Simard (1999) describes a sentence aligner of this
type that improves alignment on a trilingual paral-
lel text. Since we do not currently have access to
such an aligner, we simulate that situation with Sets
2 and 3: AC2/AC3 do not insist that a sentence-pair
be present in all input word alignments. We note that
Set 2 is a data scenario that falls between Sets 1 and
3.
Set 3 provides the best baseline for Arabic-
English based on the UN data by training on
all parallel sentence-pairs. In this situation, sys-
tem combination with bridge languages (AC3/TC3)
gives reasonable improvements in BLEU on the test
set (0.4/1.0 points) but only modest improvements
(0.1/0.4 points) on the blind set. However, this does
show that the bridge systems continue to provide or-
thogonal evidence at different operating points.
6 Discussion
We have described a simple approach to improve
word alignments using bridge languages. This in-
cludes two components: a matrix multiplication to
assemble a posterior probability matrix for the de-
sired language-pair FE using a pair of posterior
probability matrices FG and GE relative to a bridge
language G. The second component is a recipe for
combining word alignment systems by linearly in-
48
Bridge Lang # Msents BLEU (%)
test blind
Es 4.7 53.7 40.9
Fr 4.7 53.2 40.7
Ru 4.5 52.4 39.9
Zh 3.4 49.7 37.9
Set 2
Union 7.2 53.8 41.5
AC2 7.2 54.1 42.0?
TC2 - 54.8? 42.2?
Set 3
Direct 7.0 53.9 42.2
AC3 9.0 54.3+ 42.3
TC3 - 54.9+ 42.6+
Table 7: Translation performance for Sets 2 and 3 on
test and blind:NIST portion of 2006 NIST eval set.
terpolating posterior probability matrices from dif-
ferent sources. In our case, these sources are multi-
ple bridge languages. However, this method is more
generally applicable for combining posterior matri-
ces from different alignment models such as HMM
and Model-4. Such an approach contrasts with the
log-linear HMM/Model-4 combination proposed by
Och and Ney (2003).
There has been recent work by Ayan and Dorr
(2006) on combining word alignments from differ-
ent alignment systems; this paper describes a maxi-
mum entropy framework for this combination. Their
approach operates at the level of the alignment links
and uses maximum entropy to decide whether or
not to include an alignment link in the final out-
put. In contrast, we use posterior probabilities as the
interface between different alignment models. An-
other difference is that this maxent framework re-
quires human word aligned data for training feature
weights. We do not require any human word aligned
data to train our combiner.
Another advantage of our approach is that it is
based on word alignment posterior probability ma-
trices that can be generated by any underlying align-
ment model. Therefore, this method can be used to
combine word alignments generated by fairly dis-
similar word alignment systems as long as the sys-
tems can produce posterior probabilities.
Bridge languages have been used by NLP re-
searchers as a means to induce translation lexicons
between distant languages without the need for par-
allel corpora (Schafer and Yarowsky, 2002; Mann
and Yarowsky, 2001). Our current approach differs
from these efforts in that we use bridge languages to
improve word alignment quality between sentence
pairs. Furthermore, we do not use linguistic insight
to identify bridge languages. In our framework, a
good bridge language is one that provides the best
translation performance using the posterior matrix
multiplication. Our experiments show that Spanish
is a better bridge language relative to Chinese for
Arabic-to-English translation. We speculate that if
our approach was carried out on a data set with hun-
dreds of languages, we might be able to automati-
cally identify language families.
A downside of our approach is the requirement
for exact sentence-aligned parallel data. Except for
a few corpora such as UN, European Parliament etc,
such a resource is hard to find. One solution is to cre-
ate such parallel data by automatic translation and
then retaining reliable translations by using confi-
dence metrics (Ueffing and Ney, 2005).
Our approach to using bridge languages is ex-
tremely simple. Despite its simplicity, the system
combination gives improvements in alignment and
translation performance. In future work, we will
consider several extensions to this framework that
lead to more powerful system combination strategies
using multiple bridge languages. We recall that the
present approach trains bridge systems (e.g. Arabic-
to-French, French-to-English) until the alignment
stage and then uses these for constructing Arabic-
to-English word alignment. An alternate scenario
would be to build phrase-based SMT systems for
Arabic-to-Spanish and Spanish-to-English, and then
obtain Arabic-to-English translation by first trans-
lating from Arabic into Spanish and then Spanish
into English. Such end-to-end bridge systems may
lead to an even more diverse pool of hypotheses that
could further improve system combination.
References
N. Ayan and B. Dorr. 2006. A maximum entropy
approach to combining word alignments. In HLT-
NAACL, New York, New York.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping bilingual data using consensus translation
for a multilingual instant messaging system. In COL-
ING, Taipei, Taiwan.
L. Borin. 2000. You?ll take the high road and I?ll take the
49
low road: Using a third language to improve bilingual
word alignment. In COLING, pages 97?103, Saar-
brucken, Germany.
T. Brants, A. Popat, P. Xu, F. Och, and J. Dean. 2007.
Large language models in machine translation. In
EMNLP, Prague, Czech Republic.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Y. Deng and W. Byrne. 2005. HMM word and
phrase alignment for statistical machine translation. In
EMNLP, Vancouver, Canada.
EU, 2005. European Parliament Proceedings.
http://www.europarl.europa.eu.
EU, 2007. JRC Acquis Corpus. http://langtech.jrc.it/JRC-
Acquis.html.
K. Filali and J. Bilmes. 2005. Leveraging multiple lan-
guages to improve statistical mt word alignments. In
IEEE Workshop on Automatic Speech Recognition and
Understanding, San Juan, Puerto Rico.
A. Fraser and D. Marcu. 2006a. Measuring word align-
ment quality for statistical machine translation. Tech-
nical Report ISI-TR-616, ISI/University of Southern
California.
A. Fraser and D. Marcu. 2006b. Semi-supervised train-
ing for statistical word alignment. In ACL, pages 769?
776, Sydney, Australia.
N. Ge. 2004. Improvements in word alignments. In
Presentation given at DARPA/TIDES workshop.
A. Ittycheriah and S. Roukos. 2005. A maximum en-
tropy word aligner for arabic-english machine transla-
tion. In EMNLP, Vancouver, Canada.
P. Koehn, 2003. European Parlia-
ment Proceedings, Sentence Aligned.
http://people.csail.mit.edu/koehn/publications/europarl/.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In EMNLP, Barcelona, Spain.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
NAACL, pages 169?176, Boston, MA, USA.
W. Macherey and F. Och. 2007. An empirical study on
computing consensus translations from multiple ma-
chine translation systems. In EMNLP, Prague, Czech
Republic.
G. Mann and D. Yarowsky. 2001. Multipath translation
lexicon induction via bridge languages. In NAACL,
Pittsburgh, PA, USA.
J. Martin, R. Mihalcea, and T. Pedersen. 2005. Word
alignment for languages with scarce resources. In ACL
Workshop on Building and Using Parallel Texts, pages
65?74, Ann Arbor, MI, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. InCOL-
ING, Geneva, Switzerland.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
EACL, Trento, Italy.
R. C. Moore. 2005. A discriminative framework for
bilingual word alignment. In EMNLP, Vancouver,
Canada.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 ? 51.
F. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, Sapporo, Japan.
P. Resnik, M. Olsen, and M. Diab. 1997. Creating a
parallel corpus from the book of 2000 tongues. In
Text Encoding Initiative 10th Anniversary User Con-
ference, Providence, RI, USA.
C. Schafer and D. Yarowsky. 2002. Inducing translation
lexicons via diverse similarity measures and bridge
languages. In CoNLL, Taipei, Taiwan.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and P. C.
Woodland. 2007. Consensus network decoding for
statistical machine translation system combination. In
IEEE International Conference on Acoustics, Speech,
and Signal Processing, Honolulu, HI, USA.
M. Simard. 1999. Text translation alignment: Three lan-
guages are better than two. In EMNLP-VLC, College
Park, MD, USA.
N. Ueffing and H. Ney. 2005. Word-level confidence
estimation for machine translation using phrase-based
translation models. In EMNLP, pages 763 ? 770, Van-
couver, Canada.
UN, 2006. ODS UN Parallel Corpus. http://ods.un.org/.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based
word alignment in statistical translation. In COLING,
pages 836?841, Copenhagen, Denmark.
50
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620?629,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
Roy W. Tromble1 and Shankar Kumar2 and Franz Och2 and Wolfgang Macherey2
1Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
royt@jhu.edu
2 Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,och,wmach}@google.com
Abstract
We present Minimum Bayes-Risk (MBR) de-
coding over translation lattices that compactly
encode a huge number of translation hypothe-
ses. We describe conditions on the loss func-
tion that will enable efficient implementation
of MBR decoders on lattices. We introduce
an approximation to the BLEU score (Pap-
ineni et al, 2001) that satisfies these condi-
tions. The MBR decoding under this approx-
imate BLEU is realized using Weighted Fi-
nite State Automata. Our experiments show
that the Lattice MBR decoder yields mod-
erate, consistent gains in translation perfor-
mance over N-best MBR decoding on Arabic-
to-English, Chinese-to-English and English-
to-Chinese translation tasks. We conduct a
range of experiments to understand why Lat-
tice MBR improves upon N-best MBR and
study the impact of various parameters on
MBR performance.
1 Introduction
Statistical language processing systems for speech
recognition, machine translation or parsing typically
employ the Maximum A Posteriori (MAP) deci-
sion rule which optimizes the 0-1 loss function. In
contrast, these systems are evaluated using metrics
based on string-edit distance (Word Error Rate), n-
gram overlap (BLEU score (Papineni et al, 2001)),
or precision/recall relative to human annotations.
Minimum Bayes-Risk (MBR) decoding (Bickel and
Doksum, 1977) aims to address this mismatch by se-
lecting the hypothesis that minimizes the expected
error in classification. Thus it directly incorporates
the loss function into the decision criterion. The ap-
proach has been shown to give improvements over
the MAP classifier in many areas of natural lan-
guage processing including automatic speech recog-
nition (Goel and Byrne, 2000), machine transla-
tion (Kumar and Byrne, 2004; Zhang and Gildea,
2008), bilingual word alignment (Kumar and Byrne,
2002), and parsing (Goodman, 1996; Titov and Hen-
derson, 2006; Smith and Smith, 2007).
In statistical machine translation, MBR decoding
is generally implemented by re-ranking an N -best
list of translations produced by a first-pass decoder;
this list typically contains between 100 and 10, 000
hypotheses. Kumar and Byrne (2004) show that
MBR decoding gives optimal performance when the
loss function is matched to the evaluation criterion;
in particular, MBR under the sentence-level BLEU
loss function (Papineni et al, 2001) gives gains on
BLEU. This is despite the fact that the sentence-level
BLEU loss function is an approximation to the exact
corpus-level BLEU.
A different MBR inspired decoding approach is
pursued in Zhang and Gildea (2008) for machine
translation using Synchronous Context Free Gram-
mars. A forest generated by an initial decoding pass
is rescored using dynamic programming to maxi-
mize the expected count of synchronous constituents
in the tree that corresponds to the translation. Since
each constituent adds a new 4-gram to the existing
translation, this approach approximately maximizes
the expected BLEU.
In this paper we explore a different strategy
to perform MBR decoding over Translation Lat-
tices (Ueffing et al, 2002) that compactly encode a
huge number of translation alternatives relative to an
N -best list. This is a model-independent approach
620
in that the lattices could be produced by any statis-
tical MT system ? both phrase-based and syntax-
based systems would work in this framework. We
will introduce conditions on the loss functions that
can be incorporated in Lattice MBR decoding. We
describe an approximation to the BLEU score (Pa-
pineni et al, 2001) that will satisfy these condi-
tions. Our Lattice MBR decoding is realized using
Weighted Finite State Automata.
We expect Lattice MBR decoding to improve
upon N -best MBR primarily because lattices con-
tain many more candidate translations than the N -
best list. This has been demonstrated in speech
recognition (Goel and Byrne, 2000). We conduct
a range of translation experiments to analyze lattice
MBR and compare it with N -best MBR. An impor-
tant aspect of our lattice MBR is the linear approxi-
mation to the BLEU score. We will show that MBR
decoding under this score achieves a performance
that is at least as good as the performance obtained
under sentence-level BLEU score.
The rest of the paper is organized as follows. We
review MBR decoding in Section 2 and give the for-
mulation in terms of a gain function. In Section 3,
we describe the conditions on the gain function for
efficient decoding over a lattice. The implementa-
tion of lattice MBR with Weighted Finite State Au-
tomata is presented in Section 4. In Section 5, we in-
troduce the corpus BLEU approximation that makes
it possible to perform efficient lattice MBR decod-
ing. An example of lattice MBR with a toy lattice
is presented in Section 6. We present lattice MBR
experiments in Section 7. A final discussion is pre-
sented in Section 8.
2 Minimum Bayes Risk Decoding
Minimum Bayes-Risk (MBR) decoding aims to find
the candidate hypothesis that has the least expected
loss under the probability model (Bickel and Dok-
sum, 1977). We begin with a review of MBR decod-
ing for Statistical Machine Translation (SMT).
Statistical MT (Brown et al, 1990; Och and Ney,
2004) can be described as a mapping of a word se-
quence F in the source language to a word sequence
E in the target language; this mapping is produced
by the MT decoder ?(F ). If the reference transla-
tion E is known, the decoder performance can be
measured by the loss function L(E, ?(F )). Given
such a loss function L(E,E?) between an automatic
translation E? and the reference E, and an under-
lying probability model P (E|F ), the MBR decoder
has the following form (Goel and Byrne, 2000; Ku-
mar and Byrne, 2004):
E? = argmin
E??E
R(E?)
= argmin
E??E
?
E?E
L(E,E?)P (E|F ),
where R(E?) denotes the Bayes risk of candidate
translation E? under the loss function L.
If the loss function between any two hypotheses
can be bounded: L(E,E?) ? Lmax, the MBR de-
coder can be rewritten in terms of a gain function
G(E,E?) = Lmax ? L(E,E?):
E? = argmax
E??E
?
E?E
G(E,E?)P (E|F ). (1)
We are interested in performing MBR decoding
under a sentence-level BLEU score (Papineni et al,
2001) which behaves like a gain function: it varies
between 0 and 1, and a larger value reflects a higher
similarity. We will therefore use Equation 1 as the
MBR decoder.
We note that E represents the space of transla-
tions. For N -best MBR, this space E is the N -best
list produced by a baseline decoder. We will investi-
gate the use of a translation lattice for MBR decod-
ing; in this case, E will represent the set of candi-
dates encoded in the lattice.
In general, MBR decoding can use different
spaces for hypothesis selection and risk computa-
tion: argmax and the sum in Equation 1 (Goel,
2001). As an example, the hypothesis could be se-
lected from the N -best list while the risk is com-
puted based on the entire lattice. Therefore, the
MBR decoder can be more generally written as fol-
lows:
E? = argmax
E??Eh
?
E?Ee
G(E,E?)P (E|F ), (2)
where Eh refers to the Hypothesis space from where
the translations are chosen, and Ee refers to the Evi-
dence space that is used for computing the Bayes-
risk. We will present experiments (Section 7) to
show the relative importance of these two spaces.
621
3 Lattice MBR Decoding
We now present MBR decoding on translation lat-
tices. A translation word lattice is a compact rep-
resentation for very large N -best lists of transla-
tion hypotheses and their likelihoods. Formally,
it is an acyclic Weighted Finite State Acceptor
(WFSA) (Mohri, 2002) consisting of states and arcs
representing transitions between states. Each arc is
labeled with a word and a weight. Each path in the
lattice, consisting of consecutive transitions begin-
ning at the distinguished initial state and ending at a
final state, expresses a candidate translation. Aggre-
gation of the weights along the path1 produces the
weight of the path?s candidate H(E,F ) according
to the model. In our setting, this weight will imply
the posterior probability of the translation E given
the source sentence F :
P (E|F ) =
exp (?H(E,F ))
?
E??E exp (?H(E
?, F ))
. (3)
The scaling factor ? ? [0,?) flattens the distribu-
tion when ? < 1, and sharpens it when ? > 1.
Because a lattice may represent a number of can-
didates exponential in the size of its state set, it is of-
ten impractical to compute the MBR decoder (Equa-
tion 1) directly. However, if we can express the gain
function G as a sum of local gain functions gi, then
we now show that Equation 1 can be refactored and
the MBR decoder can be computed efficiently. We
loosely call a gain function local if it can be ap-
plied to all paths in the lattice via WFSA intersec-
tion (Mohri, 2002) without significantly multiplying
the number of states.
In this paper, we are primarily concerned with lo-
cal gain functions that weight n-grams. Let N =
{w1, . . . , w|N |} be the set of n-grams and let a local
gain function gw : E ? E ? R, for w ? N , be as
follows:
gw(E,E
?) = ?w#w(E
?)?w(E), (4)
where ?w is a constant, #w(E?) is the number of
times that w occurs in E?, and ?w(E) is 1 if w ? E
and 0 otherwise. That is, gw is ?w times the number
of occurrences of w in E?, or zero if w does not oc-
cur in E. We first assume that the overall gain func-
tion G(E,E?) can then be written as a sum of local
1using the log semiring?s extend operator
gain functions and a constant ?0 times the length of
the hypothesis E?.
G(E,E?) = ?0|E
?|+
?
w?N
gw(E,E
?) (5)
= ?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
Given a gain function of this form, we can rewrite
the risk (sum in Equation 1) as follows
?
E?E
G(E,E?)P (E|F )
=
?
E?E
(
?0|E
?|+
?
w?N
?w#w(E
?)?w(E)
)
P (E|F )
= ?0|E
?|+
?
w?N
?w#w(E
?)
?
E?Ew
P (E|F ),
where Ew = {E ? E|?w(E) > 0} represents the
paths of the lattice containing the n-gram w at least
once. TheMBR decoder on lattices (Equation 1) can
therefore be written as
E? = argmax
E??E
{
?0|E
?|+
?
w?N
?w#w(E
?)p(w|E)
}
. (6)
Here p(w|E) =
?
E?Ew P (E|F ) is the posterior
probability of the n-gram w in the lattice. We have
thus replaced a summation over a possibly exponen-
tial number of items (E ? E) with a summation over
the number of n-grams that occur in E , which is at
worst polynomial in the number of edges in the lat-
tice that defines E . We compute the posterior proba-
bility of each n-gram w as:
p(w|E) =
?
E?Ew
P (E|F ) =
Z(Ew)
Z(E)
, (7)
where Z(E) =
?
E??E exp(?H(E
?, F )) (denomi-
nator in Equation 3) and
Z(Ew) =
?
E??Ew exp(?H(E
?, F )). Z(E) and
Z(Ew) represent the sums2 of weights of all paths
in the lattices Ew and E respectively.
4 WFSA MBR Computations
We now show how the Lattice MBR Decision Rule
(Equation 6) can be implemented using Weighted
Finite State Automata (Mohri, 1997). There are four
steps involved in decoding starting from weighted
finite-state automata representing the candidate out-
puts of a translation system. We will describe these
2in the log semiring, where log+(x, y) = log(ex + ey) is
the collect operator (Mohri, 2002)
622
steps in the setting where the evidence lattice Ee may
be different from the hypothesis lattice Eh (Equa-
tion 2).
1. Extract the set of n-grams that occur in the ev-
idence lattice Ee. For the usual BLEU score, n
ranges from one to four.
2. Compute the posterior probability p(w|E) of
each of these n-grams.
3. Intersect each n-gram w, with an appropriate
weight (from Equation 6), to an initially un-
weighted copy of the hypothesis lattice Eh.
4. Find the best path in the resulting automaton.
Computing the set of n-grams N that occur in a
finite automaton requires a traversal, in topological
order, of all the arcs in the automaton. Because the
lattice is acyclic, this is possible. Each state q in the
automaton has a corresponding set of n-grams Nq
ending there.
1. For each state q,Nq is initialized to {}, the set
containing the empty n-gram.
2. Each arc in the automaton extends each of its
source state?s n-grams by its word label, and
adds the resulting n-grams to the set of its tar-
get state. ( arcs do not extend n-grams, but
transfer them unchanged.) n-grams longer than
the desired order are discarded.
3. N is the union over all states q of Nq.
Given an n-gram, w, we construct an automaton
matching any path containing the n-gram, and in-
tersect that automaton with the lattice to find the set
of paths containing the n-gram (Ew in Equation 7).
Suppose E represent the weighted lattice, we com-
pute3: Ew = E ? (w w ??), where w = (?? w ??)
is the language that contains all strings that do not
contain the n-gram w. The posterior probability
p(w|E) of n-gram w can be computed as a ratio of
the total weights of paths in Ew to the total weights
of paths in the original lattice (Equation 7).
For each n-gram w ? N , we then construct
an automaton that accepts an input E with weight
3in the log semiring (Mohri, 2002)
equal to the product of the number of times the n-
gram occurs in the input (#w(E)), the n-gram fac-
tor ?w from Equation 6, and the posterior proba-
bility p(w|E). The automaton corresponds to the
weighted regular expression (Karttunen et al, 1996):
w?(w/(?wp(w|E)) w?)?.
We successively intersect each of these automata
with an automaton that begins as an unweighted
copy of the lattice Eh. This automaton must also
incorporate the factor ?0 of each word. This can
be accomplished by intersecting the unweighted lat-
tice with the automaton accepting (?/?0)?. The
resulting MBR automaton computes the total ex-
pected gain of each path. A path in this automa-
ton that corresponds to the word sequence E? has
cost: ?0|E?|+
?
w?N ?w#w(E)p(w|E) (expression
within the curly brackets in Equation 6).
Finally, we extract the best path from the resulting
automaton4, giving the lattice MBR candidate trans-
lation according to the gain function (Equation 6).
5 Linear Corpus BLEU
Our Lattice MBR formulation relies on the decom-
position of the overall gain function as a sum of lo-
cal gain functions (Equation 5). We here describe a
linear approximation to the log(BLEU score) (Pap-
ineni et al, 2001) which allows such a decomposi-
tion. This will enable us to rewrite the log(BLEU)
as a linear function of n-gram matches and the hy-
pothesis length. Our strategy will be to use a first
order Taylor-series approximation to what we call
the corpus log(BLEU) gain: the change in corpus
log(BLEU) contributed by the sentence relative to
not including that sentence in the corpus.
Let r be the reference length of the corpus, c0 the
candidate length, and {cn|1 ? n ? 4} the number
of n-gram matches. Then, the corpus BLEU score
B(r, c0, cn) can be defined as follows (Papineni et
al., 2001):
logB = min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0 ??n
,
? min
(
0, 1?
r
c0
)
+
1
4
4?
n=1
log
cn
c0
,
where we have ignored ?n, the difference between
the number of words in the candidate and the num-
4in the (max,+) semiring (Mohri, 2002)
623
ber of n-grams. If L is the average sentence length
in the corpus, ?n ? (n? 1)
c0
L .
The corpus log(BLEU) gain is defined as the
change in log(BLEU) when a new sentence?s (E?)
statistics are added to the corpus statistics:
G = logB? ? logB,
where the counts in B? are those of B plus those for
the current sentence. We will assume that the brevity
penalty (first term in the above approximation) does
not change when adding the new sentence. In exper-
iments not reported here, we found that taking into
account the brevity penalty at the sentence level can
cause large fluctuations in lattice MBR performance
on different test sets. We therefore treat only cns as
variables.
The corpus log BLEU gain is approximated by a
first-order vector Taylor series expansion about the
initial values of cn.
G ?
N?
n=0
(c?n ? cn)
? logB?
?c?n
?
?
?
?
c?n=cn
, (8)
where the partial derivatives are given by
? logB
?c0
=
?1
c0
, (9)
? logB
?cn
=
1
4cn
.
Substituting the derivatives in Equation 8 gives
G = ? logB ? ?
?c0
c0
+
1
4
4?
n=1
?cn
cn
, (10)
where each ?cn = c?n ? cn counts the statistic in
the sentence of interest, rather than the corpus as a
whole. This score is therefore a linear function in
counts of words ?c0 and n-gram matches ?cn. Our
approach ignores the count clipping present in the
exact BLEU score where a correct n-gram present
once in the reference but several times in the hypoth-
esis will be counted only once as correct. Such an
approach is also followed in Dreyer et al (2007).
Using the above first-order approximation to gain
in log corpus BLEU, Equation 9 implies that ?0, ?w
from Section 3 would have the following values:
?0 =
?1
c0
(11)
?w =
1
4c|w|
.
5.1 N-gram Factors
We now describe how the n-gram factors (Equa-
tion 11) are computed. The factors depend on
a set of n-gram matches and counts (cn; n ?
{0, 1, 2, 3, 4}). These factors could be obtained from
a decoding run on a development set. However, do-
ing so could make the performance of lattice MBR
very sensitive to the actual BLEU scores on a partic-
ular run. We would like to avoid such a dependence
and instead, obtain a set of parameters which can
be estimated from multiple decoding runs without
MBR. To achieve this, we make use of the properties
of n-gram matches. It is known that the average n-
gram precisions decay approximately exponentially
with n (Papineni et al, 2001). We now assume that
the number of matches of each n-gram is a constant
ratio r times the matches of the corresponding n? 1
gram.
If the unigram precision is p, we can obtain the
n-gram factors (n ? {1, 2, 3, 4}) (Equation 11) as a
function of the parameters p and r, and the number
of unigram tokens T :
?0 =
?1
T
(12)
?n =
1
4Tp? rn?1
We set p and r to the average values of unigram pre-
cision and precision ratio across multiple develop-
ment sets. Substituting the above factors in Equa-
tion 6, we find that the MBR decision does not de-
pend on T ; therefore any value of T can be used.
6 An Example
Figure 1 shows a toy lattice and the final MBR au-
tomaton (Section 4) for BLEU with a maximum n-
gram order of 2. We note that the MBR hypothesis
(bcde) has a higher decoder cost relative to the MAP
hypothesis (abde). However, bcde gets a higher ex-
pected gain (Equation 6) than abde since it shares
more n-grams with the Rank-3 hypothesis (bcda).
This illustrates how a lattice can help select MBR
translations that can differ from the MAP transla-
tion.
7 Experiments
We now present experiments to evaluate MBR de-
coding on lattices under the linear corpus BLEU
624
01
2
3
4
5
6
7
8
9
10
0
1
2
3
4
7
5
8
6
c/0.013
d/0.013
d/?0.008
d/?0.008
e/0.004
a/0.038
a/0.5
b/0.6
b/0.6
b/0.6
c/0.6
c/0.6
d/0.3
d/0.4
e/0.5
a/0.5
a/0.063
b/0.043
b/0.043
b/0.013
c/0.013
Figure 1: An example translation lattice with decoder
costs (top) and its MBR Automaton for BLEU-2 (bot-
tom). The bold path in the top is the MAP hypothesis
and the bold path in the bottom is the MBR hypothe-
sis. The precision parameters in Equation 12 are set to:
T = 10, p = 0.85, r = 0.72.
Dataset # of sentences
aren zhen enzh
dev1 1353 1788 1664
dev2 663 919 919
blind 1360 1357 1859
Table 1: Statistics over the development and test sets.
gain. We start with a description of the data sets
and the SMT system.
7.1 Development and Blind Test Sets
We present our experiments on the constrained data
track of the NIST 2008 Arabic-to-English (aren),
Chinese-to-English (zhen), and English-to-Chinese
(enzh) machine translation tasks.5 In all language
pairs, the parallel and monolingual data consists of
all the allowed training sets in the constrained track.
For each language pair, we use two development
sets: one for Minimum Error Rate Training (Och,
2003; Macherey et al, 2008), and the other for tun-
ing the scale factor for MBR decoding. Our devel-
opment sets consists of the NIST 2004/2003 evalu-
ation sets for both aren and zhen, and NIST 2006
(NIST portion)/2003 evaluation sets for enzh. We
report results on NIST 2008 which is our blind test
set. Statistics computed over these data sets are re-
ported in Table 1.
5http://www.nist.gov/speech/tests/mt/
7.2 MT System Description
Our phrase-based statistical MT system is similar to
the alignment template system described in Och and
Ney (2004). The system is trained on parallel cor-
pora allowed in the constrained track. We first per-
form sentence and sub-sentence chunk alignment on
the parallel documents. We then train word align-
ment models (Och and Ney, 2003) using 6 Model-1
iterations and 6 HMM iterations. An additional 2 it-
erations of Model-4 are performed for zhen and enzh
pairs. Word Alignments in both source-to-target
and target-to-source directions are obtained using
the Maximum A-Posteriori (MAP) framework (Ma-
tusov et al, 2004). An inventory of phrase-pairs
up to length 5 is then extracted from the union of
source-target and target-source alignments. Several
feature functions are then computed over the phrase-
pairs. 5-gram word language models are trained on
the allowed monolingual corpora. Minimum Error
Rate Training under BLEU is used for estimating
approximately 20 feature function weights over the
dev1 development set.
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004) using two decoding passes. The first de-
coder pass generates either a lattice or anN -best list.
MBR decoding is performed in the second pass. The
MBR scaling parameter (? in Equation 3) is tuned
on the dev2 development set.
7.3 Translation Results
We next report translation results from lattice MBR
decoding. All results will be presented on the NIST
2008 evaluation sets. We report results using the
NIST implementation of the BLEU score which
computes the brevity penalty using the shortest ref-
erence translation for each segment (NIST, 2002
2008). The BLEU scores are reported at the word-
level for aren and zhen but at the character level for
enzh. We measure statistical significance using 95%
confidence intervals computed with paired bootstrap
resampling (Koehn, 2004). In all tables, systems in a
column show statistically significant differences un-
less marked with an asterisk.
We first compare lattice MBR toN -best MBR de-
coding and MAP decoding (Table 2). In these ex-
periments, we hold the likelihood scaling factor ? a
625
BLEU(%)
aren zhen enzh
MAP 43.7 27.9 41.4
N -best MBR 43.9 28.3? 42.0
Lattice MBR 44.9 28.5? 42.6
Table 2: Lattice MBR, N -best MBR & MAP decoding.
On zhen, Lattice MBR and N -best MBR do not show
statistically significant differences.
constant; it is set to 0.2 for aren and enzh, and 0.1
for zhen. The translation lattices are pruned using
Forward-Backward pruning (Sixtus and Ortmanns,
1999) so that the average numbers of arcs per word
(lattice density) is 30. For N -best MBR, we use
N -best lists of size 1000. To match the loss func-
tion, Lattice MBR is performed at the word level for
aren/zhen and at the character level for enzh. Our
lattice MBR is implemented using the Google Open-
Fst library.6 In our experiments, p, r (Equation 12)
have values of 0.85/0.72, 0.80/0.62, and 0.63/0.48
for aren, zhen, and enzh respectively.
We note that Lattice MBR provides gains of 0.2-
1.0 BLEU points over N -best MBR, which in turn
gives 0.2-0.6 BLEU points over MAP. These gains
are obtained on top of a baseline system that has
competitive performance relative to the results re-
ported in the NIST 2008 Evaluation.7 This demon-
strates the effectiveness of lattice MBR decoding as
a realization of MBR decoding which yields sub-
stantial gains over the N -best implementation.
The gains from lattice MBR over N -best MBR
could be due to a combination of factors. These in-
clude: 1) better approximation of the corpus BLEU
score, 2) larger hypothesis space, and 3) larger evi-
dence space. We now present experiments to tease
apart these factors.
Our first experiment restricts both the hypothesis
and evidence spaces in lattice MBR to the 1000-best
list (Table 3). We compare this toN -best MBRwith:
a) sentence-level BLEU, and b) sentence-level log
BLEU.
The results show that when restricted to the 1000-
best list, Lattice MBR performs slightly better than
N -best MBR (with sentence BLEU) on aren/enzh
while N -best MBR is better on zhen. We hypothe-
6http://www.openfst.org/
7
http://www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
BLEU(%)
aren zhen enzh
Lattice MBR, Lin. Corpus BLEU 44.2 28.1 42.2
N -best MBR, Sent. BLEU 43.9? 28.3? 42.0?
N -best MBR, Sent. Log BLEU 44.0? 28.3? 41.9?
Table 3: Lattice and N-best MBR (with Sentence
BLEU/Sentence log BLEU) on a 1000-best list. In each
column, entries with an asterisk do not show statistically
significant differences.
BLEU(%)
Hyp Space Evid Space aren zhen enzh
Lattice Lattice 44.9 28.5 42.6
1000-best Lattice 44.6 28.5 42.6
Lattice 1000-best 44.1? 28.0? 42.1
1000-best 1000-best 44.2? 28.1? 42.2
Table 4: Lattice MBR with restrictions on hypothesis and
evidence spaces. In each column, entries with an asterisk
do not show statistically significant differences.
size that on aren/enzh, the linear corpus BLEU gain
(Equation 10) is better correlated to the actual cor-
pus BLEU than sentence-level BLEU while the op-
posite is true on zhen. N -best MBR gives similar
results with either sentence BLEU or sentence log
BLEU. This confirms that using a log BLEU score
does not change the outcome of MBR decoding and
further justifies our Taylor-series approximation of
the log BLEU score.
We next attempt to understand factors 2 and 3. To
do that, we carry out lattice MBR when either the
hypothesis or the evidence space in Equation 2 is re-
stricted to 1000-best hypotheses (Table 4). For com-
parison, we also include results from lattice MBR
when both hypothesis and evidence spaces are iden-
tical: either the full lattice or the 1000-best list (from
Tables 2 and 3).
These results show that lattice MBR results are
almost unchanged when the hypothesis space is re-
stricted to a 1000-best list. However, when the ev-
idence space is shrunk to a 1000-best list, there is
a significant degradation in performance; these lat-
ter results are almost identical to the scenario when
both evidence and hypothesis spaces are restricted
to the 1000-best list. This experiment throws light
on what makes lattice MBR effective over N -best
MBR. Relative to the N -best list, the translation lat-
tice provides a better estimate of the expected BLEU
score. On the other hand, there are few hypotheses
626
outside the 1000-best list which are selected by lat-
tice MBR.
Finally, we show how the performance of lattice
MBR changes as a function of the lattice density.
The lattice density is the average number of arcs per
word and can be varied using Forward-Backward
pruning (Sixtus and Ortmanns, 1999). Figure 2 re-
ports the average number of lattice paths and BLEU
scores as a function of lattice density. The results
show that Lattice MBR performance generally im-
proves when the size of the lattice is increased.
However, on zhen, there is a small drop beyond a
density of 10. This could be due to low quality (low
posterior probability) hypotheses that get included at
the larger densities and result in a poorer estimate of
the expected BLEU score. On aren and enzh, there
are some gains beyond a lattice density of 30. These
gains are relatively small and come at the expense
of higher memory usage; we therefore work with a
lattice density of 30 in all our experiments. We note
that Lattice MBR is operating over lattices which are
gigantic in comparison to the number of paths in an
N -best list. At a lattice density of 30, the lattices in
aren contain on an average about 1081 hypotheses!
7.4 Lattice MBR Scale Factor
We next examine the role of the scale factor ? in
lattice MBR decoding. The MBR scale factor de-
termines the flatness of the posterior distribution
(Equation 3). It is chosen using a grid search on the
dev2 set (Table 1). Figure 3 shows the variation in
BLEU scores on eval08 as this parameter is varied.
The results show that it is important to tune this fac-
tor. The optimal scale factor is identical for all three
language pairs. In experiments not reported in this
paper, we have found that the optimal scaling factor
on a moderately sized development set carries over
to unseen test sets.
7.5 Maximum n-gram Order
Lattice MBR Decoding (Equation 6) involves com-
puting a posterior probability for each n-gram in the
lattice. We would like to speed up the Lattice MBR
computation (Section 4) by restricting the maximum
order of the n-grams in the procedure. The results
(Table 5) show that on aren, there is no degradation
if we limit the maximum order of the n-grams to
3. However, on zhen/enzh, there is improvement by
BLEU(%)
Max n-gram order aren zhen enzh
1 38.7 26.8 40.0
2 44.1 27.4 42.2
3 44.9 28.0 42.4
4 44.9 28.5 42.6
Table 5: Lattice MBR as a function of max n-gram order.
considering 4-grams. We can therefore reduce Lat-
tice MBR computations in aren.
8 Discussion
We have presented a procedure for performing Min-
imum Bayes-Risk Decoding on translation lattices.
This is a significant development in that the MBR
decoder operates over a very large number of trans-
lations. In contrast, the current N -best implementa-
tion of MBR can be scaled to, at most, a few thou-
sands of hypotheses. If the number of hypotheses
is greater than, say 20,000, the N -best MBR be-
comes computationally expensive. The lattice MBR
technique is efficient when performed over enor-
mous number of hypotheses (up to 1080) since it
takes advantage of the compact structure of the lat-
tice. Lattice MBR gives consistent improvements in
translation performance over N -best MBR decod-
ing, which is used in many state-of-the-art research
translation systems. Moreover, we see gains on three
different language pairs.
There are two potential reasons why Lattice MBR
decoding could outperform N -best MBR: a larger
hypothesis space from which translations could be
selected or a larger evidence space for computing the
expected loss. Our experiments show that the main
improvement comes from the larger evidence space:
a larger set of translations in the lattice provides a
better estimate of the expected BLEU score. In other
words, the lattice provides a better posterior distri-
bution over translation hypotheses relative to an N -
best list. This is a novel insight into the workings
of MBR decoding. We believe this could be possi-
bly employed when designing discriminative train-
ing approaches for machine translation. More gener-
ally, we have found a component in machine transla-
tion where the posterior distribution over hypotheses
plays a crucial role.
We have shown the effect of the MBR scaling fac-
627
10 20 30 40
44
44.2
44.4
44.6
44.8
45 aren
         Lattice Density
33
85
121
161
187
208
B
L
E
U
(
%
)
10 20 30 4028.2
28.3
28.4
28.5
28.6
28.7 zhen
Lattice Density
6
22
37
49
59
65
10 20 30 4041.8
42
42.2
42.4
42.6 enzh
      Lattice Density
3
10
17
25
30
34
Figure 2: Lattice MBR vs. lattice density: aren/zhen/enzh. Each point also shows the loge(Avg. # of paths).
0 0.2 0.4 0.6 0.8 1
44
44.2
44.4
44.6
44.8 aren
Scale Factor 
B
L
E
U
(
%
)
0 0.2 0.4 0.6 0.8 127.9
28
28.1
28.2
28.3
28.4
28.5 zhen
Scale Factor
0 0.2 0.4 0.6 0.8 1
41.8
42
42.2
42.4
42.6
42.8 enzh
Scale Factor
Figure 3: Lattice MBR with various scale factors ?: aren/zhen/enzh.
tor on the performance of lattice MBR. The scale
factor determines the flatness of the posterior distri-
bution over translation hypotheses. A scale of 0.0
means a uniform distribution while 1.0 implies that
there is no scaling. This is an important parameter
that needs to be tuned on a development set. There
has been prior work in MBR speech recognition and
machine translation (Goel and Byrne, 2000; Ehling
et al, 2007) which has shown the need for tuning
this factor. Our MT system parameters are trained
with Minimum Error Rate Training which assigns a
very high posterior probability to the MAP transla-
tion. As a result, it is necessary to flatten the prob-
ability distribution so that MBR decoding can select
hypotheses other than the MAP hypothesis.
Our Lattice MBR implementation is made pos-
sible due to the linear approximation of the BLEU
score. This linearization technique has been applied
elsewhere when working with BLEU: Smith and
Eisner (2006) approximate the expectation of log
BLEU score. In both cases, a linear metric makes
it easier to compute the expectation. While we have
applied lattice MBR decoding to the approximate
BLEU score, we note that our procedure (Section 3)
is applicable to other gain functions which can be
decomposed as a sum of local gain functions. In par-
ticular, our framework might be useful with transla-
tion metrics such as TER (Snover et al, 2006) or
METEOR (Lavie and Agarwal, 2007).
In contrast to a phrase-based SMT system, a syn-
tax based SMT system (e.g. Zollmann and Venu-
gopal (2006)) can generate a hypergraph that rep-
resents a generalized translation lattice with words
and hidden tree structures. We believe that our lat-
tice MBR framework can be extended to such hy-
pergraphs with loss functions that take into account
both BLEU scores as well as parse tree structures.
Lattice and Forest based search and training pro-
cedures are not yet common in statistical machine
translation. However, they are promising because
the search space of translations is much larger than
the typical N -best list (Mi et al, 2008). We hope
that our approach will provide some insight into the
design of lattice-based search procedures along with
the use of non-linear, global loss functions such as
BLEU.
References
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J . Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.
628
Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79?85.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In SSST, NAACL-HLT
2007, pages 103?110, Rochester, NY, USA, April.
N. Ehling, R. Zens, and H. Ney. 2007. Minimum Bayes
Risk Decoding for BLEU. In ACL 2007, pages 101?
104, Prague, Czech Republic, June.
V. Goel and W. Byrne. 2000. Minimum Bayes-Risk Au-
tomatic Speech Recognition. Computer Speech and
Language, 14(2):115?135.
V. Goel. 2001. Minimum Bayes-Risk Automatic Speech
Recognition. Ph.D. thesis, Johns Hopkins University,
Baltimore, MD, USA.
J. Goodman. 1996. Parsing Algorithms and Metrics. In
ACL, pages 177?183, Santa Cruz, CA, USA.
L. Karttunen, J-p. Chanod, G. Grefenstette, and
A. Schiller. 1996. Regular Expressions for Language
Engineering. Natural Language Engineering, 2:305?
328.
P. Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In EMNLP, Barcelona,
Spain.
S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk
word alignments of bilingual texts. In EMNLP, pages
140?147, Philadelphia, PA, USA.
S. Kumar and W. Byrne. 2004. Minimum Bayes-Risk
Decoding for Statistical Machine Translation. In HLT-
NAACL, pages 169?176, Boston, MA, USA.
A. Lavie and A. Agarwal. 2007. METEOR: An Auto-
matic Metric for MT Evaluation with High Levels of
Correlation with Human Judgments. In SMT Work-
shop, ACL, pages 228?231, Prague, Czech Republic.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based Minimum Error Rate Training for Sta-
tistical Machine Translation. In EMNLP, Honolulu,
Hawaii, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Translation.
In COLING, Geneva, Switzerland.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based Trans-
lation. In ACL, Columbus, OH, USA.
M.Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(3).
M. Mohri. 2002. Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3):321?350.
NIST. 2002-2008. The NIST Machine Translation Eval-
uations. http://www.nist.gov/speech/tests/mt/.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 ? 51.
F. Och and H. Ney. 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computa-
tional Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statisti-
cal Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Technical Report RC22176 (W0109-
022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
D. Smith and J. Eisner. 2006. Minimum Risk Anneal-
ing for Training Log-Linear Models. In ACL, Sydney,
Australia.
D. Smith and N. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In EMNLP-CoNLL,
Prague, Czech Republic.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA, Boston,
MA, USA.
I. Titov and J. Henderson. 2006. Loss Minimization in
Parse Reranking. In EMNLP, Sydney, Australia.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
Word Graphs in Statistical Machine Translation. In
EMNLP, Philadelphia, PA, USA.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass De-
coding for Synchronous Context Free Grammars. In
ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
629
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163?171,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Efficient Minimum Error Rate Training and
Minimum Bayes-Risk Decoding for
Translation Hypergraphs and Lattices
Shankar Kumar1 and Wolfgang Macherey1 and Chris Dyer2 and Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,wmach,och}@google.com
2Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony@umd.edu
Abstract
Minimum Error Rate Training (MERT)
and Minimum Bayes-Risk (MBR) decod-
ing are used in most current state-of-the-
art Statistical Machine Translation (SMT)
systems. The algorithms were originally
developed to work with N -best lists of
translations, and recently extended to lat-
tices that encode many more hypotheses
than typical N -best lists. We here extend
lattice-based MERT and MBR algorithms
to work with hypergraphs that encode a
vast number of translations produced by
MT systems based on Synchronous Con-
text Free Grammars. These algorithms
are more efficient than the lattice-based
versions presented earlier. We show how
MERT can be employed to optimize pa-
rameters for MBR decoding. Our exper-
iments show speedups from MERT and
MBR as well as performance improve-
ments from MBR decoding on several lan-
guage pairs.
1 Introduction
Statistical Machine Translation (SMT) systems
have improved considerably by directly using the
error criterion in both training and decoding. By
doing so, the system can be optimized for the
translation task instead of a criterion such as like-
lihood that is unrelated to the evaluation met-
ric. Two popular techniques that incorporate the
error criterion are Minimum Error Rate Train-
ing (MERT) (Och, 2003) and Minimum Bayes-
Risk (MBR) decoding (Kumar and Byrne, 2004).
These two techniques were originally developed
for N -best lists of translation hypotheses and re-
cently extended to translation lattices (Macherey
et al, 2008; Tromble et al, 2008) generated by a
phrase-based SMT system (Och and Ney, 2004).
Translation lattices contain a significantly higher
number of translation alternatives relative to N -
best lists. The extension to lattices reduces the
runtimes for both MERT and MBR, and gives per-
formance improvements from MBR decoding.
SMT systems based on synchronous context
free grammars (SCFG) (Chiang, 2007; Zollmann
and Venugopal, 2006; Galley et al, 2006) have
recently been shown to give competitive perfor-
mance relative to phrase-based SMT. For these
systems, a hypergraph or packed forest provides a
compact representation for encoding a huge num-
ber of translation hypotheses (Huang, 2008).
In this paper, we extend MERT and MBR
decoding to work on hypergraphs produced by
SCFG-based MT systems. We present algorithms
that are more efficient relative to the lattice al-
gorithms presented in Macherey et al (2008;
Tromble et al (2008). Lattice MBR decoding uses
a linear approximation to the BLEU score (Pap-
ineni et al, 2001); the weights in this linear loss
are set heuristically by assuming that n-gram pre-
cisions decay exponentially with n. However, this
may not be optimal in practice. We employ MERT
to select these weights by optimizing BLEU score
on a development set.
A related MBR-inspired approach for hyper-
graphs was developed by Zhang and Gildea
(2008). In this work, hypergraphs were rescored to
maximize the expected count of synchronous con-
stituents in the translation. In contrast, our MBR
algorithm directly selects the hypothesis in the
hypergraph with the maximum expected approx-
imate corpus BLEU score (Tromble et al, 2008).
will soon announce 
X1  X2
X1  X2
X1  X2
X1  X2
X1  X2
X1 its future in the 
X1 its future in the 
Suzuki
soon
its future in
X1 announces
Rally World Championship
Figure 1: An example hypergraph.
163
2 Translation Hypergraphs
A translation lattice compactly encodes a large
number of hypotheses produced by a phrase-based
SMT system. The corresponding representation
for an SMT system based on SCFGs (e.g. Chi-
ang (2007), Zollmann and Venugopal (2006), Mi
et al (2008)) is a directed hypergraph or a packed
forest (Huang, 2008).
Formally, a hypergraph is a pair H = ?V, E?
consisting of a vertex set V and a set of hyperedges
E ? V? ? V . Each hyperedge e ? E connects a
head vertex h(e) with a sequence of tail vertices
T (e) = {v1, ..., vn}. The number of tail vertices
is called the arity (|e|) of the hyperedge. If the ar-
ity of a hyperedge is zero, h(e) is called a source
vertex. The arity of a hypergraph is the maximum
arity of its hyperedges. A hyperedge of arity 1 is a
regular edge, and a hypergraph of arity 1 is a regu-
lar graph (lattice). Each hyperedge is labeled with
a rule re from the SCFG. The number of nontermi-
nals on the right-hand side of re corresponds with
the arity of e. An example without scores is shown
in Figure 1. A path in a translation hypergraph in-
duces a translation hypothesis E along with its se-
quence of SCFG rules D = r1, r2, ..., rK which,
if applied to the start symbol, derives E. The se-
quence of SCFG rules induced by a path is also
called a derivation tree for E.
3 Minimum Error Rate Training
Given a set of source sentences FS1 with corre-
sponding reference translations RS1 , the objective
of MERT is to find a parameter set ??M1 which min-
imizes an automated evaluation criterion under a
linear model:
??M1 = argmin
?M1
? SX
s=1
Err
`
Rs, E?(Fs; ?
M
1 )
?
ff
E?(Fs; ?
M
1 ) = argmax
E
? SX
s=1
?mhm(E, Fs)
ff
.
In the context of statistical machine translation,
the optimization procedure was first described in
Och (2003) for N -best lists and later extended to
phrase-lattices in Macherey et al (2008). The al-
gorithm is based on the insight that, under a log-
linear model, the cost function of any candidate
translation can be represented as a line in the plane
if the initial parameter set ?M1 is shifted along a
direction dM1 . Let C = {E1, ..., EK} denote a set
of candidate translations, then computing the best
scoring translation hypothesis E? out of C results in
the following optimization problem:
E?(F ; ?) = argmax
E?C
n
(?M1 + ? ? d
M
1 )
> ? hM1 (E,F )
o
= argmax
E?C
?
X
m
?mhm(E,F )
| {z }
=a(E,F )
+ ? ?
X
m
dmhm(E,F )
| {z }
=b(E,F )
ff
= argmax
E?C
?
a(E,F ) + ? ? b(E,F )
| {z }
(?)
?
Hence, the total score (?) for each candidate trans-
lation E ? C can be described as a line with
? as the independent variable. For any particu-
lar choice of ?, the decoder seeks that translation
which yields the largest score and therefore corre-
sponds to the topmost line segment. If ? is shifted
from ?? to +?, other translation hypotheses
may at some point constitute the topmost line seg-
ments and thus change the decision made by the
decoder. The entire sequence of topmost line seg-
ments is called upper envelope and provides an ex-
haustive representation of all possible outcomes
that the decoder may yield if ? is shifted along
the chosen direction. Both the translations and
their corresponding line segments can efficiently
be computed without incorporating any error crite-
rion. Once the envelope has been determined, the
translation candidates of its constituent line seg-
ments are projected onto their corresponding error
counts, thus yielding the exact and unsmoothed er-
ror surface for all candidate translations encoded
in C. The error surface can now easily be traversed
in order to find that ?? under which the new param-
eter set ?M1 + ?? ? d
M
1 minimizes the global error.
In this section, we present an extension of the
algorithm described in Macherey et al (2008)
that allows us to efficiently compute and repre-
sent upper envelopes over all candidate transla-
tions encoded in hypergraphs. Conceptually, the
algorithm works by propagating (initially empty)
envelopes from the hypergraph?s source nodes
bottom-up to its unique root node, thereby ex-
panding the envelopes by applying SCFG rules to
the partial candidate translations that are associ-
ated with the envelope?s constituent line segments.
To recombine envelopes, we need two operators:
the sum and the maximum over convex polygons.
To illustrate which operator is applied when, we
transform H = ?V, E? into a regular graph with
typed nodes by (1) marking all vertices v ? V with
the symbol ? and (2) replacing each hyperedge
e ? E , |e| > 1, with a small subgraph consisting
of a new vertex v?(e) whose incoming and out-
going edges connect the same head and tail nodes
164
Algorithm 1 ?-operation (Sum)
input: associative map a: V ? Env(V), hyperarc e
output: Minkowski sum of envelopes over T (e)
for (i = 0; i < |T (e)|; ++i) {
v = Ti(e);
pq.enqueue(? v, i, 0?);
}
L = ?;
D = ? e, ?1 ? ? ? ?|e|?
while (!pq.empty()) {
? v, i, j? = pq.dequeue();
` = A[v][j];
D[i+1] = `.D;
if (L.empty() ? L.back().x < `.x) {
if (0 < j) {
`.y += L.back().y - A[v][j-1].y;
`.m += L.back().m - A[v][j-1].m;
}
L.push_back(`);
L.back().D = D;
} else {
L.back().y += `.y;
L.back().m += `.m;
L.back().D[i+1] = `.D;
if (0 < j) {
L.back().y -= A[v][j-1].y;
L.back().m -= A[v][j-1].m;
}
}
if (++j < A[v].size())
pq.enqueue(? v, i, j?);
}
return L;
in the transformed graph as were connected by e
in the original graph. The unique outgoing edge
of v?(e) is associated with the rule re; incoming
edges are not linked to any rule. Figure 2 illus-
trates the transformation for a hyperedge with ar-
ity 3. The graph transformation is isomorphic.
The rules associated with every hyperedge spec-
ify how line segments in the envelopes of a hyper-
edge?s tail nodes can be combined. Suppose we
have a hyperedge e with rule re : X ? aX1bX2c
and T (e) = {v1, v2}. Then we substitute X1 and
X2 in the rule with candidate translations associ-
ated with line segments in envelopes Env(v1) and
Env(v2) respectively.
To derive the algorithm, we consider the gen-
eral case of a hyperedge e with rule re : X ?
w1X1w2...wnXnwn+1. Because the right-hand
side of re has n nonterminals, the arity of e is
|e| = n. Let T (e) = {v1, ..., vn} denote the
tail nodes of e. We now assume that each tail
node vi ? T (e) is associated with the upper en-
velope over all candidate translations that are in-
duced by derivations of the corresponding nonter-
minal symbol Xi. These envelopes shall be de-
Algorithm 2 ?-operation (Max)
input: array L[0..K-1] containing line objects
output: upper envelope of L
Sort(L:m);
j = 0; K = size(L);
for (i = 0; i < K; ++i) {
` = L[i];
`.x = -?;
if (0 < j) {
if (L[j-1].m == `.m) {
if (`.y <= L[j-1].y) continue;
--j;
}
while (0 < j) {
`.x = (`.y - L[j-1].y)/
(L[j-1].m - `.m);
if (L[j-1].x < `.x) break;
--j;
}
if (0 == j) `.x = -?;
L[j++] = `;
} else L[j++] = `;
}
L.resize(j);
return L;
noted by Env(vi). To decompose the problem of
computing and propagating the tail envelopes over
the hyperedge e to its head node, we now define
two operations, one for either node type, to spec-
ify how envelopes associated with the tail vertices
are propagated to the head vertex.
Nodes of Type ???: For a type ? node, the
resulting envelope is the Minkowski sum over
the envelopes of the incoming edges (Berg et
al., 2008). Since the envelopes of the incoming
edges are convex hulls, the Minkowski sum pro-
vides an upper bound to the number of line seg-
ments that constitute the resulting envelope: the
bound is the sum over the number of line seg-
ments in the envelopes of the incoming edges, i.e.:?
?Env(v?(e))
?
? ?
?
v??T (e)
?
?Env(v?)
?
?.
Algorithm 1 shows the pseudo code for comput-
ing the Minkowski sum over multiple envelopes.
The line objects ` used in this algorithm are
encoded as 4-tuples, each consisting of the x-
intercept with `?s left-adjacent line stored as `.x,
the slope `.m, the y-intercept `.y, and the (partial)
derivation tree `.D. At the beginning, the leftmost
line segment of each envelope is inserted into a
priority queue pq. The priority is defined in terms
of a line?s x-intercept such that lower values imply
higher priority. Hence, the priority queue enumer-
ates all line segments from left to right in ascend-
ing order of their x-intercepts, which is the order
needed to compute the Minkowski sum.
Nodes of Type ???: The operation performed
165
=?
= max
Figure 2: Transformation of a hypergraph into
a factor graph and bottom-up propagation of en-
velopes.
at nodes of type ??? computes the convex hull
over the union of the envelopes propagated over
the incoming edges. This operation is a ?max?
operation and it is identical to the algorithm de-
scribed in (Macherey et al, 2008) for phrase lat-
tices. Algorithm 2 contains the pseudo code.
The complete algorithm then works as follows:
Traversing all nodes in H bottom-up in topolog-
ical order, we proceed for each node v ? V over
its incoming hyperedges and combine in each such
hyperedge e the envelopes associated with the tail
nodes T (e) by computing their sum according to
Algorithm 1 (?-operation). For each incoming
hyperedge e, the resulting envelope is then ex-
panded by applying the rule re to its constituent
line segments. The envelopes associated with dif-
ferent incoming hyperedges of node v are then
combined and reduced according to Algorithm 2
(?-operation). By construction, the envelope at
the root node is the convex hull over the line seg-
ments of all candidate translations that can be de-
rived from the hypergraph.
The suggested algorithm has similar properties
as the algorithm presented in (Macherey et al,
2008). In particular, it has the same upper bound
on the number of line segments that constitute the
envelope at the root node, i.e, the size of this enve-
lope is guaranteed to be no larger than the number
of edges in the transformed hypergraph.
4 Minimum Bayes-Risk Decoding
We first review Minimum Bayes-Risk (MBR) de-
coding for statistical MT. An MBR decoder seeks
the hypothesis with the least expected loss under a
probability model (Bickel and Doksum, 1977). If
we think of statistical MT as a classifier that maps
a source sentence F to a target sentence E, the
MBR decoder can be expressed as follows:
E? = argmin
E??G
?
E?G
L(E,E?)P (E|F ), (1)
where L(E,E?) is the loss between any two hy-
potheses E and E?, P (E|F ) is the probability
model, and G is the space of translations (N -best
list, lattice, or a hypergraph).
MBR decoding for translation can be performed
by reranking an N -best list of hypotheses gener-
ated by an MT system (Kumar and Byrne, 2004).
This reranking can be done for any sentence-
level loss function such as BLEU (Papineni et al,
2001), Word Error Rate, or Position-independent
Error Rate.
Recently, Tromble et al (2008) extended
MBR decoding to translation lattices under an
approximate BLEU score. They approximated
log(BLEU) score by a linear function of n-gram
matches and candidate length. If E and E? are the
reference and the candidate translations respec-
tively, this linear function is given by:
G(E,E?) = ?0|E
?|+
?
w
?|w|#w(E
?)?w(E), (2)
where w is an n-gram present in either E or E?,
and ?0, ?1, ..., ?N are weights which are deter-
mined empirically, where N is the maximum n-
gram order.
Under such a linear decomposition, the MBR
decoder (Equation 1) can be written as
E? = argmax
E??G
?0|E
?|+
?
w
?|w|#w(E
?)p(w|G), (3)
where the posterior probability of an n-gram in the
lattice is given by
p(w|G) =
?
E?G
1w(E)P (E|F ). (4)
Tromble et al (2008) implement the MBR
decoder using Weighted Finite State Automata
(WFSA) operations. First, the set of n-grams
is extracted from the lattice. Next, the posterior
probability of each n-gram is computed. A new
automaton is then created by intersecting each n-
gram with weight (from Equation 2) to an un-
weighted lattice. Finally, the MBR hypothesis is
extracted as the best path in the automaton. We
will refer to this procedure as FSAMBR.
The above steps are carried out one n-gram at
a time. For a moderately large lattice, there can
be several thousands of n-grams and the proce-
dure becomes expensive. We now present an alter-
nate approximate procedure which can avoid this
166
enumeration making the resulting algorithm much
faster than FSAMBR.
4.1 Efficient MBR for lattices
The key idea behind this new algorithm is to
rewrite the n-gram posterior probability (Equa-
tion 4) as follows:
p(w|G) =
?
E?G
?
e?E
f(e, w,E)P (E|F ) (5)
where f(e, w,E) is a score assigned to edge e on
path E containing n-gram w:
f(e, w,E) =
?
?
?
1 w ? e, p(e|G) > p(e?|G),
e? precedes e on E
0 otherwise
(6)
In other words, for each pathE, we count the edge
that contributes n-gramw and has the highest edge
posterior probability relative to its predecessors on
the path E; there is exactly one such edge on each
lattice path E.
We note that f(e, w,E) relies on the full path
E which means that it cannot be computed based
on local statistics. We therefore approximate the
quantity f(e, w,E) with f?(e, w,G) that counts
the edge e with n-gram w that has the highest arc
posterior probability relative to predecessors in the
entire lattice G. f?(e, w,G) can be computed lo-
cally, and the n-gram posterior probability based
on f? can be determined as follows:
p(w|G) =
X
E?G
X
e?E
f?(e, w,G)P (E|F ) (7)
=
X
e?E
1w?ef
?(e, w,G)
X
E?G
1E(e)P (E|F )
=
X
e?E
1w?ef
?(e, w,G)P (e|G),
where P (e|G) is the posterior probability of a lat-
tice edge. The algorithm to perform Lattice MBR
is given in Algorithm 3. For each node t in the lat-
tice, we maintain a quantity Score(w, t) for each
n-gram w that lies on a path from the source node
to t. Score(w, t) is the highest posterior probabil-
ity among all edges on the paths that terminate on t
and contain n-gram w. The forward pass requires
computing the n-grams introduced by each edge;
to do this, we propagate n-grams (up to maximum
order ?1) terminating on each node.
4.2 Extension to Hypergraphs
We next extend the Lattice MBR decoding algo-
rithm (Algorithm 3) to rescore hypergraphs pro-
duced by a SCFG based MT system. Algorithm 4
is an extension to the MBR decoder on lattices
Algorithm 3 MBR Decoding on Lattices
1: Sort the lattice nodes topologically.
2: Compute backward probabilities of each node.
3: Compute posterior prob. of each n-gram:
4: for each edge e do
5: Compute edge posterior probability P (e|G).
6: Compute n-gram posterior probs. P (w|G):
7: for each n-gram w introduced by e do
8: Propagate n? 1 gram suffix to he.
9: if p(e|G) > Score(w, T (e)) then
10: Update posterior probs. and scores:
p(w|G) += p(e|G) ? Score(w, T (e)).
Score(w, he) = p(e|G).
11: else
12: Score(w, he) = Score(w, T (e)).
13: end if
14: end for
15: end for
16: Assign scores to edges (given by Equation 3).
17: Find best path in the lattice (Equation 3).
(Algorithm 3). However, there are important dif-
ferences when computing the n-gram posterior
probabilities (Step 3). In this inside pass, we now
maintain both n-gram prefixes and suffixes (up to
the maximum order?1) on each hypergraph node.
This is necessary because unlike a lattice, new n-
grams may be created at subsequent nodes by con-
catenating words both to the left and the right side
of the n-gram. When the arity of the edge is 2,
a rule has the general form aX1bX2c, where X1
and X2 are sequences from tail nodes. As a result,
we need to consider all new sequences which can
be created by the cross-product of the n-grams on
the two tail nodes. E.g. if X1 = {c, cd, d} and
X2 = {f, g}, then a total of six sequences will
result. In practice, such a cross-product is not pro-
Algorithm 4 MBR Decoding on Hypergraphs
1: Sort the hypergraph nodes topologically.
2: Compute inside probabilities of each node.
3: Compute posterior prob. of each hyperedge P (e|G).
4: Compute posterior prob. of each n-gram:
5: for each hyperedge e do
6: Merge the n-grams on the tail nodes T (e). If the
same n-gram is present on multiple tail nodes, keep
the highest score.
7: Apply the rule on e to the n-grams on T (e).
8: Propagate n? 1 gram prefixes/suffixes to he.
9: for each n-gram w introduced by this hyperedge do
10: if p(e|G) > Score(w, T (e)) then
11: p(w|G) += p(e|G) ? Score(w, T (e))
Score(w, he) = p(e|G)
12: else
13: Score(w, he) = Score(w, T (e))
14: end if
15: end for
16: end for
17: Assign scores to hyperedges (Equation 3).
18: Find best path in the hypergraph (Equation 3).
167
hibitive when the maximum n-gram order in MBR
does not exceed the order of the n-gram language
model used in creating the hypergraph. In the lat-
ter case, we will have a small set of unique prefixes
and suffixes on the tail nodes.
5 MERT for MBR Parameter
Optimization
Lattice MBR Decoding (Equation 3) assumes a
linear form for the gain function (Equation 2).
This linear function contains n + 1 parameters
?0, ?1, ..., ?N , where N is the maximum order of
the n-grams involved. Tromble et al (2008) ob-
tained these factors as a function of n-gram preci-
sions derived from multiple training runs. How-
ever, this does not guarantee that the resulting
linear score (Equation 2) is close to the corpus
BLEU. We now describe how MERT can be used
to estimate these factors to achieve a better ap-
proximation to the corpus BLEU.
We recall that MERT selects weights in a lin-
ear model to optimize an error criterion (e.g. cor-
pus BLEU) on a training set. The lattice MBR
decoder (Equation 3) can be written as a lin-
ear model: E? = argmaxE??G
?N
i=0 ?igi(E
?, F ),
where g0(E?, F ) = |E?| and gi(E?, F ) =?
w:|w|=i #w(E
?)p(w|G).
The linear approximation to BLEU may not
hold in practice for unseen test sets or language-
pairs. Therefore, we would like to allow the de-
coder to backoff to the MAP translation in such
cases. To do that, we introduce an additional fea-
ture function gN+1(E,F ) equal to the original de-
coder cost for this sentence. A weight assignment
of 1.0 for this feature function and zeros for the
other feature functions would imply that the MAP
translation is chosen. We now have a total ofN+2
feature functions which we optimize using MERT
to obtain highest BLEU score on a training set.
6 Experiments
We now describe our experiments to evaluate
MERT and MBR on lattices and hypergraphs, and
show how MERT can be used to tune MBR pa-
rameters.
6.1 Translation Tasks
We report results on two tasks. The first one is
the constrained data track of the NIST Arabic-
to-English (aren) and Chinese-to-English (zhen)
translation task1. On this task, the parallel and the
1http://www.nist.gov/speech/tests/mt
Dataset # of sentences
aren zhen
dev 1797 1664
nist02 1043 878
nist03 663 919
Table 1: Statistics over the NIST dev/test sets.
monolingual data included all the allowed train-
ing sets for the constrained track. Table 1 reports
statistics computed over these data sets. Our de-
velopment set (dev) consists of the NIST 2005 eval
set; we use this set for optimizing MBR parame-
ters. We report results on NIST 2002 and NIST
2003 evaluation sets.
The second task consists of systems for 39
language-pairs with English as the target language
and trained on at most 300M word tokens mined
from the web and other published sources. The de-
velopment and test sets for this task are randomly
selected sentences from the web, and contain 5000
and 1000 sentences respectively.
6.2 MT System Description
Our phrase-based statistical MT system is simi-
lar to the alignment template system described in
(Och and Ney, 2004; Tromble et al, 2008). Trans-
lation is performed using a standard dynamic pro-
gramming beam-search decoder (Och and Ney,
2004) using two decoding passes. The first de-
coder pass generates either a lattice or an N -best
list. MBR decoding is performed in the second
pass.
We also train two SCFG-based MT systems:
a hierarchical phrase-based SMT (Chiang, 2007)
system and a syntax augmented machine transla-
tion (SAMT) system using the approach described
in Zollmann and Venugopal (2006). Both systems
are built on top of our phrase-based systems. In
these systems, the decoder generates an initial hy-
pergraph or anN -best list, which are then rescored
using MBR decoding.
6.3 MERT Results
Table 2 shows runtime experiments for the hyper-
graph MERT implementation in comparison with
the phrase-lattice implementation on both the aren
and the zhen system. The first two columns show
the average amount of time in msecs that either
algorithm requires to compute the upper envelope
when applied to phrase lattices. Compared to the
algorithm described in (Macherey et al, 2008)
which is optimized for phrase lattices, the hyper-
graph implementation causes a small increase in
168
Avg. Runtime/sent [msec]
(Macherey 2008) Suggested Alg.
aren zhen aren zhen
phrase lattice 8.57 7.91 10.30 8.65
hypergraph ? ? 8.19 8.11
Table 2: Average time for computing envelopes.
running time. This increase is mainly due to the
representation of line segments; while the phrase-
lattice implementation stores a single backpointer,
the hypergraph version stores a vector of back-
pointers.
The last two columns show the average amount
of time that is required to compute the upper en-
velope on hypergraphs. For comparison, we prune
hypergraphs to the same density (# of edges per
edge on the best path) and achieve identical run-
ning times for computing the error surface.
6.4 MBR Results
We first compare the new lattice MBR (Algo-
rithm 3) with MBR decoding on 1000-best lists
and FSAMBR (Tromble et al, 2008) on lattices
generated by the phrase-based systems; evaluation
is done using both BLEU and average run-time per
sentence (Table 3). Note that N -best MBR uses
a sentence BLEU loss function. The new lattice
MBR algorithm gives about the same performance
as FSAMBR while yielding a 20X speedup.
We next report the performance of MBR on hy-
pergraphs generated by Hiero/SAMT systems. Ta-
ble 4 compares Hypergraph MBR (HGMBR) with
MAP and MBR decoding on 1000 best lists. On
some systems such as the Arabic-English SAMT,
the gains from Hypergraph MBR over 1000-best
MBR are significant. In other cases, Hypergraph
MBR performs at least as well as N -best MBR.
In all cases, we observe a 7X speedup in run-
time. This shows the usefulness of Hypergraph
MBR decoding as an efficient alternative to N -
best MBR.
6.5 MBR Parameter Tuning with MERT
We now describe the results by tuning MBR n-
gram parameters (Equation 2) using MERT. We
first compute N + 1 MBR feature functions on
each edge of the lattice/hypergraph. We also in-
clude the total decoder cost on the edge as as addi-
tional feature function. MERT is then performed
to optimize the BLEU score on a development set;
For MERT, we use 40 random initial parameters as
well as parameters computed using corpus based
statistics (Tromble et al, 2008).
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
MAP 54.2 64.2 40.1 39.0 -
N -best MBR 54.3 64.5 40.2 39.2 3.7
Lattice MBR
FSAMBR 54.9 65.2 40.6 39.5 3.7
LatMBR 54.8 65.2 40.7 39.4 0.2
Table 3: Lattice MBR for a phrase-based system.
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
Hiero
MAP 52.8 62.9 41.0 39.8 -
N -best MBR 53.2 63.0 41.0 40.1 3.7
HGMBR 53.3 63.1 41.0 40.2 0.5
SAMT
MAP 53.4 63.9 41.3 40.3 -
N -best MBR 53.8 64.3 41.7 41.1 3.7
HGMBR 54.0 64.6 41.8 41.1 0.5
Table 4: Hypergraph MBR for Hiero/SAMT systems.
Table 5 shows results for NIST systems. We
report results on nist03 set and present three sys-
tems for each language pair: phrase-based (pb),
hierarchical (hier), and SAMT; Lattice MBR is
done for the phrase-based system while HGMBR
is used for the other two. We select the MBR
scaling factor (Tromble et al, 2008) based on the
development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5
and 1.0 for the aren-phrase, aren-hier, aren-samt,
zhen-phrase zhen-hier and zhen-samt systems re-
spectively. For the multi-language case, we train
phrase-based systems and perform lattice MBR
for all language pairs. We use a scaling factor of
0.7 for all pairs. Additional gains can be obtained
by tuning this factor; however, we do not explore
that dimension in this paper. In all cases, we prune
the lattices/hypergraphs to a density of 30 using
forward-backward pruning (Sixtus and Ortmanns,
1999).
We consider a BLEU score difference to be a)
gain if is at least 0.2 points, b) drop if it is at most
-0.2 points, and c) no change otherwise. The re-
sults are shown in Table 6. In both tables, the fol-
lowing results are reported: Lattice/HGMBR with
default parameters (?5, 1.5, 2, 3, 4) computed us-
ing corpus statistics (Tromble et al, 2008),
Lattice/HGMBR with parameters derived from
MERT both without/with the baseline model cost
feature (mert?b, mert+b). For multi-language
systems, we only show the # of language-pairs
with gains/no-changes/drops for each MBR vari-
ant with respect to the MAP translation.
169
We observed in the NIST systems that MERT
resulted in short translations relative to MAP on
the unseen test set. To prevent this behavior,
we modify the MERT error criterion to include
a sentence-level brevity scorer with parameter ?:
BLEU+brevity(?). This brevity scorer penalizes
each candidate translation that is shorter than the
average length over its reference translations, us-
ing a penalty term which is linear in the difference
between either length. We tune ? on the develop-
ment set so that the brevity score of MBR transla-
tion is close to that of the MAP translation.
In the NIST systems, MERT yields small im-
provements on top of MBR with default param-
eters. This is the case for Arabic-English Hi-
ero/SAMT. In all other cases, we see no change
or even a slight degradation due to MERT.
We hypothesize that the default MBR parame-
ters (Tromble et al, 2008) are well tuned. There-
fore there is little gain by additional tuning using
MERT.
In the multi-language systems, the results show
a different trend. We observe that MBR with de-
fault parameters results in gains on 18 pairs, no
differences on 9 pairs, and losses on 12 pairs.
When we optimize MBR features with MERT, the
number of language pairs with gains/no changes/-
drops is 22/5/12. Thus, MERT has a bigger impact
here than in the NIST systems. We hypothesize
that the default MBR parameters are sub-optimal
for some language pairs and that MERT helps to
find better parameter settings. In particular, MERT
avoids the need for manually tuning these param-
eters by language pair.
Finally, when baseline model costs are added
as an extra feature (mert+b), the number of pairs
with gains/no changes/drops is 26/8/5. This shows
that this feature can allow MBR decoding to back-
off to the MAP translation. When MBR does not
produce a higher BLEU score relative to MAP
on the development set, MERT assigns a higher
weight to this feature function. We see such an
effect for 4 systems.
7 Discussion
We have presented efficient algorithms
which extend previous work on lattice-based
MERT (Macherey et al, 2008) and MBR de-
coding (Tromble et al, 2008) to work with
hypergraphs. Our new MERT algorithm can work
with both lattices and hypergraphs. On lattices, it
achieves similar run-times as the implementation
System BLEU (%)
MAP MBR
default mert-b mert+b
aren.pb 54.2 54.8 54.8 54.9
aren.hier 52.8 53.3 53.5 53.7
aren.samt 53.4 54.0 54.4 54.0
zhen.pb 40.1 40.7 40.7 40.9
zhen.hier 41.0 41.0 41.0 41.0
zhen.samt 41.3 41.8 41.6 41.7
Table 5: MBR Parameter Tuning on NIST systems
MBR wrt. MAP default mert-b mert+b
# of gains 18 22 26
# of no-changes 9 5 8
# of drops 12 12 5
Table 6: MBR on Multi-language systems.
described in Macherey et al (2008). The new
Lattice MBR decoder achieves a 20X speedup
relative to either FSAMBR implementation
described in Tromble et al (2008) or MBR on
1000-best lists. The algorithm gives comparable
results relative to FSAMBR. On hypergraphs
produced by Hierarchical and Syntax Augmented
MT systems, our MBR algorithm gives a 7X
speedup relative to 1000-best MBR while giving
comparable or even better performance.
Lattice MBR decoding is obtained under a lin-
ear approximation to BLEU, where the weights
are obtained using n-gram precisions derived from
development data. This may not be optimal in
practice for unseen test sets and language pairs,
and the resulting linear loss may be quite differ-
ent from the corpus level BLEU. In this paper, we
have described how MERT can be employed to
estimate the weights for the linear loss function
to maximize BLEU on a development set. On an
experiment with 40 language pairs, we obtain im-
provements on 26 pairs, no difference on 8 pairs
and drops on 5 pairs. This was achieved with-
out any need for manual tuning for each language
pair. The baseline model cost feature helps the al-
gorithm effectively back off to the MAP transla-
tion in language pairs where MBR features alone
would not have helped.
MERT and MBR decoding are popular tech-
niques for incorporating the final evaluation met-
ric into the development of SMT systems. We be-
lieve that our efficient algorithms will make them
more widely applicable in both SCFG-based and
phrase-based MT systems.
170
References
M. Berg, O. Cheong, M. Krefeld, and M. Overmars,
2008. Computational Geometry: Algorithms and
Applications, chapter 13, pages 290?296. Springer-
Verlag, 3rd edition.
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
D. Chiang. 2007. Hierarchical phrase based transla-
tion . Computational Linguistics, 33(2):201 ? 228.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation
Models. . In COLING/ACL, Sydney, Australia.
L. Huang. 2008. Advanced Dynamic Programming
in Semiring and Hypergraph Frameworks. In COL-
ING, Manchester, UK.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, Boston, MA, USA.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based Minimum Error Rate Train-
ing for Statistical Machine Translation. In EMNLP,
Honolulu, Hawaii, USA.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based
Translation. In ACL, Columbus, OH, USA.
F. Och and H. Ney. 2004. The Alignment Template
Approach to Statistical Machine Translation. Com-
putational Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176
(W0109-022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
R. Tromble, S. Kumar, F. Och, andW.Macherey. 2008.
Lattice Minimum Bayes-Risk Decoding for Statis-
tical Machine Translation. In EMNLP, Honolulu,
Hawaii.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass
Decoding for Synchronous Context Free Grammars.
In ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
171
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 957?965,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Expected Sequence Similarity Maximization
Cyril Allauzen1, Shankar Kumar1, Wolfgang Macherey1, Mehryar Mohri2,1 and Michael Riley1
1Google Research, 76 Ninth Avenue, New York, NY 10011
2Courant Institute of Mathematical Sciences, 251 Mercer Street, New York, NY 10012
Abstract
This paper presents efficient algorithms for
expected similarity maximization, which co-
incides with minimum Bayes decoding for a
similarity-based loss function. Our algorithms
are designed for similarity functions that are
sequence kernels in a general class of posi-
tive definite symmetric kernels. We discuss
both a general algorithm and a more efficient
algorithm applicable in a common unambigu-
ous scenario. We also describe the applica-
tion of our algorithms to machine translation
and report the results of experiments with sev-
eral translation data sets which demonstrate a
substantial speed-up. In particular, our results
show a speed-up by two orders of magnitude
with respect to the original method of Tromble
et al (2008) and by a factor of 3 or more
even with respect to an approximate algorithm
specifically designed for that task. These re-
sults open the path for the exploration of more
appropriate or optimal kernels for the specific
tasks considered.
1 Introduction
The output of many complex natural language pro-
cessing systems such as information extraction,
speech recognition, or machine translation systems
is a probabilistic automaton. Exploiting the full in-
formation provided by this probabilistic automaton
can lead to more accurate results than just using the
one-best sequence.
Different techniques have been explored in the
past to take advantage of the full lattice, some based
on the use of a more complex model applied to
the automaton as in rescoring, others using addi-
tional data or information for reranking the hypothe-
ses represented by the automaton. One method for
using these probabilistic automata that has been suc-
cessful in large-vocabulary speech recognition (Goel
and Byrne, 2000) and machine translation (Kumar
and Byrne, 2004; Tromble et al, 2008) applications
and that requires no additional data or other com-
plex models is the minimum Bayes risk (MBR) de-
coding technique. This returns that sequence of the
automaton having the minimum expected loss with
respect to all sequences accepted by the automaton
(Bickel and Doksum, 2001). Often, minimizing the
loss function L can be equivalently viewed as max-
imizing a similarity function K between sequences,
which corresponds to a kernel function when it is
positive definite symmetric (Berg et al, 1984). The
technique can then be thought of as an expected se-
quence similarity maximization.
This paper considers this expected similarity max-
imization view. Since different similarity functions
can be used within this framework, one may wish to
select the one that is the most appropriate or relevant
to the task considered. However, a crucial require-
ment for this choice to be realistic is to ensure that
for the family of similarity functions considered the
expected similarity maximization is efficiently com-
putable. Thus, we primarily focus on this algorith-
mic problem in this paper, leaving it to future work
to study the question of determining how to select
the similarity function and report on the benefits of
this choice.
A general family of sequence kernels including
the sequence kernels used in computational biology,
text categorization, spoken-dialog classification, and
many other tasks is that of rational kernels (Cortes
et al, 2004). We show how the expected similarity
maximization can be efficiently computed for these
kernels. In section 3, we describe more specifically
the framework of expected similarity maximization
in the case of rational kernels and the correspond-
ing algorithmic problem. In Section 4, we describe
both a general method for the computation of the ex-
pected similarity maximization, and a more efficient
method that can be used with a broad sub-family
of rational kernels that verify a condition of non-
ambiguity. This latter family includes the class of
n-gram kernels which have been previously used to
957
apply MBR to machine translation (Tromble et al,
2008). We examine in more detail the use and ap-
plication of our algorithms to machine translation
in Section 5. Section 6 reports the results of ex-
periments applying our algorithms in several large
data sets in machine translation. These experiments
demonstrate the efficiency of our algorithm which
is shown empirically to be two orders of magnitude
faster than Tromble et al (2008) and more than 3
times faster than even an approximation algorithm
specifically designed for this problem (Kumar et al,
2009). We start with some preliminary definitions
and algorithms related to weighted automata and
transducers, following the definitions and terminol-
ogy of Cortes et al (2004).
2 Preliminaries
Weighted transducers are finite-state transducers in
which each transition carries some weight in addi-
tion to the input and output labels. The weight set
has the structure of a semiring.
A semiring (K,?,?, 0, 1) verifies all the axioms
of a ring except from the existence of a negative el-
ement ?x for each x ? K, which it may verify or
not. Thus, roughly speaking, a semiring is a ring
that may lack negation. It is specified by a set of
values K, two binary operations ? and ?, and two
designated values 0 and 1. When ? is commutative,
the semiring is said to be commutative.
The real semiring (R+,+,?, 0, 1) is used when
the weights represent probabilities. The log
semiring (R ? {??,+?},?log,+,?, 0) is iso-
morphic to the real semiring via the negative-
log mapping and is often used in practice
for numerical stability. The tropical semiring
(R?, {??,+?},min,+,?, 0) is derived from
the log semiring via the Viterbi approximation and
is often used in shortest-path applications.
Figure 1(a) shows an example of a weighted
finite-state transducer over the real semiring
(R+,+,?, 0, 1). In this figure, the input and out-
put labels of a transition are separated by a colon
delimiter and the weight is indicated after the slash
separator. A weighted transducer has a set of initial
states represented in the figure by a bold circle and
a set of final states, represented by double circles. A
path from an initial state to a final state is an accept-
ing path.
The weight of an accepting path is obtained by
first ?-multiplying the weights of its constituent
0                          a:b/1
1
a:b/2
2/1
a:b/4
3/8
b:a/6
b:a/3
b:a/5
0                        b/1
1
b/2
2/1
b/4
3/8
a/6
a/3
a/5
(a) (b)
Figure 1: (a) Example of weighted transducer T over the
real semiring (R+,+,?, 0, 1). (b) Example of weighted
automaton A. A can be obtained from T by projection on
the output and T (aab, bba) = A(bba) = 1? 2? 6? 8+
2? 4? 5? 8.
transitions and?-multiplying this product on the left
by the weight of the initial state of the path (which
equals 1 in our work) and on the right by the weight
of the final state of the path (displayed after the slash
in the figure). The weight associated by a weighted
transducer T to a pair of strings (x, y) ? ?? ??? is
denoted by T (x, y) and is obtained by ?-summing
the weights of all accepting paths with input label x
and output label y.
For any transducer T , T?1 denotes its inverse,
that is the transducer obtained from T by swapping
the input and output labels of each transition. For all
x, y ? ??, we have T?1(x, y) = T (y, x).
The composition of two weighted transducers T1
and T2 with matching input and output alphabets ?,
is a weighted transducer denoted by T1 ? T2 when
the semiring is commutative and the sum:
(T1 ? T2)(x, y) =
?
z???
T1(x, z)? T2(z, y) (1)
is well-defined and in K for all x, y (Salomaa and
Soittola, 1978).
Weighted automata can be defined as weighted
transducers A with identical input and output labels,
for any transition. Since only pairs of the form (x, x)
can have a non-zero weight associated to them by
A, we denote the weight associated by A to (x, x)
by A(x) and call it the weight associated by A to
x. Similarly, in the graph representation of weighted
automata, the output (or input) label is omitted. Fig-
ure 1(b) shows an example of a weighted automa-
ton. When A and B are weighted automata, A ? B
is called the intersection of A and B. Omitting the
input labels of a weighted transducer T results in a
weighted automaton which is said to be the output
projection of T .
958
3 General Framework
Let X be a probabilistic automaton representing the
output of a complex model for a specific query input.
The model may be for example a speech recognition
system, an information extraction system, or a ma-
chine translation system (which originally motivated
our study). For machine translation, the sequences
accepted by X are the potential translations of the
input sentence, each with some probability given by
X .
Let ? be the alphabet for the task considered, e.g.,
words of the target language in machine translation,
and let L : ?? ? ?? ? R denote a loss function
defined over the sequences on that alphabet. Given
a reference or hypothesis set H ? ??, minimum
Bayes risk (MBR) decoding consists of selecting a
hypothesis x ? H with minimum expected loss with
respect to the probability distribution X (Bickel and
Doksum, 2001; Tromble et al, 2008):
x? = argmin
x?H
E
x??X
[L(x, x?)]. (2)
Here, we shall consider the case, frequent in prac-
tice, where minimizing the loss L is equivalent to
maximizing a similarity measure K : ????? ? R.
When K is a sequence kernel that can be repre-
sented by weighted transducers, it is a rational ker-
nel (Cortes et al, 2004). The problem is then equiv-
alent to the following expected similarity maximiza-
tion:
x? = argmax
x?H
E
x??X
[K(x, x?)]. (3)
When K is a positive definite symmetric rational
kernel, it can often be rewritten as K(x, y) = (T ?
T?1)(x, y), where T is a weighted transducer over
the semiring (R+?{+?},+,?, 0, 1). Equation (3)
can then be rewritten as
x? = argmax
x?H
E
x??X
[(T ? T?1)(x, x?)] (4)
= argmax
x?H
?A(x) ? T ? T?1 ?X?, (5)
where we denote by A(x) an automaton accepting
(only) the string x and by ??? the sum of the weights
of all accepted paths of a transducer.
4 Algorithms
4.1 General method
Equation (5) could suggest computing A(x) ? T ?
T?1 ? X for each possible x ? H . Instead, we
can compute a composition based on an automa-
ton accepting all sequences in H , A(H). This leads
to a straightforward method for determining the se-
quence maximizing the expected similarity having
the following steps:
1. compute the composition X ? T , project on
the output and optimize (epsilon-remove, de-
terminize, minimize (Mohri, 2009)) and let Y2
be the result;1
2. compute the composition Y1 = A(H) ? T ;
3. compute Y1 ? Y2 and project on the input, let Z
be the result;2
4. determinize Z;
5. find the maximum weight path with the label of
that path giving x?.
While this method can be efficient in various scenar-
ios, in some instances the weighted determinization
yielding Z can be both space- and time-consuming,
even though the input is acyclic. The next two sec-
tions describe more efficient algorithms.
Note that in practice, for numerical stability, all
of these computations are done in the log semiring
which is isomorphic to (R+?{+?},+,?, 0, 1). In
particular, the maximum weight path in the last step
is then obtained by using a standard single-source
shortest-path algorithm.
4.2 Efficient method for n-gram kernels
A common family of rational kernels is the family
of n-gram kernels. These kernels are widely use as
a similarity measure in natural language processing
and computational biology applications, see (Leslie
et al, 2002; Lodhi et al, 2002) for instance.
The n-gram kernel Kn of order n is defined as
Kn(x, y) =
?
|z|=n
cx(z)cy(z), (6)
where cx(z) is the number of occurrences of z in
x. Kn is a positive definite symmetric rational ker-
nel since it corresponds to the weighted transducer
Tn ? T?1n where the transducer Tn is defined such
that Tn(x, z) = cx(z) for all x, z ? ?? with |z| = n.
1Equivalent to computing T?1 ? X and projecting on the
input.
2Z is then the projection on the input of A(H)?T ?T?1?X .
959
0a:?
b:?
1a:a
b:b
2a:a
b:b
a:?
b:?
0
1a/0.5
2
b/0.5
3b/1
4b/1
5a/1
6a/1
7a/0.4
8
b/0.6
b/1
9/1
b/1
a/1
(a) (b)
0
1a
2
b
3b
4b
5a
6a
7a
8
b
b
9
b
a
0
1a/1
2
b/1 3/1
a/0.2
b/1.5
a/1.8
b/0.5
(c) (d)
?
a/0
a/0
b/0
b/0
a/0.2
b/1.5
a/1.8
b/0.5
0
1a/0
2
b/0
3b/1.5
4b/0.5
5a/1.8
6a/1.8
7a/0.2
8
b/1.5
b/0.5
9/0
b/1.5
a/1.8
(e) (f)
Figure 2: Efficient method for bigram kernel: (a) Counting transducer T2 for ? = {a, b} (over the real semiring). (b)
Probabilistic automaton X (over the real semiring). (c) The hypothesis automaton A(H) (unweighted). (d) Automaton
Y2 representing the expected bigram counts in X (over the real semiring). (e) Automaton Y1: the context dependency
model derived from Y2 (over the tropical semiring). (f) The composition A(H) ? Y1 (over the tropical semiring).
The transducer T2 for ? = {a, b} is shown in Fig-
ure 2(a).
Taking advantage of the special structure of n-
gram kernels and of the fact that A(H) is an un-
weighted automaton, we can devise a new and sig-
nificantly more efficient method for computing x?
based on the following steps.
1. Compute the expected n-gram counts in X: We
compute the composition X ?T , project on out-
put and optimize (epsilon-remove, determinize,
minimize) and let Y2 be the result. Observe that
the weighted automaton Y2 is a compact repre-
sentation of the expected n-gram counts in X ,
i.e. for an n-gram w (i.e. |w| = n):
Y2(w) =
?
x???
X(x)cx(w)
= E
x?X
[cx(w)] = cX(w).
(7)
2. Construct a context-dependency model: We
compute the weighted automaton Y1 over the
tropical semiring as follow: the set of states is
Q = {w ? ??| |w| ? n and w occurs in X},
the initial state being ? and every state being fi-
nal; the set of transitions E contains all 4-tuple
(origin, label, weight, destination) of the form:
? (w, a, 0, wa) with wa ? Q and |w| ? n?
2 and
? (aw, b, Y2(awb), wb) with Y2(awb) 6= 0
and |w| = n? 2
where a, b ? ? and w ? ??. Observe that
w ? Q when wa ? Q and that aw,wb ? Q
when Y2(awb) 6= 0. Given a string x, we have
Y1(x) =
?
|w|=n
cX(w)cx(w). (8)
Observe that Y1 is a deterministic automaton,
hence Y1(x) can be computed in O(|x|) time.
3. Compute x?: We compute the composition
A(H) ? Y1. x? is then the label of the accepting
path with the largest weight in this transducer
and can be obtained by applying a shortest-path
algorithm to ?A(H) ? Y1 in the tropical semir-
ing.
The main computational advantage of this method
is that it avoids the determinization of Z in the
960
0 1a/1 2/1a/c1
b/c2
0 1a
2/c1
a
3/c2
b
0
b
1a
2/c1
a
3/c2
b
b
a
b
a
0
b/0
1a/0
2
a/0
3
b/0
2?/0b/0 a/0
?      /c1
3?/0?      /c2
b/0
a/0
(a) (b) (c) (d)
Figure 3: Illustration of the construction of Y1 in the unambiguous case. (a) Weighted automaton Y2 (over the real
semiring). (b) Deterministic tree automaton Y ?2 accepting {aa, ab} (over the tropical semiring). (c) Result of deter-
minization of ??Y ?2 (over the tropical semiring). (d) Weighted automaton Y1 (over the tropical semiring).
(+,?) semiring, which can sometimes be costly.
The method has also been shown empirically to be
significantly faster than the one described in the pre-
vious section.
The algorithm is illustrated in Figure 2. The al-
phabet is ? = {a, b} and the counting transducer
corresponding to the bigram kernel is given in Fig-
ure 2(a). The evidence probabilistic automaton X
is given in Figure 2(b) and we use as hypothesis
set the set of strings that were assigned a non-zero
probability by X; this set is represented by the deter-
ministic finite automaton A(H) given in Figure 2(c).
The result of step 1 of the algorithm is the weighted
automaton Y2 over the real semiring given in Fig-
ure 2(d). The result of step 2 is the weighted au-
tomaton Y1 over the tropical semiring is given in
Figure 2(e). Finally, the result of the composition
A(H) ? Y1 (step 3) is the weighted automaton over
the tropical semiring given in Figure 2(f). The re-
sult of the expected similarity maximization is the
string x? = ababa, which is obtained by applying
a shortest-path algorithm to ?A(H) ? Y1. Observe
that the string x with the largest probability in X is
x = bbaba and is hence different from x? = ababa in
this example.
4.3 Efficient method for the unambiguous case
The algorithm presented in the previous section for
n-gram kernels can be generalized to handle a wide
variety of rational kernels.
Let K be an arbitrary rational kernel defined by a
weighted transducer T . Let XT denote the regular
language of the strings output by T . We shall as-
sume that XT is a finite language, though the results
of this section generalize to the infinite case. Let
? denote a new alphabet defined by ? = {#x : x ?
XT } and consider the simple grammar G of context-
dependent batch rules:
? ? #x/x ?. (9)
Each such rule inserts the symbol #x immediately
after an occurrence x in the input string. For batch
context-dependent rules, the context of the applica-
tion for all rules is determined at once before their
application (Kaplan and Kay, 1994). Assume that
this grammar is unambiguous for a parallel applica-
tion of the rules. This condition means that there is
a unique way of parsing an input string using the
strings of XT . The assumption holds for n-gram
sequences, for example, since the rules applicable
are uniquely determined by the n-grams (making the
previous section a special case).
Given an acyclic weighted automaton Y2 over the
tropical semiring accepting a subset of XT , we can
construct a deterministic weighted automaton Y1 for
??L(Y2) when this grammar is unambiguous. The
weight assigned by Y1 to an input string is then the
sum of the weights of the substrings accepted by Y2.
This can be achieved using weighted determiniza-
tion.
This suggests a new method for generalizing Step
2 of the algorithm described in the previous section
as follows (see illustration in Figure 3):
(i) use Y2 to construct a deterministic weighted
tree Y ?2 defined on the tropical semiring ac-
cepting the same strings as Y2 with the same
weights, with the final weights equal to the to-
tal weight given by Y2 to the string ending at
that leaf;
(ii) let Y1 be the weighted automaton obtained by
first adding self-loops labeled with all elements
of ? at the initial state of Y ?2 and then deter-
minizing it, and then inserting new transitions
leaving final states as described in (Mohri and
Sproat, 1996).
961
Step (ii) consists of computing a deterministic
weighted automaton for ??Y ?2 . This step corre-
sponds to the Aho-Corasick construction (Aho and
Corasick, 1975) and can be done in time linear in
the size of Y ?2 .
This approach assumes that the grammar G of
batch context-dependent rules inferred by XT is un-
ambiguous. This can be tested by constructing the
finite automaton corresponding to all rules in G. The
grammar G is unambiguous iff the resulting automa-
ton is unambiguous (which can be tested using a
classical algorithm). An alternative and more ef-
ficient test consists of checking the presence of a
failure or default transition to a final state during
the Aho-Corasick construction, which occurs if and
only if there is ambiguity.
5 Application to Machine Translation
In machine translation, the BLEU score (Papineni et
al., 2001) is typically used as an evaluation metric.
In (Tromble et al, 2008), a Minimum Bayes-Risk
decoding approach for MT lattices was introduced.3
The loss function used in that approach was an ap-
proximation of the log-BLEU score by a linear func-
tion of n-gram matches and candidate length. This
loss function corresponds to the following similarity
measure:
KLB(x, x?) = ?0|x?|+
?
|w|?n
?|w|cx(w)1x?(w).
(10)
where 1x(w) is 1 if w occurs in x and 0 otherwise.
(Tromble et al, 2008) implements the MBR de-
coder using weighted automata operations. First,
the set of n-grams is extracted from the lat-
tice. Next, the posterior probability p(w|X) of
each n-gram is computed. Starting with the un-
weighted lattice A(H), the contribution of each n-
gram w to (10) is applied by iteratively compos-
ing with the weighted automaton corresponding to
w(w/(?|w|p(w|X))w)? where w = ?? \ (??w??).
Finally, the MBR hypothesis is extracted as the best
path in the automaton. The above steps are carried
out one n-gram at a time. For a moderately large lat-
tice, there can be several thousands of n-grams and
the procedure becomes expensive. This leads us to
investigate methods that do not require processing
the n-grams one at a time in order to achieve greater
efficiency.
3Related approaches were presented in (DeNero et al, 2009;
Kumar et al, 2009; Li et al, 2009).
0
1?:?
2
?:?
b:?
3
a:a
a:? b:b
a:?
b:?
Figure 4: Transducer T 1 over the real semiring for the
alphabet {a, b}.
The first idea is to approximate the KLB similar-
ity measure using a weighted sum of n-gram ker-
nels. This corresponds to approximating 1x?(w) by
cx?(w) in (10). This leads us to the following simi-
larity measure:
KNG(x, x?) = ?0|x?|+
?
|w|?n
?|w|cx(w)cx?(w)
= ?0|x?|+
?
1?i?n
?iKi(x, x?)
(11)
Intuitively, the larger the length of w the less likely
it is that cx(w) 6= 1x(w), which suggests comput-
ing the contribution to KLB(x, x?) of lower-order
n-grams (|w| ? k) exactly, but using the approxima-
tion by n-gram kernels for the higher-order n-grams
(|w| > k). This gives the following similarity mea-
sure:
KkNG(x, x?) = ?0|x?|+
?
1?|w|?k
?|w|cx(w)1x?(w)
+
?
k<|w|?n
?|w|cx(w)cx?(w)
(12)
Observe that K0NG = KNG and KnNG = KLB .
All these similarity measures can still be com-
puted using the framework described in Section 4.
Indeed, there exists a transducer Tn over the real
semiring such that Tn(x, z) = 1x(z) for all x ? ??
and z ? ?n. The transducer T 1 for ? = {a, b} is
given by Figure 4. Let us define the similarity mea-
sure Kn as:
Kn(x, x?) = (Tn?T?1n )(x, x?) =
?
|w|=n
cx(w)1x?(w).
(13)
Observe that the framework described in Section 4
can still be applied even though Kn is not symmet-
ric. The similarity measures KLB , KNG and KkNG
962
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
no mbr 38.7 39.2 38.3 33.5 26.5 64.0 51.8 57.3 45.5 43.8
exact 37.0 39.2 38.6 34.3 27.5 65.2 51.4 58.1 45.2 45.0
approx 39.0 39.9 38.6 34.4 27.4 65.2 52.5 58.1 46.2 45.0
ngram 36.6 39.1 38.1 34.4 27.7 64.3 50.1 56.7 44.1 42.8
ngram1 37.1 39.2 38.5 34.4 27.5 65.2 51.4 58.0 45.2 44.8
Table 1: BLEU score (%)
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
exact 3560 7863 5553 6313 5738 12341 23266 11152 11417 11405
approx 168 422 279 335 328 504 1296 528 619 808
ngram 28 72 34 70 43 85 368 105 63 66
ngram1 58 175 96 99 89 368 943 308 167 191
Table 2: MBR Time (in seconds)
can then be expressed as the relevant linear combi-
nation of Ki and Ki.
6 Experimental Results
Lattices were generated using a phrase-based MT
system similar to the alignment template system de-
scribed in (Och and Ney, 2004). Given a source sen-
tence, the system produces a word lattice A that is a
compact representation of a very large N -best list of
translation hypotheses for that source sentence and
their likelihoods. The lattice A is converted into a
lattice X that represents a probability distribution
(i.e. the posterior probability distribution given the
source sentence) following:
X(x) = exp(?A(x))?
y??? exp(?A(y))
(14)
where the scaling factor ? ? [0,?) flattens the dis-
tribution when ? < 1 and sharpens it when ? > 1.
We then applied the methods described in Section 5
to the lattice X using as hypothesis set H the un-
weighted lattice obtained from X .
The following parameters for the n-gram factors
were used:
?0 =
?1
T and ?n =
1
4Tprn?1 for n ? 1. (15)
Experiments were conducted on two language
pairs Arabic-English (aren) and Chinese-English
(zhen) and for a variety of datasets from the NIST
Open Machine Translation (OpenMT) Evaluation.4
The values of ?, p and r used for each pair are given
4http://www.nist.gov/speech/tests/mt
? p r
aren 0.2 0.85 0.72
zhen 0.1 0.80 0.62
Table 3: Parameters used for performing MBR.
in Table 3. We used the IBM implementation of the
BLEU score (Papineni et al, 2001).
We implemented the following methods using the
OpenFst library (Allauzen et al, 2007):
? exact: uses the similarity measure KLB based
on the linearized log-BLEU, implemented as
described in (Tromble et al, 2008);
? approx: uses the approximation to KLB from
(Kumar et al, 2009) and described in the ap-
pendix;
? ngram: uses the similarity measure KNG im-
plemented using the algorithm of Section 4.2;
? ngram1: uses the similarity measure K1NG
also implemented using the algorithm of Sec-
tion 4.2.
The results from Tables 1-2 show that ngram1
performs as well as exact on all datasets5 while be-
ing two orders of magnitude faster than exact and
overall more than 3 times faster than approx.
7 Conclusion
We showed that for broad families of transducers
T and thus rational kernels, the expected similar-
5We consider BLEU score differences of less than 0.4% not
significant (Koehn, 2004).
963
ity maximization problem can be solved efficiently.
This opens up the option of seeking the most appro-
priate rational kernel or transducer T for the spe-
cific task considered. In particular, the kernel K
used in our machine translation applications might
not be optimal. One may well imagine for exam-
ple that some n-grams should be further emphasized
and others de-emphasized in the definition of the
similarity. This can be easily accommodated in the
framework of rational kernels by modifying the tran-
sition weights of T . But, ideally, one would wish
to select those weights in an optimal fashion. As
mentioned earlier, we leave this question to future
work. However, we can offer a brief look at how
one could tackle this question. One method for de-
termining an optimal kernel for the expected sim-
ilarity maximization problem consists of solving a
problem similar to that of learning kernels in classi-
fication or regression. Let X1, . . . , Xm be m lattices
with Ref(X1), . . . ,Ref(Xm) the associated refer-
ences and let x?(K,Xi) be the solution of the ex-
pected similarity maximization for lattice Xi when
using kernel K. Then, the kernel learning optimiza-
tion problem can be formulated as follows:
min
K?K
1
m
m?
i=1
L(x?(K,Xi),Ref(Xi))
s. t. K = T ? T?1 ? Tr[K] ? C,
where K is a convex family of rational kernels and
Tr[K] denotes the trace of the kernel matrix. In
particular, we could choose K as a family of linear
combinations of base rational kernels. Techniques
and ideas similar to those discussed by Cortes et al
(2008) for learning sequence kernels could be di-
rectly relevant to this problem.
A Appendix
We describe here the approximation of the KLB
similarity measure from Kumar et al (2009). We
assume in this section that the lattice X is determin-
istic in order to simplify the notations. The posterior
probability of n-gram w in the lattice X can be for-
mulated as:
p(w|X) =
?
x???
1x(w)P (x|s) =
?
x???
1x(w)X(x)
(16)
where s denotes the source sentence. When using
the similarity measure KLB defined Equation (10),
Equation (3) can then be reformulated as:
x? = argmax
x??H
?0|x?|+
?
w
?|w|cx?(w)p(w|X). (17)
The key idea behind this new approximation algo-
rithm is to rewrite the n-gram posterior probability
(Equation 16) as follows:
p(w|X) =
?
x???
?
e?EX
f(e, w, ?x)X(x) (18)
where EX is the set of transitions of X , ?x is
the unique accepting path labeled by x in X and
f(e, w, ?) is a score assigned to transition e on path
? containing n-gram w:
f(e, w, ?) =
?
?
?
1 if w ? e, p(e|X) > p(e?|X),
and e? precedes e on ?
0 otherwise.
(19)
In other words, for each path ?, we count the tran-
sition that contributes n-gram w and has the highest
transition posterior probability relative to its prede-
cessors on the path ?; there is exactly one such tran-
sition on each lattice path ?.
We note that f(e, w, ?) relies on the full path ?
which means that it cannot be computed based on
local statistics. We therefore approximate the quan-
tity f(e, w, ?) with f?(e, w,X) that counts the tran-
sition e with n-gram w that has the highest arc poste-
rior probability relative to predecessors in the entire
lattice X . f?(e, w,X) can be computed locally, and
the n-gram posterior probability based on f? can be
determined as follows:
p(w|G) =
?
x???
?
e?EX
f?(e, w,X)X(x)
=
?
e?Ex
1w?ef?(e, w,X)
?
x???
1pix(e)X(x)
=
?
e?EX
1w?ef?(e, w,X)P (e|X),
(20)
where P (e|X) is the posterior probability of a lat-
tice transition e ? EX . The algorithm to perform
Lattice MBR is given in Algorithm 1. For each state
t in the lattice, we maintain a quantity Score(w, t)
for each n-gram w that lies on a path from the initial
state to t. Score(w, t) is the highest posterior prob-
ability among all transitions on the paths that termi-
nate on t and contain n-gram w. The forward pass
requires computing the n-grams introduced by each
transition; to do this, we propagate n-grams (up to
maximum order ?1) terminating on each state.
964
Algorithm 1 MBR Decoding on Lattices
1: Sort the lattice states topologically.
2: Compute backward probabilities of each state.
3: Compute posterior prob. of each n-gram:
4: for each transition e do
5: Compute transition posterior probability P (e|X).
6: Compute n-gram posterior probs. P (w|X):
7: for each n-gram w introduced by e do
8: Propagate n? 1 gram suffix to he.
9: if p(e|X) > Score(w, T (e)) then
10: Update posterior probs. and scores:
p(w|X) += p(e|X) ? Score(w, T (e)).
Score(w, he) = p(e|X).
11: else
12: Score(w, he) = Score(w, T (e)).
13: end if
14: end for
15: end for
16: Assign scores to transitions (given by Equation 17).
17: Find best path in the lattice (Equation 17).
References
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
String Matching: An Aid to Bibliographic Search.
Communications of the ACM, 18(6):333?340.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In CIAA 2007, volume 4783 of LNCS, pages
11?23. Springer. http://www.openfst.org.
Christian Berg, Jens Peter Reus Christensen, and Paul
Ressel. 1984. Harmonic Analysis on Semigroups.
Springer-Verlag: Berlin-New York.
Peter J. Bickel and Kjell A. Doksum. 2001. Mathemati-
cal Statistics, vol. I. Prentice Hall.
Corinna Cortes, Patrick Haffner, and Mehryar Mohri.
2004. Rational Kernels: Theory and Algorithms.
Journal of Machine Learning Research, 5:1035?1062.
Corinna Cortes, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Learning sequence kernels. In Pro-
ceedings of MLSP 2008, October.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of ACL and IJCNLP, pages 567?575.
Vaibhava Goel and William J. Byrne. 2000. Minimum
Bayes-risk automatic speech recognition. Computer
Speech and Language, 14(2):115?135.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3).
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In EMNLP,
Barcelona, Spain.
Shankar Kumar and William J. Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL, Boston, MA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Christina S. Leslie, Eleazar Eskin, and William Stafford
Noble. 2002. The Spectrum Kernel: A String Kernel
for SVM Protein Classification. In Pacific Symposium
on Biocomputing, pages 566?575.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of ACL and IJCNLP, pages 593?
601.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watskins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?44.
Mehryar Mohri and Richard Sproat. 1996. An Efficient
Compiler for Weighted Rewrite Rules. In Proceedings
of ACL ?96, Santa Cruz, California.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213?254. Springer.
Franz J. Och and Hermann Ney. 2004. The align-
ment template approach to statistical mchine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division.
Arto Salomaa and Matti Soittola. 1978. Automata-
Theoretic Aspects of Formal Power Series. Springer.
Roy W. Tromble, Shankar Kumar, Franz J. Och, and
Wolfgang Macherey. 2008. Lattice minimum Bayes-
risk decoding for statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 620?
629.
965
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 975?983,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Model Combination for Machine Translation
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och
UC Berkeley Google, Inc.
denero@berkeley.edu {shankarkumar,ciprianchelba,och}@google.com
Abstract
Machine translation benefits from two types
of decoding techniques: consensus decoding
over multiple hypotheses under a single model
and system combination over hypotheses from
different models. We present model combina-
tion, a method that integrates consensus de-
coding and system combination into a uni-
fied, forest-based technique. Our approach
makes few assumptions about the underly-
ing component models, enabling us to com-
bine systems with heterogenous structure. Un-
like most system combination techniques, we
reuse the search space of component models,
which entirely avoids the need to align trans-
lation hypotheses. Despite its relative sim-
plicity, model combination improves trans-
lation quality over a pipelined approach of
first applying consensus decoding to individ-
ual systems, and then applying system combi-
nation to their output. We demonstrate BLEU
improvements across data sets and language
pairs in large-scale experiments.
1 Introduction
Once statistical translation models are trained, a de-
coding approach determines what translations are fi-
nally selected. Two parallel lines of research have
shown consistent improvements over the standard
max-derivation decoding objective, which selects
the highest probability derivation. Consensus de-
coding procedures select translations for a single
system by optimizing for model predictions about
n-grams, motivated either as minimizing Bayes risk
(Kumar and Byrne, 2004), maximizing sentence
similarity (DeNero et al, 2009), or approximating a
max-translation objective (Li et al, 2009b). System
combination procedures, on the other hand, generate
translations from the output of multiple component
systems (Frederking and Nirenburg, 1994). In this
paper, we present model combination, a technique
that unifies these two approaches by learning a con-
sensus model over the n-gram features of multiple
underlying component models.
Model combination operates over the compo-
nent models? posterior distributions over translation
derivations, encoded as a forest of derivations.1 We
combine these components by constructing a linear
consensus model that includes features from each
component. We then optimize this consensus model
over the space of all translation derivations in the
support of all component models? posterior distribu-
tions. By reusing the components? search spaces,
we entirely avoid the hypothesis alignment problem
that is central to standard system combination ap-
proaches (Rosti et al, 2007).
Forest-based consensus decoding techniques dif-
fer in whether they capture model predictions
through n-gram posteriors (Tromble et al, 2008;
Kumar et al, 2009) or expected n-gram counts
(DeNero et al, 2009; Li et al, 2009b). We evaluate
both in controlled experiments, demonstrating their
empirical similarity. We also describe algorithms for
expanding translation forests to ensure that n-grams
are local to a forest?s hyperedges, and for exactly
computing n-gram posteriors efficiently.
Model combination assumes only that each trans-
lation model can produce expectations of n-gram
features; the latent derivation structures of compo-
nent systems can differ arbitrarily. This flexibility
allows us to combine phrase-based, hierarchical, and
syntax-augmented translation models. We evaluate
by combining three large-scale systems on Chinese-
English and Arabic-English NIST data sets, demon-
strating improvements of up to 1.4 BLEU over the
1In this paper, we use the terms translation forest and hyper-
graph interchangeably.
975
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Single-Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?:
[
v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?:
[
v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 1: An example translation forest encoding two
synchronous derivations for a Spanish sentence: one solid
and one dotted. Nodes are annotated with their left and
right unigram contexts, and hyperedges are annotated
with scores ? ? ?(r) and the bigrams they introduce.
best single systemmax-derivation baseline, and con-
sistent improvements over a more complex multi-
system pipeline that includes independent consensus
decoding and system combination.
2 Model Combination
Model combination is a model-based approach to se-
lecting translations using information from multiple
component systems. Each system provides its poste-
rior distributions over derivations Pi(d|f), encoded
as a weighted translation forest (i.e., translation hy-
pergraph) in which hyperedges correspond to trans-
lation rule applications r.2 The conditional distribu-
tion over derivations takes the form:
Pi(d|f) =
exp
[?
r?d ?i ? ?i(r)
]
?
d??D(f) exp
[?
r?d? ?i ? ?i(r)
]
whereD(f) is the set of synchronous derivations en-
coded in the forest, r iterates over rule applications
in d, and ?i is the parameter vector for system i. The
feature vector ?i is system specific and includes both
translation model and language model features. Fig-
ure 1 depicts an example forest.
Model combination includes four steps, described
below. The entire sequence is illustrated in Figure 2.
2Phrase-based systems produce phrase lattices, which are in-
stances of forests with arity 1.
2.1 Computing Combination Features
The first step in model combination is to com-
pute n-gram expectations from component system
posteriors?the same quantities found in MBR, con-
sensus, and variational decoding techniques. For an
n-gram g and system i, the expectation
vni (g) = EPi(d|f) [h(d, g)]
can be either an n-gram expected count, if h(d, g)
is the count of g in d, or the posterior probability
that d contains g, if h(d, g) is an indicator function.
Section 3 describes how to compute these features
efficiently.
2.2 Constructing a Search Space
The second step in model combination constructs a
hypothesis space of translation derivations, which
includes all derivations present in the forests con-
tributed by each component system. This search
space D is also a translation forest, and consists of
the conjoined union of the component forests. Let
Ri be the root node of component hypergraph Di.
For all i, we include all of Di in D, along with an
edge from Ri to R, the root of D. D may contain
derivations from different types of translation sys-
tems. However, D only contains derivations (and
therefore translations) that appeared in the hypothe-
sis space of some component system. We do not in-
termingle the component search spaces in any way.
2.3 Features for the Combination Model
The third step defines a new combination model over
all of the derivations in the search space D, and then
annotates D with features that allow for efficient
model inference. We use a linear model over four
types of feature functions of a derivation:
1. Combination feature functions on n-grams
vni (d) =
?
g?Ngrams(d) v
n
i (g) score a deriva-
tion according to the n-grams it contains.
2. Model score feature function b gives the model
score ?i ? ?i(d) of a derivation d under the sys-
tem i that d is from.
3. A length feature ` computes the word length of
the target-side yield of a derivation.
4. A system indicator feature ?i is 1 if the deriva-
tion came from system i, and 0 otherwise.
976
All of these features are local to rule applications
(hyperedges) in D. The combination features pro-
vide information sharing across the derivations of
different systems, but are functions of n-grams, and
so can be scored on any translation forest. Model
score features are already local to rule applications.
The length feature is scored in the standard way.
System indicator features are scored only on the hy-
peredges fromRi toR that link each component for-
est to the common root.
Scoring the joint search space D with these fea-
tures involves annotating each rule application r (i.e.
hyperedge) with the value of each feature.
2.4 Model Training and Inference
We have defined the following combination model
sw(d) with weights w over derivations d from I dif-
ferent component models:
I?
i=1
[
4?
n=1
wni v
n
i (d) + w
?
i ?i(d)
]
+wb?b(d)+w`?`(d)
Because we have assessed all of these features on
local rule applications, we can find the highest scor-
ing derivation d? = arg max
d?D
sw(d) using standard
max-sum (Viterbi) inference over D.
We learn the weights of this consensus model us-
ing hypergraph-based minimum-error-rate training
(Kumar et al, 2009). This procedure maximizes the
translation quality of d? on a held-out set, according
to a corpus-level evaluation metric B(?; e) that com-
pares to a reference set e. We used BLEU, choosing
w to maximize the BLEU score of the set of transla-
tions predicted by the combination model.
3 Computing Combination Features
The combination features vni (d) score derivations
from each model with the n-gram predictions of the
others. These predictions sum over all derivations
under a single component model to compute a pos-
terior belief about each n-gram. In this paper, we
compare two kinds of combination features, poste-
rior probabilities and expected counts.3
3The model combination framework could incorporate ar-
bitrary features on the common output space of the models, but
we focus on features that have previously proven useful for con-
sensus decoding.
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Singl -Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?:
[
v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?:
[
v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 2: Model combination applied to a phrase-based
(pb) and a hierarchical model (h) includes four steps. (1)
shows an excerpt of the bigram feature function for each
component, (2) depicts the result of conjoining a phrase
lattice with a hierarchical forest, (3) shows example hy-
peredge features of the combination model, including bi-
gram features vni and system indicators ?i, and (4) gives
training and decoding objectives.
Posterior probabilities represent a model?s be-
lief that the translation will contain a particular n-
gram at least once. They can be expressed as
EP (d|f) [?(d, g)] for an indicator function ?(d, g)
that is 1 if n-gram g appears in derivation d. These
quantities arise in approximating BLEU for lattice-
based and hypergraph-based minimum Bayes risk
decoding (Tromble et al, 2008; Kumar et al, 2009).
Expected n-gram counts EP (d|f) [c(d, g)] represent
the model?s belief of how many times an n-gram g
will appear in the translation. These quantities ap-
pear in forest-based consensus decoding (DeNero et
al., 2009) and variational decoding (Li et al, 2009b).
977
Methods for computing both of these quantities ap-
pear in the literature. However, we address two out-
standing issues below. In Section 5, we also com-
pare the two quantities experimentally.
3.1 Computing N -gram Posteriors Exactly
Kumar et al (2009) describes an efficient approx-
imate algorithm for computing n-gram posterior
probabilities. Algorithm 1 is an exact algorithm that
computes all n-gram posteriors from a forest in a
single inside pass. The algorithm tracks two quanti-
ties at each node n: regular inside scores ?(n) and
n-gram inside scores ??(n, g) that sum the scores of
all derivations rooted at n that contain n-gram g.
For each hyperedge, we compute b?(g), the sum of
scores for derivations that do not contain g (Lines 8-
11). We then use that quantity to compute the score
of derivations that do contain g (Line 17).
Algorithm 1 Computing n-gram posteriors
1: for n ? N in topological order do
2: ?(n)? 0
3: ??(n, g)? 0, ?g ? Ngrams(n)
4: for r ? Rules(n) do
5: w ? exp [? ? ?(r)]
6: b? w
7: b?(g)? w, ?g ? Ngrams(n)
8: for ` ? Leaves(r) do
9: b? b? ?(`)
10: for g ? Ngrams(n) do
11: b?(g)? b?(g)?
(
?(`)? ??(`, g)
)
12: ?(n)? ?(n) + b
13: for g ? Ngrams(n) do
14: if g ? Ngrams(r) then
15: ??(n, g)? ??(n, g)+b
16: else
17: ??(n, g)? ??(n, g)+b? b?(g)
18: for g ? Ngrams(root) (all g in the HG) do
19: P (g|f)? ??(root,g)?(root)
This algorithm can in principle compute the pos-
terior probability of any indicator function on local
features of a derivation. More generally, this algo-
rithm demonstrates how vector-backed inside passes
can compute quantities beyond expectations of local
features (Li and Eisner, 2009).4 Chelba and Maha-
jan (2009) developed a similar algorithm for lattices.
4Indicator functions on derivations are not locally additive
3.2 Ensuring N -gram Locality
DeNero et al (2009) describes an efficient algorithm
for computing n-gram expected counts from a trans-
lation forest. This method assumes n-gram local-
ity of the forest, the property that any n-gram intro-
duced by a hyperedge appears in all derivations that
include the hyperedge. However, decoders may re-
combine forest nodes whenever the language model
does not distinguish between n-grams due to back-
off (Li and Khudanpur, 2008). In this case, a forest
encoding of a posterior distribution may not exhibit
n-gram locality in all regions of the search space.
Figure 3 shows a hypergraph which contains non-
local trigrams, along with its local expansion.
Algorithm 2 expands a forest to ensure n-gram lo-
cality while preserving the encoded distribution over
derivations. Let a forest (N,R) consist of nodes N
and hyperedges R, which correspond to rule appli-
cations. Let Rules(n) be the subset of R rooted by
n, and Leaves(r) be the leaf nodes of rule applica-
tion r. The expanded forest (Ne, Re) is constructed
by a function Reapply(r, L) that applies the rule of r
to a new set of leavesL ? Ne, forming a pair (r?, n?)
consisting of a new rule application r? rooted by n?.
P is a map from nodes in N to subsets of Ne which
tracks how N projects to Ne. Two nodes in Ne are
identical if they have the same (n?1)-gram left and
right contexts and are projections of the same node
in N . The symbol
?
denotes a set cross-product.
Algorithm 2 Expanding for n-gram locality
1: Ne ? {}; Re ? {}
2: for n ? N in topological order do
3: P (n)? {}
4: for r ? Rules(n) do
5: for L ?
?
`?Leaves(r) [P (`)] do
6: r?, n? ? Reapply(r, L)
7: P (n)? P (n) ? {n?}
8: Ne ? Ne ? {n?}
9: Re ? Re ? {r?}
This transformation preserves the original distri-
bution over derivations by splitting states, but main-
taining continuations from those split states by du-
plicating rule applications. The process is analogous
over the rules of a derivation, even if the features they indicate
are local. Therefore, Algorithm 1 is not an instance of an ex-
pectation semiring computation.
978
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Single-Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?: [v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?: [v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 3: Hypergraph expansion ensures n-gram locality
without affecting the distribution over derivations. In the
left example, trigrams ?green witch was? and ?blue witch
was? are non-local due to language model back-off. On
the right, states are split to enforce trigram locality.
to expanding bigram lattices to e code a trigram his-
tory at each lattice node (Weng et al, 1998).
4 Relationship to Prior Work
Model combination is a multi-system generaliza-
tion of consensus or minimum Bayes risk decod-
ing. When only one component system is included,
model combination is identical to minimum Bayes
risk decoding over hypergraphs, as described in Ku-
mar et al (2009).5
4.1 System Combination
System combination techniques in machine trans-
lat on take as input the outputs {e1, ? ? ? , ek} of k
translation systems, where ei is a structured transla-
tion object (or k-best lists thereof), typically viewed
as a sequence of words. The dominant approach in
the field chooses a primary translation ep as a back-
bone, then finds an alignment ai to the backbone for
each ei. A new search space is constructed from
these backbone-aligned outputs, and then a voting
procedure or feature-based model predicts a final
consensus translation (Rosti et al, 2007). Model
combination entirely avoids this alignment problem
by viewing hypotheses as n-gram occurrence vec-
tors rather than word sequences.
Model combination also requires less total com-
putation than applying system combination to
5We do not refer to model combination as a minimum Bayes
risk decoding procedure despite this similarity because risk im-
plies a belief distribution over outputs, and we now have mul-
tiple output distributions that are not necessarily calibrated.
Moreover, our generalized, multi-model objective (Section 2.4)
is motivated by BLEU, but not a direct approximation to it.
consensus-decoded outputs. The best consensus de-
coding methods for individual systems already re-
quire the computation-intensive steps of model com-
bination: producing lattices or forests, computing n-
gram feature expectations, and re-decoding to max-
imize a secondary consensus objective. Hence, to
maximize the performance of system combination,
these steps must be performed for each system,
whereas model combination requires only one for-
est rescoring pass over all systems.
Model combination also leverages aggregate
statistics from the components? posteriors, whereas
system combiners typically do not. Zhao and He
(2009) showed that n-gram posterior features are
useful in the context of a system combination model,
even when computed from k-best lists.
Despite these advantages, system combination
may be more appropriate in some settings. In par-
ticular, model combination is designed primarily for
st tistical systems that generate hypergraph outputs.
Model combination can in principle integrate a non-
statisti al system that generates either a single hy-
pothesis or an unweighted forest.6 Likewise, the pro-
cedure c uld be applied to statistical systems that
only generate k-best lists. However, we would not
expect the same strong performance from model
combination in these constrained settings.
4.2 Joint Decoding and Collaborative Decoding
Liu et al (2009) describes two techniques for com-
bining multiple synchronous grammars, which the
authors characterize as joint decoding. Joint de-
coding does not involve a consensus or minimum-
Bayes-risk decoding objective; indeed, their best
results come from standard max-derivation decod-
ing (with a multi-system grammar). More impor-
tantly, their computations rely on a correspondence
between nodes in the hypergraph outputs of differ-
ent systems, and so they can only joint decode over
models with similar search strategies. We combine a
phrase-based model that uses left-to-right decoding
with two hierarchical systems that use bottom-up de-
coding ? a scenario to which joint decoding is not
applicable. Though Liu et al (2009) rightly point
out that most models can be decoded either left-to-
6A single hypothesis can be represented as a forest, while an
unweighted forest could be assigned a uniform distribution.
979
right or bottom-up, such changes can have substan-
tial implications for search efficiency and search er-
ror. We prefer to maintain the flexibility of using dif-
ferent search strategies in each component system.
Li et al (2009a) is another related technique for
combining translation systems by leveraging model
predictions of n-gram features. K-best lists of par-
tial translations are iteratively reranked using n-
gram features from the predictions of other mod-
els (which are also iteratively updated). Our tech-
nique differs in that we use no k-best approxima-
tions, have fewer parameters to learn (one consensus
weight vector rather than one for each collaborating
decoder) and produce only one output, avoiding an
additional system combination step at the end.
5 Experiments
We report results on the constrained data track of the
NIST 2008 Arabic-to-English (ar-en) and Chinese-
to-English (zh-en) translation tasks.7 We train on all
parallel and monolingual data allowed in the track.
We use the NIST 2004 eval set (dev) for optimiz-
ing parameters in model combination and test on
the NIST 2008 evaluation set. We report results
using the IBM implementation of the BLEU score
which computes the brevity penalty using the clos-
est reference translation for each segment (Papineni
et al, 2002). We measure statistical significance us-
ing 95% confidence intervals computed using paired
bootstrap resampling. In all table cells (except for
Table 3) systems without statistically significant dif-
ferences are marked with the same superscript.
5.1 Base Systems
We combine outputs from three systems. Our
phrase-based system is similar to the alignment tem-
plate system described by Och and Ney (2004).
Translation is performed using a standard left-
to-right beam-search decoder. Our hierarchical
systems consist of a syntax-augmented system
(SAMT) that includes target-language syntactic cat-
egories (Zollmann and Venugopal, 2006) and a
Hiero-style system with a single non-terminal (Chi-
ang, 2007). Each base system yields state-of-the-art
translation performance, summarized in Table 1.
7http://www.nist.gov/speech/tests/mt
BLEU (%)
ar-en zh-en
Sys Base dev nist08 dev nist08
PB MAX 51.6 43.9 37.7 25.4
PB MBR 52.4? 44.6? 38.6? 27.3?
PB CON 52.4? 44.6? 38.7? 27.2?
Hiero MAX 50.9 43.3 40.0 27.2
Hiero MBR 51.4? 43.8? 40.6? 27.8
Hiero CON 51.5? 43.8? 40.5? 28.2
SAMT MAX 51.7 43.8 40.8? 28.4
SAMT MBR 52.7? 44.5? 41.1? 28.8?
SAMT CON 52.6? 44.4? 41.1? 28.7?
Table 1: Performance of baseline systems.
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
Best MAX system 51.7 43.9 40.8 28.4
Best MBR system 52.7 44.5 41.1 28.8?
MC Conjoin/SI 53.5 45.3 41.6 29.0?
Table 2: Performance from the best single system for
each language pair without consensus decoding (Best
MAX system), the best system with minimum Bayes risk
decoding (Best MBR system), and model combination
across three systems.
For each system, we report the performance of
max-derivation decoding (MAX), hypergraph-based
MBR (Kumar et al, 2009), and a linear version of
forest-based consensus decoding (CON) (DeNero et
al., 2009). MBR and CON differ only in that the first
uses n-gram posteriors, while the second uses ex-
pected n-gram counts. The two consensus decoding
approaches yield comparable performance. Hence,
we report performance for hypergraph-based MBR
in our comparison to model combination below.
5.2 Experimental Results
Table 2 compares model combination (MC) to the
best MAX and MBR systems. Model combination
uses a conjoined search space wherein each hyper-
edge is annotated with 21 features: 12 n-gram poste-
rior features vni computed from the PB/Hiero/SAMT
forests for n ? 4; 4 n-gram posterior features vn
computed from the conjoined forest; 1 length fea-
ture `; 1 feature b for the score assigned by the base
model; and 3 system indicator (SI) features ?i that
select which base system a derivation came from.
We refer to this model combination approach as MC
980
BLEU (%)
ar-en zh-en
Strategy dev nist08 dev nist08
Best MBR system 52.7 44.5 41.1 28.8
MBR Conjoin 52.3 44.5 40.5 28.3
MBR Conjoin/feats-best 52.7 44.9 41.2 28.8
MBR Conjoin/SI 53.1 44.9 41.2 28.9
MC 1-best HG 52.7 44.6 41.1 28.7
MC Conjoin 52.9 44.6 40.3 28.1
MC Conjoin/base/SI 53.5 45.1 41.2 28.9
MC Conjoin/SI 53.5 45.3 41.6 29.0
Table 3: Model Combination experiments.
Conjoin/SI. Model combination improves over the
single best MAX system by 1.4 BLEU in ar-en and
0.6 BLEU in zh-en, and always improves over MBR.
This improvement could arise due to multiple rea-
sons: a bigger search space, the consensus features
from constituent systems, or the system indicator
features. Table 3 teases apart these contributions.
We first perform MBR on the conjoined hyper-
graph (MBR-Conjoin). In this case, each edge is
tagged with 4 conjoined n-gram features vn, along
with length and base model features. MBR-Conjoin
is worse than MBR on the hypergraph from the
single best system. This could imply that either
the larger search space introduces poor hypotheses
or that the n-gram posteriors obtained are weaker.
When we now restrict the n-gram features to those
from the best system (MBR Conjoin/feats-best),
BLEU scores increase relative to MBR-Conjoin.
This implies that the n-gram features computed over
the conjoined hypergraph are weaker than the corre-
sponding features from the best system.
Adding system indicator features (MBR Con-
join+SI) helps the MBR-Conjoin system consider-
ably; the resulting system is better than the best
MBR system. This could mean that the SI features
guide search towards stronger parts of the larger
search space. In addition, these features provide a
normalization of scores across systems.
We next do several model-combination experi-
ments. We perform model combination using the
search space of only the best MBR system (MC
1best HG). Here, the hypergraph is annotated with
n-gram features from the 3 base systems, as well as
length and base model features. A total of 3 ? 4 +
1 + 1 = 14 features are added to each edge. Sur-
BLEU (%)
ar-en zh-en
Approach Base dev nist08 dev nist08
Sent-level MAX 51.8? 44.4? 40.8? 28.2?
Word-level MAX 52.0? 44.4? 40.8? 28.1?
Sent-level MBR 52.7+ 44.6? 41.2 28.8+
Word-level MBR 52.5+ 44.7? 40.9 28.8+
MC-conjoin-SI 53.5 45.3 41.6 29.0+
Table 4: BLEU performance for different system and
model combination approaches. Sentence-level and
word-level system combination operate over the sentence
output of the base systems, which are either decoded to
maximize derivation score (MAX) or to minimize Bayes
risk (MBR).
prisingly, n-gram features from the additional sys-
tems did not help select a better hypothesis within
the search space of a single system.
When we expand the search space to the con-
joined hypergraph (MC Conjoin), it performs worse
relative to MC 1-best. Since these two systems are
identical in their feature set, we hypothesize that
the larger search space has introduced erroneous hy-
potheses. This is similar to the scenario where MBR
Conjoin is worse than MBR 1-best. As in the MBR
case, adding system indicator features helps (MC
Conjoin/base/SI). The result is comparable to MBR
on the conjoined hypergraph with SI features.
We finally add extra n-gram features which are
computed from the conjoined hypergraph (MC Con-
join + SI). This gives the best performance although
the gains over MC Conjoin/base/SI are quite small.
Note that these added features are the same n-gram
features used in MBR Conjoin. Although they are
not strong by themselves, they provide additional
discriminative power by providing a consensus score
across all 3 base systems.
5.3 Comparison to System Combination
Table 4 compares model combination to two sys-
tem combination algorithms. The first, which we
call sentence-level combination, chooses among the
base systems? three translations the sentence that
has the highest consensus score. The second, word-
level combination, builds a ?word sausage? from
the outputs of the three systems and chooses a path
through the sausage with the highest score under
a similar model (Macherey and Och, 2007). Nei-
981
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
HG-expand 52.7? 44.5? 41.1? 28.8?
HG-noexpand 52.7? 44.5? 41.1? 28.8?
Table 5: MBR decoding on the syntax augmented system,
with and without hypergraph expansion.
ther system combination technique provides much
benefit, presumably because the underlying systems
all share the same data, pre-processing, language
model, alignments, and code base.
Comparing system combination when no consen-
sus (i.e., minimum Bayes risk) decoding is utilized
at all, we find that model combination improves
upon the result by up to 1.1 BLEU points. Model
combination also performs slightly better relative to
system combination over MBR-decoded systems. In
the latter case, system combination actually requires
more computation compared to model combination;
consensus decoding is performed for each system
rather than only once for model combination. This
experiment validates our approach. Model combina-
tion outperforms system combination while avoid-
ing the challenge of aligning translation hypotheses.
5.4 Algorithmic Improvements
Section 3 describes two improvements to comput-
ing n-gram posteriors: hypergraph expansion for n-
gram locality and exact posterior computation. Ta-
ble 5 shows MBR decoding with and without expan-
sion (Algorithm 2) in a decoder that collapses nodes
due to language model back-off. These results show
that while expansion is necessary for correctness, it
does not affect performance.
Table 6 compares exact n-gram posterior compu-
tation (Algorithm 1) to the approximation described
by Kumar et al (2009). Both methods yield identical
results. Again, while the exact method guarantees
correctness of the computation, the approximation
suffices in practice.
6 Conclusion
Model combination is a consensus decoding strat-
egy over a collection of forests produced by multi-
ple machine translation systems. These systems can
BLEU (%)
ar-en zh-en
Posteriors dev nist08 dev nist08
Exact 52.4? 44.6? 38.6? 27.3?
Approximate 52.5? 44.6? 38.6? 27.2?
Table 6: MBR decoding on the phrase-based system with
either exact or approximate posteriors.
have varied decoding strategies; we only require that
each system produce a forest (or a lattice) of trans-
lations. This flexibility allows the technique to be
applied quite broadly. For instance, de Gispert et al
(2009) describe combining systems based on mul-
tiple source representations using minimum Bayes
risk decoding?likewise, they could be combined
via model combination.
Model combination has two significant advan-
tages over current approaches to system combina-
tion. First, it does not rely on hypothesis alignment
between outputs of individual systems. Aligning
translation hypotheses accurately can be challeng-
ing, and has a substantial effect on combination per-
formance (He et al, 2008). Instead of aligning hy-
potheses, we compute expectations of local features
of n-grams. This is analogous to how BLEU score is
computed, which also views sentences as vectors of
n-gram counts (Papineni et al, 2002) . Second, we
do not need to pick a backbone system for combina-
tion. Choosing a backbone system can also be chal-
lenging, and also affects system combination perfor-
mance (He and Toutanova, 2009). Model combina-
tion sidesteps this issue by working with the con-
joined forest produced by the union of the compo-
nent forests, and allows the consensus model to ex-
press system preferences via weights on system in-
dicator features.
Despite its simplicity, model combination pro-
vides strong performance by leveraging existing
consensus, search, and training techniques. The
technique outperforms MBR and consensus decod-
ing on each of the component systems. In addition,
it performs better than standard sentence-based or
word-based system combination techniques applied
to either max-derivation or MBR outputs of the indi-
vidual systems. In sum, it is a natural and effective
model-based approach to multi-system decoding.
982
References
Ciprian Chelba and M. Mahajan. 2009. A dynamic
programming algorithm for computing the posterior
probability of n-gram occurrences in automatic speech
recognition lattices. Personal communication.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum bayes risk combination of translation
hypotheses from alternative morphological decompo-
sitions. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Con-
ference on Applied Natural Language Processing.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
Workshop on Syntax and Structure in Statistical Trans-
lation.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009a. Collaborative decoding: Partial
hypothesis re-ranking using translation consensus be-
tween decoders. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Association for Compu-
tational Linguistics and IJCNLP.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Wolfgang Macherey and Franz Och. 2007. An empirical
study on computing consensus translations from mul-
tiple machine translation systems. In EMNLP, Prague,
Czech Republic.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(4):417 ? 449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie J.
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Fuliang Weng, Andreas Stolcke, and Ananth Sankar.
1998. Efficient lattice representation and generation.
In Intl. Conf. on Spoken Language Processing.
Yong Zhao and Xiaodong He. 2009. Using n-gram based
features for machine translation system combination.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the NAACL 2006 Workshop on statisti-
cal machine translation.
983
