First Joint Conference on Lexical and Computational Semantics (*SEM), pages 275?281,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UABCoRAL: A Preliminary study for Resolving the Scope of Negation
Binod Gyawali, Thamar Solorio
CoRAL Lab
Department of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, Alabama, USA
{bgyawali,solorio}@cis.uab.edu
Abstract
This paper describes our participation in the
closed track of the *SEM 2012 Shared Task
of finding the scope of negation. To perform
the task, we propose a system that has three
components: negation cue detection, scope of
negation detection, and negated event detec-
tion. In the first phase, the system creates a
lexicon of negation signals from the training
data and uses the lexicon to identify the nega-
tion cues. Then, it applies machine learning
approaches to detect the scope and negated
event for each negation cue identified in the
first phase. Using a preliminary approach, our
system achieves a reasonably good accuracy
in identifying the scope of negation.
1 Introduction
All human language samples, either written or spo-
ken, contain some information in negated form. In
tasks such as information retrieval, sometimes, we
should consider only the positive information of an
event and disregard its negation information, and
vice versa. For example, while searching for the pa-
tients with diabetes, we should not include a patient
who has a clinical report saying No symptoms of di-
abetes were observed. Thus, finding the negation
and its scope is important in tasks where the nega-
tion and assertion information need to be treated dif-
ferently. However, most of the systems developed
for processing natural language data do not consider
negations present in the sentences. Although various
works (Morante et al, 2008; Morante and Daele-
mans, 2009; Li et al, 2010; Councill et al, 2010;
Apostolova et al, 2011) have dealt with the identifi-
cation of negations and their scope in sentences, this
is still a challenging task.
The first task in *SEM 2012 Shared
Task (Morante and Blanco, 2012) is concerned
with finding the scope of negation. The task
includes identifying: i) negation cues, ii) scope of
negation, and iii) negated event for each negation
present in the sentences. Negation cue is a word,
part of a word, or a combination of words that
carries the negation information. Scope of negation
in a sentence is the longest group of words in
the sentence that is influenced by the negation
cue. Negated event is the shortest group of words
that is actually affected by the negation cue. In
Example (1) below, word no is a negation cue, the
discontinuous word sequences ?I gave him? and
?sign of my occupation? are the scopes, and ?gave?
is the negated event.
(1) I [gave] him no sign of my occupation.
In this paper, we propose a system to detect the
scope of negation for the closed track of *SEM 2012
Shared Task. Our system uses a combination of
a rule based approach, and a machine learning ap-
proach. We use a rule based approach to create a
lexicon of all the negation words present in the train-
ing data. Then we use this lexicon to detect the
negation cues present in the test data. We do a pre-
liminary analysis of finding the scope of negation
and the negated events by applying a machine learn-
ing approach, and using basic features created from
the words, lemmas, and parts-of-speech (POS) tags
of words in the sentences. The F-measure scores
275
achieved by our system are about 85% for negation
cue detection, 65% in full scope identification, 48%
in negated event detection, and 39% in identifying
full negation. Our error analysis shows that the use
of lexicon is not very appropriate to detect the nega-
tion cues. We also describe the challenges in identi-
fying the scope and the negated events.
2 Problem Description
The *SEM 2012 shared task competition provided
three data sets: training, development, and test data
set. Each sentence in each data set is split into
words. The dataset contains the information such
as lemma, part of speech, and other syntactic infor-
mation of each word. Each sentence of training and
development data is annotated with negation cues,
scopes and negated events. Using the training and
the development data, the task is to identify negation
cues, scopes and negated events in all unannotated
sentences of the test data.
Sentence
tokens
Negation
cue
Scope Negated
event
I - I -
am - am -
not not - -
sure - sure sure
whether - whether -
I - I -
left - left -
it - it -
here - here -
Table 1: An example of negation cue, scope and the
negated event
A sentence can contain more than one negation
cue. Negation cues in the data set can be i) a sin-
gle word token such as n?t, nowhere, ii) a contin-
uous sequence of two or more words, such as no
more, by no means or iii) two or more discontinu-
ous words such as ..neither...nor... A negation cue
is either a part or same as its corresponding nega-
tion word. This corresponding negation word is re-
ferred as a negation signal in the remaining sections
of the paper. For example, for a negation signal
unnecessary, the negation cue is un, and similarly,
for a negation signal needless, the negation cue is
less.
Scope of a negation in a sentence can be a con-
tinuous sequence of words or a discontinuous set
of words in the sentence. Scope of negation some-
times includes the negation word. A negation word
may not have a negated event. Presence of a negated
event in a sentence depends upon the facts described
by the sentence. Non-factual sentences such as in-
terrogative, imperative, and conditional do not con-
tain negated events. Morante and Daelemans (2012)
describe the details of the negation cue, scope, and
negated event, and the annotation guidelines. An ex-
ample of the task is shown in Table 1.
3 System Description
We decompose the system to identify the scope of
negation into three tasks. They are:
1. Finding the negation cue
2. Finding the scope of negation
3. Finding the negated event
The scope detection and the negated event de-
tection tasks are dependent on the task of finding
the negation cue. But the scope detection and the
negated event detection tasks are independent of
each other.
We identify the negation cues present in the test
data based on a lexicon of negation signals that
are present in the training and the development
data. The tasks of identifying scope of negation and
negated event are modeled as classification prob-
lems. To identify scope and negated event, we train
classifiers with the instances created from the train-
ing data provided. We create test instances from the
test data annotated with negation cues predicted by
our cue detection component. Due to the use of test
data annotated by our cue detection component, the
false negative rate in predicting the negation cues is
propagated to the scope detection as well as negated
event detection components. The details of all the
three components are described in the subsections
below.
3.1 Identifying the negation cue
In this task, we identify all the negation cues present
in the sentences. We group the negation cues under
three types depending upon how they are present in
the data. They are: single word cues, continuous
276
multiword cues, and discontinuous multiword cues.
All the cues present in the training and development
datasets are shown in Table 2.
Cue types Cues
Single word
cues
absence, dis, except, fail, im, in, ir, less, n?t,
neglected, neither, never, no, nobody, none,
nor, not, nothing, nowhere, prevent, refused,
save, un, without
Continuous
multiword
cues
no more, rather than, by no means, nothing
at all, on the contrary, not for the world
Discontinuous
multiword
cues
neither nor, no nor, not not
Table 2: Negation cues present in training and develop-
ment data
In the training and development data, multiword
negation cues account for only 1.40% of the total
negation cues. At this stage, we decided to focus
on identifying the single word negation cues. The
system first creates a lexicon that contains the pairs
of negation cues and their corresponding negation
signals for all the single word negation cues present
in the training and the development datasets. In or-
der to identify a negation cue in the test set, the sys-
tem searches all the words in the sentences of the
test data that match the negation signals of the lexi-
con. For each word that matches, it assigns the cor-
responding cue of the signal from the lexicon as its
negation cue.
3.2 Identifying the scope of negation
We apply a machine learning technique to identify
the scope of negation. For each negation cue present
in a sentence, we create the problem instances as the
tuple of the negation signal and each word present
in the same sentence. To create the instances, we
use only those sentences having at least one nega-
tion. For training, we create instances from the train-
ing data, but we consider only those words that are
within a window of size 20 from the negation signal
and within the sentence boundary. We restricted the
words to be within the window in order to minimize
the problem of imbalanced data. This window was
chosen following our observation that only 1.26%
of the scope tokens go beyond the 20 word win-
dow from the negation signal. Including the words
beyond this window causes a major increase in the
negative instances resulting in a highly imbalanced
training set. While creating test instances, we do not
restrict the words by window size. This restriction is
not done in order to include all the words of the sen-
tences in the test instances. An instance is labeled
as positive if the word used to create the instance is
the scope of the negation signal; else it is labeled as
negative.
We extract 10 features to identify the scope of
negation as follows:
1. Negation signal in the tuple
2. Lemma of the negation signal
3. POS tag of the negation signal
4. Word in the tuple
5. Lemma of the word in the tuple
6. POS tag of the word in the tuple
7. Distance between the negation signal and the
word in terms of number of words
8. Position of the word from the negation signal
(left, right)
9. Whether a punctuation character (?,?, ?:?,?;?) ex-
ists between the word and the negation signal
10. Sequence of POS tags in between the negation
signal and the word
After the classification, if an instance is predicted
as positive, the word used to create the instance is
considered as the scope of the negation signal. If a
negation signal has prefix such as ?dis?, ?un?, ?in?,
?ir?, or ?im?, the scope of negation includes only the
part of word (signal) excluding the prefix. Thus, for
each negation signal having these prefix, we remove
the prefix from the signal and consider the remain-
ing part of it as the scope, regardless of whether the
classifier classifies the instance pair as positive or
negative.
277
3.3 Identifying the negated event
The task of identifying the negated event is simi-
lar to the task of identifying the scope of negation.
The process of creating the instances for this task
is almost the same to that of finding the scope of
negation, except that, we limit the window size to
4 words from the negation signal. 4.24% of the
negated events lie away from the 4 word window.
Beyond this window, the events are very sparse and
a small increment in the window size leads to abrupt
increase in negative instances and creates an imbal-
ance in the data. The 4 word window size was se-
lected based on the best result obtained among var-
ious experiments performed with different window
sizes greater than and equal to 4. The same rule
applies while creating instances for training data as
well as test data. We use only nine features in this
step, excluding the 9th feature used in the scope de-
tection. We also apply the same rule of mapping the
negation signals starting with ?dis?, ?un?, ?in?, ?ir?,
and ?im? to the negated event as in the previous step.
4 Experimental Settings
We evaluated our system only on the test data of the
shared task. For the machine learning tasks, we used
the SVM light classifier (Joachims, 1999) with 4th
degree polynomial kernel and other default param-
eters. The identification of cues, scopes, negated
events, and full negation are evaluated on the basis
of the F-measures. We also use ?B? variant for cues,
scopes, negated events and the full negation for eval-
uation. The precision of ?B? variant is calculated as
the ratio of true positives to the system count. Iden-
tification of cues and negated events are measured
independent of any other steps. But the identifica-
tion of the scopes is measured depending upon the
correct identification of cues in three different ways
as follows:
i) scopes (cue match): the cue has to be correct
for the scope to be correct
ii) scopes (no cue match): the system must iden-
tify part of the cue for the scope to be correct
iii) scope tokens (no cue match): a part of the sys-
tem identified cue must overlap with the gold stan-
dard cue for the scope tokens to be correct
The F1 score of the full negation detection was
used to rank the systems of the participants. The
details about the evaluation measures can be found
in Morante and Blanco (2012).
5 Results Analysis
The results obtained by our system over the test data
are shown in Table 3. The results obtained by each
component, and their analysis are described in the
subsections below.
5.1 Identifying the negation cues
The system is able to achieve an 85.77% F1 score in
the task of identifying the negation cues using a sim-
ple approach based on the lexicon of the negation
signals. Because of the system?s inability to iden-
tify multiword negation cues, it could not detect the
multiword cues such as ..neither..nor.., ..absolutely
nothing.., ..far from.., ..never more.., that account for
3.5% of the total negation cues present in the test
data.
The accuracy of the system is limited by the cov-
erage of the lexicon. Due to the low coverage of the
lexicon, the system fails to identify signals such as
ceaseless, discoloured, incredulity, senseless,
and unframed that are present only in the test data.
These signals account for 4.5% of the total negation
signals present in the test data. Some words such
as never, nothing, not, n?t, no, and without are
mostly present as the negation signals in the data.
But these words are not always the negation signals.
The phrase no doubt is present nine times in the test
data, but the word no is a negation signal in only
four of them. This accounts for 1.89% error in the
negation cue detection. The word save is present
once as a negation signal in the training data, but it
is never a negation signal in the test data. Therefore,
our lexicon based system invariably predicts two oc-
currences of save in the test data as negation signals.
5.2 Identifying the scope of negation
The system achieves 63.46% F1 score in identifying
scopes with cue match, 64.76% F1 score in identify-
ing scopes with no cue match, and 76.23% F1 score
in identifying scope tokens with no cue match. The
results show that our system has a higher precision
than recall in identifying the scope. As mentioned
278
gold system tp fp fn precision (%) recall (%) F1 (%)
Cues 264 284 226 37 38 85.93 85.61 85.77
Scopes (cue match) 249 239 132 35 117 79.04 53.01 63.46
Scopes (no cue match) 249 239 132 35 113 79.53 54.62 64.76
Scope tokens (no cue match) 1805 1456 1243 213 562 85.37 68.86 76.23
Negated (no cue match) 173 104 65 35 104 65.00 38.46 48.33
Full negation 264 284 73 37 191 66.36 27.65 39.04
Cues B 264 284 226 37 38 79.58 85.61 82.48
Scopes B (cue match) 249 239 132 35 117 55.23 53.01 54.10
Scopes B (no cue match) 249 239 132 35 113 56.90 54.62 55.74
Negated B (no cue match) 173 104 65 35 104 62.50 38.46 47.62
Full negation B 264 284 73 37 191 25.70 27.65 26.64
Total sentences: 1089
Negation sentences: 235
Negation sentences with errors: 172
% Correct sentences: 81.73
% Correct negation sentences: 26.81
Table 3: Results of the system
earlier, the negation cues identified in the first task
are used to identify the scope of negation and the
negated events. Using the test data with 15% error
in negation cues as the input to this component and
some of the wrong predictions of the scope by this
component led to a low recall value in the scope de-
tection.
The results show that the system works well when
a negation signal has fewer scope tokens and when
the scope tokens are closer to the negation signal.
There are some cases where the system could not
identify the scope tokens properly. It is unable to de-
tect the scope tokens that are farther in distance from
the negation signals. The system is not performing
well in predicting the discontinuous scopes. When
a negation cue has discontinuous scope, mostly the
system predicts one sequence of words correctly but
could not identify the next sequence. In sentence
(2) in the example below, the underlined word se-
quences are the discontinuous scopes of the nega-
tion cue not. In the sentence, our system predicts
only the second sequence of scope, but not the first
sequence. In some cases, our system does not have a
good coverage of scope tokens. In sentence (3), the
underlined word sequence is the scope of the signal
no, but our system detects only at ninety was hard-
ship as its scope. These inabilities to detect the full
scope have led to have a higher accuracy in predict-
ing the partial scope tokens (76.23%) than predicting
the full scope (64.76%).
(2) the box is a half pound box of honeydew to-
bacco and does not help us in any way
(3) ...a thermometer at ninety was no hardship
(4) ...I cannot see anything save very vague
indications
Analyzing the results, we see that the error in pre-
dicting the scope of the negation is high when the
scope is distributed in two different phrases. In the
example (2) above, does not help us in any way is
a single verb phrase and all the scope within the
phrase is correctly identified by our system. The
box being a separate phrase, it is unable to identify
it. However, in some cases such as example (4), the
system could not identify any scope tokens for nega-
tion cue not.
Some of the findings of previous works have
shown that the features related to syntactic path are
helpful in identifying the scope of negation. Li et
al. (2010) used the syntactic path from the word to
the negation signal and showed that this helped to
improve the accuracy of scope detection. Similarly,
work by Councill et al (2010) showed that the ac-
curacy of scope detection could be increased using
the features from the dependency parse tree. In our
experiment, there was a good improvement in the
scope detection rate when we included ?sequence
of POS tags? between the negation signal and the
word as a feature. This improvement after including
the sequence of POS tags feature and its consistency
279
with the previous works implies that adding path re-
lated features might help to improve the accuracy in
scope detection.
5.3 Identifying the negated event
We are able to achieve an F1 score of 48.33% in pre-
dicting the negated events, which is the lowest score
among all three components. As in the scope de-
tection task, error in negation cue detection led to
lower the recall rate of the negated event detection
system. The accuracy of full negation is based on
the correct identification of the negation cues, scope
and the negated events of all the negations present
in the sentences. The output shows that there are
many cases where negation cues and the scope are
correctly identified but there is an error in identify-
ing the negated events. The higher error in predict-
ing the negated events led to reduce the score of full
negation and achieve an F1 score of 39.04%.
Our system is unable to detect some negated
events even though they are adjacent to the nega-
tion signal. This shows that the use of simple fea-
tures extracted from words, lemmas, and POS tags
is not enough to predict the negated events properly.
Adding features related to words in left and right of
the negation signal and the path feature may help to
improve the detection of negated events.
In order to analyze the impact of error in the nega-
tion cue detection component upon the scope and
negated event detection components, we performed
an experiment using the gold standard negation cues
to detect the scope and the negated events. F1 scores
achieved by this system are 73.1% in full scope de-
tection, 54.87% in negated event detection, 81.46%
in scope tokens detection, and 49.57% in full nega-
tion detection. The result shows that there is al-
most 10% increment in the F1 score in all the com-
ponents. Thus, having an improved cue detection
component greatly helps to improve the accuracy of
scope and negated event detection components.
6 Discussion and Conclusion
In this paper we outline a combination of a rule
based approach and a machine learning approach to
identify the negation cue, scope of negation, and the
negated event. We show that applying a basic ap-
proach of using a lexicon to predict the negation cues
achieves a considerable accuracy. However, our sys-
tem is unable to identify the negation cues such as
never, not, nothing, n?t, and save that can appear
as a negation signal as well as in other non-negated
contexts. It also cannot cover the negation cues of
the signals that are not present in the training data.
Moreover, in order to improve the overall accuracy
of the scope and negated event detection, we need an
accurate system to detect the negation cues since the
error in the negation cue detection propagates to the
next steps of identifying the scope and the negated
event. It is difficult to identify the scope of nega-
tions that are farther in distance from the negation
signal. Detecting the tokens of the scope that are
discontinuous is also challenging.
As future work, we would like to extend our task
to use a machine learning approach instead of the
lexicon of negation signals to better predict the nega-
tion cues. The system we presented here uses a pre-
liminary approach without including any syntactic
information to detect the scope and negated events.
We would also incorporate syntactic information to
identify the scope and negated events in our future
work. To improve the accuracy of identifying the
scope and the negated events, adding other features
related to the neighbor words of the negation signal
might be helpful. In our tasks, we limit the scope
and negated event instances by the window size in
order to avoid imbalance data problem. Another in-
teresting work to achieve better accuracy could be to
use other approaches of imbalanced dataset classifi-
cation instead of limiting the training instances by
the window size.
References
Emilia Apostolova, Noriko Tomuro, and Dina Demner-
Fushman. 2011. Automatic extraction of lexico-
syntactic patterns for detection of negation and spec-
ulation scopes. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 283?287, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
280
cessing, NeSp-NLP ?10, pages 51?59, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in ker-
nel methods: support sector searning, pages 169?184.
MIT Press, Cambridge, MA, USA.
Junhui Li, Guodong Zhou, Hongling Wang, and Qiaom-
ing Zhu. 2010. Learning the scope of negation via
shallow semantic parsing. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10, pages 671?679, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
Shared Task: Resolving the Scope and Focus of Nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics (*SEM 2012),
Montreal, Canada.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning, CoNLL
?09, pages 21?29, Stroudsburg, PA, USA.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), Istanbul.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 715?724. Association for Compu-
tational Linguistics.
281
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 176?184,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Grading the Quality of Medical Evidence
Binod Gyawali, Thamar Solorio
CoRAL Lab
Department of Computer and Information Sciences
University of Alabama at Birmingham, AL, USA
{bgyawali,solorio}@cis.uab.edu
Yassine Benajiba
Clinical Decision Support Solutions Department
Philips Research North America, Briarcliff Manor, NY, USA
yassine.benajiba@philips.com
Abstract
Evidence Based Medicine (EBM) is the prac-
tice of using the knowledge gained from the
best medical evidence to make decisions in
the effective care of patients. This medi-
cal evidence is extracted from medical docu-
ments such as research papers. The increas-
ing number of available medical documents
has imposed a challenge to identify the ap-
propriate evidence and to access the quality
of the evidence. In this paper, we present
an approach for the automatic grading of ev-
idence using the dataset provided by the 2011
Australian Language Technology Association
(ALTA) shared task competition. With the
feature sets extracted from publication types,
Medical Subject Headings (MeSH), title, and
body of the abstracts, we obtain a 73.77%
grading accuracy with a stacking based ap-
proach, a considerable improvement over pre-
vious work.
1 Introduction
?Evidence Based Medicine (EBM) is the conscien-
tious, explicit, and judicious use of current best evi-
dence in making decisions about the care of individ-
ual patients? (Sackett et al, 1996). EBM requires to
identify the best evidence, understand the method-
ology and strength of the approaches reported in
the evidence, and bring relevant findings into clin-
ical practice. Davidoff et al (1995) express EBM in
terms of five related ideas. Their ideas imply that
the conclusions should be derived based on the best
evidence available, the clinical decisions should be
made based on the conclusions derived, and the per-
formance of the clinical decisions should be evalu-
ated constantly. Thus, physicians practicing EBM
should be constantly aware of the new ideas and
the best methodologies available based on the most
recent literature. But the amount of clinical docu-
ments available is increasing everyday. For exam-
ple, Pubmed, a service of the US National Library of
Medicine contains more than 21 million citations for
biomedical literature from MEDLINE, life science
journals, and online books (last updated on Decem-
ber 7, 2011) 1. The abundance of digital informa-
tion makes difficult the task of evaluating the quality
of results presented and the significance of the con-
clusions drawn. Thus, it has become an important
task to grade the quality of evidence so that the most
significant evidence is incorporated into the clinical
practices.
There are several scale systems available to grade
medical evidence. Some of them are: hierarchy
of evidence proposed by Evans (2003), Grading of
Recommendations Assessment, Development, and
Evaluation (GRADE) scale by GRADE (2004), and
Strength of Recommendation Taxonomy (SORT)
scale by Ebell et al (2004). The SORT scale ad-
dresses the quality, quantity, and consistency of evi-
dence and proposes three levels of ratings: A, B, and
C. Grade A is recommended based on the consistent,
good-quality patient-oriented evidence, grade B is
based on the inconsistent or limited-quality patient-
oriented evidence, and grade C is based on consen-
sus, disease-oriented evidence, usual practice, ex-
pert opinion or case studies.
1http://www.ncbi.nlm.nih.gov/books/NBK3827/
176
The Australasian Language Technology Associa-
tion (ALTA) 2011 organized the shared task compe-
tition2 to build an automatic evidence grading sys-
tem for EBM based on the SORT grading scale. We
carry out our experiments using the data set provided
for the competition and compare the accuracy of
grading the evidence by applying basic approaches
and an ensemble (stacking) based approach of clas-
sification. We show that the later approach can
achieve 73.77% of grading accuracy, a significant
improvement over the basic approaches. We further
extend our experiments to show that, using feature
sets generated from the method and conclusion sec-
tions of the abstracts helps to obtain higher accuracy
in evidence grading than using a feature set gener-
ated from the entire body of the abstracts.
2 Related Work
To the best of our knowledge, automatic evidence
grading based on a grading scale was initiated by
Sarker et al (2011). Their work was based on the
SORT scale to grade the evidence using the corpus
developed by Molla-Aliod (2010). They showed
that using only publication types as features could
yield an accuracy of 68% while other information
like publication types, journal names, publication
years, and article titles could not significantly help
to improve the accuracy of the grading. Molla-Aliod
and Sarker (2011) worked on the evidence grading
problem of 2011 ALTA shared task and achieved
an accuracy of 62.84% using three sequential clas-
sifiers, each trained by one of the following feature
sets: word n-grams from the abstract, publication
types, and word n-grams from the title. They ap-
plied a three way classification approach where the
instances classified as A or C were removed from
the test set and labeled as such, while instances
classified as B were passed to the next classifier in
the pipeline. They repeated this process until they
reached the end of three sequential classifiers.
Most of the EBM related work is focused on ei-
ther the identification of important statements from
the medical abstracts or the classification of med-
ical abstracts to facilitate the retrieval of impor-
tant documents. Work by Demner-Fushman et al
(2006), Dawes et al (2007), Kim et al (2011) au-
2http://www.alta.asn.au/events/sharedtask2011
tomatically identify the key statements in the med-
ical abstracts and classify them into different levels
that are considered important for EBM practitioners
in making decisions. Kilicoglu et al (2009) worked
on recognizing the clinically important medical ab-
stracts using an ensemble learning method (stack-
ing). They used different combinations of feature
vectors extracted from documents to classify the ev-
idence into relevant or non relevant classes. They
approached the problem as a binary classification
problem without using any grading scales.
Systematic Reviews (SRs) are very important
to support EBM. Creating and updating SRs is
highly inefficient and needs to identify the best evi-
dence. Cohen et al (2010) used a binary classifica-
tion system to identify the documents that are most
likely to be included in creating and updating SRs.
In this work, we grade the quality of evidence
based on the SORT scale, that is different from most
of the existing works related to classification of ab-
stracts and identification of key statements of ab-
stracts. We work on the same problem as by Molla-
Aliod and Sarker (2011) but, we undertake the prob-
lem with a different approach and use different sets
of features.
3 Dataset
We use the data of 2011 ALTA shared task compe-
tition that contains three different sets: training, de-
velopment and test set. The number of evidence in-
stances present in each set is shown in Table 1. Each
data set consists of instances with grades A, B, or C
based on the SORT scale. The distribution of evi-
dence grades is shown in Table 2.
Data Set No. of Evidence Instances
Training Set 677
Development Set 178
Test Set 183
Table 1: Evidence per data set
The evidence instances were obtained from the
corpus developed by Molla-Aliod and Santiago-
Martinez (2011). The corpus was generated based
on the question and the evidence based answer for
the question along with SOR grade obtained from
the ?Clinical Inquiries? section of the Journal of
177
Grades Training
set (%)
Development
set (%)
Test set
(%)
A 31.3 27.0 30.6
B 45.9 44.9 48.6
C 22.7 28.1 20.8
Table 2: Evidence distribution per grade
Family Practice (JFP). A sample question from the
JFP Clinical Inquiries section is ?How does smoking
in the home affect children with asthma??. Each ev-
idence contains at least one or more publications de-
pending upon from which publications the evidence
was generated. Each publication is an XML file con-
taining information such as abstract title, abstract
body, publication types, and MeSH terms. Each
publication is assigned at least one publication type
and zero or more MeSH terms. The MeSH terms
vocabulary 3 is developed and maintained by the
National Library of Medicine and is used in rep-
resentation, indexing and retrieval of medical doc-
uments. Some of the medical document retrieval
work emphasizes the use of MeSH terms in the ef-
ficient retrieval of documents (Trieschnigg et al,
2009; Huang et al, 2011). MeSH terms are also
used in document summarization (Bhattacharya et
al., 2011).
Figure 1: Sample data file
Each data set contains an additional grade file
with the information related to the evidence in-
stances, their grades, and the publications. A sam-
ple of the file is shown in Figure 1. The first column
contains the evidence id, the second column contains
the grades A, B, or C of the evidence based on the
SORT scale, and the remaining columns show the
publication id of each publication in the evidence.
3http://www.nlm.nih.gov/mesh
The problem in this task is to analyze the publica-
tions in each evidence provided and classify them
into A, B or C.
The dataset available for our research has ab-
stracts in two different formats. One of them con-
tains abstracts divided into sections: background,
objective, method, result, and conclusion. The other
format contains abstracts with all the information in
a single block without any sections. A sample of an
abstract having only four sections in the given data
is shown below:
Objectives: To determine the effectiveness of a muscle
strengthening program compared to a stretching program in
women with fibromyalgia (FM).
Methods: Sixty-eight women with FM were randomly as-
signed to a 12 week, twice weekly exercise program consisting
of either muscle strengthening or stretching. Outcome measures
included muscle strength (main outcome variable), flexibility,
weight, body fat, tender point count, and disease and symptom
severity scales.
Results: No statistically significant differences between
groups were found on independent t tests. Paired t tests revealed
twice the number of significant improvements in the strengthen-
ing group compared to the stretching group. Effect size scores
indicated that the magnitude of change was generally greater in
the strengthening group than the stretching group.
Conclusions: Patients with FM can engage in a specially
tailored muscle strengthening program and experience an im-
provement in overall disease activity, without a significant exer-
cise induced flare in pain. Flexibility training alone also results
in overall improvements, albeit of a lesser degree.
In the abstract above, we see that the approaches
applied for the study are described in the method
section, and the outcome and its effectiveness are
described in the conclusion section.
4 Proposed Methodology
In this paper we propose a system to identify the
correct grade of an evidence given publications in
the evidence. We deal with the problem of evi-
dence grading as a classification problem. In evi-
dence grading, basic approaches have been shown
to have poor performance. Molla-Aliod and Sarker
(2011) showed that a basic approach of using simple
bag-of-word features and a Naive Bayes classifier
achieved 45% accuracy and proposed a sequential
approach to improve the accuracy at each step. Our
preliminary studies of applying the simple classifi-
cation approach also showed similar results. Here,
we propose a stacking based approach (Wolpert,
178
1992) of evidence grading. Stacking based approach
builds a final classifier by combining the predictions
made by multiple classifiers to improve the predic-
tion accuracy. It involves two steps. In the first step,
multiple base-level classifiers are trained with dif-
ferent feature sets extracted from a dataset and the
classifiers are used to predict the classes of a sec-
ond dataset. Then, a higher level classifier is trained
using the predictions made by the base-level clas-
sifiers on the second dataset and used to predict the
classes of the actual test data. In this approach, base-
level classifiers are trained independent of each other
and allowed to predict the classes. Based on the
predictions made by these base-level classifiers, the
higher level classifier learns from those predictions
and makes a new prediction that is the final class.
Our stacking based approach of classification uses
five feature sets. In the first step of classification, we
train five classifiers using different feature sets per
classifier and use the classifiers to predict the grades
of the development dataset. Thus, at the end of the
first step, five different predictions on the develop-
ment dataset are obtained. In the second step, a new
classifier is trained using the grades predicted by the
five classifiers as features. This new classifier is then
used to predict the grades of the test dataset.
5 Features
We extracted six sets of features from the publica-
tions to perform our experiments. They are as fol-
lows:
1. Publication types
2. MeSH terms
3. Abstract title
4. Abstract body
5. Abstract method section
6. Abstract conclusion section
For feature set 1, we extracted 30 distinct publi-
cation types from the training data. For the MeSH
terms feature set, we selected 452 unique MeSH
terms extracted from the training data. The publi-
cations contained the descriptor name of the MeSH
terms having an attribute ?majortopicyn? with value
?Y? or ?N?. As MeSH terms feature set, we selected
only those MeSH term descriptor names having ma-
jortopicyn=?Y?.
We extracted the last four sets of features from
the title, body, method, and conclusion sections of
the abstracts. Here, the body of an abstract means
the whole content of the abstract, that includes back-
ground, objective, method, result, and conclusion
sections. We applied some preprocessing steps to
generate these feature sets. We also applied a feature
selection technique to reduce the number of features
and include only the high informative features from
these feature sets. The details about preprocess-
ing and feature selection techniques are described in
Section 6.
We performed all the experiments on the basis of
evidence, i.e. we created a single feature vector per
evidence. If an evidence contained more than one
publication, we generate its features as the union of
the features extracted from all its publications.
The grades of the evidence in the SORT scale
are based on the quality of evidence, basis of ex-
periments, the methodologies used, and the types of
analysis done. Grades also depend upon the effec-
tiveness of the approach used in the experiments.
The method section of an abstract contains the in-
formation related to the basis of the experiments,
such as randomized controlled trails, systematic re-
view, cohort studies, and the methods used in their
research. The conclusion section of the abstract
usually contains the assertion statements about how
strongly the experiment supports the claims. Anal-
ysis of the contents of abstracts shows that the in-
formation needed for grading on SORT scale is typ-
ically available in the method and conclusion sec-
tions, more than in the other sections of the abstracts.
Thus, we used the method and conclusion sections
of the abstracts to generate two different feature sets
so that only the features more likely to be important
in grading using the SORT rating would be included.
Separating method and conclusion sections of
the abstracts
In order to extract features from the method and con-
clusion sections, we should separate them from the
body of abstracts, which is a challenging task for
those abstracts without section headers. Of the to-
tal number of abstracts, more than one-third of the
abstracts do not contain the section headers. In or-
der to separate these sections, we used a very simple
approach based on the number of sentences present
179
in the method and conclusion sections, and the body
of the abstracts. We used the following information
to separate the method and conclusion sections from
these abstracts: i) Order of sections in the abstracts,
ii) Average number of sentences in the method and
conclusion sections of the abstracts having sections,
and iii) Average number of sentences in the entire
body of the abstracts not having sections. All the ab-
stracts having section headers contained the sections
in the same order: background, objective, method,
result and conclusion. From the available training
dataset, we calculated:
i. The average number of sentences in the method
(4.14) and conclusion (2.11) sections of the abstracts di-
vided into sections
ii. The average number of sentences (8.78) of the ab-
stracts not having sections
Based on these values, we fragmented the ab-
stracts that do not have the section headers and sepa-
rated the method and conclusion sections from them.
Table 3 shows how the method and conclusion sec-
tions of those abstracts were generated. For exam-
ple, the fourth row of the table says that, if an ab-
stract without section headers has 6, 7 or 8 sentences
(let it be n), then the 3rd, 4th and 5th sentences were
considered as the method section, and the nth sen-
tence was considered as the conclusion section.
Total sentences in
Abstracts(n)
Method Conclusion
1 None 1
2 or 3 1 n
4 or 5 2 and 3 n
6 or 7 or 8 2, 3 and 4 n
More than 8 3, 4 and 5 n-1 and n
Table 3: Selecting method and conclusion of the abstracts
having a single block
6 Experiments and Results
This section describes the two sets of experiments
performed to compare the performance of the stack-
ing based approach and the effectiveness of the base-
level classifiers used. The first set of experiments
was done to provide a baseline comparison against
our stacking based approach. The second set con-
sists of five experiments to evaluate different con-
figurations of stack based classifiers. The basic ap-
proach of classification implies the use of a single
classifier trained by using a single feature vector.
We applied preprocessing steps to generate fea-
ture sets from the title, body, method and conclusion
sections of the abstracts. The preprocessing steps
were: detecting sentences using OpenNLP Sentence
Detector4, stemming words in each sentence using
Porter Stemmer (Porter, 1980), changing the sen-
tences into lower-case, and removing punctuation
characters from the sentences. After the preprocess-
ing step, we generated features from the unigrams,
bigrams and trigrams in each part. We removed
those features from the feature sets that contained
the stopwords listed by Pubmed5 or contained any
token having a length less than three characters. To
remove the less informative features, we calculated
the information gain of the features in the training
data using Weka (Hall et al, 2009) and selected only
the top 500 high informative features for each fea-
ture set. We used the Weka SVM classifier for all the
experiments. Based on the best result obtained af-
ter a series of experiments run with different kernel
functions and regularization parameters, we chose
the SVM classifier with a linear kernel and regular-
ization parameter equals 1 for all the experiments.
We used a binary weight for all the features.
6.1 First set of experiments
In the first set, we performed nine experiments using
the basic classification approach and one experiment
using the stacking based approach. The details of
the experiments and the combinations of the features
used in them are as shown in Table 4.
The first six experiments in the table were imple-
mented by applying a basic approach of classifica-
tion and each using only a single set of features. Ex-
periments 7, 8, and 9 were similar to the first six
experiments except, they used more than one set of
features to create the feature vector. Each feature in
the experiments 7, 8, and 9 encode the section of its
origin. For example, if feature abdomen is present
in method as well as conclusion sections, it is rep-
resented as two distinct features conc abdomen and
method abdomen. In experiment 10, we applied
4http://incubator.apache.org/opennlp
5http://www.ncbi.nlm.nih.gov/books/NBK3827
/table/pubmedhelp.T43/?report=objectonly)
180
the stacking approach of classification using five
base-level classifiers. The base-level classifiers in
this experiment are the basic classifiers used in ex-
periments 1 to 5.
Exp.
No.
Features used Exp. type
1. Publication types
Basic approach
2. MeSH terms
3. Abstract title
4. Abstract method
5. Abstract conclusion
6. Abstract body
7.
Publication types,
MeSH terms
8.
Publication types,
MeSH terms,
Abstract title,
Abstract body
9.
Publication types,
MeSH terms,
Abstract title,
Abstract method,
Abstract conclusion
10.
Publication types
Stacking based
approach
MeSH terms
Abstract title
Abstract method
Abstract conclusion
Table 4: Experiments to compare basic approaches to a
stacking based approach
Figure 2 shows the results of the 10 experiments
described in Table 4 in the same order, from 1st to
10th place and the result of the experiment by Molla-
Aliod and Sarker (2011). The results show that
the stacking based approach gives the highest ac-
curacy (73.77%), outperforming all the basic ap-
proaches applying any combination of feature sets.
The stacking based approach outperforms the base-
line of a single layered classification approach (Exp
9) that uses all the five sets of features. Molla-Aliod
and Sarker (2011) showed that a simple approach of
using a single classifier and bag-of-words features
could not achieve a good accuracy (45.9%) and pro-
posed a new approach of using a sequence of classi-
fiers to achieve a better result. Similar to their simple
approach, our basic approaches could not achieve
good results, but their performance is comparable
to Molla-Aliod and Sarker (2011)?s baseline system.
The result of our stacking based approach shows that
our approach has a better accuracy than the sequen-
cial classification approach (62.84%) proposed by
Figure 2: Comparison of accuracy of basic approaches to
a stacking based approach. X-axis shows the experiments
and Y-axis shows the accuracy of the experiments. The
first nine experiments are based on the basic approach
and the tenth experiment is based on the stacking based
approach.
Molla-Aliod and Sarker (2011).
Our stacking based approach works on two lev-
els. In the first level, the base-level classifiers pre-
dict the grades of the evidence. In the next level,
these predictions are used to train a new classifier
that learns from the predictions to identify the grades
correctly. Moreover, the five feature sets used in our
experiments were unrelated to each other. For ex-
ample, the features present in MeSH headings were
different from the features used in publication types,
and similarly, the features present in the method sec-
tion of the abstract were different from the features
present in the conclusion section. Each base-level
classifier trained by one of these feature sets is spe-
cialized in that particular feature set. Thus, using
the predictions made by these specialized base-level
classifiers to train a higher level classifier helps to
better predict the grades, this cannot be achieved by
a single classifier trained by a set of features (Exp.
1, 2, 3, 4, 5, 6), or a group of different feature sets
(Exp. 7, 8, 9).
6.2 Second set of experiments
In the second set of experiments, we compared five
experiments performed varying the base-level clas-
sifiers used in our stack based approach. Experi-
ments 1 and 2 were performed using a single base-
level classifier, that means that the second classifier
is trained on only one feature. Experiments 3 and 4
were performed by using four base-level classifiers,
and experiment 5 was performed using five base-
181
level classifiers. The 5th experiment in this set is
same as the 10th experiment in the first set. The de-
tails about the feature sets used in each experiment
are shown in Table 5.
Exp.
No.
Features used No. of Base level
classifiers
1.
Publication types,
1
MeSH terms,
Abstract title,
Abstract body
2.
Publication types,
1
MeSH terms,
Abstract title,
Abstract method,
Abstract conclusion
3.
Publication types
4
MeSH terms
Abstract title
Abstract body
4.
Publication types
4
MeSH terms
Abstract title
Abstract method,
Abstract conclusion
5.
Publication types
5
MeSH terms
Abstract title
Abstract method
Abstract conclusion
Table 5: Experiments to compare stacking based ap-
proach
Figure 3 shows the accuracy of the five experi-
ments shown in Table 5 in the same order. It shows
that the accuracy of 1st and 2nd experiments is lower
than the accuracy of 3rd, 4th, and 5th experiments.
In these two experiments, a feature vector generated
from the prediction of a single base-level classifier
is used to train the higher level classifier, that is not
sufficient to make a correct decision.
Experiments 3, 4, and 5 show a considerable im-
provement in the accuracy of the grading. Compar-
ing the results of experiments 3 and 4, we see that
the 4th experiment has higher accuracy than the 3rd
one. The difference between these experiments was
the use of features from the method and conclusion
sections of the abstracts in the 4th experiment, while
using features from the entire body of abstracts in
the 3rd experiment. The higher accuracy in the 4th
experiment shows that the method and conclusion
sections of the experiment contain high informative
text that is important for evidence grading, while
Figure 3: Comparison of accuracy of the stacking based
approaches. X-axis shows the experiments and Y-axis
shows the accuracy of the experiments. 1st and 2nd ex-
periments use only one base-level classifier, 3rd and 4th
experiment are based on four base-level classifiers and
5th one uses five base-level classifiers.
the body of abstracts may contain some information
that is not relevant to the task. The same analysis
can also be inferred from the results of experiment
8 and 9 in the first set of experiments. The high-
est accuracy obtained in the 5th experiment of apply-
ing 5 base-level classifiers shows that identifying the
sections of the abstracts containing high informative
features and using a sufficient number of base-level
classifiers can help to achieve a good accuracy in ev-
idence grading.
7 Error Analysis
The result obtained by the stacking based approach
(5th experiment in Table 5) using five base-level clas-
sifiers gave a higher error rate in predicting grades
A and C, compared to the error rate in predict-
ing grade B. Most of the error is the misclassifica-
tion of A to C and vice versa. One of the possi-
ble reasons of this might be due to the use of the
feature set extracted from the conclusion section.
Among the five base-level classifiers used in the ex-
periment, the one trained by the features extracted
from the conclusion sections has the lowest accu-
racy (5th experiment in Figure 2). We evaluated the
text contained in the conclusion section of the ab-
stracts in our dataset. The section mostly contains
the assertion statements having the words showing
strong positive/negative meanings. Conclusion of A
grade evidence mostly contains the information that
strongly asserts the claim (e.g. emollient treatment
182
significantly reduced the high-potency topical cor-
ticosteroid consumption in infants with AD), while
that of C grade evidence is not strong enough to as-
sert the claim (e.g. PDL therapy should be consid-
ered among the better established approaches in the
treatment of warts, although data from this trial sug-
gest that this approach is probably not superior). It
seems that the problem might be because of not pro-
cessing the negations appropriately. So, in order to
preserve some negation information present in the
conclusion sections, we performed another experi-
ment by merging words no, not, nor with their suc-
cessor word to create a single token from the two
words. This approach still could not reduce the mis-
classification. Thus, the simple approach of extract-
ing unigram, bigram, and trigram features from the
conclusion section might not be sufficient and might
need to include higher level analysis related to as-
sertion/certainty of the statements to reduce the mis-
classification of the evidence.
Other possible reasons of the misclassification
of the evidence might be the imbalanced data set.
Our dataset (Table 2) contains higher number of in-
stances with grade B than those with grades A and C.
Moreover, the number of publications per evidence
is not uniform, that ranges from 1 to 8 publications
per evidence in the test data. Analyzing the results,
we found that misclassification of evidence having
only one publication is higher than that of the evi-
dence having more than one publication. If an ev-
idence contains only one publication, the features
of the evidence extracted from a single publication
might not be sufficient to accurately grade the evi-
dence and might lead to misclassification.
In order to evaluate the appropriateness of our
approach in extracting the method and conclusion
sections, we performed a manual inspection of ab-
stracts. We could not revise all the abstracts to ver-
ify the approach. Thus, we randomly selected 25
abstracts without section headers from the test data
and viewed the content in them. We found that the
conclusion section was appropriately extracted in al-
most all abstracts, while the selection of method sec-
tion was partially effective. Our approach was based
on the assumption that all the abstracts having many
sentences have all the sections (background, objec-
tive, method, result, and conclusion). But we found
that the abstracts do not follow the same format, and
the start sentence of the method section is not con-
sistent. Even a long abstract might sometimes start
with the method section, and sometimes the objec-
tive section might not be present in the abstracts.
This could lead to increase the error in our grading
system.
8 Conclusion
This paper presents an approach of grading the med-
ical evidence applying a stacking based classifier
using the features from publication types, MeSH
terms, abstract body, and method, and conclusion
sections of the abstracts. The results show that
this approach achieves an accuracy of 73.77%, that
is significantly better than the previously reported
work. Here, we present two findings: 1) We show
that the stacking based approach helps to obtain a
better result in evidence grading than the basic ap-
proach of classification. 2) We also show that the
method and conclusion sections of the abstracts con-
tain important information necessary for evidence
grading. Using the feature sets generated from these
two sections helps to achieve a higher accuracy than
by using the feature set generated from the entire
body of the abstracts.
In this work, all the information available in the
method and conclusion sections of the abstracts is
treated with equal weight. Evidence grading should
not depend upon specific disease names and syn-
dromes, but should be based on how strong the facts
are presented. We would like to extend our ap-
proach by removing the words describing specific
disease names, disease syndromes, and medications,
and giving higher weight to the terms that describe
the assertion of the statements. In our current work,
we apply a simple approach to extract the method
and conclusion sections from the abstracts not hav-
ing sections. Improving the approach by using a ma-
chine learning algorithm that can more accurately
extract the sections might help to increase the accu-
racy of grading. Including the information about the
strength of assertions made in the conclusion sec-
tions could also help in boosting the accuracy. Fu-
ture work would also include testing the effective-
ness of our approach on other diverse data sets hav-
ing complex structures of the evidence, or on a dif-
ferent grading scale.
183
References
Sanmitra Bhattacharya, Viet HaThuc, and Padmini Srini-
vasan. 2011. Mesh: a window into full text for doc-
ument summarization. Bioinformatics, 27(13):i120?
i128.
Aaron M. Cohen, Kyle Ambert, and Marian McDon-
agh. 2010. A Prospective Evaluation of an Au-
tomated Classification System to Support Evidence-
based Medicine and Systematic Review. AMIA Annu
Symp Proc., 2010:121 ? 125.
Frank Davidoff, Brian Haynes, Dave Sackett, and
Richard Smith. 1995. Evidence based medicine.
BMJ, 310(6987):1085?1086, 4.
Martin Dawes, Pierre Pluye, Laura Shea, Roland Grad,
Arlene Greenberg, and Jian-Yun Nie. 2007. The iden-
tification of clinically important elements within med-
ical journal abstracts: Patient-Population-Problem,
Exposure-Intervention, Comparison, Outcome, Dura-
tion and Results (PECODR). Informatics in Primary
Care, 15(1):9?16.
Dina Demner-Fushman, Barbara Few, Susan E. Hauser,
and George Thoma. 2006. Automatically Identifying
Health Outcome Information in MEDLINE Records.
Journal of the American Medical Informatics Associa-
tion, 13(1):52 ? 60.
M. H. Ebell, J. Siwek, B. D. Weiss, S. H. Woolf, J. Sus-
man, B. Ewigman, and M. Bowman. 2004. Strength
of recommendation taxonomy (SORT): a patient-
centered approach to grading evidence in the medi-
cal literature. American Family Physician, 69(3):548?
56+.
David Evans. 2003. Hierarchy of evidence: a frame-
work for ranking evidence evaluating healthcare inter-
ventions. Journal of Clinical Nursing, 12(1):77?84.
GRADE. 2004. Grading quality of evidence and strength
of recommendations. BMJ, 328(7454):1490, 6.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1).
Minlie Huang, Aurlie Nvol, and Zhiyong Lu. 2011. Rec-
ommending MeSH terms for annotating biomedical
articles. Journal of the American Medical Informat-
ics Association, 18(5):660?667.
Halil Kilicoglu, Dina Demner-Fushman, Thomas C Rind-
flesch, Nancy L Wilczynski, and R Brian Haynes.
2009. Towards Automatic Recognition of Scientif-
ically Rigorous Clinical Research Evidence. Jour-
nal of the American Medical Informatics Association,
16(1):25?31.
Su Kim, David Martinez, Lawrence Cavedon, and Lars
Yencken. 2011. Automatic classification of sentences
to support Evidence Based Medicine. BMC Bioinfor-
matics, 12(Suppl 2):S5.
Diego Molla-Aliod and Maria Elena Santiago-Martinez.
2011. Development of a Corpus for Evidence Based
Medicine Summarisation. In Proceedings of the
Australasian Language Technology Association Work-
shop.
Diego Molla-Aliod and Abeed Sarker. 2011. Automatic
Grading of Evidence: the 2011 ALTA Shared Task.
In Proceedings of Australasian Language Technology
Association Workshop, pages 4?8.
Diego Molla-Aliod. 2010. A Corpus for Evidence
Based Medicine Summarisation. In Proceedings of the
Australasian Language Technology Association Work-
shop, volume 8.
MF Porter. 1980. An algorithm for sufx stripping. Pro-
gram, 14(3):130?137.
David L Sackett, William M C Rosenberg, J A Muir Gray,
R Brian Haynes, and W Scott Richardson. 1996. Ev-
idence based medicine: what it is and what it isn?t.
BMJ, 312(7023):71?72, 1.
Abeed Sarker, Diego Molla-Aliod, and Cecile Paris.
2011. Towards automatic grading of evidence. In Pro-
ceedings of LOUHI 2011 Third International Work-
shop on Health Document Text Mining and Informa-
tion Analysis, pages 51?58.
Dolf Trieschnigg, Piotr Pezik, Vivian Lee, Franciska
de Jong, Wessel Kraaij, and Dietrich Rebholz-
Schuhmann. 2009. MeSH Up: effective MeSH text
classification for improved document retrieval. Bioin-
formatics, 25(11):1412?1418.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5(2):241 ? 259.
184
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 224?231,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Native Language Identification: a Simple n-gram Based Approach
Binod Gyawali and Gabriela Ramirez and Thamar Solorio
CoRAL Lab
Department of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, Alabama, USA
{bgyawali,gabyrr,solorio}@cis.uab.edu
Abstract
This paper describes our approaches to Na-
tive Language Identification (NLI) for the NLI
shared task 2013. NLI as a sub area of au-
thor profiling focuses on identifying the first
language of an author given a text in his sec-
ond language. Researchers have reported sev-
eral sets of features that have achieved rel-
atively good performance in this task. The
type of features used in such works are: lex-
ical, syntactic and stylistic features, depen-
dency parsers, psycholinguistic features and
grammatical errors. In our approaches, we se-
lected lexical and syntactic features based on
n-grams of characters, words, Penn TreeBank
(PTB) and Universal Parts Of Speech (POS)
tagsets, and perplexity values of character of
n-grams to build four different models. We
also combine all the four models using an en-
semble based approach to get the final result.
We evaluated our approach over a set of 11 na-
tive languages reaching 75% accuracy.
1 Introduction
Recently, a growing number of applications are tak-
ing advantage of author profiling to improve their
services. For instance, in security applications (Ab-
basi and Chen, 2005; Estival et al, 2007) to help
limit the search space of, for example, the author of
an email threat, or in marketing where the demog-
raphy information about customers is important to
predict behaviors or to develop new products.
Particularly, author profiling is a task of identi-
fying several demographic characteristics of an au-
thor from a written text. Demographic groups can be
identified by age, gender, geographic origin, level of
education and native language. The idea of identi-
fying the native language based on the manner of
speaking and writing a second language is borrowed
from Second Language Acquisition (SLA), where
this is known as language transfer. The theory of
language transfer says that the first language (L1)
influences the way that a second language (L2) is
learned (Ahn, 2011; Tsur and Rappoport, 2007).
According to this theory, if we learn to identify what
is being transfered from one language to another,
then it is possible to identify the native language of
an author given a text written in L2. For instance,
a Korean native speaker can be identified by the er-
rors in the use of articles a and the in his English
writings due to the lack of similar function words in
Korean. As we see, error identification is very com-
mon in automatic approaches, however, a previous
analysis and understanding of linguistic markers are
often required in such approaches.
In this paper we investigate if it is possible to build
native language classifiers that are not based on the
analysis of common grammatical errors or in deeper
semantic analysis. On the contrary, we want to find
a simple set of features related to n-grams of words,
characters, and POS tags that can be used in an ef-
fective way. To the best of our knowledge, almost
all the works related to L1 identification use fine
grained POS tags, but do not look into whether a
coarse grained POS tagset could help in their work.
Here, we explore the use of coarse grained Univer-
sal POS tags with 12 POS categories in the NLI task
and compare the result with the fine grained Penn
TreeBank (PTB) POS tags with 36 POS categories.
224
Moreover, we also investigate how the system works
when perplexity values are used as features in iden-
tifying native languages. Using an ensemble based
approach that combines four different models built
by various combinations of feature sets of n-grams
of words, characters, and POS tags, and perplexity
values, we identify the native language of the author,
over 11 different languages, with an accuracy close
to 80% and 75% in development and test dataset re-
spectively.
2 Related Work
The first known work about native language identifi-
cation appears in 2005 (Koppel et al, 2005). In their
study, the authors experimented with three types of
features, i.e. function words, letter n-grams, er-
rors and idiosyncrasies. But their analysis was fo-
cused on the identification of common errors. They
found that using a combination of all the features in
a Support Vector Machine (SVM), they can obtain
an accuracy of 80% in the classification of 5 differ-
ent native languages. As in this first study, analyz-
ing errors is common in native language identifica-
tion methods, since it is a straightforward adapta-
tion of how this task is performed in SLA. For in-
stance, Wong and Dras (2009) investigate the use
of error types such as disagreement on subject-verb
and noun-number, as well as misuse of determin-
ers to show that error analysis is helpful in this task.
But their results could not outperform the results ob-
tained by Koppel et al (2005). They also suggested
that analyzing other types of errors might help to im-
prove their approach. In the same path, Jarvis et al
(2012) investigate a larger variety of errors, for ex-
ample lexical words and phrase errors, determiner
errors, spelling errors, adjective order errors and er-
rors in the use of punctuation marks, among others.
But they also could not achieve results comparable
to the previous results in this task.
Since language transfer occurs when grammati-
cal structures from a first language determine the
grammatical structures of a second language, the in-
clusion of function words and dependency parsers
as features seem to be helpful to find such trans-
fers as well as error types (Tetreault et al, 2012;
Brooke and Hirst, 2011; Wong et al, 2012). It
is common that the analysis of the structure of
certain grammatical patterns is also informative to
find the use or misuse of well-established gram-
matical structures (e.g. to distinguish between the
use of verb-subject-object, subject-verb-object, and
subject-object-verb), in such cases n-grams of POS
tags can be used. Finally, according to Tsur and
Rappoport (2007), the transfer of phonemes is use-
ful in identifying the native language. Even though
the phonemes are usually speech features, the au-
thors suggest that this transfer can be captured by
the use of character n-grams in the text. Character
n-grams have been proved to be a good feature in
author profiling as well since they also capture hints
of style, lexical information, use of punctuation and
capitalization.
In sum, there are varieties of feature types used
in native language identification, most of them com-
bine three to nine types. Each type aims to capture
specific information such as lexical and syntactic in-
formation, structural information, idiosyncrasies, or
errors.
3 Shared Task Description
The Native Language Identification (NLI) shared
task focuses on identifying the L1 of an author based
on his writing in a second language. In this case,
the second language is English. The shared task had
three sub-tasks: one closed training and two open
training. The details about the tasks are described
by Tetreault et al (2013). For each subtask, the par-
ticipants were allowed to submit up to five runs. We
participated in the closed training sub-task and sub-
mitted five runs.
The data sets provided for the shared task were
generated from the TOEFL corpus (Blanchard et al,
2013) that contains 12, 100 English essays. The
corpus comprised 11 native languages (L1s): Ara-
bic (ARA), Chinese (CHI), French (FRE), German
(GER), Hindi (HIN), Italian (ITA), Japanese (JPN),
Korean (KOR), Spanish (SPA), Telugu (TEL), and
Turkish (TUR), each containing 1100 essays. The
corpus was divided into training, development, and
test datasets with 9900, 1100, and 1100 essays re-
spectively. Each L1 contained an equal number of
essays in each dataset.
225
Feature Sets N-grams
Error rates for top k features
500 800 1000 3000 6000
Character n-grams
2 grams 78.27 77.64 77.18 75.82 -
3 grams 78.55 60.55 64.27 43.73 44.36
Word n-grams
2 grams 66.55 58.36 55.64 44.91 38.73
3 grams 75.55 69.18 76.36 67.09 54.18
PTB POS n-grams
2 grams 69.73 76.73 69.55 72.09 -
3 grams 72.82 72.45 67.27 56.18 62.27
Universal POS n-grams
2 grams 85.36 - - - -
3 grams 78.1818 79.55 72.36 85.27 -
Table 1: Error rates in L1 identification using various feature sets with different number of features
4 General System Description
In this paper we describe two sets of experiments.
We performed a first set of experiments to evaluate
the accuracy of different sets of features in order to
find the best selection. This set was also intended to
determine the threshold of the number of top fea-
tures in each set needed to obtain a good perfor-
mance in the classification task. These experiments
are described in Section 5.
In the second set, we performed five different ex-
periments for five runs. Four of the five models
used different combinations of feature sets to train
the classifier. The major goal of these experiments
was to find out how good the results achieved can
be by using lower level lexical and shallow syntactic
features. We also compared the accuracy obtained
by using the fine grained POS tags and the coarse
grained POS tags. In one of these experiments, we
used perplexity values as features to see how effec-
tive these features can be in NLI tasks. Finally, the
fifth experiment was an ensemble based approach
where we applied a voting scheme to the predictions
of the four approaches to get the final result. The de-
tails of these experiments are described in Section 6.
In our experiments, we trained the classifier using
the training dataset, and using the model we tested
the accuracy on the development and test dataset.
We used an SVM multiclass classifier (Crammer and
Singer, 2002) with default parameter settings for the
machine learning tasks. We used character n-grams,
word n-grams, Parts of Speech (POS) tag n-grams,
and perplexity of character trigrams as features. For
all the features except perplexity, we used a TF-IDF
weighting scheme. To reduce the number of fea-
tures, we selected only the top k features based on
the document frequency in the training data.
The provided dataset contained all the sentences
in the essays tokenized by using ETS?s proprietary
tokenizers. For the POS tags based features, we
used two tagsets: Penn TreeBank (PTB) and Uni-
versal POS tags. For PTB POS tags, we tagged the
text with the Stanford parser (Klein and Manning,
2003). In order to tag the sentences with Universal
POS tags, we mapped the PTB POS tags to universal
POS tags using the mapping described by Petrov et
al. (2011).
We also used perplexity values from language
models in our experiments. To generate the lan-
guage models and compute perplexity, we used the
SRILM toolkit (Stolcke et al, 2011). We used train-
ing data to generate the language models and train
the classifier. Finally, all the sentences were con-
verted into lower case before finding the word and
character n-grams.
5 Feature Sets Evaluation
We performed a series of experiments using a sin-
gle feature set per experiment in order to find the
best combinations of features to use in classification
models. All of the feature sets were based on n-
grams. We ranked the n-grams by their frequencies
on the training set and then used the development set
to find out the best top k features in the training set.
We used the values of k as 500, 800, 1000, 3000,
and 6000 for this set of experiments. The error rates
of these experiments are shown in Table 1. Since the
total number of features in character bigrams, PTB
226
Exp-W2,3PTB3C3 Exp-W2,3Univ3C3 Exp ClassBased Exp Perplexity Exp Ensemble
L1 P R F1 P R F1 P R F1 P R F1 P R F1
ARA 90.7 68.0 77.7 87.1 54.0 66.7 72.2 70.0 71.1 70.8 51.0 59.3 90.9 70.0 79.1
CHI 79.0 83.0 81.0 57.9 84.0 68.6 75.0 78.0 76.5 71.7 66.0 68.8 78.4 87.0 82.5
FRE 91.5 75.0 82.4 75.7 81.0 78.3 92.8 64.0 75.7 71.2 74.0 72.5 90.8 79.0 84.5
GRE 86.0 92.0 88.9 77.5 86.0 81.5 84.2 85.0 84.6 63.8 83.0 72.2 88.3 91.0 89.7
HIN 67.3 66.0 66.7 70.0 63.0 66.3 66.3 63.0 64.6 52.3 45.0 48.4 70.2 66.0 68.0
ITA 72.3 94.0 81.7 76.9 83.0 79.8 66.4 89.0 76.1 65.3 77.0 70.6 74.6 94.0 83.2
JPN 86.6 71.0 78.0 76.0 76.0 76.0 64.3 81.0 71.7 51.7 60.0 55.6 85.2 75.0 79.8
KOR 78.3 83.0 80.6 65.0 80.0 71.7 68.1 64.0 66.0 55.1 49.0 51.9 78.8 82.0 80.4
SPA 72.3 68.0 70.1 90.9 50.0 64.5 65.4 68.0 66.7 58.5 38.0 46.1 74.5 70.0 72.2
TEL 68.4 80.0 73.7 66.9 83.0 74.1 68.2 75.0 71.4 53.4 71.0 60.9 69.2 81.0 74.7
TUR 77.9 81.0 79.4 84.0 63.0 72.0 83.3 55.0 66.3 69.5 66.0 67.7 81.8 81.0 81.4
Overall 78.3 73.0 72.0 61.8 79.6
Table 2: L1 identification accuracy in development data
POS bigrams, Universal POS bigrams, and Univer-
sal POS trigrams were 1275, 1386, 144, and 1602
respectively, some fields in the table are blank.
A trivial baseline for this task is to classify all the
instances to a single class, which gives 9.09% ac-
curacy. The table above shows that the results ob-
tained in all cases is better than the baseline. In five
cases, better results were obtained when using the
top 3000 or 6000 features compared to other feature
counts. In the case of the character trigram feature
set, though the result using top 3000 features is bet-
ter than the others, the difference is very small com-
pared to the experiment using top 6000 features. The
accuracy obtained by using top 3000 features in PTB
POS tags is 6% higher than that with top 6000 fea-
tures. In case of Universal POS tags trigrams, better
results were obtained with top 1000 features.
Results show that bigram and trigram feature sets
of words give higher accuracy compared to bigrams
and trigrams of characters and POS tags. Comparing
the results of n-grams of two different POS tagsets,
the results obtained when using the PTB tagset are
better than those when using the Universal tagsets.
In the case of character, PTB POS tag, and Univer-
sal POS tag bigram feature sets, the overall accu-
racy is less than 30%. Based on these results, we de-
cided to use the following sets of features: trigrams
of characters and POS tags (PTB and Universal) and
bigrams of words in our experiments below.
6 Final Evaluation
We submitted five runs for the task based on five
classifiers. We named the experiments based on the
features used and the approaches used for feature se-
lection. Details about the experiments and their re-
sults are described below.
1. Exp-W2,3PTB3C3: In this experiment, we
used bigrams at the word level, and trigrams at
the word, character level, as well as PTB POS
tag trigrams as feature sets. We selected these
feature sets based on the accuracies obtained
in the experiments described in Section 5. We
tried to use a consistent number of features in
each feature set. As seen in Table 1, though
the results obtained by using top 3000 and 6000
features are better in equal number of cases (2
and 2), the difference in accuracies when us-
ing 6000 features is higher than that when us-
ing 3000 features. Thus, we decided to use the
top 6000 features in all the four feature sets.
2. Exp-W2,3Univ3C3: The PTB POS tagset con-
tains 36 fine grained POS categories while the
Universal POS tagset contains only 12 coarse
POS categories. In the second experiment, we
tried to see how the performance changes when
using coarse grained Universal POS categories
instead of fine grained PTB POS tags. Thus,
we performed the second experiment with the
same settings as the first experiment except we
used Universal POS tags instead of PTB POS
tags. Since the total number of Universal POS
227
Exp-W2,3PTB3C3 Exp-W2,3Univ3C3 Exp ClassBased Exp Perplexity Exp Ensemble
L1 P R F1 P R F1 P R F1 P R F1 P R F1
ARA 74.3 55.0 63.2 90.9 50.0 64.5 67.9 74.0 70.8 54.3 44.0 48.6 79.7 63.0 70.4
CHI 76.2 80.0 78.0 65.9 81.0 72.6 74.5 73.0 73.7 69.3 61.0 64.9 80.2 81.0 80.6
FRE 86.4 70.0 77.3 75.8 75.0 75.4 90.6 58.0 70.7 54.5 54.0 54.3 85.7 72.0 78.3
GRE 83.2 89.0 86.0 79.1 91.0 84.7 82.7 86.0 84.3 65.2 86.0 74.1 87.6 92.0 89.8
HIN 63.7 65.0 64.4 64.5 69.0 66.7 59.6 56.0 57.7 60.0 54.0 56.8 67.0 67.0 67.0
ITA 62.5 90.0 73.8 70.0 84.0 76.4 61.4 86.0 71.7 52.5 64.0 57.7 62.5 90.0 73.8
JPN 85.7 72.0 78.3 67.2 78.0 72.2 62.1 87.0 72.5 52.6 50.0 51.3 81.9 77.0 79.4
KOR 75.0 75.0 75.0 60.3 73.0 66.1 68.1 62.0 64.9 52.6 50.0 51.3 72.8 75.0 73.9
SPA 60.0 57.0 58.5 81.1 43.0 56.2 57.6 57.0 57.3 55.6 45.0 49.7 67.1 57.0 61.6
TEL 75.3 67.0 70.9 70.0 77.0 73.3 71.7 71.0 71.4 66.1 74.0 69.8 73.0 73.0 73.0
TUR 66.4 79.0 72.1 79.0 64.0 70.7 80.6 50.0 61.7 61.4 51.0 55.7 72.4 76.0 74.1
Accuracy 72.6 71.4 69.1 58.6 74.8
Table 3: L1 identification accuracy in test data
trigrams was only 1602, we replaced 6000 PTB
POS trigrams with 1602 Universal POS tri-
grams.
3. Exp ClassBased: The difference in this exper-
iment from the first one lies in the process of
feature selection. Instead of selecting the top k
features from the whole training data, the se-
lection was done considering the top m fea-
tures for each L1 class present in the training
dataset, i.e., we first selected the top m features
from each L1 class and combined them for a
total of p where p is greater than or equal to
m and k. After a number of experiments per-
formed with different combinations of features
to train the classifier and testing on the develop-
ment dataset, we obtained the best result using
character trigrams, PTB POS tag bigrams and
trigrams, and word bigrams feature sets with
3000, 1000, 1000, and 6000 features from each
L1 respectively. This makes the total number
of features in character trigrams, POS tag bi-
grams, POS tag trigrams, and word bigrams as
3781, 1278, 1475, and 15592 respectively.
4. Exp Perplexity: In this experiment, we used
the perplexity values as the features that were
computed from character trigram language
models. Language models define the proba-
bility distribution of a sequence of tokens in
a given text. We used perplexity values since
these have been successfully used in some au-
thorship attribution tasks (Sapkota et al, 2013).
5. Exp Ensemble: In the fifth experiment, we
used an ensemble based approach with our
above mentioned four different models. We
allowed each of the four models to have two
votes. The first vote is a weighted voting
schema in which the models were ranked ac-
cording to their results in the development
dataset and the weight for each model was
given by wc = 1/rank(c), where rank(c) is
the position of c in the ranked list. The final
output was based on the second vote that used
a majority voting schema. In the second vote,
the output of the first voting schema was also
used along with the output of four models.
The results obtained by the above mentioned five
experiments on the development and test datasets are
shown in Tables 2 and 3 respectively. The tables
show that the results obtained in the development
dataset are better than those in the test dataset for
all the approaches. In both datasets, we achieved the
best results using the ensemble based approach, i.e.
79.2% and 74.8% accuracies in the development and
test dataset respectively. Considering the accuracies
of individual L1s, this approach achieved the high-
est accuracy in 10 L1s in the development dataset
and in 7 L1s in the test dataset. Our system has the
best accuracy for German in both development and
test dataset. The other classes with higher accura-
cies in both datasets are French and Chinese. In both
datasets, our system had the lowest accuracy for the
Hindi and Spanish classes. Arabic and Telugu have
228
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
ARA 63 2 1 0 6 8 1 5 6 4 4
CHI 2 81 0 1 2 1 5 4 0 0 4
FRE 2 0 72 7 1 11 0 0 4 0 3
GER 0 2 2 92 1 1 0 0 1 0 1
HIN 2 2 0 0 67 2 0 2 3 19 3
ITA 0 0 2 2 0 90 0 0 3 0 3
JPN 3 3 1 1 0 3 77 9 1 1 1
KOR 1 7 1 0 0 0 8 75 4 1 3
SPA 1 1 3 0 2 25 1 4 57 0 6
TEL 1 0 0 0 21 0 1 0 3 73 1
TUR 4 3 2 2 0 3 1 4 3 2 76
Table 4: Confusion Matrix
3rd and 4th lowest accuracies.
Besides the ensemble based approach, the sec-
ond best result was obtained by the first experiment
(Exp W2,3PTB3C3). Comparing the overall accura-
cies of the first and second (Exp-W2,3Univ3C3) ex-
periments, though the difference between them does
not seem very high in the test dataset, there is a dif-
ference of more than 5% in the development dataset.
In the test dataset, the second experiment has the
best results among all the approaches for classes
Italian and Telugu, and has better results than the
first experiment for classes Arabic and Hindi. The
difference in the approaches used in the first and sec-
ond experiments was the use of n-grams of different
POS tagsets. The use of coarse grained Universal
POS tagset features generalizes the information and
loses the discriminating features that the fine grained
PTB POS tagset features captures. For instance, the
PTB POS tagset differentiates verbs into six cate-
gories while the Universal POS tagset has only one
category for that grammatical class. Because of this,
the fine grained POS tagset seems better for identify-
ing the native languages than using a coarse grained
POS tagset in most of the cases. More studies are
needed to analyze the cases where Universal POS
tagset works better than the fine grained PTB POS
tagset.
The difference in accuracies obtained between the
first experiment (Exp W2,3PTB3C3) and the third
experiment (Exp ClassBased) is more than 6% in
the development dataset and more than 3% in the test
dataset. In the test dataset, the third experiment has
the highest accuracy for Arabic class and has better
accuracy than the first experiment for Telugu class.
The difference between these approaches was the
feature selection approach used to create the feature
vector. The results show that in most of the cases se-
lecting the features from the whole dataset achieves
better accuracy in identifying native languages com-
pared to using the stratified approach of selecting the
features from individual classes. The main reason
behind using the class based feature selection was
that we tried to capture some features that are specif-
ically present in one class and not in others. Since all
the texts in our dataset were about one of the eight
prompts, and we have a balanced dataset, there was
no benefit of doing the class based feature selection
approach.
The fourth experiment (Exp Perplexity) using
perplexity values as features did not achieve accu-
racy comparable to the first three experiments. Be-
cause of the time constraint, we calculated perplex-
ity based on only character trigram language mod-
els. Though the result we achieved is not promis-
ing, this approach could be an interesting work in fu-
ture experiments where we could use other language
models or the combination of various language mod-
els to compute the perplexity.
7 Error Analysis
The confusion matrix of the results obtained in the
test dataset by using the ensemble based approach
is shown in Table 4. The table shows the German
class has the best accuracy with only a small number
of texts of German mispredicted to other languages,
while 7 texts of French class are mispredicted as
German. The German language is rich in morpohol-
ogy and shares a common ancestor with English. It
also has a different grammatical structure from the
229
other languages in the task. The features we used
in our experiments are shallow syntactic and lexical
features, which could discriminate the writing styles
and the structure of the German class texts, thus hav-
ing a higher prediction accuracy.
The table shows that French, Italian, and Spanish
classes seem to be confused with each other. Though
the misclassification rate of texts in the Italian class
is considerably low, a good number of texts in the
French and Spanish classes are misclassified as Ital-
ian. The highest number of documents mispredicted
is from Spanish to Italian, i.e. 25 texts of Span-
ish class are mispredicted as Italian. These three
languages fall under the same language family i.e.
Indo-European/Romance and have a similar gram-
matical features. The grammatical structure is a par-
ticular example of the high rate of misclassification
among these classes. While English language is very
strict in the order of words (Subject-Verb-Object),
Spanish, Italian and French allow more flexibility.
For instance, in Spanish, the phrases ?the car red?
(el auto rojo) and ?the red car? (el rojo auto) are
both correct although the later is a much less com-
mon construction. In this scenario, it is easy to see
that the n-grams of words and POS tags are benefi-
cial to distinguish them from English, but these n-
grams might be confusing to identify the differences
among these three languages since the patterns of
language transfer might be similar.
Though Hindi and Telugu languages do not fall
under the same language family, they are highly con-
fused with each other. After Spanish to Italian, the
second highest number of misclassified texts is from
Telugu to Hindi. Similarly, 19 texts from the class
Hindi are mispredicted as Telugu. Both of these lan-
guages are spoken in India. Hindi is the National
and official language of India, while Telugu is an of-
ficial language in some states of India. Moreover,
English is also one of the official languages. So, it
is very likely that the speakers are exposed to the
same English dialect and therefore their language
transfer patterns might be very similar. This might
have caused our approach of lexical and syntactic
features to be unable to capture enough information
to identify the differences between the texts of these
classes.
Texts from Arabic class are equally misclassified
to almost all the other classes, while misclassifica-
tion to Arabic do not seem that high. Texts of the
Japanese, Korean, Chinese classes seem to be con-
fused with each other, but the confusion does not
seem very high thus having a good accuracy rate.
8 Conclusion and Future Work
In this paper, we describe our approaches to Na-
tive Language identification for the NLI Shared Task
2013. We present four different models for L1 iden-
tification, three of them using various combinations
of n-gram features at the word, character and POS
tag levels and a fourth one using perplexity values as
features. Results show that all these approaches give
a good accuracy in L1 identification. We achieved
the best result among these by using the combina-
tion of character, words, and PTB POS tags. Fi-
nally, we applied an ensemble based approach over
the results of the four different models that gave the
highest overall accuracy of 79.6% and 74.8% in the
development and test dataset respectively.
In our approaches, we use simple n-grams and do
not consider grammatical errors in L1 identification.
We would like to expand our approach by using the
errors such as misspelled words and subject-verb,
and noun-number disagreements as features. More-
over, in our current work of using perplexity values,
the result seems good but is not promising. In this
approach, we used the perplexity values based on
only character trigram language models. We would
like to incorporate other word and character n-gram
language models to calculate perplexity values in
our future work.
Acknowledgements
We would like to thank the organizers of NLI shared
task 2013. We would also like to thank CONACyT
for its partial support of this work under scholarship
310473.
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying
authorship analysis to Arabic web content. In Pro-
ceedings of the 2005 IEEE international conference
on Intelligence and Security Informatics, ISI?05, pages
183?197, Berlin, Heidelberg. Springer-Verlag.
Charles S. Ahn. 2011. Automatically Detecting Authors?
230
Native Language. Master?s thesis, Naval Postgraduate
School, Monterey, CA.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Koby Crammer and Yoram Singer. 2002. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265?292.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 263?272, Melbourne, Australia.
Scott Jarvis, Yves Bestgen, Scott A. Crossley, Syl-
viane Granger, Magali Paquot, Jennifer Thewissen,
and Danielle McNamara. 2012. The Comparative
and Combined Contributions of n-Grams, Coh-Metrix
Indices and Error Types in the L1 Classification of
Learner Texts. In Scott Jarvis and Scott A. Crosley,
editors, Approaching Language Transfer through Text
Classification, pages 154?177. Multilingual Matters.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
Upendra Sapkota, Thamar Solorio, Manuel Montes-y
Go?mez, and Paolo Rosso. 2013. The use of orthogo-
nal similarity relations in the prediction of authorship.
In Computational Linguistics and Intelligent Text Pro-
cessing, pages 463?475. Springer.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, pages 9?16, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
231
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 66?73 Dublin, Ireland, August 23-29 2014.
Self-Training for Parsing Learner Text
Aoife Cahill, Binod Gyawali and James V. Bruno
Educational Testing Service,
660 Rosedale Road,
Princeton, NJ 08541,
USA
{acahill, bgyawali, jbruno}@ets.org
Abstract
We apply the well-known parsing technique of self-training to a new type of text: language-
learner text. This type of text often contains grammatical and other errors which can cause
problems for traditional treebank-based parsers. Evaluation on a small test set of student data
shows improvement over the baseline, both by training on native or non-native text. The main
contribution of this paper adds additional support for the claim that the new self-trained parser
has improved over the baseline by carrying out a qualitative linguistic analysis of the kinds of
differences between two parsers on non-native text. We show that for a number of linguistically
interesting cases, the self-trained parser is able to provide better analyses, despite the sometimes
ungrammatical nature of the text.
1 Introduction
The vast majority of treebank-based parsing research assumes that the text to be parsed is well-formed.
In this paper, we are concerned with parsing text written by non-native speakers of English into phrase
structure trees, as a precursor for applications in automated scoring and error detection. Non-native text
often contains grammatical errors ranging in severity from minor collocational differences to extremely
garbled strings that are difficult to interpret. These kinds of errors are known to cause difficulty for
automated analyses (De Felice and Pulman, 2007; Lee and Knutsson, 2008).
We explore a previously documented technique for adapting a state-of-the-art parser to be able to bet-
ter parse learner text: domain adaptation using self-training. Self-training is a semi-supervised learning
technique that relies on some labeled data to train an initial model, and then uses large amounts of unla-
beled data to iteratively improve that model. Self-training was first successfully applied in the newspaper
parsing domain by McClosky et al. (2006) who used the Penn Treebank WSJ as their labeled data and un-
labeled data from the North American News Text corpus. Previous attempts (Charniak, 1997; Steedman
et al., 2003) had not shown encouraging results, and McClosky et al. (2006) hypothesize that the gain
they saw was due to the two-phase nature of the BLLIP parser used in their experiments. In a follow-up
study (McClosky et al., 2008) they find that one major factor leading to successful self-training is when
the process sees known words in new combinations.
2 Related Work
Foster et al. (2011) compare edited newspaper text and unedited forum posts in a self-training parsing
experiment, evaluating on a treebank of informal discussion forum entries about football. They find that
both data sources perform about equally well on their small test set overall, but that the underlying gram-
mars learned from the two sources were different. Ott and Ziai (2010) apply an out-of-the-box German
dependency parser to learner text and analyze the impact on down-stream semantic interpretation. They
find that core functions such as subject and object can generally be reliably detected, but that when there
are key elements (e.g. main verbs) missing from the sentence that the parses are less reliable. They
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
66
also found that less-severe grammatical errors such as agreement did not tend to cause problems for the
parser.
An alternative approach to parsing learner text is to modify the underlying dependency scheme used in
parsing to account for any grammatical errors. This can be useful because it is not always clear what the
syntactic analysis of ungrammatical text should be, given some scheme designed for native text. Dick-
inson and Ragheb (2009) present such a modified scheme for English, designed for annotating syntactic
dependencies over a modified POS tagset. Dickinson and Lee (2009) retrain a Korean dependency parser,
but rather than adding additional unlabeled data as we do, they modify the original annotated training
data. The modifications are specifically targeted to be able to detect errors relating to Korean postpo-
sitional particles. They show that the modified parser can be useful in detecting those kinds of particle
errors and in their conclusion suggest self-training as an alternative approach to parsing of learner text.
A similar alternative approach is to directly integrate error detection into the parsing process (Menzel
and Schro?der, 1999; Vandeventer Faltin, 2003).
3 Self-training a new parser
We first describe the data that we use for both training and evaluating our parsers, and then we describe
our experiments and results.
We take the standard portion of the Penn Treebank (sections 02?21) as our seed labeled data. We
then compare two different unlabeled training data sets. The first data set consists of 480,000 sentences
of newspaper text extracted from the LA Times portion of the North American News Corpus (NANC).
The second is a corpus of non-native written English text randomly sampled from a large dataset of
student essays. It consists of 480,900 sentences from 33,637 essays written as part of a test of English
proficiency, usually administered to non-native college-level students. The essays have been written to
422 different prompts (topics) and so cover a wide range of vocabulary and usage. Each essay has been
assigned a proficiency level (high, medium, low) by a trained human grader. 17.5% of the sentences were
from low proficiency essays, 42% from medium proficiency and 40.5% from high proficiency essays.
In order to determine the optimal number of self-training iterations and carry out our final evaluations
we use a small corpus of manually treebanked sentences. The corpus consists of 1,731 sentences written
by secondary level students which we randomly split into a development set (865 sentences) and a test set
(866 sentences). The native language of the students is unknown, but it is likely that many spoke English
as their first language. In addition, this corpus had originally been developed for another purpose and
therefore contains modifications that are not ideal for our experiments. The main changes are that spelling
and punctuation errors were corrected before the trees were annotated (and we do not have access to the
original text). Although the treebanked corpus does not align perfectly with our requirements, we believe
that it is a more useful evaluation data set than any other existing treebanked corpus.
We used the Charniak and Johnson (2005) (BLLIP) parser1 to perform the self training experiments.
Our experiment is setup as follows: first we train a baseline model on the Penn Treebank WSJ data
(sections 02-21). Then, iteratively, sentences are selected from the unlabeled data sets, parsed by the
parser, and combined with the previously annotated data to retrain the parser. The parser also requires
development data, for which we use section 22 of the WSJ data. After each iteration we evaluate the
parser using our 865-sentence development set. Parser evaluation was done using the EVALB2 tool and
we report the performance in terms of F1 score.
There are two main parameters in our self-training setup: the size of the unlabeled data set added at
each iteration and the weight given to the original labeled data.3 In preliminary experiments, we found
that a block size of 40,000 sentences per each iteration and a weight of 5 on the original labeled data
performed best. Given our training data, and a block size of 40K, this results in 12 iterations. In each
iteration, the training data consists of the PTB data repeated 5 times, plus the parsed output of previous
blocks of unlabeled data.
1https://github.com/BLLIP/bllip-parser
2http://nlp.cs.nyu.edu/evalb/
3Note that this approach differs to that outlined in McClosky et al. (2006) who only perform one self-training iteration. It is
more similar to the approach described in Reichart and Rappoport (2007).
67
The results of our experiments are as shown in Figure 1. Iteration 0 corresponds to the baseline parser
while iterations 1?12 are the self trained parsers. We see that the F1 score of the baseline parser is
80.9%.4 The self trained parsers have higher accuracies compared to the baseline parser starting at the
first iteration. The highest score training on non-native text (82.3%) was achieved on the 11th iteration,
and the highest score training on newspaper text (81.8%) was achieved on the 8th iteration. Both of these
results are statistically significantly better than the baseline parser only trained on WSJ text.5 The graph
also shows that the non-native training results in slightly higher overall f-scores than the parser trained
on the native data after iteration 5, however these differences are not statistically significant.
81.
5
81.
7582
82.
2582.
5
F 1 S c o r
F1?
sco
re?o
f?ea
ch?
iter
atio
n
No
n?N
ativ
e?E
ngl
ish
NA
NC
80.
7581
81.
25
0
1
2
3
4
5
6
7
8
9
10
11
12
r e ( % )
Ite
rat
ion
?nu
mb
er
Figure 1: Performance of parsers after each iteration. Parsers used WSJ Section 22 as development data
and were evaluated on the student response development data.
The final evaluation was carried out by evaluating on the student test corpus of 866 sentences, using
the parsing model that performed best on the student dev corpus. The parser trained on native text
achieved an f-score of 82.4% and the parser trained on the non-native text achieved an f-score of 82.6%.
This difference is not statistically significant and is a similar finding to Foster et al. (2011). In another
experiment, we found that if the development data used during self-training is similar to the test data, we
see even smaller differences between the two different kinds of training data.6
4 Analysis
We carry out a qualitative analysis of the differences in parses between the original parser and one of the
best-performing self-trained ones, trained on non-native text. We randomly sample 5 essays written by
non-native speakers (but not overlapping with the data used to self-train the parser). Table 1 shows the
number of sentences and the number of parse trees that differ, according to each proficiency level.
Proficiency # Essays # Sentences # Words # Differing Parses % Differing Parses
High 2 30 694 12 40
Mid 1 22 389 12 54
Low 2 17 374 8 47
Totals 5 69 1457 32 46
Table 1: Descriptive Statistics for Essays in the Qualitative Sample
4Note that these overall f-scores are considerably lower than current state-of-the-art for newspaper text, indicating that this
set of student texts are considerably different.
5Significance testing was carried out using Dan Bikel?s Randomized Parsing Evaluation Comparator script for comparing
evalb output files. We performed 1000 random shuffles and tested for p-values < 0.01.
6These data sets were all quite small, however, so further investigation is required to fully assess this finding.
68
Figure 2 reports the number of differences by proficiency level. It is important to note that these
differences only included ones that were considered to be independent (e.g. a change in POS tag that
necessitated a change in constituent label was only counted once). We note a trend in which the self-
trained parser produces better parses than the baseline more often; however, at the highest proficiency
level the baseline parser produces better parses more often. In some applications it might be possible to
take the proficiency level into account before running the parser. However for many applications this will
present a challenge since the parser output plays a role in predicting the proficiency level. A possible
alternative would be to approximate proficiency using frequencies of spelling and other grammatical
errors that can be automatically detected without relying on parser output and use this information to
decide which version of the parser to use.
10 14 7
5 4
3
4 3 9
10%20%
30%40%
50%60%
70%80%
90%100%
Indep
enden
t Diffe
rence
s
0% Low(Total = 19) Mid(Total = 21) High(Total = 19)
I
Proficiency Level
Better Self-Trained Better Unclear Better Original
Figure 2: Unrelated Differences by Proficiency Level.
We systematically examine each of the 32 pairs of differing parse trees in the sample and manually
categorize the differences. Figure 3 shows the 5 most frequent types of differences, their breakdown
by proficiency level, as well as the results of a subjective evaluation on which parse was better. These
judgements were made by one of the authors of this paper who is a trained linguist.
3 3 5 4 2 4 5 4
2
2
2
1 1 1
1
2
5
1
2
3
1 1 2 10
2
4
6
8
10
12
14
Low Mid High Low Mid High Low Mid High Low Mid High Low Mid High
Attachment Site(Total = 22) POS Tag(Total = 15) Sentential Components(Total = 10)
POS of Misspelled Terminal(Total = 6)
Headedness(Total = 5)
Better Self-Trained Better Unclear Better Original
Figure 3: Parse Tree Differences by Proficiency Level.
The differences in Figure 3 are defined as follows. Attachment Site: the same constituent is attached
to different nodes in each parse; POS Tag: the same terminal bears a different POS tag in each parse,
where the terminal exists in our dictionary of known English7 words; Sentential Components: One parse
groups a set of constituents exhaustively into an S-node, while the other does not; POS of misspelled
terminal: the same terminal bears a different POS tag in each parse, where the terminal has been flagged
as a misspelling; Headedness: a terminal heads a maximal projection of a different syntactic category in
one parse but not the other, (e.g. a VP headed by a nominal).
7We use the python package enchant with US spelling dictionaries to carry out spelling error detection.
69
We characterized the differences according to whether the better output was produced by the original
parser, the self trained parser, or if it was not clear that either parse was better than the other. Attachment
Site differences were evaluated according to whether or not they were attached to the constituent they
modified; POS Tag differences were evaluated according to the Penn Treebank Guidelines (Santorini,
1995); Sentential Components differences were evaluated according to whether or not the terminals
should indeed form a clausal constituent, infinitive, or gerund; POS of Misspelled Terminal differences
were evaluated according to the evaluator?s perception of the writer?s intended target. We note that
the most abundant differences are in Attachment Site, that the biggest improvements resulting from self-
training are in the recognition of Sentential Components and in the identification of the POS of Misspelled
Terminals, and that the biggest degradation is in Headedness.
4.1 General Difference Patterns
Using the categories defined during the manual analysis of the 5 essays, we develop rules to automatically
detect these kinds of differences in a large dataset. We expect that the automatic rules will identify more
differences than the linguist, however we hope to see the same general patterns. We apply our rules to an
additional set of data consisting of roughly 10,000 sentences written by non-native speakers of English.
Table 2 shows the number of sentences for which the parsers found different parses at each proficiency
level, and Table 3 gives the totals for each of the five difference categories described above.
Proficiency # Essays # Sentences # Words # Differing Parses % Differing Parses
High 256 4178 266543 2214 53
Mid 285 4168 263685 2364 57
Low 149 1657 93466 971 59
Totals 690 10003 623694 5549 55
Table 2: Descriptive Statistics for Essays in the Larger Sample
Difference Total Low Medium High
Attachment Site 7805 1331 3474 3000
POS Tag 6827 1205 3238 2384
Sentential Components 4103 778 1786 1539
POS of Misspelled Terminal 2040 346 894 800
Headedness 1357 353 568 436
Table 3: Total number of differences detected automatically by proficiency level
We see that the proportion of sentences with different parses is similar to the 5-essay sample and also
that the relative ordering of the five difference categories is identical. This at least indicates that the
5-essay sample does not differ largely in its general properties from a larger set.
4.2 Illustrative Observations
We highlight some of the most interesting differences between the baseline parser and the self-trained
parser, using examples from our 5-essay sample described above.
Ambiguity of subordinating conjunctions: Figure 4 shows an example from a lower proficiency
essay that contains multiple interacting differences, primarily stemming from the fact that the POS tag
for a subordinating conjunction is the same as the POS tag for a regular preposition according to the
Penn Treebank guidelines (Santorini, 1995). The original parser (4a) treats it as a preposition: it is
dominated by PP and takes NP as a complement. The self-trained parser (4b) correctly treats because
as a subordinating conjunction: it is dominated by SBAR and takes S as a complement. In addition,
the original parser identified suffer as the main verb in the sentence. The self-trained parser correctly
analyzes this as part of the dependent clause, however this results in no main verb being identified and
an overall FRAGMENT analysis. Since it is unclear what the original intention of the writer was, this
fragment analysis could be more useful for identifying grammatical errors and giving feedback.
70
S. . . PP
IN
because
NP
DT
the
NN
world
VP
VBP
suffer
. . .
(a) Original Parser
FRAG
. . . SBAR
IN
because
S
NP
DT
the
NN
world
VP
VBP
suffer
. . .
(b) Self-Trained Parser
Figure 4: Parses for Especaily, in this time, because the world suffer, the economy empress.
Ambiguity of to: Figure 5 exemplifies a difference related to the analysis of infinitives. Here we can
see that the original parser analyzed the to phrase as a PP (c.f. afraid of) whereas the self-trained parser
analyzes it as an infinitival. We believe that the infinitival interpretation is slightly more likely (with a
missing verb do), though of course it is difficult to say for sure what the intended meaning is. Here there
are two interacting difference types: Sentential Components and Headedness. In the self-trained parse,
anything is an NN that heads a VP, whereas it is an NN that appropriately heads an NP in the original
parse. However, it is important to note that the self-trained parse treats to anything as an infinitive: a TO
dominated by a VP, which is dominated by a unary-branching S. The original parse treats to anything as
a regular PP. The fact that the self-trained parse contains a set of terminals exhaustively dominated by
an S-node that does not exist in the original parse constitutes a Sentential Components difference. We
believe that it is more useful to correctly identify infinitives and gerunds as sentential constituents, even
at the cost of an XP that is apparently headed by an inappropriate terminal (VP headed by NN).
S
. . . AdjP
JJ
afraid
PP
TO
to
NP
NN
anything
PP
during your life
(a) Original Parser
S
. . . AdjP
JJ
afraid
S
VP
TO
to
VP
NN
anything
PP
during your life
(b) Self-Trained Parser
Figure 5: Parses for If you have this experience, you will do not afraid to anything during your life.
Attachment ambiguity: We turn now to Figure 6. The main difference has to do with the attachment
of the phrase that you think it worth: the SBAR is attached to the VP in the original parse (as a clausal
complement) and to the NP in the self-trained parse (as a relative clause). This example also shows that
a change in POS-tag can have a significant impact on the final parse tree.
5 Future Work and Conclusions
We have shown that it is possible to apply self-training techniques in order to adapt a state-of-the-art
parser to be able to better parse English language learner text. We experimented with training the parser
on native text as well as non-native text. In an evaluation on student data (not necessarily language-
71
VP
VBN
used
PP
IN
in
NP
DT
the
NN
thing
SBAR
IN
that
S
you think it worth
(a) Original Parser
VP
VBN
used
PP
IN
in
NP
NP
DT
the
NN
thing
SBAR
WHNP
WDT
that
S
you think it worth
(b) Self-Trained Parser
Figure 6: Parses for So I support that the money should be used in the thing that you think it worth.
learner data) we found that both training sets performed at about the same level, but that both significantly
out-performed the baseline parser trained only on WSJ text.
We carry out an in-depth study on a small data set of 5 learner essays and define a set of difference
categories in order to describe the parse-tree differences from a linguistic perspective. We implement
rules to automatically detect these parse-tree differences and show that the general proportions of errors
found in the small data set are similar to that of a larger data set. We highlight some of the most interesting
improvements of the parser, and we show that despite various grammatical errors present in sentences,
the self-trained parser is, in general, able to assign better analyses than the baseline parser.
Of course, the self-trained parser does sometimes choose a parse that is less appropriate than the
baseline one. In particular, we noticed that this happened most frequently for the highest proficiency
essays. Further investigation is required to be able to better understand the reasons for this. In future
work, the most informative evaluation of the self-trained parser would be in a task-based setting. We
plan to investigate whether the self-trained parser improves the overall performance of tasks such as
automated essay scoring or automated error detection, which internally rely on parser output.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05), pages
173?180, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the
Fourteenth National Conference on Artificial Intelligence, pages 598?603, Menlo Park, CA. AAAI Press/MIT
Press.
Rachele De Felice and Stephen Pulman. 2007. Automatically Acquiring Models of Preposition Use. In Pro-
ceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pages 45?50, Prague, Czech Republic, June.
Association for Computational Linguistics.
Markus Dickinson and Chong Min Lee. 2009. Modifying corpus annotation to support the analysis of learner
language. CALICO Journal, 26(3):545?561.
Markus Dickinson and Marwa Ragheb. 2009. Dependency Annotation for Learner Corpora. In Proceedings of
the Eighth Workshop on Treebanks and Linguistic Theories (TLT-8), pages 59?70, Milan, Italy.
Jennifer Foster, O?zlem C?etinog?lu, Joachim Wagner, and Josef van Genabith. 2011. Comparing the Use of Edited
and Unedited Text in Parser Self-Training. In Proceedings of the 12th International Conference on Parsing
Technologies, pages 215?219, Dublin, Ireland. Association for Computational Linguistics.
72
John Lee and Ola Knutsson. 2008. The Role of PP Attachment in Preposition Generation. In Proceedings
of CICLing 2008, 9th International Conference on Intelligent Text Processing and Computational Linguistics,
pages 643?654, Haifa, Israel.
David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. In Proceedings
of the Human Language Technology Conference of the NAACL, Main Conference, pages 152?159, New York
City, USA, June. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson. 2008. When is Self-Training Effective for Parsing? In
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561?568,
Manchester, UK, August. Coling 2008 Organizing Committee.
Wolfgang Menzel and Ingo Schro?der. 1999. Error diagnosis for language learning systems. ReCALL, 11:20?30.
Niels Ott and Ramon Ziai. 2010. Evaluating dependency parsing performance on German learner language. In
Proceedings of the Ninth Workshop on Treebanks and Linguistic Theories (TLT-9), pages 175?186.
Roi Reichart and Ari Rappoport. 2007. Self-Training for Enhancement and Domain Adaptation of Statistical
Parsers Trained on Small Datasets. In Proceedings of the 45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 616?623, Prague, Czech Republic, June. Association for Computational Linguistics.
Beatrice Santorini. 1995. Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision). Techni-
cal Report, Department of Computer and Information Science, University of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings
of EACL 03, pages 331?228.
Anne Vandeventer Faltin. 2003. Syntactic Error Diagnosis in the context of Computer Assisted Language Learn-
ing. Ph.D. thesis, Universite? de Gene`ve.
73
