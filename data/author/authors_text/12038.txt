Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612?620,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Analysing Wikipedia and Gold-Standard Corpora for NER Training
Joel Nothman and Tara Murphy and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{jnot4610,tm,james}@it.usyd.edu.au
Abstract
Named entity recognition (NER) for En-
glish typically involves one of three gold
standards: MUC, CoNLL, or BBN, all created
by costly manual annotation. Recent work
has used Wikipedia to automatically cre-
ate a massive corpus of named entity an-
notated text.
We present the first comprehensive cross-
corpus evaluation of NER. We identify
the causes of poor cross-corpus perfor-
mance and demonstrate ways of making
them more compatible. Using our process,
we develop a Wikipedia corpus which out-
performs gold standard corpora on cross-
corpus evaluation by up to 11%.
1 Introduction
Named Entity Recognition (NER), the task of iden-
tifying and classifying the names of people, organ-
isations and other entities within text, is central
to many NLP systems. NER developed from in-
formation extraction in the Message Understand-
ing Conferences (MUC) of the 1990s. By MUC 6
and 7, NER had become a distinct task: tagging
proper names, and temporal and numerical expres-
sions (Chinchor, 1998).
Statistical machine learning systems have
proven successful for NER. These learn patterns
associated with individual entity classes, mak-
ing use of many contextual, orthographic, linguis-
tic and external knowledge features. However,
they rely heavily on large annotated training cor-
pora. This need for costly expert annotation hin-
ders the creation of more task-adaptable, high-
performance named entity recognisers.
In acquiring new sources for annotated corpora,
we require an analysis of training data as a variable
in NER. This paper compares the three main gold-
standard corpora. We found that tagging mod-
els built on each corpus perform relatively poorly
when tested on the others. We therefore present
three methods for analysing internal and inter-
corpus inconsistencies. Our analysis demonstrates
that seemingly minor variations in the text itself,
starting right from tokenisation can have a huge
impact on practical NER performance.
We take this experience and apply it to a corpus
created automatically using Wikipedia. This cor-
pus was created following the method of Nothman
et al (2008). By training the C&C tagger (Curran
and Clark, 2003) on the gold-standard corpora and
our new Wikipedia-derived training data, we eval-
uate the usefulness of the latter and explore the
nature of the training corpus as a variable in NER.
Our Wikipedia-derived corpora exceed the per-
formance of non-corresponding training and test
sets by up to 11% F -score, and can be engineered
to automatically produce models consistent with
various NE-annotation schema. We show that it is
possible to automatically create large, free, named
entity-annotated corpora for general or domain
specific tasks.
2 NER and annotated corpora
Research into NER has rarely considered the im-
pact of training corpora. The CoNLL evalua-
tions focused on machine learning methods (Tjong
Kim Sang, 2002; Tjong Kim Sang and De Meul-
der, 2003) while more recent work has often in-
volved the use of external knowledge. Since many
tagging systems utilise gazetteers of known enti-
ties, some research has focused on their automatic
extraction from the web (Etzioni et al, 2005) or
Wikipedia (Toral et al, 2008), although Mikheev
et al (1999) and others have shown that larger
NE lists do not necessarily correspond to increased
NER performance. Nadeau et al (2006) use such
lists in an unsupervised NE recogniser, outper-
forming some entrants of the MUC Named Entity
Task. Unlike statistical approaches which learn
612
patterns associated with a particular type of entity,
these unsupervised approaches are limited to iden-
tifying common entities present in lists or those
caught by hand-built rules.
External knowledge has also been used to aug-
ment supervised NER approaches. Kazama and
Torisawa (2007) improve their F -score by 3% by
including a Wikipedia-based feature in their ma-
chine learner. Such approaches are limited by the
gold-standard data already available.
Less common is the automatic creation of train-
ing data. An et al (2003) extracted sentences con-
taining listed entities from the web, and produced
a 1.8 million word Korean corpus that gave sim-
ilar results to manually-annotated training data.
Richman and Schone (2008) used a method sim-
ilar to Nothman et al (2008) in order to derive
NE-annotated corpora in languages other than En-
glish. They classify Wikipedia articles in foreign
languages by transferring knowledge from English
Wikipedia via inter-language links. With these
classifications they automatically annotate entire
articles for NER training, and suggest that their re-
sults with a 340k-word Spanish corpus are compa-
rable to 20k-40k words of gold-standard training
data when using MUC-style evaluation metrics.
2.1 Gold-standard corpora
We evaluate our Wikipedia-derived corpora
against three sets of manually-annotated data from
(a) the MUC-7 Named Entity Task (MUC, 2001);
(b) the English CoNLL-03 Shared Task (Tjong
Kim Sang and De Meulder, 2003); (c) the
BBN Pronoun Coreference and Entity Type
Corpus (Weischedel and Brunstein, 2005). We
consider only the generic newswire NER task,
although domain-specific annotated corpora have
been developed for applications such as bio-text
mining (Kim et al, 2003).
Stylistic and genre differences between the
source texts affect compatibility for NER evalua-
tion e.g., the CoNLL corpus formats headlines in
all-caps, and includes non-sentential data such as
tables of sports scores.
Each corpus uses a different set of entity labels.
MUC marks locations (LOC), organisations (ORG)
and personal names (PER), in addition to numeri-
cal and time information. The CoNLL NER shared
tasks (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003) mark PER, ORG and LOC
entities, as well as a broad miscellaneous class
Corpus # tags
Number of tokens
TRAIN DEV TEST
MUC-7 3 83601 18655 60436
CoNLL-03 4 203621 51362 46435
BBN 54 901894 142218 129654
Table 1: Gold-standard NE-annotated corpora
(MISC; e.g. events, artworks and nationalities).
BBN annotates the entire Penn Treebank corpus
with 105 fine-grained tags (Brunstein, 2002): 54
corresponding to CoNLL entities; 21 for numeri-
cal and time data; and 30 for other classes. For
our evaluation, BBN?s tags were reduced to the
equivalent CoNLL tags, with extra tags in the BBN
and MUC data removed. Since no MISC tags are
marked in MUC, they need to be removed from
CoNLL, BBN and Wikipedia data for comparison.
We transformed all three corpora into a com-
mon format and annotated them with part-of-
speech tags using the Penn Treebank-trained
C&C POS tagger. We altered the default MUC
tokenisation to attach periods to abbreviations
when sentence-internal. While standard training
(TRAIN), development (DEV) and final test (TEST)
set divisions were available for CoNLL and MUC,
the BBN corpus was split at our discretion: sec-
tions 03?21 for TRAIN, 00?02 for DEV and 22-24
for TEST. Corpus sizes are compared in Table 1.
2.2 Evaluating NER performance
One challenge for NER research is establishing an
appropriate evaluation metric (Nadeau and Sekine,
2007). In particular, entities may be correctly
delimited but mis-classified, or entity boundaries
may be mismatched.
MUC (Chinchor, 1998) awarded equal score for
matching type, where an entity?s class is identi-
fied with at least one boundary matching, and text,
where an entity?s boundaries are precisely delim-
ited, irrespective of the classification. This equal
weighting is unrealistic, as some boundary errors
are highly significant, while others are arbitrary.
CoNLL awarded exact (type and text) phrasal
matches, ignoring boundary issues entirely and
providing a lower-bound measure of NER per-
formance. Manning (2006) argues that CoNLL-
style evaluation is biased towards systems which
leave entities with ambiguous boundaries un-
tagged, since boundary errors amount simultane-
ously to false positives and false negatives. In both
MUC and CoNLL, micro-averaged precision, recall
and F1 score summarise the results.
613
Tsai et al (2006) compares a number of meth-
ods for relaxing boundary requirements: matching
only the left or right boundary, any tag overlap,
per-token measures, or more semantically-driven
matching. ACE evaluations instead use a customiz-
able evaluation metric with weights specified for
different types of error (NIST-ACE, 2008).
3 Corpus and error analysis approaches
To evaluate the performance impact of a corpus
we may analyse (a) the annotations themselves; or
(b) the model built on those annotations and its
performance. A corpus can be considered in isola-
tion or by comparison with other corpora. We use
three methods to explore intra- and inter-corpus
consistency in MUC, CoNLL, and BBN in Section 4.
3.1 N-gram tag variation
Dickinson and Meurers (2003) present a clever
method for finding inconsistencies within POS an-
notated corpora, which we apply to NER corpora.
Their approach finds all n-grams in a corpus which
appear multiple times, albeit with variant tags for
some sub-sequence, the nucleus (see e.g. Table
3). To remove valid ambiguity, they suggest us-
ing (a) a minimum n-gram length; (b) a minimum
margin of invariant terms around the nucleus.
For example, the BBN TRAIN corpus includes
eight occurrences of the 6-gram the San Francisco
Bay area ,. Six instances of area are tagged as non-
entities, but two instances are tagged as part of the
LOC that precedes it. The other five tokens in this
n-gram are consistently labelled.
3.2 Entity type frequency
An intuitive approach to finding discrepancies be-
tween corpora is to compare the distribution of en-
tities within each corpus. To make this manage-
able, instances need to be grouped by more than
their class labels. We used the following groups:
POS sequences: Types of candidate entities may
often be distinguished by their POS tags, e.g.
nationalities are often JJ or NNPS.
Wordtypes: Collins (2002) proposed wordtypes
where all uppercase characters map to A, low-
ercase to a, and digits to 0. Adjacent charac-
ters in the same orthographic class were col-
lapsed. However, we distinguish single from
multiple characters by duplication. e.g. USS
Nimitz (CVN-68) has wordtype AA Aaa (AA-00).
Wordtype with functions: We also map content
words to wordtypes only?function words
are retained, e.g. Bank of New England Corp.
maps to Aaa of Aaa Aaa Aaa..
No approach provides sufficient discrimination
alone: wordtype patterns are able to distinguish
within common POS tags and vice versa. Each
method can be further simplified by merging re-
peated tokens, NNP NNP becoming NNP.
By calculating the distribution of entities over
these groupings, we can find anomalies between
corpora. For instance, 4% of MUC?s and 5.9%
of BBN?s PER entities have wordtype Aaa A. Aaa,
e.g. David S. Black, while CoNLL has only 0.05% of
PERs like this. Instead, CoNLL has many names of
form A. Aaa, e.g. S. Waugh, while BBN and MUC
have none. We can therefore predict incompatibil-
ities between systems trained on BBN and evalu-
ated on CoNLL or vice-versa.
3.3 Tag sequence confusion
A confusion matrix between predicted and correct
classes is an effective method of error analysis.
For phrasal sequence tagging, this can be applied
to either exact boundary matches or on a per-token
basis, ignoring entity bounds. We instead compile
two matrices: C/P comparing correct entity classes
against predicted tag sequences; and P/C compar-
ing predicted classes to correct tag sequences.
C/P equates oversized boundaries to correct
matches, and tabulates cases of undersized bound-
aries. For example, if [ORG Johnson and Johnson] is
tagged [PER Johnson] and [PER Johnson], it is marked
in matrix coordinates (ORG, PER O PER). P/C em-
phasises oversized boundaries: if gold-standard
Mr. [PER Ross] is tagged PER, it is counted as con-
fusion between PER and O PER. To further dis-
tinguish classes of error, the entity type groupings
from Section 3.2 are also used.
This analysis is useful for both tagger evalua-
tion and cross-corpus evaluation, e.g. BBN versus
CoNLL on a BBN test set. This involves finding
confusion matrix entries where BBN and CoNLL?s
performance differs significantly, identifying com-
mon errors related to difficult instances in the test
corpus as well as errors in the NER model.
4 Comparing gold-standard corpora
We trained the C&C NER tagger (Curran and Clark,
2003) to build separate models for each gold-
standard corpus. The C&C tagger utilises a number
614
TRAIN
With MISC Without MISC
CoNLL BBN MUC CoNLL BBN
MUC ? ? 73.5 55.5 67.5
CoNLL 81.2 62.3 65.9 82.1 62.4
BBN 54.7 86.7 77.9 53.9 88.4
Table 2: Gold standard F -scores (exact-match)
of orthographic, contextual and in-document fea-
tures, as well as gazetteers for personal names. Ta-
ble 2 shows that each training set performs much
better on corresponding (same corpus) test sets
(italics) than on test sets from other sources, also
identified by (Ciaramita and Altun, 2005). NER
research typically deals with small improvements
(?1% F -score). The 12-32% mismatch between
training and test corpora suggests that an appropri-
ate training corpus is a much greater concern. The
exception is BBN on MUC, due to differing TEST
and DEV subject matter. Here we analyse the vari-
ation within and between the gold standards.
Table 3 lists some n-gram tag variations for BBN
and CoNLL (TRAIN + DEV). These include cases of
schematic variations (e.g. the period in Co .) and
tagging errors. Some n-grams have three variants,
e.g. the Standard & Poor ?s 500 which appears un-
tagged, as the [ORG Standard & Poor] ?s 500, or the
[ORG Standard & Poor ?s] 500. MUC is too small for
this method. CoNLL only provides only a few ex-
amples, echoing BBN in the ambiguities of trailing
periods and leading determiners or modifiers.
Wordtype distributions were also used to com-
pare the three gold standards. We investigated all
wordtypes which occur with at least twice the fre-
quency in one corpus as in another, if that word-
type was sufficiently frequent. Among the differ-
ences recovered from this analysis are:
? CoNLL has an over-representation of uppercase words
due to all-caps headlines.
? Since BBN also annotates common nouns, some have
been mistakenly labelled as proper-noun entities.
? BBN tags text like Munich-based as LOC; CoNLL
tags it as MISC; MUC separates the hyphen as a token.
? CoNLL is biased to sports and has many event names
in the form of 1990 World Cup.
? BBN separates organisation names from their products
as in [ORG Commodore] [MISC 64].
? CoNLL has few references to abbreviated US states.
? CoNLL marks conjunctions of people (e.g. Ruth and
Edwin Brooks) as a single PER entity.
? CoNLL text has Co Ltd instead of Co. Ltd.
We analysed the tag sequence confusion when
training with each corpus and testing on BBN DEV.
While full confusion matrices are too large for this
paper, Table 4 shows some examples where the
Figure 1: Deriving training data from Wikipedia
NER models disagree. MUC fails to correctly tag
U.K. and U.S.. U.K. only appears once in MUC, and
U.S. appears 22 times as ORG and 77 times as LOC.
CoNLL has only three instances of Mr., so it often
mis-labels Mr. as part of a PER entity. The MUC
model also has trouble recognising ORG names
ending with corporate abbreviations, and may fail
to identify abbreviated US state names.
Our analysis demonstrates that seemingly mi-
nor orthographic variations in the text, tokenisa-
tion and annotation schemes can have a huge im-
pact on practical NER performance.
5 From Wikipedia to NE-annotated text
Wikipedia is a collaborative, multilingual, online
encyclopedia which includes over 2.3 million arti-
cles in English alone. Our baseline approach de-
tailed in Nothman et al (2008) exploits the hyper-
linking between articles to derive a NE corpus.
Since ?74% of Wikipedia articles describe top-
ics covering entity classes, many of Wikipedia?s
links correspond to entity annotations in gold-
standard NE corpora. We derive a NE-annotated
corpus by the following steps:
1. Classify all articles into entity classes
2. Split Wikipedia articles into sentences
3. Label NEs according to link targets
4. Select sentences for inclusion in a corpus
615
N-gram Tag # Tag #
Co . - 52 ORG 111
Smith Barney , Harris Upham & Co. - 1 ORG 9
the Contra rebels MISC 1 ORG 2
in the West is - 1 LOC 1
that the Constitution MISC 2 - 1
Chancellor of the Exchequer Nigel Lawson - 11 ORG 2
the world ?s - 80 LOC 1
1993 BellSouth Classic - 1 MISC 1
Atlanta Games LOC 1 MISC 1
Justice Minister - 1 ORG 1
GOLF - GERMAN OPEN - 2 LOC 1
Table 3: Examples of n-gram tag variations in BBN (top) and CoNLL (bottom). Nucleus is in bold.
Tag sequence
Grouping
# if trained on
Example
Correct Pred. MUC CoNLL BBN
LOC LOC A.A. 101 349 343 U.K.
- PER PER Aa. Aaa 9 242 0 Mr. Watson
- LOC Aa. 16 109 0 Mr.
ORG ORG Aaa Aaa. 118 214 218 Campeau Corp.
LOC - Aaa. 20 0 3 Calif.
Table 4: Tag sequence confusion on BBN DEV when training on gold-standard corpora (no MISC)
In Figure 1, a sentence introducing Holden as an
Australian car maker based in Port Melbourne has
links to separate articles about each entity. Cues
in the linked article about Holden indicate that it is
an organisation, and the article on Port Melbourne
is likewise classified as a location. The original
sentence can then be automatically annotated with
these facts. We thus extract millions of sentences
from Wikipedia to form a new NER corpus.
We classify each article in a bootstrapping pro-
cess using its category head nouns, definitional
nouns from opening sentences, and title capital-
isation. Each article is classified as one of: un-
known; a member of a NE category (LOC, ORG,
PER, MISC, as per CoNLL); a disambiguation page
(these list possible referent articles for a given ti-
tle); or a non-entity (NON). This classifier classi-
fier achieves 89% F -score.
A sentence is selected for our corpus when all
of its capitalised words are linked to articles with a
known class. Exceptions are made for common ti-
tlecase words, e.g. I, Mr., June, and sentence-initial
words. We also infer additional links ? variant ti-
tles are collected for each Wikipedia topic and are
marked up in articles which link to them ? which
Nothman et al (2008) found increases coverage.
Transforming links into annotations that con-
form to a gold standard is far from trivial. Link
boundaries need to be adjusted, e.g. to remove ex-
cess punctuation. Adjectival forms of entities (e.g.
American, Islamic) generally link to nominal arti-
cles. However, they are treated by CoNLL and our
N-gram Tag # Tag #
of Batman ?s MISC 2 PER 5
in the Netherlands - 58 LOC 4
Chicago , Illinois - 8 LOC 3
the American and LOC 1 MISC 2
Table 5: N-gram variations in the Wiki baseline
BBN mapping as MISC. POS tagging the corpus and
relabelling entities ending with JJ as MISC solves
this heuristically. Although they are capitalised in
English, personal titles (e.g. Prime Minister) are not
typically considered entities. Initially we assume
that all links immediately preceding PER entities
are titles and delete their entity classification.
6 Improving Wikipedia performance
The baseline system described above achieves
only 58.9% and 62.3% on the CoNLL and
BBN TEST sets (exact-match scoring) with 3.5-
million training tokens. We apply methods pro-
posed in Section 3 to to identify and minimise
Wikipedia errors on the BBN DEV corpus.
We begin by considering Wikipedia?s internal
consistency using n-gram tag variation (Table 5).
The breadth of Wikipedia leads to greater genuine
ambiguity, e.g. Batman (a character or a comic
strip). It also shares gold-standard inconsistencies
like leading modifiers. Variations in American and
Chicago, Illinois indicate errors in adjectival entity
labels and in correcting link boundaries.
Some errors identified with tag sequence confu-
sion are listed in Table 6. These correspond to re-
616
Tag sequence
Grouping
# if trained on
Example
Correct Pred. BBN Wiki
LOC LOC Aaa. 103 14 Calif.
LOC - LOC ORG Aaa , Aaa. 0 15 Norwalk , Conn.
LOC LOC Aaa-aa 23 0 Texas-based
- PER PER Aa. Aaa 4 208 Mr. Yamamoto
- PER PER Aaa Aaa 1 49 Judge Keenan
- PER Aaa 7 58 President
MISC MISC A. 25 1 R.
MISC LOC NNPS 0 39 Soviets
Table 6: Tag sequence confusion on BBN DEV with training on BBN and the Wikipedia baseline
sults of an entity type frequency analysis and mo-
tivate many of our Wikipedia extensions presented
below. In particular, personal titles are tagged as
PER rather than unlabelled; plural nationalities are
tagged LOC, not MISC; LOCs hyphenated to fol-
lowing words are not identified; nor are abbrevi-
ated US state names. Using R. to abbreviate Re-
publican in BBN is also a high-frequency error.
6.1 Inference from disambiguation pages
Our baseline system infers extra links using a set
of alternative titles identified for each article. We
extract the alternatives from the article and redirect
titles, the text of all links to the article, and the first
and last word of the article title if it is labelled PER.
Our extension is to extract additional inferred ti-
tles fromWikipedia?s disambiguation pages. Most
disambiguation pages are structured as lists of ar-
ticles that are often referred to by the titleD being
disambiguated. For each link with target A that
appears at the start of a list item on D?s page, D
and its redirect aliases are added to the list of al-
ternative titles for A.
Our new source of alternative titles includes
acronyms and abbreviations (AMP links to AMP
Limited and Ampere), and given or family names
(Howard links to Howard Dean and John Howard).
6.2 Personal titles
Personal titles (e.g. Brig. Gen., Prime Minister-
elect) are capitalised in English. Titles are some-
times linked in Wikipedia, but the target articles,
e.g. U.S. President, are in Wikipedia categories like
Presidents of the United States, causing their incor-
rect classification as PER.
Our initial implementation assumed that links
immediately preceding PER entity links are titles.
While this feature improved performance, it only
captured one context for personal titles and failed
to handle instances where the title was only a
portion of the link text, such as Australian Prime
Minister-elect or Prime Minister of Australia.
To handle titles more comprehensively, we
compiled a list of the terms most frequently linked
immediately prior to PER links. These were man-
ually filtered, removing LOC or ORG mentions and
complemented with abbreviated titles extracted
from BBN, producing a list of 384 base title forms,
11 prefixes (e.g. Vice) and 3 suffixes (e.g. -elect).
Using these gazetteers, titles are stripped of erro-
neous NE tags.
6.3 Adjectival forms
In English, capitalisation is retained in adjectival
entity forms, such as American or Islamic. While
these are not exactly entities, both CoNLL and BBN
annotate them as MISC. Our baseline approach
POS tagged the corpus and marked all adjectival
entities as MISC. This missed instances where na-
tionalities are used nominally, e.g. five Italians.
We extracted 339 frequent LOC and ORG ref-
erences with POS tag JJ. Words from this list
(e.g. Italian) are relabelled MISC, irrespective of
POS tag or pluralisation (e.g. Italian/JJ, Italian/NNP,
Italian/NNPS). This unfiltered list includes some er-
rors from POS tagging, e.g. First, Emmy; and others
where MISC is rarely the appropriate tag, e.g. the
Democrats (an ORG).
6.4 Miscellaneous changes
Entity-word aliases Longest-string matching for
inferred links often adds redundant words, e.g.
both Australian and Australian people are redirects to
Australia. We therefore exclude from inference ti-
tles of form X Y where X is an alias of the same
article and Y is lowercase.
State abbreviations A gold standard may use
stylistic forms which are rare in Wikipedia. For
instance, the Wall Street Journal (BBN) uses US
state abbreviations, while Wikipedia nearly al-
ways refers to states in full. We boosted perfor-
mance by substituting a random selection of US
state names in Wikipedia with their abbreviations.
617
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 82.3 54.9 69.3
CoNLL 85.9 61.9 69.9 86.9 60.2
BBN 59.4 86.5 80.2 59.0 88.0
WP0 ? no inf. 62.8 69.7 69.7 64.7 70.0
WP1 67.2 73.4 75.3 67.7 73.6
WP2 69.0 74.0 76.6 69.4 75.1
WP3 68.9 73.5 77.2 69.5 73.7
WP4 ? all inf. 66.2 72.3 75.6 67.3 73.3
Table 7: Exact-match DEV F -scores
Removing rare cases We explicitly removed
sentences containing title abbreviations (e.g. Mr.)
appearing in non-PER entities such as movie titles.
Compared to newswire, these forms as personal
titles are rare in Wikipedia, so their appearance in
entities causes tagging errors. We used a similar
approach to personal names including of, which
also act as noise.
Fixing tokenization Hyphenation is a problem
in tokenisation: should London-based be one token,
two, or three? Both BBN and CoNLL treat it as one
token, but BBN labels it a LOC and CoNLL a MISC.
Our baseline had split hyphenated portions from
entities. Fixing this to match the BBN approach
improved performance significantly.
7 Experiments
We evaluated our annotation process by build-
ing separate NER models learned from Wikipedia-
derived and gold-standard data. Our results are
given as micro-averaged precision, recall and F -
scores both in terms of MUC-style and CoNLL-style
(exact-match) scoring. We evaluated all experi-
ments with and without the MISC category.
Wikipedia?s articles are freely available for
download.1 We have used data from the 2008
May 22 dump of English Wikipedia which in-
cludes 2.3 million articles. Splitting this into sen-
tences and tokenising produced 32 million sen-
tences each containing an average of 24 tokens.
Our experiments were performed with a
Wikipedia corpus of 3.5 million tokens. Although
we had up to 294 million tokens available, we
were limited by the RAM required by the C&C tag-
ger training software.
8 Results
Tables 7 and 8 show F -scores on the MUC, CoNLL,
and BBN development sets for CoNLL-style exact
1http://download.wikimedia.org/
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 89.0 68.2 79.2
CoNLL 91.0 75.1 81.4 90.9 72.6
BBN 72.7 91.1 87.6 71.8 91.5
WP0 ? no inf. 71.0 79.3 76.3 71.1 78.7
WP1 74.9 82.3 81.4 73.1 81.0
WP2 76.1 82.7 81.6 74.5 81.9
WP3 76.3 82.2 81.9 74.7 80.7
WP4 ? all inf. 74.3 81.4 80.9 73.1 80.7
Table 8: MUC-style DEV F -scores
Training corpus
DEV (MUC-style F )
MUC CoNLL BBN
Corresponding TRAIN 89.0 91.0 91.1
TRAIN + WP2 90.6 91.7 91.2
Table 9: Wikipedia as additional training data
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 73.5 55.5 67.5
CoNLL 81.2 62.3 65.9 82.1 62.4
BBN 54.7 86.7 77.9 53.9 88.4
WP2 60.9 69.3 76.9 61.5 69.9
Table 10: Exact-match TEST results for WP2
TRAIN
With MISC No MISC
CoN. BBN MUC CoN. BBN
MUC ? ? 81.0 68.5 77.6
CoNLL 87.8 75.0 76.2 87.9 74.1
BBN 69.3 91.1 83.6 68.5 91.9
WP2 70.2 79.1 81.3 68.6 77.3
Table 11: MUC-eval TEST results for WP2
match and MUC-style evaluations (which are typi-
cally a few percent higher). The cross-corpus gold
standard experiments on the DEV sets are shown
first in both tables. As in Table 2, the performance
drops significantly when the training and test cor-
pus are from different sources. The corresponding
TEST set scores are given in Tables 9 and 10.
The second group of experiments in these ta-
bles show the performance of Wikipedia corpora
with increasing levels of link inference (described
in Section 6.1). Links inferred upon match-
ing article titles (WP1) and disambiguation ti-
tles (WP2) consistently increase F -score by ?5%,
while surnames for PER entities (WP3) and all link
texts (WP4) tend to introduce error. A key re-
sult of our work is that the performance of non-
corresponding gold standards is often significantly
exceeded by our Wikipedia training data.
Our third group of experiments combined our
Wikipedia corpora with gold-standard data to im-
prove performance beyond traditional train-test
pairs. Table 9 shows that this approach may lead
618
Token Corr. Pred. Count Why?
. ORG - 90 Inconsistencies in BBN
House ORG LOC 56 Article White House is a LOC due to classification bootstrapping
Wall - LOC 33 Wall Street is ambiguously a location and a concept
Gulf ORG LOC 29 Georgia Gulf is common in BBN, but Gulf indicates LOC
, ORG - 26 A difficult NER ambiguity in e.g. Robertson , Stephens & Co.
?s ORG - 25 Unusually high frequency of ORGs ending ?s in BBN
Senate ORG LOC 20 Classification bootstrapping identifies Senate as a house, i.e. LOC
S&P - MISC 20 Rare in Wikipedia, and inconsistently labelled in BBN
D. MISC PER 14 BBN uses D. to abbreviate Democrat
Table 12: Tokens in BBN DEV that our Wikipedia model frequently mis-tagged
Class
By exact phrase By token
P R F P R F
LOC 66.7 87.9 75.9 64.4 89.8 75.0
MISC 48.8 58.7 53.3 46.5 61.6 53.0
ORG 76.9 56.5 65.1 88.9 68.1 77.1
PER 67.3 91.4 77.5 70.5 93.6 80.5
All 68.6 69.9 69.3 80.9 75.3 78.0
Table 13: Category results for WP2 on BBN TEST
to small F -score increases.
Our per-class Wikipedia results are shown in
Table 13. LOC and PER entities are relatively easy
to identify, although a low precision for PER sug-
gests that many other entities have been marked
erroneously as people, unlike the high precision
and low recall of ORG. As an ill-defined category,
with uncertain mapping between BBN and CoNLL
classes, MISC precision is unsurprisingly low. We
also show results evaluating the correct labelling
of each token, where Nothman et al (2008) had
reported results 13% higher than phrasal match-
ing, reflecting a failure to correctly identify entity
boundaries. We have reduced this difference to
9%. A BBN-trained model gives only 5% differ-
ence between phrasal and token F -score.
Among common tagging errors, we identified:
tags continuing over additional words as in New
York-based Loews Corp. all being marked as a sin-
gle ORG; nationalities marked as LOC rather than
MISC; White House a LOC rather than ORG, as
with many sports teams; single-word ORG entities
marked as PER; titles such as Dr. included in PER
tags; mis-labelling un-tagged title-case terms and
tagged lowercase terms in the gold-standard.
The corpus analysis methods described in
Section 3 show greater similarity between our
Wikipedia-derived corpus and BBN after imple-
menting our extensions. There is nonetheless
much scope for further analysis and improvement.
Notably, the most commonly mis-tagged tokens in
BBN (see Table 12) relate more often to individual
entities and stylistic differences than to a general-
isable class of errors.
9 Conclusion
We have demonstrated the enormous variability in
performance between using NER models trained
and tested on the same corpus versus tested on
other gold standards. This variability arises from
not only mismatched annotation schemes but also
stylistic conventions, tokenisation, and missing
frequent lexical items. Therefore, NER corpora
must be carefully matched to the target text for rea-
sonable performance. We demonstrate three ap-
proaches for gauging corpus and annotation mis-
match, and apply them to MUC, CoNLL and BBN,
and our automatically-derived Wikipedia corpora.
There is much room for improving the results of
our Wikipedia-based NE annotations. In particu-
lar, a more careful approach to link inference may
further reduce incorrect boundaries of tagged en-
tities. We plan to increase the largest training set
the C&C tagger can support so that we can fully
exploit the enormous Wikipedia corpus.
However, we have shown that Wikipedia can
be used a source of free annotated data for train-
ing NER systems. Although such corpora need
to be engineered specifically to a desired appli-
cation, Wikipedia?s breadth may permit the pro-
duction of large corpora even within specific do-
mains. Our results indicate that Wikipedia data
can perform better (up to 11% for CoNLL on MUC)
than training data that is not matched to the eval-
uation, and hence is widely applicable. Trans-
formingWikipedia into training data thus provides
a free and high-yield alternative to the laborious
manual annotation required for NER.
Acknowledgments
We would like to thank the Language Technol-
ogy Research Group and the anonymous review-
ers for their feedback. This project was sup-
ported by Australian Research Council Discovery
Project DP0665973 and Nothman was supported
by a University of Sydney Honours Scholarship.
619
References
Joohui An, Seungwoo Lee, and Gary Geunbae Lee.
2003. Automatic acquisition of named entity tagged
corpus from world wide web. In The Companion
Volume to the Proceedings of 41st Annual Meeting
of the Association for Computational Linguistics,
pages 165?168.
Ada Brunstein. 2002. Annotation guidelines for an-
swer types. LDC2005T33.
Nancy Chinchor. 1998. Overview of MUC-7. In Proc.
of the 7th Message Understanding Conference.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Proceedings of the
NIPS Workshop on Advances in Structured Learning
for Text and Speech Processing.
Michael Collins. 2002. Ranking algorithms for
named-entity extraction: boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
489?496, Morristown, NJ, USA.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 164?167.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 107?114, Budapest, Hungary.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91?134.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 698?707.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus?a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1):i180?i182.
Christopher Manning. 2006. Doing named entity
recognition? Don?t optimize for F1. In NLPers
Blog, 25 August. http://nlpers.blogspot.
com.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of the 9th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 1?8, Bergen, Norway.
2001. Message Understanding Conference (MUC) 7.
Linguistic Data Consortium, Philadelphia.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30:3?26.
David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Proceedings of the 19th Canadian Conference on
Artificial Intelligence, volume 4013 of LNCS, pages
266?277.
NIST-ACE. 2008. Automatic content extraction 2008
evaluation plan (ACE08). NIST, April 7.
Joel Nothman, James R. Curran, and Tara Murphy.
2008. Transforming Wikipedia into named entity
training data. In Proceedings of the Australian Lan-
guage Technology Workshop, pages 124?132, Ho-
bart.
Alexander E. Richman and Patrick Schone. 2008.
Mining wiki resources for multilingual named entity
recognition. In 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1?9, Columbus, Ohio.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the 7th Conference on Natural Lan-
guage Learning, pages 142?147.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning, pages
1?4.
Antonio Toral, Rafael Mun?oz, and Monica Monachini.
2008. Named entityWordNet. In Proceedings of the
6th International Language Resources and Evalua-
tion Conference.
Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi
Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting-
Yi Sung, and Wen-Lian Hsu. 2006. Various criteria
in the evaluation of biomedical named entity recog-
nition. BMC Bioinformatics, 7:96?100.
RalphWeischedel and Ada Brunstein. 2005. BBN Pro-
noun Coreference and Entity Type Corpus. Linguis-
tic Data Consortium, Philadelphia.
620
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Named Entity Recognition in Wikipedia
Dominic Balasuriya Nicky Ringland Joel Nothman Tara Murphy James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{dbal7610,nicky,joel,tm,james}@it.usyd.edu.au
Abstract
Named entity recognition (NER) is used in
many domains beyond the newswire text
that comprises current gold-standard cor-
pora. Recent work has used Wikipedia?s
link structure to automatically generate
near gold-standard annotations. Until now,
these resources have only been evaluated
on newswire corpora or themselves.
We present the first NER evaluation on
a Wikipedia gold standard (WG) corpus.
Our analysis of cross-corpus performance
on WG shows that Wikipedia text may
be a harder NER domain than newswire.
We find that an automatic annotation of
Wikipedia has high agreement with WG
and, when used as training data, outper-
forms newswire models by up to 7.7%.
1 Introduction
Named Entity Recognition (NER) is the task of
identifying and classifying people, organisations
and other named entities (NE) within text. NER is
central to many NLP systems, especially informa-
tion extraction and question answering.
Machine learning approaches now dominate
NER, learning patterns associated with individual
entity classes from annotated training data. This
training data, including English newswire from
the MUC-6, MUC-7 (Chinchor, 1998), and CONLL-
03 (Tjong Kim Sang and De Meulder, 2003) com-
petitive evaluation tasks, and the BBN Pronoun
Coreference and Entity Type Corpus (Weischedel
and Brunstein, 2005), is critical to the success of
these approaches.
This data dependence has impeded the adapta-
tion or porting of existing NER systems to new
domains, such as scientific or biomedical text,
e.g. Nobata et al (2000). Similar domain sensi-
tivity is exhibited by most tasks across NLP, e.g.
parsing (Gildea, 2001), and the adaptation penalty
is still apparent even when the same set of named
entity classes is used in text from similar domains
(Ciaramita and Altun, 2005).
Wikipedia is an important corpus for informa-
tion extraction, e.g. Bunescu and Pas?ca (2006)
and Wu et al (2008) because of its size, cur-
rency, rich semi-structured content, and its closer
resemblance to web text than newswire. Recently,
Wikipedia?s markup has been exploited to auto-
matically derive NE annotated text for training sta-
tistical models (Richman and Schone, 2008; Mika
et al, 2008; Nothman et al, 2008).
However, without a gold standard, existing eval-
uations of these models were forced to compare
against mismatched newswire corpora or the noisy
Wikipedia-derived annotations themselves. Fur-
ther, it was not possible to directly ascertain the
accuracy of these automatic extraction methods.
We have manually annotated 39,007 tokens of
Wikipedia with coarse-grained named entity tags
(WG). We present the first evaluation of Wiki-
pedia-trained models on Wikipedia: the C&C NER
tagger (Curran and Clark, 2003b) trained on (a)
automatically annotated Wikipedia text (WP2) ex-
tracted by Nothman et al (2009); and (b) tradi-
tional newswire NER corpora (MUC, CONLL and
BBN). The WP2 model, though trained on noisy
annotations, outperforms newswire models on WG
by 7.7%. However, every model, including WP2,
performs far worse on WG than on the newswire.
We examined the quality of WG, and found that
our annotation strategy produced a high-quality,
consistent corpus. Our analysis suggests that it is
the form and distribution of NEs in Wikipedia that
make it a difficult target domain.
Finally, we compared WG with the annotations
extracted by Nothman et al (2009), and found
agreement comparable to our inter-annotator
agreement, demonstrating that NE corpora can be
derived very accurately from Wikipedia.
10
2 Background
Traditional evaluations of NER have considered
the performance of a tagger on test data from the
same source as its training data. Although the
majority of annotated corpora available consist of
newswire text, recent practical applications cover
a far wider range of genres, including Wikipedia,
blogs, RSS feeds, and other data sources. Cia-
ramita and Altun (2005) showed that even when
moving a short distance, e.g. annotating WSJ text
with the same scheme as CONLL?s Reuters, the per-
formance was 26% worse than on the original text.
Similar differences are reported by Nothman
et al (2009) who compared MUC, CONLL and
BBN annotations reduced to a common tag-set.
They found poor cross-corpus performance to be
due to tokenisation and annotation scheme mis-
match, missing frequent lexical items, and naming
conventions. They then compared automatically-
annotated Wikipedia text as training data and
found it also differs in otherwise inconsequen-
tial ways from the newswire corpora, in particular
lacking abbreviations necessary to tag news text.
2.1 Automatic Wikipedia annotation
Wikipedia, a collaboratively-written online ency-
clopedia, is readily exploited in NLP, because it is
large, semi-structured and multilingual. Its arti-
cles often correspond to NEs, so it has been used
for NE recognition (Kazama and Torisawa, 2007)
and disambiguation (Bunescu and Pas?ca, 2006;
Cucerzan, 2007). Wikipedia links often span NEs,
which may be exploited to automatically create
annotated NER training data by determining the
entity class of the linked article and then labelling
the link text with it.
Richman and Schone (2008) use article clas-
sification knowledge from English Wikipedia to
produce NE-annotated corpora in other languages
(evaluated against NE gold standards for French,
Spanish, and Ukrainian). Mika et al (2008) ex-
plored the use of tags from a CONLL-trained tag-
ger to seed the labelling of entities and evaluate
the performance of a Wikipedia-trained model by
hand.
We make use of an approach described by Noth-
man et al (2009) which is engineered to perform
well on BBN data with a reduced tag-set (LOC,
MISC, ORG, PER). They derive an annotated cor-
pus with the following steps:
1. Classify Wikipedia articles into entity classes
2. Split the articles into tokenised sentences
3. Label expanded links according to target NEs
4. Select sentences for inclusion in a corpus
To prepare the text, they use mwlib (Pedi-
aPress, 2007) to parse Wikipedia?s native markup
retaining only paragraph text with links, ap-
ply Punkt (Kiss and Strunk, 2006) estimated on
Wikipedia text to perform sentence boundary de-
tection, and tokenise the resulting text using regu-
lar expressions.
Nothman et al (2009) infer additional NEs not
provided by existing links, and apply rules to ad-
just link boundaries and classifications to closer
match BBN annotations.
2.2 NER evaluation
Meaningful automatic evaluation of NER is dif-
ficult and a number of metrics have been pro-
posed (Nadeau and Sekine, 2007). Ambiguity
leads to entities correctly delimited but misclas-
sified, or boundaries mismatched despite correct
classification.
Although the MUC-7 evaluation (Chinchor,
1998) defined a metric which was less sensitive
to often-meaningless boundary errors, we consider
only exact entity matches as correct, following
the standard CONLL evaluation (Tjong Kim Sang,
2002). We report precision, recall and F -score for
each entity type.
3 Creating the Wikipedia gold standard
We created a corpus by manually annotating the
text of 149 articles from the May 22, 2008 dump
of English Wikipedia. The articles were selected
at random from all articles describing named en-
tities, with a roughly equal proportion of arti-
cle topics from each of the four CONLL-03 classes
(LOC, MISC, ORG, PER). We adopted Nothman et
al.?s (2008) preprocessing described above to pro-
duce tokenised sentences for annotation.
Only body text was extracted from the chosen
articles for inclusion in the corpus. Four articles
were found not to have any usable text, consisting
solely of tables, lists, templates and section head-
ings, which we remove. Their exclusion leaves a
corpus of 145 articles.
3.1 Annotation
Annotation was initially carried out using a fine-
grained tag-set which was expanded by the an-
11
[COMPANY Aero Gare] was a kitplane manufacturer
founded by [PERSON Gary LeGare] in [CITY Mojave] ,
[STATE California] to marketed the [PLANE Sea Hawker]
amphibious aircraft .
[ORG Aero Gare] was a kitplane manufacturer founded
by [PER Gary LeGare] in [LOC Mojave] , [LOC Califor-
nia] to marketed the [MISC Sea Hawker] amphibious
aircraft .
(a) Fine-grained annotation (b) Coarse-grained annotation
Figure 1: An example of coarse and fine-grained annotation of Wikipedia text.
notators as annotation progressed, and eventually
contained 96 tags.
We created a mapping from these fine-grained
tags to the four coarse-grained tags used in the
CONLL-03 data: PER, LOC, MISC and ORG. This
enables evaluation with existing NER models. We
believe this two-phase approach allowed anno-
tators to defer difficult mapping decisions, (e.g.
should an airport be classified as a LOC, ORG, or
MISC?) which can then be made after discussion.
The mapping could also be modified to suit a par-
ticular evaluation task.
Figure 1 shows an example of the use of fine and
coarse-grained tags to annotate a sentence. Tags
such as PERSON correspond directly to coarse-
grained tags, while most map to a more general
tag, such as STATE and CITY mapping to LOC.
PLANE is an example of a fine-grained tag that
cannot be mapped to LOC, ORG, or PER. These
tags may be mapped to MISC; some are not con-
sidered entities under the CONLL scheme and are
left unlabelled in the coarse-grained annotation.
Three independent annotators were involved in
the annotation process. Annotator 1 annotated all
145 articles using the fine-grained tags. Annota-
tors 2 and 3 then re-annotated 19 of these articles
(316 sentences or 8030 tokens), amounting to 21%
of the corpus. Annotator 2 used the fine-grained
tags described above, while Annotator 3 used the
four coarse-grained CONLL tags. To measure vari-
ation, all three annotations of this common portion
were mapped down to the CONLL tag-set and inter-
annotator agreement was calculated.
We found that 202 tokens were disagreed upon
by at least one annotator (2.5% of all tokens
annotated), and these discrepancies were then
discussed by the three annotators. The inter-
annotator agreement will be analysed in more de-
tail in Section 5.
Sentences containing grammatical and typo-
graphical errors were not corrected, so that the cor-
pus would be as close as possible to the source
text. Web text often contains errors, such as to
Train Test P R F
WP2 WG 66.5 67.4 66.9
BBN WG 59.2 59.1 59.2
CONLL WG 54.3 57.2 55.7
WP2 * WG * 75.1 67.7 71.2
BBN * WG * 57.2 64.1 60.4
CONLL * WG * 53.1 62.7 57.5
MUC * WG * 52.3 57.2 54.6
WP2 BBN 73.4 74.6 74.0
WP2 CONLL 73.6 64.9 69.0
WP2 * MUC * 86.2 68.9 76.6
BBN BBN 85.7 87.3 86.5
CONLL CONLL 85.3 86.5 85.9
MUC MUC 81.0 83.6 82.3
Table 2: Tagger performance on various corpora.
Asterisks indicate that MISC tags are ignored.
marketed the Sea Hawker from the example in Fig-
ure 1, so any NER system must deal with these er-
rors. Sentences with poor tokenisation or sentence
boundary detection were identified and corrected
manually, since these errors are introduced by our
processing and annotation, and do not exist in the
source text.
The final corpus was created by correcting an-
notation mistakes, with annotators 2 and 3 each
correcting 50% of the corpus. The fine-grained
tags were mapped to the four CONLL tags before
the final corrections were made. The final WG cor-
pus consists of the body text of 145 Wikipedia ar-
ticles tagged with the four CONLL-03 tags.
4 NER on the Wikipedia gold-standard
Nothman et al (2009) have previously shown that
that an NER system trained on automatically anno-
tated Wikipedia corpora performs reasonably well
on non-Wikipedia text. Having created our WG
corpus of gold-standard annotations, we are able
to evaluate the performance of these models on
Wikipedia text.
We compare the C&C NE maximum-entropy
tagger (Curran and Clark, 2003b) trained on
gold-standard newswire corpora (MUC-7, BBN and
CONLL-03) with the same tagger trained on auto-
matically annotated Wikipedia text, WP2. WG is
12
WG WP2 BBN CONLL-03 MUC-7
Test Train Train Test Train Test Train Test
Tokens 39 007 3 500 032 901 849 129 654 203 621 46 435 83 601 60 436
Sentences 1 696 146 543 37 843 5 462 14 987 3 453 3 485 2 419
Articles 145 ? 1775 238 946 231 102 99
NEs 3 558 288 545 49 999 7 307 23 498 5 648 4 315 3 540
Table 1: Corpus sizes.
too small to train a reasonable NER model on gold-
standard Wikipedia annotations. Part-of-speech
tags are added to all corpora using the C&C POS
tagger (Curran and Clark, 2003a) before training
and testing.
1
We evaluate each model on tradi-
tional newswire evaluation corpora as well as WG.
Table 1 gives the size of each corpus.
The results are shown in Table 2. The WP2 tag-
ger performed substantially better on WG than tag-
gers trained on newswire text, with a 7?11% in-
crease in F -score compared to BBN and CONLL-
03, and a 16% increase compared to MUC-7, when
miscellaneous NEs in the corpus are not consid-
ered in the evaluation. The Wikipedia trained
model thus outperforms newswire models on our
new WG corpus even though the training annota-
tions were automatically extracted.
The WP2 tagger performed worse on WG than
on gold-standard news corpora (BBN and CONLL),
with a 2?7% reduction in F -score. Further, the
performance of WP2 on WG is 11?20% F -score
lower than same-source evaluation results, e.g.
BBN on BBN, CONLL on CONLL. Therefore, de-
spite WP2 showing an advantage in tagging WG
due to their common source domain, we find that
WG?s annotations are harder to predict than the
newswire test data commonly used for evaluation.
One possible explanation is that our WG corpus
has been inconsistently annotated. When NEs of
miscellaneous type are not considered in the eval-
uation (asterisks in Table 2), the performance of all
taggers on WG improves, with WP2 demonstrating
a 4% increase. This result suggests another par-
tial explanation: that MISC NEs in Wikipedia are
more difficult to annotate correctly, due to their
poor definition and broad coverage. A third ex-
planation is that the automatic conversion process
proposed by Nothman et al (2008) produces much
lower quality training data than manual annota-
tion. We explore these three possibilities below.
1
Both taggers are available from http://svn.ask.
it.usyd.edu.au/trac/candc.
Token Exact NE only
A1 and A2 0.95 0.99 0.88
A1 and A3 0.91 0.95 0.81
A2 and A3 0.91 0.96 0.79
Fleiss? Kappa 0.92 0.97 0.83
Table 3: Initial human inter-annotator agreement.
5 Quality of the Wikipedia gold standard
The low performance observed on WG may be
due to the poor quality of its annotation. We en-
sure that this is not the case by measuring inter-
annotator agreement. The WG annotation process
produced three independent annotations of a sub-
set of WG. These annotations were compared us-
ing Cohen?s ? (Fleiss and Cohen, 1973) between
pairs of annotators, and Fleiss? ? (Fleiss, 1971),
which generalises Cohen?s ? to more than two
concurrent annotations.
Table 3 shows the three types of ? values cal-
culated. Token is calculated on a per token basis,
comparing the agreement of annotators on each
token in the corpus; NE only, is calculated on
the agreement between entities alone, excluding
agreement in cases where all annotators agreed
that a token was not a NE; Exact refers to the
agreement between annotators where all annota-
tors have agreed on the boundaries of a NE, but
disagree on the type of NE.
Annotator 1 originally annotated the entire cor-
pus, and Annotators 2 and 3 then corrected exactly
half of the corpus each after a discussion between
the three annotators to resolve ambiguities. Landis
and Koch (1977) determine that a ? value greater
than 0.81 indicates almost perfect agreement. By
this standard, our three annotators were in strong
agreement prior to discussion, with our Fleiss? ?
values all greater than 0.81. Inconsistencies in the
corpus due to annotation mistakes by Annotator 1
were corrected by Annotators 2 and 3.
Inter-annotator agreement for cases where the
annotators agreed on NE boundaries was higher
than agreement on each token, which suggests
that many discrepancies resulted from NE bound-
13
LOC MISC ORG PER H(C): With O Without O Total NEs % NE tokens
WG 28.5 20.0 25.2 26.3 0.98 2.0 3 558 17.1
BBN 22.4 9.8 46.4 21.3 0.61 1.7 49 999 9.6
MUC 33.3 ? 40.7 26.1 0.52 1.5 4 315 8.1
CONLL 30.4 14.6 26.9 28.1 0.98 1.9 23 498 17.1
Table 4: NE class distribution, tag entropy and NE density statistics for gold-standard corpora and WG.
ary ambiguities, or disagreement as to whether
a phrase constituted a NE at all. Higher inter-
annotator agreement between Annotators 1 and 2
leads us to believe that the two-phase annotation
strategy, where an initially fine-grained tag-set is
reduced, results in more consistent annotation.
Our analysis demonstrates that WG is annotated
in a consistent and accurate manner and the small
number of errors cannot alone explain the reduced
performance figures.
6 Comparing gold-standard corpora
6.1 NE class distribution
Table 4 compares the distribution of different
classes of NEs across different corpora on the four
CONLL categories. WG has a higher proportion of
PER and MISC NEs and a lower proportion of ORG
NEs than the BBN corpus. This is also found in the
MUC corpus, although comparisons to MUC are af-
fected by its lack of a MISC category. The CONLL-
03 corpus is most similar to WG in terms of the
distribution of the NE classes, although CONLL-03
has a smaller proportion of MISC NEs than WG.
An analysis of the lengths of NEs in CONLL shows,
however, that they are very different to those in
WG (see Table 8), perhaps explaining the differ-
ence in performance observed.
Tag entropy H(C) was calculated for each cor-
pus with respect to the 5 possible classes (4 NE
classes, and the O tag, indicating non-entities).
H(C) is a measure of the amount of information
required to represent the classification of each to-
ken in the corpus. Two calculations are made, in-
cluding and excluding the frequent O tag. Our re-
sults (Table 4) suggest that WG?s tags are least pre-
dictable, with a tag entropy of 2.0 bits (without the
O class) compared to 1.7 and 1.9 bits for BBN and
CONLL respectively.
6.2 Fine-grained class distribution
While the CONLL-03 and MUC evaluation corpora
are marked up with only very coarse tags, the BBN
corpus uses 29 coarse tags, many with specific
subtypes, including NEs, descriptors of NEs and
Mapped BBN tag WG BBN
PERSON 25.9 19.3
ORGANIZATION:OTHER 13.0 2.8
ORGANIZATION:CORPORATION 9.2 43.1
GPE:CITY 8.0 6.7
WORK OF ART:SONG 4.7 0.1
NORP 4.3 3.1
WORK OF ART:OTHER 4.1 1.3
GPE:COUNTRY 3.5 5.1
ORGANIZATION:EDUCATIONAL 3.0 0.9
GPE:STATE PROVINCE 2.8 2.8
ORGANIZATION:POLITICAL 2.6 0.6
EVENT:OTHER 2.5 0.4
ORGANIZATION:GOVERNMENT 2.0 7.5
WORK OF ART:BOOK 1.6 0.4
EVENT:WAR 1.6 0.1
FAC:OTHER 1.4 0.2
LOCATION:REGION 1.3 0.8
FAC:ATTRACTION 1.2 0.0
Table 5: Distribution of some fine-grained tags
non-NEs, intended as answer types for question
answering (Brunstein, 2002). Non-NE types in-
clude MONEY and TIME, which are also tagged in
the MUC corpus, and others such as ANIMAL. When
evaluating the performance of the taggers, each of
BBN?s 150 fine-grained tags was mapped to one of
four coarse-grained classes or none, using a map-
ping described in Nothman (2008).
However, since the WG corpus was initially an-
notated using 96 distinct classes, we map these
tags to the corresponding fine-grained BBN NE
classes. In some cases, the tags map exactly
(e.g. COUNTRY mapped to LOCATION:COUNTRY);
in other cases, classes have to be merged or not
mapped at all, where the BBN and WG annotations
differ in granularity. Where possible, we map to
fine-grained BBN categories.
We create mappings to a total of 36 BBN entity
types, and apply them across the WG corpus. Table
5 shows the distribution of the most common tags,
calculated as a percentage of all counts of the 36
selected tags across each corpus. Tags for which
there is at least a two-fold difference in proportion
between BBN and WG are marked in bold.
The comparison is dominated by the
presence of a disproportionate number of
ORG:CORPORATIONS in the BBN corpus com-
14
1 2 3 4 5 6 7+ # NEs
WG 53.0 77.0 88.9 94.8 96.6 98.2 100 712
BBN (train) 75.0 91.0 95.4 97.2 98.2 98.7 100 4 913
CONLL (train) 75.0 93.8 98.1 99.5 99.9 99.9 100 3 437
Table 6: Comparing MISC NE lengths (cumulative).
Feature group WG BBN CONLL
Current token 0.88 0.89 0.93
Current POS 0.43 0.57 0.48
Current word-type 0.42 0.49 0.48
Previous token 0.46 0.43 0.47
Previous POS 0.12 0.19 0.14
Previous word-type 0.07 0.14 0.12
Table 7: Feature-tag gain ratios.
pared to WG. It also mentions many more
governmental organisations. Prominent cases of
tags found in higher proportions in WG are works
of art, organisations of type OTHER (e.g. bands,
sports teams, clubs), events and attractions.
This comparison demonstrates that there are ob-
servable differences in NE types between the news
and Wikipedia domains. These differences are re-
flected in the distribution of both coarse and fine-
grained types of NEs. The more complex entity
distribution in Wikipedia is a likely cause for re-
duced NER performance on WG.
6.3 Feature-tag gain
Nobata et al (2000) use gain ratio as an infor-
mation-theoretic measure of corpus difficulty:
GR(C;F ) =
I(C;F )
H(C)
where I(C;F ) = H(C) ? H(C|F ) is the infor-
mation gain of the NE tag distribution (C) with re-
spect to a feature set F .
This gain ratio normalises the information gain
over the tag entropy, which Nobata et al (2000)
suggest allows us to compare gain ratios between
corpora. It also makes the impact of including the
?O? tag negligible for our calculations.
We apply this approach to measure the relative
difficulty of tagging NEs in the WG corpus. Ta-
ble 7 shows that WG tags seem generally harder
to predict than those in newswire, on the basis of
words, POS tags or orthographic word-types (like
those used in the Curran and Clark (2003b) tagger
as proposed by Collins (2002)).
In particular, POS tags are less indicative than
in BBN and CONLL, suggesting a wider variety of
1 2 3 4 5 6 7+
WG 49.9 81.7 93.1 97.4 98.6 99.4 100
BBN (train) 57.4 83.3 92.9 97.4 99.1 99.6 100
CONLL (train) 63.1 94.5 98.4 99.4 99.8 99.9 100
MUC (train) 62.0 89.1 96.1 99.1 99.7 99.8 100
Table 8: Comparing all NE lengths (cumulative).
grammatical functions in NE names in Wikipedia
? this might be expected with more band names,
and song and movie titles. Alternatively, it may be
an indication that the POS tagging is less reliable
on Wikipedia using newswire-trained models.
The previous word?s orthographic form also
provides less information, which may relate to ti-
tles like Mr. and Mrs., strong indicators of PER en-
tities, which are frequent in BBN and to a lesser
extent CONLL, but are almost absent in Wikipedia.
6.4 Lengths of named entities
The number of tokens in NEs is substantially dif-
ferent between WG and other gold-standard cor-
pora. When compared with WG, other gold-
standard corpora have a larger proportion of
single-word NEs (between 7 and 13% more), as
shown in Table 8. The distribution of NE lengths
in BBN is most similar to WG, but it still differs
significantly in the proportion of single-word NEs.
Additionally, WG has a larger number of long
multi-word NEs than the other gold-standard cor-
pora. Longer entities are more difficult to clas-
sify, since boundary resolution is more error prone
and they typically contain lowercase words with
a wider range of syntactic roles. This adds to the
difficulty of correctly identifying NEs in WG.
The difference in entity lengths is most pro-
nounced MISC NEs (Table 6), with Wikipedia hav-
ing a substantially smaller number of single-word
MISC NEs. The presence of a large number of long
miscellaneous NEs, including song, film and book
titles, and other works of art are a feature that char-
acterises the nature of Wikipedia text in contrast
to newswire text. Typically, longer MISC NEs in
newswire text are laws and NORPs, which also ap-
pear in Wikipedia text.
15
1 2 3 4 5 6 7+ # NEs
WG 49.2 82.9 94.2 98.0 99.2 99.8 100 2 846
BBN (train) 55.4 82.4 92.6 97.4 99.2 99.7 100 45 086
CONLL (train) 61.1 94.7 98.4 99.4 99.8 99.9 100 20 061
MUC (train) 62.0 89.1 96.1 99.1 99.7 99.8 100 4 315
Table 9: Comparing non-MISC NE lengths (cumulative).
# Sents # with NEs # NEs
WG 1 696 1 341 3 558
WG WP2-style 571 298 569
WG WP4-style 698 425 831
Table 10: Size of WG and auto-annotated subsets.
7 Evaluation of automatic annotation
We compared the gold-standard annotations in our
WG corpus to those sentences that were automati-
cally annotated by Nothman et al (2009). Their
automatic annotation process does not retain all
Wikipedia sentences. Rather, it selects sentences
where, on the basis of capitalisation heuristics,
it seems all named entities in the sentence have
been tagged by the automatic process. We adopt
this confidence criterion to produce automatically-
annotated subsets of the WG corpus.
Two variants of their automatic annotation pro-
cedure were used: WP2 uses a few rules to infer
tags for non-linked NEs in Wikipedia; WP4 has
looser criteria for inferring additional links, and its
over-generation typically reduced its performance
as training data (Nothman et al, 2009).
A large proportion of sentences in our WG cor-
pus cannot be automatically tagged with confi-
dence. Sentence selection leaves 571 sentences
(33.7%) after the WP2 process and 698 (41.2%)
after the WP4 process (see Table 10). The use of
the more permissive WP4 process may lead to the
labelling of more NEs, but many may be spurious.
We use three approaches to compare automatic
and manual annotations of WG text: (a) treat each
corpus as test data and evaluate NER performance
on each; (b) treat WP2 and WP4-style subsets as
NER predictions on the WG corpus to calculate an
F -score; and (c) treat the automatic annotations
like human annotators and calculate ? values.
We first evaluate the WP2 model on each
corpus and find that performance is higher on
automatically-annotated subsets of WG (Table 11).
This is unsurprising given the common automatic
annotation process and the effects of the selection
criterion. However, Nothman (2008) provides an
TRAIN TEST P R F
WP2 WG manual 66.5 67.4 66.9
WP2 WG WP2-style 76.0 72.9 74.4
WP2 WG WP4-style 75.5 71.4 73.4
WP2 WP2 ten folds ? ? 83.6
WP2 * WG manual * 75.1 67.7 71.2
WP2 * WG WP2-style * 81.5 74.4 77.8
WP2 * WG WP4-style * 81.9 74.6 78.1
WP2 * WP2 ten folds * ? ? 86.1
Table 11: NER performance of the WP2-trained
model on auto-annotated subsets of WG.
? NE ? P R F
WP2-style 0.94 0.84 89.0 89.0 89.0
WP4-style 0.93 0.83 86.8 87.6 87.2
Table 12: Comparing WP2-style WG and WP4-
style WG on WG. The automatically annotated data
was treated as predicted annotations on WG.
F -score for the WP2 model when evaluated on 10
folds of automatically-annotated (WP2-style) test
data. This F -score is 8?10% higher than WP2?s
performance on the WP2-style subset of WG, sug-
gesting that WG?s text is somewhat more difficult
to annotate than typical portions of WP2-style text.
We compare the annotations of WG text more
directly by treating the automatic annotations as
if they are the output from a tagger run on the 698
and 571 sentences that were confidently chosen. A
reasonable agreement between the gold standard
and automatic annotation is observed (Table 12),
with F -scores of 87.2% and 89.0% achieved by
WP2 and WP4.
Table 12 also shows inter-annotator agreement
calculated between the automatically annotated
subsets and the gold-standard annotations in WG,
using Cohen?s ? in the same way as for human an-
notators. The agreement was very high: equal or
better than the agreement between human annota-
tors prior to discussion and correction.
8 Conclusion
We have presented the first evaluation of named
entity recognition (NER) on a gold-standard eval-
uation of Wikipedia, a resource of increasing
16
importance in Computational Linguistics. We
annotated a corpus of Wikipedia articles (WG)
with gold-standard NE tags. Using this new re-
source as test data we have evaluated models
trained on three gold-standard newswire corpora
for NER, and compared them to a model trained on
Wikipedia-derived NER annotations (Nothman et
al., 2009). We found that this WP2 model outper-
formed models trained on MUC, CONLL, and BBN
data by more than 7.7% F -score.
However, we found that all four models per-
formed significantly worse on the WG corpus than
they did on news text, suggesting that Wikipedia
as a textual domain is more difficult for NER. We
initially suspected that annotation quality was re-
sponsible, but found that we had very high inter-
annotator agreement even before further discus-
sion and correction of the corpus. This also val-
idates our approach of creating many fine-grained
categories and then reducing them down to the
four CONLL types.
To further examine the difficulty of tagging WG,
we compared the distribution of fine-grained entity
types in WG and BBN, finding a more even dis-
tribution over a larger range of types in WG. We
found that the standard NER features such as cur-
rent and previous POS tags and words had lower
predictive power on WG. We also compared the
distribution of NEs lengths and showed that WG
entities are longer on average (for instance song
and book titles). This all suggests that Wikipedia
is genuinely more difficult to automatically anno-
tate with named entities than newswire.
Finally, we compared the common sentences
between Nothman et al?s (2009) automatic NE an-
notation of Wikipedia and WG, directly measuring
the quality of automatically deriving NE annota-
tions from Wikipedia.
We found that WP2 agreed with our final
WG corpus to a high degree, demonstrating that
Wikipedia is a viable source of automatically an-
notated NE annotated data, reducing our depen-
dence on expensive manual annotation for training
NER systems.
Acknowledgements
We would like to thank the anonymous review-
ers for their helpful feedback. This work was
supported by the Australian Research Council un-
der Discovery Project DP0665973. Dominic Bal-
asuriya was supported by a University of Syd-
ney Outstanding Achievement Scholarship. Nicky
Ringland was supported by a Capital Markets
CRC High Achievers Scholarship. Joel Noth-
man was supported by a Capital Markets CRC
PhD Scholarship and a University of Sydney Vice-
Chancellor?s Research Scholarship.
References
Ada Brunstein. 2002. Annotation guidelines for an-
swer types. LDC2005T33, Linguistic Data Consor-
tium, Philadelphia.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of the 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 9?16.
Nancy Chinchor. 1998. Overview of MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Proceedings of the
NIPS Workshop on Advances in Structured Learning
for Text and Speech Processing.
Michael Collins. 2002. Ranking algorithms for
named-entity extraction: boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 489?496.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 708?
716.
James R. Curran and Stephen Clark. 2003a. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proceedings of the 10th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 91?98, Budapest, Hungary,
12?17 April.
James R. Curran and Stephen Clark. 2003b. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 164?167.
Joseph L. Fleiss and Jacob Cohen. 1973. The equiv-
alence of weighted kappa and the intraclass corre-
lation coefficient as measures of reliability. Educa-
tional and Psychological Measurement, 33(3):613.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
17
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In 2001 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
Pittsburgh, PA.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 698?707.
Tibor Kiss and Jan Strunk. 2006. Unsupervised mul-
tilingual sentence boundary detection. Computa-
tional Linguistics, 32(4):485?525.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,
and Jordi Atserias. 2008. Learning to tag and tag-
ging to learn: A case study on wikipedia. IEEE In-
telligent Systems, 23(5, Sep./Oct.):26?33.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30:3?26.
Chikashi Nobata, Nigel Collier, and Jun?ichi Tsuji.
2000. Comparison between tagged corpora for the
named entity task. In Proceedings of the Workshop
on Comparing Corpora, pages 20?27.
Joel Nothman, James R Curran, and Tara Murphy.
2008. Transforming Wikipedia into named entity
training data. In Proceedings of the Australasian
Language Technology Association Workshop 2008,
pages 124?132, Hobart, Australia, December.
Joel Nothman, Tara Murphy, and James R. Curran.
2009. Analysing Wikipedia and gold-standard cor-
pora for NER training. In Proceedings of the
12th Conference of the European Chapter of the
ACL (EACL 2009), pages 612?620, Athens, Greece,
March.
Joel Nothman. 2008. Learning Named Entity Recogni-
tion from Wikipedia. Honours Thesis. School of IT,
University of Sydney.
PediaPress. 2007. mwlib MediaWiki parsing library.
http://code.pediapress.com.
Alexander E. Richman and Patrick Schone. 2008.
Mining wiki resources for multilingual named en-
tity recognition. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1?
9, Columbus, Ohio.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recogni-
tion. In Proceedings of the 7th Conference on Natu-
ral Language Learning, pages 142?147, Edmonton,
Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning, pages
1?4, Taipei, Taiwan.
Ralph Weischedel and Ada Brunstein. 2005.
BBN Pronoun Coreference and Entity Type Cor-
pus. LDC2005T33, Linguistic Data Consortium,
Philadelphia.
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008.
Information extraction from Wikipedia: Moving
down the long tail. In Proceedings of the 14th In-
ternational Conference on Knowledge Discovery &
Data Mining, Las Vegas, USA, August.
18
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 19?26,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Accurate Argumentative Zoning with Maximum Entropy models
Stephen Merity and Tara Murphy and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{smerity,tm,james}@it.usyd.edu.au
Abstract
We present a maximum entropy classifier
that significantly improves the accuracy of
Argumentative Zoning in scientific litera-
ture. We examine the features used to
achieve this result and experiment with
Argumentative Zoning as a sequence tag-
ging task, decoded with Viterbi using up
to four previous classification decisions.
The result is a 23% F-score increase on the
Computational Linguistics conference pa-
pers marked up by Teufel (1999).
Finally, we demonstrate the performance
of our system in different scientific do-
mains by applying it to a corpus of As-
tronomy journal articles annotated using a
modified Argumentative Zoning scheme.
1 Introduction
The task of generating automatic summarizations
of one or more texts is a central problem in Natu-
ral Language Processing (NLP). Summarization is
a fundamental component for future information
retrieval and question answering systems, incor-
porating both natural language understanding and
natural language generation.
Comprehension-based summarization, e.g.
Kintsch and Van Dijk (1978) and Brown et al
(1983), is the most ambitious model of automatic
summarization, requiring a complete understand-
ing of the text. Due to the failure of rule-based
NLP and knowledge representation, other less
knowledge-intensive methods now dominate.
Sentence extraction, e.g. Brandow et al (1995)
and Kupiec et al (1995), selects a small number
of abstract worthy sentences from a larger text.
The resulting sentences form a collection of ex-
cerpt sentences meant to capture the essence of the
text. The next stage is information fusion (Barzi-
lay et al, 1999; Knight and Marcu, 2000) which
attempts to combine the excerpts into a more cohe-
sive text. These methods can create inflexible and
incoherent extracts that result in under-informative
results (Teufel et al, 1999).
Argumentative Zoning (Teufel, 1999; Teufel
and Moens, 2002) attempts to solve this prob-
lem by representing the structure of a text us-
ing a rhetorically-based schema. Sentences are
classified into one of a small number of non-
hierarchical argumentative roles, which can then
be used in both the sentence extraction and text
generation/fusion phase of automatic summariza-
tion. Argumentative Zoning can enable tailored
summarizations depending on the needs of the
user, e.g. a layperson versus a domain expert.
The first experiments in Argumentative Zon-
ing used Na??ve Bayes (NB) classifiers (Kupiec et
al., 1995; Teufel, 1999) which assume conditional
independence of the features. However, this as-
sumption is rarely true for the kinds of rich feature
representations we want to use for most NLP tasks.
Maximum entropy (ME) models have become
popular in NLP because they can incorporate evi-
dence from the complex, diverse and overlapping
features needed to represent language. Some ex-
ample applications include part-of-speech (POS)
tagging (Ratnaparkhi, 1996), parsing (Johnson et
al., 1999), language modelling (Rosenfeld, 1996),
and text categorisation (Nigam et al, 1999).
We have developed an Argumentative Zoning
(zone) classifier using a ME model. We compare
our zone classifier to a reimplementation of Teufel
and Moens (2002)?s NB classifier and features on
their original Computational Linguistics corpus.
Like Teufel (1999), we model zone classification
as a sequence tagging task. Our zone classifier
achieves an F-score of 96.88%, a 20% improve-
ment. We also show how Argumentative Zoning
can be applied to other domains by evaluating our
system on a corpus of Astronomy journal articles,
achieving an F-measure of 97.9%.
19
Category Abbr. Description
Background BKG general scientific background
Other OTH neutral descriptions of other researcher?s work
Own OWN neutral descriptions of the authors? new work
Aim AIM statements of the particular aim of the current paper
Textual TXT statements of textual organisation of the current paper
Contrast CTR contrastive or comparative statements about other work
explicit mention of weaknesses of other work
Basis BAS statements that own work is based on other work
Table 1: Teufel?s (1999) Argumentative Zones
2 Argumentative Zoning
Teufel (1999) introduced a new rhetorical analy-
sis for scientific texts called Argumentative Zon-
ing. Each sentence of an article from the scien-
tific literature is classified into one of seven basic
rhetorical structures shown in Table 1.
The first three: Background, Other, and Own,
are part of the basic schema and represent attribu-
tion of intellectual ownership. The four additional
categories: aim, textual, contrast, and basis, are
based upon Swales (1990)?s Creating A Research
Space (CARS) model, and provide pointed infor-
mation about the author?s stance and the paper it-
self. Teufel assumes that each sentence only re-
quires a single classification and that all sentences
clearly fit into the above structure. The assump-
tion is clearly not always correct, but is a useful
approximation nevertheless.
Due to the specific nature of these classifica-
tions it is hoped that this will allow for much more
robust automatic abstraction generation. Sum-
maries of a paper could be created specifically for
the user, either focusing on the aim of the work,
the work?s stance in the field (what other works it
is based upon or compared with) and so on.
Teufel used Argumentative Zoning to determine
the author?s use and opinion of other authors they
cite in their work and also to create Rhetorical
Document Profiles (RDP), a type of summariza-
tion used to provide typical information that a new
reader may need in a systematic manner.
For the use of Argumentative Zoning in RDPs
Teufel (1999) points out that due to the redun-
dancy in language that near perfect accuracy is not
required as important pieces of information will be
repeated in the paper. Recognising these salient
points once is enough for them to be included in
the RDP. In further tasks, such as the analysis of
the function of citations (Teufel et al, 2006) and
automatic summarization, higher levels of accu-
racy are more critical.
3 Maximum Entropy models
Maximum entropy (ME) or log-linear models are
statistical models that can incorporate evidence
from a diverse range of complex and potentially
overlapping features. Unlike Na??ve Bayes (NB),
the features can be conditionally dependent given
the class, which is important since feature sets in
NLP rarely satisfy this independence constraint.
The ME classifier uses models of the form:
p(y|x) =
1
Z(x)
exp
(
n?
i=1
?ifi(x, y)
)
(1)
where y is the zone label, x is the context (the sen-
tence) and the fi(x, y) are the features with asso-
ciated weights ?i.
The probability of a sequence of zone labels
y1 . . . yn given a sequence of sentences is s1 . . . sn
is approximated as follows:
p(y1 . . . yn|s1 . . . sn) ?
n?
i=1
p(yi|xi) (2)
where xi is the context for sentence si. In our ex-
periments that treat argumentative zoning as a se-
quence labelling task, the context xi incorporates
history information ? i.e. the previous labelling
decisions of the classifier. Optimal decoding of
this sequence uses the Viterbi algorithm, which we
compare against the Oracle case of knowing the
correct label for the previous sentence.
The features are binary valued functions which
pair a zone label with various elements of the sen-
tential context; for example:
fj(x, y) =
{
1 if goal ? x & y = AIM
0 otherwise (3)
goal ? x, that is, the word goal is part of the
context of the sentence, is a contextual predicate.
The central idea in maximum entropy mod-
elling is that the model chosen should satisfy all of
the constraints imposed by the training data (in the
20
form of empirical feature counts from the train-
ing data) whilst remaining as unbiased as possi-
ble. This is achieved by selecting the model with
the maximum entropy, i.e. the most uniform dis-
tribution, given the constraints.
Our classifier uses the maximum entropy imple-
mentation described in Curran and Clark (2003).
Generalised Iterative Scaling (GIS) is used to esti-
mate the values of the weights and we use a Gaus-
sian prior over the weights (Chen and Rosenfeld,
1999) which allows many rare, but informative,
features to be used without overfitting. This will
be an important property when we use sparse fea-
tures like bigrams in the models below.
4 Modelling Argumentative Zones
4.1 Our Features
The two primary sources of features for our zone
classifier were the words in the sentences and the
position of the sentence relative to the rest of the
paper. A number of feature types use additional
external resources (e.g. semantic lists of agents or
common rhetorical patterns) or annotations (e.g.
named entities). Where feasible we have reimple-
mented the features described in Teufel (1999). In
other cases, our features are somewhat simpler.
Since the Curran and Clark (2003) classifier
only accepts binary features, any numerical fea-
tures had to be bucketed into smaller sets of alter-
natives to reduce sparseness, either by integer di-
vision or through reducing the number by scaling
to a small integer range. The features we imple-
mented are described below.
Unigrams, bigrams and n-grams
A sub-sequence of n words from a given sentence.
We include unigram and bigram features and re-
port them individually and together (as n-grams).
These features include all of the unigrams and
bigrams above the feature cutoff, unlike Teufel?s
cont-1 features below. Also, both the Compu-
tational Linguistics and Astronomy corpora con-
tain marked up citations, cross-references to ta-
bles, figures, and sections and mathematical ex-
pressions. In the Computational Linguistics cor-
pus self citations are distinguished from other ci-
tations. These structured elements have been nor-
malised to a single token each, e.g. __CITE__.
These tokens have been retained in the unigram
and bigram features.
first The first four words of a sentence, added in-
dividually.
Sections, positions, and lengths
section A section counter which increments on
each heading to measure the distance into the doc-
ument. It does not take into consideration whether
they are sub-headings or similar. There are two
versions of this feature. The first is a straight
counter (1 to n) and the second is grouped into two
buckets representing each half of the paper (break-
ing at the middle section).
location The position of a sentence between two
headings (representing a section). There are two
versions of this feature, one counts to a maxi-
mum of 10 and the other represents a percentage
through the section bucketed into 20% intervals.
paragraph The position of the sentence within a
paragraph. Again there are two features ? either
straight counts (with a maximum of 10) or buck-
eted into thirds of a paragraph.
length of sentence grouped into multiples of 3.
Named entity features
Our astronomy corpus has been manually anno-
tated with domain-specific named entity informa-
tion (Murphy et al, 2006). There are 12 coarse-
grained categories and 43 fine-grained categories
including star, galaxy, telescope, as well as a num-
ber of the usual categories including person, or-
ganisation and location. Both the coarse-grained
and fine-grained categories were used as features.
4.2 Teufel (1999)?s features
To compare with previous work, we also im-
plemented most of the features that gave Teufel
(1999) the best performance. We list all of the fea-
ture types in Table 2, indicating which ones have
and have not been implemented.
Teufel?s unigram features (cont-1) are filtered
using TF-IDF to select the top scoring 10 words in
each document, and then these are used to mark
the top 40 sentences in each document containing
those filtered words.
TLoc marks the position of the sentence over
the entire paper, using 10 unevenly sized segments
(larger segments are in the middle of the paper).
Struct-1 marks where a sentence appears in a
section. It divides each section into three equally
sized segments; singles out the first and the last
sentence as separate segments; the second and
21
Name Impl? Description
Cont-1 yes An application of TF-IDF over the words and sentences
Cont-2 partial Does the sentence contain words in the title or heading (excluding stop words)
TLoc yes Position of the sentence in relation to 10 segments (A-J)
Struct-1 yes Position within a section
Struct-2 yes Relative position of sentence within a paragraph
Struct-3 partial Type of headline of the current section
TLength yes Is the sentence longer than 15 words?
Syn-1 no Voice of the first finite verb in the sentence
Syn-2 no Tense of the first finite verb in the sentence
Syn-3 no Is the first finite verb modified by a modal auxiliary
Cit-1 yes Does the sentence contain a citation or name of author?
Formu yes Does a formulaic expression occur in the sentence
Ag-1 yes Type of agent
Ag-2 yes Type of action (with or without negation)
Table 2: Teufel (1999)?s set of features
third sentence as a sixth segment; and the second-
last plus third-last sentence as a seventh segment.
Struct-3 the type of section heading for the cur-
rent section. In our case, we have not mapped
these down to the reduced set used by Teufel.
Formu uses pattern matching rules to iden-
tify formulaic expressions. Ag-1 and Ag-2 iden-
tify agent and action expressions from gazetteers.
Teufel (1999) provides these in the appendices.
4.3 Feature Cutoff
Features that occur rarely in the training set are
problematic because the statistics extracted for
these features are not reliable. They may still con-
tribute positively to the ME model because we use
Gaussian smoothing (Chen and Rosenfeld, 1999)
help avoid overfitting.
Instead of including every possible feature, we
used a cutoff to remove features that occur less
than four times. This primarily applies to the
n-gram features, especially bigrams, which were
quite sparse given the small quantity of training
data. Due to the speed of the ME implementation
it is possible to have quite a low cut-off.
4.4 History features and Viterbi
In order to take advantage of the predictabil-
ity of tags given prior sequences (for example,
AIM commonly following itself) we used history
features and treated Argumentative Zoning as a
sequence labelling task. Since each prediction
now relies on the previous decisions we used the
Viterbi algorithm to find the optimal sequence.
Given the small number of labelling alter-
natives, we experimented with several history
lengths ranging from previous label to the previ-
ous four labels. To determine the impact of this
feature in an ideal situation, we also experimented
with using an Oracle set of history features.
5 Results
Our results are produced using ten-fold cross val-
idation and are reported in terms of precision, re-
call and f-score for each of the zone classes, and a
weighted average over all classes. We have inves-
tigated the impact of each feature type using sub-
tractive analysis, where we have also calculated
paired t-test confidence intervals (the error values
reported are the 95% confidence interval).
The baselines for both sets were already quite
high (at least 70%) due to the common tag of
OWN, representing the author?s own work, but our
results show significant improvements over this
baseline.
5.1 CMP-LG Corpus
The CMP-LG corpus is a collection of 80 con-
ference papers collected by Teufel (1999) from
the Computation and Language E-Print Archive 1.
The LATEX source was converted to HTML with La-
tex2HTML then transformed into XML with cus-
tom PERL scripts. This text was then tokenized us-
ing the TTT (Text Tokenization) System into Penn
Treebank format. The result is a corpus of 12,000
annotated sentences, containing 333,000 word to-
kens, in XML format.
We attempted to recreate Teufel?s original ex-
periments by emulating the features she used with
the same type of classifier. We usedWeka?s (Frank
et al, 2005) implementation of the NB classifier.
Table 3 reproduces the results from Teufel and
Moens (2002) alongside our reimplementation of
1http://xxx.lanl.gov/cmp-lg/
22
original reproduced
Tag P R F P R F
AIM 44 65 52 45.8 57.8 51.1
BAS 37 40 38 23.8 37.0 28.9
CTR 34 20 26 33.1 19.2 24.3
BKG 40 50 45 46.9 53.6 50.1
OTH 52 39 44 70.6 55.0 61.8
TXT 57 66 61 66.3 47.6 55.4
OWN 84 88 86 86.7 90.8 88.7
Weighted 72 73 72 76.8 76.8 76.8
Table 3: Teufel and Moens (2002)?s and our NB
performance on CMP-LG
History Type Order Performance
Baseline None 93.16
Viterbi First 1.77 ? 0.49%
Viterbi Second 1.97 ? 0.42%
Viterbi Third 2.08 ? 0.45%
Viterbi Fourth 2.1 ? 0.46%
Viterbi Fifth 2.13 ? 0.46%
Oracle First 3.67 ? 0.68%
Oracle Second 4.06 ? 0.70%
Table 4: History features on the CMP-LG corpus
with ME model of unigram/bigram features only
Feature Classifier Viterbi
Ngrams -21.39?2.35% -23.23?3.24%
Unigram -8.00?1.02% -7.53?1.14%
Bigram -7.89?1.20% -6.87?1.44%
Concept -0.06?0.24% -0.06?0.16%
First -1.24?0.44% -1.14?0.39%
Length -0.34?0.24% -0.40?0.25%
Section -0.42?0.27% -0.27?0.33%
Location 0.03?0.20% 0.04?0.07%
Paragraph 0.10?0.15% 0.01?0.08%
All 95.69% 96.88%
Table 5: Subtractive analysis CMP-LG ME model
the features using Weka?s NB classifier. We have
been able to replicate their results to a reasonable
extent ? gaining higher overall performance using
most of their original features. Notably, our Other
class is significantly more accurate whilst the orig-
inal Basis class did better.
Our next experiment investigated the value of
treating Argumentative Zoning as a sequence la-
belling task, i.e. the impact of the Markov history
features and Viterbi decoding on performance. For
these experiments we only used the unigram and
bigram features with the maximum entropy clas-
sifier. Table 4 presents the results: the baseline is
already much higher than the NB classifier which
is a result of both the unigram/bigram features and
the ME classifier itself.
The improvement using longer Markov win-
dows (up to 2.13%) is also shown ? and longer
windows are better, although there is diminishing
returns. We chose a Markov history of the four
previous decisions for the rest of our experiments.
Table 4 also shows that knowing the previous label
perfectly (with the Oracle experiment) can make a
large difference to classification accuracy.
Feature Change
TLength -2.09?9.96%
Struct-1 0.38?6.08%
TLoc 0.96?7.25%
Struct-3 -1.65?6.76%
Cont-2 -1.10?6.39%
Struct-2 1.59?7.99%
Ag-1/2 -0.39?8.97%
Formu 0.14?8.46%
Cit-1 -1.88?5.19%
Cont-1 -0.38?5.85%
All 70.25%
Table 6: Teufel?s Subtractive analysis CMP-LG ME
Table 5 presents the subtractive analysis to de-
termine the impact of different feature types. From
this we can see that the n-grams (unigrams and bi-
grams) have by far the largest impact ? and neither
of these feature types was directly implemented by
Teufel and Moens (2002). The next most impor-
tant features are the first few words (again a uni-
gram type feature), length and the section number.
The Markov history features also have an impact
of just over 1%.
Table 6 shows a different story for Teufel?s fea-
tures using the maximum entropy model. It seems
that none of the feature types alone are making an
enormous contribution and that the impact of them
varies enormously between folds (the confidence
intervals are far bigger than the differences).
Finally, Table 7 gives the results of using the
maximum entropy model with Markov history
length four and all of the features. Overall, we
improve Teufel and Moens? performance by just
under 20% on our reproduced experiments.
5.2 Astronomical Corpus
The astronomical corpus was created by Mur-
phy et al (2006) and consists of papers obtained
from arXiv (2005)?s astrophysics section (astro-
ph). The papers were converted from LATEX to
Unicode by a custom script which attempted to re-
tain as much of the paper?s special characters and
formatting as possible.
The resulting text was then processed using
MXTerminator (Reynar and Ratnaparkhi, 1997)
with an additional Python script to find sentence
23
Category Abbr. Description
Background BKG As has been noted in prior studies , Abell|GXYC 2255|GXYC has an unusually large number
of galaxies with extended radio emission .
Other OTH This is consistent with the findings of Hogg|P Fruchter|P ( 1999|DAT ) who found that GRB
hosts are in general subluminous galaxies .
Own OWN We scanned the data of about 1.8|DUR year|DUR ( TJDs|DUR 11000-11699|DUR ) and found
30 new GRB-like events .
Data DAT In Fig . REF we present the 1.4|FRQ GHz|FRQ radio images of the cluster A2744|GXYC ,
at different angular resolutions . (subclassed from OWN)
Observation OBS Smith|P et al ( 2001|DAT ) reported no detection of transient emission at sub-mm ( 850|WAV
um|WAV ) wavelengths . (subclassed from OTH)
Technique TEC Reduction of the NIR images was performed with the IRAF|CODE and STSDAS|CODE pack-
ages . (subclassed from OWN)
Figure 1: Examples of sentences with the given tags in the astronomical corpus
Tag P R F
AIM 96.5 88.2 92.2
BAS 86.7 89.8 88.2
CTR 92.1 89.0 90.5
BKG 86.0 96.3 90.9
OTH 96.3 91.7 93.9
TXT 98.2 93.8 95.9
OWN 98.6 99.2 98.9
Weighted 96.88 96.88 96.88
Table 7: Final CMP-LG ME performance
Feature Classifier Viterbi
Ngrams -18.83?3.74% -16.03?2.99%
Unigram -5.51?1.37% -5.25?2.00%
Bigram -2.04?0.78% -1.79?0.87%
Concept -0.18?0.29% -0.05?0.12%
Entity -0.18?0.39% -0.31?0.23%
First -0.02?0.29% -0.86?0.79%
Length -0.06?0.16% -0.08?0.10%
Paragraph -0.04?0.20% 0.07?0.19%
Section -0.29?0.24% -0.40?0.57%
Location -0.09?0.25% 0.06?0.15%
All 98.15% 96.68%
Table 8: Subtractive analysis ASTRO ME model
boundaries, and then tokenized using the Penn
Treebank (Marcus et al, 1993) sed script, with an-
other Python script fixing common errors. The
LATEX, which the tokenizer split off incorrectly,
was then reattached.
Each sentence of the corpus was then anno-
tated using a modified version of the Argumen-
tative Zoning schema. While the original three
zones: Background, Own, Other are used, we have
replaced the CARS labels with content labels de-
scribing aspects of the work: DAT for data used
in the analysis, OBS for observations performed,
and TEC for techniques applied. Only Own and
Other are subclassed with the extended schema of
Data, Observation and Techniques. Examples of
each zone classification are shown in Figure 1.
Tag P R F
BKG 92.1 97.1 94.5
OTH 95.0 97.1 96.1
OTH-DAT 100.0 92.3 96.0
OTH-OBS 91.3 93.3 92.3
OTH-TEC 100.0 100.0 100.0
OWN 99.9 99.3 99.6
OWN-DAT 95.9 86.6 91.0
OWN-OBS 98.2 89.4 93.6
OWN-TEC 90.4 100.0 94.9
Weighted 97.9 97.9 97.9
Table 9: Final ASTRO ME model performance
Table 8 shows the impact of different feature
types on classification accuracy for the Astron-
omy corpus. Again, the most important features
are the n-grams (although to a slightly lesser ex-
tent than for the Computational Linguistics cor-
pus). The other features make very little contri-
bution at all. Disappointingly, the (gold-standard)
named entity features contribute very little addi-
tional information ? which is surprising given that
the content categories (data and observation) are
directly connected with some of the entity types
(like telescope).
In the Astronomy corpus, the Markov history
features actually have a detrimental effect, which
suggests the history is misleading. This warrants
further exploration, but we suspect there may be
more changing backwards and forwards between
argumentative zones in the Astronomy corpus.
Overall, we can see that the two tasks are of a sim-
ilar level of difficulty of around 96% F-score.
Table 9 shows the distribution over zones and
content labels for the Astronomy corpus. The
Background label is the hardest to reproduce even
though it is not split into content sub-types. The
sub-types are relatively rare for Other, so the re-
sults should not be considered as reliable.
24
Tag P R F
BKG NB CMP-LG 51.5% 61.1% 55.9%
OTH NB CMP-LG 73.0% 64.2% 68.3%
OWN NB CMP-LG 91.9% 93.1% 92.5%
BKG NB ASTRO 63.1% 63.5% 63.3%
OTH NB ASTRO 53.9% 39.7% 45.7%
OWN NB ASTRO 88.5% 93.0% 90.7%
BKG ME CMP-LG 53.6% 27.5% 36.3%
OTH ME CMP-LG 63.0% 24.4% 35.2%
OWN ME CMP-LG 81.7% 96.8% 88.6%
BKG ME ASTRO 61.2% 29.5% 39.8%
OTH ME ASTRO 50.4% 20.0% 28.6%
OWN ME ASTRO 81.2% 96.7% 88.2%
Table 10: Comparing CMP-LG and ASTRO directly
on the basic annotation scheme
Table 10 compares the performance of our
Na??ve Bayes and Maximum Entropy classifiers
on the two corpora for just the basic annotation
scheme: Background, Own and Other. The fea-
tures used are the set of Teufel features we have
implemented (so it does not include unigram or
bigram features).
The results show that classifiers for both cor-
pora behave in quite similar ways on the basic
scheme. Own is by far the most frequent category,
and not surprisingly, it is most accurately classi-
fied in both domains. Background seems to be eas-
ier to distinguish in Astronomy, but Other is more
distinct in Computational Linguistics.
Further, we see no advantage to using maximum
entropy models over Na??ve Bayes when the fea-
ture set is not sophisticated/overlapping enough,
and the dataset large enough, to warrant the extra
power (and cost).
6 Conclusion
This paper has presented newmodels of Argumen-
tative Zoning using Maximum Entropy (ME) mod-
els. We have demonstrated that using ME models
with standard word features, such as unigrams and
bigrams, significantly outperforms Na??ve Bayes
models incorporating task-specific features. Fur-
ther, these task-specific features had very little ad-
ditional impact on the ME model.
Our ME model has raised the state-of-the-art
in automatic Argumentative Zoning classification
from 76% to 96.88% F-score on Teufel?s Compu-
tational Linguistics conference paper corpus.
To test the wider applicability of Argumentative
Zoning, we have annotated a corpus of Astronomy
journal articles with a modified zone and content
scheme, and achieved a similar level of perfor-
mance using our maximum entropy classifier. We
found that more sophisticated semantic features,
e.g. gold-standard named entities, also had little
impact on the accuracy of our classifier.
Now that we have a very accurate Argumenta-
tive Zone classifier, we would like to investigate
the impact of Argumentative Zones in information
retrieval, question answering, and summarization
tasks, particularly in the astronomy domain, where
we have additional tools such as the named entity
recognizer.
In summary, using a maximum entropy classi-
fier with simple unigram and bigram features re-
sults in a very accurate classifier for Argumenta-
tive Zones across multiple domains.
Acknowledgements
We would like to thank Sophie Liang and the
anonymous reviewers for their helpful feedback
on this paper. This work has been supported by
the Australian Research Council under Discovery
project DP0665973. The first author was sup-
ported by the Microsoft Research Asia Scholar-
ship in IT at the University of Sydney.
References
arXiv. 2005. arxiv.org archive. http://arxiv.org.
R. Barzilay, K.R. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. In Proceedings of the 37th an-
nual meeting of the Association for Computational
Linguistics on Computational Linguistics, pages
550?557. Association for Computational Linguistics
Morristown, NJ, USA.
R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Information Processing and management,
31(5):675?685.
A.L. Brown, J.D. Day, and R.S. Jones. 1983. The
development of plans for summarizing texts. Child
Development, pages 968?979.
Stanley Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical report, Carnegie Mellon University, Pitts-
burgh, PA.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proceedings of the 10th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 91?98, Budapest, Hungary,
12?17 April.
25
E. Frank, M.A. Hall, G. Holmes, R. Kirkby,
B. Pfahringer, I.H. Witten, and L. Trigg. 2005.
Weka-a machine learning workbench for data min-
ing. The Data Mining and Knowledge Discovery
Handbook, pages 1305?1314.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Rie-
zler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proceedings of the 37th Meet-
ing of the ACL, pages 535?541, University of Mary-
land, MD.
W. Kintsch and T.A. Van Dijk. 1978. Toward a model
of text comprehension and production. Psychologi-
cal review, 85(5):363?94.
K. Knight and D. Marcu. 2000. Statistics-based
summarization-step one: Sentence compression. In
Proceedings of the National Conference on Artifi-
cial Intelligence, pages 703?710. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of the
18th annual international ACM SIGIR conference
on Research and development in information re-
trieval, pages 68?73. ACM New York, NY, USA.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The
penn treebank.
T. Murphy, T. McIntosh, and J.R. Curran. 2006.
Named entity recognition for astronomy literature.
In Proceedings of the 2006 Australasian Language
Technology Workshop (ALTW).
K. Nigam, J. Lafferty, and A. McCallum. 1999. Us-
ing maximum entropy for text classification. In
Proceedings of the IJCAI-99 Workshop on Machine
Learning for Information Filtering, pages 61?67,
Stockholm, Sweden.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the EMNLP
Conference, pages 133?142, Philadelphia, PA.
J.C. Reynar and A. Ratnaparkhi. 1997. A maximum
entropy approach to identifying sentence bound-
aries. In Proceedings of the fifth conference on Ap-
plied natural language processing, pages 16?19.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modeling. Computer,
Speech and Language, 10:187?228.
J.M. Swales. 1990. Genre analysis: English in aca-
demic and research settings. Cambridge University
Press.
S. Teufel and M. Moens. 2002. Summarising scientific
articles? experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
S. Teufel, J. Carletta, and M. Moens. 1999. An anno-
tation scheme for discourse-level argumentation in
research articles. In Proceedings of EACL 1999.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 103?110.
S. Teufel. 1999. Argumentative zoning: Information
extraction from scientific text. Ph.D. thesis, Univer-
sity of Edinburgh, Edinburgh, UK.
26
